docid,title,abstract,url,published,authors
0,RAG vs. GraphRAG: A Systematic Evaluation and Key Insights,"Retrieval-Augmented Generation (RAG) enhances the performance of LLMs across
various tasks by retrieving relevant information from external sources,
particularly on text-based data. For structured data, such as knowledge graphs,
GraphRAG has been widely used to retrieve relevant information. However, recent
studies have revealed that structuring implicit knowledge from text into graphs
can benefit certain tasks, extending the application of GraphRAG from graph
data to general text-based data. Despite their successful extensions, most
applications of GraphRAG for text data have been designed for specific tasks
and datasets, lacking a systematic evaluation and comparison between RAG and
GraphRAG on widely used text-based benchmarks. In this paper, we systematically
evaluate RAG and GraphRAG on well-established benchmark tasks, such as Question
Answering and Query-based Summarization. Our results highlight the distinct
strengths of RAG and GraphRAG across different tasks and evaluation
perspectives. Inspired by these observations, we investigate strategies to
integrate their strengths to improve downstream tasks. Additionally, we provide
an in-depth discussion of the shortcomings of current GraphRAG approaches and
outline directions for future research.",http://arxiv.org/abs/2502.11371v1,2/17/25,"Haoyu Han, Harry Shomer, Yu Wang, Yongjia Lei, Kai Guo, Zhigang Hua, Bo Long, Hui Liu, Jiliang Tang"
1,Empowering GraphRAG with Knowledge Filtering and Integration,"In recent years, large language models (LLMs) have revolutionized the field
of natural language processing. However, they often suffer from knowledge gaps
and hallucinations. Graph retrieval-augmented generation (GraphRAG) enhances
LLM reasoning by integrating structured knowledge from external graphs.
However, we identify two key challenges that plague GraphRAG:(1) Retrieving
noisy and irrelevant information can degrade performance and (2)Excessive
reliance on external knowledge suppresses the model's intrinsic reasoning. To
address these issues, we propose GraphRAG-FI (Filtering and Integration),
consisting of GraphRAG-Filtering and GraphRAG-Integration. GraphRAG-Filtering
employs a two-stage filtering mechanism to refine retrieved information.
GraphRAG-Integration employs a logits-based selection strategy to balance
external knowledge from GraphRAG with the LLM's intrinsic reasoning,reducing
over-reliance on retrievals. Experiments on knowledge graph QA tasks
demonstrate that GraphRAG-FI significantly improves reasoning performance
across multiple backbone models, establishing a more reliable and effective
GraphRAG framework.",http://arxiv.org/abs/2503.13804v1,3/18/25,"Kai Guo, Harry Shomer, Shenglai Zeng, Haoyu Han, Yu Wang, Jiliang Tang"
2,A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models,"Large language models (LLMs) have demonstrated remarkable capabilities in a
wide range of tasks, yet their application to specialized domains remains
challenging due to the need for deep expertise. Retrieval-augmented generation
(RAG) has emerged as a promising solution to customize LLMs for professional
fields by seamlessly integrating external knowledge bases, enabling real-time
access to domain-specific expertise during inference. Despite its potential,
traditional RAG systems, based on flat text retrieval, face three critical
challenges: (i) complex query understanding in professional contexts, (ii)
difficulties in knowledge integration across distributed sources, and (iii)
system efficiency bottlenecks at scale. This survey presents a systematic
analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new
paradigm that revolutionizes domain-specific LLM applications. GraphRAG
addresses traditional RAG limitations through three key innovations: (i)
graph-structured knowledge representation that explicitly captures entity
relationships and domain hierarchies, (ii) efficient graph-based retrieval
techniques that enable context-preserving knowledge retrieval with multihop
reasoning ability, and (iii) structure-aware knowledge integration algorithms
that leverage retrieved knowledge for accurate and logical coherent generation
of LLMs. In this survey, we systematically analyze the technical foundations of
GraphRAG and examine current implementations across various professional
domains, identifying key technical challenges and promising research
directions. All the related resources of GraphRAG, including research papers,
open-source data, and projects, are collected for the community in
\textcolor{blue}{\url{https://github.com/DEEP-PolyU/Awesome-GraphRAG}}.",http://arxiv.org/abs/2501.13958v1,1/21/25,"Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Junnan Dong, Hao Chen, Yi Chang, Xiao Huang"
3,RAKG:Document-level Retrieval Augmented Knowledge Graph Construction,"With the rise of knowledge graph based retrieval-augmented generation (RAG)
techniques such as GraphRAG and Pike-RAG, the role of knowledge graphs in
enhancing the reasoning capabilities of large language models (LLMs) has become
increasingly prominent. However, traditional Knowledge Graph Construction (KGC)
methods face challenges like complex entity disambiguation, rigid schema
definition, and insufficient cross-document knowledge integration. This paper
focuses on the task of automatic document-level knowledge graph construction.
It proposes the Document-level Retrieval Augmented Knowledge Graph Construction
(RAKG) framework. RAKG extracts pre-entities from text chunks and utilizes
these pre-entities as queries for RAG, effectively addressing the issue of
long-context forgetting in LLMs and reducing the complexity of Coreference
Resolution. In contrast to conventional KGC methods, RAKG more effectively
captures global information and the interconnections among disparate nodes,
thereby enhancing the overall performance of the model. Additionally, we
transfer the RAG evaluation framework to the KGC field and filter and evaluate
the generated knowledge graphs, thereby avoiding incorrectly generated entities
and relationships caused by hallucinations in LLMs. We further developed the
MINE dataset by constructing standard knowledge graphs for each article and
experimentally validated the performance of RAKG. The results show that RAKG
achieves an accuracy of 95.91 % on the MINE dataset, a 6.2 % point improvement
over the current best baseline, GraphRAG (89.71 %). The code is available at
https://github.com/LMMApplication/RAKG.",http://arxiv.org/abs/2504.09823v1,4/14/25,"Hairong Zhang, Jiaheng Si, Guohang Yan, Boyuan Qi, Pinlong Cai, Song Mao, Ding Wang, Botian Shi"
4,GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation,"Retrieval-augmented generation (RAG) has proven effective in integrating
knowledge into large language models (LLMs). However, conventional RAGs
struggle to capture complex relationships between pieces of knowledge, limiting
their performance in intricate reasoning that requires integrating knowledge
from multiple sources. Recently, graph-enhanced retrieval augmented generation
(GraphRAG) builds graph structure to explicitly model these relationships,
enabling more effective and efficient retrievers. Nevertheless, its performance
is still hindered by the noise and incompleteness within the graph structure.
To address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for
retrieval augmented generation. GFM-RAG is powered by an innovative graph
neural network that reasons over graph structure to capture complex
query-knowledge relationships. The GFM with 8M parameters undergoes a two-stage
training process on large-scale datasets, comprising 60 knowledge graphs with
over 14M triples and 700k documents. This results in impressive performance and
generalizability for GFM-RAG, making it the first graph foundation model
applicable to unseen datasets for retrieval without any fine-tuning required.
Extensive experiments on three multi-hop QA datasets and seven domain-specific
RAG datasets demonstrate that GFM-RAG achieves state-of-the-art performance
while maintaining efficiency and alignment with neural scaling laws,
highlighting its potential for further improvement.",http://arxiv.org/abs/2502.01113v1,2/3/25,"Linhao Luo, Zicheng Zhao, Gholamreza Haffari, Dinh Phung, Chen Gong, Shirui Pan"
5,GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph Databases,"Large language models have shown remarkable language processing and reasoning
ability but are prone to hallucinate when asked about private data.
Retrieval-augmented generation (RAG) retrieves relevant data that fit into an
LLM's context window and prompts the LLM for an answer. GraphRAG extends this
approach to structured Knowledge Graphs (KGs) and questions regarding entities
multiple hops away. The majority of recent GraphRAG methods either overlook the
retrieval step or have ad hoc retrieval processes that are abstract or
inefficient. This prevents them from being adopted when the KGs are stored in
graph databases supporting graph query languages. In this work, we present
GraphRAFT, a retrieve-and-reason framework that finetunes LLMs to generate
provably correct Cypher queries to retrieve high-quality subgraph contexts and
produce accurate answers. Our method is the first such solution that can be
taken off-the-shelf and used on KGs stored in native graph DBs. Benchmarks
suggest that our method is sample-efficient and scales with the availability of
training data. Our method achieves significantly better results than all
state-of-the-art models across all four standard metrics on two challenging
Q\&As on large text-attributed KGs.",http://arxiv.org/abs/2504.05478v1,4/7/25,"Alfred Clemedtson, Borun Shi"
6,FG-RAG: Enhancing Query-Focused Summarization with Context-Aware Fine-Grained Graph RAG,"Retrieval-Augmented Generation (RAG) enables large language models to provide
more precise and pertinent responses by incorporating external knowledge. In
the Query-Focused Summarization (QFS) task, GraphRAG-based approaches have
notably enhanced the comprehensiveness and diversity of generated responses.
However, existing GraphRAG-based approaches predominantly focus on
coarse-grained information summarization without being aware of the specific
query, and the retrieved content lacks sufficient contextual information to
generate comprehensive responses. To address the deficiencies of current RAG
systems, we propose Context-Aware Fine-Grained Graph RAG (FG-RAG) to enhance
the performance of the QFS task. FG-RAG employs Context-Aware Entity Expansion
in graph retrieval to expand the coverage of retrieved entities in the graph,
thus providing enough contextual information for the retrieved content.
Furthermore, FG-RAG utilizes Query-Level Fine-Grained Summarization to
incorporate fine-grained details during response generation, enhancing query
awareness for the generated summarization. Our evaluation demonstrates that
FG-RAG outperforms other RAG systems in multiple metrics of comprehensiveness,
diversity, and empowerment when handling the QFS task. Our implementation is
available at https://github.com/BuptWululu/FG-RAG.",http://arxiv.org/abs/2504.07103v1,3/13/25,"Yubin Hong, Chaofan Li, Jingyi Zhang, Yingxia Shao"
7,Graph Retrieval-Augmented Generation: A Survey,"Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable
success in addressing the challenges of Large Language Models (LLMs) without
necessitating retraining. By referencing an external knowledge base, RAG
refines LLM outputs, effectively mitigating issues such as ``hallucination'',
lack of domain-specific knowledge, and outdated information. However, the
complex structure of relationships among different entities in databases
presents challenges for RAG systems. In response, GraphRAG leverages structural
information across entities to enable more precise and comprehensive retrieval,
capturing relational knowledge and facilitating more accurate, context-aware
responses. Given the novelty and potential of GraphRAG, a systematic review of
current technologies is imperative. This paper provides the first comprehensive
overview of GraphRAG methodologies. We formalize the GraphRAG workflow,
encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced
Generation. We then outline the core technologies and training methods at each
stage. Additionally, we examine downstream tasks, application domains,
evaluation methodologies, and industrial use cases of GraphRAG. Finally, we
explore future research directions to inspire further inquiries and advance
progress in the field. In order to track recent progress in this field, we set
up a repository at \url{https://github.com/pengboci/GraphRAG-Survey}.",http://arxiv.org/abs/2408.08921v2,8/15/24,"Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang"
8,Retrieval-Augmented Generation with Graphs (GraphRAG),"Retrieval-augmented generation (RAG) is a powerful technique that enhances
downstream task execution by retrieving additional information, such as
knowledge, skills, and tools from external sources. Graph, by its intrinsic
""nodes connected by edges"" nature, encodes massive heterogeneous and relational
information, making it a golden resource for RAG in tremendous real-world
applications. As a result, we have recently witnessed increasing attention on
equipping RAG with Graph, i.e., GraphRAG. However, unlike conventional RAG,
where the retriever, generator, and external data sources can be uniformly
designed in the neural-embedding space, the uniqueness of graph-structured
data, such as diverse-formatted and domain-specific relational knowledge, poses
unique and significant challenges when designing GraphRAG for different
domains. Given the broad applicability, the associated design challenges, and
the recent surge in GraphRAG, a systematic and up-to-date survey of its key
concepts and techniques is urgently desired. Following this motivation, we
present a comprehensive and up-to-date survey on GraphRAG. Our survey first
proposes a holistic GraphRAG framework by defining its key components,
including query processor, retriever, organizer, generator, and data source.
Furthermore, recognizing that graphs in different domains exhibit distinct
relational patterns and require dedicated designs, we review GraphRAG
techniques uniquely tailored to each domain. Finally, we discuss research
challenges and brainstorm directions to inspire cross-disciplinary
opportunities. Our survey repository is publicly maintained at
https://github.com/Graph-RAG/GraphRAG/.",http://arxiv.org/abs/2501.00309v2,12/31/24,"Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Mahantesh Halappanavar, Ryan A Rossi, Subhabrata Mukherjee, Xianfeng Tang, Qi He, Zhigang Hua, Bo Long, Tong Zhao, Neil Shah, Amin Javari, Yinglong Xia, Jiliang Tang"
9,LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation for Design Space Exploration,"GraphRAG integrates (knowledge) graphs with large language models (LLMs) to
improve reasoning accuracy and contextual relevance. Despite its promising
applications and strong relevance to multiple research communities, such as
databases and natural language processing, GraphRAG currently lacks modular
workflow analysis, systematic solution frameworks, and insightful empirical
studies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework
that enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)
systematic classification of existing techniques and implemented GraphRAG
instances, and 3) creation of new GraphRAG instances. Our framework facilitates
comprehensive empirical studies of GraphRAG on large-scale real-world graphs
and diverse query sets, revealing insights into balancing reasoning quality,
runtime efficiency, and token or GPU cost, that are essential for building
advanced GraphRAG systems.",http://arxiv.org/abs/2411.05844v2,11/6/24,"Yukun Cao, Zengyi Gao, Zhiyang Li, Xike Xie, Kevin Zhou, Jianliang Xu"
10,"Generating Knowledge Graphs from Large Language Models: A Comparative Study of GPT-4, LLaMA 2, and BERT","Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a
form of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks
requiring structured reasoning and semantic understanding. However, creating
KGs for GraphRAGs remains a significant challenge due to accuracy and
scalability limitations of traditional methods. This paper introduces a novel
approach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and
BERT to generate KGs directly from unstructured data, bypassing traditional
pipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit
Distance, and Semantic Similarity, we evaluate the models' ability to generate
high-quality KGs. Results demonstrate that GPT-4 achieves superior semantic
fidelity and structural accuracy, LLaMA 2 excels in lightweight,
domain-specific graphs, and BERT provides insights into challenges in
entity-relationship modeling. This study underscores the potential of LLMs to
streamline KG creation and enhance GraphRAG accessibility for real-world
applications, while setting a foundation for future advancements.",http://arxiv.org/abs/2412.07412v1,12/10/24,"Ahan Bhatt, Nandan Vaghela, Kush Dudhia"
11,"Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative Knowledge Retrieval","Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective
in enhancing the performance of Large Language Models (LLMs) on tasks that
require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG
improves information retrieval for complex reasoning tasks, providing more
precise and comprehensive retrieval and generating more accurate responses to
QAs. However, most RAG methods fall short in addressing multi-step reasoning,
particularly when both information extraction and inference are necessary. To
address this limitation, this paper presents Knowledge Graph-Based Iterative
Retrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs
with iterative reasoning to improve LLMs' ability to handle queries involving
temporal and logical dependencies. Through iterative retrieval steps, KG-IRAG
incrementally gathers relevant data from external KGs, enabling step-by-step
reasoning. The proposed approach is particularly suited for scenarios where
reasoning is required alongside dynamic temporal data extraction, such as
determining optimal travel times based on weather conditions or traffic
patterns. Experimental results show that KG-IRAG improves accuracy in complex
reasoning tasks by effectively integrating external knowledge with iterative,
logic-based retrieval. Additionally, three new datasets: weatherQA-Irish,
weatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's
performance, demonstrating its potential beyond traditional RAG applications.",http://arxiv.org/abs/2503.14234v3,3/18/25,"Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D Salim"
12,GraphRAG under Fire,"GraphRAG advances retrieval-augmented generation (RAG) by structuring
external knowledge as multi-scale knowledge graphs, enabling language models to
integrate both broad context and granular details in their generation. While
GraphRAG has demonstrated success across domains, its security implications
remain largely unexplored. To bridge this gap, this work examines GraphRAG's
vulnerability to poisoning attacks, uncovering an intriguing security paradox:
compared to conventional RAG, GraphRAG's graph-based indexing and retrieval
enhance resilience against simple poisoning attacks; yet, the same features
also create new attack surfaces. We present GRAGPoison, a novel attack that
exploits shared relations in the underlying knowledge graph to craft poisoning
text capable of compromising multiple queries simultaneously. GRAGPoison
employs three key strategies: i) relation injection to introduce false
knowledge, ii) relation enhancement to amplify poisoning influence, and iii)
narrative generation to embed malicious content within coherent text. Empirical
evaluation across diverse datasets and models shows that GRAGPoison
substantially outperforms existing attacks in terms of effectiveness (up to
98\% success rate) and scalability (using less than 68\% poisoning text) on
various GraphRAG-based systems. We also explore potential defensive measures
and their limitations, identifying promising directions for future research.",http://arxiv.org/abs/2501.14050v2,1/23/25,"Jiacheng Liang, Yuhui Wang, Changjiang Li, Rongyi Zhu, Tanqiu Jiang, Neil Gong, Ting Wang"
13,When Graph Meets Retrieval Augmented Generation for Wireless Networks: A Tutorial and Case Study,"The rapid development of next-generation networking technologies underscores
their transformative role in revolutionizing modern communication systems,
enabling faster, more reliable, and highly interconnected solutions. However,
such development has also brought challenges to network optimizations. Thanks
to the emergence of Large Language Models (LLMs) in recent years, tools
including Retrieval Augmented Generation (RAG) have been developed and applied
in various fields including networking, and have shown their effectiveness.
Taking one step further, the integration of knowledge graphs into RAG
frameworks further enhanced the performance of RAG in networking applications
such as Intent-Driven Networks (IDNs) and spectrum knowledge maps by providing
more contextually relevant responses through more accurate retrieval of related
network information. This paper introduces the RAG framework that integrates
knowledge graphs in its database and explores such framework's application in
networking. We begin by exploring RAG's applications in networking and the
limitations of conventional RAG and present the advantages that knowledge
graphs' structured knowledge representation brings to the retrieval and
generation processes. Next, we propose a detailed GraphRAG-based framework for
networking, including a step-by-step tutorial on its construction. Our
evaluation through a case study on channel gain prediction demonstrates
GraphRAG's enhanced capability in generating accurate, contextually rich
responses, surpassing traditional RAG models. Finally, we discuss key future
directions for applying knowledge-graphs-empowered RAG frameworks in
networking, including robust updates, mitigation of hallucination, and enhanced
security measures for networking applications.",http://arxiv.org/abs/2412.07189v1,12/10/24,"Yang Xiong, Ruichen Zhang, Yinqiu Liu, Dusit Niyato, Zehui Xiong, YingChang Liang, Shiwen Mao"
14,GTR: Graph-Table-RAG for Cross-Table Question Answering,"Beyond pure text, a substantial amount of knowledge is stored in tables. In
real-world scenarios, user questions often require retrieving answers that are
distributed across multiple tables. GraphRAG has recently attracted much
attention for enhancing LLMs' reasoning capabilities by organizing external
knowledge to address ad-hoc and complex questions, exemplifying a promising
direction for cross-table question answering. In this paper, to address the
current gap in available data, we first introduce a multi-table benchmark,
MutliTableQA, comprising 60k tables and 25k user queries collected from
real-world sources. Then, we propose the first Graph-Table-RAG framework,
namely GTR, which reorganizes table corpora into a heterogeneous graph, employs
a hierarchical coarse-to-fine retrieval process to extract the most relevant
tables, and integrates graph-aware prompting for downstream LLMs' tabular
reasoning. Extensive experiments show that GTR exhibits superior cross-table
question-answering performance while maintaining high deployment efficiency,
demonstrating its real-world practical applicability.",http://arxiv.org/abs/2504.01346v3,4/2/25,"Jiaru Zou, Dongqi Fu, Sirui Chen, Xinrui He, Zihao Li, Yada Zhu, Jiawei Han, Jingrui He"
15,HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured Knowledge Representation,"Standard Retrieval-Augmented Generation (RAG) relies on chunk-based
retrieval, whereas GraphRAG advances this approach by graph-based knowledge
representation. However, existing graph-based RAG approaches are constrained by
binary relations, as each edge in an ordinary graph connects only two entities,
limiting their ability to represent the n-ary relations (n >= 2) in real-world
knowledge. In this work, we propose HyperGraphRAG, a novel hypergraph-based RAG
method that represents n-ary relational facts via hyperedges, and consists of
knowledge hypergraph construction, retrieval, and generation. Experiments
across medicine, agriculture, computer science, and law demonstrate that
HyperGraphRAG outperforms both standard RAG and previous graph-based RAG
methods in answer accuracy, retrieval efficiency, and generation quality.",http://arxiv.org/abs/2503.21322v2,3/27/25,"Haoran Luo, Haihong E, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo, Qika Lin, Yu Feng, Zemin Kuang, Meina Song, Yifan Zhu, Luu Anh Tuan"
16,From Local to Global: A Graph RAG Approach to Query-Focused Summarization,"The use of retrieval-augmented generation (RAG) to retrieve relevant
information from an external knowledge source enables large language models
(LLMs) to answer questions over private and/or previously unseen document
collections. However, RAG fails on global questions directed at an entire text
corpus, such as ""What are the main themes in the dataset?"", since this is
inherently a query-focused summarization (QFS) task, rather than an explicit
retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of
text indexed by typical RAG systems. To combine the strengths of these
contrasting methods, we propose GraphRAG, a graph-based approach to question
answering over private text corpora that scales with both the generality of
user questions and the quantity of source text. Our approach uses an LLM to
build a graph index in two stages: first, to derive an entity knowledge graph
from the source documents, then to pregenerate community summaries for all
groups of closely related entities. Given a question, each community summary is
used to generate a partial response, before all partial responses are again
summarized in a final response to the user. For a class of global sensemaking
questions over datasets in the 1 million token range, we show that GraphRAG
leads to substantial improvements over a conventional RAG baseline for both the
comprehensiveness and diversity of generated answers.",http://arxiv.org/abs/2404.16130v2,4/24/24,"Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson"
17,FastRAG: Retrieval Augmented Generation for Semi-structured Data,"Efficiently processing and interpreting network data is critical for the
operation of increasingly complex networks. Recent advances in Large Language
Models (LLM) and Retrieval-Augmented Generation (RAG) techniques have improved
data processing in network management. However, existing RAG methods like
VectorRAG and GraphRAG struggle with the complexity and implicit nature of
semi-structured technical data, leading to inefficiencies in time, cost, and
retrieval. This paper introduces FastRAG, a novel RAG approach designed for
semi-structured data. FastRAG employs schema learning and script learning to
extract and structure data without needing to submit entire data sources to an
LLM. It integrates text search with knowledge graph (KG) querying to improve
accuracy in retrieving context-rich information. Evaluation results demonstrate
that FastRAG provides accurate question answering, while improving up to 90% in
time and 85% in cost compared to GraphRAG.",http://arxiv.org/abs/2411.13773v1,11/21/24,"Amar Abane, Anis Bekri, Abdella Battou"
18,NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes,"Retrieval-augmented generation (RAG) empowers large language models to access
external and private corpus, enabling factually consistent responses in
specific domains. By exploiting the inherent structure of the corpus,
graph-based RAG methods further enrich this process by building a knowledge
graph index and leveraging the structural nature of graphs. However, current
graph-based RAG approaches seldom prioritize the design of graph structures.
Inadequately designed graph not only impede the seamless integration of diverse
graph algorithms but also result in workflow inconsistencies and degraded
performance. To further unleash the potential of graph for RAG, we propose
NodeRAG, a graph-centric framework introducing heterogeneous graph structures
that enable the seamless and holistic integration of graph-based methodologies
into the RAG workflow. By aligning closely with the capabilities of LLMs, this
framework ensures a fully cohesive and efficient end-to-end process. Through
extensive experiments, we demonstrate that NodeRAG exhibits performance
advantages over previous methods, including GraphRAG and LightRAG, not only in
indexing time, query time, and storage efficiency but also in delivering
superior question-answering performance on multi-hop benchmarks and open-ended
head-to-head evaluations with minimal retrieval tokens. Our GitHub repository
could be seen at https://github.com/Terry-Xu-666/NodeRAG.",http://arxiv.org/abs/2504.11544v1,4/15/25,"Tianyang Xu, Haojie Zheng, Chengze Li, Haoxiang Chen, Yixin Liu, Ruoxi Chen, Lichao Sun"
19,PolyG: Effective and Efficient GraphRAG with Adaptive Graph Traversal,"GraphRAG enhances large language models (LLMs) to generate quality answers
for user questions by retrieving related facts from external knowledge graphs.
Existing GraphRAG methods adopt a fixed graph traversal strategy for fact
retrieval but we observe that user questions come in different types and
require different graph traversal strategies. As such, existing GraphRAG
methods are limited in effectiveness (i.e., quality of the generated answers)
and/or efficiency (i.e., response time or the number of used tokens). In this
paper, we propose to classify the questions according to a complete four-class
taxonomy and adaptively select the appropriate graph traversal strategy for
each type of questions. Our system PolyG is essentially a query planner for
GraphRAG and can handle diverse questions with an unified interface and
execution engine. Compared with SOTA GraphRAG methods, PolyG achieves an
overall win rate of 75% on generation quality and a speedup up to 4x on
response time.",http://arxiv.org/abs/2504.02112v1,4/2/25,"Renjie Liu, Haitian Jiang, Xiao Yan, Bo Tang, Jinyang Li"
20,Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study,"Extracting meaningful insights from large and complex datasets poses
significant challenges, particularly in ensuring the accuracy and relevance of
retrieved information. Traditional data retrieval methods such as sequential
search and index-based retrieval often fail when handling intricate and
interconnected data structures, resulting in incomplete or misleading outputs.
To overcome these limitations, we introduce Structured-GraphRAG, a versatile
framework designed to enhance information retrieval across structured datasets
in natural language queries. Structured-GraphRAG utilizes multiple knowledge
graphs, which represent data in a structured format and capture complex
relationships between entities, enabling a more nuanced and comprehensive
retrieval of information. This graph-based approach reduces the risk of errors
in language model outputs by grounding responses in a structured format,
thereby enhancing the reliability of results. We demonstrate the effectiveness
of Structured-GraphRAG by comparing its performance with that of a recently
published method using traditional retrieval-augmented generation. Our findings
show that Structured-GraphRAG significantly improves query processing
efficiency and reduces response times. While our case study focuses on soccer
data, the framework's design is broadly applicable, offering a powerful tool
for data analysis and enhancing language model applications across various
structured domains.",http://arxiv.org/abs/2409.17580v1,9/26/24,"Zahra Sepasdar, Sushant Gautam, Cise Midoglu, Michael A Riegler, Pl Halvorsen"
21,KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG,"Graph-RAG constructs a knowledge graph from text chunks to improve retrieval
in Large Language Model (LLM)-based question answering. It is particularly
useful in domains such as biomedicine, law, and political science, where
retrieval often requires multi-hop reasoning over proprietary documents. Some
existing Graph-RAG systems construct KNN graphs based on text chunk relevance,
but this coarse-grained approach fails to capture entity relationships within
texts, leading to sub-par retrieval and generation quality. To address this,
recent solutions leverage LLMs to extract entities and relationships from text
chunks, constructing triplet-based knowledge graphs. However, this approach
incurs significant indexing costs, especially for large document collections.
  To ensure a good result accuracy while reducing the indexing cost, we propose
KET-RAG, a multi-granular indexing framework. KET-RAG first identifies a small
set of key text chunks and leverages an LLM to construct a knowledge graph
skeleton. It then builds a text-keyword bipartite graph from all text chunks,
serving as a lightweight alternative to a full knowledge graph. During
retrieval, KET-RAG searches both structures: it follows the local search
strategy of existing Graph-RAG systems on the skeleton while mimicking this
search on the bipartite graph to improve retrieval quality. We evaluate eight
solutions on two real-world datasets, demonstrating that KET-RAG outperforms
all competitors in indexing cost, retrieval effectiveness, and generation
quality. Notably, it achieves comparable or superior retrieval quality to
Microsoft's Graph-RAG while reducing indexing costs by over an order of
magnitude. Additionally, it improves the generation quality by up to 32.4%
while lowering indexing costs by around 20%.",http://arxiv.org/abs/2502.09304v1,2/13/25,"Yiqian Huang, Shiqi Zhang, Xiaokui Xiao"
22,Knowledge Retrieval in LLM Gaming: A Shift from Entity-Centric to Goal-Oriented Graphs,"Large Language Models (LLMs) demonstrate impressive general capabilities but
often struggle with step-by-step reasoning, especially in complex applications
such as games. While retrieval-augmented methods like GraphRAG attempt to
bridge this gap through cross-document extraction and indexing, their
fragmented entity-relation graphs and overly dense local connectivity hinder
the construction of coherent reasoning. In this paper, we propose a novel
framework based on Goal-Oriented Graphs (GoGs), where each node represents a
goal and its associated attributes, and edges encode logical dependencies
between goals. This structure enables explicit retrieval of reasoning paths by
first identifying high-level goals and recursively retrieving their subgoals,
forming coherent reasoning chains to guide LLM prompting. Our method
significantly enhances the reasoning ability of LLMs in game-playing tasks, as
demonstrated by extensive experiments on the Minecraft testbed, outperforming
GraphRAG and other baselines.",http://arxiv.org/abs/2505.18607v1,5/24/25,"Jonathan Leung, Yongjie Wang, Zhiqi Shen"
23,HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction,"Extraction and interpretation of intricate information from unstructured text
data arising in financial applications, such as earnings call transcripts,
present substantial challenges to large language models (LLMs) even using the
current best practices to use Retrieval Augmented Generation (RAG) (referred to
as VectorRAG techniques which utilize vector databases for information
retrieval) due to challenges such as domain specific terminology and complex
formats of the documents. We introduce a novel approach based on a combination,
called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called
GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for
information extraction from financial documents that is shown to be capable of
generating accurate and contextually relevant answers. Using experiments on a
set of financial earning call transcripts documents which come in the form of
Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we
show that HybridRAG which retrieves context from both vector database and KG
outperforms both traditional VectorRAG and GraphRAG individually when evaluated
at both the retrieval and generation stages in terms of retrieval accuracy and
answer generation. The proposed technique has applications beyond the financial
domain",http://arxiv.org/abs/2408.04948v1,8/9/24,"Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, Dhagash Mehta"
24,Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain,"Airports from the top 20 in terms of annual passengers are highly dynamic
environments with thousands of flights daily, and they aim to increase the
degree of automation. To contribute to this, we implemented a Conversational AI
system that enables staff in an airport to communicate with flight information
systems. This system not only answers standard airport queries but also
resolves airport terminology, jargon, abbreviations, and dynamic questions
involving reasoning. In this paper, we built three different
Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL
RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that
traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally
produced hallucinations, which is risky to airport safety. In contrast, SQL RAG
and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with
significantly fewer hallucinations. Moreover, Graph RAG was especially
effective for questions that involved reasoning. Based on our observations, we
thus recommend SQL RAG and Graph RAG are better for airport environments, due
to fewer hallucinations and the ability to handle dynamic questions.",http://arxiv.org/abs/2505.13006v1,5/19/25,"Yuyang Li, Philip J M Kerbusch, Raimon H R Pruim, Tobias Kfer"
25,HuixiangDou2: A Robustly Optimized GraphRAG Approach,"Large Language Models (LLMs) perform well on familiar queries but struggle
with specialized or emerging topics. Graph-based Retrieval-Augmented Generation
(GraphRAG) addresses this by structuring domain knowledge as a graph for
dynamic retrieval. However, existing pipelines involve complex engineering
workflows, making it difficult to isolate the impact of individual components.
Evaluating retrieval effectiveness is also challenging due to dataset overlap
with LLM pretraining data. In this work, we introduce HuixiangDou2, a robustly
optimized GraphRAG framework. Specifically, we leverage the effectiveness of
dual-level retrieval and optimize its performance in a 32k context for maximum
precision, and compare logic-based retrieval and dual-level retrieval to
enhance overall functionality. Our implementation includes comparative
experiments on a test set, where Qwen2.5-7B-Instruct initially underperformed.
With our approach, the score improved significantly from 60 to 74.5, as
illustrated in the Figure. Experiments on domain-specific datasets reveal that
dual-level retrieval enhances fuzzy matching, while logic-form retrieval
improves structured reasoning. Furthermore, we propose a multi-stage
verification mechanism to improve retrieval robustness without increasing
computational cost. Empirical results show significant accuracy gains over
baselines, highlighting the importance of adaptive retrieval. To support
research and adoption, we release HuixiangDou2 as an open-source resource
https://github.com/tpoisonooo/huixiangdou2.",http://arxiv.org/abs/2503.06474v1,3/9/25,"Huanjun Kong, Zhefan Wang, Chenyang Wang, Zhe Ma, Nanqing Dong"
26,Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation,"We introduce a novel graph-based Retrieval-Augmented Generation (RAG)
framework specifically designed for the medical domain, called
\textbf{MedGraphRAG}, aimed at enhancing Large Language Model (LLM)
capabilities for generating evidence-based medical responses, thereby improving
safety and reliability when handling private medical data. Graph-based RAG
(GraphRAG) leverages LLMs to organize RAG data into graphs, showing strong
potential for gaining holistic insights from long-form documents. However, its
standard implementation is overly complex for general use and lacks the ability
to generate evidence-based responses, limiting its effectiveness in the medical
field. To extend the capabilities of GraphRAG to the medical domain, we propose
unique Triple Graph Construction and U-Retrieval techniques over it. In our
graph construction, we create a triple-linked structure that connects user
documents to credible medical sources and controlled vocabularies. In the
retrieval process, we propose U-Retrieval which combines Top-down Precise
Retrieval with Bottom-up Response Refinement to balance global context
awareness with precise indexing. These effort enable both source information
retrieval and comprehensive response generation. Our approach is validated on 9
medical Q\&A benchmarks, 2 health fact-checking benchmarks, and one collected
dataset testing long-form generation. The results show that MedGraphRAG
consistently outperforms state-of-the-art models across all benchmarks, while
also ensuring that responses include credible source documentation and
definitions. Our code is released at:
https://github.com/MedicineToken/Medical-Graph-RAG.",http://arxiv.org/abs/2408.04187v2,8/8/24,"Junde Wu, Jiayuan Zhu, Yunli Qi, Jingkun Chen, Min Xu, Filippo Menolascina, Vicente Grau"
27,Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge Reasoning and Text Generation,"This study aims to optimize the existing retrieval-augmented generation model
(RAG) by introducing a graph structure to improve the performance of the model
in dealing with complex knowledge reasoning tasks. The traditional RAG model
has the problem of insufficient processing efficiency when facing complex graph
structure information (such as knowledge graphs, hierarchical relationships,
etc.), which affects the quality and consistency of the generated results. This
study proposes a scheme to process graph structure data by combining graph
neural network (GNN), so that the model can capture the complex relationship
between entities, thereby improving the knowledge consistency and reasoning
ability of the generated text. The experiment used the Natural Questions (NQ)
dataset and compared it with multiple existing generation models. The results
show that the graph-based RAG model proposed in this paper is superior to the
traditional generation model in terms of quality, knowledge consistency, and
reasoning ability, especially when dealing with tasks that require
multi-dimensional reasoning. Through the combination of the enhancement of the
retrieval module and the graph neural network, the model in this study can
better handle complex knowledge background information and has broad potential
value in multiple practical application scenarios.",http://arxiv.org/abs/2411.03572v1,11/6/24,"Yuxin Dong, Shuo Wang, Hongye Zheng, Jiajing Chen, Zhenhong Zhang, Chihang Wang"
28,Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented Generation via Knowledge Graph Walks,"Large Language Models (LLMs) have showcased impressive reasoning abilities,
but often suffer from hallucinations or outdated knowledge. Knowledge Graph
(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by
grounding LLM responses in structured external information from a knowledge
base. However, many KG-based RAG approaches struggle with (i) aligning KG and
textual representations, (ii) balancing retrieval accuracy and efficiency, and
(iii) adapting to dynamically updated KGs. In this work, we introduce
Walk&Retrieve, a simple yet effective KG-based framework that leverages
walk-based graph traversal and knowledge verbalization for corpus generation
for zero-shot RAG. Built around efficient KG walks, our method does not require
fine-tuning on domain-specific data, enabling seamless adaptation to KG
updates, reducing computational overhead, and allowing integration with any
off-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs
competitively, often outperforming existing RAG systems in response accuracy
and hallucination reduction. Moreover, it demonstrates lower query latency and
robust scalability to large KGs, highlighting the potential of lightweight
retrieval strategies as strong baselines for future RAG research.",http://arxiv.org/abs/2505.16849v1,5/22/25,"Martin Bckling, Heiko Paulheim, Andreea Iana"
29,"Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning","Retrieval-Augmented Generation (RAG) systems empower large language models
(LLMs) with external knowledge, yet struggle with efficiency-accuracy
trade-offs when scaling to large knowledge graphs. Existing approaches often
rely on monolithic graph retrieval, incurring unnecessary latency for simple
queries and fragmented reasoning for complex multi-hop questions. To address
these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework
that addresses these limitations with question-driven semantic graph
partitioning and collaborative subgraph retrieval. The innovative framework
first create Semantic Partitioning of Linked Information, then use the
Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware
graph segmentation manages to divide knowledge graphs into semantically
coherent subgraphs, ensuring subgraphs align with different query types, while
lightweight LLM agents are assigned to partitioned subgraphs, and only relevant
partitions are activated during retrieval, thus reduce search space while
enhancing efficiency. Finally, a hierarchical merging module resolves
inconsistencies across subgraph-derived answers through logical verifications.
Extensive experimental validation demonstrates considerable improvements
compared to existing approaches.",http://arxiv.org/abs/2505.13994v1,5/20/25,"Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D Salim"
30,GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning,"Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form
of triplets (head, relation, tail), which collectively form a graph. Question
Answering over KGs (KGQA) is the task of answering natural questions grounding
the reasoning to the information provided by the KG. Large Language Models
(LLMs) are the state-of-the-art models for QA tasks due to their remarkable
ability to understand natural language. On the other hand, Graph Neural
Networks (GNNs) have been widely used for KGQA as they can handle the complex
graph information stored in the KG. In this work, we introduce GNN-RAG, a novel
method for combining language understanding abilities of LLMs with the
reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.
First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for
a given question. Second, the shortest paths in the KG that connect question
entities and answer candidates are extracted to represent KG reasoning paths.
The extracted paths are verbalized and given as input for LLM reasoning with
RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to
extract useful graph information, while the LLM leverages its natural language
processing ability for ultimate KGQA. Furthermore, we develop a retrieval
augmentation (RA) technique to further boost KGQA performance with GNN-RAG.
Experimental results show that GNN-RAG achieves state-of-the-art performance in
two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching
GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop
and multi-entity questions outperforming competing approaches by 8.9--15.5%
points at answer F1.",http://arxiv.org/abs/2405.20139v1,5/30/24,"Costas Mavromatis, George Karypis"
31,How to Mitigate Information Loss in Knowledge Graphs for GraphRAG: Leveraging Triple Context Restoration and Query-Driven Feedback,"Knowledge Graph (KG)-augmented Large Language Models (LLMs) have recently
propelled significant advances in complex reasoning tasks, thanks to their
broad domain knowledge and contextual awareness. Unfortunately, current methods
often assume KGs to be complete, which is impractical given the inherent
limitations of KG construction and the potential loss of contextual cues when
converting unstructured text into entity-relation triples. In response, this
paper proposes the Triple Context Restoration and Query-driven Feedback
(TCR-QF) framework, which reconstructs the textual context underlying each
triple to mitigate information loss, while dynamically refining the KG
structure by iteratively incorporating query-relevant missing knowledge.
Experiments on five benchmark question-answering datasets substantiate the
effectiveness of TCR-QF in KG and LLM integration, where itachieves a 29.1%
improvement in Exact Match and a 15.5% improvement in F1 over its
state-of-the-art GraphRAG competitors.",http://arxiv.org/abs/2501.15378v1,1/26/25,"Manzong Huang, Chenyang Bu, Yi He, Xindong Wu"
32,Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation,"Large language models (LLMs) have demonstrated remarkable capabilities, but
still struggle with issues like hallucinations and outdated information.
Retrieval-augmented generation (RAG) addresses these issues by grounding LLM
outputs in external knowledge with an Information Retrieval (IR) system.
Building on this foundation, graph-based RAG systems go a step further by
retrieving subgraphs, which preserve the relationships between knowledge
entities and provide more comprehensive context. However, graph RAG faces two
challenges: (1) Retrieving relevant information introduces irrelevant nodes
(especially in dense graph databases, where retrieval usually extends to
adjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)
The representation gap between graph and language during generation with LLMs
limits the ability to fully leverage graph structures for enhanced
understanding. To address these limitations, we propose Align-GRAG, a novel
reasoning-guided dual alignment framework in post-retrieval phrase. It first
formulates a subgraph by retrieving nodes and edges. Then an Aligner is
proposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It
achieves dual alignment of graph node and representation by leveraging KL
divergence loss and contrastive loss, facilitating efficient pruning of
irrelevant knowledge and establishing a unified semantic space. The Generator
integrates the aligned graph data with LLM to produce coherent and accurate
answers. Experiments on GraphQA benchmark across three tasks (including common
sense reasoning, scene graph understanding, and knowledge graph reasoning)
validate the effectiveness of our method. The code will be available upon
accepted.",http://arxiv.org/abs/2505.16237v1,5/22/25,"Derong Xu, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Maolin Wang, Qidong Liu, Xiangyu Zhao, Yichao Wang, Huifeng Guo, Ruiming Tang, Enhong Chen, Tong Xu"
33,KAQG: A Knowledge-Graph-Enhanced RAG for Difficulty-Controlled Question Generation,"KAQG introduces a decisive breakthrough for Retrieval-Augmented Generation
(RAG) by explicitly tackling the two chronic weaknesses of current pipelines:
transparent multi-step reasoning and fine-grained cognitive difficulty control.
This transforms RAG from a passive retriever into an accountable generator of
calibrated exam items. Technically, the framework fuses knowledge graphs, RAG
retrieval, and educational assessment theory into a single pipeline. Domain
passages are parsed into a structured graph; graph-aware retrieval feeds fact
chains to an LLM; and an assessment layer governed by Bloom's Taxonomy levels
and Item Response Theory (IRT) transforms those chains into psychometrically
sound questions. This cross-disciplinary marriage yields two scholarly
contributions: it shows how semantic graph contexts guide LLM reasoning paths,
and it operationalizes difficulty metrics within the generation process,
producing items whose IRT parameters match expert benchmarks. Every module,
from KG construction scripts to the multi-agent reasoning scheduler and the
automatic IRT validator, is openly released on GitHub. This enables peer
laboratories to replicate experiments, benchmark against baselines, and extend
individual components without licensing barriers. Its reproducible design paves
the way for rigorous ablation studies, cross-domain transfer experiments, and
shared leaderboards on multi-step reasoning benchmarks.",http://arxiv.org/abs/2505.07618v1,5/12/25,"Ching Han Chen, Ming Fang Shiu"
34,CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has significantly enhanced large
language models (LLMs) in knowledge-intensive tasks by incorporating external
knowledge retrieval. However, existing RAG frameworks primarily rely on
semantic similarity and correlation-driven retrieval, limiting their ability to
distinguish true causal relationships from spurious associations. This results
in responses that may be factually grounded but fail to establish
cause-and-effect mechanisms, leading to incomplete or misleading insights. To
address this issue, we introduce Causal Dynamic Feedback for Adaptive
Retrieval-Augmented Generation (CDF-RAG), a framework designed to improve
causal consistency, factual accuracy, and explainability in generative
reasoning. CDF-RAG iteratively refines queries, retrieves structured causal
graphs, and enables multi-hop causal reasoning across interconnected knowledge
sources. Additionally, it validates responses against causal pathways, ensuring
logically coherent and factually grounded outputs. We evaluate CDF-RAG on four
diverse datasets, demonstrating its ability to improve response accuracy and
causal correctness over existing RAG-based methods. Our code is publicly
available at https://github.com/ elakhatibi/CDF-RAG.",http://arxiv.org/abs/2504.12560v1,4/17/25,"Elahe Khatibi, Ziyu Wang, Amir M Rahmani"
35,DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation,"Domain-specific QA systems require not just generative fluency but high
factual accuracy grounded in structured expert knowledge. While recent
Retrieval-Augmented Generation (RAG) frameworks improve context recall, they
struggle with integrating heterogeneous data and maintaining reasoning
consistency. To address these challenges, we propose DO-RAG, a scalable and
customizable hybrid QA framework that integrates multi-level knowledge graph
construction with semantic vector retrieval. Our system employs a novel agentic
chain-of-thought architecture to extract structured relationships from
unstructured, multimodal documents, constructing dynamic knowledge graphs that
enhance retrieval precision. At query time, DO-RAG fuses graph and vector
retrieval results to generate context-aware responses, followed by
hallucination mitigation via grounded refinement. Experimental evaluations in
the database and electrical domains show near-perfect recall and over 94%
answer relevancy, with DO-RAG outperforming baseline frameworks by up to
33.38%. By combining traceability, adaptability, and performance efficiency,
DO-RAG offers a reliable foundation for multi-domain, high-precision QA at
scale.",http://arxiv.org/abs/2505.17058v1,5/17/25,"David Osei Opoku, Ming Sheng, Yong Zhang"
36,Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in Graph-Augmented LLMs,"In knowledge-intensive tasks, especially in high-stakes domains like medicine
and law, it is critical not only to retrieve relevant information but also to
provide causal reasoning and explainability. Large language models (LLMs) have
achieved remarkable performance in natural language understanding and
generation tasks. However, they often suffer from limitations such as
difficulty in incorporating new knowledge, generating hallucinations, and
explaining their reasoning process. To address these challenges, integrating
knowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has
emerged as an effective solution. Traditional Graph RAG methods often rely on
simple graph traversal or semantic similarity, which do not capture causal
relationships or align well with the model's internal reasoning steps. This
paper proposes a novel pipeline that filters large knowledge graphs to
emphasize cause-effect edges, aligns the retrieval process with the model's
chain-of-thought (CoT), and enhances reasoning through multi-stage path
improvements. Experiments on medical question-answering tasks show consistent
gains, with up to a 10\% absolute improvement across multiple large language
models (LLMs). This approach demonstrates the value of combining causal
reasoning with stepwise retrieval, leading to more interpretable and logically
grounded solutions for complex queries.",http://arxiv.org/abs/2501.14892v2,1/24/25,"Hang Luo, Jian Zhang, Chujun Li"
37,RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration,"This paper presents RAG-KG-IL, a novel multi-agent hybrid framework designed
to enhance the reasoning capabilities of Large Language Models (LLMs) by
integrating Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs)
with an Incremental Learning (IL) approach. Despite recent advancements, LLMs
still face significant challenges in reasoning with structured data, handling
dynamic knowledge evolution, and mitigating hallucinations, particularly in
mission-critical domains. Our proposed RAG-KG-IL framework addresses these
limitations by employing a multi-agent architecture that enables continuous
knowledge updates, integrates structured knowledge, and incorporates autonomous
agents for enhanced explainability and reasoning. The framework utilizes RAG to
ensure the generated responses are grounded in verifiable information, while
KGs provide structured domain knowledge for improved consistency and depth of
understanding. The Incremental Learning approach allows for dynamic updates to
the knowledge base without full retraining, significantly reducing
computational overhead and improving the model's adaptability. We evaluate the
framework using real-world case studies involving health-related queries,
comparing it to state-of-the-art models like GPT-4o and a RAG-only baseline.
Experimental results demonstrate that our approach significantly reduces
hallucination rates and improves answer completeness and reasoning accuracy.
The results underscore the potential of combining RAG, KGs, and multi-agent
systems to create intelligent, adaptable systems capable of real-time knowledge
integration and reasoning in complex domains.",http://arxiv.org/abs/2503.13514v1,3/14/25,"Hong Qing Yu, Frank McQuade"
38,Synergizing RAG and Reasoning: A Systematic Review,"Recent breakthroughs in large language models (LLMs), particularly in
reasoning capabilities, have propelled Retrieval-Augmented Generation (RAG) to
unprecedented levels. By synergizing retrieval mechanisms with advanced
reasoning, LLMs can now tackle increasingly complex problems. This paper
presents a systematic review of the collaborative interplay between RAG and
reasoning, clearly defining ""reasoning"" within the RAG context. It construct a
comprehensive taxonomy encompassing multi-dimensional collaborative objectives,
representative paradigms, and technical implementations, and analyze the
bidirectional synergy methods. Additionally, we critically evaluate current
limitations in RAG assessment, including the absence of intermediate
supervision for multi-step reasoning and practical challenges related to
cost-risk trade-offs. To bridge theory and practice, we provide practical
guidelines tailored to diverse real-world applications. Finally, we identify
promising research directions, such as graph-based knowledge integration,
hybrid model collaboration, and RL-driven optimization. Overall, this work
presents a theoretical framework and practical foundation to advance RAG
systems in academia and industry, fostering the next generation of RAG
solutions.",http://arxiv.org/abs/2504.15909v2,4/22/25,"Yunfan Gao, Yun Xiong, Yijie Zhong, Yuxi Bi, Ming Xue, Haofen Wang"
39,Automating construction contract review using knowledge graph-enhanced large language models,"An effective and efficient review of construction contracts is essential for
minimizing construction projects losses, but current methods are time-consuming
and error-prone. Studies using methods based on Natural Language Processing
(NLP) exist, but their scope is often limited to text classification or
segmented label prediction. This paper investigates whether integrating Large
Language Models (LLMs) and Knowledge Graphs (KGs) can enhance the accuracy and
interpretability of automated contract risk identification. A tuning-free
approach is proposed that integrates LLMs with a Nested Contract Knowledge
Graph (NCKG) using a Graph Retrieval-Augmented Generation (GraphRAG) framework
for contract knowledge retrieval and reasoning. Tested on international EPC
contracts, the method achieves more accurate risk evaluation and interpretable
risk summaries than baseline models. These findings demonstrate the potential
of combining LLMs and KGs for reliable reasoning in tasks that are
knowledge-intensive and specialized, such as contract review.",http://arxiv.org/abs/2309.12132v2,9/21/23,"Chunmo Zheng, Saika Wong, Xing Su, Yinqiu Tang, Ahsan Nawaz, Mohamad Kassem"
40,Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering,"Recently, large language models (LLMs) have demonstrated impressive
performance in Knowledge Graph Question Answering (KGQA) tasks, which aim to
find answers based on knowledge graphs (KGs) for natural language questions.
Existing LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented
Generation (GraphRAG) paradigm, which first retrieves reasoning paths from the
large KGs, and then generates the answers based on them. However, these methods
emphasize the exploration of new optimal reasoning paths in KGs while ignoring
the exploitation of historical reasoning paths, which may lead to sub-optimal
reasoning paths. Additionally, the complex semantics contained in questions may
lead to the retrieval of inaccurate reasoning paths. To address these issues,
this paper proposes a novel and training-free framework for KGQA tasks called
Reward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original
question into a series of simpler and well-defined sub-questions to handle the
complex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided
by a reward model is introduced to iteratively retrieve weighted reasoning
paths as contextual knowledge. Finally, it stacks the weighted reasoning paths
according to their weights to generate the final answers. Extensive experiments
on four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves
8.7\% and 7.0\% performance improvement over the state-of-the-art method on the
GrailQA and the WebQSP respectively.",http://arxiv.org/abs/2505.12476v1,5/18/25,"Xiao Long, Liansheng Zhuang, Chen Shen, Shaotian Yan, Yifei Li, Shafei Wang"
41,GraPPI: A Retrieve-Divide-Solve GraphRAG Framework for Large-scale Protein-protein Interaction Exploration,"Drug discovery (DD) has tremendously contributed to maintaining and improving
public health. Hypothesizing that inhibiting protein misfolding can slow
disease progression, researchers focus on target identification (Target ID) to
find protein structures for drug binding. While Large Language Models (LLMs)
and Retrieval-Augmented Generation (RAG) frameworks have accelerated drug
discovery, integrating models into cohesive workflows remains challenging. We
conducted a user study with drug discovery researchers to identify the
applicability of LLMs and RAGs in Target ID. We identified two main findings:
1) an LLM should provide multiple Protein-Protein Interactions (PPIs) based on
an initial protein and protein candidates that have a therapeutic impact; 2)
the model must provide the PPI and relevant explanations for better
understanding. Based on these observations, we identified three limitations in
previous approaches for Target ID: 1) semantic ambiguity, 2) lack of
explainability, and 3) short retrieval units. To address these issues, we
propose GraPPI, a large-scale knowledge graph (KG)-based retrieve-divide-solve
agent pipeline RAG framework to support large-scale PPI signaling pathway
exploration in understanding therapeutic impacts by decomposing the analysis of
entire PPI pathways into sub-tasks focused on the analysis of PPI edges.",http://arxiv.org/abs/2501.16382v1,1/24/25,"Ziwen Li, Xiang Anthony Chen, Youngseung Jeon"
42,GRAG: Graph Retrieval-Augmented Generation,"Naive Retrieval-Augmented Generation (RAG) focuses on individual documents
during retrieval and, as a result, falls short in handling networked documents
which are very popular in many applications such as citation graphs, social
media, and knowledge graphs. To overcome this limitation, we introduce Graph
Retrieval-Augmented Generation (GRAG), which tackles the fundamental challenges
in retrieving textual subgraphs and integrating the joint textual and
topological information into Large Language Models (LLMs) to enhance its
generation. To enable efficient textual subgraph retrieval, we propose a novel
divide-and-conquer strategy that retrieves the optimal subgraph structure in
linear time. To achieve graph context-aware generation, incorporate textual
graphs into LLMs through two complementary views-the text view and the graph
view-enabling LLMs to more effectively comprehend and utilize the graph
context. Extensive experiments on graph reasoning benchmarks demonstrate that
in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach
significantly outperforms current state-of-the-art RAG methods.",http://arxiv.org/abs/2405.16506v2,5/26/24,"Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, Liang Zhao"
43,TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking,"In the age of social media, the rapid spread of misinformation and rumors has
led to the emergence of infodemics, where false information poses a significant
threat to society. To combat this issue, we introduce TrumorGPT , a novel
generative artificial intelligence solution designed for fact-checking in the
health domain. TrumorGPT aims to distinguish ""trumors"", which are
health-related rumors that turn out to be true, providing a crucial tool in
differentiating between mere speculation and verified facts. This framework
leverages a large language model (LLM) with few-shot learning for semantic
health knowledge graph construction and semantic reasoning. TrumorGPT
incorporates graph-based retrieval-augmented generation (GraphRAG) to address
the hallucination issue common in LLMs and the limitations of static training
data. GraphRAG involves accessing and utilizing information from regularly
updated semantic health knowledge graphs that consist of the latest medical
news and health information, ensuring that fact-checking by TrumorGPT is based
on the most recent data. Evaluating with extensive healthcare datasets,
TrumorGPT demonstrates superior performance in fact-checking for public health
claims. Its ability to effectively conduct fact-checking across various
platforms marks a critical step forward in the fight against health-related
misinformation, enhancing trust and accuracy in the digital information age.",http://arxiv.org/abs/2505.07891v1,5/11/25,"Ching Nam Hang, PeiDuo Yu, Chee Wei Tan"
44,Graph-based Approaches and Functionalities in Retrieval-Augmented Generation: A Comprehensive Survey,"Large language models (LLMs) struggle with the factual error during inference
due to the lack of sufficient training data and the most updated knowledge,
leading to the hallucination problem. Retrieval-Augmented Generation (RAG) has
gained attention as a promising solution to address the limitation of LLMs, by
retrieving relevant information from external source to generate more accurate
answers to the questions. Given the pervasive presence of structured knowledge
in the external source, considerable strides in RAG have been made to employ
the techniques related to graphs and achieve more complex reasoning based on
the topological information between knowledge entities. However, there is
currently neither unified review examining the diverse roles of graphs in RAG,
nor a comprehensive resource to help researchers navigate and contribute to
this evolving field. This survey offers a novel perspective on the
functionality of graphs within RAG and their impact on enhancing performance
across a wide range of graph-structured data. It provides a detailed breakdown
of the roles that graphs play in RAG, covering database construction,
algorithms, pipelines, and tasks. Finally, it identifies current challenges and
outline future research directions, aiming to inspire further developments in
this field. Our graph-centered analysis highlights the commonalities and
differences in existing methods, setting the stage for future researchers in
areas such as graph learning, database systems, and natural language
processing.",http://arxiv.org/abs/2504.10499v1,4/8/25,"Zulun Zhu, Tiancheng Huang, Kai Wang, Junda Ye, Xinghe Chen, Siqiang Luo"
45,TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) offers an effective approach for
addressing question answering (QA) tasks. However, the imperfections of the
retrievers in RAG models often result in the retrieval of irrelevant
information, which could introduce noises and degrade the performance,
especially when handling multi-hop questions that require multiple steps of
reasoning. To enhance the multi-hop reasoning ability of RAG models, we propose
TRACE. TRACE constructs knowledge-grounded reasoning chains, which are a series
of logically connected knowledge triples, to identify and integrate supporting
evidence from the retrieved documents for answering questions. Specifically,
TRACE employs a KG Generator to create a knowledge graph (KG) from the
retrieved documents, and then uses an Autoregressive Reasoning Chain
Constructor to build reasoning chains. Experimental results on three multi-hop
QA datasets show that TRACE achieves an average performance improvement of up
to 14.03% compared to using all the retrieved documents. Moreover, the results
indicate that using reasoning chains as context, rather than the entire
documents, is often sufficient to correctly answer questions.",http://arxiv.org/abs/2406.11460v1,6/17/24,"Jinyuan Fang, Zaiqiao Meng, Craig Macdonald"
46,HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation,"While Retrieval-Augmented Generation (RAG) augments Large Language Models
(LLMs) with external knowledge, conventional single-agent RAG remains
fundamentally limited in resolving complex queries demanding coordinated
reasoning across heterogeneous data ecosystems. We present HM-RAG, a novel
Hierarchical Multi-agent Multimodal RAG framework that pioneers collaborative
intelligence for dynamic knowledge synthesis across structured, unstructured,
and graph-based data. The framework is composed of three-tiered architecture
with specialized agents: a Decomposition Agent that dissects complex queries
into contextually coherent sub-tasks via semantic-aware query rewriting and
schema-guided context augmentation; Multi-source Retrieval Agents that carry
out parallel, modality-specific retrieval using plug-and-play modules designed
for vector, graph, and web-based databases; and a Decision Agent that uses
consistency voting to integrate multi-source answers and resolve discrepancies
in retrieval results through Expert Model Refinement. This architecture attains
comprehensive query understanding by combining textual, graph-relational, and
web-derived evidence, resulting in a remarkable 12.95% improvement in answer
accuracy and a 3.56% boost in question classification accuracy over baseline
RAG systems on the ScienceQA and CrisisMMD benchmarks. Notably, HM-RAG
establishes state-of-the-art results in zero-shot settings on both datasets.
Its modular architecture ensures seamless integration of new data modalities
while maintaining strict data governance, marking a significant advancement in
addressing the critical challenges of multimodal reasoning and knowledge
synthesis in RAG systems. Code is available at
https://github.com/ocean-luna/HMRAG.",http://arxiv.org/abs/2504.12330v1,4/13/25,"Pei Liu, Xin Liu, Ruoyu Yao, Junming Liu, Siyuan Meng, Ding Wang, Jun Ma"
47,Simple Is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation,"Large Language Models (LLMs) demonstrate strong reasoning abilities but face
limitations such as hallucinations and outdated knowledge. Knowledge Graph
(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by
grounding LLM outputs in structured external knowledge from KGs. However,
current KG-based RAG frameworks still struggle to optimize the trade-off
between retrieval effectiveness and efficiency in identifying a suitable amount
of relevant graph information for the LLM to digest. We introduce SubgraphRAG,
extending the KG-based RAG framework that retrieves subgraphs and leverages
LLMs for reasoning and answer prediction. Our approach innovatively integrates
a lightweight multilayer perceptron with a parallel triple-scoring mechanism
for efficient and flexible subgraph retrieval while encoding directional
structural distances to enhance retrieval effectiveness. The size of retrieved
subgraphs can be flexibly adjusted to match the query's need and the downstream
LLM's capabilities. This design strikes a balance between model complexity and
reasoning power, enabling scalable and generalizable retrieval processes.
Notably, based on our retrieved subgraphs, smaller LLMs like
Llama3.1-8B-Instruct deliver competitive results with explainable reasoning,
while larger models like GPT-4o achieve state-of-the-art accuracy compared with
previous baselines -- all without fine-tuning. Extensive evaluations on the
WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,
accuracy, and reliability by reducing hallucinations and improving response
grounding.",http://arxiv.org/abs/2410.20724v4,10/28/24,"Mufei Li, Siqi Miao, Pan Li"
48,CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation,"Large language models (LLMs) have revolutionized natural language processing
(NLP), particularly through Retrieval-Augmented Generation (RAG), which
enhances LLM capabilities by integrating external knowledge. However,
traditional RAG systems face critical limitations, including disrupted
contextual integrity due to text chunking, and over-reliance on semantic
similarity for retrieval. To address these issues, we propose CausalRAG, a
novel framework that incorporates causal graphs into the retrieval process. By
constructing and tracing causal relationships, CausalRAG preserves contextual
continuity and improves retrieval precision, leading to more accurate and
interpretable responses. We evaluate CausalRAG against regular RAG and
graph-based RAG approaches, demonstrating its superiority across several
metrics. Our findings suggest that grounding retrieval in causal reasoning
provides a promising approach to knowledge-intensive tasks.",http://arxiv.org/abs/2503.19878v2,3/25/25,"Nengbo Wang, Xiaotian Han, Jagdip Singh, Jing Ma, Vipin Chaudhary"
49,Human Cognition Inspired RAG with Knowledge Graph for Complex Problem Solving,"Large language models (LLMs) have demonstrated transformative potential
across various domains, yet they face significant challenges in knowledge
integration and complex problem reasoning, often leading to hallucinations and
unreliable outputs. Retrieval-Augmented Generation (RAG) has emerged as a
promising solution to enhance LLMs accuracy by incorporating external
knowledge. However, traditional RAG systems struggle with processing complex
relational information and multi-step reasoning, limiting their effectiveness
in advanced problem-solving tasks. To address these limitations, we propose
CogGRAG, a cognition inspired graph-based RAG framework, designed to improve
LLMs performance in Knowledge Graph Question Answering (KGQA). Inspired by the
human cognitive process of decomposing complex problems and performing
self-verification, our framework introduces a three-stage methodology:
decomposition, retrieval, and reasoning with self-verification. By integrating
these components, CogGRAG enhances the accuracy of LLMs in complex problem
solving. We conduct systematic experiments with three LLM backbones on four
benchmark datasets, where CogGRAG outperforms the baselines.",http://arxiv.org/abs/2503.06567v1,3/9/25,"Yao Cheng, Yibo Zhao, Jiapeng Zhu, Yao Liu, Xing Sun, Xiang Li"
50,Adaptive Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge,"Large Language Models (LLMs) have significantly advanced medical
question-answering by leveraging extensive clinical data and medical
literature. However, the rapid evolution of medical knowledge and the
labor-intensive process of manually updating domain-specific resources pose
challenges to the reliability of these systems. To address this, we introduce
Adaptive Medical Graph-RAG (AMG-RAG), a comprehensive framework that automates
the construction and continuous updating of medical knowledge graphs,
integrates reasoning, and retrieves current external evidence, such as PubMed
and WikiSearch. By dynamically linking new findings and complex medical
concepts, AMG-RAG not only improves accuracy but also enhances interpretability
in medical queries.
  Evaluations on the MEDQA and MEDMCQA benchmarks demonstrate the effectiveness
of AMG-RAG, achieving an F1 score of 74.1 percent on MEDQA and an accuracy of
66.34 percent on MEDMCQA, outperforming both comparable models and those 10 to
100 times larger. Notably, these improvements are achieved without increasing
computational overhead, highlighting the critical role of automated knowledge
graph generation and external evidence retrieval in delivering up-to-date,
trustworthy medical insights.",http://arxiv.org/abs/2502.13010v1,2/18/25,"Mohammad Reza Rezaei, Reza Saadati Fard, Jayson Parker, Rahul G Krishnan, Milad Lankarany"
51,HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications,"Large Language Models (LLMs) face limitations in AI legal and policy
applications due to outdated knowledge, hallucinations, and poor reasoning in
complex contexts. Retrieval-Augmented Generation (RAG) systems address these
issues by incorporating external knowledge, but suffer from retrieval errors,
ineffective context integration, and high operational costs. This paper
presents the Hybrid Parameter-Adaptive RAG (HyPA-RAG) system, designed for the
AI legal domain, with NYC Local Law 144 (LL144) as the test case. HyPA-RAG
integrates a query complexity classifier for adaptive parameter tuning, a
hybrid retrieval approach combining dense, sparse, and knowledge graph methods,
and a comprehensive evaluation framework with tailored question types and
metrics. Testing on LL144 demonstrates that HyPA-RAG enhances retrieval
accuracy, response fidelity, and contextual precision, offering a robust and
adaptable solution for high-stakes legal and policy applications.",http://arxiv.org/abs/2409.09046v2,8/29/24,"Rishi Kalra, Zekun Wu, Ayesha Gulley, Airlie Hilliard, Xin Guan, Adriano Koshiyama, Philip Treleaven"
52,Knowledge Graph-extended Retrieval Augmented Generation for Question Answering,"Large Language Models (LLMs) and Knowledge Graphs (KGs) offer a promising
approach to robust and explainable Question Answering (QA). While LLMs excel at
natural language understanding, they suffer from knowledge gaps and
hallucinations. KGs provide structured knowledge but lack natural language
interaction. Ideally, an AI system should be both robust to missing facts as
well as easy to communicate with. This paper proposes such a system that
integrates LLMs and KGs without requiring training, ensuring adaptability
across different KGs with minimal human effort. The resulting approach can be
classified as a specific form of a Retrieval Augmented Generation (RAG) with a
KG, thus, it is dubbed Knowledge Graph-extended Retrieval Augmented Generation
(KG-RAG). It includes a question decomposition module to enhance multi-hop
information retrieval and answer explainability. Using In-Context Learning
(ICL) and Chain-of-Thought (CoT) prompting, it generates explicit reasoning
chains processed separately to improve truthfulness. Experiments on the MetaQA
benchmark show increased accuracy for multi-hop questions, though with a slight
trade-off in single-hop performance compared to LLM with KG baselines. These
findings demonstrate KG-RAG's potential to improve transparency in QA by
bridging unstructured language understanding with structured knowledge
retrieval.",http://arxiv.org/abs/2504.08893v1,4/11/25,"Jasper Linders, Jakub M Tomczak"
53,Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation,"Retrieval-augmented generation (RAG) has improved large language models
(LLMs) by using knowledge retrieval to overcome knowledge deficiencies.
However, current RAG methods often fall short of ensuring the depth and
completeness of retrieved information, which is necessary for complex reasoning
tasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG
framework that iteratively retrieves information from both unstructured and
structured knowledge sources in a tight-coupling manner. Specifically, ToG-2
leverages knowledge graphs (KGs) to link documents via entities, facilitating
deep and knowledge-guided context retrieval. Simultaneously, it utilizes
documents as entity contexts to achieve precise and efficient graph retrieval.
ToG-2 alternates between graph retrieval and context retrieval to search for
in-depth clues relevant to the question, enabling LLMs to generate answers. We
conduct a series of well-designed experiments to highlight the following
advantages of ToG-2: 1) ToG-2 tightly couples the processes of context
retrieval and graph retrieval, deepening context retrieval via the KG while
enabling reliable graph retrieval based on contexts; 2) it achieves deep and
faithful reasoning in LLMs through an iterative knowledge retrieval process of
collaboration between contexts and the KG; and 3) ToG-2 is training-free and
plug-and-play compatible with various LLMs. Extensive experiments demonstrate
that ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7
knowledge-intensive datasets with GPT-3.5, and can elevate the performance of
smaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning.
The source code is available on https://github.com/IDEA-FinAI/ToG-2.",http://arxiv.org/abs/2407.10805v7,7/15/24,"Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, Jian Guo"
54,"Mixture-of-PageRanks: Replacing Long-Context with Real-Time, Sparse GraphRAG","Recent advances have extended the context window of frontier LLMs
dramatically, from a few thousand tokens up to millions, enabling entire books
and codebases to fit into context. However, the compute costs of inferencing
long-context LLMs are massive and often prohibitive in practice. RAG offers an
efficient and effective alternative: retrieve and process only the subset of
the context most important for the current task. Although promising, recent
work applying RAG to long-context tasks has two core limitations: 1) there has
been little focus on making the RAG pipeline compute efficient, and 2) such
works only test on simple QA tasks, and their performance on more challenging
tasks is unclear. To address this, we develop an algorithm based on PageRank, a
graph-based retrieval algorithm, which we call mixture-of-PageRanks (MixPR).
MixPR uses a mixture of PageRank-based graph-retrieval algorithms implemented
using sparse matrices for efficent, cheap retrieval that can deal with a
variety of complex tasks. Our MixPR retriever achieves state-of-the-art results
across a wide range of long-context benchmark tasks, outperforming both
existing RAG methods, specialized retrieval architectures, and long-context
LLMs despite being far more compute efficient. Due to using sparse embeddings,
our retriever is extremely compute efficient, capable of embedding and
retrieving millions of tokens within a few seconds and runs entirely on CPU.",http://arxiv.org/abs/2412.06078v1,12/8/24,"Nicholas Alonso, Beren Millidge"
55,G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering,"Given a graph with textual attributes, we enable users to `chat with their
graph': that is, to ask questions about the graph using a conversational
interface. In response to a user's questions, our method provides textual
replies and highlights the relevant parts of the graph. While existing works
integrate large language models (LLMs) and graph neural networks (GNNs) in
various ways, they mostly focus on either conventional graph tasks (such as
node, edge, and graph classification), or on answering simple graph queries on
small or synthetic graphs. In contrast, we develop a flexible
question-answering framework targeting real-world textual graphs, applicable to
multiple applications including scene graph understanding, common sense
reasoning, and knowledge graph reasoning. Toward this goal, we first develop a
Graph Question Answering (GraphQA) benchmark with data collected from different
tasks. Then, we propose our G-Retriever method, introducing the first
retrieval-augmented generation (RAG) approach for general textual graphs, which
can be fine-tuned to enhance graph understanding via soft prompting. To resist
hallucination and to allow for textual graphs that greatly exceed the LLM's
context window size, G-Retriever performs RAG over a graph by formulating this
task as a Prize-Collecting Steiner Tree optimization problem. Empirical
evaluations show that our method outperforms baselines on textual graph tasks
from multiple domains, scales well with larger graph sizes, and mitigates
hallucination.~\footnote{Our codes and datasets are available at:
\url{https://github.com/XiaoxinHe/G-Retriever}}",http://arxiv.org/abs/2402.07630v3,2/12/24,"Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi"
56,KG-RAG: Bridging the Gap Between Knowledge and Creativity,"Ensuring factual accuracy while maintaining the creative capabilities of
Large Language Model Agents (LMAs) poses significant challenges in the
development of intelligent agent systems. LMAs face prevalent issues such as
information hallucinations, catastrophic forgetting, and limitations in
processing long contexts when dealing with knowledge-intensive tasks. This
paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)
pipeline, a novel framework designed to enhance the knowledge capabilities of
LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities
of LLMs, thereby significantly reducing the reliance on the latent knowledge of
LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then
performs information retrieval over the newly created graph to perform KGQA
(Knowledge Graph Question Answering). The retrieval methodology leverages a
novel algorithm called Chain of Explorations (CoE) which benefits from LLMs
reasoning to explore nodes and relationships within the KG sequentially.
Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable
improvements in the reduction of hallucinated content and suggest a promising
path toward developing intelligent systems adept at handling
knowledge-intensive tasks.",http://arxiv.org/abs/2405.12035v1,5/20/24,Diego Sanmartin
57,EACO-RAG: Towards Distributed Tiered LLM Deployment using Edge-Assisted and Collaborative RAG with Adaptive Knowledge Update,"Large language models (LLMs) have demonstrated impressive capabilities in
language tasks, but they require high computing power and rely on static
knowledge. To overcome these limitations, Retrieval-Augmented Generation (RAG)
incorporates up-to-date external information into LLMs without extensive
fine-tuning. Meanwhile, small language models (SLMs) deployed on edge devices
offer efficiency and low latency but often struggle with complex reasoning
tasks. Unfortunately, current RAG approaches are predominantly based on
centralized databases and have not been adapted to address the distinct
constraints associated with deploying SLMs in edge environments. To bridge this
gap, we propose Edge-Assisted and Collaborative RAG (EACO-RAG), a lightweight
framework that leverages distributed edge nodes for adaptive knowledge updates
and retrieval. EACO-RAG also employs a hierarchical collaborative gating
mechanism to dynamically select among local, edge-assisted, and cloud-based
strategies, with a carefully designed algorithm based on Safe Online Bayesian
Optimization to maximize the potential performance enhancements. Experimental
results demonstrate that EACO-RAG matches the accuracy of cloud-based knowledge
graph RAG systems while reducing total costs by up to 84.6% under relaxed delay
constraints and by 65.3% under stricter delay requirements. This work
represents our initial effort toward achieving a distributed and scalable
tiered LLM deployments, with EACO-RAG serving as a promising first step in
unlocking the full potential of hybrid edge-cloud intelligence.",http://arxiv.org/abs/2410.20299v2,10/27/24,"Jiaxing Li, Chi Xu, Lianchen Jia, Feng Wang, Cong Zhang, Jiangchuan Liu"
58,OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese Medicine Knowledge Retrieval and Diagnosis,"Traditional Chinese Medicine (TCM) represents a rich repository of ancient
medical knowledge that continues to play an important role in modern
healthcare. Due to the complexity and breadth of the TCM literature, the
integration of AI technologies is critical for its modernization and broader
accessibility. However, this integration poses considerable challenges,
including the interpretation of obscure classical Chinese texts and the
modeling of intricate semantic relationships among TCM concepts. In this paper,
we develop OpenTCM, an LLM-based system that combines a domain-specific TCM
knowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG).
First, we extract more than 3.73 million classical Chinese characters from 68
gynecological books in the Chinese Medical Classics Database, with the help of
TCM and gynecology experts. Second, we construct a comprehensive
multi-relational knowledge graph comprising more than 48,000 entities and
152,000 interrelationships, using customized prompts and Chinese-oriented LLMs
such as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last,
we integrate OpenTCM with this knowledge graph, enabling high-fidelity
ingredient knowledge retrieval and diagnostic question-answering without model
fine-tuning. Experimental evaluations demonstrate that our prompt design and
model selection significantly improve knowledge graph quality, achieving a
precision of 98. 55% and an F1 score of 99. 55%. In addition, OpenTCM achieves
mean expert scores of 4.5 in ingredient information retrieval and 3.8 in
diagnostic question-answering tasks, outperforming state-of-the-art solutions
in real-world TCM use cases.",http://arxiv.org/abs/2504.20118v2,4/28/25,"Jinglin He, Yunqi Guo, Lai Kwan Lam, Waikei Leung, Lixing He, Yuanan Jiang, Chi Chiu Wang, Guoliang Xing, Hongkai Chen"
59,FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation based on Knowledge Graphs,"To mitigate the hallucination and knowledge deficiency in large language
models (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)
has shown promising potential by utilizing KGs as external resource to enhance
LLMs reasoning. However, existing KG-RAG approaches struggle with a trade-off
between flexibility and retrieval quality. Modular methods prioritize
flexibility by avoiding the use of KG-fine-tuned models during retrieval,
leading to fixed retrieval strategies and suboptimal retrieval quality.
Conversely, coupled methods embed KG information within models to improve
retrieval quality, but at the expense of flexibility. In this paper, we propose
a novel flexible modular KG-RAG framework, termed FRAG, which synergizes the
advantages of both approaches. FRAG estimates the hop range of reasoning paths
based solely on the query and classify it as either simple or complex. To match
the complexity of the query, tailored pipelines are applied to ensure efficient
and accurate reasoning path retrieval, thus fostering the final reasoning
process. By using the query text instead of the KG to infer the structural
information of reasoning paths and employing adaptable retrieval strategies,
FRAG improves retrieval quality while maintaining flexibility. Moreover, FRAG
does not require extra LLMs fine-tuning or calls, significantly boosting
efficiency and conserving resources. Extensive experiments show that FRAG
achieves state-of-the-art performance with high efficiency and low resource
consumption.",http://arxiv.org/abs/2501.09957v2,1/17/25,"Zengyi Gao, Yukun Cao, Hairu Wang, Ao Ke, Yuan Feng, Xike Xie, S Kevin Zhou"
60,PropRAG: Guiding Retrieval with Beam Search over Proposition Paths,"Retrieval Augmented Generation (RAG) has become the standard non-parametric
approach for equipping Large Language Models (LLMs) with up-to-date knowledge
and mitigating catastrophic forgetting common in continual learning. However,
standard RAG, relying on independent passage retrieval, fails to capture the
interconnected nature of human memory crucial for complex reasoning
(associativity) and contextual understanding (sense-making). While structured
RAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,
the inherent context loss limits fidelity. We introduce PropRAG, a framework
leveraging contextually rich propositions and a novel beam search algorithm
over proposition paths to explicitly discover multi-step reasoning chains.
Crucially, PropRAG's online retrieval process operates entirely without
invoking generative LLMs, relying instead on efficient graph traversal and
pre-computed embeddings. This avoids online LLM inference costs and potential
inconsistencies during evidence gathering. LLMs are used effectively offline
for high-quality proposition extraction and post-retrieval for answer
generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on
PopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside
top F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through
richer representation and explicit, LLM-free online path finding, PropRAG
advances non-parametric continual learning.",http://arxiv.org/abs/2504.18070v1,4/25/25,Jingjin Wang
61,HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) systems often struggle with imperfect
retrieval, as traditional retrievers focus on lexical or semantic similarity
rather than logical relevance. To address this, we propose \textbf{HopRAG}, a
novel RAG framework that augments retrieval with logical reasoning through
graph-structured knowledge exploration. During indexing, HopRAG constructs a
passage graph, with text chunks as vertices and logical connections established
via LLM-generated pseudo-queries as edges. During retrieval, it employs a
\textit{retrieve-reason-prune} mechanism: starting with lexically or
semantically similar passages, the system explores multi-hop neighbors guided
by pseudo-queries and LLM reasoning to identify truly relevant ones.
Experiments on multiple multi-hop benchmarks demonstrate that HopRAG's
\textit{retrieve-reason-prune} mechanism can expand the retrieval scope based
on logical connections and improve final answer quality.",http://arxiv.org/abs/2502.12442v2,2/18/25,"Hao Liu, Zhengren Wang, Xi Chen, Zhiyu Li, Feiyu Xiong, Qinhan Yu, Wentao Zhang"
62,RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation,"Large language models (LLMs) have achieved impressive performance on
knowledge-intensive tasks, yet they often struggle with multi-step reasoning
due to the unstructured nature of retrieved context. While retrieval-augmented
generation (RAG) methods provide external information, the lack of explicit
organization among retrieved passages limits their effectiveness, leading to
brittle reasoning pathways. Recent interpretability studies highlighting the
importance of structured intermediate reasoning further align with this
perspective. We propose Retrieval-And-Structuring (RAS), a framework that
dynamically constructs query-specific knowledge graphs through iterative
retrieval and structured knowledge building. RAS interleaves targeted retrieval
planning with incremental graph construction, enabling models to assemble and
reason over evolving knowledge structures tailored to each query. On seven
knowledge-intensive benchmarks, RAS consistently outperforms strong baselines,
achieving up to 6.4% and 7.0% gains with open-source and proprietary LLMs,
respectively. Our results demonstrate that dynamic, query-specific knowledge
structuring offers a robust path to improving reasoning accuracy and robustness
in language model generation. Our data and code can be found at
https://github.com/pat-jj/RAS.",http://arxiv.org/abs/2502.10996v2,2/16/25,"Pengcheng Jiang, Lang Cao, Ruike Zhu, Minhao Jiang, Yunyi Zhang, Jimeng Sun, Jiawei Han"
63,WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs,"Large Language Models (LLMs) have greatly contributed to the development of
adaptive intelligent agents and are positioned as an important way to achieve
Artificial General Intelligence (AGI). However, LLMs are prone to produce
factually incorrect information and often produce ""phantom"" content that
undermines their reliability, which poses a serious challenge for their
deployment in real-world scenarios. Enhancing LLMs by combining external
databases and information retrieval mechanisms is an effective path. To address
the above challenges, we propose a new approach called WeKnow-RAG, which
integrates Web search and Knowledge Graphs into a ""Retrieval-Augmented
Generation (RAG)"" system. First, the accuracy and reliability of LLM responses
are improved by combining the structured representation of Knowledge Graphs
with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes
domain-specific knowledge graphs to satisfy a variety of queries and domains,
thereby improving performance on factual information and complex reasoning
tasks by employing multi-stage web page retrieval techniques using both sparse
and dense retrieval methods. Our approach effectively balances the efficiency
and accuracy of information retrieval, thus improving the overall retrieval
process. Finally, we also integrate a self-assessment mechanism for the LLM to
evaluate the trustworthiness of the answers it generates. Our approach proves
its outstanding effectiveness in a wide range of offline experiments and online
submissions.",http://arxiv.org/abs/2408.07611v2,8/14/24,"Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, Zetian Hu"
64,CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models,"Chain-of-thought (CoT) reasoning boosts large language models' (LLMs)
performance on complex tasks but faces two key limitations: a lack of
reliability when solely relying on LLM-generated reasoning chains and
interference from natural language reasoning steps with the models' inference
process, also known as the inference logic of LLMs. To address these issues, we
propose CoT-RAG, a novel reasoning framework with three key designs: (i)
Knowledge Graph-driven CoT Generation,featuring knowledge graphs to modulate
reasoning chain generation of LLMs, thereby enhancing reasoning credibility;
(ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented
generation (RAG) into knowledge graphs to retrieve relevant sub-cases and
sub-descriptions, providing LLMs with learnable information; (iii)
Pseudo-Program Prompting Execution, which promotes greater logical rigor by
guiding LLMs to execute reasoning tasks as pseudo-programs. Evaluations on nine
public datasets spanning three reasoning tasks reveal significant accuracy
gains--ranging from 4.0% to 44.3%--over state-of-the-art methods. Furthermore,
tests on four domain-specific datasets demonstrate exceptional accuracy and
efficient execution, underscoring its practical applicability and scalability.",http://arxiv.org/abs/2504.13534v2,4/18/25,"Feiyang Li, Peng Fang, Zhan Shi, Arijit Khan, Fang Wang, Dan Feng, Weihao Wang, Xin Zhang, Yongjian Cui"
65,Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models,"Retrieval-Augmented Generation (RAG) is a promising technique for applying
LLMs to proprietary domains. However, retrieved documents may contain sensitive
knowledge, posing risks of privacy leakage in generative results. Thus,
effectively erasing private information from retrieved documents is a key
challenge for RAG. Unlike traditional text anonymization, RAG should consider:
(1) the inherent multi-document reasoning may face de-anonymization attacks;
(2) private knowledge varies by scenarios, so users should be allowed to
customize which information to erase; (3) preserving sufficient publicly
available knowledge for generation tasks. This paper introduces the privacy
erasure task for RAG and proposes Eraser4RAG, a private knowledge eraser which
effectively removes user-defined private knowledge from documents while
preserving sufficient public knowledge for generation. Specifically, we first
construct a global knowledge graph to identify potential knowledge across
documents, aiming to defend against de-anonymization attacks. Then we randomly
split it into private and public sub-graphs, and fine-tune Flan-T5 to rewrite
the retrieved documents excluding private triples. Finally, PPO algorithm
optimizes the rewriting model to minimize private triples and maximize public
triples retention. Experiments on four QA datasets demonstrate that Eraser4RAG
achieves superior erase performance than GPT-4o.",http://arxiv.org/abs/2504.09910v1,4/14/25,"Yujing Wang, Hainan Zhang, Liang Pang, Yongxin Tong, Binghui Guo, Hongwei Zheng, Zhiming Zheng"
66,SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache,"Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to incorporate structured knowledge via graph retrieval as
contextual input, enhancing more accurate and context-aware reasoning. We
observe that for different queries, it could retrieve similar subgraphs as
prompts, and thus we propose SubGCache, which aims to reduce inference latency
by reusing computation across queries with similar structural prompts (i.e.,
subgraphs). Specifically, SubGCache clusters queries based on subgraph
embeddings, constructs a representative subgraph for each cluster, and
pre-computes the key-value (KV) cache of the representative subgraph. For each
query with its retrieved subgraph within a cluster, it reuses the pre-computed
KV cache of the representative subgraph of the cluster without computing the KV
tensors again for saving computation. Experiments on two new datasets across
multiple LLM backbones and graph-based RAG frameworks demonstrate that
SubGCache consistently reduces inference latency with comparable and even
improved generation quality, achieving up to 6.68$\times$ reduction in
time-to-first-token (TTFT).",http://arxiv.org/abs/2505.10951v2,5/16/25,"Qiuyu Zhu, Liang Zhang, Qianxiong Xu, Cheng Long, Jie Zhang"
67,Plan*RAG: Efficient Test-Time Planning for Retrieval Augmented Generation,"We introduce Plan*RAG, a novel framework that enables structured multi-hop
reasoning in retrieval-augmented generation (RAG) through test-time reasoning
plan generation. While existing approaches such as ReAct maintain reasoning
chains within the language model's context window, we observe that this often
leads to plan fragmentation and execution failures. Our key insight is that by
isolating the reasoning plan as a directed acyclic graph (DAG) outside the LM's
working memory, we can enable (1) systematic exploration of reasoning paths,
(2) atomic subqueries enabling precise retrievals and grounding, and (3)
efficiency through parallel execution and bounded context window utilization.
Moreover, Plan*RAG's modular design allows it to be integrated with existing
RAG methods, thus providing a practical solution to improve current RAG
systems. On standard multi-hop reasoning benchmarks, Plan*RAG consistently
achieves improvements over recently proposed methods such as RQ-RAG and
Self-RAG, while maintaining comparable computational costs.",http://arxiv.org/abs/2410.20753v2,10/28/24,"Prakhar Verma, Sukruta Prakash Midigeshi, Gaurav Sinha, Arno Solin, Nagarajan Natarajan, Amit Sharma"
68,Knowledge Graph-Guided Retrieval Augmented Generation,"Retrieval-augmented generation (RAG) has emerged as a promising technology
for addressing hallucination issues in the responses generated by large
language models (LLMs). Existing studies on RAG primarily focus on applying
semantic-based approaches to retrieve isolated relevant chunks, which ignore
their intrinsic relationships. In this paper, we propose a novel Knowledge
Graph-Guided Retrieval Augmented Generation (KG$^2$RAG) framework that utilizes
knowledge graphs (KGs) to provide fact-level relationships between chunks,
improving the diversity and coherence of the retrieved results. Specifically,
after performing a semantic-based retrieval to provide seed chunks, KG$^2$RAG
employs a KG-guided chunk expansion process and a KG-based chunk organization
process to deliver relevant and important knowledge in well-organized
paragraphs. Extensive experiments conducted on the HotpotQA dataset and its
variants demonstrate the advantages of KG$^2$RAG compared to existing RAG-based
approaches, in terms of both response quality and retrieval quality.",http://arxiv.org/abs/2502.06864v1,2/8/25,"Xiangrong Zhu, Yuexiang Xie, Yi Liu, Yaliang Li, Wei Hu"
69,A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph,"This study aims to improve knowledge-based question-answering (QA) systems by
overcoming the limitations of existing Retrieval-Augmented Generation (RAG)
models and implementing an advanced RAG system based on Graph technology to
develop high-quality generative AI services. While existing RAG models
demonstrate high accuracy and fluency by utilizing retrieved information, they
may suffer from accuracy degradation as they generate responses using
pre-loaded knowledge without reprocessing. Additionally, they cannot
incorporate real-time data after the RAG configuration stage, leading to issues
with contextual understanding and biased information. To address these
limitations, this study implemented an enhanced RAG system utilizing Graph
technology. This system is designed to efficiently search and utilize
information. Specifically, it employs LangGraph to evaluate the reliability of
retrieved information and synthesizes diverse data to generate more accurate
and enhanced responses. Furthermore, the study provides a detailed explanation
of the system's operation, key implementation steps, and examples through
implementation code and validation results, thereby enhancing the understanding
of advanced RAG technology. This approach offers practical guidelines for
implementing advanced RAG systems in corporate services, making it a valuable
resource for practical application.",http://arxiv.org/abs/2407.19994v3,7/29/24,Cheonsu Jeong
70,"Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering","Recent advances in large language models (LLMs) have led to impressive
progress in natural language generation, yet their tendency to produce
hallucinated or unsubstantiated content remains a critical concern. To improve
factual reliability, Retrieval-Augmented Generation (RAG) integrates external
knowledge during inference. However, existing RAG systems face two major
limitations: (1) unreliable adaptive control due to limited external knowledge
supervision, and (2) hallucinations caused by inaccurate or irrelevant
references. To address these issues, we propose Know3-RAG, a knowledge-aware
RAG framework that leverages structured knowledge from knowledge graphs (KGs)
to guide three core stages of the RAG process, including retrieval, generation,
and filtering. Specifically, we introduce a knowledge-aware adaptive retrieval
module that employs KG embedding to assess the confidence of the generated
answer and determine retrieval necessity, a knowledge-enhanced reference
generation strategy that enriches queries with KG-derived entities to improve
generated reference relevance, and a knowledge-driven reference filtering
mechanism that ensures semantic alignment and factual accuracy of references.
Experiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG
consistently outperforms strong baselines, significantly reducing
hallucinations and enhancing answer reliability.",http://arxiv.org/abs/2505.12662v1,5/19/25,"Xukai Liu, Ye Liu, Shiwen Wu, Yanghai Zhang, Yihao Yuan, Kai Zhang, Qi Liu"
71,MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot,"Retrieval-augmented generation (RAG) is a well-suited technique for
retrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a
key module of the healthcare copilot, helping reduce misdiagnosis for
healthcare practitioners and patients. However, the diagnostic accuracy and
specificity of existing heuristic-based RAG models used in the medical domain
are inadequate, particularly for diseases with similar manifestations. This
paper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited
reasoning for the medical domain that retrieves diagnosis and treatment
recommendations based on manifestations. MedRAG systematically constructs a
comprehensive four-tier hierarchical diagnostic KG encompassing critical
diagnostic differences of various diseases. These differences are dynamically
integrated with similar EHRs retrieved from an EHR database, and reasoned
within a large language model. This process enables more accurate and specific
decision support, while also proactively providing follow-up questions to
enhance personalized medical decision-making. MedRAG is evaluated on both a
public dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD)
collected from Tan Tock Seng Hospital, and its performance is compared against
various existing RAG methods. Experimental results show that, leveraging the
information integration and relational abilities of the KG, our MedRAG provides
more specific diagnostic insights and outperforms state-of-the-art models in
reducing misdiagnosis rates. Our code will be available at
https://github.com/SNOWTEAM2023/MedRAG",http://arxiv.org/abs/2502.04413v1,2/6/25,"Xuejiao Zhao, Siyan Liu, SuYin Yang, Chunyan Miao"
72,RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models for Question Answering,"Retrieval-augmented generation (RAG) has shown promising potential in
knowledge intensive question answering (QA). However, existing approaches only
consider the query itself, neither specifying the retrieval preferences for the
retrievers nor informing the generators of how to refer to the retrieved
documents for the answers, which poses a significant challenge to the QA
performance. To address these issues, we propose Rule-guided
Retrieval-Augmented Generation with LMs, which explicitly introduces rules for
in-context learning (RuleRAG-ICL) to guide retrievers to recall related
documents in the directions of rules and uniformly guide generators to reason
attributed by the same rules. Moreover, most existing RAG datasets were
constructed without considering rules and Knowledge Graphs (KGs) are recognized
as providing high-quality rules. Therefore, we construct five rule-aware RAG
benchmarks for QA, RuleQA, based on KGs to stress the significance of retrieval
and reasoning with rules. Experiments on RuleQA demonstrate RuleRAG-ICL
improves the retrieval quality of +89.2% in Recall@10 and answer accuracy of
+103.1% in Exact Match, and RuleRAG-FT yields more enhancement. In addition,
experiments on four existing RAG datasets show RuleRAG is also effective by
offering rules in RuleQA to them, further proving the generalization of rule
guidance in RuleRAG.",http://arxiv.org/abs/2410.22353v3,10/15/24,"Zhongwu Chen, Chengjin Xu, Dingmin Wang, Zhen Huang, Yong Dou, Xuhui Jiang, Jian Guo"
73,AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge Reasoning,"Despite the outstanding capabilities of large language models (LLMs),
knowledge-intensive reasoning still remains a challenging task due to LLMs'
limitations in compositional reasoning and the hallucination problem. A
prevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented
generation (RAG), which first formulates a reasoning plan by decomposing
complex questions into simpler sub-questions, and then applies iterative RAG at
each sub-question. However, prior works exhibit two crucial problems:
inadequate reasoning planning and poor incorporation of heterogeneous
knowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct
accurate heterogeneous knowledge reasoning at the atomic level. Inspired by how
knowledge graph query languages model compositional reasoning through combining
predefined operations, we propose three atomic knowledge operators, a unified
set of operators for LLMs to retrieve and manipulate knowledge from
heterogeneous sources. First, in the reasoning planning stage, AtomR decomposes
a complex question into a reasoning tree where each leaf node corresponds to an
atomic knowledge operator, achieving question decomposition that is highly
fine-grained and orthogonal. Subsequently, in the reasoning execution stage,
AtomR executes each atomic knowledge operator, which flexibly selects,
retrieves, and operates atomic level knowledge from heterogeneous sources. We
also introduce BlendQA, a challenging benchmark specially tailored for
heterogeneous knowledge reasoning. Experiments on three single-source and two
multi-source datasets show that AtomR outperforms state-of-the-art baselines by
a large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on
BlendQA. We release our code and datasets.",http://arxiv.org/abs/2411.16495v3,11/25/24,"Amy Xin, Jinxin Liu, Zijun Yao, Zhicheng Lee, Shulin Cao, Lei Hou, Juanzi Li"
74,LLM-assisted Graph-RAG Information Extraction from IFC Data,"IFC data has become the general building information standard for
collaborative work in the construction industry. However, IFC data can be very
complicated because it allows for multiple ways to represent the same product
information. In this research, we utilise the capabilities of LLMs to parse the
IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to
retrieve building object properties and their relations. We will show that,
despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG
parsing enhances generative LLMs like GPT-4o with graph-based knowledge,
enabling natural language query-response retrieval without the need for a
complex pipeline.",http://arxiv.org/abs/2504.16813v1,4/23/25,"Sima Iranmanesh, Hadeel Saadany, Edlira Vakaj"
75,Knowledge Management for Automobile Failure Analysis Using Graph RAG,"This paper presents a knowledge management system for automobile failure
analysis using retrieval-augmented generation (RAG) with large language models
(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a
growing demand for knowledge transfer of failure analysis from experienced
engineers to young engineers. However, failure events are phenomena that occur
in a chain reaction, making them difficult for beginners to analyze them. While
knowledge graphs, which can describe semantic relationships and structure
information is effective in representing failure events, due to their
capability of representing the relationships between components, there is much
information in KGs, so it is challenging for young engineers to extract and
understand sub-graphs from the KG. On the other hand, there is increasing
interest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for
knowledge management. However, when using the current Graph RAG framework with
an existing knowledge graph for automobile failures, several issues arise
because it is difficult to generate executable queries for a knowledge graph
database which is not constructed by LLMs. To address this, we focused on
optimizing the Graph RAG pipeline for existing knowledge graphs. Using an
original Q&A dataset, the ROUGE F1 score of the sentences generated by the
proposed method showed an average improvement of 157.6% compared to the current
method. This highlights the effectiveness of the proposed method for automobile
failure analysis.",http://arxiv.org/abs/2411.19539v1,11/29/24,"Yuta Ojima, Hiroki Sakaji, Tadashi Nakamura, Hiroaki Sakata, Kazuya Seki, Yuu Teshigawara, Masami Yamashita, Kazuhiro Aoyama"
76,Pseudo-Knowledge Graph: Meta-Path Guided Retrieval and In-Graph Text for RAG-Equipped LLM,"The advent of Large Language Models (LLMs) has revolutionized natural
language processing. However, these models face challenges in retrieving
precise information from vast datasets. Retrieval-Augmented Generation (RAG)
was developed to combining LLMs with external information retrieval systems to
enhance the accuracy and context of responses. Despite improvements, RAG still
struggles with comprehensive retrieval in high-volume, low-information-density
databases and lacks relational awareness, leading to fragmented answers.
  To address this, this paper introduces the Pseudo-Knowledge Graph (PKG)
framework, designed to overcome these limitations by integrating Meta-path
Retrieval, In-graph Text and Vector Retrieval into LLMs. By preserving natural
language text and leveraging various retrieval techniques, the PKG offers a
richer knowledge representation and improves accuracy in information retrieval.
Extensive evaluations using Open Compass and MultiHop-RAG benchmarks
demonstrate the framework's effectiveness in managing large volumes of data and
complex relationships.",http://arxiv.org/abs/2503.00309v1,3/1/25,"Yuxin Yang, Haoyang Wu, Tao Wang, Jia Yang, Hao Ma, Guojie Luo"
77,Optimizing open-domain question answering with graph-based retrieval augmented generation,"In this work, we benchmark various graph-based retrieval-augmented generation
(RAG) systems across a broad spectrum of query types, including OLTP-style
(fact-based) and OLAP-style (thematic) queries, to address the complex demands
of open-domain question answering (QA). Traditional RAG methods often fall
short in handling nuanced, multi-document synthesis tasks. By structuring
knowledge as graphs, we can facilitate the retrieval of context that captures
greater semantic depth and enhances language model operations. We explore
graph-based RAG methodologies and introduce TREX, a novel, cost-effective
alternative that combines graph-based and vector-based retrieval techniques.
Our benchmarking across four diverse datasets highlights the strengths of
different RAG methodologies, demonstrates TREX's ability to handle multiple
open-domain QA types, and reveals the limitations of current evaluation
methods.
  In a real-world technical support case study, we demonstrate how TREX
solutions can surpass conventional vector-based RAG in efficiently synthesizing
data from heterogeneous sources. Our findings underscore the potential of
augmenting large language models with advanced retrieval and orchestration
capabilities, advancing scalable, graph-based AI solutions.",http://arxiv.org/abs/2503.02922v1,3/4/25,"Joyce Cahoon, Prerna Singh, Nick Litombe, Jonathan Larson, Ha Trinh, Yiwen Zhu, Andreas Mueller, Fotis Psallidas, Carlo Curino"
78,Hierarchical Planning for Complex Tasks with Knowledge Graph-RAG and Symbolic Verification,"Large Language Models (LLMs) have shown promise as robotic planners but often
struggle with long-horizon and complex tasks, especially in specialized
environments requiring external knowledge. While hierarchical planning and
Retrieval-Augmented Generation (RAG) address some of these challenges, they
remain insufficient on their own and a deeper integration is required for
achieving more reliable systems. To this end, we propose a neuro-symbolic
approach that enhances LLMs-based planners with Knowledge Graph-based RAG for
hierarchical plan generation. This method decomposes complex tasks into
manageable subtasks, further expanded into executable atomic action sequences.
To ensure formal correctness and proper decomposition, we integrate a Symbolic
Validator, which also functions as a failure detector by aligning expected and
observed world states. Our evaluation against baseline methods demonstrates the
consistent significant advantages of integrating hierarchical planning,
symbolic verification, and RAG across tasks of varying complexity and different
LLMs. Additionally, our experimental setup and novel metrics not only validate
our approach for complex planning but also serve as a tool for assessing LLMs'
reasoning and compositional capabilities.",http://arxiv.org/abs/2504.04578v1,4/6/25,"Cristina Cornelio, Flavio Petruzzellis, Pietro Lio"
79,A Pilot Empirical Study on When and How to Use Knowledge Graphs as Retrieval Augmented Generation,"The integration of Knowledge Graphs (KGs) into the Retrieval Augmented
Generation (RAG) framework has attracted significant interest, with early
studies showing promise in mitigating hallucinations and improving model
accuracy. However, a systematic understanding and comparative analysis of the
rapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the
foundation for systematically answering the question of when and how to use
KG-RAG by analyzing their performance in various application scenarios
associated with different technical configurations. After outlining the mind
map using KG-RAG framework and summarizing its popular pipeline, we conduct a
pilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG
methods across 9 datasets in diverse domains and scenarios, analyzing the
impact of 9 KG-RAG configurations in combination with 17 LLMs, and combining
Metacognition with KG-RAG as a pilot attempt. Our results underscore the
critical role of appropriate application conditions and optimal configurations
of KG-RAG components.",http://arxiv.org/abs/2502.20854v3,2/28/25,"Xujie Yuan, Yongxu Liu, Shimin Di, Shiwen Wu, Libin Zheng, Rui Meng, Lei Chen, Xiaofang Zhou, Jian Yin"
80,Knowledge graph enhanced retrieval-augmented generation for failure mode and effects analysis,"Failure mode and effects analysis (FMEA) is an essential tool for mitigating
potential failures, particularly during the ramp-up phases of new products.
However, its effectiveness is often limited by the reasoning capabilities of
the FMEA tools, which are usually tabular structured. Meanwhile, large language
models (LLMs) offer novel prospects for advanced natural language processing
tasks. However, LLMs face challenges in tasks that require factual knowledge, a
gap that retrieval-augmented generation (RAG) approaches aim to fill. RAG
retrieves information from a non-parametric data store and uses a language
model to generate responses. Building on this concept, we propose to enhance
the non-parametric data store with a knowledge graph (KG). By integrating a KG
into the RAG framework, we aim to leverage analytical and semantic
question-answering capabilities for FMEA data. This paper contributes by
presenting set-theoretic standardization and a schema for FMEA data, an
algorithm for creating vector embeddings from the FMEA-KG, and a KG-enhanced
RAG framework. Our approach is validated through a user experience design
study, and we measure the precision and performance of the context retrieval
recall.",http://arxiv.org/abs/2406.18114v3,6/26/24,"Lukas Bahr, Christoph Wehner, Judith Wewerka, Jos Bittencourt, Ute Schmid, Rdiger Daub"
81,ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has proven effective in integrating
external knowledge into large language models (LLMs) for question-answer (QA)
tasks. The state-of-the-art RAG approaches often use the graph data as the
external data since they capture the rich semantic information and link
relationships between entities. However, existing graph-based RAG approaches
cannot accurately identify the relevant information from the graph and also
consume large numbers of tokens in the online retrieval process. To address
these issues, we introduce a novel graph-based RAG approach, called Attributed
Community-based Hierarchical RAG (ArchRAG), by augmenting the question using
attributed communities, and also introducing a novel LLM-based hierarchical
clustering method. To retrieve the most relevant information from the graph for
the question, we build a novel hierarchical index structure for the attributed
communities and develop an effective online retrieval method. Experimental
results demonstrate that ArchRAG outperforms existing methods in terms of both
accuracy and token cost.",http://arxiv.org/abs/2502.09891v1,2/14/25,"Shu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, Yuchi Ma"
82,Hyper-RAG: Combating LLM Hallucinations using Hypergraph-Driven Retrieval-Augmented Generation,"Large language models (LLMs) have transformed various sectors, including
education, finance, and medicine, by enhancing content generation and
decision-making processes. However, their integration into the medical field is
cautious due to hallucinations, instances where generated content deviates from
factual accuracy, potentially leading to adverse outcomes. To address this, we
introduce Hyper-RAG, a hypergraph-driven Retrieval-Augmented Generation method
that comprehensively captures both pairwise and beyond-pairwise correlations in
domain-specific knowledge, thereby mitigating hallucinations. Experiments on
the NeurologyCrop dataset with six prominent LLMs demonstrated that Hyper-RAG
improves accuracy by an average of 12.3% over direct LLM use and outperforms
Graph RAG and Light RAG by 6.3% and 6.0%, respectively. Additionally, Hyper-RAG
maintained stable performance with increasing query complexity, unlike existing
methods which declined. Further validation across nine diverse datasets showed
a 35.5% performance improvement over Light RAG using a selection-based
assessment. The lightweight variant, Hyper-RAG-Lite, achieved twice the
retrieval speed and a 3.3% performance boost compared with Light RAG. These
results confirm Hyper-RAG's effectiveness in enhancing LLM reliability and
reducing hallucinations, making it a robust solution for high-stakes
applications like medical diagnostics.",http://arxiv.org/abs/2504.08758v1,3/30/25,"Yifan Feng, Hao Hu, Xingliang Hou, Shiquan Liu, Shihui Ying, Shaoyi Du, Han Hu, Yue Gao"
83,Graph RAG-Tool Fusion,"Recent developments in retrieval-augmented generation (RAG) for selecting
relevant tools from a tool knowledge base enable LLM agents to scale their
complex tool calling capabilities to hundreds or thousands of external tools,
APIs, or agents-as-tools. However, traditional RAG-based tool retrieval fails
to capture structured dependencies between tools, limiting the retrieval
accuracy of a retrieved tool's dependencies. For example, among a vector
database of tools, a ""get stock price"" API requires a ""stock ticker"" parameter
from a ""get stock ticker"" API, and both depend on OS-level internet
connectivity tools. In this paper, we address this limitation by introducing
Graph RAG-Tool Fusion, a novel plug-and-play approach that combines the
strengths of vector-based retrieval with efficient graph traversal to capture
all relevant tools (nodes) along with any nested dependencies (edges) within
the predefined tool knowledge graph. We also present ToolLinkOS, a new tool
selection benchmark of 573 fictional tools, spanning over 15 industries, each
with an average of 6.3 tool dependencies. We demonstrate that Graph RAG-Tool
Fusion achieves absolute improvements of 71.7% and 22.1% over na\""ive RAG on
ToolLinkOS and ToolSandbox benchmarks, respectively (mAP@10). ToolLinkOS
dataset is available at
https://github.com/EliasLumer/Graph-RAG-Tool-Fusion-ToolLinkOS",http://arxiv.org/abs/2502.07223v1,2/11/25,"Elias Lumer, Pradeep Honaganahalli Basavaraju, Myles Mason, James A Burke, Vamse Kumar Subbiah"
84,"Fast Think-on-Graph: Wider, Deeper and Faster Reasoning of Large Language Model on Knowledge Graph","Graph Retrieval Augmented Generation (GRAG) is a novel paradigm that takes
the naive RAG system a step further by integrating graph information, such as
knowledge graph (KGs), into large-scale language models (LLMs) to mitigate
hallucination. However, existing GRAG still encounter limitations: 1) simple
paradigms usually fail with the complex problems due to the narrow and shallow
correlations capture from KGs 2) methods of strong coupling with KGs tend to be
high computation cost and time consuming if the graph is dense. In this paper,
we propose the Fast Think-on-Graph (FastToG), an innovative paradigm for
enabling LLMs to think ``community by community"" within KGs. To do this,
FastToG employs community detection for deeper correlation capture and two
stages community pruning - coarse and fine pruning for faster retrieval.
Furthermore, we also develop two Community-to-Text methods to convert the graph
structure of communities into textual form for better understanding by LLMs.
Experimental results demonstrate the effectiveness of FastToG, showcasing
higher accuracy, faster reasoning, and better explainability compared to the
previous works.",http://arxiv.org/abs/2501.14300v1,1/24/25,"Xujian Liang, Zhaoquan Gu"
85,CypherBench: Towards Precise Retrieval over Full-scale Modern Knowledge Graphs in the LLM Era,"Retrieval from graph data is crucial for augmenting large language models
(LLM) with both open-domain knowledge and private enterprise data, and it is
also a key component in the recent GraphRAG system (edge et al., 2024). Despite
decades of research on knowledge graphs and knowledge base question answering,
leading LLM frameworks (e.g. Langchain and LlamaIndex) have only minimal
support for retrieval from modern encyclopedic knowledge graphs like Wikidata.
In this paper, we analyze the root cause and suggest that modern RDF knowledge
graphs (e.g. Wikidata, Freebase) are less efficient for LLMs due to overly
large schemas that far exceed the typical LLM context window, use of resource
identifiers, overlapping relation types and lack of normalization. As a
solution, we propose property graph views on top of the underlying RDF graph
that can be efficiently queried by LLMs using Cypher. We instantiated this idea
on Wikidata and introduced CypherBench, the first benchmark with 11
large-scale, multi-domain property graphs with 7.8 million entities and over
10,000 questions. To achieve this, we tackled several key challenges, including
developing an RDF-to-property graph conversion engine, creating a systematic
pipeline for text-to-Cypher task generation, and designing new evaluation
metrics.",http://arxiv.org/abs/2412.18702v2,12/24/24,"Yanlin Feng, Simone Papicchio, Sajjadur Rahman"
86,KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large Language Models,"Large language models with retrieval-augmented generation encounter a pivotal
challenge in intricate retrieval tasks, e.g., multi-hop question answering,
which requires the model to navigate across multiple documents and generate
comprehensive responses based on fragmented information. To tackle this
challenge, we introduce a novel Knowledge Graph-based RAG framework with a
hierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing
in KG-Retriever is constructed on a hierarchical index graph that consists of a
knowledge graph layer and a collaborative document layer. The associative
nature of graph structures is fully utilized to strengthen intra-document and
inter-document connectivity, thereby fundamentally alleviating the information
fragmentation problem and meanwhile improving the retrieval efficiency in
cross-document retrieval of LLMs. With the coarse-grained collaborative
information from neighboring documents and concise information from the
knowledge graph, KG-Retriever achieves marked improvements on five public QA
datasets, showing the effectiveness and efficiency of our proposed RAG
framework.",http://arxiv.org/abs/2412.05547v2,12/7/24,"Weijie Chen, Ting Bai, Jinbo Su, Jian Luan, Wei Liu, Chuan Shi"
87,CommunityKG-RAG: Leveraging Community Structures in Knowledge Graphs for Advanced Retrieval-Augmented Generation in Fact-Checking,"Despite advancements in Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG) systems, their effectiveness is often hindered by a lack of
integration with entity relationships and community structures, limiting their
ability to provide contextually rich and accurate information retrieval for
fact-checking. We introduce CommunityKG-RAG (Community Knowledge
Graph-Retrieval Augmented Generation), a novel zero-shot framework that
integrates community structures within Knowledge Graphs (KGs) with RAG systems
to enhance the fact-checking process. Capable of adapting to new domains and
queries without additional training, CommunityKG-RAG utilizes the multi-hop
nature of community structures within KGs to significantly improve the accuracy
and relevance of information retrieval. Our experimental results demonstrate
that CommunityKG-RAG outperforms traditional methods, representing a
significant advancement in fact-checking by offering a robust, scalable, and
efficient solution.",http://arxiv.org/abs/2408.08535v1,8/16/24,"RongChing Chang, Jiawei Zhang"
88,Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning,"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. Current hybrid RAG system retrieves evidence
from both knowledge graphs (KGs) and text documents to support LLM reasoning.
However, it faces challenges like handling multi-hop reasoning, multi-entity
questions, multi-source verification, and effective graph utilization. To
address these limitations, we present Hydra, a training-free framework that
unifies graph topology, document semantics, and source reliability to support
deep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity
problems through agent-driven exploration that combines structured and
unstructured retrieval, increasing both diversity and precision of evidence. To
tackle multi-source verification, Hydra uses a tri-factor cross-source
verification (source trustworthiness assessment, cross-source corroboration,
and entity-path alignment), to balance topic relevance with cross-modal
agreement. By leveraging graph structure, Hydra fuses heterogeneous sources,
guides efficient exploration, and prunes noise early. Comprehensive experiments
on seven benchmark datasets show that Hydra achieves overall state-of-the-art
results on all benchmarks with GPT-3.5, outperforming the strong hybrid
baseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra
enables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance
comparable to that of GPT-4-Turbo.",http://arxiv.org/abs/2505.17464v1,5/23/25,"Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang"
89,Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for Improved Coverage and Efficiency,"Knowledge graphs (KGs) generated by large language models (LLMs) are becoming
increasingly valuable for Retrieval-Augmented Generation (RAG) applications
that require knowledge-intensive reasoning. However, existing KG extraction
methods predominantly rely on prompt-based approaches, which are inefficient
for processing large-scale corpora. These approaches often suffer from
information loss, particularly with long documents, due to the lack of
specialized design for KG construction. Additionally, there is a gap in
evaluation datasets and methodologies for ontology-free KG construction. To
overcome these limitations, we propose SynthKG, a multi-step, document-level
ontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM
on the synthesized document-KG pairs, we streamline the multi-step process into
a single-step KG generation approach called Distill-SynthKG, substantially
reducing the number of LLM inference calls. Furthermore, we re-purpose existing
question-answering datasets to establish KG evaluation datasets and introduce
new evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a
novel graph-based retrieval framework for RAG. Experimental results demonstrate
that Distill-SynthKG not only surpasses all baseline models in KG quality --
including models up to eight times larger -- but also consistently excels in
retrieval and question-answering tasks. Our proposed graph retrieval framework
also outperforms all KG-retrieval methods across multiple benchmark datasets.
We release the SynthKG dataset and Distill-SynthKG model publicly to support
further research and development.",http://arxiv.org/abs/2410.16597v1,10/22/24,"Prafulla Kumar Choubey, Xin Su, Man Luo, Xiangyu Peng, Caiming Xiong, Tiep Le, Shachar Rosenman, Vasudev Lal, Phil Mui, Ricky Ho, Phillip Howard, ChienSheng Wu"
90,Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval,"Large language models (LLMs) have demonstrated significant potential in
clinical decision support. Yet LLMs still suffer from hallucinations and lack
fine-grained contextual medical knowledge, limiting their high-stake healthcare
applications such as clinical diagnosis. Traditional retrieval-augmented
generation (RAG) methods attempt to address these limitations but frequently
retrieve sparse or irrelevant information, undermining prediction accuracy. We
introduce KARE, a novel framework that integrates knowledge graph (KG)
community-level retrieval with LLM reasoning to enhance healthcare predictions.
KARE constructs a comprehensive multi-source KG by integrating biomedical
databases, clinical literature, and LLM-generated insights, and organizes it
using hierarchical graph community detection and summarization for precise and
contextually relevant information retrieval. Our key innovations include: (1) a
dense medical knowledge structuring approach enabling accurate retrieval of
relevant information; (2) a dynamic knowledge retrieval mechanism that enriches
patient contexts with focused, multi-faceted medical insights; and (3) a
reasoning-enhanced prediction framework that leverages these enriched contexts
to produce both accurate and interpretable clinical predictions. Extensive
experiments demonstrate that KARE outperforms leading models by up to
10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and
readmission predictions. In addition to its impressive prediction accuracy, our
framework leverages the reasoning capabilities of LLMs, enhancing the
trustworthiness of clinical predictions.",http://arxiv.org/abs/2410.04585v2,10/6/24,"Pengcheng Jiang, Cao Xiao, Minhao Jiang, Parminder Bhatia, Taha KassHout, Jimeng Sun, Jiawei Han"
91,PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel PennyLane-Centric Dataset,"Large Language Models (LLMs) offer remarkable capabilities in code
generation, natural language processing, and domain-specific reasoning.
However, their application in quantum software development remains
underexplored, particularly for PennyLane-a leading framework for hybrid
quantum-classical computing. To address this gap, we introduce a novel,
high-quality dataset comprising 3,347 PennyLane-specific quantum code samples
and contextual descriptions, specifically curated to support LLM training and
fine-tuning for quantum code assistance. Our contributions are threefold: (1)
the automatic construction and open-source release of a comprehensive PennyLane
dataset derived from textbooks, official documentation, and open-source
repositories; (2) a structured methodology for data curation, annotation, and
formatting to enhance LLM usability and relevance; and (3) a rigorous
evaluation of code generation capabilities using both baseline
Retrieval-Augmented Generation (RAG) and a GraphRAG-enhanced pipeline. Using
the PennyLang framework, we demonstrate that GraphRAG, when applied to a GPT-4o
Mini model, substantially outperforms standard prompting and baseline RAG.
Accuracy improves from 20.5% (without RAG) to 58.2% with GraphRAG, showcasing
its effectiveness in reducing hallucinations and improving code correctness in
quantum programming tasks. Compared to prior efforts focused largely on Qiskit,
our work expands LLM-based assistance to the PennyLane ecosystem, contributing
practical tools and reproducible methodologies for advancing AI-assisted
quantum software development.",http://arxiv.org/abs/2503.02497v2,3/4/25,"Abdul Basit, Nouhaila Innan, Haider Asif, Minghao Shao, Muhammad Kashif, Alberto Marchisio, Muhammad Shafique"
92,Graph RAG for Legal Norms: A Hierarchical and Temporal Approach,"This article proposes an adaptation of Graph Retrieval Augmented Generation
(Graph RAG) specifically designed for the analysis and comprehension of legal
norms, which are characterized by their predefined hierarchical structure,
extensive network of internal and external references and multiple temporal
versions. By combining structured knowledge graphs with contextually enriched
text segments, Graph RAG offers a promising solution to address the inherent
complexity and vast volume of legal data. The integration of hierarchical
structure and temporal evolution into knowledge graphs - along with the concept
of comprehensive Text Units - facilitates the construction of richer,
interconnected representations of legal knowledge. Through a detailed analysis
of Graph RAG and its application to legal norm datasets, this article aims to
advance the field of Artificial Intelligence applied to Law, creating
opportunities for more effective systems in legal research, legislative
analysis, and decision support.",http://arxiv.org/abs/2505.00039v2,4/29/25,Hudson de Martim
93,"mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text, Tables, and Knowledge Graphs","Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing the capabilities of large language models. However, existing RAG
evaluation predominantly focuses on text retrieval and relies on opaque,
end-to-end assessments of generated outputs. To address these limitations, we
introduce mmRAG, a modular benchmark designed for evaluating multi-modal RAG
systems. Our benchmark integrates queries from six diverse question-answering
datasets spanning text, tables, and knowledge graphs, which we uniformly
convert into retrievable documents. To enable direct, granular evaluation of
individual RAG components -- such as the accuracy of retrieval and query
routing -- beyond end-to-end generation quality, we follow standard information
retrieval procedures to annotate document relevance and derive dataset
relevance. We establish baseline performance by evaluating a wide range of RAG
implementations on mmRAG.",http://arxiv.org/abs/2505.11180v1,5/16/25,"Chuan Xu, Qiaosheng Chen, Yutong Feng, Gong Cheng"
94,TOBUGraph: Knowledge Graph-Based Retrieval for Enhanced LLM Performance Beyond RAG,"Retrieval-Augmented Generation (RAG) is one of the leading and most widely
used techniques for enhancing LLM retrieval capabilities, but it still faces
significant limitations in commercial use cases. RAG primarily relies on the
query-chunk text-to-text similarity in the embedding space for retrieval and
can fail to capture deeper semantic relationships across chunks, is highly
sensitive to chunking strategies, and is prone to hallucinations. To address
these challenges, we propose TOBUGraph, a graph-based retrieval framework that
first constructs the knowledge graph from unstructured data dynamically and
automatically. Using LLMs, TOBUGraph extracts structured knowledge and diverse
relationships among data, going beyond RAG's text-to-text similarity. Retrieval
is achieved through graph traversal, leveraging the extracted relationships and
structures to enhance retrieval accuracy, eliminating the need for chunking
configurations while reducing hallucination. We demonstrate TOBUGraph's
effectiveness in TOBU, a real-world application in production for personal
memory organization and retrieval. Our evaluation using real user data
demonstrates that TOBUGraph outperforms multiple RAG implementations in both
precision and recall, significantly improving user experience through improved
retrieval accuracy.",http://arxiv.org/abs/2412.05447v2,12/6/24,"Savini Kashmira, Jayanaka L Dantanarayana, Joshua Brodsky, Ashish Mahendra, Yiping Kang, Krisztian Flautner, Lingjia Tang, Jason Mars"
95,PathRAG: Pruning Graph-based Retrieval Augmented Generation with Relational Paths,"Retrieval-augmented generation (RAG) improves the response quality of large
language models (LLMs) by retrieving knowledge from external databases. Typical
RAG approaches split the text database into chunks, organizing them in a flat
structure for efficient searches. To better capture the inherent dependencies
and structured relationships across the text database, researchers propose to
organize textual information into an indexing graph, known asgraph-based RAG.
However, we argue that the limitation of current graph-based RAG methods lies
in the redundancy of the retrieved information, rather than its insufficiency.
Moreover, previous methods use a flat structure to organize retrieved
information within the prompts, leading to suboptimal performance. To overcome
these limitations, we propose PathRAG, which retrieves key relational paths
from the indexing graph, and converts these paths into textual form for
prompting LLMs. Specifically, PathRAG effectively reduces redundant information
with flow-based pruning, while guiding LLMs to generate more logical and
coherent responses with path-based prompting. Experimental results show that
PathRAG consistently outperforms state-of-the-art baselines across six datasets
and five evaluation dimensions. The code is available at the following link:
https://github.com/BUPT-GAMMA/PathRAG",http://arxiv.org/abs/2502.14902v1,2/18/25,"Boyu Chen, Zirui Guo, Zidan Yang, Yuluo Chen, Junze Chen, Zhenghao Liu, Chuan Shi, Cheng Yang"
96,Biomedical Question Answering via Multi-Level Summarization on a Local Knowledge Graph,"In Question Answering (QA), Retrieval Augmented Generation (RAG) has
revolutionized performance in various domains. However, how to effectively
capture multi-document relationships, particularly critical for biomedical
tasks, remains an open question. In this work, we propose a novel method that
utilizes propositional claims to construct a local knowledge graph from
retrieved documents. Summaries are then derived via layerwise summarization
from the knowledge graph to contextualize a small language model to perform QA.
We achieved comparable or superior performance with our method over RAG
baselines on several biomedical QA benchmarks. We also evaluated each
individual step of our methodology over a targeted set of metrics,
demonstrating its effectiveness.",http://arxiv.org/abs/2504.01309v1,4/2/25,"Lingxiao Guan, Yuanhao Huang, Jie Liu"
97,Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research,"We introduce Agentic Reasoning, a framework that enhances large language
model (LLM) reasoning by integrating external tool-using agents. Unlike
conventional LLM-based reasoning approaches, which rely solely on internal
inference, Agentic Reasoning dynamically engages web search, code execution,
and structured reasoning-context memory to solve complex problems requiring
deep research and multi-step logical deduction. Our framework introduces the
Mind Map agent, which constructs a structured knowledge graph to track logical
relationships, improving deductive reasoning. Additionally, the integration of
web-search and coding agents enables real-time retrieval and computational
analysis, enhancing reasoning accuracy and decision-making. Evaluations on
PhD-level scientific reasoning (GPQA) and domain-specific deep research tasks
demonstrate that our approach significantly outperforms existing models,
including leading retrieval-augmented generation (RAG) systems and
closed-source LLMs. Moreover, our results indicate that agentic reasoning
improves expert-level knowledge synthesis, test-time scalability, and
structured problem-solving. The code is at:
https://github.com/theworldofagents/Agentic-Reasoning.",http://arxiv.org/abs/2502.04644v1,2/7/25,"Junde Wu, Jiayuan Zhu, Yuyuan Liu"
98,Leveraging LLM Agents for Automated Optimization Modeling for SASP Problems: A Graph-RAG based Approach,"Automated optimization modeling (AOM) has evoked considerable interest with
the rapid evolution of large language models (LLMs). Existing approaches
predominantly rely on prompt engineering, utilizing meticulously designed
expert response chains or structured guidance. However, prompt-based techniques
have failed to perform well in the sensor array signal processing (SASP) area
due the lack of specific domain knowledge. To address this issue, we propose an
automated modeling approach based on retrieval-augmented generation (RAG)
technique, which consists of two principal components: a multi-agent (MA)
structure and a graph-based RAG (Graph-RAG) process. The MA structure is
tailored for the architectural AOM process, with each agent being designed
based on principles of human modeling procedure. The Graph-RAG process serves
to match user query with specific SASP modeling knowledge, thereby enhancing
the modeling result. Results on ten classical signal processing problems
demonstrate that the proposed approach (termed as MAG-RAG) outperforms several
AOM benchmarks.",http://arxiv.org/abs/2501.18320v1,1/30/25,"Tianpeng Pan, Wenqiang Pu, Licheng Zhao, Rui Zhou"
99,A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning,"Retrieval-augmented generation (RAG) is a framework enabling large language
models (LLMs) to enhance their accuracy and reduce hallucinations by
integrating external knowledge bases. In this paper, we introduce a hybrid RAG
system enhanced through a comprehensive suite of optimizations that
significantly improve retrieval quality, augment reasoning capabilities, and
refine numerical computation ability. We refined the text chunks and tables in
web pages, added attribute predictors to reduce hallucinations, conducted LLM
Knowledge Extractor and Knowledge Graph Extractor, and finally built a
reasoning strategy with all the references. We evaluated our system on the CRAG
dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and
online evaluations demonstrate that our system significantly enhances complex
reasoning capabilities. In local evaluations, we have significantly improved
accuracy and reduced error rates compared to the baseline model, achieving a
notable increase in scores. In the meanwhile, we have attained outstanding
results in online assessments, demonstrating the performance and generalization
capabilities of the proposed system. The source code for our system is released
in \url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",http://arxiv.org/abs/2408.05141v3,8/9/24,"Ye Yuan, Chengwu Liu, Jingyang Yuan, Gongbo Sun, Siqi Li, Ming Zhang"
100,Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs,"Massive Open Online Courses (MOOCs) lack direct interaction between learners
and instructors, making it challenging for learners to understand new knowledge
concepts. Recently, learners have increasingly used Large Language Models
(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to
hallucinations which limits their reliability. Retrieval-Augmented Generation
(RAG) addresses this issue by retrieving relevant documents before generating a
response. However, the application of RAG across different MOOCs is limited by
unstructured learning material. Furthermore, current RAG systems do not
actively guide learners toward their learning needs. To address these
challenges, we propose a Graph RAG pipeline that leverages Educational
Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide
learners to understand knowledge concepts in the MOOC platform CourseMapper.
Specifically, we implement (1) a PKG-based Question Generation method to
recommend personalized questions for learners in context, and (2) an
EduKG-based Question Answering method that leverages the relationships between
knowledge concepts in the EduKG to answer learner selected questions. To
evaluate both methods, we conducted a study with 3 expert instructors on 3
different MOOCs in the MOOC platform CourseMapper. The results of the
evaluation show the potential of Graph RAG to empower learners to understand
new knowledge concepts in a personalized learning experience.",http://arxiv.org/abs/2505.10074v2,5/15/25,"Mohamed Abdelmagied, Mohamed Amine Chatti, Shoeb Joarder, Qurat Ul Ain, Rawaa Alatrash"
101,SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven Retrieval-Augmented Generation,"Recent advancements in large language models (LLMs) have shown impressive
versatility across various tasks. To eliminate its hallucinations,
retrieval-augmented generation (RAG) has emerged as a powerful approach,
leveraging external knowledge sources like knowledge graphs (KGs). In this
paper, we study the task of KG-driven RAG and propose a novel Similar Graph
Enhanced Retrieval-Augmented Generation (SimGRAG) method. It effectively
addresses the challenge of aligning query texts and KG structures through a
two-stage process: (1) query-to-pattern, which uses an LLM to transform queries
into a desired graph pattern, and (2) pattern-to-subgraph, which quantifies the
alignment between the pattern and candidate subgraphs using a graph semantic
distance (GSD) metric. We also develop an optimized retrieval algorithm that
efficiently identifies the top-$k$ subgraphs within 1-second latency on a
10-million-scale KG. Extensive experiments show that SimGRAG outperforms
state-of-the-art KG-driven RAG methods in both question answering and fact
verification, offering superior plug-and-play usability and scalability.",http://arxiv.org/abs/2412.15272v1,12/17/24,"Yuzheng Cai, Zhenyue Guo, Yiwen Pei, Wanrui Bian, Weiguo Zheng"
102,In-depth Analysis of Graph-based RAG in a Unified Framework,"Graph-based Retrieval-Augmented Generation (RAG) has proven effective in
integrating external knowledge into large language models (LLMs), improving
their factual accuracy, adaptability, interpretability, and trustworthiness. A
number of graph-based RAG methods have been proposed in the literature.
However, these methods have not been systematically and comprehensively
compared under the same experimental settings. In this paper, we first
summarize a unified framework to incorporate all graph-based RAG methods from a
high-level perspective. We then extensively compare representative graph-based
RAG methods over a range of questing-answering (QA) datasets -- from specific
questions to abstract questions -- and examine the effectiveness of all
methods, providing a thorough analysis of graph-based RAG approaches. As a
byproduct of our experimental analysis, we are also able to identify new
variants of the graph-based RAG methods over specific QA and abstract QA tasks
respectively, by combining existing techniques, which outperform the
state-of-the-art methods. Finally, based on these findings, we offer promising
research opportunities. We believe that a deeper understanding of the behavior
of existing methods can provide new valuable insights for future research.",http://arxiv.org/abs/2503.04338v1,3/6/25,"Yingli Zhou, Yaodong Su, Youran Sun, Shu Wang, Taotao Wang, Runyuan He, Yongwei Zhang, Sicong Liang, Xilin Liu, Yuchi Ma, Yixiang Fang"
103,Don't Forget to Connect! Improving RAG with Graph-based Reranking,"Retrieval Augmented Generation (RAG) has greatly improved the performance of
Large Language Model (LLM) responses by grounding generation with context from
existing documents. These systems work well when documents are clearly relevant
to a question context. But what about when a document has partial information,
or less obvious connections to the context? And how should we reason about
connections between documents? In this work, we seek to answer these two core
questions about RAG generation. We introduce G-RAG, a reranker based on graph
neural networks (GNNs) between the retriever and reader in RAG. Our method
combines both connections between documents and semantic information (via
Abstract Meaning Representation graphs) to provide a context-informed ranker
for RAG. G-RAG outperforms state-of-the-art approaches while having smaller
computational footprint. Additionally, we assess the performance of PaLM 2 as a
reranker and find it to significantly underperform G-RAG. This result
emphasizes the importance of reranking for RAG even when using Large Language
Models.",http://arxiv.org/abs/2405.18414v1,5/28/24,"Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F Yang, Anton Tsitsulin"
104,OntologyRAG: Better and Faster Biomedical Code Mapping with Retrieval-Augmented Generation (RAG) Leveraging Ontology Knowledge Graphs and Large Language Models,"Biomedical ontologies, which comprehensively define concepts and relations
for biomedical entities, are crucial for structuring and formalizing
domain-specific information representations. Biomedical code mapping identifies
similarity or equivalence between concepts from different ontologies. Obtaining
high-quality mapping usually relies on automatic generation of unrefined
mapping with ontology domain fine-tuned language models (LMs), followed by
manual selections or corrections by coding experts who have extensive domain
expertise and familiarity with ontology schemas. The LMs usually provide
unrefined code mapping suggestions as a list of candidates without reasoning or
supporting evidence, hence coding experts still need to verify each suggested
candidate against ontology sources to pick the best matches. This is also a
recurring task as ontology sources are updated regularly to incorporate new
research findings. Consequently, the need of regular LM retraining and manual
refinement make code mapping time-consuming and labour intensive. In this work,
we created OntologyRAG, an ontology-enhanced retrieval-augmented generation
(RAG) method that leverages the inductive biases from ontological knowledge
graphs for in-context-learning (ICL) in large language models (LLMs). Our
solution grounds LLMs to knowledge graphs with unrefined mappings between
ontologies and processes questions by generating an interpretable set of
results that include prediction rational with mapping proximity assessment. Our
solution doesn't require re-training LMs, as all ontology updates could be
reflected by updating the knowledge graphs with a standard process. Evaluation
results on a self-curated gold dataset show promises of using our method to
enable coding experts to achieve better and faster code mapping. The code is
available at https://github.com/iqvianlp/ontologyRAG.",http://arxiv.org/abs/2502.18992v1,2/26/25,"Hui Feng, Yuntzu Yin, Emiliano Reynares, Jay Nanavati"
105,G-RAG: Knowledge Expansion in Material Science,"In the field of Material Science, effective information retrieval systems are
essential for facilitating research. Traditional Retrieval-Augmented Generation
(RAG) approaches in Large Language Models (LLMs) often encounter challenges
such as outdated information, hallucinations, limited interpretability due to
context constraints, and inaccurate retrieval. To address these issues, Graph
RAG integrates graph databases to enhance the retrieval process. Our proposed
method processes Material Science documents by extracting key entities
(referred to as MatIDs) from sentences, which are then utilized to query
external Wikipedia knowledge bases (KBs) for additional relevant information.
We implement an agent-based parsing technique to achieve a more detailed
representation of the documents. Our improved version of Graph RAG called G-RAG
further leverages a graph database to capture relationships between these
entities, improving both retrieval accuracy and contextual understanding. This
enhanced approach demonstrates significant improvements in performance for
domains that require precise information retrieval, such as Material Science.",http://arxiv.org/abs/2411.14592v2,11/21/24,"Radeen Mostafa, Mirza Nihal Baig, Mashaekh Tausif Ehsan, Jakir Hasan"
106,Empowering Large Language Models to Set up a Knowledge Retrieval Indexer via Self-Learning,"Retrieval-Augmented Generation (RAG) offers a cost-effective approach to
injecting real-time knowledge into large language models (LLMs). Nevertheless,
constructing and validating high-quality knowledge repositories require
considerable effort. We propose a pre-retrieval framework named Pseudo-Graph
Retrieval-Augmented Generation (PG-RAG), which conceptualizes LLMs as students
by providing them with abundant raw reading materials and encouraging them to
engage in autonomous reading to record factual information in their own words.
The resulting concise, well-organized mental indices are interconnected through
common topics or complementary facts to form a pseudo-graph database. During
the retrieval phase, PG-RAG mimics the human behavior in flipping through
notes, identifying fact paths and subsequently exploring the related contexts.
Adhering to the principle of the path taken by many is the best, it integrates
highly corroborated fact paths to provide a structured and refined sub-graph
assisting LLMs. We validated PG-RAG on three specialized question-answering
datasets. In single-document tasks, PG-RAG significantly outperformed the
current best baseline, KGP-LLaMA, across all key evaluation metrics, with an
average overall performance improvement of 11.6%. Specifically, its BLEU score
increased by approximately 14.3%, and the QE-F1 metric improved by 23.7%. In
multi-document scenarios, the average metrics of PG-RAG were at least 2.35%
higher than the best baseline. Notably, the BLEU score and QE-F1 metric showed
stable improvements of around 7.55% and 12.75%, respectively. Our code:
https://github.com/IAAR-Shanghai/PGRAG.",http://arxiv.org/abs/2405.16933v1,5/27/24,"Xun Liang, Simin Niu, Zhiyu li, Sensen Zhang, Shichao Song, Hanyu Wang, Jiawei Yang, Feiyu Xiong, Bo Tang, Chenyang Xi"
107,SLIDE: Sliding Localized Information for Document Extraction,"Constructing accurate knowledge graphs from long texts and low-resource
languages is challenging, as large language models (LLMs) experience degraded
performance with longer input chunks. This problem is amplified in low-resource
settings where data scarcity hinders accurate entity and relationship
extraction. Contextual retrieval methods, while improving retrieval accuracy,
struggle with long documents. They truncate critical information in texts
exceeding maximum context lengths of LLMs, significantly limiting knowledge
graph construction. We introduce SLIDE (Sliding Localized Information for
Document Extraction), a chunking method that processes long documents by
generating local context through overlapping windows. SLIDE ensures that
essential contextual information is retained, enhancing knowledge graph
extraction from documents exceeding LLM context limits. It significantly
improves GraphRAG performance, achieving a 24% increase in entity extraction
and a 39% improvement in relationship extraction for English. For Afrikaans, a
low-resource language, SLIDE achieves a 49% increase in entity extraction and
an 82% improvement in relationship extraction. Furthermore, it improves upon
state-of-the-art in question-answering metrics such as comprehensiveness,
diversity and empowerment, demonstrating its effectiveness in multilingual and
resource-constrained settings.",http://arxiv.org/abs/2503.17952v1,3/23/25,"Divyansh Singh, Manuel Nunez Martinez, Bonnie J Dorr, Sonja Schmer Galunder"
108,DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud Systems,"Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
enhance the capabilities of language models by integrating external knowledge.
Due to the diversity of data sources and the constraints of memory and
computing resources, real-world data is often scattered in multiple devices.
Conventional RAGs that store massive amounts of scattered data centrally face
increasing privacy concerns and high computational costs. Additionally, RAG in
a central node raises latency issues when searching over a large-scale
knowledge base. To address these challenges, we propose a distributed Knowledge
Graph-based RAG approach, referred to as DGRAG, in an edge-cloud system, where
each edge device maintains a local knowledge base without the need to share it
with the cloud, instead sharing only summaries of its knowledge. Specifically,
DGRAG has two main phases. In the Distributed Knowledge Construction phase,
DGRAG organizes local knowledge using knowledge graphs, generating subgraph
summaries and storing them in a summary database in the cloud as information
sharing. In the Collaborative Retrieval and Generation phase, DGRAG first
performs knowledge retrieval and answer generation locally, and a gate
mechanism determines whether the query is beyond the scope of local knowledge
or processing capabilities. For queries that exceed the local knowledge scope,
the cloud retrieves knowledge from the most relevant edges based on the
summaries and generates a more precise answer. Experimental results demonstrate
the effectiveness of the proposed DGRAG approach in significantly improving the
quality of question-answering tasks over baseline approaches.",http://arxiv.org/abs/2505.19847v1,5/26/25,"Wenqing Zhou, Yuxuan Yan, Qianqian Yang"
109,GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding,"Although Large Language Models (LLMs) have demonstrated potential in
processing graphs, they struggle with comprehending graphical structure
information through prompts of graph description sequences, especially as the
graph size increases. We attribute this challenge to the uneven memory
performance of LLMs across different positions in graph description sequences,
known as ''positional biases''. To address this, we propose GraphInsight, a
novel framework aimed at improving LLMs' comprehension of both macro- and
micro-level graphical information. GraphInsight is grounded in two key
strategies: 1) placing critical graphical information in positions where LLMs
exhibit stronger memory performance, and 2) investigating a lightweight
external knowledge base for regions with weaker memory performance, inspired by
retrieval-augmented generation (RAG). Moreover, GraphInsight explores
integrating these two strategies into LLM agent processes for composite graph
tasks that require multi-step reasoning. Extensive empirical studies on
benchmarks with a wide range of evaluation tasks show that GraphInsight
significantly outperforms all other graph description methods (e.g., prompting
techniques and reordering strategies) in understanding graph structures of
varying sizes.",http://arxiv.org/abs/2409.03258v3,9/5/24,"Yukun Cao, Shuo Han, Zengyi Gao, Zezhong Ding, Xike Xie, S Kevin Zhou"
110,Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use,"This paper presents a domain-specific implementation of Retrieval-Augmented
Generation (RAG) tailored to the Fair Use Doctrine in U.S. copyright law.
Motivated by the increasing prevalence of DMCA takedowns and the lack of
accessible legal support for content creators, we propose a structured approach
that combines semantic search with legal knowledge graphs and court citation
networks to improve retrieval quality and reasoning reliability. Our prototype
models legal precedents at the statutory factor level (e.g., purpose, nature,
amount, market effect) and incorporates citation-weighted graph representations
to prioritize doctrinally authoritative sources. We use Chain-of-Thought
reasoning and interleaved retrieval steps to better emulate legal reasoning.
Preliminary testing suggests this method improves doctrinal relevance in the
retrieval process, laying groundwork for future evaluation and deployment of
LLM-based legal assistance tools.",http://arxiv.org/abs/2505.02164v1,5/4/25,"Justin Ho, Alexandra Colby, William Fisher"
111,Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods under Knowledge Incompleteness,"Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique
that enhances Large Language Model (LLM) inference in tasks like Question
Answering (QA) by retrieving relevant information from knowledge graphs (KGs).
However, real-world KGs are often incomplete, meaning that essential
information for answering questions may be missing. Existing benchmarks do not
adequately capture the impact of KG incompleteness on KG-RAG performance. In
this paper, we systematically evaluate KG-RAG methods under incomplete KGs by
removing triples using different methods and analyzing the resulting effects.
We demonstrate that KG-RAG methods are sensitive to KG incompleteness,
highlighting the need for more robust approaches in realistic settings.",http://arxiv.org/abs/2504.05163v1,4/7/25,"Dongzhuoran Zhou, Yuqicheng Zhu, Yuan He, Jiaoyan Chen, Evgeny Kharlamov, Steffen Staab"
112,KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation,"The recently developed retrieval-augmented generation (RAG) technology has
enabled the efficient construction of domain-specific applications. However, it
also has limitations, including the gap between vector similarity and the
relevance of knowledge reasoning, as well as insensitivity to knowledge logic,
such as numerical values, temporal relations, expert rules, and others, which
hinder the effectiveness of professional knowledge services. In this work, we
introduce a professional domain knowledge service framework called Knowledge
Augmented Generation (KAG). KAG is designed to address the aforementioned
challenges with the motivation of making full use of the advantages of
knowledge graph(KG) and vector retrieval, and to improve generation and
reasoning performance by bidirectionally enhancing large language models (LLMs)
and KGs through five key aspects: (1) LLM-friendly knowledge representation,
(2) mutual-indexing between knowledge graphs and original chunks, (3)
logical-form-guided hybrid reasoning engine, (4) knowledge alignment with
semantic reasoning, and (5) model capability enhancement for KAG. We compared
KAG with existing RAG methods in multihop question answering and found that it
significantly outperforms state-of-theart methods, achieving a relative
improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We
have successfully applied KAG to two professional knowledge Q&A tasks of Ant
Group, including E-Government Q&A and E-Health Q&A, achieving significant
improvement in professionalism compared to RAG methods.",http://arxiv.org/abs/2409.13731v3,9/10/24,"Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu, Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen, Wenguang Chen, Jun Zhou"
113,Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced Retrieval-Augmented Generation on Knowledge Graphs,"Despite the superior performance of Large language models on many NLP tasks,
they still face significant limitations in memorizing extensive world
knowledge. Recent studies have demonstrated that leveraging the
Retrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs
that encapsulate extensive factual data in a structured format, robustly
enhances the reasoning capabilities of LLMs. However, deploying such systems in
real-world scenarios presents challenges: the continuous evolution of
non-stationary environments may lead to performance degradation and user
satisfaction requires a careful balance of performance and responsiveness. To
address these challenges, we introduce a Multi-objective Multi-Armed Bandit
enhanced RAG framework, supported by multiple retrieval methods with diverse
capabilities under rich and evolving retrieval contexts in practice. Within
this framework, each retrieval method is treated as a distinct ``arm''. The
system utilizes real-time user feedback to adapt to dynamic environments, by
selecting the appropriate retrieval method based on input queries and the
historical multi-objective performance of each arm. Extensive experiments
conducted on two benchmark KGQA datasets demonstrate that our method
significantly outperforms baseline methods in non-stationary settings while
achieving state-of-the-art performance in stationary environments. Code and
data are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git",http://arxiv.org/abs/2412.07618v2,12/10/24,"Xiaqiang Tang, Jian Li, Nan Du, Sihong Xie"
114,Are Large Language Models In-Context Graph Learners?,"Large language models (LLMs) have demonstrated remarkable in-context
reasoning capabilities across a wide range of tasks, particularly with
unstructured inputs such as language or images. However, LLMs struggle to
handle structured data, such as graphs, due to their lack of understanding of
non-Euclidean structures. As a result, without additional fine-tuning, their
performance significantly lags behind that of graph neural networks (GNNs) in
graph learning tasks. In this paper, we show that learning on graph data can be
conceptualized as a retrieval-augmented generation (RAG) process, where
specific instances (e.g., nodes or edges) act as queries, and the graph itself
serves as the retrieved context. Building on this insight, we propose a series
of RAG frameworks to enhance the in-context learning capabilities of LLMs for
graph learning tasks. Comprehensive evaluations demonstrate that our proposed
RAG frameworks significantly improve LLM performance on graph-based tasks,
particularly in scenarios where a pretrained LLM must be used without
modification or accessed via an API.",http://arxiv.org/abs/2502.13562v1,2/19/25,"Jintang Li, Ruofan Wu, Yuchang Zhu, Huizhe Zhang, Liang Chen, Zibin Zheng"
115,PRAGyan -- Connecting the Dots in Tweets,"As social media platforms grow, understanding the underlying reasons behind
events and statements becomes crucial for businesses, policymakers, and
researchers. This research explores the integration of Knowledge Graphs (KGs)
with Large Language Models (LLMs) to perform causal analysis of tweets dataset.
The LLM aided analysis techniques often lack depth in uncovering the causes
driving observed effects. By leveraging KGs and LLMs, which encode rich
semantic relationships and temporal information, this study aims to uncover the
complex interplay of factors influencing causal dynamics and compare the
results obtained using GPT-3.5 Turbo. We employ a Retrieval-Augmented
Generation (RAG) model, utilizing a KG stored in a Neo4j (a.k.a PRAGyan) data
format, to retrieve relevant context for causal reasoning. Our approach
demonstrates that the KG-enhanced LLM RAG can provide improved results when
compared to the baseline LLM (GPT-3.5 Turbo) model as the source corpus
increases in size. Our qualitative analysis highlights the advantages of
combining KGs with LLMs for improved interpretability and actionable insights,
facilitating informed decision-making across various domains. Whereas,
quantitative analysis using metrics such as BLEU and cosine similarity show
that our approach outperforms the baseline by 10\%.",http://arxiv.org/abs/2407.13909v1,7/18/24,"Rahul Ravi, Gouri Ginde, Jon Rokne"
116,Learning to Retrieve and Reason on Knowledge Graph through Active Self-Reflection,"Extensive research has investigated the integration of large language models
(LLMs) with knowledge graphs to enhance the reasoning process. However,
understanding how models perform reasoning utilizing structured graph knowledge
remains underexplored. Most existing approaches rely on LLMs or retrievers to
make binary judgments regarding the utilization of knowledge, which is too
coarse. Meanwhile, there is still a lack of feedback mechanisms for reflection
and correction throughout the entire reasoning path. This paper proposes an
Active self-Reflection framework for knowledge Graph reasoning ARG, introducing
for the first time an end-to-end training approach to achieve iterative
reasoning grounded on structured graphs. Within the framework, the model
leverages special tokens to \textit{actively} determine whether knowledge
retrieval is necessary, performs \textit{reflective} critique based on the
retrieved knowledge, and iteratively reasons over the knowledge graph. The
reasoning paths generated by the model exhibit high interpretability, enabling
deeper exploration of the model's understanding of structured knowledge.
Ultimately, the proposed model achieves outstanding results compared to
existing baselines in knowledge graph reasoning tasks.",http://arxiv.org/abs/2502.14932v1,2/20/25,"Han Zhang, Langshi Zhou, Hanfang Yang"
117,A Tripartite Perspective on GraphRAG,"Large Language Models (LLMs) have shown remarkable capabilities across
various domains, yet they struggle with knowledge-intensive tasks in areas that
demand factual accuracy, e.g. industrial automation and healthcare. Key
limitations include their tendency to hallucinate, lack of source traceability
(provenance), and challenges in timely knowledge updates. Combining language
models with knowledge graphs (GraphRAG) offers promising avenues for overcoming
these deficits. However, a major challenge lies in creating such a knowledge
graph in the first place. Here, we propose a novel approach that combines LLMs
with a tripartite knowledge graph representation, which is constructed by
connecting complex, domain-specific objects via a curated ontology of
corresponding, domain-specific concepts to relevant sections within chunks of
text through a concept-anchored pre-analysis of source documents starting from
an initial lexical graph. As a consequence, our Tripartite-GraphRAG approach
implements: i) a concept-specific, information-preserving pre-compression of
textual chunks; ii) allows for the formation of a concept-specific relevance
estimation of embedding similarities grounded in statistics; and iii) avoids
common challenges w.r.t. continuous extendability, such as the need for entity
resolution and deduplication. By applying a transformation to the knowledge
graph, we formulate LLM prompt creation as an unsupervised node classification
problem, drawing on ideas from Markov Random Fields. We evaluate our approach
on a healthcare use case, involving multi-faceted analyses of patient anamneses
given a set of medical concepts as well as clinical literature. Experiments
indicate that it can optimize information density, coverage, and arrangement of
LLM prompts while reducing their lengths, which may lead to reduced costs and
more consistent and reliable LLM outputs.",http://arxiv.org/abs/2504.19667v1,4/28/25,"Michael Banf, Johannes Kuhn"
118,HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses,"In this paper, we investigate the retrieval-augmented generation (RAG) based
on Knowledge Graphs (KGs) to improve the accuracy and reliability of Large
Language Models (LLMs). Recent approaches suffer from insufficient and
repetitive knowledge retrieval, tedious and time-consuming query parsing, and
monotonous knowledge utilization. To this end, we develop a Hypothesis
Knowledge Graph Enhanced (HyKGE) framework, which leverages LLMs' powerful
reasoning capacity to compensate for the incompleteness of user queries,
optimizes the interaction process with LLMs, and provides diverse retrieved
knowledge. Specifically, HyKGE explores the zero-shot capability and the rich
knowledge of LLMs with Hypothesis Outputs to extend feasible exploration
directions in the KGs, as well as the carefully curated prompt to enhance the
density and efficiency of LLMs' responses. Furthermore, we introduce the HO
Fragment Granularity-aware Rerank Module to filter out noise while ensuring the
balance between diversity and relevance in retrieved knowledge. Experiments on
two Chinese medical multiple-choice question datasets and one Chinese
open-domain medical Q&A dataset with two LLM turbos demonstrate the superiority
of HyKGE in terms of accuracy and explainability.",http://arxiv.org/abs/2312.15883v2,12/26/23,"Xinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong Qiu, Yue Fang, Zhiyuan Wang, Jinyi Tang, Hongxin Ding, Xu Chu, Junfeng Zhao, Yasha Wang"
119,Retrieval-Augmented Generation with Hierarchical Knowledge,"Graph-based Retrieval-Augmented Generation (RAG) methods have significantly
enhanced the performance of large language models (LLMs) in domain-specific
tasks. However, existing RAG methods do not adequately utilize the naturally
inherent hierarchical knowledge in human cognition, which limits the
capabilities of RAG systems. In this paper, we introduce a new RAG approach,
called HiRAG, which utilizes hierarchical knowledge to enhance the semantic
understanding and structure capturing capabilities of RAG systems in the
indexing and retrieval processes. Our extensive experiments demonstrate that
HiRAG achieves significant performance improvements over the state-of-the-art
baseline methods. The code of our proposed method is available at
\href{https://github.com/hhy-huang/HiRAG}{https://github.com/hhy-huang/HiRAG}.",http://arxiv.org/abs/2503.10150v1,3/13/25,"Haoyu Huang, Yongfeng Huang, Junjie Yang, Zhenyu Pan, Yongqiang Chen, Kaili Ma, Hongzhi Chen, James Cheng"
120,GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation,"The ability to form, retrieve, and reason about memories in response to
stimuli serves as the cornerstone for general intelligence - shaping entities
capable of learning, adaptation, and intuitive insight. Large Language Models
(LLMs) have proven their ability, given the proper memories or context, to
reason and respond meaningfully to stimuli. However, they are still unable to
optimally encode, store, and retrieve memories - the ability to do this would
unlock their full ability to operate as AI agents, and to specialize to niche
domains. To remedy this, one promising area of research is Retrieval Augmented
Generation (RAG), which aims to augment LLMs by providing them with rich
in-context examples and information. In question-answering (QA) applications,
RAG methods embed the text of interest in chunks, and retrieve the most
relevant chunks for a prompt using text embeddings. Motivated by human memory
encoding and retrieval, we aim to improve over standard RAG methods by
generating and encoding higher-level information and tagging the chunks by
their utility to answer questions. We introduce Graphical Eigen Memories For
Retrieval Augmented Generation (GEM-RAG). GEM-RAG works by tagging each chunk
of text in a given text corpus with LLM generated ``utility'' questions,
connecting chunks in a graph based on the similarity of both their text and
utility questions, and then using the eigendecomposition of the memory graph to
build higher level summary nodes that capture the main themes of the text. We
evaluate GEM-RAG, using both UnifiedQA and GPT-3.5 Turbo as the LLMs, with
SBERT, and OpenAI's text encoders on two standard QA tasks, showing that
GEM-RAG outperforms other state-of-the-art RAG methods on these tasks. We also
discuss the implications of having a robust RAG system and future directions.",http://arxiv.org/abs/2409.15566v1,9/23/24,"Brendan Hogan Rappazzo, Yingheng Wang, Aaron Ferber, Carla Gomes"
121,Towards Evaluating Large Language Models for Graph Query Generation,"Large Language Models (LLMs) are revolutionizing the landscape of Generative
Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging
rapidly. However, when applied to database technologies, specifically query
generation for graph databases and Knowledge Graphs (KGs), LLMs still face
significant challenges. While research on LLM-driven query generation for
Structured Query Language (SQL) exists, similar systems for graph databases
remain underdeveloped. This paper presents a comparative study addressing the
challenge of generating Cypher queries a powerful language for interacting with
graph databases using open-access LLMs. We rigorously evaluate several LLM
agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a
locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and
Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)
reasoning. Our empirical analysis of query generation accuracy reveals that
Claude Sonnet 3.5 outperforms its counterparts in this specific domain.
Further, we highlight promising future research directions to address the
identified limitations and advance LLM-driven query generation for graph
databases.",http://arxiv.org/abs/2411.08449v2,11/13/24,"Siraj Munir, Alessandro Aldini"
122,MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries,"Retrieval-augmented generation (RAG) augments large language models (LLM) by
retrieving relevant knowledge, showing promising potential in mitigating LLM
hallucinations and enhancing response quality, thereby facilitating the great
adoption of LLMs in practice. However, we find that existing RAG systems are
inadequate in answering multi-hop queries, which require retrieving and
reasoning over multiple pieces of supporting evidence. Furthermore, to our
knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.
In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a
knowledge base, a large collection of multi-hop queries, their ground-truth
answers, and the associated supporting evidence. We detail the procedure of
building the dataset, utilizing an English news article dataset as the
underlying RAG knowledge base. We demonstrate the benchmarking utility of
MultiHop-RAG in two experiments. The first experiment compares different
embedding models for retrieving evidence for multi-hop queries. In the second
experiment, we examine the capabilities of various state-of-the-art LLMs,
including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop
queries given the evidence. Both experiments reveal that existing RAG methods
perform unsatisfactorily in retrieving and answering multi-hop queries. We hope
MultiHop-RAG will be a valuable resource for the community in developing
effective RAG systems, thereby facilitating greater adoption of LLMs in
practice. The MultiHop-RAG and implemented RAG system is publicly available at
https://github.com/yixuantt/MultiHop-RAG/.",http://arxiv.org/abs/2401.15391v1,1/27/24,"Yixuan Tang, Yi Yang"
123,Augmented Knowledge Graph Querying leveraging LLMs,"Adopting Knowledge Graphs (KGs) as a structured, semantic-oriented, data
representation model has significantly improved data integration, reasoning,
and querying capabilities across different domains. This is especially true in
modern scenarios such as Industry 5.0, in which the integration of data
produced by humans, smart devices, and production processes plays a crucial
role. However, the management, retrieval, and visualization of data from a KG
using formal query languages can be difficult for non-expert users due to their
technical complexity, thus limiting their usage inside industrial environments.
For this reason, we introduce SparqLLM, a framework that utilizes a
Retrieval-Augmented Generation (RAG) solution, to enhance the querying of
Knowledge Graphs (KGs). SparqLLM executes the Extract, Transform, and Load
(ETL) pipeline to construct KGs from raw data. It also features a natural
language interface powered by Large Language Models (LLMs) to enable automatic
SPARQL query generation. By integrating template-based methods as
retrieved-context for the LLM, SparqLLM enhances query reliability and reduces
semantic errors, ensuring more accurate and efficient KG interactions.
Moreover, to improve usability, the system incorporates a dynamic visualization
dashboard that adapts to the structure of the retrieved data, presenting the
query results in an intuitive format. Rigorous experimental evaluations
demonstrate that SparqLLM achieves high query accuracy, improved robustness,
and user-friendly interaction with KGs, establishing it as a scalable solution
to access semantic data.",http://arxiv.org/abs/2502.01298v1,2/3/25,"Marco Arazzi, Davide Ligari, Serena Nicolazzo, Antonino Nocera"
124,"Neural, Symbolic and Neural-Symbolic Reasoning on Knowledge Graphs","Knowledge graph reasoning is the fundamental component to support machine
learning applications such as information extraction, information retrieval,
and recommendation. Since knowledge graphs can be viewed as the discrete
symbolic representations of knowledge, reasoning on knowledge graphs can
naturally leverage the symbolic techniques. However, symbolic reasoning is
intolerant of the ambiguous and noisy data. On the contrary, the recent
advances of deep learning promote neural reasoning on knowledge graphs, which
is robust to the ambiguous and noisy data, but lacks interpretability compared
to symbolic reasoning. Considering the advantages and disadvantages of both
methodologies, recent efforts have been made on combining the two reasoning
methods. In this survey, we take a thorough look at the development of the
symbolic, neural and hybrid reasoning on knowledge graphs. We survey two
specific reasoning tasks, knowledge graph completion and question answering on
knowledge graphs, and explain them in a unified reasoning framework. We also
briefly discuss the future directions for knowledge graph reasoning.",http://arxiv.org/abs/2010.05446v5,10/12/20,"Jing Zhang, Bo Chen, Lingxi Zhang, Xirui Ke, Haipeng Ding"
125,Mindful-RAG: A Study of Points of Failure in Retrieval Augmented Generation,"Large Language Models (LLMs) are proficient at generating coherent and
contextually relevant text but face challenges when addressing
knowledge-intensive queries in domain-specific and factual question-answering
tasks. Retrieval-augmented generation (RAG) systems mitigate this by
incorporating external knowledge sources, such as structured knowledge graphs
(KGs). However, LLMs often struggle to produce accurate answers despite access
to KG-extracted information containing necessary facts. Our study investigates
this dilemma by analyzing error patterns in existing KG-based RAG methods and
identifying eight critical failure points. We observed that these errors
predominantly occur due to insufficient focus on discerning the question's
intent and adequately gathering relevant context from the knowledge graph
facts. Drawing on this analysis, we propose the Mindful-RAG approach, a
framework designed for intent-based and contextually aligned knowledge
retrieval. This method explicitly targets the identified failures and offers
improvements in the correctness and relevance of responses provided by LLMs,
representing a significant step forward from existing methods.",http://arxiv.org/abs/2407.12216v2,7/16/24,"Garima Agrawal, Tharindu Kumarage, Zeyad Alghamdi, Huan Liu"
126,Cyber Knowledge Completion Using Large Language Models,"The integration of the Internet of Things (IoT) into Cyber-Physical Systems
(CPSs) has expanded their cyber-attack surface, introducing new and
sophisticated threats with potential to exploit emerging vulnerabilities.
Assessing the risks of CPSs is increasingly difficult due to incomplete and
outdated cybersecurity knowledge. This highlights the urgent need for
better-informed risk assessments and mitigation strategies. While previous
efforts have relied on rule-based natural language processing (NLP) tools to
map vulnerabilities, weaknesses, and attack patterns, recent advancements in
Large Language Models (LLMs) present a unique opportunity to enhance
cyber-attack knowledge completion through improved reasoning, inference, and
summarization capabilities. We apply embedding models to encapsulate
information on attack patterns and adversarial techniques, generating mappings
between them using vector embeddings. Additionally, we propose a
Retrieval-Augmented Generation (RAG)-based approach that leverages pre-trained
models to create structured mappings between different taxonomies of threat
patterns. Further, we use a small hand-labeled dataset to compare the proposed
RAG-based approach to a baseline standard binary classification model. Thus,
the proposed approach provides a comprehensive framework to address the
challenge of cyber-attack knowledge graph completion.",http://arxiv.org/abs/2409.16176v1,9/24/24,"Braden K Webb, Sumit Purohit, Rounak Meyur"
127,Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge,"Large language models (LLMs) are transforming the way information is
retrieved with vast amounts of knowledge being summarized and presented via
natural language conversations. Yet, LLMs are prone to highlight the most
frequently seen pieces of information from the training set and to neglect the
rare ones. In the field of biomedical research, latest discoveries are key to
academic and industrial actors and are obscured by the abundance of an
ever-increasing literature corpus (the information overload problem). Surfacing
new associations between biomedical entities, e.g., drugs, genes, diseases,
with LLMs becomes a challenge of capturing the long-tail knowledge of the
biomedical scientific production. To overcome this challenge, Retrieval
Augmented Generation (RAG) has been proposed to alleviate some of the
shortcomings of LLMs by augmenting the prompts with context retrieved from
external datasets. RAG methods typically select the context via maximum
similarity search over text embeddings. In this study, we show that RAG methods
leave out a significant proportion of relevant information due to clusters of
over-represented concepts in the biomedical literature. We introduce a novel
information-retrieval method that leverages a knowledge graph to downsample
these clusters and mitigate the information overload problem. Its retrieval
performance is about twice better than embedding similarity alternatives on
both precision and recall. Finally, we demonstrate that both embedding
similarity and knowledge graph retrieval methods can be advantageously combined
into a hybrid model that outperforms both, enabling potential improvements to
biomedical question-answering models.",http://arxiv.org/abs/2402.12352v1,2/19/24,"Julien Delile, Srayanta Mukherjee, Anton Van Pamel, Leonid Zhukov"
128,CodeRAG: Supportive Code Retrieval on Bigraph for Real-World Code Generation,"Large language models (LLMs) have shown promising performance in automated
code generation, especially excelling in simple tasks such as generating
standalone codes. Different from simple tasks, real-world code generation
usually depends on specific programming environment (e.g., code repositories).
It contains complex dependencies and domain knowledge, which is needed for LLMs
when generating target code snippets. In this paper, we propose CodeRAG, a
retrieval-augmented code generation (RAG) framework to comprehensively retrieve
supportive codes for real-world code generation. Beginning with the
requirement, CodeRAG first constructs a requirement graph for the current
repository, and retrieves sub- and similar- requirement nodes of the target
requirement on the graph. Meanwhile, it models the repository into a DS-code
graph. CodeRAG then maps these relevant requirement nodes into their
corresponding code nodes, and treats these code nodes as archors for LLM
reasoning on DS-code graph. Finally, CodeRAG introduces a code-oriented agentic
reasoning process, seamlessly allowing LLMs to reason and comprehensively
retrieve for supportive codes which LLMs' need for generating correct programs.
Experiments show that CodeRAG achieves significant improvements (i.e.,
increasing 40.90 and 37.79 Pass@1 on GPT-4o and Gemini-Pro on DevEval) compared
to no RAG scenarios. Further tests on reasoning LLMs (i.e., QwQ-32B) confirm
CodeRAG's adaptability and efficacy across various types of LLMs. In addition,
CodeRAG outperforms commercial programming products such as Copilit and Cursor.
We further investigate the performance of our framework on different dependency
types, and observe that CodeRAG is superior in generating examples where target
codes invoke predefined cross-file code snippets. These results demonstrate
CodeRAG's potential in solving real-world repo-level coding challenges.",http://arxiv.org/abs/2504.10046v1,4/14/25,"Jia Li, Xianjie Shi, Kechi Zhang, Lei Li, Ge Li, Zhengwei Tao, Jia Li, Fang Liu, Chongyang Tao, Zhi Jin"
129,Context-Augmented Code Generation Using Programming Knowledge Graphs,"Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly
improved code generation, but, they frequently face difficulties when dealing
with challenging and complex problems. Retrieval-Augmented Generation (RAG)
addresses this issue by retrieving and integrating external knowledge at the
inference time. However, retrieval models often fail to find most relevant
context, and generation models, with limited context capacity, can hallucinate
when given irrelevant data. We present a novel framework that leverages a
Programming Knowledge Graph (PKG) to semantically represent and retrieve code.
This approach enables fine-grained code retrieval by focusing on the most
relevant segments while reducing irrelevant context through a tree-pruning
technique. PKG is coupled with a re-ranking mechanism to reduce even more
hallucinations by selectively integrating non-RAG solutions. We propose two
retrieval approaches-block-wise and function-wise-based on the PKG, optimizing
context granularity. Evaluations on the HumanEval and MBPP benchmarks show our
method improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art
models by up to 34% on MBPP. Our contributions include PKG-based retrieval,
tree pruning to enhance retrieval precision, a re-ranking method for robust
solution selection and a Fill-in-the-Middle (FIM) enhancer module for automatic
code augmentation with relevant comments and docstrings.",http://arxiv.org/abs/2410.18251v1,10/9/24,"Iman Saberi, Fatemeh Fard"
130,Path Pooling: Train-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation,"Although Large Language Models achieve strong success in many tasks, they
still suffer from hallucinations and knowledge deficiencies in real-world
applications. Many knowledge graph-based retrieval-augmented generation
(KG-RAG) methods enhance the quality and credibility of LLMs by leveraging
structure and semantic information in KGs as external knowledge bases. However,
these methods struggle to effectively incorporate structure information, either
incurring high computational costs or underutilizing available knowledge.
Inspired by smoothing operations in graph representation learning, we propose
path pooling, a simple, train-free strategy that introduces structure
information through a novel path-centric pooling operation. It seamlessly
integrates into existing KG-RAG methods in a plug-and-play manner, enabling
richer structure information utilization. Extensive experiments demonstrate
that incorporating the path pooling into the state-of-the-art KG-RAG method
consistently improves performance across various settings while introducing
negligible additional cost. Code is coming soon at
https://github.com/hrwang00/path-pooling.",http://arxiv.org/abs/2503.05203v1,3/7/25,"Hairu Wang, Yuan Feng, Xike Xie, S Kevin Zhou"
131,"RGL: A Graph-Centric, Modular Framework for Efficient Retrieval-Augmented Generation on Graphs","Recent advances in graph learning have paved the way for innovative
retrieval-augmented generation (RAG) systems that leverage the inherent
relational structures in graph data. However, many existing approaches suffer
from rigid, fixed settings and significant engineering overhead, limiting their
adaptability and scalability. Additionally, the RAG community has largely
overlooked the decades of research in the graph database community regarding
the efficient retrieval of interesting substructures on large-scale graphs. In
this work, we introduce the RAG-on-Graphs Library (RGL), a modular framework
that seamlessly integrates the complete RAG pipeline-from efficient graph
indexing and dynamic node retrieval to subgraph construction, tokenization, and
final generation-into a unified system. RGL addresses key challenges by
supporting a variety of graph formats and integrating optimized implementations
for essential components, achieving speedups of up to 143x compared to
conventional methods. Moreover, its flexible utilities, such as dynamic node
filtering, allow for rapid extraction of pertinent subgraphs while reducing
token consumption. Our extensive evaluations demonstrate that RGL not only
accelerates the prototyping process but also enhances the performance and
applicability of graph-based RAG systems across a range of tasks.",http://arxiv.org/abs/2503.19314v1,3/25/25,"Yuan Li, Jun Hu, Jiaxin Jiang, Zemin Liu, Bryan Hooi, Bingsheng He"
132,FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering,"Large Language Models (LLMs) are often challenged by generating erroneous or
hallucinated responses, especially in complex reasoning tasks. Leveraging
Knowledge Graphs (KGs) as external knowledge sources has emerged as a viable
solution. However, existing KG-enhanced methods, either retrieval-based or
agent-based, encounter difficulties in accurately retrieving knowledge and
efficiently traversing KGs at scale. In this paper, we propose a unified
framework, FiDeLiS, designed to improve the factuality of LLM responses by
anchoring answers to verifiable reasoning steps retrieved from KGs. To achieve
this, we leverage step-wise beam search with a deductive scoring function,
allowing the LLM to validate reasoning process step by step, and halt the
search once the question is deducible. In addition, we propose a Path-RAG
module to pre-select a smaller candidate set for each beam search step,
reducing computational costs by narrowing the search space. Extensive
experiments show that our method, as a training-free framework, not only
improve the performance but also enhance the factuality and interpretability
across different benchmarks. Code is released at
https://github.com/Y-Sui/FiDeLiS.",http://arxiv.org/abs/2405.13873v4,5/22/24,"Yuan Sui, Yufei He, Nian Liu, Xiaoxin He, Kun Wang, Bryan Hooi"
133,Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models,"Large language models (LLMs) have demonstrated impressive reasoning
abilities, but they still struggle with faithful reasoning due to knowledge
gaps and hallucinations. To address these issues, knowledge graphs (KGs) have
been utilized to enhance LLM reasoning through their structured knowledge.
However, existing KG-enhanced methods, either retrieval-based or agent-based,
encounter difficulties in accurately retrieving knowledge and efficiently
traversing KGs at scale. In this work, we introduce graph-constrained reasoning
(GCR), a novel framework that bridges structured knowledge in KGs with
unstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures
faithful KG-grounded reasoning by integrating KG structure into the LLM
decoding process through KG-Trie, a trie-based index that encodes KG reasoning
paths. KG-Trie constrains the decoding process, allowing LLMs to directly
reason on graphs and generate faithful reasoning paths grounded in KGs.
Additionally, GCR leverages a lightweight KG-specialized LLM for
graph-constrained reasoning alongside a powerful general LLM for inductive
reasoning over multiple reasoning paths, resulting in accurate reasoning with
zero reasoning hallucination. Extensive experiments on several KGQA benchmarks
demonstrate that GCR achieves state-of-the-art performance and exhibits strong
zero-shot generalizability to unseen KGs without additional training.",http://arxiv.org/abs/2410.13080v1,10/16/24,"Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, Shirui Pan"
134,Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG,"We introduce a novel approach to enhance the capabilities of text-to-image
models by incorporating a graph-based RAG. Our system dynamically retrieves
detailed character information and relational data from the knowledge graph,
enabling the generation of visually accurate and contextually rich images. This
capability significantly improves upon the limitations of existing T2I models,
which often struggle with the accurate depiction of complex or culturally
specific subjects due to dataset constraints. Furthermore, we propose a novel
self-correcting mechanism for text-to-image models to ensure consistency and
fidelity in visual outputs, leveraging the rich context from the graph to guide
corrections. Our qualitative and quantitative experiments demonstrate that
Context Canvas significantly enhances the capabilities of popular models such
as Flux, Stable Diffusion, and DALL-E, and improves the functionality of
ControlNet for fine-grained image editing tasks. To our knowledge, Context
Canvas represents the first application of graph-based RAG in enhancing T2I
models, representing a significant advancement for producing high-fidelity,
context-aware multi-faceted images.",http://arxiv.org/abs/2412.09614v1,12/12/24,"Kavana Venkatesh, Yusuf Dalva, Ismini Lourentzou, Pinar Yanardag"
135,ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding,"Understanding visual art requires reasoning across multiple perspectives --
cultural, historical, and stylistic -- beyond mere object recognition. While
recent multimodal large language models (MLLMs) perform well on general image
captioning, they often fail to capture the nuanced interpretations that fine
art demands. We propose ArtRAG, a novel, training-free framework that combines
structured knowledge with retrieval-augmented generation (RAG) for
multi-perspective artwork explanation. ArtRAG automatically constructs an Art
Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing
entities such as artists, movements, themes, and historical events into a rich,
interpretable graph. At inference time, a multi-granular structured retriever
selects semantically and topologically relevant subgraphs to guide generation.
This enables MLLMs to produce contextually grounded, culturally informed art
descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG
outperforms several heavily trained baselines. Human evaluations further
confirm that ArtRAG generates coherent, insightful, and culturally enriched
interpretations.",http://arxiv.org/abs/2505.06020v1,5/9/25,"Shuai Wang, Ivona Najdenkoska, Hongyi Zhu, Stevan Rudinac, Monika Kackovic, Nachoem Wijnberg, Marcel Worring"
136,A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting,"Recently, Large Language Models (LLMs) have demonstrated great potential in
various data mining tasks, such as knowledge question answering, mathematical
reasoning, and commonsense reasoning. However, the reasoning capability of LLMs
on temporal event forecasting has been under-explored. To systematically
investigate their abilities in temporal event forecasting, we conduct a
comprehensive evaluation of LLM-based methods for temporal event forecasting.
Due to the lack of a high-quality dataset that involves both graph and textual
data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on
this dataset, we design a series of baseline methods, characterized by various
input formats and retrieval augmented generation (RAG) modules. From extensive
experiments, we find that directly integrating raw texts into the input of LLMs
does not enhance zero-shot extrapolation performance. In contrast, fine-tuning
LLMs with raw texts can significantly improve performance. Additionally, LLMs
enhanced with retrieval modules can effectively capture temporal relational
patterns hidden in historical events. However, issues such as popularity bias
and the long-tail problem persist in LLMs, particularly in the
retrieval-augmented generation (RAG) method. These findings not only deepen our
understanding of LLM-based event forecasting methods but also highlight several
promising research directions. We consider that this comprehensive evaluation,
along with the identified research opportunities, will significantly contribute
to future research on temporal event forecasting through LLMs.",http://arxiv.org/abs/2407.11638v2,7/16/24,"He Chang, Chenchen Ye, Zhulin Tao, Jie Wu, Zhengmao Yang, Yunshan Ma, Xianglin Huang, TatSeng Chua"
137,CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering,"Recent studies have explored the use of Large Language Models (LLMs) with
Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering
(KGQA). They typically require rewriting retrieved subgraphs into natural
language formats comprehensible to LLMs. However, when tackling complex
questions, the knowledge rewritten by existing methods may include irrelevant
information, omit crucial details, or fail to align with the question's
semantics. To address them, we propose a novel rewriting method CoTKR,
Chain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces
and corresponding knowledge in an interleaved manner, thereby mitigating the
limitations of single-step knowledge rewriting. Additionally, to bridge the
preference gap between the knowledge rewriter and the question answering (QA)
model, we propose a training strategy PAQAF, Preference Alignment from Question
Answering Feedback, for leveraging feedback from the QA model to further
optimize the knowledge rewriter. We conduct experiments using various LLMs
across several KGQA benchmarks. Experimental results demonstrate that, compared
with previous knowledge rewriting methods, CoTKR generates the most beneficial
knowledge representation for QA models, which significantly improves the
performance of LLMs in KGQA.",http://arxiv.org/abs/2409.19753v3,9/29/24,"Yike Wu, Yi Huang, Nan Hu, Yuncheng Hua, Guilin Qi, Jiaoyan Chen, Jeff Z Pan"
138,ER-RAG: Enhance RAG with ER-Based Unified Modeling of Heterogeneous Data Sources,"Large language models (LLMs) excel in question-answering (QA) tasks, and
retrieval-augmented generation (RAG) enhances their precision by incorporating
external evidence from diverse sources like web pages, databases, and knowledge
graphs. However, current RAG methods rely on agent-specific strategies for
individual data sources, posing challenges low-resource or black-box
environments and complicates operations when evidence is fragmented across
sources. To address these limitations, we propose ER-RAG, a framework that
unifies evidence integration across heterogeneous data sources using the
Entity-Relationship (ER) model. ER-RAG standardizes entity retrieval and
relationship querying through ER-based APIs with GET and JOIN operations. It
employs a two-stage generation process: first, a preference optimization module
selects optimal sources; second, another module constructs API chains based on
source schemas. This unified approach allows efficient fine-tuning and seamless
integration across diverse data sources. ER-RAG demonstrated its effectiveness
by winning all three tracks of the 2024 KDDCup CRAG Challenge, achieving
performance on par with commercial RAG pipelines using an 8B LLM backbone. It
outperformed hybrid competitors by 3.1% in LLM score and accelerated retrieval
by 5.5X.",http://arxiv.org/abs/2504.06271v1,3/2/25,"Yikuan Xia, Jiazun Chen, Yirui Zhan, Suifeng Zhao, Weipeng Jiang, Chaorui Zhang, Wei Han, Bo Bai, Jun Gao"
139,Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing,"Retrieval Augmented Generation (RAG) has shown strong capability in enhancing
language models' knowledge and reducing AI generative hallucinations, driving
its widespread use. However, complex tasks requiring multi-round retrieval
remain challenging, and early attempts tend to be overly optimistic without a
good sense of self-skepticism. Current multi-round RAG systems may continue
searching even when enough information has already been retrieved, or they may
provide incorrect answers without having sufficient information or knowledge.
Existing solutions either require large amounts of expensive human-labeled
process supervision data or lead to subpar performance.
  This paper aims to address these limitations by introducing a new framework,
\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and
multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system
self-practice multi-round retrieval, augmenting existing question-answer pairs
with intermediate inner monologue reasoning steps to generate synthetic
training data. For each pair, the system may explore multiple retrieval paths,
which are labeled as successful if they reach the correct answer and
unsuccessful otherwise. Using this data, we train a lightweight information
sufficiency Critic. At inference time, the Critic evaluates whether the RAG
system has retrieved sufficient information at each round, guiding retrieval
decisions and improving system-level self-awareness through in-context
reinforcement learning.
  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an
effective multi-round RAG solution. Furthermore, this framework is
system-efficient, adding a lightweight component to RAG without requiring
modifications to existing LLMs or search engines, and data-efficient,
eliminating the need for costly human-annotated mid-step retrieval process
supervision data.",http://arxiv.org/abs/2505.02811v1,5/5/25,"Diji Yang, Linda Zeng, Jinmeng Rao, Yi Zhang"
140,MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation,"The growing demand for efficient and lightweight Retrieval-Augmented
Generation (RAG) systems has highlighted significant challenges when deploying
Small Language Models (SLMs) in existing RAG frameworks. Current approaches
face severe performance degradation due to SLMs' limited semantic understanding
and text processing capabilities, creating barriers for widespread adoption in
resource-constrained scenarios. To address these fundamental limitations, we
present MiniRAG, a novel RAG system designed for extreme simplicity and
efficiency. MiniRAG introduces two key technical innovations: (1) a
semantic-aware heterogeneous graph indexing mechanism that combines text chunks
and named entities in a unified structure, reducing reliance on complex
semantic understanding, and (2) a lightweight topology-enhanced retrieval
approach that leverages graph structures for efficient knowledge discovery
without requiring advanced language capabilities. Our extensive experiments
demonstrate that MiniRAG achieves comparable performance to LLM-based methods
even when using SLMs while requiring only 25\% of the storage space.
Additionally, we contribute a comprehensive benchmark dataset for evaluating
lightweight RAG systems under realistic on-device scenarios with complex
queries. We fully open-source our implementation and datasets at:
https://github.com/HKUDS/MiniRAG.",http://arxiv.org/abs/2501.06713v3,1/12/25,"Tianyu Fan, Jingyuan Wang, Xubin Ren, Chao Huang"
141,SRAG: Structured Retrieval-Augmented Generation for Multi-Entity Question Answering over Wikipedia Graph,"Multi-entity question answering (MEQA) poses significant challenges for large
language models (LLMs), which often struggle to consolidate scattered
information across multiple documents. An example question might be ""What is
the distribution of IEEE Fellows among various fields of study?"", which
requires retrieving information from diverse sources e.g., Wikipedia pages. The
effectiveness of current retrieval-augmented generation (RAG) methods is
limited by the LLMs' capacity to aggregate insights from numerous pages. To
address this gap, this paper introduces a structured RAG (SRAG) framework that
systematically organizes extracted entities into relational tables (e.g.,
tabulating entities with schema columns like ""name"" and ""field of study"") and
then apply table-based reasoning techniques. Our approach decouples retrieval
and reasoning, enabling LLMs to focus on structured data analysis rather than
raw text aggregation. Extensive experiments on Wikipedia-based multi-entity QA
tasks demonstrate that SRAG significantly outperforms state-of-the-art
long-context LLMs and RAG solutions, achieving a 29.6% improvement in accuracy.
The results underscore the efficacy of structuring unstructured data to enhance
LLMs' reasoning capabilities.",http://arxiv.org/abs/2503.01346v2,3/3/25,"Teng Lin, Yizhang Zhu, Yuyu Luo, Nan Tang"
142,MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search,"We introduce MCTS-RAG, a novel approach that enhances the reasoning
capabilities of small language models on knowledge-intensive tasks by
leveraging retrieval-augmented generation (RAG) to provide relevant context and
Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically
integrates retrieval and reasoning through an iterative decision-making
process. Unlike standard RAG methods, which typically retrieve information
independently from reasoning and thus integrate knowledge suboptimally, or
conventional MCTS reasoning, which depends solely on internal model knowledge
without external facts, MCTS-RAG combines structured reasoning with adaptive
retrieval. This integrated approach enhances decision-making, reduces
hallucinations, and ensures improved factual accuracy and response consistency.
The experimental results on multiple reasoning and knowledge-intensive datasets
datasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method
enables small-scale LMs to achieve performance comparable to frontier LLMs like
GPT-4o by effectively scaling inference-time compute, setting a new standard
for reasoning in small-scale models.",http://arxiv.org/abs/2503.20757v1,3/26/25,"Yunhai Hu, Yilun Zhao, Chen Zhao, Arman Cohan"
143,LightRAG: Simple and Fast Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) systems enhance large language models
(LLMs) by integrating external knowledge sources, enabling more accurate and
contextually relevant responses tailored to user needs. However, existing RAG
systems have significant limitations, including reliance on flat data
representations and inadequate contextual awareness, which can lead to
fragmented answers that fail to capture complex inter-dependencies. To address
these challenges, we propose LightRAG, which incorporates graph structures into
text indexing and retrieval processes. This innovative framework employs a
dual-level retrieval system that enhances comprehensive information retrieval
from both low-level and high-level knowledge discovery. Additionally, the
integration of graph structures with vector representations facilitates
efficient retrieval of related entities and their relationships, significantly
improving response times while maintaining contextual relevance. This
capability is further enhanced by an incremental update algorithm that ensures
the timely integration of new data, allowing the system to remain effective and
responsive in rapidly changing data environments. Extensive experimental
validation demonstrates considerable improvements in retrieval accuracy and
efficiency compared to existing approaches. We have made our LightRAG
open-source and available at the link: https://github.com/HKUDS/LightRAG",http://arxiv.org/abs/2410.05779v3,10/8/24,"Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, Chao Huang"
144,Knowledge Graph Retrieval-Augmented Generation for LLM-based Recommendation,"Recommender systems have become increasingly vital in our daily lives,
helping to alleviate the problem of information overload across various
user-oriented online services. The emergence of Large Language Models (LLMs)
has yielded remarkable achievements, demonstrating their potential for the
development of next-generation recommender systems. Despite these advancements,
LLM-based recommender systems face inherent limitations stemming from their LLM
backbones, particularly issues of hallucinations and the lack of up-to-date and
domain-specific knowledge. Recently, Retrieval-Augmented Generation (RAG) has
garnered significant attention for addressing these limitations by leveraging
external knowledge sources to enhance the understanding and generation of LLMs.
However, vanilla RAG methods often introduce noise and neglect structural
relationships in knowledge, limiting their effectiveness in LLM-based
recommendations. To address these limitations, we propose to retrieve
high-quality and up-to-date structure information from the knowledge graph (KG)
to augment recommendations. Specifically, our approach develops a
retrieval-augmented framework, termed K-RagRec, that facilitates the
recommendation generation process by incorporating structure information from
the external KG. Extensive experiments have been conducted to demonstrate the
effectiveness of our proposed method.",http://arxiv.org/abs/2501.02226v1,1/4/25,"Shijie Wang, Wenqi Fan, Yue Feng, Xinyu Ma, Shuaiqiang Wang, Dawei Yin"
145,Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA,"Medical question answering (QA) is a reasoning-intensive task that remains
challenging for large language models (LLMs) due to hallucinations and outdated
domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising
post-training solution by leveraging external knowledge. However, existing
medical RAG systems suffer from two key limitations: (1) a lack of modeling for
human-like reasoning behaviors during information retrieval, and (2) reliance
on suboptimal medical corpora, which often results in the retrieval of
irrelevant or noisy snippets. To overcome these challenges, we propose
Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG
system through collaborative agent-based reasoning. Our method introduces a
summarizer agent that orchestrates a team of medical experts to emulate
multi-turn brainstorming, thereby improving the relevance of retrieved content.
Additionally, a decision-making agent evaluates the retrieved snippets before
their final integration. Experimental results on four benchmark medical QA
datasets show that Discuss-RAG consistently outperforms MedRAG, especially
significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on
PubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.",http://arxiv.org/abs/2504.21252v1,4/30/25,"Xuanzhao Dong, Wenhui Zhu, Hao Wang, Xiwen Chen, Peijie Qiu, Rui Yin, Yi Su, Yalin Wang"
146,CG-RAG: Research Question Answering by Citation Graph Retrieval-Augmented LLMs,"Research question answering requires accurate retrieval and contextual
understanding of scientific literature. However, current Retrieval-Augmented
Generation (RAG) methods often struggle to balance complex document
relationships with precise information retrieval. In this paper, we introduce
Contextualized Graph Retrieval-Augmented Generation (CG-RAG), a novel framework
that integrates sparse and dense retrieval signals within graph structures to
enhance retrieval efficiency and subsequently improve generation quality for
research question answering. First, we propose a contextual graph
representation for citation graphs, effectively capturing both explicit and
implicit connections within and across documents. Next, we introduce
Lexical-Semantic Graph Retrieval (LeSeGR), which seamlessly integrates sparse
and dense retrieval signals with graph encoding. It bridges the gap between
lexical precision and semantic understanding in citation graph retrieval,
demonstrating generalizability to existing graph retrieval and hybrid retrieval
methods. Finally, we present a context-aware generation strategy that utilizes
the retrieved graph-structured information to generate precise and contextually
enriched responses using large language models (LLMs). Extensive experiments on
research question answering benchmarks across multiple domains demonstrate that
our CG-RAG framework significantly outperforms RAG methods combined with
various state-of-the-art retrieval approaches, delivering superior retrieval
accuracy and generation quality.",http://arxiv.org/abs/2501.15067v1,1/25/25,"Yuntong Hu, Zhihan Lei, Zhongjie Dai, Allen Zhang, Abhinav Angirekula, Zheng Zhang, Liang Zhao"
147,Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs,"Large language models (LLMs) perform well in medical QA, but their
effectiveness in Japanese contexts is limited due to privacy constraints that
prevent the use of commercial models like GPT-4 in clinical settings. As a
result, recent efforts focus on instruction-tuning open-source LLMs, though the
potential of combining them with retrieval-augmented generation (RAG) remains
underexplored. To bridge this gap, we are the first to explore a knowledge
graph-based (KG) RAG framework for Japanese medical QA small-scale open-source
LLMs. Experimental results show that KG-based RAG has only a limited impact on
Japanese medical QA using small-scale open-source LLMs. Further case studies
reveal that the effectiveness of the RAG is sensitive to the quality and
relevance of the external retrieved content. These findings offer valuable
insights into the challenges and potential of applying RAG in Japanese medical
QA, while also serving as a reference for other low-resource languages.",http://arxiv.org/abs/2504.10982v5,4/15/25,"Yingjian Chen, Feiyang Li, Xingyu Song, Tianxiao Li, Zixin Xu, Xiujie Chen, Issey Sukeda, Irene Li"
148,Graph-Augmented Reasoning: Evolving Step-by-Step Knowledge Graph Retrieval for LLM Reasoning,"Recent large language model (LLM) reasoning, despite its success, suffers
from limited domain knowledge, susceptibility to hallucinations, and
constrained reasoning depth, particularly in small-scale models deployed in
resource-constrained environments. This paper presents the first investigation
into integrating step-wise knowledge graph retrieval with step-wise reasoning
to address these challenges, introducing a novel paradigm termed as
graph-augmented reasoning. Our goal is to enable frozen, small-scale LLMs to
retrieve and process relevant mathematical knowledge in a step-wise manner,
enhancing their problem-solving abilities without additional training. To this
end, we propose KG-RAR, a framework centered on process-oriented knowledge
graph construction, a hierarchical retrieval strategy, and a universal
post-retrieval processing and reward model (PRP-RM) that refines retrieved
information and evaluates each reasoning step. Experiments on the Math500 and
GSM8K benchmarks across six models demonstrate that KG-RAR yields encouraging
results, achieving a 20.73\% relative improvement with Llama-3B on Math500.",http://arxiv.org/abs/2503.01642v1,3/3/25,"Wenjie Wu, Yongcheng Jing, Yingjie Wang, Wenbin Hu, Dacheng Tao"
149,VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context Videos,"Retrieval-Augmented Generation (RAG) has demonstrated remarkable success in
enhancing Large Language Models (LLMs) through external knowledge integration,
yet its application has primarily focused on textual content, leaving the rich
domain of multi-modal video knowledge predominantly unexplored. This paper
introduces VideoRAG, the first retrieval-augmented generation framework
specifically designed for processing and understanding extremely long-context
videos. Our core innovation lies in its dual-channel architecture that
seamlessly integrates (i) graph-based textual knowledge grounding for capturing
cross-video semantic relationships, and (ii) multi-modal context encoding for
efficiently preserving visual features. This novel design empowers VideoRAG to
process unlimited-length videos by constructing precise knowledge graphs that
span multiple videos while maintaining semantic dependencies through
specialized multi-modal retrieval paradigms. Through comprehensive empirical
evaluation on our proposed LongerVideos benchmark-comprising over 160 videos
totaling 134+ hours across lecture, documentary, and entertainment
categories-VideoRAG demonstrates substantial performance compared to existing
RAG alternatives and long video understanding methods. The source code of
VideoRAG implementation and the benchmark dataset are openly available at:
https://github.com/HKUDS/VideoRAG.",http://arxiv.org/abs/2502.01549v1,2/3/25,"Xubin Ren, Lingrui Xu, Long Xia, Shuaiqiang Wang, Dawei Yin, Chao Huang"
150,Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs,"Retrieval-augmented generation (RAG) enhances large language models (LLMs)
for domain-specific question-answering (QA) tasks by leveraging external
knowledge sources. However, traditional RAG systems primarily focus on
relevance-based retrieval and often struggle with redundancy, especially when
reasoning requires connecting information from multiple sources. This paper
introduces Vendi-RAG, a framework based on an iterative process that jointly
optimizes retrieval diversity and answer quality. This joint optimization leads
to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages
the Vendi Score (VS), a flexible similarity-based diversity metric, to promote
semantic diversity in document retrieval. It then uses an LLM judge that
evaluates candidate answers, generated after a reasoning step, and outputs a
score that the retriever uses to balance relevance and diversity among the
retrieved documents during each iteration. Experiments on three challenging
datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's
effectiveness in multi-hop reasoning tasks. The framework achieves significant
accuracy improvements over traditional single-step and multi-step RAG
approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on
2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current
best baseline. The benefits of Vendi-RAG are even more pronounced as the number
of retrieved documents increases. Finally, we evaluated Vendi-RAG across
different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and
observed consistent improvements, demonstrating that the framework's advantages
are model-agnostic.",http://arxiv.org/abs/2502.11228v2,2/16/25,"Mohammad Reza Rezaei, Adji Bousso Dieng"
151,Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models,"Iterative retrieval refers to the process in which the model continuously
queries the retriever during generation to enhance the relevance of the
retrieved knowledge, thereby improving the performance of Retrieval-Augmented
Generation (RAG). Existing work typically employs few-shot prompting or
manually constructed rules to implement iterative retrieval. This introduces
additional inference overhead and overlooks the remarkable reasoning
capabilities of Large Language Models (LLMs). In this paper, we introduce
Auto-RAG, an autonomous iterative retrieval model centered on the LLM's
powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues
with the retriever, systematically planning retrievals and refining queries to
acquire valuable knowledge. This process continues until sufficient external
information is gathered, at which point the results are presented to the user.
To this end, we develop a method for autonomously synthesizing reasoning-based
decision-making instructions in iterative retrieval and fine-tuned the latest
open-source LLMs. The experimental results indicate that Auto-RAG is capable of
autonomous iterative interaction with the retriever, effectively leveraging the
remarkable reasoning and decision-making abilities of LLMs, which lead to
outstanding performance across six benchmarks. Further analysis reveals that
Auto-RAG can autonomously adjust the number of iterations based on the
difficulty of the questions and the utility of the retrieved knowledge, without
requiring any human intervention. Moreover, Auto-RAG expresses the iterative
retrieval process in natural language, enhancing interpretability while
providing users with a more intuitive experience\footnote{Code is available at
\url{https://github.com/ictnlp/Auto-RAG}.",http://arxiv.org/abs/2411.19443v1,11/29/24,"Tian Yu, Shaolei Zhang, Yang Feng"
152,Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation,"Large language models (LLMs) have made significant progress in
general-purpose natural language processing tasks. However, LLMs are still
facing challenges when applied to domain-specific areas like
telecommunications, which demands specialized expertise and adaptability to
evolving standards. This paper presents a novel framework that combines
knowledge graph (KG) and retrieval-augmented generation (RAG) techniques to
enhance LLM performance in the telecom domain. The framework leverages a KG to
capture structured, domain-specific information about network protocols,
standards, and other telecom-related entities, comprehensively representing
their relationships. By integrating KG with RAG, LLMs can dynamically access
and utilize the most relevant and up-to-date knowledge during response
generation. This hybrid approach bridges the gap between structured knowledge
representation and the generative capabilities of LLMs, significantly enhancing
accuracy, adaptability, and domain-specific comprehension. Our results
demonstrate the effectiveness of the KG-RAG framework in addressing complex
technical queries with precision. The proposed KG-RAG model attained an
accuracy of 88% for question answering tasks on a frequently used
telecom-specific dataset, compared to 82% for the RAG-only and 48% for the
LLM-only approaches.",http://arxiv.org/abs/2503.24245v2,3/31/25,"Dun Yuan, Hao Zhou, Di Wu, Xue Liu, Hao Chen, Yan Xin, Jianzhong, Zhang"
153,How to Build an Adaptive AI Tutor for Any Course Using Knowledge Graph-Enhanced Retrieval-Augmented Generation (KG-RAG),"Integrating Large Language Models (LLMs) in Intelligent Tutoring Systems
(ITS) presents transformative opportunities for personalized education.
However, current implementations face two critical challenges: maintaining
factual accuracy and delivering coherent, context-aware instruction. While
Retrieval-Augmented Generation (RAG) partially addresses these issues, its
reliance on pure semantic similarity limits its effectiveness in educational
contexts where conceptual relationships are crucial. This paper introduces
Knowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG), a novel
framework that integrates structured knowledge representation with
context-aware retrieval to enable more effective AI tutoring. We present three
key contributions: (1) a novel architecture that grounds AI responses in
structured domain knowledge, (2) empirical validation through controlled
experiments (n=76) demonstrating significant learning improvements (35%
increase in assessment scores, p<0.001), and (3) a comprehensive implementation
framework addressing practical deployment considerations. These results
establish KG-RAG as a robust solution for developing adaptable AI tutoring
systems across diverse educational contexts.",http://arxiv.org/abs/2311.17696v7,11/29/23,"Chenxi Dong, Yimin Yuan, Kan Chen, Shupei Cheng, Chujie Wen"
154,DynaGRAG | Exploring the Topology of Information for Advancing Language Understanding and Generation in Graph Retrieval-Augmented Generation,"Graph Retrieval-Augmented Generation (GRAG or Graph RAG) architectures aim to
enhance language understanding and generation by leveraging external knowledge.
However, effectively capturing and integrating the rich semantic information
present in textual and structured data remains a challenge. To address this, a
novel GRAG framework, Dynamic Graph Retrieval-Agumented Generation (DynaGRAG),
is proposed to focus on enhancing subgraph representation and diversity within
the knowledge graph. By improving graph density, capturing entity and relation
information more effectively, and dynamically prioritizing relevant and diverse
subgraphs and information within them, the proposed approach enables a more
comprehensive understanding of the underlying semantic structure. This is
achieved through a combination of de-duplication processes, two-step mean
pooling of embeddings, query-aware retrieval considering unique nodes, and a
Dynamic Similarity-Aware BFS (DSA-BFS) traversal algorithm. Integrating Graph
Convolutional Networks (GCNs) and Large Language Models (LLMs) through hard
prompting further enhances the learning of rich node and edge representations
while preserving the hierarchical subgraph structure. Experimental results
demonstrate the effectiveness of DynaGRAG, showcasing the significance of
enhanced subgraph representation and diversity for improved language
understanding and generation.",http://arxiv.org/abs/2412.18644v3,12/24/24,Karishma Thakrar
155,RAG-based Explainable Prediction of Road Users Behaviors for Automated Driving using Knowledge Graphs and Large Language Models,"Prediction of road users' behaviors in the context of autonomous driving has
gained considerable attention by the scientific community in the last years.
Most works focus on predicting behaviors based on kinematic information alone,
a simplification of the reality since road users are humans, and as such they
are highly influenced by their surrounding context. In addition, a large
plethora of research works rely on powerful Deep Learning techniques, which
exhibit high performance metrics in prediction tasks but may lack the ability
to fully understand and exploit the contextual semantic information contained
in the road scene, not to mention their inability to provide explainable
predictions that can be understood by humans. In this work, we propose an
explainable road users' behavior prediction system that integrates the
reasoning abilities of Knowledge Graphs (KG) and the expressiveness
capabilities of Large Language Models (LLM) by using Retrieval Augmented
Generation (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE)
and Bayesian inference are combined to allow the deployment of a fully
inductive reasoning system that enables the issuing of predictions that rely on
legacy information contained in the graph as well as on current evidence
gathered in real time by onboard sensors. Two use cases have been implemented
following the proposed approach: 1) Prediction of pedestrians' crossing
actions; 2) Prediction of lane change maneuvers. In both cases, the performance
attained surpasses the current state of the art in terms of anticipation and
F1-score, showing a promising avenue for future research in this field.",http://arxiv.org/abs/2405.00449v1,5/1/24,"Mohamed Manzour Hussien, Angie Nataly Melo, Augusto Luis Ballardini, Carlota Salinas Maldonado, Rubn Izquierdo, Miguel ngel Sotelo"
156,From Documents to Dialogue: Building KG-RAG Enhanced AI Assistants,"The Adobe Experience Platform AI Assistant is a conversational tool that
enables organizations to interact seamlessly with proprietary enterprise data
through a chatbot. However, due to access restrictions, Large Language Models
(LLMs) cannot retrieve these internal documents, limiting their ability to
generate accurate zero-shot responses. To overcome this limitation, we use a
Retrieval-Augmented Generation (RAG) framework powered by a Knowledge Graph
(KG) to retrieve relevant information from external knowledge sources, enabling
LLMs to answer questions over private or previously unseen document
collections. In this paper, we propose a novel approach for building a
high-quality, low-noise KG. We apply several techniques, including incremental
entity resolution using seed concepts, similarity-based filtering to
deduplicate entries, assigning confidence scores to entity-relation pairs to
filter for high-confidence pairs, and linking facts to source documents for
provenance. Our KG-RAG system retrieves relevant tuples, which are added to the
user prompts context before being sent to the LLM generating the response. Our
evaluation demonstrates that this approach significantly enhances response
relevance, reducing irrelevant answers by over 50% and increasing fully
relevant answers by 88% compared to the existing production system.",http://arxiv.org/abs/2502.15237v1,2/21/25,"Manisha Mukherjee, Sungchul Kim, Xiang Chen, Dan Luo, Tong Yu, Tung Mai"
157,Towards Graph-hop Retrieval and Reasoning in Complex Question Answering over Textual Database,"In Textual question answering (TQA) systems, complex questions often require
retrieving multiple textual fact chains with multiple reasoning steps. While
existing benchmarks are limited to single-chain or single-hop retrieval
scenarios. In this paper, we propose to conduct Graph-Hop -- a novel
multi-chains and multi-hops retrieval and reasoning paradigm in complex
question answering. We construct a new benchmark called ReasonGraphQA, which
provides explicit and fine-grained evidence graphs for complex questions to
support interpretable reasoning, comprehensive and detailed reasoning. And
ReasonGraphQA also shows an advantage in reasoning diversity and scale.
Moreover, We propose a strong graph-hop baseline called Bidirectional Graph
Retrieval (BGR) method for generating an explanation graph of textual evidence
in knowledge reasoning and question answering. We have thoroughly evaluated
existing evidence retrieval and reasoning models on the ReasonGraphQA.
Experiments highlight Graph-Hop is a promising direction for answering complex
questions, but it still has certain limitations. We have further studied
mitigation strategies to meet these challenges and discuss future directions.",http://arxiv.org/abs/2305.14211v1,5/23/23,"Minjun Zhu, Yixuan Weng, Shizhu He, Kang Liu, Jun Zhao"
158,Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals,"Retrieval-augmented generation (RAG) has shown impressive capabilities in
mitigating hallucinations in large language models (LLMs). However, LLMs
struggle to handle misleading retrievals and often fail to maintain their own
reasoning when exposed to conflicting or selectively-framed evidence, making
them vulnerable to real-world misinformation. In such real-world retrieval
scenarios, misleading and conflicting information is rampant, particularly in
the political domain, where evidence is often selectively framed, incomplete,
or polarized. However, existing RAG benchmarks largely assume a clean retrieval
setting, where models succeed by accurately retrieving and generating answers
from gold-standard documents. This assumption fails to align with real-world
conditions, leading to an overestimation of RAG system performance. To bridge
this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate
the robustness of RAG systems against misleading retrievals. Unlike prior
benchmarks that rely on synthetic noise, our dataset constructs its retrieval
corpus from Reddit discussions, capturing naturally occurring misinformation.
It categorizes retrieved evidence into three types: supporting, misleading, and
irrelevant, providing a realistic and challenging testbed for assessing how
well RAG systems navigate different retrieval information. Our benchmark
experiments reveal that when exposed to misleading retrievals, all tested
LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no
retrieval at all), highlighting their susceptibility to noisy environments. To
the best of our knowledge, RAGuard is the first benchmark to systematically
assess RAG robustness against misleading evidence. We expect this benchmark
will drive future research toward improving RAG systems beyond idealized
datasets, making them more reliable for real-world applications.",http://arxiv.org/abs/2502.16101v2,2/22/25,"Linda Zeng, Rithwik Gupta, Divij Motwani, Diji Yang, Yi Zhang"
159,From RAG to Memory: Non-Parametric Continual Learning for Large Language Models,"Our ability to continuously acquire, organize, and leverage knowledge is a
key feature of human intelligence that AI systems must approximate to unlock
their full potential. Given the challenges in continual learning with large
language models (LLMs), retrieval-augmented generation (RAG) has become the
dominant way to introduce new information. However, its reliance on vector
retrieval hinders its ability to mimic the dynamic and interconnected nature of
human long-term memory. Recent RAG approaches augment vector embeddings with
various structures like knowledge graphs to address some of these gaps, namely
sense-making and associativity. However, their performance on more basic
factual memory tasks drops considerably below standard RAG. We address this
unintended deterioration and propose HippoRAG 2, a framework that outperforms
standard RAG comprehensively on factual, sense-making, and associative memory
tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in
HippoRAG and enhances it with deeper passage integration and more effective
online use of an LLM. This combination pushes this RAG system closer to the
effectiveness of human long-term memory, achieving a 7% improvement in
associative memory tasks over the state-of-the-art embedding model while also
exhibiting superior factual knowledge and sense-making memory capabilities.
This work paves the way for non-parametric continual learning for LLMs. Our
code and data will be released at https://github.com/OSU-NLP-Group/HippoRAG.",http://arxiv.org/abs/2502.14802v1,2/20/25,"Bernal Jimnez Gutirrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, Yu Su"
160,StreamingRAG: Real-time Contextual Retrieval and Generation Framework,"Extracting real-time insights from multi-modal data streams from various
domains such as healthcare, intelligent transportation, and satellite remote
sensing remains a challenge. High computational demands and limited knowledge
scope restrict the applicability of Multi-Modal Large Language Models (MM-LLMs)
on these data streams. Traditional Retrieval-Augmented Generation (RAG) systems
address knowledge limitations of these models, but suffer from slow
preprocessing, making them unsuitable for real-time analysis. We propose
StreamingRAG, a novel RAG framework designed for streaming data. StreamingRAG
constructs evolving knowledge graphs capturing scene-object-entity
relationships in real-time. The knowledge graph achieves temporal-aware scene
representations using MM-LLMs and enables timely responses for specific events
or user queries. StreamingRAG addresses limitations in existing methods,
achieving significant improvements in real-time analysis (5-6x faster
throughput), contextual accuracy (through a temporal knowledge graph), and
reduced resource consumption (using lightweight models by 2-3x).",http://arxiv.org/abs/2501.14101v1,1/23/25,"Murugan Sankaradas, Ravi K Rajendran, Srimat T Chakradhar"
161,Zep: A Temporal Knowledge Graph Architecture for Agent Memory,"We introduce Zep, a novel memory layer service for AI agents that outperforms
the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR)
benchmark. Additionally, Zep excels in more comprehensive and challenging
evaluations than DMR that better reflect real-world enterprise use cases. While
existing retrieval-augmented generation (RAG) frameworks for large language
model (LLM)-based agents are limited to static document retrieval, enterprise
applications demand dynamic knowledge integration from diverse sources
including ongoing conversations and business data. Zep addresses this
fundamental limitation through its core component Graphiti -- a
temporally-aware knowledge graph engine that dynamically synthesizes both
unstructured conversational data and structured business data while maintaining
historical relationships. In the DMR benchmark, which the MemGPT team
established as their primary evaluation metric, Zep demonstrates superior
performance (94.8% vs 93.4%). Beyond DMR, Zep's capabilities are further
validated through the more challenging LongMemEval benchmark, which better
reflects enterprise use cases through complex temporal reasoning tasks. In this
evaluation, Zep achieves substantial results with accuracy improvements of up
to 18.5% while simultaneously reducing response latency by 90% compared to
baseline implementations. These results are particularly pronounced in
enterprise-critical tasks such as cross-session information synthesis and
long-term context maintenance, demonstrating Zep's effectiveness for deployment
in real-world applications.",http://arxiv.org/abs/2501.13956v1,1/20/25,"Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, Daniel Chalef"
162,Winning Solution For Meta KDD Cup' 24,"This paper describes the winning solutions of all tasks in Meta KDD Cup 24
from db3 team. The challenge is to build a RAG system from web sources and
knowledge graphs. We are given multiple sources for each query to help us
answer the question. The CRAG challenge involves three tasks: (1) condensing
information from web pages into accurate answers, (2) integrating structured
data from mock knowledge graphs, and (3) selecting and integrating critical
data from extensive web pages and APIs to reflect real-world retrieval
challenges. Our solution for Task #1 is a framework of web or open-data
retrieval and answering. The large language model (LLM) is tuned for better RAG
performance and less hallucination. Task #2 and Task #3 solutions are based on
a regularized API set for domain questions and the API generation method using
tuned LLM. Our knowledge graph API interface extracts directly relevant
information to help LLMs answer correctly. Our solution achieves 1st place on
all three tasks, achieving a score of 28.4%, 42.7%, and 47.8%, respectively.",http://arxiv.org/abs/2410.00005v1,9/13/24,"Yikuan Xia, Jiazun Chen, Jun Gao"
163,Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs,"Retrieval-augmented generation (RAG) has revitalized Large Language Models
(LLMs) by injecting non-parametric factual knowledge. Compared with
long-context LLMs, RAG is considered an effective summarization tool in a more
concise and lightweight manner, which can interact with LLMs multiple times
using diverse queries to get comprehensive responses. However, the
LLM-generated historical responses, which contain potentially insightful
information, are largely neglected and discarded by existing approaches,
leading to suboptimal results. In this paper, we propose \textit{graph of
records} (\textbf{GoR}), which leverages historical responses generated by LLMs
to enhance RAG for long-context global summarization. Inspired by the
\textit{retrieve-then-generate} paradigm of RAG, we construct a graph by
establishing an edge between the retrieved text chunks and the corresponding
LLM-generated response. To further uncover the intricate correlations between
them, GoR further features a \textit{graph neural network} and an elaborately
designed \textit{BERTScore}-based objective for self-supervised model training,
enabling seamless supervision signal backpropagation between reference
summaries and node embeddings. We comprehensively compare GoR with 12 baselines
across four long-context summarization datasets, and the results indicate that
our proposed method reaches the best performance e.g., 15\%, 8\%, and 19\%
improvement over retrievers w.r.t. Rouge-L, Rouge-1, and Rouge-2 on the WCEP
dataset). Extensive experiments further demonstrate the effectiveness of GoR.
Code is available at https://github.com/ulab-uiuc/GoR",http://arxiv.org/abs/2410.11001v1,10/14/24,"Haozhen Zhang, Tao Feng, Jiaxuan You"
164,Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs,"Large language models (LLMs), while exhibiting exceptional performance,
suffer from hallucinations, especially on knowledge-intensive tasks. Existing
works propose to augment LLMs with individual text units retrieved from
external knowledge corpora to alleviate the issue. However, in many domains,
texts are interconnected (e.g., academic papers in a bibliographic graph are
linked by citations and co-authorships) which form a (text-attributed) graph.
The knowledge in such graphs is encoded not only in single texts/nodes but also
in their associated connections. To facilitate the research of augmenting LLMs
with graphs, we manually construct a Graph Reasoning Benchmark dataset called
GRBench, containing 1,740 questions that can be answered with the knowledge
from 10 domain graphs. Then, we propose a simple and effective framework called
Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging
LLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of
three sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We
conduct systematic experiments with three LLM backbones on GRBench, where
Graph-CoT outperforms the baselines consistently. The code is available at
https://github.com/PeterGriffinJin/Graph-CoT.",http://arxiv.org/abs/2404.07103v3,4/10/24,"Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, Jiawei Han"
165,RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement,"Existing large language models (LLMs) show exceptional problem-solving
capabilities but might struggle with complex reasoning tasks. Despite the
successes of chain-of-thought and tree-based search methods, they mainly depend
on the internal knowledge of LLMs to search over intermediate reasoning steps,
limited to dealing with simple tasks involving fewer reasoning steps. In this
paper, we propose \textbf{RAG-Star}, a novel RAG approach that integrates the
retrieved information to guide the tree-based deliberative reasoning process
that relies on the inherent knowledge of LLMs. By leveraging Monte Carlo Tree
Search, RAG-Star iteratively plans intermediate sub-queries and answers for
reasoning based on the LLM itself. To consolidate internal and external
knowledge, we propose an retrieval-augmented verification that utilizes query-
and answer-aware reward modeling to provide feedback for the inherent reasoning
of LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate
that RAG-Star significantly outperforms previous RAG and reasoning methods.",http://arxiv.org/abs/2412.12881v1,12/17/24,"Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, Tao Zhang"
166,"Graph-Based Re-ranking: Emerging Techniques, Limitations, and Opportunities","Knowledge graphs have emerged to be promising datastore candidates for
context augmentation during Retrieval Augmented Generation (RAG). As a result,
techniques in graph representation learning have been simultaneously explored
alongside principal neural information retrieval approaches, such as two-phased
retrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have
been proposed to demonstrate proficiency in graph learning for re-ranking,
there are ongoing limitations in modeling and evaluating input graph structures
for training and evaluation for passage and document ranking tasks. In this
survey, we review emerging GNN-based ranking model architectures along with
their corresponding graph representation construction methodologies. We
conclude by providing recommendations on future research based on
community-wide challenges and opportunities.",http://arxiv.org/abs/2503.14802v1,3/19/25,"Md Shahir Zaoad, Niamat Zawad, Priyanka Ranade, Richard Krogman, Latifur Khan, James Holt"
167,"Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization","Agentic Generative AI, powered by Large Language Models (LLMs) with
Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores
(VSs), represents a transformative technology applicable to specialized domains
such as legal systems, research, recommender systems, cybersecurity, and global
security, including proliferation research. This technology excels at inferring
relationships within vast unstructured or semi-structured datasets. The legal
domain here comprises complex data characterized by extensive, interrelated,
and semi-structured knowledge systems with complex relations. It comprises
constitutions, statutes, regulations, and case law. Extracting insights and
navigating the intricate networks of legal documents and their relations is
crucial for effective legal research. Here, we introduce a generative AI system
that integrates RAG, VS, and KG, constructed via Non-Negative Matrix
Factorization (NMF), to enhance legal information retrieval and AI reasoning
and minimize hallucinations. In the legal system, these technologies empower AI
agents to identify and analyze complex connections among cases, statutes, and
legal precedents, uncovering hidden relationships and predicting legal
trends-challenging tasks that are essential for ensuring justice and improving
operational efficiency. Our system employs web scraping techniques to
systematically collect legal texts, such as statutes, constitutional
provisions, and case law, from publicly accessible platforms like Justia. It
bridges the gap between traditional keyword-based searches and contextual
understanding by leveraging advanced semantic representations, hierarchical
relationships, and latent topic discovery. This framework supports legal
document clustering, summarization, and cross-referencing, for scalable,
interpretable, and accurate retrieval for semi-structured data while advancing
computational law and AI.",http://arxiv.org/abs/2502.20364v2,2/27/25,"Ryan C Barron, Maksim E Eren, Olga M Serafimova, Cynthia Matuszek, Boian S Alexandrov"
168,A Multi-Source Retrieval Question Answering Framework Based on RAG,"With the rapid development of large-scale language models,
Retrieval-Augmented Generation (RAG) has been widely adopted. However, existing
RAG paradigms are inevitably influenced by erroneous retrieval information,
thereby reducing the reliability and correctness of generated results.
Therefore, to improve the relevance of retrieval information, this study
proposes a method that replaces traditional retrievers with GPT-3.5, leveraging
its vast corpus knowledge to generate retrieval information. We also propose a
web retrieval based method to implement fine-grained knowledge retrieval,
Utilizing the powerful reasoning capability of GPT-3.5 to realize semantic
partitioning of problem.In order to mitigate the illusion of GPT retrieval and
reduce noise in Web retrieval,we proposes a multi-source retrieval framework,
named MSRAG, which combines GPT retrieval with web retrieval. Experiments on
multiple knowledge-intensive QA datasets demonstrate that the proposed
framework in this study performs better than existing RAG framework in
enhancing the overall efficiency and accuracy of QA systems.",http://arxiv.org/abs/2405.19207v1,5/29/24,"Ridong Wu, Shuhong Chen, Xiangbiao Su, Yuankai Zhu, Yifei Liao, Jianming Wu"
169,SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval,"Retrieval-Augmented Generation (RAG) systems have become pivotal in
leveraging vast corpora to generate informed and contextually relevant
responses, notably reducing hallucinations in Large Language Models. Despite
significant advancements, these systems struggle to efficiently process and
retrieve information from large datasets while maintaining a comprehensive
understanding of the context. This paper introduces SKETCH, a novel methodology
that enhances the RAG retrieval process by integrating semantic text retrieval
with knowledge graphs, thereby merging structured and unstructured data for a
more holistic comprehension. SKETCH, demonstrates substantial improvements in
retrieval performance and maintains superior context integrity compared to
traditional methods. Evaluated across four diverse datasets: QuALITY, QASPER,
NarrativeQA, and Italian Cuisine-SKETCH consistently outperforms baseline
approaches on key RAGAS metrics such as answer_relevancy, faithfulness,
context_precision and context_recall. Notably, on the Italian Cuisine dataset,
SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99,
representing the highest performance across all evaluated metrics. These
results highlight SKETCH's capability in delivering more accurate and
contextually relevant responses, setting new benchmarks for future retrieval
systems.",http://arxiv.org/abs/2412.15443v1,12/19/24,"Aakash Mahalingam, Vinesh Kumar Gande, Aman Chadha, Vinija Jain, Divya Chaudhary"
170,"Driving-RAG: Driving Scenarios Embedding, Search, and RAG Applications","Driving scenario data play an increasingly vital role in the development of
intelligent vehicles and autonomous driving. Accurate and efficient scenario
data search is critical for both online vehicle decision-making and planning,
and offline scenario generation and simulations, as it allows for leveraging
the scenario experiences to improve the overall performance. Especially with
the application of large language models (LLMs) and
Retrieval-Augmented-Generation (RAG) systems in autonomous driving, urgent
requirements are put forward. In this paper, we introduce the Driving-RAG
framework to address the challenges of efficient scenario data embedding,
search, and applications for RAG systems. Our embedding model aligns
fundamental scenario information and scenario distance metrics in the vector
space. The typical scenario sampling method combined with hierarchical
navigable small world can perform efficient scenario vector search to achieve
high efficiency without sacrificing accuracy. In addition, the reorganization
mechanism by graph knowledge enhances the relevance to the prompt scenarios and
augment LLM generation. We demonstrate the effectiveness of the proposed
framework on typical trajectory planning task for complex interactive scenarios
such as ramps and intersections, showcasing its advantages for RAG
applications.",http://arxiv.org/abs/2504.04419v1,4/6/25,"Cheng Chang, Jingwei Ge, Jiazhe Guo, Zelin Guo, Binghong Jiang, Li Li"
171,Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented Generation,"Integrating information from various reference databases is a major challenge
for Retrieval-Augmented Generation (RAG) systems because each knowledge source
adopts a unique data structure and follows different conventions. Retrieving
from multiple knowledge sources with one fixed strategy usually leads to
under-exploitation of information. To mitigate this drawback, inspired by
Mix-of-Expert, we introduce Mix-of-Granularity (MoG), a method that dynamically
determines the optimal granularity of a knowledge source based on input queries
using a router. The router is efficiently trained with a newly proposed loss
function employing soft labels. We further extend MoG to MoG-Graph (MoGG),
where reference documents are pre-processed as graphs, enabling the retrieval
of distantly situated snippets. Experiments demonstrate that MoG and MoGG
effectively predict optimal granularity levels, significantly enhancing the
performance of the RAG system in downstream tasks. The code of both MoG and
MoGG are released in https://github.com/ZGChung/Mix-of-Granularity.",http://arxiv.org/abs/2406.00456v2,6/1/24,"Zijie Zhong, Hanwen Liu, Xiaoya Cui, Xiaofan Zhang, Zengchang Qin"
172,Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering,"In customer service technical support, swiftly and accurately retrieving
relevant past issues is critical for efficiently resolving customer inquiries.
The conventional retrieval methods in retrieval-augmented generation (RAG) for
large language models (LLMs) treat a large corpus of past issue tracking
tickets as plain text, ignoring the crucial intra-issue structure and
inter-issue relations, which limits performance. We introduce a novel customer
service question-answering method that amalgamates RAG with a knowledge graph
(KG). Our method constructs a KG from historical issues for use in retrieval,
retaining the intra-issue structure and inter-issue relations. During the
question-answering phase, our method parses consumer queries and retrieves
related sub-graphs from the KG to generate answers. This integration of a KG
not only improves retrieval accuracy by preserving customer service structure
information but also enhances answering quality by mitigating the effects of
text segmentation. Empirical assessments on our benchmark datasets, utilizing
key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR)
metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by
0.32 in BLEU. Our method has been deployed within LinkedIn's customer service
team for approximately six months and has reduced the median per-issue
resolution time by 28.6%.",http://arxiv.org/abs/2404.17723v2,4/26/24,"Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, Zheng Li"
173,GraphAide: Advanced Graph-Assisted Query and Reasoning System,"Curating knowledge from multiple siloed sources that contain both structured
and unstructured data is a major challenge in many real-world applications.
Pattern matching and querying represent fundamental tasks in modern data
analytics that leverage this curated knowledge. The development of such
applications necessitates overcoming several research challenges, including
data extraction, named entity recognition, data modeling, and designing query
interfaces. Moreover, the explainability of these functionalities is critical
for their broader adoption.
  The emergence of Large Language Models (LLMs) has accelerated the development
lifecycle of new capabilities. Nonetheless, there is an ongoing need for
domain-specific tools tailored to user activities. The creation of digital
assistants has gained considerable traction in recent years, with LLMs offering
a promising avenue to develop such assistants utilizing domain-specific
knowledge and assumptions.
  In this context, we introduce an advanced query and reasoning system,
GraphAide, which constructs a knowledge graph (KG) from diverse sources and
allows to query and reason over the resulting KG. GraphAide harnesses both the
KG and LLMs to rapidly develop domain-specific digital assistants. It
integrates design patterns from retrieval augmented generation (RAG) and the
semantic web to create an agentic LLM application. GraphAide underscores the
potential for streamlined and efficient development of specialized digital
assistants, thereby enhancing their applicability across various domains.",http://arxiv.org/abs/2411.08041v1,10/29/24,"Sumit Purohit, George Chin, Patrick S Mackey, Joseph A Cottam"
174,"Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation","Recent Retrieval Augmented Generation (RAG) aims to enhance Large Language
Models (LLMs) by incorporating extensive knowledge retrieved from external
sources. However, such approach encounters some challenges: Firstly, the
original queries may not be suitable for precise retrieval, resulting in
erroneous contextual knowledge; Secondly, the language model can easily
generate inconsistent answer with external references due to their knowledge
boundary limitation. To address these issues, we propose the
chain-of-verification (CoV-RAG) to enhance the external retrieval correctness
and internal generation consistency. Specifically, we integrate the
verification module into the RAG, engaging in scoring, judgment, and rewriting.
To correct external retrieval errors, CoV-RAG retrieves new knowledge using a
revised query. To correct internal generation errors, we unify QA and
verification tasks with a Chain-of-Thought (CoT) reasoning during training. Our
comprehensive experiments across various LLMs demonstrate the effectiveness and
adaptability compared with other strong baselines. Especially, our CoV-RAG can
significantly surpass the state-of-the-art baselines using different LLM
backbones.",http://arxiv.org/abs/2410.05801v1,10/8/24,"Bolei He, Nuo Chen, Xinran He, Lingyong Yan, Zhenkai Wei, Jinchang Luo, ZhenHua Ling"
175,How Much Can RAG Help the Reasoning of LLM?,"Retrieval-Augmented Generation (RAG) has gained significant popularity in
modern Large Language Models (LLMs) due to its effectiveness in introducing new
knowledge and reducing hallucinations. However, the deep understanding of RAG
remains limited, how does RAG help the reasoning process and can RAG help
improve the reasoning capability remains question. While external documents are
typically considered as a method to incorporate domain-specific information,
they also contain intermediate reasoning results related to the query, this
suggests that documents could enhance the reasoning capability of LLMs, which
has not been previously explored. In this paper, we investigate this issue in
depth and find that while RAG can assist with reasoning, the help is limited.
If we conceptualize the reasoning process as a tree with fixed depth, then RAG
struggles to assist LLMs in performing deeper reasoning. Additionally, the
information in the documents requires preprocessing to filter out noise. We
demonstrate that this preprocessing is difficult to achieve simply fine-tuning
of the LLM, it often necessitates numerous additional transformer layers to
solve the problem. To simplify the problem, we propose DPrompt tuning, which
effectively resolves the issue within just limited transformer layers, leading
to improved performance.",http://arxiv.org/abs/2410.02338v2,10/3/24,"Jingyu Liu, Jiaen Lin, Yong Liu"
176,TC-RAG:Turing-Complete RAG's Case study on Medical LLM Systems,"In the pursuit of enhancing domain-specific Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) emerges as a promising solution to
mitigate issues such as hallucinations, outdated knowledge, and limited
expertise in highly specialized queries. However, existing approaches to RAG
fall short by neglecting system state variables, which are crucial for ensuring
adaptive control, retrieval halting, and system convergence. In this paper, we
introduce the TC-RAG through rigorous proof, a novel framework that addresses
these challenges by incorporating a Turing Complete System to manage state
variables, thereby enabling more efficient and accurate knowledge retrieval. By
leveraging a memory stack system with adaptive retrieval, reasoning, and
planning capabilities, TC-RAG not only ensures the controlled halting of
retrieval processes but also mitigates the accumulation of erroneous knowledge
via Push and Pop actions. In the case study of the medical domain, our
extensive experiments on real-world healthcare datasets demonstrate the
superiority of TC-RAG over existing methods in accuracy by over 7.20\%. Our
dataset and code have been available at
https://https://github.com/Artessay/SAMA.git.",http://arxiv.org/abs/2408.09199v1,8/17/24,"Xinke Jiang, Yue Fang, Rihong Qiu, Haoyu Zhang, Yongxin Xu, Hao Chen, Wentao Zhang, Ruizhe Zhang, Yuchen Fang, Xu Chu, Junfeng Zhao, Yasha Wang"
177,InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning,"Recent advancements in large language models (LLMs) have enabled their use as
agents for planning complex tasks. Existing methods typically rely on a
thought-action-observation (TAO) process to enhance LLM performance, but these
approaches are often constrained by the LLMs' limited knowledge of complex
tasks. Retrieval-augmented generation (RAG) offers new opportunities by
leveraging external databases to ground generation in retrieved information. In
this paper, we identify two key challenges (enlargability and transferability)
in applying RAG to task planning. We propose InstructRAG, a novel solution
within a multi-agent meta-reinforcement learning framework, to address these
challenges. InstructRAG includes a graph to organize past instruction paths
(sequences of correct actions), an RL-Agent with Reinforcement Learning to
expand graph coverage for enlargability, and an ML-Agent with Meta-Learning to
improve task generalization for transferability. The two agents are trained
end-to-end to optimize overall planning performance. Our experiments on four
widely used task planning datasets demonstrate that InstructRAG significantly
enhances performance and adapts efficiently to new tasks, achieving up to a
19.2% improvement over the best existing approach.",http://arxiv.org/abs/2504.13032v1,4/17/25,"Zheng Wang, Shu Xian Teo, Jun Jie Chew, Wei Shi"
178,Improving Multilingual Retrieval-Augmented Language Models through Dialectic Reasoning Argumentations,"Retrieval-augmented generation (RAG) is key to enhancing large language
models (LLMs) to systematically access richer factual knowledge. Yet, using RAG
brings intrinsic challenges, as LLMs must deal with potentially conflicting
knowledge, especially in multilingual retrieval, where the heterogeneity of
knowledge retrieved may deliver different outlooks. To make RAG more
analytical, critical and grounded, we introduce Dialectic-RAG (DRAG), a modular
approach guided by Argumentative Explanations, i.e., structured reasoning
process that systematically evaluates retrieved
  information by comparing, contrasting, and resolving conflicting
perspectives. Given a query and a set of multilingual related documents, DRAG
selects and exemplifies relevant knowledge for delivering dialectic
explanations that, by critically weighing opposing arguments and filtering
extraneous content, clearly determine the final response. Through a series of
in-depth experiments, we show the impact of our framework both as an in-context
learning strategy and for constructing demonstrations to instruct smaller
models. The final results demonstrate that DRAG significantly improves RAG
approaches, requiring low-impact computational effort and providing robustness
to knowledge perturbations.",http://arxiv.org/abs/2504.04771v1,4/7/25,"Leonardo Ranaldi, Federico Ranaldi, Fabio Massimo Zanzotto, Barry Haddow, Alexandra Birch"
179,LLM-based SPARQL Query Generation from Natural Language over Federated Knowledge Graphs,"We introduce a Retrieval-Augmented Generation (RAG) system for translating
user questions into accurate federated SPARQL queries over bioinformatics
knowledge graphs (KGs) leveraging Large Language Models (LLMs). To enhance
accuracy and reduce hallucinations in query generation, our system utilises
metadata from the KGs, including query examples and schema information, and
incorporates a validation step to correct generated queries. The system is
available online at chat.expasy.org.",http://arxiv.org/abs/2410.06062v4,10/8/24,"Vincent Emonet, Jerven Bolleman, Severine Duvaud, Tarcisio Mendes de Farias, Ana Claudia Sima"
180,Developing an Artificial Intelligence Tool for Personalized Breast Cancer Treatment Plans based on the NCCN Guidelines,"Cancer treatments require personalized approaches based on a patient's
clinical condition, medical history, and evidence-based guidelines. The
National Comprehensive Cancer Network (NCCN) provides frequently updated,
complex guidelines through visuals like flowcharts and diagrams, which can be
time consuming for oncologists to stay current with treatment protocols. This
study presents an AI (Artificial Intelligence)-driven methodology to accurately
automate treatment regimens following NCCN guidelines for breast cancer
patients.
  We proposed two AI-driven methods: Agentic-RAG (Retrieval-Augmented
Generation) and Graph-RAG. Agentic-RAG used a three-step Large Language Model
(LLM) process to select clinical titles from NCCN guidelines, retrieve matching
JSON content, and iteratively refine recommendations based on insufficiency
checks. Graph-RAG followed a Microsoft-developed framework with proprietary
prompts, where JSON data was converted to text via an LLM, summarized, and
mapped into graph structures representing key treatment relationships. Final
recommendations were generated by querying relevant graph summaries. Both were
evaluated using a set of patient descriptions, each with four associated
questions.
  As shown in Table 1, Agentic RAG achieved a 100% adherence (24/24) with no
hallucinations or incorrect treatments. Graph-RAG had 95.8% adherence (23/24)
with one incorrect treatment and no hallucinations. Chat GPT-4 showed 91.6%
adherence (22/24) with two wrong treatments and no hallucinations. Both Agentic
RAG and Graph-RAG provided detailed treatment recommendations with accurate
references to relevant NCCN document page numbers.",http://arxiv.org/abs/2502.15698v1,1/6/25,"Abdul M Mohammed, Iqtidar Mansoor, Sarah Blythe, Dennis Trujillo"
181,Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based Automated Requirement Traceability and Compliance Checks,"Ensuring that Software Requirements Specifications (SRS) align with
higher-level organizational or national requirements is vital, particularly in
regulated environments such as finance and aerospace. In these domains,
maintaining consistency, adhering to regulatory frameworks, minimizing errors,
and meeting critical expectations are essential for the reliable functioning of
systems. The widespread adoption of large language models (LLMs) highlights
their immense potential, yet there remains considerable scope for improvement
in retrieving relevant information and enhancing reasoning capabilities. This
study demonstrates that integrating a robust Graph-RAG framework with advanced
prompt engineering techniques, such as Chain of Thought and Tree of Thought,
can significantly enhance performance. Compared to baseline RAG methods and
simple prompting strategies, this approach delivers more accurate and
context-aware results. While this method demonstrates significant improvements
in performance, it comes with challenges. It is both costly and more complex to
implement across diverse contexts, requiring careful adaptation to specific
scenarios. Additionally, its effectiveness heavily relies on having complete
and accurate input data, which may not always be readily available, posing
further limitations to its scalability and practicality.",http://arxiv.org/abs/2412.08593v1,12/11/24,"Arsalan Masoudifard, Mohammad Mowlavi Sorond, Moein Madadi, Mohammad Sabokrou, Elahe Habibi"
182,ConceptFormer: Towards Efficient Use of Knowledge-Graph Embeddings in Large Language Models,"Retrieval Augmented Generation (RAG) has enjoyed increased attention in the
recent past and recent advancements in Large Language Models (LLMs) have
highlighted the importance of integrating world knowledge into these systems.
Current RAG methodologies often modify the internal architecture of pre-trained
language models (PLMs) or rely on textifying knowledge graphs (KGs), which is
inefficient in terms of token usage. This paper introduces ConceptFormer, a new
approach to augment LLMs with structured knowledge from KGs, such as Wikidata,
without altering their internal structure or relying on textual input of KGs.
ConceptFormer operates in the LLM embedding vector space, creating and
injecting \emph{concept vectors} that encapsulate the information of the KG
nodes directly. Trained in conjunction with a frozen LLM, ConceptFormer
generates a comprehensive lookup table that maps KG nodes to their respective
concept vectors. The approach aims to enhance the factual recall capabilities
of LLMs by enabling them to process these concept vectors natively, thus
enriching them with structured world knowledge in an efficient and scalable
manner. Our experiments demonstrate that the addition of concept vectors to
GPT-2 0.1B substantially increases its factual recall ability (Hit@10) by up to
272\% when tested on sentences from Wikipedia and up to 348\% on synthetically
generated sentences. Even injecting only a single concept vector into the
prompt increases factual recall ability (Hit@10) by up to 213\% on Wikipedia
sentences, significantly outperforming RAG with graph textification while
consuming 130x fewer input tokens.",http://arxiv.org/abs/2504.07624v1,4/10/25,"Joel Barmettler, Abraham Bernstein, Luca Rossetto"
183,Talking to GDELT Through Knowledge Graphs,"In this work we study various Retrieval Augmented Regeneration (RAG)
approaches to gain an understanding of the strengths and weaknesses of each
approach in a question-answering analysis. To gain this understanding we use a
case-study subset of the Global Database of Events, Language, and Tone (GDELT)
dataset as well as a corpus of raw text scraped from the online news articles.
To retrieve information from the text corpus we implement a traditional vector
store RAG as well as state-of-the-art large language model (LLM) based
approaches for automatically constructing KGs and retrieving the relevant
subgraphs. In addition to these corpus approaches, we develop a novel
ontology-based framework for constructing knowledge graphs (KGs) from GDELT
directly which leverages the underlying schema of GDELT to create structured
representations of global events. For retrieving relevant information from the
ontology-based KGs we implement both direct graph queries and state-of-the-art
graph retrieval approaches. We compare the performance of each method in a
question-answering task. We find that while our ontology-based KGs are valuable
for question-answering, automated extraction of the relevant subgraphs is
challenging. Conversely, LLM-generated KGs, while capturing event summaries,
often lack consistency and interpretability. Our findings suggest benefits of a
synergistic approach between ontology and LLM-based KG construction, with
proposed avenues toward that end.",http://arxiv.org/abs/2503.07584v1,3/10/25,"Audun Myers, Max Vargas, Sinan G Aksoy, Cliff Joslyn, Benjamin Wilson, Tom Grimes"
184,Injecting Knowledge Graphs into Large Language Models,"Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) remains a key challenge for symbolic reasoning. Existing
methods mainly rely on prompt engineering or fine-tuning, which lose structural
fidelity or incur high computational costs. Building on recent encoding
techniques which integrate graph embeddings within the LLM input as tokens, we
extend this paradigm to the KG domain by leveraging Knowledge Graph Embedding
(KGE) models, thus enabling graph-aware reasoning. Our approach is
model-agnostic, resource-efficient, and compatible with any LLMs. Extensive
experimentation on synthetic and real-world datasets shows that our method
improves reasoning performance over established baselines, further achieving
the best trade-off in terms of accuracy and efficiency against state-of-the-art
LLMs.",http://arxiv.org/abs/2505.07554v1,5/12/25,Erica Coppolillo
185,RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF for Conversational QA over KGs with RAG,"Conversational question answering (ConvQA) is a convenient means of searching
over RDF knowledge graphs (KGs), where a prevalent approach is to translate
natural language questions to SPARQL queries. However, SPARQL has certain
shortcomings: (i) it is brittle for complex intents and conversational
questions, and (ii) it is not suitable for more abstract needs. Instead, we
propose a novel two-pronged system where we fuse: (i) SQL-query results over a
database automatically derived from the KG, and (ii) text-search results over
verbalizations of KG facts. Our pipeline supports iterative retrieval: when the
results of any branch are found to be unsatisfactory, the system can
automatically opt for further rounds. We put everything together in a retrieval
augmented generation (RAG) setup, where an LLM generates a coherent response
from accumulated search results. We demonstrate the superiority of our proposed
system over several baselines on a knowledge graph of BMW automobiles.",http://arxiv.org/abs/2412.17690v3,12/23/24,"Rishiraj Saha Roy, Chris Hinze, Joel Schlotthauer, Farzad Naderi, Viktor Hangya, Andreas Foltyn, Luzian Hahn, Fabian Kuech"
186,Retrieval-Augmented Generation Meets Data-Driven Tabula Rasa Approach for Temporal Knowledge Graph Forecasting,"Pre-trained large language models (PLLMs) like OpenAI ChatGPT and Google
Gemini face challenges such as inaccurate factual recall, hallucinations,
biases, and future data leakage for temporal Knowledge Graph (tKG) forecasting.
To address these issues, we introduce sLA-tKGF (small-scale language assistant
for tKG forecasting), which utilizes Retrieval-Augmented Generation (RAG)
aided, custom-trained small-scale language models through a tabula rasa
approach from scratch for effective tKG forecasting. Our framework constructs
knowledge-infused prompts with relevant historical data from tKGs, web search
results, and PLLMs-generated textual descriptions to understand historical
entity relationships prior to the target time. It leverages these external
knowledge-infused prompts for deeper understanding and reasoning of
context-specific semantic and temporal information to zero-shot prompt
small-scale language models for more accurate predictions of future events
within tKGs. It reduces hallucinations and mitigates distributional shift
challenges through comprehending changing trends over time. As a result, it
enables more accurate and contextually grounded forecasts of future events
while minimizing computational demands. Rigorous empirical studies demonstrate
our framework robustness, scalability, and state-of-the-art (SOTA) performance
on benchmark datasets with interpretable and trustworthy tKG forecasting.",http://arxiv.org/abs/2408.13273v1,8/18/24,"Geethan Sannidhi, Sagar Srinivas Sakhinana, Venkataramana Runkana"
187,A Survey on Knowledge-Oriented Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has gained significant attention in
recent years for its potential to enhance natural language understanding and
generation by combining large-scale retrieval systems with generative models.
RAG leverages external knowledge sources, such as documents, databases, or
structured data, to improve model performance and generate more accurate and
contextually relevant outputs. This survey aims to provide a comprehensive
overview of RAG by examining its fundamental components, including retrieval
mechanisms, generation processes, and the integration between the two. We
discuss the key characteristics of RAG, such as its ability to augment
generative models with dynamic external knowledge, and the challenges
associated with aligning retrieved information with generative objectives. We
also present a taxonomy that categorizes RAG methods, ranging from basic
retrieval-augmented approaches to more advanced models incorporating
multi-modal data and reasoning capabilities. Additionally, we review the
evaluation benchmarks and datasets commonly used to assess RAG systems, along
with a detailed exploration of its applications in fields such as question
answering, summarization, and information retrieval. Finally, we highlight
emerging research directions and opportunities for improving RAG systems, such
as enhanced retrieval efficiency, model interpretability, and domain-specific
adaptations. This paper concludes by outlining the prospects for RAG in
addressing real-world challenges and its potential to drive further
advancements in natural language processing.",http://arxiv.org/abs/2503.10677v2,3/11/25,"Mingyue Cheng, Yucong Luo, Jie Ouyang, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang, Jiawei Cao, Jie Ma, Daoyu Wang, Enhong Chen"
188,Question-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering,"Knowledge graph question answering (KGQA) involves answering natural language
questions by leveraging structured information stored in a knowledge graph.
Typically, KGQA initially retrieve a targeted subgraph from a large-scale
knowledge graph, which serves as the basis for reasoning models to address
queries. However, the retrieved subgraph inevitably brings distraction
information for knowledge utilization, impeding the model's ability to perform
accurate reasoning. To address this issue, we propose a Question-guided
Knowledge Graph Re-scoring method (Q-KGR) to eliminate noisy pathways for the
input question, thereby focusing specifically on pertinent factual knowledge.
Moreover, we introduce Knowformer, a parameter-efficient method for injecting
the re-scored knowledge graph into large language models to enhance their
ability to perform factual reasoning. Extensive experiments on multiple KGQA
benchmarks demonstrate the superiority of our method over existing systems.",http://arxiv.org/abs/2410.01401v1,10/2/24,"Yu Zhang, Kehai Chen, Xuefeng Bai, zhao kang, Quanjiang Guo, Min Zhang"
189,Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models,"Retrieval-Augmented Generation (RAG) has been shown to enhance the factual
accuracy of Large Language Models (LLMs), but existing methods often suffer
from limited reasoning capabilities in effectively using the retrieved
evidence, particularly when using open-source LLMs. To mitigate this gap, we
introduce a novel framework, Open-RAG, designed to enhance reasoning
capabilities in RAG with open-source LLMs. Our framework transforms an
arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE)
model capable of handling complex reasoning tasks, including both single- and
multi-hop queries. Open-RAG uniquely trains the model to navigate challenging
distractors that appear relevant but are misleading. As a result, Open-RAG
leverages latent learning, dynamically selecting relevant experts and
integrating external knowledge effectively for more accurate and contextually
relevant responses. In addition, we propose a hybrid adaptive retrieval method
to determine retrieval necessity and balance the trade-off between performance
gain and inference speed. Experimental results show that the Llama2-7B-based
Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT,
Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source
our code and models at https://openragmoe.github.io/",http://arxiv.org/abs/2410.01782v1,10/2/24,"Shayekh Bin Islam, Md Asib Rahman, K S M Tozammel Hossain, Enamul Hoque, Shafiq Joty, Md Rizwan Parvez"
190,RAG-based Question Answering over Heterogeneous Data and Text,"This article presents the QUASAR system for question answering over
unstructured text, structured tables, and knowledge graphs, with unified
treatment of all sources. The system adopts a RAG-based architecture, with a
pipeline of evidence retrieval followed by answer generation, with the latter
powered by a moderate-sized language model. Additionally and uniquely, QUASAR
has components for question understanding, to derive crisper input for evidence
retrieval, and for re-ranking and filtering the retrieved evidence before
feeding the most informative pieces into the answer generation. Experiments
with three different benchmarks demonstrate the high answering quality of our
approach, being on par with or better than large GPT models, while keeping the
computational cost and energy consumption orders of magnitude lower.",http://arxiv.org/abs/2412.07420v1,12/10/24,"Philipp Christmann, Gerhard Weikum"
191,DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients,"Existing medical RAG systems mainly leverage knowledge from medical knowledge
bases, neglecting the crucial role of experiential knowledge derived from
similar patient cases -- a key component of human clinical reasoning. To bridge
this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like
reasoning by integrating both explicit clinical knowledge and implicit
case-based experience. DoctorRAG enhances retrieval precision by first
allocating conceptual tags for queries and knowledge sources, together with a
hybrid retrieval mechanism from both relevant knowledge and patient. In
addition, a Med-TextGrad module using multi-agent textual gradients is
integrated to ensure that the final output adheres to the retrieved knowledge
and patient query. Comprehensive experiments on multilingual, multitask
datasets demonstrate that DoctorRAG significantly outperforms strong baseline
RAG models and gains improvements from iterative refinements. Our approach
generates more accurate, relevant, and comprehensive responses, taking a step
towards more doctor-like medical reasoning systems.",http://arxiv.org/abs/2505.19538v1,5/26/25,"Yuxing Lu, Gecheng Fu, Wei Wu, Xukai Zhao, Sin Yee Goi, Jinzhuo Wang"
192,Biomedical knowledge graph-optimized prompt generation for large language models,"Large Language Models (LLMs) are being adopted at an unprecedented rate, yet
still face challenges in knowledge-intensive domains like biomedicine.
Solutions such as pre-training and domain-specific fine-tuning add substantial
computational overhead, requiring further domain expertise. Here, we introduce
a token-optimized and robust Knowledge Graph-based Retrieval Augmented
Generation (KG-RAG) framework by leveraging a massive biomedical KG (SPOKE)
with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to generate meaningful
biomedical text rooted in established knowledge. Compared to the existing RAG
technique for Knowledge Graphs, the proposed method utilizes minimal graph
schema for context extraction and uses embedding methods for context pruning.
This optimization in context extraction results in more than 50% reduction in
token consumption without compromising the accuracy, making a cost-effective
and robust RAG implementation on proprietary LLMs. KG-RAG consistently enhanced
the performance of LLMs across diverse biomedical prompts by generating
responses rooted in established knowledge, accompanied by accurate provenance
and statistical evidence (if available) to substantiate the claims. Further
benchmarking on human curated datasets, such as biomedical true/false and
multiple-choice questions (MCQ), showed a remarkable 71% boost in the
performance of the Llama-2 model on the challenging MCQ dataset, demonstrating
the framework's capacity to empower open-source models with fewer parameters
for domain specific questions. Furthermore, KG-RAG enhanced the performance of
proprietary GPT models, such as GPT-3.5 and GPT-4. In summary, the proposed
framework combines explicit and implicit knowledge of KG and LLM in a token
optimized fashion, thus enhancing the adaptability of general-purpose LLMs to
tackle domain-specific questions in a cost-effective fashion.",http://arxiv.org/abs/2311.17330v2,11/29/23,"Karthik Soman, Peter W Rose, John H Morris, Rabia E Akbas, Brett Smith, Braian Peetoom, Catalina VilloutaReyes, Gabriel Cerono, Yongmei Shi, Angela RizkJackson, Sharat Israni, Charlotte A Nelson, Sui Huang, Sergio E Baranzini"
193,Chain-of-Thought Poisoning Attacks against R1-based Retrieval-Augmented Generation Systems,"Retrieval-augmented generation (RAG) systems can effectively mitigate the
hallucination problem of large language models (LLMs),but they also possess
inherent vulnerabilities. Identifying these weaknesses before the large-scale
real-world deployment of RAG systems is of great importance, as it lays the
foundation for building more secure and robust RAG systems in the future.
Existing adversarial attack methods typically exploit knowledge base poisoning
to probe the vulnerabilities of RAG systems, which can effectively deceive
standard RAG models. However, with the rapid advancement of deep reasoning
capabilities in modern LLMs, previous approaches that merely inject incorrect
knowledge are inadequate when attacking RAG systems equipped with deep
reasoning abilities. Inspired by the deep thinking capabilities of LLMs, this
paper extracts reasoning process templates from R1-based RAG systems, uses
these templates to wrap erroneous knowledge into adversarial documents, and
injects them into the knowledge base to attack RAG systems. The key idea of our
approach is that adversarial documents, by simulating the chain-of-thought
patterns aligned with the model's training signals, may be misinterpreted by
the model as authentic historical reasoning processes, thus increasing their
likelihood of being referenced. Experiments conducted on the MS MARCO passage
ranking dataset demonstrate the effectiveness of our proposed method.",http://arxiv.org/abs/2505.16367v1,5/22/25,"Hongru Song, Yuan Liu, Ruqing Zhang, Jiafeng Guo, Yixing Fan"
194,Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata,"The retrieval-augmented generation (RAG) enables retrieval of relevant
information from an external knowledge source and allows large language models
(LLMs) to answer queries over previously unseen document collections. However,
it was demonstrated that traditional RAG applications perform poorly in
answering multi-hop questions, which require retrieving and reasoning over
multiple elements of supporting evidence. We introduce a new method called
Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to
improve the RAG selection of the relevant documents from various sources,
relevant to the question. While database filtering is specific to a set of
questions from a particular domain and format, we found out that Multi-Meta-RAG
greatly improves the results on the MultiHop-RAG benchmark. The code is
available at https://github.com/mxpoliakov/Multi-Meta-RAG.",http://arxiv.org/abs/2406.13213v2,6/19/24,"Mykhailo Poliakov, Nadiya Shvai"
195,KRAG Framework for Enhancing LLMs in the Legal Domain,"This paper introduces Knowledge Representation Augmented Generation (KRAG), a
novel framework designed to enhance the capabilities of Large Language Models
(LLMs) within domain-specific applications. KRAG points to the strategic
inclusion of critical knowledge entities and relationships that are typically
absent in standard data sets and which LLMs do not inherently learn. In the
context of legal applications, we present Soft PROLEG, an implementation model
under KRAG, which uses inference graphs to aid LLMs in delivering structured
legal reasoning, argumentation, and explanations tailored to user inquiries.
The integration of KRAG, either as a standalone framework or in tandem with
retrieval augmented generation (RAG), markedly improves the ability of language
models to navigate and solve the intricate challenges posed by legal texts and
terminologies. This paper details KRAG's methodology, its implementation
through Soft PROLEG, and potential broader applications, underscoring its
significant role in advancing natural language understanding and processing in
specialized knowledge domains.",http://arxiv.org/abs/2410.07551v1,10/10/24,"Nguyen Ha Thanh, Ken Satoh"
196,Knowledge-Enhanced Program Repair for Data Science Code,"This paper introduces DSrepair, a knowledge-enhanced program repair method
designed to repair the buggy code generated by LLMs in the data science domain.
DSrepair uses knowledge graph based RAG for API knowledge retrieval as well as
bug knowledge enrichment to construct repair prompts for LLMs. Specifically, to
enable knowledge graph based API retrieval, we construct DS-KG (Data Science
Knowledge Graph) for widely used data science libraries. For bug knowledge
enrichment, we employ an abstract syntax tree (AST) to localize errors at the
AST node level. DSrepair's effectiveness is evaluated against five
state-of-the-art LLM-based repair baselines using four advanced LLMs on the
DS-1000 dataset. The results show that DSrepair surpasses all five baselines.
Specifically, when compared to the second-best baseline, DSrepair demonstrates
significant improvements, fixing 44.4%, 14.2%, 20.6%, and 32.1% more buggy code
snippets for each of the four evaluated LLMs, respectively. Additionally, it
achieves greater efficiency, reducing the number of tokens required per code
task by 17.49%, 34.24%, 24.71%, and 17.59%, respectively.",http://arxiv.org/abs/2502.09771v1,2/13/25,"Shuyin Ouyang, Jie M Zhang, Zeyu Sun, Albert Merono Penuela"
197,OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large Language Models,"This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented
Generation method designed to enhance LLM-generated responses by anchoring
retrieval processes in domain-specific ontologies. While LLMs are widely used
for tasks like question answering and search, they struggle to adapt to
specialized knowledge, such as industrial workflows or knowledge work, without
expensive fine-tuning or sub-optimal retrieval methods. Existing
retrieval-augmented models, such as RAG, offer improvements but fail to account
for structured domain knowledge, leading to suboptimal context generation.
Ontologies, which conceptually organize domain knowledge by defining entities
and their interrelationships, offer a structured representation to address this
gap. OG-RAG constructs a hypergraph representation of domain documents, where
each hyperedge encapsulates clusters of factual knowledge grounded using
domain-specific ontology. An optimization algorithm then retrieves the minimal
set of hyperedges that constructs a precise, conceptually grounded context for
the LLM. This method enables efficient retrieval while preserving the complex
relationships between entities. OG-RAG applies to domains where fact-based
reasoning is essential, particularly in tasks that require workflows or
decision-making steps to follow predefined rules and procedures. These include
industrial workflows in healthcare, legal, and agricultural sectors, as well as
knowledge-driven tasks such as news journalism, investigative research,
consulting and more. Our evaluations demonstrate that OG-RAG increases the
recall of accurate facts by 55% and improves response correctness by 40% across
four different LLMs. Additionally, OG-RAG enables 30% faster attribution of
responses to context and boosts fact-based reasoning accuracy by 27% compared
to baseline methods.",http://arxiv.org/abs/2412.15235v1,12/12/24,"Kartik Sharma, Peeyush Kumar, Yunqing Li"
198,BRIT: Bidirectional Retrieval over Unified Image-Text Graph,"Retrieval-Augmented Generation (RAG) has emerged as a promising technique to
enhance the quality and relevance of responses generated by large language
models. While recent advancements have mainly focused on improving RAG for
text-based queries, RAG on multi-modal documents containing both texts and
images has not been fully explored. Especially when fine-tuning does not work.
This paper proposes BRIT, a novel multi-modal RAG framework that effectively
unifies various text-image connections in the document into a multi-modal graph
and retrieves the texts and images as a query-specific sub-graph. By traversing
both image-to-text and text-to-image paths in the graph, BRIT retrieve not only
directly query-relevant images and texts but also further relevant contents to
answering complex cross-modal multi-hop questions. To evaluate the
effectiveness of BRIT, we introduce MM-RAG test set specifically designed for
multi-modal question answering tasks that require to understand the text-image
relations. Our comprehensive experiments demonstrate the superiority of BRIT,
highlighting its ability to handle cross-modal questions on the multi-modal
documents.",http://arxiv.org/abs/2505.18450v1,5/24/25,"Ainulla Khan, Yamada Moyuru, Srinidhi Akella"
199,"Honest AI: Fine-Tuning ""Small"" Language Models to Say ""I Don't Know"", and Reducing Hallucination in RAG","Hallucination is a key roadblock for applications of Large Language Models
(LLMs), particularly for enterprise applications that are sensitive to
information accuracy. To address this issue, two general approaches have been
explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated
information as context, and fine-tuning the LLMs with new information and
desired output styles. In this paper, we propose Honest AI: a novel strategy to
fine-tune ""small"" language models to say ""I don't know"" to reduce
hallucination, along with several alternative RAG approaches. The solution
ranked 1st in Task 2 for the false premise question. The alternative approaches
include using RAG with search engine and knowledge graph results, fine-tuning
base LLMs with new information and combinations of both approaches. Although
all approaches improve the performance of the LLMs, RAG alone does not
significantly improve the performance and fine-tuning is needed for better
results. Finally, the hybrid approach achieved the highest score in the CRAG
benchmark. In addition, our approach emphasizes the use of relatively small
models with fewer than 10 billion parameters, promoting resource efficiency.",http://arxiv.org/abs/2410.09699v1,10/13/24,"Xinxi Chen, Li Wang, Wei Wu, Qi Tang, Yiyao Liu"
200,Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) has demonstrated effectiveness in
mitigating the hallucination problem of large language models (LLMs). However,
the difficulty of aligning the retriever with the diverse LLMs' knowledge
preferences inevitably poses an inevitable challenge in developing a reliable
RAG system. To address this issue, we propose DPA-RAG, a universal framework
designed to align diverse knowledge preferences within RAG systems.
Specifically, we initially introduce a preference knowledge construction
pipline and incorporate five novel query augmentation strategies to alleviate
preference data scarcity. Based on preference data, DPA-RAG accomplishes both
external and internal preference alignment: 1) It jointly integrate pair-wise,
point-wise, and contrastive preference alignment abilities into the reranker,
achieving external preference alignment among RAG components. 2) It further
introduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),
enabling LLMs to implicitly capture knowledge aligned with their reasoning
preferences, achieving LLMs' internal alignment. Experimental results across
four knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all
baselines and seamlessly integrates both black-box and open-sourced LLM
readers. Further qualitative analysis and discussions also provide empirical
guidance for achieving reliable RAG systems. Our code is publicly available at
https://github.com/dongguanting/DPA-RAG.",http://arxiv.org/abs/2406.18676v2,6/26/24,"Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, JiRong Wen"
201,Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level RAG,"Vulnerability detection is essential for software quality assurance. In
recent years, deep learning models (especially large language models) have
shown promise in vulnerability detection. In this work, we propose a novel
LLM-based vulnerability detection technique Vul-RAG, which leverages
knowledge-level retrieval-augmented generation (RAG) framework to detect
vulnerability for the given code in three phases. First, Vul-RAG constructs a
vulnerability knowledge base by extracting multi-dimension knowledge via LLMs
from existing CVE instances; second, for a given code snippet, Vul-RAG}
retrieves the relevant vulnerability knowledge from the constructed knowledge
base based on functional semantics; third, Vul-RAG leverages LLMs to check the
vulnerability of the given code snippet by reasoning the presence of
vulnerability causes and fixing solutions of the retrieved vulnerability
knowledge. Our evaluation of Vul-RAG on our constructed benchmark PairVul shows
that Vul-RAG substantially outperforms all baselines by 12.96\%/110\% relative
improvement in accuracy/pairwise-accuracy. In addition, our user study shows
that the vulnerability knowledge generated by Vul-RAG can serve as high-quality
explanations which can improve the manual detection accuracy from 0.60 to 0.77.",http://arxiv.org/abs/2406.11147v2,6/17/24,"Xueying Du, Geng Zheng, Kaixin Wang, Jiayi Feng, Wentai Deng, Mingwei Liu, Bihuan Chen, Xin Peng, Tao Ma, Yiling Lou"
202,A Retrieval-Augmented Knowledge Mining Method with Deep Thinking LLMs for Biomedical Research and Clinical Support,"Knowledge graphs and large language models (LLMs) are key tools for
biomedical knowledge integration and reasoning, facilitating structured
organization of scientific articles and discovery of complex semantic
relationships. However, current methods face challenges: knowledge graph
construction is limited by complex terminology, data heterogeneity, and rapid
knowledge evolution, while LLMs show limitations in retrieval and reasoning,
making it difficult to uncover cross-document associations and reasoning
pathways. To address these issues, we propose a pipeline that uses LLMs to
construct a biomedical knowledge graph (BioStrataKG) from large-scale articles
and builds a cross-document question-answering dataset (BioCDQA) to evaluate
latent knowledge retrieval and multi-hop reasoning. We then introduce
Integrated and Progressive Retrieval-Augmented Reasoning (IP-RAR) to enhance
retrieval accuracy and knowledge reasoning. IP-RAR maximizes information recall
through Integrated Reasoning-based Retrieval and refines knowledge via
Progressive Reasoning-based Generation, using self-reflection to achieve deep
thinking and precise contextual understanding. Experiments show that IP-RAR
improves document retrieval F1 score by 20\% and answer generation accuracy by
25\% over existing methods. This framework helps doctors efficiently integrate
treatment evidence for personalized medication plans and enables researchers to
analyze advancements and research gaps, accelerating scientific discovery and
decision-making.",http://arxiv.org/abs/2503.23029v1,3/29/25,"Yichun Feng, Jiawei Wang, Ruikun He, Lu Zhou, Yixue Li"
203,Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating additional information from retrieval. However, studies have
shown that LLMs still face challenges in effectively using the retrieved
information, even ignoring it or being misled by it. The key reason is that the
training of LLMs does not clearly make LLMs learn how to utilize input
retrieved texts with varied quality. In this paper, we propose a novel
perspective that considers the role of LLMs in RAG as ``Information Refiner'',
which means that regardless of correctness, completeness, or usefulness of
retrieved texts, LLMs can consistently integrate knowledge within the retrieved
texts and model parameters to generate the texts that are more concise,
accurate, and complete than the retrieved texts. To this end, we propose an
information refinement training method named InFO-RAG that optimizes LLMs for
RAG in an unsupervised manner. InFO-RAG is low-cost and general across various
tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse
tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,
and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an
average of 9.39\% relative points. InFO-RAG also shows advantages in in-context
learning and robustness of RAG.",http://arxiv.org/abs/2402.18150v2,2/28/24,"Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, Jie Zhou"
204,EnronQA: Towards Personalized RAG over Private Documents,"Retrieval Augmented Generation (RAG) has become one of the most popular
methods for bringing knowledge-intensive context to large language models (LLM)
because of its ability to bring local context at inference time without the
cost or data leakage risks associated with fine-tuning. A clear separation of
private information from the LLM training has made RAG the basis for many
enterprise LLM workloads as it allows the company to augment LLM's
understanding using customers' private documents. Despite its popularity for
private documents in enterprise deployments, current RAG benchmarks for
validating and optimizing RAG pipelines draw their corpora from public data
such as Wikipedia or generic web pages and offer little to no personal context.
Seeking to empower more personal and private RAG we release the EnronQA
benchmark, a dataset of 103,638 emails with 528,304 question-answer pairs
across 150 different user inboxes. EnronQA enables better benchmarking of RAG
pipelines over private data and allows for experimentation on the introduction
of personalized retrieval settings over realistic data. Finally, we use EnronQA
to explore the tradeoff in memorization and retrieval when reasoning over
private documents.",http://arxiv.org/abs/2505.00263v1,5/1/25,"Michael J Ryan, Danmei Xu, Chris Nivera, Daniel Campos"
205,DepsRAG: Towards Agentic Reasoning and Planning for Software Dependency Management,"In the era of Large Language Models (LLMs) with their advanced capabilities,
a unique opportunity arises to develop LLM-based digital assistant tools that
can support software developers by facilitating comprehensive reasoning about
software dependencies and open-source libraries before importing them. This
reasoning process is daunting, mandating multiple specialized tools and
dedicated expertise, each focusing on distinct aspects (e.g., security analysis
tools may overlook design flaws such as circular dependencies, which hinder
software maintainability). Creating a significant bottleneck in the software
development lifecycle. In this paper, we introduce DepsRAG, a multi-agent
framework designed to assist developers in reasoning about software
dependencies. DepsRAG first constructs a comprehensive Knowledge Graph (KG)
that includes both direct and transitive dependencies. Developers can interact
with DepsRAG through a conversational interface, posing queries about the
dependencies. DepsRAG employs Retrieval-Augmented Generation (RAG) to enhance
these queries by retrieving relevant information from the KG as well as
external sources, such as the Web and vulnerability databases, thus
demonstrating its adaptability to novel scenarios. DepsRAG incorporates a
Critic-Agent feedback loop to ensure the accuracy and clarity of LLM-generated
responses. We evaluated DepsRAG using GPT-4-Turbo and Llama-3 on three
multi-step reasoning tasks, observing a threefold increase in accuracy with the
integration of the Critic-Agent mechanism. DepsRAG demo and implementation are
available: https://github.com/Mohannadcse/DepsRAG.",http://arxiv.org/abs/2405.20455v5,5/30/24,"Mohannad Alhanahnah, Yazan Boshmaf"
206,Talking like Piping and Instrumentation Diagrams (P&IDs),"We propose a methodology that allows communication with Piping and
Instrumentation Diagrams (P&IDs) using natural language. In particular, we
represent P&IDs through the DEXPI data model as labeled property graphs and
integrate them with Large Language Models (LLMs). The approach consists of
three main parts: 1) P&IDs are cast into a graph representation from the DEXPI
format using our pyDEXPI Python package. 2) A tool for generating P&ID
knowledge graphs from pyDEXPI. 3) Integration of the P&ID knowledge graph to
LLMs using graph-based retrieval augmented generation (graph-RAG). This
approach allows users to communicate with P&IDs using natural language. It
extends LLM's ability to retrieve contextual data from P&IDs and mitigate
hallucinations. Leveraging the LLM's large corpus, the model is also able to
interpret process information in PIDs, which could help engineers in their
daily tasks. In the future, this work will also open up opportunities in the
context of other generative Artificial Intelligence (genAI) solutions on P&IDs,
and AI-assisted HAZOP studies.",http://arxiv.org/abs/2502.18928v1,2/26/25,"Achmad Anggawirya Alimin, Dominik P Goldstein, Lukas Schulze Balhorn, Artur M Schweidtmann"
207,HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases,"Given a semi-structured knowledge base (SKB), where text documents are
interconnected by relations, how can we effectively retrieve relevant
information to answer user questions? Retrieval-Augmented Generation (RAG)
retrieves documents to assist large language models (LLMs) in question
answering; while Graph RAG (GRAG) uses structured knowledge bases as its
knowledge source. However, many questions require both textual and relational
information from SKB - referred to as ""hybrid"" questions - which complicates
the retrieval process and underscores the need for a hybrid retrieval method
that leverages both information. In this paper, through our empirical analysis,
we identify key insights that show why existing methods may struggle with
hybrid question answering (HQA) over SKB. Based on these insights, we propose
HybGRAG for HQA consisting of a retriever bank and a critic module, with the
following advantages: (1) Agentic, it automatically refines the output by
incorporating feedback from the critic module, (2) Adaptive, it solves hybrid
questions requiring both textual and relational information with the retriever
bank, (3) Interpretable, it justifies decision making with intuitive refinement
path, and (4) Effective, it surpasses all baselines on HQA benchmarks. In
experiments on the STaRK benchmark, HybGRAG achieves significant performance
gains, with an average relative improvement in Hit@1 of 51%.",http://arxiv.org/abs/2412.16311v1,12/20/24,"MengChieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N Ioannidis, Huzefa Rangwala, Christos Faloutsos"
208,Memory-Aware and Uncertainty-Guided Retrieval for Multi-Hop Question Answering,"Multi-hop question answering (QA) requires models to retrieve and reason over
multiple pieces of evidence. While Retrieval-Augmented Generation (RAG) has
made progress in this area, existing methods often suffer from two key
limitations: (1) fixed or overly frequent retrieval steps, and (2) ineffective
use of previously retrieved knowledge.
  We propose MIND (Memory-Informed and INteractive Dynamic RAG), a framework
that addresses these challenges through: (i) prompt-based entity extraction to
identify reasoning-relevant elements, (ii) dynamic retrieval triggering based
on token-level entropy and attention signals, and (iii) memory-aware filtering,
which stores high-confidence facts across reasoning steps to enable consistent
multi-hop generation.",http://arxiv.org/abs/2503.23095v1,3/29/25,"Yuelyu Ji, Rui Meng, Zhuochun Li, Daqing He"
209,A Socratic RAG Approach to Connect Natural Language Queries on Research Topics with Knowledge Organization Systems,"In this paper, we propose a Retrieval Augmented Generation (RAG) agent that
maps natural language queries about research topics to precise,
machine-interpretable semantic entities. Our approach combines RAG with
Socratic dialogue to align a user's intuitive understanding of research topics
with established Knowledge Organization Systems (KOSs). The proposed approach
will effectively bridge ""little semantics"" (domain-specific KOS structures)
with ""big semantics"" (broad bibliometric repositories), making complex academic
taxonomies more accessible. Such agents have the potential for broad use. We
illustrate with a sample application called CollabNext, which is a
person-centric knowledge graph connecting people, organizations, and research
topics. We further describe how the application design has an intentional focus
on HBCUs and emerging researchers to raise visibility of people historically
rendered invisible in the current science system.",http://arxiv.org/abs/2502.15005v1,2/20/25,"Lew Lefton, Kexin Rong, Chinar Dankhara, Lila Ghemri, Firdous Kausar, A Hannibal Hamdallahi"
210,Audio Captioning RAG via Generative Pair-to-Pair Retrieval with Refined Knowledge Base,"Recent advances in audio understanding tasks leverage the reasoning
capabilities of LLMs. However, adapting LLMs to learn audio concepts requires
massive training data and substantial computational resources. To address these
challenges, Retrieval-Augmented Generation (RAG) retrieves audio-text pairs
from a knowledge base (KB) and augments them with query audio to generate
accurate textual responses. In RAG, the relevance of the retrieved information
plays a crucial role in effectively processing the input. In this paper, we
analyze how different retrieval methods and knowledge bases impact the
relevance of audio-text pairs and the performance of audio captioning with RAG.
We propose generative pair-to-pair retrieval, which uses the generated caption
as a text query to accurately find relevant audio-text pairs to the query
audio, thereby improving the relevance and accuracy of retrieved information.
Additionally, we refine the large-scale knowledge base to retain only
audio-text pairs that align with the contextualized intents. Our approach
achieves state-of-the-art results on benchmarks including AudioCaps, Clotho,
and Auto-ACD, with detailed ablation studies validating the effectiveness of
our retrieval and KB construction methods.",http://arxiv.org/abs/2410.10913v2,10/14/24,"Choi Changin, Lim Sungjun, Rhee Wonjong"
211,"Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization","Large Language Models (LLMs) are pre-trained on large-scale corpora and excel
in numerous general natural language processing (NLP) tasks, such as question
answering (QA). Despite their advanced language capabilities, when it comes to
domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,
knowledge cut-offs, and lack of knowledge attributions. Additionally, fine
tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and
time consuming process. The retrieval-augmented generation (RAG) process has
recently emerged as a method capable of optimization of LLM responses, by
referencing them to a predetermined ontology. It was shown that using a
Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into
account relevant sub-graphs that preserve the information in a structured
manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM
framework, that integrates RAG with KG and a vector store (VS) that store
factual domain specific information. Importantly, to avoid hallucinations in
the KG, we build these highly domain-specific KGs and VSs without the use of
LLMs, but via NLP, data mining, and nonnegative tensor factorization with
automatic model selection. Pairing our RAG with a domain-specific: (i) KG
(containing structured information), and (ii) VS (containing unstructured
information) enables the development of domain-specific chat-bots that
attribute the source of information, mitigate hallucinations, lessen the need
for fine-tuning, and excel in highly domain-specific question answering tasks.
We pair SMART-SLIC with chain-of-thought prompting agents. The framework is
designed to be generalizable to adapt to any specific or specialized domain. In
this paper, we demonstrate the question answering capabilities of our framework
on a corpus of scientific publications on malware analysis and anomaly
detection.",http://arxiv.org/abs/2410.02721v1,10/3/24,"Ryan C Barron, Ves Grantcharov, Selma Wanna, Maksim E Eren, Manish Bhattarai, Nicholas Solovyev, George Tompkins, Charles Nicholas, Kim  Rasmussen, Cynthia Matuszek, Boian S Alexandrov"
212,GARLIC: LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph for Long Document QA,"In the past, Retrieval-Augmented Generation (RAG) methods split text into
chunks to enable language models to handle long documents. Recent tree-based
RAG methods are able to retrieve detailed information while preserving global
context. However, with the advent of more powerful LLMs, such as Llama 3.1,
which offer better comprehension and support for longer inputs, we found that
even recent tree-based RAG methods perform worse than directly feeding the
entire document into Llama 3.1, although RAG methods still hold an advantage in
reducing computational costs. In this paper, we propose a new retrieval method,
called LLM-Guided Dynamic Progress Control with Hierarchical Weighted Graph
(GARLIC), which outperforms previous state-of-the-art baselines, including
Llama 3.1, while retaining the computational efficiency of RAG methods. Our
method introduces several improvements: (1) Rather than using a tree structure,
we construct a Hierarchical Weighted Directed Acyclic Graph with many-to-many
summarization, where the graph edges are derived from attention mechanisms, and
each node focuses on a single event or very few events. (2) We introduce a
novel retrieval method that leverages the attention weights of LLMs rather than
dense embedding similarity. Our method allows for searching the graph along
multiple paths and can terminate at any depth. (3) We use the LLM to control
the retrieval process, enabling it to dynamically adjust the amount and depth
of information retrieved for different queries. Experimental results show that
our method outperforms previous state-of-the-art baselines, including Llama
3.1, on two single-document and two multi-document QA datasets, while
maintaining similar computational complexity to traditional RAG methods.",http://arxiv.org/abs/2410.04790v1,10/7/24,"Xinyu Wang, Yanzheng Xiang, Lin Gui, Yulan He"
213,Retrieval-Augmented Purifier for Robust LLM-Empowered Recommendation,"Recently, Large Language Model (LLM)-empowered recommender systems have
revolutionized personalized recommendation frameworks and attracted extensive
attention. Despite the remarkable success, existing LLM-empowered RecSys have
been demonstrated to be highly vulnerable to minor perturbations. To mitigate
the negative impact of such vulnerabilities, one potential solution is to
employ collaborative signals based on item-item co-occurrence to purify the
malicious collaborative knowledge from the user's historical interactions
inserted by attackers. On the other hand, due to the capabilities to expand
insufficient internal knowledge of LLMs, Retrieval-Augmented Generation (RAG)
techniques provide unprecedented opportunities to enhance the robustness of
LLM-empowered recommender systems by introducing external collaborative
knowledge. Therefore, in this paper, we propose a novel framework (RETURN) by
retrieving external collaborative signals to purify the poisoned user profiles
and enhance the robustness of LLM-empowered RecSys in a plug-and-play manner.
Specifically, retrieval-augmented perturbation positioning is proposed to
identify potential perturbations within the users' historical sequences by
retrieving external knowledge from collaborative item graphs. After that, we
further retrieve the collaborative knowledge to cleanse the perturbations by
using either deletion or replacement strategies and introduce a robust ensemble
recommendation strategy to generate final robust predictions. Extensive
experiments on three real-world datasets demonstrate the effectiveness of the
proposed RETURN.",http://arxiv.org/abs/2504.02458v1,4/3/25,"Liangbo Ning, Wenqi Fan, Qing Li"
214,BugRepro: Enhancing Android Bug Reproduction with Domain-Specific Knowledge Integration,"Mobile application development is a fast-paced process where maintaining
high-quality user experiences is crucial. Current bug reproduction methods
predominantly depend on precise feature descriptions in bug reports. However,
the growing complexity and dynamism of modern software systems pose significant
challenges to this crucial quality assurance process, as ambiguous or
incomplete steps-to-reproduce (S2Rs) in reports frequently impede effective
debugging and maintenance. To address these challenges, we propose BugRepro, a
novel technique that integrates domain-specific knowledge to enhance the
accuracy and efficiency of bug reproduction. BugRepro adopts a
Retrieval-Augmented Generation (RAG) approach. It retrieves similar bug reports
along with their corresponding S2R entities from an example-rich RAG document.
This document serves as a valuable reference for improving the accuracy of S2R
entity extraction. In addition, BugRepro incorporates app-specific knowledge.
It explores the app's graphical user interface (GUI) and extracts UI transition
graphs. These graphs are used to guide large language models (LLMs) in their
exploration process when they encounter bottlenecks. Our experiments
demonstrate the effectiveness of BugRepro. Our method significantly outperforms
two state-of-the-art methods. For S2R entity extraction accuracy, it achieves
improvements of 8.85% and 28.89%. For bug reproduction success rate, the
improvements reach 74.55% and 152.63%. In reproduction efficiency, the gains
are 0.72% and 76.68%.",http://arxiv.org/abs/2505.14528v1,5/20/25,"Hongrong Yin, Tao Zhang"
215,Reasoning of Large Language Models over Knowledge Graphs with Super-Relations,"While large language models (LLMs) have made significant progress in
processing and reasoning over knowledge graphs, current methods suffer from a
high non-retrieval rate. This limitation reduces the accuracy of answering
questions based on these graphs. Our analysis reveals that the combination of
greedy search and forward reasoning is a major contributor to this issue. To
overcome these challenges, we introduce the concept of super-relations, which
enables both forward and backward reasoning by summarizing and connecting
various relational paths within the graph. This holistic approach not only
expands the search space, but also significantly improves retrieval efficiency.
In this paper, we propose the ReKnoS framework, which aims to Reason over
Knowledge Graphs with Super-Relations. Our framework's key advantages include
the inclusion of multiple relation paths through super-relations, enhanced
forward and backward reasoning capabilities, and increased efficiency in
querying LLMs. These enhancements collectively lead to a substantial
improvement in the successful retrieval rate and overall reasoning performance.
We conduct extensive experiments on nine real-world datasets to evaluate
ReKnoS, and the results demonstrate the superior performance of ReKnoS over
existing state-of-the-art baselines, with an average accuracy gain of 2.92%.",http://arxiv.org/abs/2503.22166v1,3/28/25,"Song Wang, Junhong Lin, Xiaojie Guo, Julian Shun, Jundong Li, Yada Zhu"
216,CommGPT: A Graph and Retrieval-Augmented Multimodal Communication Foundation Model,"Large Language Models (LLMs) possess human-level cognitive and
decision-making capabilities, making them a key technology for 6G. However,
applying LLMs to the communication domain faces three major challenges: 1)
Inadequate communication data; 2) Restricted input modalities; and 3)
Difficulty in knowledge retrieval. To overcome these issues, we propose
CommGPT, a multimodal foundation model designed specifically for
communications. First, we create high-quality pretraining and fine-tuning
datasets tailored in communication, enabling the LLM to engage in further
pretraining and fine-tuning with communication concepts and knowledge. Then, we
design a multimodal encoder to understand and process information from various
input modalities. Next, we construct a Graph and Retrieval-Augmented Generation
(GRG) framework, efficiently coupling Knowledge Graph (KG) with
Retrieval-Augmented Generation (RAG) for multi-scale learning. Finally, we
demonstrate the feasibility and effectiveness of the CommGPT through
experimental validation.",http://arxiv.org/abs/2502.18763v1,2/26/25,"Feibo Jiang, Wanyun Zhu, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Octavia A Dobre"
217,SRTK: A Toolkit for Semantic-relevant Subgraph Retrieval,"Information retrieval based knowledge base question answering (KBQA) first
retrieves a subgraph to reduce search space, then reasons on the subgraph to
select answer entities. Existing approaches have three issues that impede the
retrieval of such subgraphs. Firstly, there is no off-the-shelf toolkit for
semantic-relevant subgraph retrieval. Secondly, existing methods are
knowledge-graph-dependent, resulting in outdated knowledge graphs used even in
recent studies. Thirdly, previous solutions fail to incorporate the best
available techniques for entity linking or path expansion. In this paper, we
present SRTK, a user-friendly toolkit for semantic-relevant subgraph retrieval
from large-scale knowledge graphs. SRTK is the first toolkit that streamlines
the entire lifecycle of subgraph retrieval across multiple knowledge graphs.
Additionally, it comes with state-of-the-art subgraph retrieval algorithms,
guaranteeing an up-to-date solution set out of the box.",http://arxiv.org/abs/2305.04101v4,5/6/23,Yuanchun Shen
218,Tree-based RAG-Agent Recommendation System: A Case Study in Medical Test Data,"We present HiRMed (Hierarchical RAG-enhanced Medical Test Recommendation), a
novel tree-structured recommendation system that leverages Retrieval-Augmented
Generation (RAG) for intelligent medical test recommendations. Unlike
traditional vector similarity-based approaches, our system performs medical
reasoning at each tree node through a specialized RAG process. Starting from
the root node with initial symptoms, the system conducts step-wise medical
analysis to identify potential underlying conditions and their corresponding
diagnostic requirements. At each level, instead of simple matching, our
RAG-enhanced nodes analyze retrieved medical knowledge to understand
symptom-disease relationships and determine the most appropriate diagnostic
path. The system dynamically adjusts its recommendation strategy based on
medical reasoning results, considering factors such as urgency levels and
diagnostic uncertainty. Experimental results demonstrate that our approach
achieves superior performance in terms of coverage rate, accuracy, and miss
rate compared to conventional retrieval-based methods. This work represents a
significant advance in medical test recommendation by introducing medical
reasoning capabilities into the traditional tree-based retrieval structure.",http://arxiv.org/abs/2501.02727v1,1/6/25,"Yahe Yang, Chengyue Huang"
219,Augmenting Textual Generation via Topology Aware Retrieval,"Despite the impressive advancements of Large Language Models (LLMs) in
generating text, they are often limited by the knowledge contained in the input
and prone to producing inaccurate or hallucinated content. To tackle these
issues, Retrieval-augmented Generation (RAG) is employed as an effective
strategy to enhance the available knowledge base and anchor the responses in
reality by pulling additional texts from external databases. In real-world
applications, texts are often linked through entities within a graph, such as
citations in academic papers or comments in social networks. This paper
exploits these topological relationships to guide the retrieval process in RAG.
Specifically, we explore two kinds of topological connections: proximity-based,
focusing on closely connected nodes, and role-based, which looks at nodes
sharing similar subgraph structures. Our empirical research confirms their
relevance to text relationships, leading us to develop a Topology-aware
Retrieval-augmented Generation framework. This framework includes a retrieval
module that selects texts based on their topological relationships and an
aggregation module that integrates these texts into prompts to stimulate LLMs
for text generation. We have curated established text-attributed networks and
conducted comprehensive experiments to validate the effectiveness of this
framework, demonstrating its potential to enhance RAG with topological
awareness.",http://arxiv.org/abs/2405.17602v1,5/27/24,"Yu Wang, Nedim Lipka, Ruiyi Zhang, Alexa Siu, Yuying Zhao, Bo Ni, Xin Wang, Ryan Rossi, Tyler Derr"
220,Semantic Mastery: Enhancing LLMs with Advanced Natural Language Understanding,"Large language models (LLMs) have greatly improved their capability in
performing NLP tasks. However, deeper semantic understanding, contextual
coherence, and more subtle reasoning are still difficult to obtain. The paper
discusses state-of-the-art methodologies that advance LLMs with more advanced
NLU techniques, such as semantic parsing, knowledge integration, and contextual
reinforcement learning. We analyze the use of structured knowledge graphs,
retrieval-augmented generation (RAG), and fine-tuning strategies that match
models with human-level understanding. Furthermore, we address the
incorporation of transformer-based architectures, contrastive learning, and
hybrid symbolic-neural methods that address problems like hallucinations,
ambiguity, and inconsistency in the factual perspectives involved in performing
complex NLP tasks, such as question-answering text summarization and dialogue
generation. Our findings show the importance of semantic precision for
enhancing AI-driven language systems and suggest future research directions to
bridge the gap between statistical language models and true natural language
understanding.",http://arxiv.org/abs/2504.00409v1,4/1/25,Mohanakrishnan Hariharan
221,Towards Scalable and Cross-Lingual Specialist Language Models for Oncology,"Clinical oncology generates vast, unstructured data that often contain
inconsistencies, missing information, and ambiguities, making it difficult to
extract reliable insights for data-driven decision-making. General-purpose
large language models (LLMs) struggle with these challenges due to their lack
of domain-specific reasoning, including specialized clinical terminology,
context-dependent interpretations, and multi-modal data integration. We address
these issues with an oncology-specialized, efficient, and adaptable NLP
framework that combines instruction tuning, retrieval-augmented generation
(RAG), and graph-based knowledge integration. Our lightweight models prove
effective at oncology-specific tasks, such as named entity recognition (e.g.,
identifying cancer diagnoses), entity linking (e.g., linking entities to
standardized ontologies), TNM staging, document classification (e.g., cancer
subtype classification from pathology reports), and treatment response
prediction. Our framework emphasizes adaptability and resource efficiency. We
include minimal German instructions, collected at the University Hospital
Zurich (USZ), to test whether small amounts of non-English language data can
effectively transfer knowledge across languages. This approach mirrors our
motivation for lightweight models, which balance strong performance with
reduced computational costs, making them suitable for resource-limited
healthcare settings. We validated our models on oncology datasets,
demonstrating strong results in named entity recognition, relation extraction,
and document classification.",http://arxiv.org/abs/2503.08323v1,3/11/25,"Morteza Rohanian, Tarun Mehra, Nicola Miglino, Farhad Nooralahzadeh, Michael Krauthammer, Andreas Wicki"
222,Multi-Document Financial Question Answering using LLMs,"We propose two new methods for multi-document financial question answering.
First, a method that uses semantic tagging, and then, queries the index to get
the context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that
uses semantic tagging, and, retrieves knowledge graph triples from a graph
database, as context. KG_RAG uses knowledge graphs constructed using a small
model that is fine-tuned using knowledge distillation using a large teacher
model. The data consists of 18 10K reports of Apple, Microsoft, Alphabet,
NVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of
questions in the data consists of 111 complex questions including many esoteric
questions that are difficult to answer and the answers are not completely
obvious. As evaluation metrics, we use overall scores as well as segmented
scores for measurement including the faithfulness, relevance, correctness,
similarity, an LLM based overall score and the rouge scores as well as a
similarity of embeddings. We find that both methods outperform plain RAG
significantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.",http://arxiv.org/abs/2411.07264v1,11/8/24,"Shalin Shah, Srikanth Ryali, Ramasubbu Venkatesh"
223,Natural Language Interaction with a Household Electricity Knowledge-based Digital Twin,"Domain specific digital twins, representing a digital replica of various
segments of the smart grid, are foreseen as able to model, simulate, and
control the respective segments. At the same time, knowledge-based digital
twins, coupled with AI, may also empower humans to understand aspects of the
system through natural language interaction in view of planning and policy
making. This paper is the first to assess and report on the potential of
Retrieval Augmented Generation (RAG) question answers related to household
electrical energy measurement aspects leveraging a knowledge-based energy
digital twin. Relying on the recently published electricity consumption
knowledge graph that actually represents a knowledge-based digital twin, we
study the capabilities of ChatGPT, Gemini and Llama in answering electricity
related questions. Furthermore, we compare the answers with the ones generated
through a RAG techniques that leverages an existing electricity knowledge-based
digital twin. Our findings illustrate that the RAG approach not only reduces
the incidence of incorrect information typically generated by LLMs but also
significantly improves the quality of the output by grounding responses in
verifiable data. This paper details our methodology, presents a comparative
analysis of responses with and without RAG, and discusses the implications of
our findings for future applications of AI in specialized sectors like energy
data analysis.",http://arxiv.org/abs/2406.06566v4,6/3/24,"Carolina Fortuna, Vid Hanel, Bla Bertalani"
224,AlignRAG: Leveraging Critique Learning for Evidence-Sensitive Retrieval-Augmented Reasoning,"Retrieval-augmented generation (RAG) has become a widely adopted paradigm for
enabling knowledge-grounded large language models (LLMs). However, standard RAG
pipelines often fail to ensure that model reasoning remains consistent with the
evidence retrieved, leading to factual inconsistencies or unsupported
conclusions. In this work, we reinterpret RAG as Retrieval-Augmented Reasoning
and identify a central but underexplored problem: \textit{Reasoning
Misalignment}-the divergence between an LLM's internal reasoning trajectory and
the evidential constraints provided by retrieval. To address this issue, we
propose \textsc{AlignRAG}, a novel iterative framework grounded in
Critique-Driven Alignment (CDA). At the heart of \textsc{AlignRAG} lies a
\textit{contrastive critique synthesis} mechanism that generates
retrieval-sensitive critiques while mitigating self-bias. This mechanism trains
a dedicated retrieval-augmented \textit{Critic Language Model (CLM)} using
labeled critiques that distinguish between evidence-aligned and misaligned
reasoning. Alignment signals for supervision are obtained through
self-supervised or externally guided labeling strategies. The resulting CLM is
explicitly optimized for evidence sensitivity, enabling it to detect and revise
reasoning errors during inference without relying solely on self-generated
feedback. Empirical evaluations show that our 8B-parameter CLM improves
performance over the Self-Refine baseline by 12.1\% on out-of-domain tasks and
outperforms a standard 72B-parameter CLM by 2.2\%, while remaining compatible
with existing RAG architectures as a plug-and-play module. Overall, AlignRAG
offers a principled solution for aligning model reasoning with retrieved
evidence, substantially improving the factual reliability and robustness of RAG
systems.",http://arxiv.org/abs/2504.14858v3,4/21/25,"Jiaqi Wei, Hao Zhou, Xiang Zhang, Di Zhang, Zijie Qiu, Wei Wei, Jinzhe Li, Wanli Ouyang, Siqi Sun"
225,Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering,"Understanding narratives requires reasoning about implicit world knowledge
related to the causes, effects, and states of situations described in text. At
the core of this challenge is how to access contextually relevant knowledge on
demand and reason over it.
  In this paper, we present initial studies toward zero-shot commonsense
question answering by formulating the task as inference over dynamically
generated commonsense knowledge graphs. In contrast to previous studies for
knowledge integration that rely on retrieval of existing knowledge from static
knowledge graphs, our study requires commonsense knowledge integration where
contextually relevant knowledge is often not present in existing knowledge
bases. Therefore, we present a novel approach that generates
contextually-relevant symbolic knowledge structures on demand using generative
neural commonsense knowledge models.
  Empirical results on two datasets demonstrate the efficacy of our
neuro-symbolic approach for dynamically constructing knowledge graphs for
reasoning. Our approach achieves significant performance boosts over pretrained
language models and vanilla knowledge models, all while providing interpretable
reasoning paths for its predictions.",http://arxiv.org/abs/1911.03876v2,11/10/19,"Antoine Bosselut, Ronan Le Bras, Yejin Choi"
226,Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA,"Retrieval-Augmented Generation (RAG) is widely used to inject external
non-parametric knowledge into large language models (LLMs). Recent works
suggest that Knowledge Graphs (KGs) contain valuable external knowledge for
LLMs. Retrieving information from KGs differs from extracting it from document
sets. Most existing approaches seek to directly retrieve relevant subgraphs,
thereby eliminating the need for extensive SPARQL annotations, traditionally
required by semantic parsing methods. In this paper, we model the subgraph
retrieval task as a conditional generation task handled by small language
models. Specifically, we define a subgraph identifier as a sequence of
relations, each represented as a special token stored in the language models.
Our base generative subgraph retrieval model, consisting of only 220M
parameters, achieves competitive retrieval performance compared to
state-of-the-art models relying on 7B parameters, demonstrating that small
language models are capable of performing the subgraph retrieval task.
Furthermore, our largest 3B model, when plugged with an LLM reader, sets new
SOTA end-to-end performance on both the WebQSP and CWQ benchmarks. Our model
and data will be made available online: https://github.com/hwy9855/GSR.",http://arxiv.org/abs/2410.06121v1,10/8/24,"Wenyu Huang, Guancheng Zhou, Hongru Wang, Pavlos Vougiouklis, Mirella Lapata, Jeff Z Pan"
227,Retrieval-Augmented Generation for Large Language Models: A Survey,"Large Language Models (LLMs) showcase impressive capabilities but encounter
challenges like hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the generation,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval, the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces up-to-date evaluation framework and
benchmark. At the end, this article delineates the challenges currently faced
and points out prospective avenues for research and development.",http://arxiv.org/abs/2312.10997v5,12/18/23,"Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang"
228,ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources,"Recent developments in large language models (LLMs) have led to significant
improvements in intelligent dialogue systems'ability to handle complex
inquiries. However, current LLMs still exhibit limitations in specialized
domain knowledge, particularly in technical fields such as agriculture. To
address this problem, we propose ShizishanGPT, an intelligent question
answering system for agriculture based on the Retrieval Augmented Generation
(RAG) framework and agent architecture. ShizishanGPT consists of five key
modules: including a generic GPT-4 based module for answering general
questions; a search engine module that compensates for the problem that the
large language model's own knowledge cannot be updated in a timely manner; an
agricultural knowledge graph module for providing domain facts; a retrieval
module which uses RAG to supplement domain knowledge; and an agricultural agent
module, which invokes specialized models for crop phenotype prediction, gene
expression analysis, and so on. We evaluated the ShizishanGPT using a dataset
containing 100 agricultural questions specially designed for this study. The
experimental results show that the tool significantly outperforms general LLMs
as it provides more accurate and detailed answers due to its modular design and
integration of different domain knowledge sources. Our source code, dataset,
and model weights are publicly available at https://github.com/Zaiwen/CropGPT.",http://arxiv.org/abs/2409.13537v1,9/20/24,"Shuting Yang, Zehui Liu, Wolfgang Mayer"
229,Ontology-Aware RAG for Improved Question-Answering in Cybersecurity Education,"Integrating AI into education has the potential to transform the teaching of
science and technology courses, particularly in the field of cybersecurity.
AI-driven question-answering (QA) systems can actively manage uncertainty in
cybersecurity problem-solving, offering interactive, inquiry-based learning
experiences. Large language models (LLMs) have gained prominence in AI-driven
QA systems, offering advanced language understanding and user engagement.
However, they face challenges like hallucinations and limited domain-specific
knowledge, which reduce their reliability in educational settings. To address
these challenges, we propose CyberRAG, an ontology-aware retrieval-augmented
generation (RAG) approach for developing a reliable and safe QA system in
cybersecurity education. CyberRAG employs a two-step approach: first, it
augments the domain-specific knowledge by retrieving validated cybersecurity
documents from a knowledge base to enhance the relevance and accuracy of the
response. Second, it mitigates hallucinations and misuse by integrating a
knowledge graph ontology to validate the final answer. Experiments on publicly
available cybersecurity datasets show that CyberRAG delivers accurate, reliable
responses aligned with domain knowledge, demonstrating the potential of AI
tools to enhance education.",http://arxiv.org/abs/2412.14191v1,12/10/24,"Chengshuai Zhao, Garima Agrawal, Tharindu Kumarage, Zhen Tan, Yuli Deng, YingChih Chen, Huan Liu"
230,Motif Counting in Complex Networks: A Comprehensive Survey,"Motif counting plays a crucial role in understanding the structural
properties of networks. By computing motif frequencies, researchers can draw
key insights into the structural properties of the underlying network. As
networks become increasingly complex, different graph models have been
proposed, giving rise to diverse motif patterns. These variations introduce
unique computational challenges that require specialized algorithms tailored to
specific motifs within different graph structures. This survey provides a
comprehensive and structured overview of motif counting techniques across
general graphs, heterogeneous graphs, and hypergraphs. We categorize existing
algorithms according to their underlying computational strategies, emphasizing
key similarities and distinctions. In addition to reviewing current
methodologies, we examine their strengths, limitations, and computational
trade-offs. Furthermore, we explore future directions in motif counting,
including scalable implementations to improve efficiency in large-scale
networks, algorithmic adaptations for dynamic, temporal, and attributed graphs,
and deeper integration with large language models (LLMs) and graph-based
retrieval-augmented generation (GraphRAG). By offering a detailed analysis of
these approaches, this survey aims to support researchers and practitioners in
advancing motif counting for increasingly complex network data.",http://arxiv.org/abs/2503.19573v1,3/25/25,"Haozhe Yin, Kai Wang, Wenjie Zhang, Yizhang He, Ying Zhang, Xuemin Lin"
231,Parametric Retrieval Augmented Generation,"Retrieval-augmented generation (RAG) techniques have emerged as a promising
solution to enhance the reliability of large language models (LLMs) by
addressing issues like hallucinations, outdated knowledge, and domain
adaptation. In particular, existing RAG methods append relevant documents
retrieved from external corpus or databases to the input of LLMs to guide their
generation process, which we refer to as the in-context knowledge injection
method. While this approach is simple and often effective, it has inherent
limitations. Firstly, increasing the context length and number of relevant
documents can lead to higher computational overhead and degraded performance,
especially in complex reasoning tasks. More importantly, in-context knowledge
injection operates primarily at the input level, but LLMs store their internal
knowledge in their parameters. This gap fundamentally limits the capacity of
in-context methods. To this end, we introduce Parametric retrieval-augmented
generation (Parametric RAG), a new RAG paradigm that integrates external
knowledge directly into the parameters of feed-forward networks (FFN) of an LLM
through document parameterization. This approach not only saves online
computational costs by eliminating the need to inject multiple documents into
the LLMs' input context, but also deepens the integration of external knowledge
into the parametric knowledge space of the LLM. Experimental results
demonstrate that Parametric RAG substantially enhances both the effectiveness
and efficiency of knowledge augmentation in LLMs. Also, it can be combined with
in-context RAG methods to achieve even better performance.
  We have open-sourced all the code, data, and models in the following
anonymized GitHub link: https://github.com/oneal2000/PRAG",http://arxiv.org/abs/2501.15915v1,1/27/25,"Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, Yiqun Liu"
232,Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs,"Despite the superb performance in many tasks, large language models (LLMs)
bear the risk of generating hallucination or even wrong answers when confronted
with tasks that demand the accuracy of knowledge. The issue becomes even more
noticeable when addressing logic queries that require multiple logic reasoning
steps. On the other hand, knowledge graph (KG) based question answering methods
are capable of accurately identifying the correct answers with the help of
knowledge graph, yet its accuracy could quickly deteriorate when the knowledge
graph itself is sparse and incomplete. It remains a critical challenge on how
to integrate knowledge graph reasoning with LLMs in a mutually beneficial way
so as to mitigate both the hallucination problem of LLMs as well as the
incompleteness issue of knowledge graphs. In this paper, we propose
'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs
with knowledge graph based logic query reasoning. LGOT seamlessly combines
knowledge graph reasoning and LLMs, effectively breaking down complex logic
queries into easy to answer subquestions. Through the utilization of both
knowledge graph reasoning and LLMs, it successfully derives answers for each
subquestion. By aggregating these results and selecting the highest quality
candidate answers for each step, LGOT achieves accurate results to complex
questions. Our experimental findings demonstrate substantial performance
enhancements, with up to 20% improvement over ChatGPT.",http://arxiv.org/abs/2404.04264v5,3/17/24,"Lihui Liu, Zihao Wang, Ruizhong Qiu, Yikun Ban, Eunice Chan, Yangqiu Song, Jingrui He, Hanghang Tong"
233,Enhancing LLM Generation with Knowledge Hypergraph for Evidence-Based Medicine,"Evidence-based medicine (EBM) plays a crucial role in the application of
large language models (LLMs) in healthcare, as it provides reliable support for
medical decision-making processes. Although it benefits from current
retrieval-augmented generation~(RAG) technologies, it still faces two
significant challenges: the collection of dispersed evidence and the efficient
organization of this evidence to support the complex queries necessary for EBM.
To tackle these issues, we propose using LLMs to gather scattered evidence from
multiple sources and present a knowledge hypergraph-based evidence management
model to integrate these evidence while capturing intricate relationships.
Furthermore, to better support complex queries, we have developed an
Importance-Driven Evidence Prioritization (IDEP) algorithm that utilizes the
LLM to generate multiple evidence features, each with an associated importance
score, which are then used to rank the evidence and produce the final retrieval
results. Experimental results from six datasets demonstrate that our approach
outperforms existing RAG techniques in application domains of interest to EBM,
such as medical quizzing, hallucination detection, and decision support.
Testsets and the constructed knowledge graph can be accessed at
\href{https://drive.google.com/file/d/1WJ9QTokK3MdkjEmwuFQxwH96j_Byawj_/view?usp=drive_link}{https://drive.google.com/rag4ebm}.",http://arxiv.org/abs/2503.16530v1,3/18/25,"Chengfeng Dou, Ying Zhang, Zhi Jin, Wenpin Jiao, Haiyan Zhao, Yongqiang Zhao, Zhengwei Tao"
234,AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow,"Simulated patient systems play a crucial role in modern medical education and
research, providing safe, integrative learning environments and enabling
clinical decision-making simulations. Large Language Models (LLM) could advance
simulated patient systems by replicating medical conditions and patient-doctor
interactions with high fidelity and low cost. However, ensuring the
effectiveness and trustworthiness of these systems remains a challenge, as they
require a large, diverse, and precise patient knowledgebase, along with a
robust and stable knowledge diffusion to users. Here, we developed AIPatient,
an advanced simulated patient system with AIPatient Knowledge Graph (AIPatient
KG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning
RAG) agentic workflow as the generation backbone. AIPatient KG samples data
from Electronic Health Records (EHRs) in the Medical Information Mart for
Intensive Care (MIMIC)-III database, producing a clinically diverse and
relevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).
Reasoning RAG leverages six LLM powered agents spanning tasks including
retrieval, KG query generation, abstraction, checker, rewrite, and
summarization. This agentic framework reaches an overall accuracy of 94.15% in
EHR-based medical Question Answering (QA), outperforming benchmarks that use
either no agent or only partial agent integration. Our system also presents
high readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade
5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value
0.782, p>0.1). The promising performance of the AIPatient system highlights its
potential to support a wide range of applications, including medical education,
model evaluation, and system integration.",http://arxiv.org/abs/2409.18924v2,9/27/24,"Huizi Yu, Jiayan Zhou, Lingyao Li, Shan Chen, Jack Gallifant, Anye Shi, Xiang Li, Wenyue Hua, Mingyu Jin, Guang Chen, Yang Zhou, Zhao Li, Trisha Gupte, MingLi Chen, Zahra Azizi, Yongfeng Zhang, Themistocles L Assimes, Xin Ma, Danielle S Bitterman, Lin Lu, Lizhou Fan"
235,PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering,"Existing work on Temporal Question Answering (TQA) has predominantly focused
on questions anchored to specific timestamps or events (e.g. ""Who was the US
president in 1970?""). Little work has studied questions whose temporal context
is relative to the present time (e.g. ""Who was the previous US president?""). We
refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses
unique challenges: (1) large language models (LLMs) may have outdated
knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are
hard to reason, (3) multi-hop reasoning may be required, and (4) the gold
answers of benchmarks must be continuously updated. To address these
challenges, we introduce the PAT-Questions benchmark, which includes single and
multi-hop temporal questions. The answers in PAT-Questions can be automatically
refreshed by re-running SPARQL queries on a knowledge graph, if available. We
evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model
(TEMPREASON-T5) on PAT-Questions through direct prompting and
retrieval-augmented generation (RAG). The results highlight the limitations of
existing solutions in PATQA and motivate the need for new methods to improve
PATQA reasoning capabilities.",http://arxiv.org/abs/2402.11034v2,2/16/24,"Jannat Ara Meem, Muhammad Shihab Rashid, Yue Dong, Vagelis Hristidis"
236,GRADA: Graph-based Reranker against Adversarial Documents Attack,"Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large
language models (LLMs) by integrating external knowledge from retrieved
documents, thereby overcoming the limitations of models' static intrinsic
knowledge. However, these systems are susceptible to adversarial attacks that
manipulate the retrieval process by introducing documents that are adversarial
yet semantically similar to the query. Notably, while these adversarial
documents resemble the query, they exhibit weak similarity to benign documents
in the retrieval set. Thus, we propose a simple yet effective Graph-based
Reranking against Adversarial Document Attacks (GRADA) framework aiming at
preserving retrieval quality while significantly reducing the success of
adversaries. Our study evaluates the effectiveness of our approach through
experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b,
Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with
results from the Natural Questions dataset demonstrating up to an 80% reduction
in attack success rates while maintaining minimal loss in accuracy.",http://arxiv.org/abs/2505.07546v1,5/12/25,"Jingjie Zheng, Aryo Pradipta Gema, Giwon Hong, Xuanli He, Pasquale Minervini, Youcheng Sun, Qiongkai Xu"
237,Multi-Paragraph Reasoning with Knowledge-enhanced Graph Neural Network,"Multi-paragraph reasoning is indispensable for open-domain question answering
(OpenQA), which receives less attention in the current OpenQA systems. In this
work, we propose a knowledge-enhanced graph neural network (KGNN), which
performs reasoning over multiple paragraphs with entities. To explicitly
capture the entities' relatedness, KGNN utilizes relational facts in knowledge
graph to build the entity graph. The experimental results show that KGNN
outperforms in both distractor and full wiki settings than baselines methods on
HotpotQA dataset. And our further analysis illustrates KGNN is effective and
robust with more retrieved paragraphs.",http://arxiv.org/abs/1911.02170v1,11/6/19,"Deming Ye, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Maosong Sun"
238,Zero-shot Slot Filling with DPR and RAG,"The ability to automatically extract Knowledge Graphs (KG) from a given
collection of documents is a long-standing problem in Artificial Intelligence.
One way to assess this capability is through the task of slot filling. Given an
entity query in form of [Entity, Slot, ?], a system is asked to `fill' the slot
by generating or extracting the missing value from a relevant passage or
passages. This capability is crucial to create systems for automatic knowledge
base population, which is becoming in ever-increasing demand, especially in
enterprise applications. Recently, there has been a promising direction in
evaluating language models in the same way we would evaluate knowledge bases,
and the task of slot filling is the most suitable to this intent. The recent
advancements in the field try to solve this task in an end-to-end fashion using
retrieval-based language models. Models like Retrieval Augmented Generation
(RAG) show surprisingly good performance without involving complex information
extraction pipelines. However, the results achieved by these models on the two
slot filling tasks in the KILT benchmark are still not at the level required by
real-world information extraction systems. In this paper, we describe several
strategies we adopted to improve the retriever and the generator of RAG in
order to make it a better slot filler. Our KGI0 system (available at
https://github.com/IBM/retrieve-write-slot-filling) reached the top-1 position
on the KILT leaderboard on both T-REx and zsRE dataset with a large margin.",http://arxiv.org/abs/2104.08610v1,4/17/21,"Michael Glass, Gaetano Rossiello, Alfio Gliozzo"
239,WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models,"Retrieval Augmented Generation (RAG) has gained widespread adoption owing to
its capacity to empower large language models (LLMs) to integrate external
knowledge. However, existing RAG frameworks are primarily designed for
text-based LLMs and rely on Automatic Speech Recognition to process speech
input, which discards crucial audio information, risks transcription errors,
and increases computational overhead. Therefore, we introduce WavRAG, the first
retrieval augmented generation framework with native, end-to-end audio support.
WavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw
audio for both embedding and retrieval. 2) WavRAG integrates audio and text
into a unified knowledge representation. Specifically, we propose the
WavRetriever to facilitate the retrieval from a text-audio hybrid knowledge
base, and further enhance the in-context capabilities of spoken dialogue models
through the integration of chain-of-thought reasoning. In comparison to
state-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval
performance while delivering a 10x acceleration. Furthermore, WavRAG's unique
text-audio hybrid retrieval capability extends the boundaries of RAG to the
audio modality.",http://arxiv.org/abs/2502.14727v1,2/20/25,"Yifu Chen, Shengpeng Ji, Haoxiao Wang, Ziqing Wang, Siyu Chen, Jinzheng He, Jin Xu, Zhou Zhao"
240,Accelerating Manufacturing Scale-Up from Material Discovery Using Agentic Web Navigation and Retrieval-Augmented AI for Process Engineering Schematics Design,"Process Flow Diagrams (PFDs) and Process and Instrumentation Diagrams (PIDs)
are critical tools for industrial process design, control, and safety. However,
the generation of precise and regulation-compliant diagrams remains a
significant challenge, particularly in scaling breakthroughs from material
discovery to industrial production in an era of automation and digitalization.
This paper introduces an autonomous agentic framework to address these
challenges through a twostage approach involving knowledge acquisition and
generation. The framework integrates specialized sub-agents for retrieving and
synthesizing multimodal data from publicly available online sources and
constructs ontological knowledge graphs using a Graph Retrieval-Augmented
Generation (Graph RAG) paradigm. These capabilities enable the automation of
diagram generation and open-domain question answering (ODQA) tasks with high
contextual accuracy. Extensive empirical experiments demonstrate the frameworks
ability to deliver regulation-compliant diagrams with minimal expert
intervention, highlighting its practical utility for industrial applications.",http://arxiv.org/abs/2412.05937v1,12/8/24,"Sakhinana Sagar Srinivas, Akash Das, Shivam Gupta, Venkataramana Runkana"
241,CAPRAG: A Large Language Model Solution for Customer Service and Automatic Reporting using Vector and Graph Retrieval-Augmented Generation,"The introduction of new features and services in the banking sector often
overwhelms customers, creating an opportunity for banks to enhance user
experience through financial chatbots powered by large language models (LLMs).
We initiated an AI agent designed to provide customers with relevant
information about banking services and insights from annual reports. We
proposed a hybrid Customer Analysis Pipeline Retrieval-Augmented Generation
(CAPRAG) that effectively addresses both relationship-based and contextual
queries, thereby improving customer engagement in the digital banking
landscape. To implement this, we developed a processing pipeline to refine text
data, which we utilized in two main frameworks: Vector RAG and Graph RAG. This
dual approach enables us to populate both vector and graph databases with
processed data for efficient retrieval. The Cypher query component is employed
to effectively query the graph database. When a user submits a query, it is
first expanded by a query expansion module before being routed to construct a
final query from the hybrid Knowledge Base (KB). This final query is then sent
to an open-source LLM for response generation. Overall, our innovative,
designed to international banks, serves bank's customers in an increasingly
complex digital environment, enhancing clarity and accessibility of
information.",http://arxiv.org/abs/2501.13993v1,1/23/25,"Hamza Landolsi, Kais Letaief, Nizar Taghouti, Ines AbdeljaouedTej"
242,Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented Generation,"This paper addresses the need for improved precision in existing
knowledge-enhanced question-answering frameworks, specifically
Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing
recall. We propose a multi-layer knowledge pyramid approach within the RAG
framework to achieve a better balance between precision and recall. The
knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),
and chunk-based raw text. We employ cross-layer augmentation techniques for
comprehensive knowledge coverage and dynamic updates of the Ontology schema and
instances. To ensure compactness, we utilize cross-layer filtering methods for
knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall
model for retrieval, starting from the top of the pyramid and progressing down
until a confident answer is obtained. We introduce two benchmarks for
domain-specific knowledge retrieval, one in the academic domain and the other
in the financial domain. The effectiveness of the methods has been validated
through comprehensive experiments by outperforming 19 SOTA methods. An
encouraging observation is that the proposed method has augmented the GPT-4,
providing 395% F1 gain by improving its performance from 0.1636 to 0.8109.",http://arxiv.org/abs/2407.21276v3,7/31/24,"Rubing Chen, Xulu Zhang, Jiaxin Wu, Wenqi Fan, XiaoYong Wei, Qing Li"
243,RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision,"Retrieval-augmented generation (RAG) has shown great potential for
knowledge-intensive tasks, but its traditional architectures rely on static
retrieval, limiting their effectiveness for complex questions that require
sequential information-seeking. While agentic reasoning and search offer a more
adaptive approach, most existing methods depend heavily on prompt engineering.
In this work, we introduce RAG-Gym, a unified optimization framework that
enhances information-seeking agents through fine-grained process supervision at
each search step. We also propose ReSearch, a novel agent architecture that
synergizes answer reasoning and search query generation within the RAG-Gym
framework. Experiments on four challenging datasets show that RAG-Gym improves
performance by up to 25.6\% across various agent architectures, with ReSearch
consistently outperforming existing baselines. Further analysis highlights the
effectiveness of advanced LLMs as process reward judges and the transferability
of trained reward models as verifiers for different LLMs. Additionally, we
examine the scaling properties of training and inference in agentic RAG. The
project homepage is available at https://rag-gym.github.io/.",http://arxiv.org/abs/2502.13957v1,2/19/25,"Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang"
244,U-NIAH: Unified RAG and LLM Evaluation for Long Context Needle-In-A-Haystack,"Recent advancements in Large Language Models (LLMs) have expanded their
context windows to unprecedented lengths, sparking debates about the necessity
of Retrieval-Augmented Generation (RAG). To address the fragmented evaluation
paradigms and limited cases in existing Needle-in-a-Haystack (NIAH), this paper
introduces U-NIAH, a unified framework that systematically compares LLMs and
RAG methods in controlled long context settings. Our framework extends beyond
traditional NIAH by incorporating multi-needle, long-needle, and
needle-in-needle configurations, along with different retrieval settings, while
leveraging the synthetic Starlight Academy dataset-a fictional magical
universe-to eliminate biases from pre-trained knowledge. Through extensive
experiments, we investigate three research questions: (1) performance
trade-offs between LLMs and RAG, (2) error patterns in RAG, and (3) RAG's
limitations in complex settings. Our findings show that RAG significantly
enhances smaller LLMs by mitigating the ""lost-in-the-middle"" effect and
improving robustness, achieving an 82.58% win-rate over LLMs. However, we
observe that retrieval noise and reverse chunk ordering degrade performance,
while surprisingly, advanced reasoning LLMs exhibit reduced RAG compatibility
due to sensitivity to semantic distractors. We identify typical error patterns
including omission due to noise, hallucination under high noise critical
condition, and self-doubt behaviors. Our work not only highlights the
complementary roles of RAG and LLMs, but also provides actionable insights for
optimizing deployments. Code: https://github.com/Tongji-KGLLM/U-NIAH.",http://arxiv.org/abs/2503.00353v1,3/1/25,"Yunfan Gao, Yun Xiong, Wenlong Wu, Zijing Huang, Bohan Li, Haofen Wang"
245,Reinforcement Knowledge Graph Reasoning for Explainable Recommendation,"Recent advances in personalized recommendation have sparked great interest in
the exploitation of rich structured information provided by knowledge graphs.
Unlike most existing approaches that only focus on leveraging knowledge graphs
for more accurate recommendation, we perform explicit reasoning with knowledge
for decision making so that the recommendations are generated and supported by
an interpretable causal inference procedure. To this end, we propose a method
called Policy-Guided Path Reasoning (PGPR), which couples recommendation and
interpretability by providing actual paths in a knowledge graph. Our
contributions include four aspects. We first highlight the significance of
incorporating knowledge graphs into recommendation to formally define and
interpret the reasoning process. Second, we propose a reinforcement learning
(RL) approach featuring an innovative soft reward strategy, user-conditional
action pruning and a multi-hop scoring function. Third, we design a
policy-guided graph search algorithm to efficiently and effectively sample
reasoning paths for recommendation. Finally, we extensively evaluate our method
on several large-scale real-world benchmark datasets, obtaining favorable
results compared with state-of-the-art methods.",http://arxiv.org/abs/1906.05237v1,6/12/19,"Yikun Xian, Zuohui Fu, S Muthukrishnan, Gerard de Melo, Yongfeng Zhang"
246,ForPKG: A Framework for Constructing Forestry Policy Knowledge Graph and Application Analysis,"A policy knowledge graph can provide decision support for tasks such as
project compliance, policy analysis, and intelligent question answering, and
can also serve as an external knowledge base to assist the reasoning process of
related large language models. Although there have been many related works on
knowledge graphs, there is currently a lack of research on the construction
methods of policy knowledge graphs. This paper, focusing on the forestry field,
designs a complete policy knowledge graph construction framework, including:
firstly, proposing a fine-grained forestry policy domain ontology; then,
proposing an unsupervised policy information extraction method, and finally,
constructing a complete forestry policy knowledge graph. The experimental
results show that the proposed ontology has good expressiveness and
extensibility, and the policy information extraction method proposed in this
paper achieves better results than other unsupervised methods. Furthermore, by
analyzing the application of the knowledge graph in the
retrieval-augmented-generation task of the large language models, the practical
application value of the knowledge graph in the era of large language models is
confirmed. The knowledge graph resource will be released on an open-source
platform and can serve as the basic knowledge base for forestry policy-related
intelligent systems. It can also be used for academic research. In addition,
this study can provide reference and guidance for the construction of policy
knowledge graphs in other fields. Our data is provided on Github
https://github.com/luozhongze/ForPKG.",http://arxiv.org/abs/2411.11090v2,11/17/24,"Jingyun Sun, Zhongze Luo"
247,Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge Reasoning,"Incorporating external knowledge in large language models (LLMs) enhances
their utility across diverse applications, but existing methods have
trade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via
similarity search, but key information may fall outside top ranked results.
Long-context models can process multiple documents but are computationally
expensive and limited by context window size. Inspired by students condensing
study material for open-book exams, we propose task-aware key-value (KV) cache
compression, which compresses external knowledge in a zero- or few-shot setup.
This enables LLMs to reason efficiently over a compacted representation of all
relevant information. Experiments show our approach outperforms both RAG and
task-agnostic compression methods. On LongBench v2, it improves accuracy by up
to 7 absolute points over RAG with a 30x compression rate, while reducing
inference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG
performs well when sparse evidence suffices, whereas task-aware compression is
superior for broad knowledge tasks.",http://arxiv.org/abs/2503.04973v1,3/6/25,"Giulio Corallo, Orion Weller, Fabio Petroni, Paolo Papotti"
248,Removal of Hallucination on Hallucination: Debate-Augmented RAG,"Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating
external knowledge, yet it introduces a critical issue: erroneous or biased
retrieval can mislead generation, compounding hallucinations, a phenomenon we
term Hallucination on Hallucination. To address this, we propose
Debate-Augmented RAG (DRAG), a training-free framework that integrates
Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages.
In retrieval, DRAG employs structured debates among proponents, opponents, and
judges to refine retrieval quality and ensure factual reliability. In
generation, DRAG introduces asymmetric information roles and adversarial
debates, enhancing reasoning robustness and mitigating factual inconsistencies.
Evaluations across multiple tasks demonstrate that DRAG improves retrieval
reliability, reduces RAG-induced hallucinations, and significantly enhances
overall factual accuracy. Our code is available at
https://github.com/Huenao/Debate-Augmented-RAG.",http://arxiv.org/abs/2505.18581v1,5/24/25,"Wentao Hu, Wengyu Zhang, Yiyang Jiang, Chen Jason Zhang, Xiaoyong Wei, Qing Li"
249,A RAG Approach for Generating Competency Questions in Ontology Engineering,"Competency question (CQ) formulation is central to several ontology
development and evaluation methodologies. Traditionally, the task of crafting
these competency questions heavily relies on the effort of domain experts and
knowledge engineers which is often time-consuming and labor-intensive. With the
emergence of Large Language Models (LLMs), there arises the possibility to
automate and enhance this process. Unlike other similar works which use
existing ontologies or knowledge graphs as input to LLMs, we present a
retrieval-augmented generation (RAG) approach that uses LLMs for the automatic
generation of CQs given a set of scientific papers considered to be a domain
knowledge base. We investigate its performance and specifically, we study the
impact of different number of papers to the RAG and different temperature
setting of the LLM. We conduct experiments using GPT-4 on two domain ontology
engineering tasks and compare results against ground-truth CQs constructed by
domain experts. Empirical assessments on the results, utilizing evaluation
metrics (precision and consistency), reveal that compared to zero-shot
prompting, adding relevant domain knowledge to the RAG improves the performance
of LLMs on generating CQs for concrete ontology engineering tasks.",http://arxiv.org/abs/2409.08820v2,9/13/24,"Xueli Pan, Jacco van Ossenbruggen, Victor de Boer, Zhisheng Huang"
250,Real-time Spatial Retrieval Augmented Generation for Urban Environments,"The proliferation of Generative Artificial Ingelligence (AI), especially
Large Language Models, presents transformative opportunities for urban
applications through Urban Foundation Models. However, base models face
limitations, as they only contain the knowledge available at the time of
training, and updating them is both time-consuming and costly. Retrieval
Augmented Generation (RAG) has emerged in the literature as the preferred
approach for injecting contextual information into Foundation Models. It
prevails over techniques such as fine-tuning, which are less effective in
dynamic, real-time scenarios like those found in urban environments. However,
traditional RAG architectures, based on semantic databases, knowledge graphs,
structured data, or AI-powered web searches, do not fully meet the demands of
urban contexts. Urban environments are complex systems characterized by large
volumes of interconnected data, frequent updates, real-time processing
requirements, security needs, and strong links to the physical world. This work
proposes a real-time spatial RAG architecture that defines the necessary
components for the effective integration of generative AI into cities,
leveraging temporal and spatial filtering capabilities through linked data. The
proposed architecture is implemented using FIWARE, an ecosystem of software
components to develop smart city solutions and digital twins. The design and
implementation are demonstrated through the use case of a tourism assistant in
the city of Madrid. The use case serves to validate the correct integration of
Foundation Models through the proposed RAG architecture.",http://arxiv.org/abs/2505.02271v1,5/4/25,"David Nazareno Campo, Javier Conde, lvaro Alonso, Gabriel Huecas, Joaqun Salvacha, Pedro Reviriego"
251,Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation,"Large Language Models (LLMs) struggle with hallucinations and outdated
knowledge due to their reliance on static training data. Retrieval-Augmented
Generation (RAG) mitigates these issues by integrating external dynamic
information enhancing factual and updated grounding. Recent advances in
multimodal learning have led to the development of Multimodal RAG,
incorporating multiple modalities such as text, images, audio, and video to
enhance the generated outputs. However, cross-modal alignment and reasoning
introduce unique challenges to Multimodal RAG, distinguishing it from
traditional unimodal RAG. This survey offers a structured and comprehensive
analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,
evaluation, methodologies, and innovations in retrieval, fusion, augmentation,
and generation. We precisely review training strategies, robustness
enhancements, and loss functions, while also exploring the diverse Multimodal
RAG scenarios. Furthermore, we discuss open challenges and future research
directions to support advancements in this evolving field. This survey lays the
foundation for developing more capable and reliable AI systems that effectively
leverage multimodal dynamic external knowledge bases. Resources are available
at https://github.com/llm-lab-org/Multimodal-RAG-Survey.",http://arxiv.org/abs/2502.08826v2,2/12/25,"Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, Ehsaneddin Asgari"
252,Retrieval-Augmented Machine Translation with Unstructured Knowledge,"Retrieval-augmented generation (RAG) introduces additional information to
enhance large language models (LLMs). In machine translation (MT), previous
work typically retrieves in-context examples from paired MT corpora, or
domain-specific knowledge from knowledge graphs, to enhance models' MT ability.
However, a large amount of world knowledge is organized in unstructured
documents, and might not be fully paired across different languages. In this
paper, we study retrieval-augmented MT using unstructured documents.
Specifically, we build RAGtrans, the first benchmark to train and evaluate
LLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples
collected via GPT-4o and human translators. Besides, documents from different
languages are also provided to supply the knowledge to these samples. Based on
RAGtrans, we further propose a multi-task training method to teach LLMs how to
use information from multilingual documents during their translation. The
method uses existing multilingual corpora to create auxiliary training
objectives without additional labeling requirements. Extensive experiments show
that the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.",http://arxiv.org/abs/2412.04342v1,12/5/24,"Jiaan Wang, Fandong Meng, Yingxue Zhang, Jie Zhou"
253,Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering,"Knowledge retrieval and reasoning are two key stages in multi-hop question
answering (QA) at web scale. Existing approaches suffer from low confidence
when retrieving evidence facts to fill the knowledge gap and lack transparent
reasoning process. In this paper, we propose a new framework to exploit more
valid facts while obtaining explainability for multi-hop QA by dynamically
constructing a semantic graph and reasoning over it. We employ Abstract Meaning
Representation (AMR) as semantic graph representation. Our framework contains
three new ideas: (a) {\tt AMR-SG}, an AMR-based Semantic Graph, constructed by
candidate fact AMRs to uncover any hop relations among question, answer and
multiple facts. (b) A novel path-based fact analytics approach exploiting {\tt
AMR-SG} to extract active facts from a large fact pool to answer questions. (c)
A fact-level relation modeling leveraging graph convolution network (GCN) to
guide the reasoning process. Results on two scientific multi-hop QA datasets
show that we can surpass recent approaches including those using additional
knowledge graphs while maintaining high explainability on OpenBookQA and
achieve a new state-of-the-art result on ARC-Challenge in a computationally
practicable setting.",http://arxiv.org/abs/2105.11776v1,5/25/21,"Weiwen Xu, Huihui Zhang, Deng Cai, Wai Lam"
254,CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models,"Large language models (LLMs) have shown great promise in machine translation,
but they still struggle with contextually dependent terms, such as new or
domain-specific words. This leads to inconsistencies and errors that are
difficult to address. Existing solutions often depend on manual identification
of such terms, which is impractical given the complexity and evolving nature of
language. While Retrieval-Augmented Generation (RAG) could provide some
assistance, its application to translation is limited by issues such as
hallucinations from information overload. In this paper, we propose CRAT, a
novel multi-agent translation framework that leverages RAG and
causality-enhanced self-reflection to address these challenges. This framework
consists of several specialized agents: the Unknown Terms Identification agent
detects unknown terms within the context, the Knowledge Graph (KG) Constructor
agent extracts relevant internal knowledge about these terms and retrieves
bilingual information from external sources, the Causality-enhanced Judge agent
validates the accuracy of the information, and the Translator agent
incorporates the refined information into the final output. This automated
process allows for more precise and consistent handling of key terms during
translation. Our results show that CRAT significantly improves translation
accuracy, particularly in handling context-sensitive terms and emerging
vocabulary.",http://arxiv.org/abs/2410.21067v1,10/28/24,"Meiqi Chen, Fandong Meng, Yingxue Zhang, Yan Zhang, Jie Zhou"
255,Fast or Better? Balancing Accuracy and Cost in Retrieval-Augmented Generation with Flexible User Control,"Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to
mitigate large language model (LLM) hallucinations by incorporating external
knowledge retrieval. However, existing RAG frameworks often apply retrieval
indiscriminately,leading to inefficiencies-over-retrieving when unnecessary or
failing to retrieve iteratively when required for complex reasoning. Recent
adaptive retrieval strategies, though adaptively navigates these retrieval
strategies, predict only based on query complexity and lacks user-driven
flexibility, making them infeasible for diverse user application needs. In this
paper, we introduce a novel user-controllable RAG framework that enables
dynamic adjustment of the accuracy-cost trade-off. Our approach leverages two
classifiers: one trained to prioritize accuracy and another to prioritize
retrieval efficiency. Via an interpretable control parameter $\alpha$, users
can seamlessly navigate between minimal-cost retrieval and high-accuracy
retrieval based on their specific requirements. We empirically demonstrate that
our approach effectively balances accuracy, retrieval cost, and user
controllability, making it a practical and adaptable solution for real-world
applications.",http://arxiv.org/abs/2502.12145v1,2/17/25,"Jinyan Su, Jennifer Healey, Preslav Nakov, Claire Cardie"
256,"A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications","A graph is a fundamental data model to represent various entities and their
complex relationships in society and nature, such as social networks,
transportation networks, financial networks, and biomedical systems. Recently,
large language models (LLMs) have showcased a strong generalization ability to
handle various NLP and multi-mode tasks to answer users' arbitrary questions
and specific-domain content generation. Compared with graph learning models,
LLMs enjoy superior advantages in addressing the challenges of generalizing
graph tasks by eliminating the need for training graph learning models and
reducing the cost of manual annotation. In this survey, we conduct a
comprehensive investigation of existing LLM studies on graph data, which
summarizes the relevant graph analytics tasks solved by advanced LLM models and
points out the existing remaining challenges and future directions.
Specifically, we study the key problems of LLM-based generative graph analytics
(LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP),
LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based
applications. LLM-GQP focuses on an integration of graph analytics techniques
and LLM prompts, including graph understanding and knowledge graph (KG) based
augmented retrieval, while LLM-GIL focuses on learning and reasoning over
graphs, including graph learning, graph-formed reasoning and graph
representation. We summarize the useful prompts incorporated into LLM to handle
different graph downstream tasks. Moreover, we give a summary of LLM model
evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM
models. We also explore open problems and future directions in this exciting
interdisciplinary research area of LLMs and graph analytics.",http://arxiv.org/abs/2404.14809v1,4/23/24,"Wenbo Shang, Xin Huang"
257,Neural-Symbolic Reasoning over Knowledge Graph for Multi-stage Explainable Recommendation,"Recent work on recommender systems has considered external knowledge graphs
as valuable sources of information, not only to produce better recommendations
but also to provide explanations of why the recommended items were chosen. Pure
rule-based symbolic methods provide a transparent reasoning process over
knowledge graph but lack generalization ability to unseen examples, while deep
learning models enhance powerful feature representation ability but are hard to
interpret. Moreover, direct reasoning over large-scale knowledge graph can be
costly due to the huge search space of pathfinding. We approach the problem
through a novel coarse-to-fine neural symbolic reasoning method called NSER. It
first generates a coarse-grained explanation to capture abstract user
behavioral pattern, followed by a fined-grained explanation accompanying with
explicit reasoning paths and recommendations inferred from knowledge graph. We
extensively experiment on four real-world datasets and observe substantial
gains of recommendation performance compared with state-of-the-art methods as
well as more diversified explanations in different granularity.",http://arxiv.org/abs/2007.13207v1,7/26/20,"Yikun Xian, Zuohui Fu, Qiaoying Huang, S Muthukrishnan, Yongfeng Zhang"
258,Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph Reasoning,"Multimodal knowledge graphs (MKGs), which intuitively organize information in
various modalities, can benefit multiple practical downstream tasks, such as
recommendation systems, and visual question answering. However, most MKGs are
still far from complete, which motivates the flourishing of MKG reasoning
models. Recently, with the development of general artificial architectures, the
pretrained transformer models have drawn increasing attention, especially for
multimodal scenarios. However, the research of multimodal pretrained
transformer (MPT) for knowledge graph reasoning (KGR) is still at an early
stage. As the biggest difference between MKG and other multimodal data, the
rich structural information underlying the MKG still cannot be fully leveraged
in existing MPT models. Most of them only utilize the graph structure as a
retrieval map for matching images and texts connected with the same entity.
This manner hinders their reasoning performances. To this end, we propose the
graph Structure Guided Multimodal Pretrained Transformer for knowledge graph
reasoning, termed SGMPT. Specifically, the graph structure encoder is adopted
for structural feature encoding. Then, a structure-guided fusion module with
two different strategies, i.e., weighted summation and alignment constraint, is
first designed to inject the structural information into both the textual and
visual features. To the best of our knowledge, SGMPT is the first MPT model for
multimodal KGR, which mines the structural information underlying the knowledge
graph. Extensive experiments on FB15k-237-IMG and WN18-IMG, demonstrate that
our SGMPT outperforms existing state-of-the-art models, and prove the
effectiveness of the designed strategies.",http://arxiv.org/abs/2307.03591v1,7/6/23,"Ke Liang, Sihang Zhou, Yue Liu, Lingyuan Meng, Meng Liu, Xinwang Liu"
259,Retrieval Meets Reasoning: Even High-school Textbook Knowledge Benefits Multimodal Reasoning,"Large language models equipped with retrieval-augmented generation (RAG)
represent a burgeoning field aimed at enhancing answering capabilities by
leveraging external knowledge bases. Although the application of RAG with
language-only models has been extensively explored, its adaptation into
multimodal vision-language models remains nascent. Going beyond mere answer
generation, the primary goal of multimodal RAG is to cultivate the models'
ability to reason in response to relevant queries. To this end, we introduce a
novel multimodal RAG framework named RMR (Retrieval Meets Reasoning). The RMR
framework employs a bi-modal retrieval module to identify the most relevant
question-answer pairs, which then serve as scaffolds for the multimodal
reasoning process. This training-free approach not only encourages the model to
engage deeply with the reasoning processes inherent in the retrieved content
but also facilitates the generation of answers that are precise and richly
interpretable. Surprisingly, utilizing solely the ScienceQA dataset, collected
from elementary and high school science curricula, RMR significantly boosts the
performance of various vision-language models across a spectrum of benchmark
datasets, including A-OKVQA, MMBench, and SEED. These outcomes highlight the
substantial potential of our multimodal retrieval and reasoning mechanism to
improve the reasoning capabilities of vision-language models.",http://arxiv.org/abs/2405.20834v1,5/31/24,"Cheng Tan, Jingxuan Wei, Linzhuang Sun, Zhangyang Gao, Siyuan Li, Bihui Yu, Ruifeng Guo, Stan Z Li"
260,Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability,"Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive domains.
However, although RAG achieved successes across distinct domains, there are
still some unsolved challenges: 1) Effectiveness. Existing research mainly
focuses on developing more powerful RAG retrievers, but how to enhance the
generator's (LLM's) ability to utilize the retrieved information for reasoning
and generation? 2) Transparency. Most RAG methods ignore which retrieved
content actually contributes to the reasoning process, resulting in a lack of
interpretability and visibility. To address this, we propose ARENA
(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator
framework trained via reinforcement learning (RL) with our proposed rewards.
Based on the structured generation and adaptive reward calculation, our
RL-based training enables the model to identify key evidence, perform
structured reasoning, and generate answers with interpretable decision traces.
Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments
with various RAG baselines demonstrate that our model achieves 10-30%
improvements on all multi-hop QA datasets, which is comparable with the SOTA
Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses
show that ARENA has strong flexibility to be adopted on new datasets without
extra training. Our models and codes are publicly released.",http://arxiv.org/abs/2505.13258v1,5/19/25,"Jingyi Ren, Yekun Xu, Xiaolong Wang, Weitao Li, Weizhi Ma, Yang Liu"
261,CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering,"Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM)
output by providing prior knowledge as context to input. This is beneficial for
knowledge-intensive and expert reliant tasks, including legal
question-answering, which require evidence to validate generated text outputs.
We highlight that Case-Based Reasoning (CBR) presents key opportunities to
structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG,
where CBR cycle's initial retrieval stage, its indexing vocabulary, and
similarity knowledge containers are used to enhance LLM queries with
contextually relevant cases. This integration augments the original LLM query,
providing a richer prompt. We present an evaluation of CBR-RAG, and examine
different representations (i.e. general and domain-specific embeddings) and
methods of comparison (i.e. inter, intra and hybrid similarity) on the task of
legal question-answering. Our results indicate that the context provided by
CBR's case reuse enforces similarity between relevant components of the
questions and the evidence base leading to significant improvements in the
quality of generated answers.",http://arxiv.org/abs/2404.04302v1,4/4/24,"Nirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawardena, Kyle Martin, Stewart Massie, Ikechukwu NkisiOrji, Ruvan Weerasinghe, Anne Liret, Bruno Fleisch"
262,FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction,"Narrative construction is the process of representing disparate event
information into a logical plot structure that models an end to end story.
Intelligence analysis is an example of a domain that can benefit tremendously
from narrative construction techniques, particularly in aiding analysts during
the largely manual and costly process of synthesizing event information into
comprehensive intelligence reports. Manual intelligence report generation is
often prone to challenges such as integrating dynamic event information,
writing fine-grained queries, and closing information gaps. This motivates the
development of a system that retrieves and represents critical aspects of
events in a form that aids in automatic generation of intelligence reports.
  We introduce a Retrieval Augmented Generation (RAG) approach to augment
prompting of an autoregressive decoder by retrieving structured information
asserted in a knowledge graph to generate targeted information based on a
narrative plot model. We apply our approach to the problem of neural
intelligence report generation and introduce FABULA, framework to augment
intelligence analysis workflows using RAG. An analyst can use FABULA to query
an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be
used to augment prompting of a Large Language Model (LLM) during intelligence
report generation. Our evaluation studies show that the plot points included in
the generated intelligence reports have high semantic relevance, high
coherency, and low data redundancy.",http://arxiv.org/abs/2310.13848v2,10/20/23,"Priyanka Ranade, Anupam Joshi"
263,Fine-Grained Retrieval-Augmented Generation for Visual Question Answering,"Visual Question Answering (VQA) focuses on providing answers to natural
language questions by utilizing information from images. Although cutting-edge
multimodal large language models (MLLMs) such as GPT-4o achieve strong
performance on VQA tasks, they frequently fall short in accessing
domain-specific or the latest knowledge. To mitigate this issue,
retrieval-augmented generation (RAG) leveraging external knowledge bases (KBs),
referred to as KB-VQA, emerges as a promising approach. Nevertheless,
conventional unimodal retrieval techniques, which translate images into textual
descriptions, often result in the loss of critical visual details. This study
presents fine-grained knowledge units, which merge textual snippets with entity
images stored in vector databases. Furthermore, we introduce a knowledge unit
retrieval-augmented generation framework (KU-RAG) that integrates fine-grained
retrieval with MLLMs. The proposed KU-RAG framework ensures precise retrieval
of relevant knowledge and enhances reasoning capabilities through a knowledge
correction chain. Experimental findings demonstrate that our approach
significantly boosts the performance of leading KB-VQA methods, achieving an
average improvement of approximately 3% and up to 11% in the best case.",http://arxiv.org/abs/2502.20964v2,2/28/25,"Zhengxuan Zhang, Yin Wu, Yuyu Luo, Nan Tang"
264,CRAG -- Comprehensive RAG Benchmark,"Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution to alleviate Large Language Model (LLM)'s deficiency in lack of
knowledge. Existing RAG datasets, however, do not adequately represent the
diverse and dynamic nature of real-world Question Answering (QA) tasks. To
bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual
question answering benchmark of 4,409 question-answer pairs and mock APIs to
simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a
diverse array of questions across five domains and eight question categories,
reflecting varied entity popularity from popular to long-tail, and temporal
dynamisms ranging from years to seconds. Our evaluation of this benchmark
highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve
<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the
accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%
of questions without any hallucination. CRAG also reveals much lower accuracy
in answering questions regarding facts with higher dynamism, lower popularity,
or higher complexity, suggesting future research directions. The CRAG benchmark
laid the groundwork for a KDD Cup 2024 challenge and attracted thousands of
participants and submissions. We commit to maintaining CRAG to serve research
communities in advancing RAG solutions and general QA solutions. CRAG is
available at https://github.com/facebookresearch/CRAG/.",http://arxiv.org/abs/2406.04744v2,6/7/24,"Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wentau Yih, Xin Luna Dong"
265,EPERM: An Evidence Path Enhanced Reasoning Model for Knowledge Graph Question and Answering,"Due to the remarkable reasoning ability, Large language models (LLMs) have
demonstrated impressive performance in knowledge graph question answering
(KGQA) tasks, which find answers to natural language questions over knowledge
graphs (KGs). To alleviate the hallucinations and lack of knowledge issues of
LLMs, existing methods often retrieve the question-related information from KGs
to enrich the input context. However, most methods focus on retrieving the
relevant information while ignoring the importance of different types of
knowledge in reasoning, which degrades their performance. To this end, this
paper reformulates the KGQA problem as a graphical model and proposes a
three-stage framework named the Evidence Path Enhanced Reasoning Model (EPERM)
for KGQA. In the first stage, EPERM uses the fine-tuned LLM to retrieve a
subgraph related to the question from the original knowledge graph. In the
second stage, EPERM filters out the evidence paths that faithfully support the
reasoning of the questions, and score their importance in reasoning. Finally,
EPERM uses the weighted evidence paths to reason the final answer. Since
considering the importance of different structural information in KGs for
reasoning, EPERM can improve the reasoning ability of LLMs in KGQA tasks.
Extensive experiments on benchmark datasets demonstrate that EPERM achieves
superior performances in KGQA tasks.",http://arxiv.org/abs/2502.16171v1,2/22/25,"Xiao Long, Liansheng Zhuang, Aodi Li, Minghong Yao, Shafei Wang"
266,Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning,"Large language models (LLMs) have demonstrated impressive reasoning abilities
in complex tasks. However, they lack up-to-date knowledge and experience
hallucinations during reasoning, which can lead to incorrect reasoning
processes and diminish their performance and trustworthiness. Knowledge graphs
(KGs), which capture vast amounts of facts in a structured format, offer a
reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM
reasoning methods only treat KGs as factual knowledge bases and overlook the
importance of their structural information for reasoning. In this paper, we
propose a novel method called reasoning on graphs (RoG) that synergizes LLMs
with KGs to enable faithful and interpretable reasoning. Specifically, we
present a planning-retrieval-reasoning framework, where RoG first generates
relation paths grounded by KGs as faithful plans. These plans are then used to
retrieve valid reasoning paths from the KGs for LLMs to conduct faithful
reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the
reasoning ability of LLMs through training but also allows seamless integration
with any arbitrary LLMs during inference. Extensive experiments on two
benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art
performance on KG reasoning tasks and generates faithful and interpretable
reasoning results.",http://arxiv.org/abs/2310.01061v2,10/2/23,"Linhao Luo, YuanFang Li, Gholamreza Haffari, Shirui Pan"
267,REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models,"The integration of multimodal Electronic Health Records (EHR) data has
significantly improved clinical predictive capabilities. Leveraging clinical
notes and multivariate time-series EHR, existing models often lack the medical
context relevent to clinical tasks, prompting the incorporation of external
knowledge, particularly from the knowledge graph (KG). Previous approaches with
KG knowledge have primarily focused on structured knowledge extraction,
neglecting unstructured data modalities and semantic high dimensional medical
knowledge. In response, we propose REALM, a Retrieval-Augmented Generation
(RAG) driven framework to enhance multimodal EHR representations that address
these limitations. Firstly, we apply Large Language Model (LLM) to encode long
context clinical notes and GRU model to encode time-series EHR data. Secondly,
we prompt LLM to extract task-relevant medical entities and match entities in
professionally labeled external knowledge graph (PrimeKG) with corresponding
medical knowledge. By matching and aligning with clinical standards, our
framework eliminates hallucinations and ensures consistency. Lastly, we propose
an adaptive multimodal fusion network to integrate extracted knowledge with
multimodal EHR data. Our extensive experiments on MIMIC-III mortality and
readmission tasks showcase the superior performance of our REALM framework over
baselines, emphasizing the effectiveness of each module. REALM framework
contributes to refining the use of multimodal EHR data in healthcare and
bridging the gap with nuanced medical context essential for informed clinical
predictions.",http://arxiv.org/abs/2402.07016v1,2/10/24,"Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, Chengwei Pan"
268,Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering,"Retrieval-augmented generation (RAG) has emerged as a promising approach to
enhance the performance of large language models (LLMs) in knowledge-intensive
tasks such as those from medical domain. However, the sensitive nature of the
medical domain necessitates a completely accurate and trustworthy system. While
existing RAG benchmarks primarily focus on the standard retrieve-answer
setting, they overlook many practical scenarios that measure crucial aspects of
a reliable medical system. This paper addresses this gap by providing a
comprehensive evaluation framework for medical question-answering (QA) systems
in a RAG setting for these situations, including sufficiency, integration, and
robustness. We introduce Medical Retrieval-Augmented Generation Benchmark
(MedRGB) that provides various supplementary elements to four medical QA
datasets for testing LLMs' ability to handle these specific scenarios.
Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art
commercial LLMs and open-source models across multiple retrieval conditions.
Our experimental results reveals current models' limited ability to handle
noise and misinformation in the retrieved documents. We further analyze the
LLMs' reasoning processes to provides valuable insights and future directions
for developing RAG systems in this critical medical domain.",http://arxiv.org/abs/2411.09213v1,11/14/24,"Nghia Trung Ngo, Chien Van Nguyen, Franck Dernoncourt, Thien Huu Nguyen"
269,LightPROF: A Lightweight Reasoning Framework for Large Language Model on Knowledge Graph,"Large Language Models (LLMs) have impressive capabilities in text
understanding and zero-shot reasoning. However, delays in knowledge updates may
cause them to reason incorrectly or produce harmful results. Knowledge Graphs
(KGs) provide rich and reliable contextual information for the reasoning
process of LLMs by structurally organizing and connecting a wide range of
entities and relations. Existing KG-based LLM reasoning methods only inject
KGs' knowledge into prompts in a textual form, ignoring its structural
information. Moreover, they mostly rely on close-source models or open-source
models with large parameters, which poses challenges to high resource
consumption. To address this, we propose a novel Lightweight and efficient
Prompt learning-ReasOning Framework for KGQA (LightPROF), which leverages the
full potential of LLMs to tackle complex reasoning tasks in a
parameter-efficient manner. Specifically, LightPROF follows a
""Retrieve-Embed-Reason process"", first accurately, and stably retrieving the
corresponding reasoning graph from the KG through retrieval module. Next,
through a Transformer-based Knowledge Adapter, it finely extracts and
integrates factual and structural information from the KG, then maps this
information to the LLM's token embedding space, creating an LLM-friendly prompt
to be used by the LLM for the final reasoning. Additionally, LightPROF only
requires training Knowledge Adapter and can be compatible with any open-source
LLM. Extensive experiments on two public KGQA benchmarks demonstrate that
LightPROF achieves superior performance with small-scale LLMs. Furthermore,
LightPROF shows significant advantages in terms of input token count and
reasoning time.",http://arxiv.org/abs/2504.03137v1,4/4/25,"Tu Ao, Yanhua Yu, Yuling Wang, Yang Deng, Zirui Guo, Liang Pang, Pinghui Wang, TatSeng Chua, Xiao Zhang, Zhen Cai"
270,KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities,"Document-level relation extraction (Doc-RE) aims to extract relations between
entities across multiple sentences. Therefore, Doc-RE requires more
comprehensive reasoning abilities like humans, involving complex cross-sentence
interactions between entities, contexts, and external general knowledge,
compared to the sentence-level RE. However, most existing Doc-RE methods focus
on optimizing single reasoning ability, but lack the ability to utilize
external knowledge for comprehensive reasoning on long documents. To solve
these problems, a knowledge retrieval augmented method, named KnowRA, was
proposed with comprehensive reasoning to autonomously determine whether to
accept external knowledge to assist DocRE. Firstly, we constructed a document
graph for semantic encoding and integrated the co-reference resolution model to
augment the co-reference reasoning ability. Then, we expanded the document
graph into a document knowledge graph by retrieving the external knowledge base
for common-sense reasoning and a novel knowledge filtration method was
presented to filter out irrelevant knowledge. Finally, we proposed the axis
attention mechanism to build direct and indirect associations with intermediary
entities for achieving cross-sentence logical reasoning. Extensive experiments
conducted on two datasets verified the effectiveness of our method compared to
the state-of-the-art baselines. Our code is available at
https://anonymous.4open.science/r/KnowRA.",http://arxiv.org/abs/2501.00571v4,12/31/24,"Chengcheng Mai, Yuxiang Wang, Ziyu Gong, Hanxiang Wang, Yihua Huang"
271,Logic-RAG: Augmenting Large Multimodal Models with Visual-Spatial Knowledge for Road Scene Understanding,"Large multimodal models (LMMs) are increasingly integrated into autonomous
driving systems for user interaction. However, their limitations in
fine-grained spatial reasoning pose challenges for system interpretability and
user trust. We introduce Logic-RAG, a novel Retrieval-Augmented Generation
(RAG) framework that improves LMMs' spatial understanding in driving scenarios.
Logic-RAG constructs a dynamic knowledge base (KB) about object-object
relationships in first-order logic (FOL) using a perception module, a
query-to-logic embedder, and a logical inference engine. We evaluated Logic-RAG
on visual-spatial queries using both synthetic and real-world driving videos.
When using popular LMMs (GPT-4V, Claude 3.5) as proxies for an autonomous
driving system, these models achieved only 55% accuracy on synthetic driving
scenes and under 75% on real-world driving scenes. Augmenting them with
Logic-RAG increased their accuracies to over 80% and 90%, respectively. An
ablation study showed that even without logical inference, the fact-based
context constructed by Logic-RAG alone improved accuracy by 15%. Logic-RAG is
extensible: it allows seamless replacement of individual components with
improved versions and enables domain experts to compose new knowledge in both
FOL and natural language. In sum, Logic-RAG addresses critical spatial
reasoning deficiencies in LMMs for autonomous driving applications. Code and
data are available at https://github.com/Imran2205/LogicRAG.",http://arxiv.org/abs/2503.12663v1,3/16/25,"Imran Kabir, Md Alimoor Reza, Syed Billah"
272,BioRAG: A RAG-LLM Framework for Biological Question Reasoning,"The question-answering system for Life science research, which is
characterized by the rapid pace of discovery, evolving insights, and complex
interactions among knowledge entities, presents unique challenges in
maintaining a comprehensive knowledge warehouse and accurate information
retrieval. To address these issues, we introduce BioRAG, a novel
Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)
framework. Our approach starts with parsing, indexing, and segmenting an
extensive collection of 22 million scientific papers as the basic knowledge,
followed by training a specialized embedding model tailored to this domain.
Additionally, we enhance the vector retrieval process by incorporating a
domain-specific knowledge hierarchy, which aids in modeling the intricate
interrelationships among each query and context. For queries requiring the most
current information, BioRAG deconstructs the question and employs an iterative
retrieval process incorporated with the search engine for step-by-step
reasoning. Rigorous experiments have demonstrated that our model outperforms
fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks
across multiple life science question-answering tasks.",http://arxiv.org/abs/2408.01107v2,8/2/24,"Chengrui Wang, Qingqing Long, Meng Xiao, Xunxin Cai, Chengjun Wu, Zhen Meng, Xuezhi Wang, Yuanchun Zhou"
273,"CancerKG.ORG A Web-scale, Interactive, Verifiable Knowledge Graph-LLM Hybrid for Assisting with Optimal Cancer Treatment and Care","Here, we describe one of the first Web-scale hybrid Knowledge Graph
(KG)-Large Language Model (LLM), populated with the latest peer-reviewed
medical knowledge on colorectal Cancer. It is currently being evaluated to
assist with both medical research and clinical information retrieval tasks at
Moffitt Cancer Center, which is one of the top Cancer centers in the U.S. and
in the world. Our hybrid is remarkable as it serves the user needs better than
just an LLM, KG or a search-engine in isolation. LLMs as is are known to
exhibit hallucinations and catastrophic forgetting as well as are trained on
outdated corpora. The state of the art KGs, such as PrimeKG, cBioPortal,
ChEMBL, NCBI, and other require manual curation, hence are quickly getting
stale. CancerKG is unsupervised and is capable of automatically ingesting and
organizing the latest medical findings. To alleviate the LLMs shortcomings, the
verified KG serves as a Retrieval Augmented Generation (RAG) guardrail.
CancerKG exhibits 5 different advanced user interfaces, each tailored to serve
different data modalities better and more convenient for the user.",http://arxiv.org/abs/2501.00223v1,12/31/24,"Michael Gubanov, Anna Pyayt, Aleksandra Karolak"
274,Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent,"Retrieval-augmented generation (RAG) is a common strategy to reduce
hallucinations in Large Language Models (LLMs). While reinforcement learning
(RL) can enable LLMs to act as search agents by activating retrieval
capabilities, existing ones often underutilize their internal knowledge. This
can lead to redundant retrievals, potential harmful knowledge conflicts, and
increased inference latency. To address these limitations, an efficient and
adaptive search agent capable of discerning optimal retrieval timing and
synergistically integrating parametric (internal) and retrieved (external)
knowledge is in urgent need. This paper introduces the Reinforced
Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could
indentify its own knowledge boundary and prioritize the utilization of internal
knowledge, resorting to external search only when internal knowledge is deemed
insufficient. This is achieved using a novel knowledge-boundary aware reward
function and a knowledge-boundary aware training dataset. These are designed
for internal-external knowledge synergy oriented RL, incentivizing the model to
deliver accurate answers, minimize unnecessary retrievals, and encourage
appropriate external searches when its own knowledge is lacking. Evaluations
across multiple knowledge reasoning tasks demonstrate that IKEA significantly
outperforms baseline methods, reduces retrieval frequency significantly, and
exhibits robust generalization capabilities.",http://arxiv.org/abs/2505.07596v1,5/12/25,"Ziyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, Kang Liu"
275,Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases,"Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for
answering queries by providing textual and structural knowledge. However,
current retrieval methods often retrieve these two types of knowledge in
isolation without considering their mutual reinforcement and some hybrid
methods even bypass structural retrieval entirely after neighboring
aggregation. To fill in this gap, we propose a Mixture of
Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge
via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR
generates textual planning graphs delineating the logic for answering queries.
Following planning graphs, in the Reasoning stage, MoR interweaves structural
traversal and textual matching to obtain candidates from TG-KBs. In the
Organizing stage, MoR further reranks fetched candidates based on their
structural trajectory. Extensive experiments demonstrate the superiority of MoR
in harmonizing structural and textual retrieval with insights, including uneven
retrieving performance across different query logics and the benefits of
integrating structural trajectories for candidate reranking. Our code is
available at https://github.com/Yoega/MoR.",http://arxiv.org/abs/2502.20317v3,2/27/25,"Yongjia Lei, Haoyu Han, Ryan A Rossi, Franck Dernoncourt, Nedim Lipka, Mahantesh M Halappanavar, Jiliang Tang, Yu Wang"
276,PASemiQA: Plan-Assisted Agent for Question Answering on Semi-Structured Data with Text and Relational Information,"Large language models (LLMs) have shown impressive abilities in answering
questions across various domains, but they often encounter hallucination issues
on questions that require professional and up-to-date knowledge. To address
this limitation, retrieval-augmented generation (RAG) techniques have been
proposed, which retrieve relevant information from external sources to inform
their responses. However, existing RAG methods typically focus on a single type
of external data, such as vectorized text database or knowledge graphs, and
cannot well handle real-world questions on semi-structured data containing both
text and relational information. To bridge this gap, we introduce PASemiQA, a
novel approach that jointly leverages text and relational information in
semi-structured data to answer questions. PASemiQA first generates a plan to
identify relevant text and relational information to answer the question in
semi-structured data, and then uses an LLM agent to traverse the
semi-structured data and extract necessary information. Our empirical results
demonstrate the effectiveness of PASemiQA across different semi-structured
datasets from various domains, showcasing its potential to improve the accuracy
and reliability of question answering systems on semi-structured data.",http://arxiv.org/abs/2502.21087v1,2/28/25,"Hansi Yang, Qi Zhang, Wei Jiang, Jianguo Li"
277,Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks,"Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities
of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The
increasing demands of application scenarios have driven the evolution of RAG,
leading to the integration of advanced retrievers, LLMs and other complementary
technologies, which in turn has amplified the intricacy of RAG systems.
However, the rapid advancements are outpacing the foundational RAG paradigm,
with many methods struggling to be unified under the process of
""retrieve-then-generate"". In this context, this paper examines the limitations
of the existing RAG paradigm and introduces the modular RAG framework. By
decomposing complex RAG systems into independent modules and specialized
operators, it facilitates a highly reconfigurable framework. Modular RAG
transcends the traditional linear architecture, embracing a more advanced
design that integrates routing, scheduling, and fusion mechanisms. Drawing on
extensive research, this paper further identifies prevalent RAG
patterns-linear, conditional, branching, and looping-and offers a comprehensive
analysis of their respective implementation nuances. Modular RAG presents
innovative opportunities for the conceptualization and deployment of RAG
systems. Finally, the paper explores the potential emergence of new operators
and paradigms, establishing a solid theoretical foundation and a practical
roadmap for the continued evolution and practical deployment of RAG
technologies.",http://arxiv.org/abs/2407.21059v1,7/26/24,"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang"
278,Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering,"Recently, large language models (LLMs) have demonstrated remarkable
capabilities in natural language processing tasks, yet they remain prone to
hallucinations when reasoning with insufficient internal knowledge. While
integrating LLMs with knowledge graphs (KGs) provides access to structured,
verifiable information, existing approaches often generate incomplete or
factually inconsistent reasoning paths. To this end, we propose Self-Reflective
Planning (SRP), a framework that synergizes LLMs with KGs through iterative,
reference-guided reasoning. Specifically, given a question and topic entities,
SRP first searches for references to guide planning and reflection. In the
planning process, it checks initial relations and generates a reasoning path.
After retrieving knowledge from KGs through a reasoning path, it implements
iterative reflection by judging the retrieval result and editing the reasoning
path until the answer is correctly retrieved. Extensive experiments on three
public datasets demonstrate that SRP surpasses various strong baselines and
further underscore its reliable reasoning ability.",http://arxiv.org/abs/2505.19410v1,5/26/25,"Jiajun Zhu, Ye Liu, Meikai Bao, Kai Zhang, Yanghai Zhang, Qi Liu"
279,Knowledge Editing with Dynamic Knowledge Graphs for Multi-Hop Question Answering,"Multi-hop question answering (MHQA) poses a significant challenge for large
language models (LLMs) due to the extensive knowledge demands involved.
Knowledge editing, which aims to precisely modify the LLMs to incorporate
specific knowledge without negatively impacting other unrelated knowledge,
offers a potential solution for addressing MHQA challenges with LLMs. However,
current solutions struggle to effectively resolve issues of knowledge
conflicts. Most parameter-preserving editing methods are hindered by inaccurate
retrieval and overlook secondary editing issues, which can introduce noise into
the reasoning process of LLMs. In this paper, we introduce KEDKG, a novel
knowledge editing method that leverages a dynamic knowledge graph for MHQA,
designed to ensure the reliability of answers. KEDKG involves two primary
steps: dynamic knowledge graph construction and knowledge graph augmented
generation. Initially, KEDKG autonomously constructs a dynamic knowledge graph
to store revised information while resolving potential knowledge conflicts.
Subsequently, it employs a fine-grained retrieval strategy coupled with an
entity and relation detector to enhance the accuracy of graph retrieval for LLM
generation. Experimental results on benchmarks show that KEDKG surpasses
previous state-of-the-art models, delivering more accurate and reliable answers
in environments with dynamic information.",http://arxiv.org/abs/2412.13782v2,12/18/24,"Yifan Lu, Yigeng Zhou, Jing Li, Yequan Wang, Xuebo Liu, Daojing He, Fangming Liu, Min Zhang"
280,Complex Factoid Question Answering with a Free-Text Knowledge Graph,"We introduce DELFT, a factoid question answering system which combines the
nuance and depth of knowledge graph question answering approaches with the
broader coverage of free-text. DELFT builds a free-text knowledge graph from
Wikipedia, with entities as nodes and sentences in which entities co-occur as
edges. For each question, DELFT finds the subgraph linking question entity
nodes to candidates using text sentences as edges, creating a dense and high
coverage semantic graph. A novel graph neural network reasons over the
free-text graph-combining evidence on the nodes via information along edge
sentences-to select a final answer. Experiments on three question answering
datasets show DELFT can answer entity-rich questions better than machine
reading based models, bert-based answer ranking and memory networks. DELFT's
advantage comes from both the high coverage of its free-text knowledge
graph-more than double that of dbpedia relations-and the novel graph neural
network which reasons on the rich but noisy free-text evidence.",http://arxiv.org/abs/2103.12876v1,3/23/21,"Chen Zhao, Chenyan Xiong, Xin Qian, Jordan BoydGraber"
281,Context Graph,"Knowledge Graphs (KGs) are foundational structures in many AI applications,
representing entities and their interrelations through triples. However,
triple-based KGs lack the contextual information of relational knowledge, like
temporal dynamics and provenance details, which are crucial for comprehensive
knowledge representation and effective reasoning. Instead, \textbf{Context
Graphs} (CGs) expand upon the conventional structure by incorporating
additional information such as time validity, geographic location, and source
provenance. This integration provides a more nuanced and accurate understanding
of knowledge, enabling KGs to offer richer insights and support more
sophisticated reasoning processes. In this work, we first discuss the inherent
limitations of triple-based KGs and introduce the concept of CGs, highlighting
their advantages in knowledge representation and reasoning. We then present a
context graph reasoning \textbf{CGR$^3$} paradigm that leverages large language
models (LLMs) to retrieve candidate entities and related contexts, rank them
based on the retrieved information, and reason whether sufficient information
has been obtained to answer a query. Our experimental results demonstrate that
CGR$^3$ significantly improves performance on KG completion (KGC) and KG
question answering (KGQA) tasks, validating the effectiveness of incorporating
contextual information on KG representation and reasoning.",http://arxiv.org/abs/2406.11160v3,6/17/24,"Chengjin Xu, Muzhi Li, Cehao Yang, Xuhui Jiang, Lumingyuan Tang, Yiyan Qi, Jian Guo"
282,Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG,"Retrieval-augmented generation (RAG) empowers large language models (LLMs) to
utilize external knowledge sources. The increasing capacity of LLMs to process
longer input sequences opens up avenues for providing more retrieved
information, to potentially enhance the quality of generated outputs. It is
plausible to assume that a larger retrieval set would contain more relevant
information (higher recall), that might result in improved performance.
However, our empirical findings demonstrate that for many long-context LLMs,
the quality of generated output initially improves first, but then subsequently
declines as the number of retrieved passages increases. This paper investigates
this phenomenon, identifying the detrimental impact of retrieved ""hard
negatives"" as a key contributor. To mitigate this and enhance the robustness of
long-context LLM-based RAG, we propose both training-free and training-based
approaches. We first showcase the effectiveness of retrieval reordering as a
simple yet powerful training-free optimization. Furthermore, we explore
training-based methods, specifically RAG-specific implicit LLM fine-tuning and
RAG-oriented fine-tuning with intermediate reasoning, demonstrating their
capacity for substantial performance gains. Finally, we conduct a systematic
analysis of design choices for these training-based methods, including data
distribution, retriever selection, and training context length.",http://arxiv.org/abs/2410.05983v1,10/8/24,"Bowen Jin, Jinsung Yoon, Jiawei Han, Sercan O Arik"
283,"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection","Despite their remarkable capabilities, large language models (LLMs) often
produce responses containing factual inaccuracies due to their sole reliance on
the parametric knowledge they encapsulate. Retrieval-Augmented Generation
(RAG), an ad hoc approach that augments LMs with retrieval of relevant
knowledge, decreases such issues. However, indiscriminately retrieving and
incorporating a fixed number of retrieved passages, regardless of whether
retrieval is necessary, or passages are relevant, diminishes LM versatility or
can lead to unhelpful response generation. We introduce a new framework called
Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's
quality and factuality through retrieval and self-reflection. Our framework
trains a single arbitrary LM that adaptively retrieves passages on-demand, and
generates and reflects on retrieved passages and its own generations using
special tokens, called reflection tokens. Generating reflection tokens makes
the LM controllable during the inference phase, enabling it to tailor its
behavior to diverse task requirements. Experiments show that Self-RAG (7B and
13B parameters) significantly outperforms state-of-the-art LLMs and
retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG
outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,
reasoning and fact verification tasks, and it shows significant gains in
improving factuality and citation accuracy for long-form generations relative
to these models.",http://arxiv.org/abs/2310.11511v1,10/17/23,"Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi"
284,Empowering Language Models with Knowledge Graph Reasoning for Question Answering,"Answering open-domain questions requires world knowledge about in-context
entities. As pre-trained Language Models (LMs) lack the power to store all
required knowledge, external knowledge sources, such as knowledge graphs, are
often used to augment LMs. In this work, we propose knOwledge REasOning
empowered Language Model (OREO-LM), which consists of a novel Knowledge
Interaction Layer that can be flexibly plugged into existing Transformer-based
LMs to interact with a differentiable Knowledge Graph Reasoning module
collaboratively. In this way, LM guides KG to walk towards the desired answer,
while the retrieved knowledge improves LM. By adopting OREO-LM to RoBERTa and
T5, we show significant performance gain, achieving state-of-art results in the
Closed-Book setting. The performance enhancement is mainly from the KG
reasoning's capacity to infer missing relational facts. In addition, OREO-LM
provides reasoning paths as rationales to interpret the model's decision.",http://arxiv.org/abs/2211.08380v1,11/15/22,"Ziniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang, Ziyi Yang, Chenguang Zhu, KaiWei Chang, Yizhou Sun"
285,Cognitive Knowledge Graph Reasoning for One-shot Relational Learning,"Inferring new facts from existing knowledge graphs (KG) with explainable
reasoning processes is a significant problem and has received much attention
recently. However, few studies have focused on relation types unseen in the
original KG, given only one or a few instances for training. To bridge this
gap, we propose CogKR for one-shot KG reasoning. The one-shot relational
learning problem is tackled through two modules: the summary module summarizes
the underlying relationship of the given instances, based on which the
reasoning module infers the correct answers. Motivated by the dual process
theory in cognitive science, in the reasoning module, a cognitive graph is
built by iteratively coordinating retrieval (System 1, collecting relevant
evidence intuitively) and reasoning (System 2, conducting relational reasoning
over collected information). The structural information offered by the
cognitive graph enables our model to aggregate pieces of evidence from multiple
reasoning paths and explain the reasoning process graphically. Experiments show
that CogKR substantially outperforms previous state-of-the-art models on
one-shot KG reasoning benchmarks, with relative improvements of 24.3%-29.7% on
MRR. The source code is available at https://github.com/THUDM/CogKR.",http://arxiv.org/abs/1906.05489v1,6/13/19,"Zhengxiao Du, Chang Zhou, Ming Ding, Hongxia Yang, Jie Tang"
286,R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning,"Large Language Models (LLMs) are powerful but prone to hallucinations due to
static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting
external information, but current methods often are costly, generalize poorly,
or ignore the internal knowledge of the model. In this paper, we introduce
R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage
both internal and external knowledge sources. R1-Searcher++ employs a two-stage
training strategy: an initial SFT Cold-start phase for preliminary format
learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses
outcome-supervision to encourage exploration, incorporates a reward mechanism
for internal knowledge utilization, and integrates a memorization mechanism to
continuously assimilate retrieved information, thereby enriching the model's
internal knowledge. By leveraging internal knowledge and external search
engine, the model continuously improves its capabilities, enabling efficient
retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++
outperforms previous RAG and reasoning methods and achieves efficient
retrieval. The code is available at
https://github.com/RUCAIBox/R1-Searcher-plus.",http://arxiv.org/abs/2505.17005v1,5/22/25,"Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, JiRong Wen"
287,Integrate Temporal Graph Learning into LLM-based Temporal Knowledge Graph Model,"Temporal Knowledge Graph Forecasting (TKGF) aims to predict future events
based on the observed events in history. Recently, Large Language Models (LLMs)
have exhibited remarkable capabilities, generating significant research
interest in their application for reasoning over temporal knowledge graphs
(TKGs). Existing LLM-based methods have integrated retrieved historical facts
or static graph representations into LLMs. Despite the notable performance of
LLM-based methods, they are limited by the insufficient modeling of temporal
patterns and ineffective cross-modal alignment between graph and language,
hindering the ability of LLMs to fully grasp the temporal and structural
information in TKGs. To tackle these issues, we propose a novel framework
TGL-LLM to integrate temporal graph learning into LLM-based temporal knowledge
graph model. Specifically, we introduce temporal graph learning to capture the
temporal and relational patterns and obtain the historical graph embedding.
Furthermore, we design a hybrid graph tokenization to sufficiently model the
temporal patterns within LLMs. To achieve better alignment between graph and
language, we employ a two-stage training paradigm to finetune LLMs on
high-quality and diverse data, thereby resulting in better performance.
Extensive experiments on three real-world datasets show that our approach
outperforms a range of state-of-the-art (SOTA) methods.",http://arxiv.org/abs/2501.11911v1,1/21/25,"He Chang, Jie Wu, Zhulin Tao, Yunshan Ma, Xianglin Huang, TatSeng Chua"
288,Formal Language Knowledge Corpus for Retrieval Augmented Generation,"The integration of retrieval-augmented techniques with LLMs has shown promise
in improving performance across various domains. However, their utility in
tasks requiring advanced reasoning, such as generating and evaluating
mathematical statements and proofs, remains underexplored. This study explores
the use of Lean, a programming language for writing mathematical proofs, to
populate the knowledge corpus used by RAG systems. We hope for this to lay the
foundation to exploring different methods of using RAGs to improve the
performance of LLMs in advanced logical reasoning tasks.",http://arxiv.org/abs/2412.16689v1,12/21/24,"Majd Zayyad, Yossi Adi"
289,Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models,"The vast amount of biomedical information available today presents a
significant challenge for investigators seeking to digest, process, and
understand these findings effectively. Large Language Models (LLMs) have
emerged as powerful tools to navigate this complex and challenging data
landscape. However, LLMs may lead to hallucinatory responses, making Retrieval
Augmented Generation (RAG) crucial for achieving accurate information. In this
protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease
Distinction), a comprehensive workflow designed to support investigators with
knowledge integration and hypothesis generation, identifying validated paths
forward. Relevant biomedical information from publications and knowledge bases
are reviewed, integrated, and extracted via text-mining association analysis
and explainable graph prediction models on disease nodes, forecasting potential
links among drugs and diseases. These analyses, along with biomedical texts,
are integrated into a framework that facilitates user-directed mechanism
elucidation as well as hypothesis exploration through RAG-enabled LLMs. A
clinical use-case demonstrates RUGGED's ability to evaluate and recommend
therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy
(DCM), analyzing prescribed drugs for molecular interactions and unexplored
uses. The platform minimizes LLM hallucinations, offers actionable insights,
and improves the investigation of novel therapeutics.",http://arxiv.org/abs/2407.12888v1,7/17/24,"Alexander R Pelletier, Joseph Ramirez, Irsyad Adam, Simha Sankar, Yu Yan, Ding Wang, Dylan Steinecke, Wei Wang, Peipei Ping"
290,Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models,"Retrieval-Augmented Generation (RAG), while effective in integrating external
knowledge to address the limitations of large language models (LLMs), can be
undermined by imperfect retrieval, which may introduce irrelevant, misleading,
or even malicious information. Despite its importance, previous studies have
rarely explored the behavior of RAG through joint analysis on how errors from
imperfect retrieval attribute and propagate, and how potential conflicts arise
between the LLMs' internal knowledge and external sources. We find that
imperfect retrieval augmentation might be inevitable and quite harmful, through
controlled analysis under realistic conditions. We identify the knowledge
conflicts between LLM-internal and external knowledge from retrieval as a
bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs
resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach
that adaptively elicits essential information from LLMs' internal knowledge,
iteratively consolidates internal and external knowledge with source-awareness,
and finalizes the answer according to information reliability. Our experiments
using Gemini and Claude demonstrate that Astute RAG significantly outperforms
previous robustness-enhanced RAG methods. Notably, Astute RAG is the only
approach that matches or exceeds the performance of LLMs without RAG under
worst-case scenarios. Further analysis reveals that Astute RAG effectively
resolves knowledge conflicts, improving the reliability and trustworthiness of
RAG systems.",http://arxiv.org/abs/2410.07176v1,10/9/24,"Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, Sercan  Ark"
291,KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph,"Large language model (LLM) has achieved outstanding performance on various
downstream tasks with its powerful natural language understanding and zero-shot
capability, but LLM still suffers from knowledge limitation. Especially in
scenarios that require long logical chains or complex reasoning, the
hallucination and knowledge limitation of LLM limit its performance in question
answering (QA). In this paper, we propose a novel framework KnowledgeNavigator
to address these challenges by efficiently and accurately retrieving external
knowledge from knowledge graph and using it as a key factor to enhance LLM
reasoning. Specifically, KnowledgeNavigator first mines and enhances the
potential constraints of the given question to guide the reasoning. Then it
retrieves and filters external knowledge that supports answering through
iterative reasoning on knowledge graph with the guidance of LLM and the
question. Finally, KnowledgeNavigator constructs the structured knowledge into
effective prompts that are friendly to LLM to help its reasoning. We evaluate
KnowledgeNavigator on multiple public KGQA benchmarks, the experiments show the
framework has great effectiveness and generalization, outperforming previous
knowledge graph enhanced LLM methods and is comparable to the fully supervised
models.",http://arxiv.org/abs/2312.15880v2,12/26/23,"Tiezheng Guo, Qingwen Yang, Chen Wang, Yanyi Liu, Pan Li, Jiawei Tang, Dapeng Li, Yingyou Wen"
292,KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification,"Hierarchical Text Classification (HTC) involves assigning documents to labels
organized within a taxonomy. Most previous research on HTC has focused on
supervised methods. However, in real-world scenarios, employing supervised HTC
can be challenging due to a lack of annotated data. Moreover, HTC often faces
issues with large label spaces and long-tail distributions. In this work, we
present Knowledge Graphs for zero-shot Hierarchical Text Classification
(KG-HTC), which aims to address these challenges of HTC in applications by
integrating knowledge graphs with Large Language Models (LLMs) to provide
structured semantic context during classification. Our method retrieves
relevant subgraphs from knowledge graphs related to the input text using a
Retrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to
understand label semantics at various hierarchy levels. We evaluate KG-HTC on
three open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental
results show that KG-HTC significantly outperforms three baselines in the
strict zero-shot setting, particularly achieving substantial improvements at
deeper levels of the hierarchy. This evaluation demonstrates the effectiveness
of incorporating structured knowledge into LLMs to address HTC's challenges in
large label spaces and long-tailed label distributions. Our code is available
at: https://github.com/QianboZang/KG-HTC.",http://arxiv.org/abs/2505.05583v1,5/8/25,"Qianbo Zang, Christophe Zgrzendek, Igor Tchappi, Afshin Khadangi, Johannes Sedlmeir"
293,Search-o1: Agentic Search-Enhanced Large Reasoning Models,"Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive
long stepwise reasoning capabilities through large-scale reinforcement
learning. However, their extended reasoning processes often suffer from
knowledge insufficiency, leading to frequent uncertainties and potential
errors. To address this limitation, we introduce \textbf{Search-o1}, a
framework that enhances LRMs with an agentic retrieval-augmented generation
(RAG) mechanism and a Reason-in-Documents module for refining retrieved
documents. Search-o1 integrates an agentic search workflow into the reasoning
process, enabling dynamic retrieval of external knowledge when LRMs encounter
uncertain knowledge points. Additionally, due to the verbose nature of
retrieved documents, we design a separate Reason-in-Documents module to deeply
analyze the retrieved information before injecting it into the reasoning chain,
minimizing noise and preserving coherent reasoning flow. Extensive experiments
on complex reasoning tasks in science, mathematics, and coding, as well as six
open-domain QA benchmarks, demonstrate the strong performance of Search-o1.
This approach enhances the trustworthiness and applicability of LRMs in complex
reasoning tasks, paving the way for more reliable and versatile intelligent
systems. The code is available at
\url{https://github.com/sunnynexus/Search-o1}.",http://arxiv.org/abs/2501.05366v1,1/9/25,"Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, Zhicheng Dou"
294,Empowering Agentic Video Analytics Systems with Video Language Models,"AI-driven video analytics has become increasingly pivotal across diverse
domains. However, existing systems are often constrained to specific,
predefined tasks, limiting their adaptability in open-ended analytical
scenarios. The recent emergence of Video-Language Models (VLMs) as
transformative technologies offers significant potential for enabling
open-ended video understanding, reasoning, and analytics. Nevertheless, their
limited context windows present challenges when processing ultra-long video
content, which is prevalent in real-world applications. To address this, we
introduce AVAS, a VLM-powered system designed for open-ended, advanced video
analytics. AVAS incorporates two key innovations: (1) the near real-time
construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or
continuous video streams, and (2) an agentic retrieval-generation mechanism
that leverages EKGs to handle complex and diverse queries. Comprehensive
evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that
AVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,
respectively, significantly surpassing existing VLM and video
Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video
analytics in ultra-long and open-world video scenarios, we introduce a new
benchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours
in duration, along with 120 manually annotated, diverse, and complex
question-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an
accuracy of 75.8%.",http://arxiv.org/abs/2505.00254v3,5/1/25,"Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu"
295,HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models,"In order to thrive in hostile and ever-changing natural environments,
mammalian brains evolved to store large amounts of knowledge about the world
and continually integrate new information while avoiding catastrophic
forgetting. Despite the impressive accomplishments, large language models
(LLMs), even with retrieval-augmented generation (RAG), still struggle to
efficiently and effectively integrate a large amount of new experiences after
pre-training. In this work, we introduce HippoRAG, a novel retrieval framework
inspired by the hippocampal indexing theory of human long-term memory to enable
deeper and more efficient knowledge integration over new experiences. HippoRAG
synergistically orchestrates LLMs, knowledge graphs, and the Personalized
PageRank algorithm to mimic the different roles of neocortex and hippocampus in
human memory. We compare HippoRAG with existing RAG methods on multi-hop
question answering and show that our method outperforms the state-of-the-art
methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves
comparable or better performance than iterative retrieval like IRCoT while
being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into
IRCoT brings further substantial gains. Finally, we show that our method can
tackle new types of scenarios that are out of reach of existing methods. Code
and data are available at https://github.com/OSU-NLP-Group/HippoRAG.",http://arxiv.org/abs/2405.14831v3,5/23/24,"Bernal Jimnez Gutirrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su"
296,GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs,"Large Language Models are now key assistants in human decision-making
processes. However, a common note always seems to follow: ""LLMs can make
mistakes. Be careful with important info."" This points to the reality that not
all outputs from LLMs are dependable, and users must evaluate them manually.
The challenge deepens as hallucinated responses, often presented with seemingly
plausible explanations, create complications and raise trust issues among
users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph
enhanced retrieval-augmented generation framework to provide Evidence-based
response generation. Specifically, when the user uploads a material document, a
knowledge graph will be created, which helps construct a retrieval-augmented
agent, enhancing the agent's responses with additional knowledge beyond its
training corpus. Then we leverage Chain-of-Thought (CoT) logic generation,
n-hop sub-graph searching, and entailment-based sentence generation to realize
accurate evidence retrieval. We demonstrate that our method improves the
existing models' performance in terms of identifying the exact evidence in a
free-form context, providing a reliable way to examine the resources of LLM's
conclusion and help with the judgment of the trustworthiness.",http://arxiv.org/abs/2505.10143v1,5/15/25,"Longchao Da, Parth Mitesh Shah, KuanRu Liou, Jiaxing Zhang, Hua Wei"
297,UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph,"Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the
answer entities that are multiple hops away from the topic entities mentioned
in a natural language question on a large-scale Knowledge Graph (KG). To cope
with the vast search space, existing work usually adopts a two-stage approach:
it first retrieves a relatively small subgraph related to the question and then
performs the reasoning on the subgraph to find the answer entities accurately.
Although these two stages are highly related, previous work employs very
different technical solutions for developing the retrieval and reasoning
models, neglecting their relatedness in task essence. In this paper, we propose
UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and
reasoning in both model architecture and parameter learning. For model
architecture, UniKGQA consists of a semantic matching module based on a
pre-trained language model~(PLM) for question-relation semantic matching, and a
matching information propagation module to propagate the matching information
along the directed edges on KGs. For parameter learning, we design a shared
pre-training task based on question-relation matching for both retrieval and
reasoning models, and then propose retrieval- and reasoning-oriented
fine-tuning strategies. Compared with previous studies, our approach is more
unified, tightly relating the retrieval and reasoning stages. Extensive
experiments on three benchmark datasets have demonstrated the effectiveness of
our method on the multi-hop KGQA task. Our codes and data are publicly
available at~\url{https://github.com/RUCAIBox/UniKGQA}.",http://arxiv.org/abs/2212.00959v2,12/2/22,"Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, JiRong Wen"
298,"Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding","We present a comprehensive framework for enhancing Retrieval-Augmented
Generation (RAG) systems through dynamic retrieval strategies and reinforcement
fine-tuning. This approach significantly improves large language models on
knowledge-intensive tasks, including opendomain question answering and complex
reasoning. Our framework integrates two complementary techniques:
Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use
of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),
which dynamically determines retrieval timing and content based on contextual
needs. Together, these techniques enhance both the utilization and relevance of
retrieved content, improving factual accuracy and response quality. Designed as
a lightweight solution compatible with any Transformer-based LLM without
requiring additional training, our framework excels in knowledge-intensive
tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a
novel method to selectively compress key-value caches by token importance,
mitigating memory bottlenecks in long-context applications. The framework also
incorporates test-time scaling techniques to dynamically balance reasoning
depth and computational resources, alongside optimized decoding strategies for
faster inference. Experiments on benchmark datasets show that our framework
reduces hallucinations, strengthens domain-specific reasoning, and achieves
significant efficiency and scalability gains over traditional RAG systems. This
integrated approach advances the development of robust, efficient, and scalable
RAG systems across diverse applications.",http://arxiv.org/abs/2504.01281v3,4/2/25,"Sakhinana Sagar Srinivas, Akash Das, Shivam Gupta, Venkataramana Runkana"
299,Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation,"Knowledge graph-grounded dialog generation requires retrieving a
dialog-relevant subgraph from the given knowledge base graph and integrating it
with the dialog history. Previous works typically represent the graph using an
external encoder, such as graph neural networks, and retrieve relevant triplets
based on the similarity between single-vector representations of triplets and
the dialog history. However, these external encoders fail to leverage the rich
knowledge of pretrained language models, and the retrieval process is also
suboptimal due to the information bottleneck caused by the single-vector
abstraction of the dialog history. In this work, we propose Dialog generation
with Generative Subgraph Retrieval (DialogGSR), which retrieves relevant
knowledge subgraphs by directly generating their token sequences on top of
language models. For effective generative subgraph retrieval, we introduce two
key methods: (i) structure-aware knowledge graph linearization with
self-supervised graph-specific tokens and (ii) graph-constrained decoding
utilizing graph structural proximity-based entity informativeness scores for
valid and relevant generative retrieval. DialogGSR achieves state-of-the-art
performance in knowledge graph-grounded dialog generation, as demonstrated on
OpenDialKG and KOMODIS datasets.",http://arxiv.org/abs/2410.09350v1,10/12/24,"Jinyoung Park, Minseok Joo, JooKyung Kim, Hyunwoo J Kim"