{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7395da4c-5b58-43dd-87d3-f9245c79a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/ashish-chouhan/arxiv_cs_papers/data/train-00000-of-00001-bf80d7e563046673.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ec6ca69-091d-4433-af45-b9f7a8f81a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.12/site-packages (14.0.2)\n",
      "Requirement already satisfied: Unidecode in /opt/anaconda3/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyarrow) (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow Unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62e27d6a-8871-4e69-82cc-50ddaf1ed1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Ghost on the Shell: An Expressive Representati...   \n",
      "1  Handling Data Heterogeneity via Architectural ...   \n",
      "2  Linear Representations of Sentiment in Large L...   \n",
      "3  Verb Conjugation in Transformers Is Determined...   \n",
      "4            Online Detection of AI-Generated Images   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  The creation of photorealistic virtual worlds ...   \n",
      "1  Federated Learning (FL) is a promising researc...   \n",
      "2  Sentiment is a pervasive feature in natural la...   \n",
      "3  Deep architectures such as Transformers are so...   \n",
      "4  With advancements in AI-generated images comin...   \n",
      "\n",
      "                                             authors            published  \\\n",
      "0  [Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu,...  2023-10-23 17:59:52   \n",
      "1  [Sara Pieri, Jose Renato Restom, Samuel Horvat...  2023-10-23 17:59:16   \n",
      "2  [Curt Tigges, Oskar John Hollinsworth, Atticus...  2023-10-23 17:55:31   \n",
      "3                           [Sophie Hao, Tal Linzen]  2023-10-23 17:53:47   \n",
      "4  [David C. Epstein, Ishan Jain, Oliver Wang, Ri...  2023-10-23 17:53:14   \n",
      "\n",
      "                                 url                            pdf_url  \\\n",
      "0  http://arxiv.org/abs/2310.15168v1  http://arxiv.org/pdf/2310.15168v1   \n",
      "1  http://arxiv.org/abs/2310.15165v1  http://arxiv.org/pdf/2310.15165v1   \n",
      "2  http://arxiv.org/abs/2310.15154v1  http://arxiv.org/pdf/2310.15154v1   \n",
      "3  http://arxiv.org/abs/2310.15151v1  http://arxiv.org/pdf/2310.15151v1   \n",
      "4  http://arxiv.org/abs/2310.15150v1  http://arxiv.org/pdf/2310.15150v1   \n",
      "\n",
      "       arxiv_id  \n",
      "0  2310.15168v1  \n",
      "1  2310.15165v1  \n",
      "2  2310.15154v1  \n",
      "3  2310.15151v1  \n",
      "4  2310.15150v1  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0aee605-3f03-46c5-beed-51f6e2bbc291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   title      5000 non-null   object\n",
      " 1   abstract   5000 non-null   object\n",
      " 2   authors    5000 non-null   object\n",
      " 3   published  5000 non-null   object\n",
      " 4   url        5000 non-null   object\n",
      " 5   pdf_url    5000 non-null   object\n",
      " 6   arxiv_id   5000 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 273.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6689955-647c-43b0-b734-5597dd5c2884",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output_file.csv', index=False)  # index=False ensures the row index is not saved in the CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea536b58-1cc5-44f1-bf37-548de588e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/fji/Desktop/CSV/output_file.csv', index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1bcc0cc-63af-4331-8d88-76655d5a1a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Ghost on the Shell: An Expressive Representati...   \n",
      "1  Handling Data Heterogeneity via Architectural ...   \n",
      "2  Linear Representations of Sentiment in Large L...   \n",
      "3  Verb Conjugation in Transformers Is Determined...   \n",
      "4            Online Detection of AI-Generated Images   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  The creation of photorealistic virtual worlds ...   \n",
      "1  Federated Learning (FL) is a promising researc...   \n",
      "2  Sentiment is a pervasive feature in natural la...   \n",
      "3  Deep architectures such as Transformers are so...   \n",
      "4  With advancements in AI-generated images comin...   \n",
      "\n",
      "                                             authors            published  \\\n",
      "0  Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu, ...  2023-10-23 17:59:52   \n",
      "1  Sara Pieri, Jose Renato, Restom Samuel, Horvat...  2023-10-23 17:59:16   \n",
      "2  Curt Tigges, Oskar John, Hollinsworth Atticus,...  2023-10-23 17:55:31   \n",
      "3                             Sophie Hao, Tal Linzen  2023-10-23 17:53:47   \n",
      "4  David C., Epstein Ishan, Jain Oliver, Wang Ric...  2023-10-23 17:53:14   \n",
      "\n",
      "                                 url                            pdf_url  \\\n",
      "0  http://arxiv.org/abs/2310.15168v1  http://arxiv.org/pdf/2310.15168v1   \n",
      "1  http://arxiv.org/abs/2310.15165v1  http://arxiv.org/pdf/2310.15165v1   \n",
      "2  http://arxiv.org/abs/2310.15154v1  http://arxiv.org/pdf/2310.15154v1   \n",
      "3  http://arxiv.org/abs/2310.15151v1  http://arxiv.org/pdf/2310.15151v1   \n",
      "4  http://arxiv.org/abs/2310.15150v1  http://arxiv.org/pdf/2310.15150v1   \n",
      "\n",
      "       arxiv_id  \n",
      "0  2310.15168v1  \n",
      "1  2310.15165v1  \n",
      "2  2310.15154v1  \n",
      "3  2310.15151v1  \n",
      "4  2310.15150v1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('/Users/fji/Desktop/CSV/output_file.csv')\n",
    "\n",
    "def clean_authors(authors):\n",
    "    # Remove the square brackets and single quotes (if they exist)\n",
    "    authors = re.sub(r'[\\[\\]\\'\"]', '', authors)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    authors = re.sub(r'\\s+', ' ', authors).strip()\n",
    "    \n",
    "    # Convert special characters (like 'Ã¼') to their ASCII equivalents (e.g., 'u')\n",
    "    authors = unidecode(authors)\n",
    "    \n",
    "    # Split the authors based on spaces, but group words together as single names\n",
    "    authors_list = authors.split(' ')\n",
    "    \n",
    "    # We'll group every two consecutive words together\n",
    "    cleaned_authors = []\n",
    "    for i in range(0, len(authors_list), 2):  # Increment by 2 to group full names\n",
    "        if i + 1 < len(authors_list):\n",
    "            cleaned_authors.append(authors_list[i] + ' ' + authors_list[i + 1])\n",
    "        else:\n",
    "            # Handle any cases where the last name might have only one part\n",
    "            cleaned_authors.append(authors_list[i])\n",
    "\n",
    "    # Join the authors with commas\n",
    "    cleaned_authors = ', '.join(cleaned_authors)\n",
    "    \n",
    "    return cleaned_authors\n",
    "\n",
    "\n",
    "# Apply the cleaning function to the 'authors' column\n",
    "df['authors'] = df['authors'].apply(clean_authors)\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv('/Users/fji/Desktop/CSV/cleaned_file.csv', index=False)\n",
    "\n",
    "print(df.head())  # Check the first few rows to ensure the authors are cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "001f63e0-0e26-4a16-9503-6a70f3505f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 100\n",
      "Number of chunks: 100\n",
      "Paper 1:\n",
      "Title: RAG vs. GraphRAG: A Systematic Evaluation and Key Insights\n",
      "Summary: Retrieval-Augmented Generation (RAG) enhances the performance of LLMs across\n",
      "various tasks by retrieving relevant information from external sources,\n",
      "particularly on text-based data. For structured data, such as knowledge graphs,\n",
      "GraphRAG has been widely used to retrieve relevant information. However, recent\n",
      "studies have revealed that structuring implicit knowledge from text into graphs\n",
      "can benefit certain tasks, extending the application of GraphRAG from graph\n",
      "data to general text-based data. Despite their successful extensions, most\n",
      "applications of GraphRAG for text data have been designed for specific tasks\n",
      "and datasets, lacking a systematic evaluation and comparison between RAG and\n",
      "GraphRAG on widely used text-based benchmarks. In this paper, we systematically\n",
      "evaluate RAG and GraphRAG on well-established benchmark tasks, such as Question\n",
      "Answering and Query-based Summarization. Our results highlight the distinct\n",
      "strengths of RAG and GraphRAG across different tasks and evaluation\n",
      "perspectives. Inspired by these observations, we investigate strategies to\n",
      "integrate their strengths to improve downstream tasks. Additionally, we provide\n",
      "an in-depth discussion of the shortcomings of current GraphRAG approaches and\n",
      "outline directions for future research.\n",
      "URL: http://arxiv.org/abs/2502.11371v1\n",
      "Authors: Haoyu Han, Harry Shomer, Yu Wang, Yongjia Lei, Kai Guo, Zhigang Hua, Bo Long, Hui Liu, Jiliang Tang\n",
      "--------------------------------------------------\n",
      "Paper 2:\n",
      "Title: Empowering GraphRAG with Knowledge Filtering and Integration\n",
      "Summary: In recent years, large language models (LLMs) have revolutionized the field\n",
      "of natural language processing. However, they often suffer from knowledge gaps\n",
      "and hallucinations. Graph retrieval-augmented generation (GraphRAG) enhances\n",
      "LLM reasoning by integrating structured knowledge from external graphs.\n",
      "However, we identify two key challenges that plague GraphRAG:(1) Retrieving\n",
      "noisy and irrelevant information can degrade performance and (2)Excessive\n",
      "reliance on external knowledge suppresses the model's intrinsic reasoning. To\n",
      "address these issues, we propose GraphRAG-FI (Filtering and Integration),\n",
      "consisting of GraphRAG-Filtering and GraphRAG-Integration. GraphRAG-Filtering\n",
      "employs a two-stage filtering mechanism to refine retrieved information.\n",
      "GraphRAG-Integration employs a logits-based selection strategy to balance\n",
      "external knowledge from GraphRAG with the LLM's intrinsic reasoning,reducing\n",
      "over-reliance on retrievals. Experiments on knowledge graph QA tasks\n",
      "demonstrate that GraphRAG-FI significantly improves reasoning performance\n",
      "across multiple backbone models, establishing a more reliable and effective\n",
      "GraphRAG framework.\n",
      "URL: http://arxiv.org/abs/2503.13804v1\n",
      "Authors: Kai Guo, Harry Shomer, Shenglai Zeng, Haoyu Han, Yu Wang, Jiliang Tang\n",
      "--------------------------------------------------\n",
      "Paper 3:\n",
      "Title: GraphRAG under Fire\n",
      "Summary: GraphRAG advances retrieval-augmented generation (RAG) by structuring\n",
      "external knowledge as multi-scale knowledge graphs, enabling language models to\n",
      "integrate both broad context and granular details in their reasoning. While\n",
      "GraphRAG has demonstrated success across domains, its security implications\n",
      "remain largely unexplored. To bridge this gap, this work examines GraphRAG's\n",
      "vulnerability to poisoning attacks, uncovering an intriguing security paradox:\n",
      "compared to conventional RAG, GraphRAG's graph-based indexing and retrieval\n",
      "enhance resilience against simple poisoning attacks; meanwhile, the same\n",
      "features also create new attack surfaces. We present GRAGPoison, a novel attack\n",
      "that exploits shared relations in the knowledge graph to craft poisoning text\n",
      "capable of compromising multiple queries simultaneously. GRAGPoison employs\n",
      "three key strategies: i) relation injection to introduce false knowledge, ii)\n",
      "relation enhancement to amplify poisoning influence, and iii) narrative\n",
      "generation to embed malicious content within coherent text. Empirical\n",
      "evaluation across diverse datasets and models shows that GRAGPoison\n",
      "substantially outperforms existing attacks in terms of effectiveness (up to 98%\n",
      "success rate) and scalability (using less than 68% poisoning text). We also\n",
      "explore potential defensive measures and their limitations, identifying\n",
      "promising directions for future research.\n",
      "URL: http://arxiv.org/abs/2501.14050v1\n",
      "Authors: Jiacheng Liang, Yuhui Wang, Changjiang Li, Rongyi Zhu, Tanqiu Jiang, Neil Gong, Ting Wang\n",
      "--------------------------------------------------\n",
      "                                                title  \\\n",
      "0   RAG vs. GraphRAG: A Systematic Evaluation and ...   \n",
      "1   Empowering GraphRAG with Knowledge Filtering a...   \n",
      "2                                 GraphRAG under Fire   \n",
      "3   A Survey of Graph Retrieval-Augmented Generati...   \n",
      "4   RAKG:Document-level Retrieval Augmented Knowle...   \n",
      "..                                                ...   \n",
      "95           PRAGyan -- Connecting the Dots in Tweets   \n",
      "96  Bridging Legal Knowledge and AI: Retrieval-Aug...   \n",
      "97  HyKGE: A Hypothesis Knowledge Graph Enhanced F...   \n",
      "98  Learning to Retrieve and Reason on Knowledge G...   \n",
      "99  Retrieval-Augmented Generation with Hierarchic...   \n",
      "\n",
      "                                              summary  \\\n",
      "0   Retrieval-Augmented Generation (RAG) enhances ...   \n",
      "1   In recent years, large language models (LLMs) ...   \n",
      "2   GraphRAG advances retrieval-augmented generati...   \n",
      "3   Large language models (LLMs) have demonstrated...   \n",
      "4   With the rise of knowledge graph based retriev...   \n",
      "..                                                ...   \n",
      "95  As social media platforms grow, understanding ...   \n",
      "96  Agentic Generative AI, powered by Large Langua...   \n",
      "97  In this paper, we investigate the retrieval-au...   \n",
      "98  Extensive research has investigated the integr...   \n",
      "99  Graph-based Retrieval-Augmented Generation (RA...   \n",
      "\n",
      "                                  url  \\\n",
      "0   http://arxiv.org/abs/2502.11371v1   \n",
      "1   http://arxiv.org/abs/2503.13804v1   \n",
      "2   http://arxiv.org/abs/2501.14050v1   \n",
      "3   http://arxiv.org/abs/2501.13958v1   \n",
      "4   http://arxiv.org/abs/2504.09823v1   \n",
      "..                                ...   \n",
      "95  http://arxiv.org/abs/2407.13909v1   \n",
      "96  http://arxiv.org/abs/2502.20364v1   \n",
      "97  http://arxiv.org/abs/2312.15883v2   \n",
      "98  http://arxiv.org/abs/2502.14932v1   \n",
      "99  http://arxiv.org/abs/2503.10150v1   \n",
      "\n",
      "                                              authors  \n",
      "0   [Haoyu Han, Harry Shomer, Yu Wang, Yongjia Lei...  \n",
      "1   [Kai Guo, Harry Shomer, Shenglai Zeng, Haoyu H...  \n",
      "2   [Jiacheng Liang, Yuhui Wang, Changjiang Li, Ro...  \n",
      "3   [Qinggang Zhang, Shengyuan Chen, Yuanchen Bei,...  \n",
      "4   [Hairong Zhang, Jiaheng Si, Guohang Yan, Boyua...  \n",
      "..                                                ...  \n",
      "95               [Rahul Ravi, Gouri Ginde, Jon Rokne]  \n",
      "96  [Ryan C. Barron, Maksim E. Eren, Olga M. Seraf...  \n",
      "97  [Xinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong...  \n",
      "98            [Han Zhang, Langshi Zhou, Hanfang Yang]  \n",
      "99  [Haoyu Huang, Yongfeng Huang, Junjie Yang, Zhe...  \n",
      "\n",
      "[100 rows x 4 columns]\n",
      "    docid                                              title  \\\n",
      "0       0  RAG vs. GraphRAG: A Systematic Evaluation and ...   \n",
      "1       1  Empowering GraphRAG with Knowledge Filtering a...   \n",
      "2       2                                GraphRAG under Fire   \n",
      "3       3  A Survey of Graph Retrieval-Augmented Generati...   \n",
      "4       4  RAKG:Document-level Retrieval Augmented Knowle...   \n",
      "..    ...                                                ...   \n",
      "95     95           PRAGyan -- Connecting the Dots in Tweets   \n",
      "96     96  Bridging Legal Knowledge and AI: Retrieval-Aug...   \n",
      "97     97  HyKGE: A Hypothesis Knowledge Graph Enhanced F...   \n",
      "98     98  Learning to Retrieve and Reason on Knowledge G...   \n",
      "99     99  Retrieval-Augmented Generation with Hierarchic...   \n",
      "\n",
      "                                              authors  \n",
      "0   Haoyu Han, Harry Shomer, Yu Wang, Yongjia Lei,...  \n",
      "1   Kai Guo, Harry Shomer, Shenglai Zeng, Haoyu Ha...  \n",
      "2   Jiacheng Liang, Yuhui Wang, Changjiang Li, Ron...  \n",
      "3   Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, ...  \n",
      "4   Hairong Zhang, Jiaheng Si, Guohang Yan, Boyuan...  \n",
      "..                                                ...  \n",
      "95                 Rahul Ravi, Gouri Ginde, Jon Rokne  \n",
      "96  Ryan C Barron, Maksim E Eren, Olga M Serafimov...  \n",
      "97  Xinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong ...  \n",
      "98              Han Zhang, Langshi Zhou, Hanfang Yang  \n",
      "99  Haoyu Huang, Yongfeng Huang, Junjie Yang, Zhen...  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import ast\n",
    "\n",
    "# search_query = \"immunology OR 'clinical trials' OR 'neuroscience'\"\n",
    "search_query = (\n",
    "    \"GraphRAG OR RAG OR 'knowledge graph' OR 'graph-based retrieval' OR 'graph reasoning' \"\n",
    "    # \"OR 'hybrid search' OR 'graph reasoning' OR 'entity graph' OR 'graph-based retrieval' \"\n",
    "    # \"OR 'embedding search' OR 'approximate nearest neighbors'\"\n",
    ")\n",
    "max_results = 100\n",
    "\n",
    "# Fetch papers from arXiv\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=search_query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for result in client.results(search):\n",
    "    docs.append(\n",
    "        {\"title\": result.title, \"summary\": result.summary, \"url\": result.entry_id, \"authors\": result.authors}\n",
    "    )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.create_documents(\n",
    "    [doc[\"summary\"]+\" \"+doc[\"title\"]+\"\"+str(doc[\"authors\"]) for doc in docs], metadatas=docs\n",
    ")\n",
    "\n",
    "docs_to_print = docs[:3]\n",
    "\n",
    "print(f\"Number of papers: {len(docs)}\")\n",
    "print(f\"Number of chunks: {len(doc_splits)}\") \n",
    "for i, doc in enumerate(docs_to_print, start=1):\n",
    "    authors_str = \", \".join([str(author) for author in doc['authors']])  # Convert authors to strings\n",
    "    print(f\"Paper {i}:\")\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Summary: {doc['summary']}\")\n",
    "    print(f\"URL: {doc['url']}\")\n",
    "    print(f\"Authors: {authors_str}\")  # Join the authors as a string\n",
    "    print(\"-\" * 50)  # Divider to separate the papers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame to store the papers and their metadata\n",
    "df = pd.DataFrame(docs)\n",
    "print(df)\n",
    "# Check if authors are in list format, and if not, convert string to list\n",
    "df['docid'] = range(len(df))\n",
    "\n",
    "df['authors'] = df['authors'].apply(lambda x: ast.literal_eval(str(x)) if isinstance(x, str) else x)\n",
    "\n",
    "# Remove special characters from author names and join authors with commas\n",
    "def clean_author_name(name):\n",
    "    # Ensure the input is a string before applying regex\n",
    "    if isinstance(name, str):\n",
    "        # Use regular expression to remove any non-alphabetic characters except spaces\n",
    "        cleaned_name = re.sub(r'[^a-zA-Z\\s]', '', name)\n",
    "        return cleaned_name.strip()  # Ensure no leading/trailing spaces\n",
    "    return str(name)  # Convert to string if it's not a string already\n",
    "\n",
    "# Apply the cleaning function to each author name and join them with commas\n",
    "df['authors'] = df['authors'].apply(lambda x: \", \".join([clean_author_name(str(author)) for author in x]))\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(df[['docid', 'title', 'authors']])\n",
    "df = df[['docid', 'title', 'summary', 'url', 'authors']]\n",
    "\n",
    "# Optionally, save the DataFrame to CSV\n",
    "output_csv_path = \"/Users/fji/Desktop/CSV/papers100.csv\"\n",
    "df.to_csv(output_csv_path, index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d705b7e3-a422-40fb-a5d7-2408414fd637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph documents: 100\n",
      "Nodes from 1st graph doc:[Node(id='Retrieval-Augmented Generation (Rag)', type='Topic', properties={}), Node(id='Graphrag', type='Topic', properties={}), Node(id='Question Answering', type='Topic', properties={}), Node(id='Query-Based Summarization', type='Topic', properties={}), Node(id='Haoyu Han', type='Author', properties={}), Node(id='Harry Shomer', type='Author', properties={}), Node(id='Yu Wang', type='Author', properties={}), Node(id='Yongjia Lei', type='Author', properties={}), Node(id='Kai Guo', type='Author', properties={}), Node(id='Zhigang Hua', type='Author', properties={}), Node(id='Bo Long', type='Author', properties={}), Node(id='Hui Liu', type='Author', properties={}), Node(id='Jiliang Tang', type='Author', properties={})]\n",
      "Relationships from 1st graph doc:[Relationship(source=Node(id='Retrieval-Augmented Generation (Rag)', type='Topic', properties={}), target=Node(id='Graphrag', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Graphrag', type='Topic', properties={}), target=Node(id='Question Answering', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Graphrag', type='Topic', properties={}), target=Node(id='Query-Based Summarization', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Haoyu Han', type='Author', properties={}), target=Node(id='Rag Vs. Graphrag: A Systematic Evaluation And Key Insights', type='Paper', properties={}), type='AUTHORED', properties={}), Relationship(source=Node(id='Harry Shomer', type='Author', properties={}), target=Node(id='Rag Vs. Graphrag: A Systematic Evaluation And Key Insights', type='Paper', properties={}), type='AUTHORED', properties={}), Relationship(source=Node(id='Yu Wang', type='Author', properties={}), target=Node(id='Rag Vs. Graphrag: A Systematic Evaluation And Key Insights', type='Paper', properties={}), type='AUTHORED', properties={}), Relationship(source=Node(id='Yongjia Lei', type='Author', properties={}), target=Node(id='Rag Vs. Graphrag: A Systematic Evaluation And Key Insights', type='Paper', properties={}), type='AUTHORED', properties={}), Relationship(source=Node(id='Kai Guo', type='Author', properties={}), target=Node(id='Rag Vs. Graphrag: A Systematic Evaluation And Key Insights', type='Paper', properties={}), type='AUTHORED', properties={}), Relationship(source=Node(id='Zhigang Hua', type='Author', properties={}), target=Node(id='Rag Vs. Graphrag: A Systematic Evaluation And Key Insights', type='Paper', properties={}), type='AUTHORED', properties={}), Relationship(source=Node(id='Bo Long', type='Author', properties={}), target=Node(id='Rag Vs. Graphrag: A Systematic Evaluation And Key Insights', type='Paper', properties={}), type='AUTHORED', properties={}), Relationship(source=Node(id='Hui Liu', type='Author', properties={}), target=Node(id='Rag Vs. Graphrag: A Systematic Evaluation And Key Insights', type='Paper', properties={}), type='AUTHORED', properties={}), Relationship(source=Node(id='Jiliang Tang', type='Author', properties={}), target=Node(id='Rag Vs. Graphrag: A Systematic Evaluation And Key Insights', type='Paper', properties={}), type='AUTHORED', properties={})]\n"
     ]
    }
   ],
   "source": [
    "# GraphRAG Setup\n",
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "gpt4omini = \"gpt-4o-mini\"\n",
    "\n",
    "model = gpt4omini\n",
    "\n",
    "graph_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=graph_llm,\n",
    "    allowed_nodes=[\"Paper\", \"Author\", \"Topic\"],\n",
    "    node_properties=[\"title\", \"abstract\", \"url\", \"author\"],\n",
    "    allowed_relationships=[ \"COVERS\", \"INCLUDES\",\"RELATED_TO\",\"AUTHORED\"],\n",
    ")\n",
    "\n",
    "graph_documents = graph_transformer.convert_to_graph_documents(doc_splits)\n",
    "\n",
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54a5e8a-c003-47b9-a375-d5c37be189c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(doc))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4eb9526-80aa-4f6f-bb71-8a6f129d8b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Retrieval-Augmented Generation with Graphs (GraphRAG)', 'summary': 'Retrieval-augmented generation (RAG) is a powerful technique that enhances\\ndownstream task execution by retrieving additional information, such as\\nknowledge, skills, and tools from external sources. Graph, by its intrinsic\\n\"nodes connected by edges\" nature, encodes massive heterogeneous and relational\\ninformation, making it a golden resource for RAG in tremendous real-world\\napplications. As a result, we have recently witnessed increasing attention on\\nequipping RAG with Graph, i.e., GraphRAG. However, unlike conventional RAG,\\nwhere the retriever, generator, and external data sources can be uniformly\\ndesigned in the neural-embedding space, the uniqueness of graph-structured\\ndata, such as diverse-formatted and domain-specific relational knowledge, poses\\nunique and significant challenges when designing GraphRAG for different\\ndomains. Given the broad applicability, the associated design challenges, and\\nthe recent surge in GraphRAG, a systematic and up-to-date survey of its key\\nconcepts and techniques is urgently desired. Following this motivation, we\\npresent a comprehensive and up-to-date survey on GraphRAG. Our survey first\\nproposes a holistic GraphRAG framework by defining its key components,\\nincluding query processor, retriever, organizer, generator, and data source.\\nFurthermore, recognizing that graphs in different domains exhibit distinct\\nrelational patterns and require dedicated designs, we review GraphRAG\\ntechniques uniquely tailored to each domain. Finally, we discuss research\\nchallenges and brainstorm directions to inspire cross-disciplinary\\nopportunities. Our survey repository is publicly maintained at\\nhttps://github.com/Graph-RAG/GraphRAG/.', 'url': 'http://arxiv.org/abs/2501.00309v2', 'authors': [arxiv.Result.Author('Haoyu Han'), arxiv.Result.Author('Yu Wang'), arxiv.Result.Author('Harry Shomer'), arxiv.Result.Author('Kai Guo'), arxiv.Result.Author('Jiayuan Ding'), arxiv.Result.Author('Yongjia Lei'), arxiv.Result.Author('Mahantesh Halappanavar'), arxiv.Result.Author('Ryan A. Rossi'), arxiv.Result.Author('Subhabrata Mukherjee'), arxiv.Result.Author('Xianfeng Tang'), arxiv.Result.Author('Qi He'), arxiv.Result.Author('Zhigang Hua'), arxiv.Result.Author('Bo Long'), arxiv.Result.Author('Tong Zhao'), arxiv.Result.Author('Neil Shah'), arxiv.Result.Author('Amin Javari'), arxiv.Result.Author('Yinglong Xia'), arxiv.Result.Author('Jiliang Tang')]}\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8765e92b-7aa3-4bf7-8067-5f46496ba451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data_path = '/Users/fji/Desktop/CSV/'\n",
    "filename = data_path + \"entities\" + str(100) + \".csv\"\n",
    "\n",
    "# Open the file in write mode with 'newline=\"\"' to avoid extra blank lines\n",
    "with open(filename, \"w\", newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)  # Create a CSV writer object\n",
    "    writer.writerow([\"docid\", \"entityid\", \"type\"])  # Write the header row\n",
    "\n",
    "    # Loop through the graph_documents\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        if hasattr(doc, 'nodes') and isinstance(doc.nodes, list):  # Ensure 'nodes' is a list\n",
    "            for node in doc.nodes:\n",
    "                try:\n",
    "                    # Check if the 'id' and 'type' attributes exist in the node\n",
    "                    if hasattr(node, 'id') and hasattr(node, 'type'):\n",
    "                        # Write the data to the CSV file, split into three columns\n",
    "                        writer.writerow([i, node.id, node.type])\n",
    "                except UnicodeEncodeError:\n",
    "                    # Handle UnicodeEncodeError if there are problematic characters\n",
    "                    continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98da61c7-280e-4976-bad3-5aa748aae22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "data_path = '/Users/fji/Desktop/CSV/'\n",
    "filename = data_path + \"relations\" + str(100) + \".csv\"\n",
    "\n",
    "# Open the file in write mode with 'newline=\"\"' to avoid extra blank lines\n",
    "with open(filename, \"w\", newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)  # Create a CSV writer object\n",
    "    writer.writerow([\"docid\", \"source\", \"sourcetype\", \"target\", \"targettype\", \"type\"])  # Write the header row\n",
    "\n",
    "    # Loop through the graph_documents\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        if hasattr(doc, 'relations') and isinstance(doc.relations, list):  # Ensure 'relations' is a list\n",
    "            for relation in doc.relations:\n",
    "                try:\n",
    "                    # Check if the necessary attributes exist in the relation\n",
    "                    if hasattr(relation, 'source') and hasattr(relation, 'sourcetype') and hasattr(relation, 'target') and hasattr(relation, 'targettype') and hasattr(relation, 'type'):\n",
    "                        # Write the relation data to the CSV file\n",
    "                        writer.writerow([i, relation.source, relation.sourcetype, relation.target, relation.targettype, relation.type])\n",
    "                except UnicodeEncodeError:\n",
    "                    # Handle UnicodeEncodeError if there are problematic characters\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "012acce0-a7a8-403c-b928-fdb8c0d5e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data_path = '/Users/fji/Desktop/CSV/'\n",
    "filename = data_path + \"relations\" + str(100) + \".csv\"\n",
    "\n",
    "# Open the file in write mode with 'newline=\"\"' to avoid extra blank lines\n",
    "with open(filename, \"w\", newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)  # Create a CSV writer object\n",
    "    writer.writerow([\"docid\", \"source\", \"sourcetype\", \"target\", \"targettype\", \"type\"])  # Write the header row\n",
    "\n",
    "    # Loop through the graph_documents\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        # Check if the document has relationships\n",
    "        if hasattr(doc, 'relationships') and isinstance(doc.relationships, list):\n",
    "            # print(f\"Processing document {i}, relationships found.\")  # Debugging: Confirm relationships exist\n",
    "            for relation in doc.relationships:\n",
    "                try:\n",
    "                    # Extract the relevant data from the relationship\n",
    "                    source = relation.source\n",
    "                    target = relation.target\n",
    "\n",
    "                    # Check if the necessary attributes are present\n",
    "                    if hasattr(source, 'id') and hasattr(source, 'type') and hasattr(target, 'id') and hasattr(target, 'type'):\n",
    "                        # Write the data to the CSV file\n",
    "                        writer.writerow([i, source.id, source.type, target.id, target.type, relation.type])\n",
    "                    else:\n",
    "                        print(f\"  Missing attributes in relation: {relation}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error processing relation: {e}\")\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "873467bb-47aa-4405-a1c5-a754a5b409a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /opt/anaconda3/lib/python3.12/site-packages (0.3.19)\n",
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: arxiv in /opt/anaconda3/lib/python3.12/site-packages (2.1.3)\n",
      "Collecting arxiv\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: langchainhub in /opt/anaconda3/lib/python3.12/site-packages (0.1.21)\n",
      "Requirement already satisfied: pymilvus in /opt/anaconda3/lib/python3.12/site-packages (2.5.5)\n",
      "Collecting pymilvus\n",
      "  Using cached pymilvus-2.5.6-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (0.3.20)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: langgraph in /opt/anaconda3/lib/python3.12/site-packages (0.3.5)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.3.27-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tavily-python in /opt/anaconda3/lib/python3.12/site-packages (0.5.1)\n",
      "Collecting tavily-python\n",
      "  Using cached tavily_python-0.5.4-py3-none-any.whl.metadata (91 kB)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (4.0.2)\n",
      "Requirement already satisfied: langchain-milvus in /opt/anaconda3/lib/python3.12/site-packages (0.1.8)\n",
      "Collecting langchain-milvus\n",
      "  Using cached langchain_milvus-0.1.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langchain-ollama in /opt/anaconda3/lib/python3.12/site-packages (0.2.3)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-huggingface in /opt/anaconda3/lib/python3.12/site-packages (0.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.13.3)\n",
      "Requirement already satisfied: langchain-experimental in /opt/anaconda3/lib/python3.12/site-packages (0.3.4)\n",
      "Requirement already satisfied: neo4j in /opt/anaconda3/lib/python3.12/site-packages (5.28.1)\n",
      "Requirement already satisfied: json-repair in /opt/anaconda3/lib/python3.12/site-packages (0.39.1)\n",
      "Collecting json-repair\n",
      "  Using cached json_repair-0.40.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.12/site-packages (0.3.8)\n",
      "Collecting langchain-openai\n",
      "  Using cached langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain_community)\n",
      "  Using cached langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (2.6.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (0.1.136)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /opt/anaconda3/lib/python3.12/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchainhub) (23.2)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchainhub) (2.32.0.20241016)\n",
      "Requirement already satisfied: setuptools>69 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (69.0.3)\n",
      "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (1.67.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (6.30.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (1.0.1)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (5.10.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (2.2.2)\n",
      "Requirement already satisfied: milvus-lite>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (2.4.11)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph) (2.0.10)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph) (0.1.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph) (0.1.51)\n",
      "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from tavily-python) (0.27.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.50.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-huggingface) (0.21.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.12/site-packages (from neo4j) (2024.1)\n",
      "Collecting openai<2.0.0,>=1.68.2 (from langchain-openai)\n",
      "  Downloading openai-1.72.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: sgmllib3k in /opt/anaconda3/lib/python3.12/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.7)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (1.0.6)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->tavily-python) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (2.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Using cached langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
      "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Using cached pymilvus-2.5.6-py3-none-any.whl (223 kB)\n",
      "Using cached langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "Downloading langgraph-0.3.27-py3-none-any.whl (142 kB)\n",
      "Using cached tavily_python-0.5.4-py3-none-any.whl (44 kB)\n",
      "Using cached langchain_milvus-0.1.9-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_ollama-0.3.1-py3-none-any.whl (20 kB)\n",
      "Using cached json_repair-0.40.0-py3-none-any.whl (20 kB)\n",
      "Using cached langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
      "Using cached langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading openai-1.72.0-py3-none-any.whl (643 kB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m643.9/643.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, json-repair, arxiv, tavily-python, pymilvus, openai, langchain-core, langchain-text-splitters, langchain-openai, langchain-ollama, langchain-milvus, langchain, langgraph, langchain_community\n",
      "  Attempting uninstall: json-repair\n",
      "    Found existing installation: json_repair 0.39.1\n",
      "    Uninstalling json_repair-0.39.1:\n",
      "      Successfully uninstalled json_repair-0.39.1\n",
      "  Attempting uninstall: arxiv\n",
      "    Found existing installation: arxiv 2.1.3\n",
      "    Uninstalling arxiv-2.1.3:\n",
      "      Successfully uninstalled arxiv-2.1.3\n",
      "  Attempting uninstall: tavily-python\n",
      "    Found existing installation: tavily-python 0.5.1\n",
      "    Uninstalling tavily-python-0.5.1:\n",
      "      Successfully uninstalled tavily-python-0.5.1\n",
      "  Attempting uninstall: pymilvus\n",
      "    Found existing installation: pymilvus 2.5.5\n",
      "    Uninstalling pymilvus-2.5.5:\n",
      "      Successfully uninstalled pymilvus-2.5.5\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.61.0\n",
      "    Uninstalling openai-1.61.0:\n",
      "      Successfully uninstalled openai-1.61.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.43\n",
      "    Uninstalling langchain-core-0.3.43:\n",
      "      Successfully uninstalled langchain-core-0.3.43\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.6\n",
      "    Uninstalling langchain-text-splitters-0.3.6:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.6\n",
      "  Attempting uninstall: langchain-openai\n",
      "    Found existing installation: langchain-openai 0.3.8\n",
      "    Uninstalling langchain-openai-0.3.8:\n",
      "      Successfully uninstalled langchain-openai-0.3.8\n",
      "  Attempting uninstall: langchain-ollama\n",
      "    Found existing installation: langchain-ollama 0.2.3\n",
      "    Uninstalling langchain-ollama-0.2.3:\n",
      "      Successfully uninstalled langchain-ollama-0.2.3\n",
      "  Attempting uninstall: langchain-milvus\n",
      "    Found existing installation: langchain-milvus 0.1.8\n",
      "    Uninstalling langchain-milvus-0.1.8:\n",
      "      Successfully uninstalled langchain-milvus-0.1.8\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.20\n",
      "    Uninstalling langchain-0.3.20:\n",
      "      Successfully uninstalled langchain-0.3.20\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 0.3.5\n",
      "    Uninstalling langgraph-0.3.5:\n",
      "      Successfully uninstalled langgraph-0.3.5\n",
      "  Attempting uninstall: langchain_community\n",
      "    Found existing installation: langchain-community 0.3.19\n",
      "    Uninstalling langchain-community-0.3.19:\n",
      "      Successfully uninstalled langchain-community-0.3.19\n",
      "Successfully installed arxiv-2.2.0 json-repair-0.40.0 langchain-0.3.23 langchain-core-0.3.51 langchain-milvus-0.1.9 langchain-ollama-0.3.1 langchain-openai-0.3.12 langchain-text-splitters-0.3.8 langchain_community-0.3.21 langgraph-0.3.27 openai-1.72.0 pymilvus-2.5.6 tavily-python-0.5.4 xxhash-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U langchain_community arxiv tiktoken langchainhub pymilvus langchain langgraph tavily-python sentence-transformers langchain-milvus langchain-ollama langchain-huggingface beautifulsoup4 langchain-experimental neo4j json-repair langchain-openai langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd90af3e-178c-44d5-b543-2cd617ce5b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   docid                              entityid    type\n",
      "0      0  Retrieval-Augmented Generation (Rag)   Topic\n",
      "1      0                              Graphrag   Topic\n",
      "2      0                    Question Answering   Topic\n",
      "3      0             Query-Based Summarization   Topic\n",
      "4      0                             Haoyu Han  Author\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/fji/Desktop/CSV/entities100.csv\")\n",
    "\n",
    "# Display the first few rows of the dataframe to check if it loaded correctly\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "19499e1d-006d-4084-a96b-3d6998d0924c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for entityid saved to '/Users/fji/Desktop/CSV/entities_embeddings.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "file_path = \"/Users/fji/Desktop/CSV/entities100.csv\"  # Update with the correct path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize the Sentence-Transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for the 'entityid' column\n",
    "embeddings = model.encode(df['entityid'].tolist())\n",
    "\n",
    "# Create a new DataFrame with only docid and embeddings\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': df['docid'],  # Keep docid\n",
    "    'Embedding': [embedding.tolist() for embedding in embeddings]  # Convert embeddings to list\n",
    "})\n",
    "\n",
    "# Save the new DataFrame with only docid and embeddings to a new CSV file\n",
    "output_file_path = \"/Users/fji/Desktop/CSV/entities_embeddings.csv\"  # Update with the output path\n",
    "output_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Embeddings for entityid saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1dc4bce3-8861-4253-b65c-e257d5a4df37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for entityid saved to '/Users/fji/Desktop/CSV/papers_embeddings.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "file_path = \"/Users/fji/Desktop/CSV/papers100.csv\"  # Update with the correct path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize the Sentence-Transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for the 'entityid' column\n",
    "embeddings = model.encode(df['abstract'].tolist())\n",
    "\n",
    "# Create a new DataFrame with only docid and embeddings\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': df['docid'],  # Keep docid\n",
    "    'Embedding': [embedding.tolist() for embedding in embeddings]  # Convert embeddings to list\n",
    "})\n",
    "\n",
    "# Save the new DataFrame with only docid and embeddings to a new CSV file\n",
    "output_file_path = \"/Users/fji/Desktop/CSV/papers_embeddings.csv\"  # Update with the output path\n",
    "output_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Embeddings for entityid saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c21a6d-c83e-42e1-8753-d4b8d031860d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
