title,summary,published,authors,pdf_url,category
"Clinical Document Corpora -- Real Ones, Translated and Synthetic Substitutes, and Assorted Domain Proxies: A Survey of Diversity in Corpus Design, with Focus on German Text Data","We survey clinical document corpora, with focus on German textual data. Due
to rigid data privacy legislation in Germany these resources, with only few
exceptions, are stored in safe clinical data spaces and locked against
clinic-external researchers. This situation stands in stark contrast with
established workflows in the field of natural language processing where easy
accessibility and reuse of data collections are common practice. Hence,
alternative corpus designs have been examined to escape from this data poverty.
Besides machine translation of English clinical datasets and the generation of
synthetic corpora with fictitious clinical contents, several other types of
domain proxies have come up as substitutes for clinical documents. Common
instances of close proxies are medical journal publications, therapy
guidelines, drug labels, etc., more distant proxies include online encyclopedic
medical articles or medical contents from social media channels. After
PRISM-conformant identification of 362 hits from 4 bibliographic systems, 78
relevant documents were finally selected for this review. They contained
overall 92 different published versions of corpora from which 71 were truly
unique in terms of their underlying document sets. Out of these, the majority
were clinical corpora -- 46 real ones, 5 translated ones, and 6 synthetic ones.
As to domain proxies, we identified 18 close and 17 distant ones. There is a
clear divide between the large number of non-accessible authentic clinical
German-language corpora and their publicly accessible substitutes: translated
or synthetic, close or more distant proxies. So on first sight, the data
bottleneck seems broken. Yet differences in genre-specific writing style,
wording and medical domain expertise in this typological space are also
obvious. This raises the question how valid alternative corpus designs really
are.",2024-11-29,Udo Hahn,http://arxiv.org/pdf/2412.00230v2,cs.CL
NushuRescue: Revitalization of the Endangered Nushu Language with AI,"The preservation and revitalization of endangered and extinct languages is a
meaningful endeavor, conserving cultural heritage while enriching fields like
linguistics and anthropology. However, these languages are typically
low-resource, making their reconstruction labor-intensive and costly. This
challenge is exemplified by Nushu, a rare script historically used by Yao women
in China for self-expression within a patriarchal society. To address this
challenge, we introduce NushuRescue, an AI-driven framework designed to train
large language models (LLMs) on endangered languages with minimal data.
NushuRescue automates evaluation and expands target corpora to accelerate
linguistic revitalization. As a foundational component, we developed NCGold, a
500-sentence Nushu-Chinese parallel corpus, the first publicly available
dataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to Nushu
and only 35 short examples from NCGold, NushuRescue achieved 48.69% translation
accuracy on 50 withheld sentences and generated NCSilver, a set of 98 newly
translated modern Chinese sentences of varying lengths. A sample of both NCGold
and NCSilver is included in the Supplementary Materials. Additionally, we
developed FastText-based and Seq2Seq models to further support research on
Nushu. NushuRescue provides a versatile and scalable tool for the
revitalization of endangered languages, minimizing the need for extensive human
input.",2024-11-29,"Ivory Yang, Weicheng Ma, Soroush Vosoughi",http://arxiv.org/pdf/2412.00218v4,cs.CL
Train Once for All: A Transitional Approach for Efficient Aspect Sentiment Triplet Extraction,"Aspect-Opinion Pair Extraction (AOPE) and Aspect Sentiment Triplet Extraction
(ASTE) have drawn growing attention in NLP. However, most existing approaches
extract aspects and opinions independently, optionally adding pairwise
relations, often leading to error propagation and high time complexity. To
address these challenges and being inspired by transition-based dependency
parsing, we propose the first transition-based model for AOPE and ASTE that
performs aspect and opinion extraction jointly, which also better captures
position-aware aspect-opinion relations and mitigates entity-level bias. By
integrating contrastive-augmented optimization, our model delivers more
accurate action predictions and jointly optimizes separate subtasks in linear
time. Extensive experiments on 4 commonly used ASTE/AOPE datasets show that,
while performing worse when trained on a single dataset than some previous
models, our model achieves the best performance on both ASTE and AOPE if
trained on combined datasets, outperforming the strongest previous models in
F1-measures (often by a large margin). We hypothesize that this is due to our
model's ability to learn transition actions from multiple datasets and domains.
Our code is available at https://anonymous.4open.science/r/trans_aste-8FCF.",2024-11-29,"Xinmeng Hou, Lingyue Fu, Chenhao Meng, Kounianhua Du, Wuqi Wang, Hai Hu",http://arxiv.org/pdf/2412.00208v2,cs.CL
Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation,"Recent years have witnessed the success of Multimodal Large Language Models
(MLLMs) in the vision understanding domain. The success of these models can
largely be attributed to the dominant scaling law, which states that larger
parameter sizes and data volumes contribute to better performance. Notably,
data scaling has mainly been powered by automatic data pipelines, which center
around the self-instruction of LLMs. The paradigm has been taken for granted
for quite some time, but the study of the effectiveness of scaling with these
data has been neglected for a long time. In this context, this work revisits
scaling with synthetic data and focuses on developing video-LLMs from a
data-centric perspective. Our main study approach is fine-tuning pre-trained
image-LLMs with video data and investigating learning efficiency through data
scaling. Results from our preliminary experiments reveal a low learning
efficiency phenomenon when simply scaling up video data samples, which, through
our probing, can be ascribed to a lack of instruction diversity. Aiming at this
issue, we propose a data augmentation method called Sparrow, which synthesizes
video-like samples from pure text instruction data. Mixing these synthetic
samples with the video data enables a more efficient training scheme. Through
comprehensive experiments, we demonstrate that our proposed method achieves
performance comparable to or even superior to baselines trained with many more
samples. Meanwhile, we find that incorporating these synthetic samples can
boost the performance of long video understanding without training with long
video data. The code and data examples are available at
https://github.com/VITA-MLLM/Sparrow.",2024-11-29,"Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan Yang, Zuwei Long, Yuhan Dai, Yongdong Luo, Haoyu Cao, Tong Xu, Xing Sun, Caifeng Shan, Ran He, Enhong Chen",http://arxiv.org/pdf/2411.19951v4,cs.CL
Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability,"Mathematical reasoning tasks pose significant challenges for large language
models (LLMs) because they require precise logical deduction and sequence
analysis. In this work, we introduce the concept of critical tokens -- elements
within reasoning trajectories that significantly influence incorrect outcomes.
We present a novel framework for identifying these tokens through rollout
sampling and demonstrate their substantial divergence from traditional error
tokens. Through extensive experiments on datasets such as GSM8K and MATH500, we
show that identifying and replacing critical tokens significantly improves
model accuracy. We propose an efficient methodology for pinpointing these
tokens in large-scale datasets using contrastive estimation and extend this
framework to enhance model training processes with direct preference
optimization (DPO). Experimental results on GSM8K and MATH500 benchmarks with
the widely used models Llama-3 (8B and 70B) and Deepseek-math (7B) demonstrate
the effectiveness of the proposed approach, cDPO. Our results underscore the
potential of leveraging critical tokens to reduce errors in reasoning tasks,
advancing the development of AI systems capable of robust logical deduction.
Our code, annotated datasets, and trained models are available at
https://github.com/chenzhiling9954/Critical-Tokens-Matter to support and
encourage future research in this promising field.",2024-11-29,"Zicheng Lin, Tian Liang, Jiahao Xu, Qiuzhi Lin, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, Zhaopeng Tu",http://arxiv.org/pdf/2411.19943v3,cs.CL
Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark,"Following the successful 2023 edition, we organised the Second Perception
Test challenge as a half-day workshop alongside the IEEE/CVF European
Conference on Computer Vision (ECCV) 2024, with the goal of benchmarking
state-of-the-art video models and measuring the progress since last year using
the Perception Test benchmark. This year, the challenge had seven tracks (up
from six last year) and covered low-level and high-level tasks, with language
and non-language interfaces, across video, audio, and text modalities; the
additional track covered hour-long video understanding and introduced a novel
video QA benchmark 1h-walk VQA. Overall, the tasks in the different tracks
were: object tracking, point tracking, temporal action localisation, temporal
sound localisation, multiple-choice video question-answering, grounded video
question-answering, and hour-long video question-answering. We summarise in
this report the challenge tasks and results, and introduce in detail the novel
hour-long video QA benchmark 1h-walk VQA.",2024-11-29,"Joseph Heyward, João Carreira, Dima Damen, Andrew Zisserman, Viorica Pătrăucean",http://arxiv.org/pdf/2411.19941v1,cs.CL
VLSBench: Unveiling Visual Leakage in Multimodal Safety,"Safety concerns of Multimodal large language models (MLLMs) have gradually
become an important problem in various applications. Surprisingly, previous
works indicate a counterintuitive phenomenon that using textual unlearning to
align MLLMs achieves comparable safety performances with MLLMs aligned with
image text pairs. To explain such a phenomenon, we discover a Visual Safety
Information Leakage (VSIL) problem in existing multimodal safety benchmarks,
i.e., the potentially risky content in the image has been revealed in the
textual query. Thus, MLLMs can easily refuse these sensitive image-text pairs
according to textual queries only, leading to unreliable cross-modality safety
evaluation of MLLMs. We also conduct a further comparison experiment between
textual alignment and multimodal alignment to highlight this drawback. To this
end, we construct multimodal Visual Leakless Safety Bench (VLSBench) with 2.2k
image-text pairs through an automated data pipeline. Experimental results
indicate that VLSBench poses a significant challenge to both open-source and
close-source MLLMs, e.g., LLaVA, Qwen2-VL and GPT-4o. Besides, we empirically
compare textual and multimodal alignment methods on VLSBench and find that
textual alignment is effective enough for multimodal safety scenarios with
VSIL, while multimodal alignment is preferable for safety scenarios without
VSIL. Code and data are released under https://github.com/AI45Lab/VLSBench",2024-11-29,"Xuhao Hu, Dongrui Liu, Hao Li, Xuanjing Huang, Jing Shao",http://arxiv.org/pdf/2411.19939v3,cs.CL
On Domain-Specific Post-Training for Multimodal Large Language Models,"Adapting general multimodal large language models (MLLMs) to specific
domains, such as scientific and industrial fields, is highly significant in
promoting their practical applications. This paper systematically investigates
domain adaptation of MLLMs through post-training, focusing on data synthesis,
training pipelines, and task evaluation. (1) Data Synthesis: Using only
open-source models, we develop a generate-then-filter pipeline that curates
diverse visual instruction tasks based on domain-specific image-caption pairs.
The resulting data surpass the data synthesized by manual rules or strong
closed-source models (e.g., GPT-4V) in enhancing domain-specific performance.
(2) Training Pipeline: While the two-stage training--initially on image-caption
pairs followed by visual instruction tasks--is commonly adopted for developing
general MLLMs, we apply a single-stage training pipeline to enhance task
diversity for domain-specific post-training. (3) Task Evaluation: We conduct
extensive experiments in high-impact domains such as biomedicine, food, and
remote sensing, by post-training a variety of MLLMs and then evaluating MLLM
performance on various domain-specific tasks. Furthermore, we fully open-source
our models, code, and data to encourage future research in this area.",2024-11-29,"Daixuan Cheng, Shaohan Huang, Ziyu Zhu, Xintong Zhang, Wayne Xin Zhao, Zhongzhi Luan, Bo Dai, Zhenliang Zhang",http://arxiv.org/pdf/2411.19930v2,cs.CL
SIMS: Simulating Stylized Human-Scene Interactions with Retrieval-Augmented Script Generation,"Simulating stylized human-scene interactions (HSI) in physical environments
is a challenging yet fascinating task. Prior works emphasize long-term
execution but fall short in achieving both diverse style and physical
plausibility. To tackle this challenge, we introduce a novel hierarchical
framework named SIMS that seamlessly bridges highlevel script-driven intent
with a low-level control policy, enabling more expressive and diverse
human-scene interactions. Specifically, we employ Large Language Models with
Retrieval-Augmented Generation (RAG) to generate coherent and diverse long-form
scripts, providing a rich foundation for motion planning. A versatile
multicondition physics-based control policy is also developed, which leverages
text embeddings from the generated scripts to encode stylistic cues,
simultaneously perceiving environmental geometries and accomplishing task
goals. By integrating the retrieval-augmented script generation with the
multi-condition controller, our approach provides a unified solution for
generating stylized HSI motions. We further introduce a comprehensive planning
dataset produced by RAG and a stylized motion dataset featuring diverse
locomotions and interactions. Extensive experiments demonstrate SIMS's
effectiveness in executing various tasks and generalizing across different
scenarios, significantly outperforming previous methods.",2024-11-29,"Wenjia Wang, Liang Pan, Zhiyang Dou, Jidong Mei, Zhouyingcheng Liao, Yuke Lou, Yifan Wu, Lei Yang, Jingbo Wang, Taku Komura",http://arxiv.org/pdf/2411.19921v2,cs.CL
Classical and Quantum Algorithms for the Deterministic L-system Inductive Inference Problem,"L-systems can be made to model and create simulations of many biological
processes, such as plant development. Finding an L-system for a given process
is typically solved by hand, by experts, in a massively time-consuming process.
It would be significant if this could be done automatically from data, such as
from sequences of images. In this paper, we are interested in inferring a
particular type of L-system, deterministic context-free L-system (D0L-system)
from a sequence of strings. We introduce the characteristic graph of a sequence
of strings, which we then utilize to translate our problem (inferring
D0L-system) in polynomial time into the maximum independent set problem (MIS)
and the SAT problem. After that, we offer a classical exact algorithm and an
approximate quantum algorithm for the problem.",2024-11-29,"Ali Lotfi, Ian McQuillan, Steven Rayan",http://arxiv.org/pdf/2411.19906v2,cs.CL
AIDetx: a compression-based method for identification of machine-learning generated text,"This paper introduces AIDetx, a novel method for detecting machine-generated
text using data compression techniques. Traditional approaches, such as deep
learning classifiers, often suffer from high computational costs and limited
interpretability. To address these limitations, we propose a compression-based
classification framework that leverages finite-context models (FCMs). AIDetx
constructs distinct compression models for human-written and AI-generated text,
classifying new inputs based on which model achieves a higher compression
ratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores
exceeding 97% and 99%, respectively, highlighting its high accuracy. Compared
to current methods, such as large language models (LLMs), AIDetx offers a more
interpretable and computationally efficient solution, significantly reducing
both training time and hardware requirements (e.g., no GPUs needed). The full
implementation is publicly available at https://github.com/AIDetx/AIDetx.",2024-11-29,"Leonardo Almeida, Pedro Rodrigues, Diogo Magalhães, Armando J. Pinho, Diogo Pratas",http://arxiv.org/pdf/2411.19869v1,cs.CL
Reverse Thinking Makes LLMs Stronger Reasoners,"Reverse thinking plays a crucial role in human reasoning. Humans can reason
not only from a problem to a solution but also in reverse, i.e., start from the
solution and reason towards the problem. This often enhances overall reasoning
performance as it enables consistency checks between their forward and backward
thinking. To enable Large Language Models (LLMs) to perform reverse thinking,
we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data
augmentation and learning objectives. In RevThink, we augment the dataset by
collecting structured forward-backward reasoning from a teacher model,
consisting of: (1) the original question, (2) forward reasoning, (3) backward
question, and (4) backward reasoning. We then employ three objectives to train
a smaller student model in a multi-task learning fashion: (a) generate forward
reasoning from a question, (b) generate a backward question from a question,
and (c) generate backward reasoning from the backward question. Experiments
across 12 datasets covering commonsense, math, and logical reasoning show an
average 13.53% improvement over the student model's zero-shot performance and a
6.84% improvement over the strongest knowledge distillation baselines.
Moreover, our method demonstrates sample efficiency -- using only 10% of the
correct forward reasoning from the training data, it outperforms a standard
fine-tuning method trained on 10x more forward reasoning. RevThink also
exhibits strong generalization to out-of-distribution held-out datasets.",2024-11-29,"Justin Chih-Yao Chen, Zifeng Wang, Hamid Palangi, Rujun Han, Sayna Ebrahimi, Long Le, Vincent Perot, Swaroop Mishra, Mohit Bansal, Chen-Yu Lee, Tomas Pfister",http://arxiv.org/pdf/2411.19865v2,cs.CL
What fifty-one years of Linguistics and Artificial Intelligence research tell us about their correlation: A scientometric review,"There is a strong correlation between linguistics and artificial intelligence
(AI), best manifested by deep learning language models. This study provides a
thorough scientometric analysis of this correlation, synthesizing the
intellectual production during 51 years, from 1974 to 2024. It involves 5750
Web of Science-indexed articles published in 2124 journals, which are written
by 20835 authors belonging to 13773 research centers in 794 countries. Two
powerful software, viz., CiteSpace and VOSviewer, were used to generate mapping
visualizations of the intellectual landscape, trending issues and (re)emerging
hotspots. The results indicate that in the 1980s and 1990s, linguistics and AI
research was not robust, characterized by unstable publication over time. It
has, however, witnessed a remarkable increase of publication since then,
reaching 1478 articles in 2023, and 546 articles in January-March timespan in
2024, involving emerging issues and hotspots, addressing new horizons, new
topics, and launching new applications and powerful deep learning language
models including ChatGPT.",2024-11-29,Mohammed Q. Shormani,http://arxiv.org/pdf/2411.19858v1,cs.CL
Artificial intelligence contribution to translation industry: looking back and forward,"This study provides a comprehensive analysis of artificial intelligence (AI)
contribution to translation industry (ACTI) research, synthesizing it over
forty-one years from 1980-2024. 13220 articles were retrieved from three
sources, namely WoS, Scopus, and Lens. We provided two types of analysis, viz.,
scientometric and thematic, focusing on cluster, subject categories, keywords,
burstness, centrality and research centers as for the former. For the latter,
we thematically review 18 articles, selected purposefully from the articles
involved, centering on purpose, approach, findings, and contribution to ACTI
future directions. The findings reveal that in the past AI contribution to
translation industry was not rigorous, resulting in rule-based machine
translation and statistical machine translation whose output was not
satisfactory. However, the more AI develops, the more machine translation
develops, incorporating Neural Networking Algorithms and (Deep) Language
Learning Models like ChatGPT whose translation output has developed
considerably. However, much rigorous research is still needed to overcome
several problems encountering translation industry, specifically concerning
low-source languages, multi-dialectical and free word order languages, and
cultural and religious registers.",2024-11-29,Mohammed Q. Shormani,http://arxiv.org/pdf/2411.19855v2,cs.CL
Sensitive Content Classification in Social Media: A Holistic Resource and Evaluation,"The detection of sensitive content in large datasets is crucial for ensuring
that shared and analysed data is free from harmful material. However, current
moderation tools, such as external APIs, suffer from limitations in
customisation, accuracy across diverse sensitive categories, and privacy
concerns. Additionally, existing datasets and open-source models focus
predominantly on toxic language, leaving gaps in detecting other sensitive
categories such as substance abuse or self-harm. In this paper, we put forward
a unified dataset tailored for social media content moderation across six
sensitive categories: conflictual language, profanity, sexually explicit
material, drug-related content, self-harm, and spam. By collecting and
annotating data with consistent retrieval strategies and guidelines, we address
the shortcomings of previous focalised research. Our analysis demonstrates that
fine-tuning large language models (LLMs) on this novel dataset yields
significant improvements in detection performance compared to open
off-the-shelf models such as LLaMA, and even proprietary OpenAI models, which
underperform by 10-15% overall. This limitation is even more pronounced on
popular moderation APIs, which cannot be easily tailored to specific sensitive
content categories, among others.",2024-11-29,"Dimosthenis Antypas, Indira Sen, Carla Perez-Almendros, Jose Camacho-Collados, Francesco Barbieri",http://arxiv.org/pdf/2411.19832v2,cs.CL
AI-assisted summary of suicide risk Formulation,"Background: Formulation, associated with suicide risk assessment, is an
individualised process that seeks to understand the idiosyncratic nature and
development of an individual's problems. Auditing clinical documentation on an
electronic health record (EHR) is challenging as it requires resource-intensive
manual efforts to identify keywords in relevant sections of specific forms.
Furthermore, clinicians and healthcare professionals often do not use keywords;
their clinical language can vary greatly and may contain various jargon and
acronyms. Also, the relevant information may be recorded elsewhere. This study
describes how we developed advanced Natural Language Processing (NLP)
algorithms, a branch of Artificial Intelligence (AI), to analyse EHR data
automatically. Method: Advanced Optical Character Recognition techniques were
used to process unstructured data sets, such as portable document format (pdf)
files. Free text data was cleaned and pre-processed using Normalisation of Free
Text techniques. We developed algorithms and tools to unify the free text.
Finally, the formulation was checked for the presence of each concept based on
similarity using NLP-powered semantic matching techniques. Results: We
extracted information indicative of formulation and assessed it to cover the
relevant concepts. This was achieved using a Weighted Score to obtain a
Confidence Level. Conclusion: The rigour to which formulation is completed is
crucial to effectively using EHRs, ensuring correct and timely identification,
engagement and interventions that may potentially avoid many suicide attempts
and suicides.",2024-11-29,"Rajib Rana, Niall Higgins, Kazi N. Haque, John Reilly, Kylie Burke, Kathryn Turner, Anthony R. Pisani, Terry Stedman",http://arxiv.org/pdf/2412.10388v2,cs.CL
SDR-GNN: Spectral Domain Reconstruction Graph Neural Network for Incomplete Multimodal Learning in Conversational Emotion Recognition,"Multimodal Emotion Recognition in Conversations (MERC) aims to classify
utterance emotions using textual, auditory, and visual modal features. Most
existing MERC methods assume each utterance has complete modalities,
overlooking the common issue of incomplete modalities in real-world scenarios.
Recently, graph neural networks (GNNs) have achieved notable results in
Incomplete Multimodal Emotion Recognition in Conversations (IMERC). However,
traditional GNNs focus on binary relationships between nodes, limiting their
ability to capture more complex, higher-order information. Moreover, repeated
message passing can cause over-smoothing, reducing their capacity to preserve
essential high-frequency details. To address these issues, we propose a
Spectral Domain Reconstruction Graph Neural Network (SDR-GNN) for incomplete
multimodal learning in conversational emotion recognition. SDR-GNN constructs
an utterance semantic interaction graph using a sliding window based on both
speaker and context relationships to model emotional dependencies. To capture
higher-order and high-frequency information, SDR-GNN utilizes weighted
relationship aggregation, ensuring consistent semantic feature extraction
across utterances. Additionally, it performs multi-frequency aggregation in the
spectral domain, enabling efficient recovery of incomplete modalities by
extracting both high- and low-frequency information. Finally, multi-head
attention is applied to fuse and optimize features for emotion recognition.
Extensive experiments on various real-world datasets demonstrate that our
approach is effective in incomplete multimodal learning and outperforms current
state-of-the-art methods.",2024-11-29,"Fangze Fu, Wei Ai, Fan Yang, Yuntao Shou, Tao Meng, Keqin Li",http://arxiv.org/pdf/2411.19822v1,cs.CL
INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge,"The performance differential of large language models (LLM) between languages
hinders their effective deployment in many regions, inhibiting the potential
economic and societal value of generative AI tools in many communities.
However, the development of functional LLMs in many languages (\ie,
multilingual LLMs) is bottlenecked by the lack of high-quality evaluation
resources in languages other than English. Moreover, current practices in
multilingual benchmark construction often translate English resources, ignoring
the regional and cultural knowledge of the environments in which multilingual
systems would be used. In this work, we construct an evaluation suite of
197,243 QA pairs from local exam sources to measure the capabilities of
multilingual LLMs in a variety of regional contexts. Our novel resource,
INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across
44 written languages that evaluates multilingual LLMs for performance in the
actual language environments where they would be deployed.",2024-11-29,"Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, Börje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzemiński, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani Moakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag, Marzieh Fadaee, Sara Hooker, Antoine Bosselut",http://arxiv.org/pdf/2411.19799v1,cs.CL
MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks,"Recently, human motion analysis has experienced great improvement due to
inspiring generative models such as the denoising diffusion model and large
language model. While the existing approaches mainly focus on generating
motions with textual descriptions and overlook the reciprocal task. In this
paper, we present~\textbf{MoTe}, a unified multi-modal model that could handle
diverse tasks by learning the marginal, conditional, and joint distributions of
motion and text simultaneously. MoTe enables us to handle the paired
text-motion generation, motion captioning, and text-driven motion generation by
simply modifying the input context. Specifically, MoTe is composed of three
components: Motion Encoder-Decoder (MED), Text Encoder-Decoder (TED), and
Moti-on-Text Diffusion Model (MTDM). In particular, MED and TED are trained for
extracting latent embeddings, and subsequently reconstructing the motion
sequences and textual descriptions from the extracted embeddings, respectively.
MTDM, on the other hand, performs an iterative denoising process on the input
context to handle diverse tasks. Experimental results on the benchmark datasets
demonstrate the superior performance of our proposed method on text-to-motion
generation and competitive performance on motion captioning.",2024-11-29,"Yiming Wu, Wei Ji, Kecheng Zheng, Zicheng Wang, Dong Xu",http://arxiv.org/pdf/2411.19786v1,cs.CL
PerLA: Perceptive 3D Language Assistant,"Enabling Large Language Models (LLMs) to understand the 3D physical world is
an emerging yet challenging research direction. Current strategies for
processing point clouds typically downsample the scene or divide it into
smaller parts for separate analysis. However, both approaches risk losing key
local details or global contextual information. In this paper, we introduce
PerLA, a 3D language assistant designed to be more perceptive to both details
and context, making visual representations more informative for the LLM. PerLA
captures high-resolution (local) details in parallel from different point cloud
areas and integrates them with (global) context obtained from a
lower-resolution whole point cloud. We present a novel algorithm that preserves
point cloud locality through the Hilbert curve and effectively aggregates
local-to-global information via cross-attention and a graph neural network.
Lastly, we introduce a novel loss for local representation consensus to promote
training stability. PerLA outperforms state-of-the-art 3D language assistants,
with gains of up to +1.34 CiDEr on ScanQA for question answering, and +4.22 on
ScanRefer and +3.88 on Nr3D for dense captioning.
https://gfmei.github.io/PerLA/",2024-11-29,"Guofeng Mei, Wei Lin, Luigi Riz, Yujiao Wu, Fabio Poiesi, Yiming Wang",http://arxiv.org/pdf/2411.19774v2,cs.CL
LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos,"Despite impressive advancements in video understanding, most efforts remain
limited to coarse-grained or visual-only video tasks. However, real-world
videos encompass omni-modal information (vision, audio, and speech) with a
series of events forming a cohesive storyline. The lack of multi-modal video
data with fine-grained event annotations and the high cost of manual labeling
are major obstacles to comprehensive omni-modality video perception. To address
this gap, we propose an automatic pipeline consisting of high-quality
multi-modal video filtering, semantically coherent omni-modal event boundary
detection, and cross-modal correlation-aware event captioning. In this way, we
present LongVALE, the first-ever Vision-Audio-Language Event understanding
benchmark comprising 105K omni-modal events with precise temporal boundaries
and detailed relation-aware captions within 8.4K high-quality long videos.
Further, we build a baseline that leverages LongVALE to enable video large
language models (LLMs) for omni-modality fine-grained temporal video
understanding for the first time. Extensive experiments demonstrate the
effectiveness and great potential of LongVALE in advancing comprehensive
multi-modal video understanding.",2024-11-29,"Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, Feng Zheng",http://arxiv.org/pdf/2411.19772v3,cs.CL
Noro: A Noise-Robust One-shot Voice Conversion System with Hidden Speaker Representation Capabilities,"One-shot voice conversion (VC) aims to alter the timbre of speech from a
source speaker to match that of a target speaker using just a single reference
speech from the target, while preserving the semantic content of the original
source speech. Despite advancements in one-shot VC, its effectiveness decreases
in real-world scenarios where reference speeches, often sourced from the
internet, contain various disturbances like background noise. To address this
issue, we introduce Noro, a Noise Robust One-shot VC system. Noro features
innovative components tailored for VC using noisy reference speeches, including
a dual-branch reference encoding module and a noise-agnostic contrastive
speaker loss. Experimental results demonstrate that Noro outperforms our
baseline system in both clean and noisy scenarios, highlighting its efficacy
for real-world applications. Additionally, we investigate the hidden speaker
representation capabilities of our baseline system by repurposing its reference
encoder as a speaker encoder. The results shows that it is competitive with
several advanced self-supervised learning models for speaker representation
under the SUPERB settings, highlighting the potential for advancing speaker
representation learning through one-shot VC task.",2024-11-29,"Haorui He, Yuchen Song, Yuancheng Wang, Haoyang Li, Xueyao Zhang, Li Wang, Gongping Huang, Eng Siong Chng, Zhizheng Wu",http://arxiv.org/pdf/2411.19770v1,cs.CL
To Ensemble or Not: Assessing Majority Voting Strategies for Phishing Detection with Large Language Models,"The effectiveness of Large Language Models (LLMs) significantly relies on the
quality of the prompts they receive. However, even when processing identical
prompts, LLMs can yield varying outcomes due to differences in their training
processes. To leverage the collective intelligence of multiple LLMs and enhance
their performance, this study investigates three majority voting strategies for
text classification, focusing on phishing URL detection. The strategies are:
(1) a prompt-based ensemble, which utilizes majority voting across the
responses generated by a single LLM to various prompts; (2) a model-based
ensemble, which entails aggregating responses from multiple LLMs to a single
prompt; and (3) a hybrid ensemble, which combines the two methods by sending
different prompts to multiple LLMs and then aggregating their responses. Our
analysis shows that ensemble strategies are most suited in cases where
individual components exhibit equivalent performance levels. However, when
there is a significant discrepancy in individual performance, the effectiveness
of the ensemble method may not exceed that of the highest-performing single LLM
or prompt. In such instances, opting for ensemble techniques is not
recommended.",2024-11-29,"Fouad Trad, Ali Chehab",http://arxiv.org/pdf/2412.00166v1,cs.CL
A Deep Learning Approach to Language-independent Gender Prediction on Twitter,"This work presents a set of experiments conducted to predict the gender of
Twitter users based on language-independent features extracted from the text of
the users' tweets. The experiments were performed on a version of TwiSty
dataset including tweets written by the users of six different languages:
Portuguese, French, Dutch, English, German, and Italian. Logistic regression
(LR), and feed-forward neural networks (FFNN) with back-propagation were used
to build models in two different settings: Inter-Lingual (IL) and Cross-Lingual
(CL). In the IL setting, the training and testing were performed on the same
language whereas in the CL, Italian and German datasets were set aside and only
used as test sets and the rest were combined to compose training and
development sets. In the IL, the highest accuracy score belongs to LR whereas
in the CL, FFNN with three hidden layers yields the highest score. The results
show that neural network based models underperform traditional models when the
size of the training set is small; however, they beat traditional models by a
non-trivial margin, when they are fed with large enough data. Finally, the
feature analysis confirms that men and women have different writing styles
independent of their language.",2024-11-29,"Reyhaneh Hashempour, Barbara Plank, Aline Villavicencio, Renato Cordeiro de Amorim",http://arxiv.org/pdf/2411.19733v1,cs.CL
Towards Santali Linguistic Inclusion: Building the First Santali-to-English Translation Model using mT5 Transformer and Data Augmentation,"Around seven million individuals in India, Bangladesh, Bhutan, and Nepal
speak Santali, positioning it as nearly the third most commonly used
Austroasiatic language. Despite its prominence among the Austroasiatic language
family's Munda subfamily, Santali lacks global recognition. Currently, no
translation models exist for the Santali language. Our paper aims to include
Santali to the NPL spectrum. We aim to examine the feasibility of building
Santali translation models based on available Santali corpora. The paper
successfully addressed the low-resource problem and, with promising results,
examined the possibility of creating a functional Santali machine translation
model in a low-resource setup. Our study shows that Santali-English parallel
corpus performs better when in transformers like mt5 as opposed to untrained
transformers, proving that transfer learning can be a viable technique that
works with Santali language. Besides the mT5 transformer, Santali-English
performs better than Santali-Bangla parallel corpus as the mT5 has been trained
in way more English data than Bangla data. Lastly, our study shows that with
data augmentation, our model performs better.",2024-11-29,"Syed Mohammed Mostaque Billah, Ateya Ahmed Subarna, Sudipta Nandi Sarna, Ahmad Shawkat Wasit, Anika Fariha, Asif Sushmit, Arig Yousuf Sadeque",http://arxiv.org/pdf/2411.19726v1,cs.CL
TakeLab Retriever: AI-Driven Search Engine for Articles from Croatian News Outlets,"TakeLab Retriever is an AI-driven search engine designed to discover,
collect, and semantically analyze news articles from Croatian news outlets. It
offers a unique perspective on the history and current landscape of Croatian
online news media, making it an essential tool for researchers seeking to
uncover trends, patterns, and correlations that general-purpose search engines
cannot provide. TakeLab retriever utilizes cutting-edge natural language
processing (NLP) methods, enabling users to sift through articles using named
entities, phrases, and topics through the web application. This technical
report is divided into two parts: the first explains how TakeLab Retriever is
utilized, while the second provides a detailed account of its design. In the
second part, we also address the software engineering challenges involved and
propose solutions for developing a microservice-based semantic search engine
capable of handling over ten million news articles published over the past two
decades.",2024-11-29,"David Dukić, Marin Petričević, Sven Ćurković, Jan Šnajder",http://arxiv.org/pdf/2411.19718v1,cs.CL
MIMDE: Exploring the Use of Synthetic vs Human Data for Evaluating Multi-Insight Multi-Document Extraction Tasks,"Large language models (LLMs) have demonstrated remarkable capabilities in
text analysis tasks, yet their evaluation on complex, real-world applications
remains challenging. We define a set of tasks, Multi-Insight Multi-Document
Extraction (MIMDE) tasks, which involves extracting an optimal set of insights
from a document corpus and mapping these insights back to their source
documents. This task is fundamental to many practical applications, from
analyzing survey responses to processing medical records, where identifying and
tracing key insights across documents is crucial. We develop an evaluation
framework for MIMDE and introduce a novel set of complementary human and
synthetic datasets to examine the potential of synthetic data for LLM
evaluation. After establishing optimal metrics for comparing extracted
insights, we benchmark 20 state-of-the-art LLMs on both datasets. Our analysis
reveals a strong correlation (0.71) between the ability of LLMs to extracts
insights on our two datasets but synthetic data fails to capture the complexity
of document-level analysis. These findings offer crucial guidance for the use
of synthetic data in evaluating text analysis systems, highlighting both its
potential and limitations.",2024-11-29,"John Francis, Saba Esnaashari, Anton Poletaev, Sukankana Chakraborty, Youmna Hashem, Jonathan Bright",http://arxiv.org/pdf/2411.19689v1,cs.CL
ChineseWebText 2.0: Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information,"During the development of large language models (LLMs), pre-training data
play a critical role in shaping LLMs' capabilities. In recent years several
large-scale and high-quality pre-training datasets have been released to
accelerate the research of LLMs, including ChineseWebText1.0, C4, Pile,
WanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has
increasingly shifted to domain-specific capabilities and safety concerns,
making those previous coarse-grained texts insufficient for meeting training
requirements. Furthermore, fine-grained information, such as quality, domain
and toxicity, is becoming increasingly important in building powerful and
reliable LLMs for various scenarios. To address these challenges, in this paper
we propose a new tool-chain called MDFG-tool for constructing large-scale and
high-quality Chinese datasets with multi-dimensional and fine-grained
information. First, we employ manually crafted rules to discard explicit noisy
texts from raw contents. Second, the quality evaluation model, domain
classifier, and toxicity evaluation model are well-designed to assess the
remaining cleaned data respectively. Finally, we integrate these three types of
fine-grained information for each text. With this approach, we release the
largest, high-quality and fine-grained Chinese text ChineseWebText2.0, which
consists of 3.8TB and each text is associated with a quality score, domain
labels, a toxicity label and a toxicity score, facilitating the LLM researchers
to select data based on various types of fine-grained information. The data,
codes and the tool-chain are available on this website
https://github.com/CASIA-LM/ChineseWebText-2.0",2024-11-29,"Wanyue Zhang, Ziyong Li, Wen Yang, Chunlin Leng, Yinan Bai, Qianlong Du, Chengqing Zong, Jiajun Zhang",http://arxiv.org/pdf/2411.19668v1,cs.CL
Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis,"After the introduction of Large Language Models (LLMs), there have been
substantial improvements in the performance of Natural Language Generation
(NLG) tasks, including Text Summarization and Machine Translation. However,
LLMs still produce outputs containing hallucinations, that is, content not
grounded in factual information. Therefore, developing methods to assess the
factuality of LLMs has become urgent.
  Indeed, resources for factuality evaluation have recently emerged. Although
challenging, these resources face one or more of the following limitations: (i)
they are tailored to a specific task or domain; (ii) they are limited in size,
thereby preventing the training of new factuality evaluators; (iii) they are
designed for simpler verification tasks, such as claim verification.
  To address these issues, we introduce LLM-Oasis, to the best of our knowledge
the largest resource for training end-to-end factuality evaluators. LLM-Oasis
is constructed by extracting claims from Wikipedia, falsifying a subset of
these claims, and generating pairs of factual and unfactual texts. We then rely
on human annotators to both validate the quality of our dataset and to create a
gold standard test set for benchmarking factuality evaluation systems.
  Our experiments demonstrate that LLM-Oasis presents a significant challenge
for state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in our
proposed end-to-end factuality evaluation task, highlighting its potential to
drive future research in the field.",2024-11-29,"Alessandro Scirè, Andrei Stefan Bejgu, Simone Tedeschi, Karim Ghonim, Federico Martelli, Roberto Navigli",http://arxiv.org/pdf/2411.19655v3,cs.CL
CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation,"The advancement of large Vision-Language-Action (VLA) models has
significantly improved robotic manipulation in terms of language-guided task
execution and generalization to unseen scenarios. While existing VLAs adapted
from pretrained large Vision-Language-Models (VLM) have demonstrated promising
generalizability, their task performance is still unsatisfactory as indicated
by the low tasks success rates in different environments. In this paper, we
present a new advanced VLA architecture derived from VLM. Unlike previous works
that directly repurpose VLM for action prediction by simple action
quantization, we propose a omponentized VLA architecture that has a specialized
action module conditioned on VLM output. We systematically study the design of
the action module and demonstrates the strong performance enhancement with
diffusion action transformers for action sequence modeling, as well as their
favorable scaling behaviors. We also conduct comprehensive experiments and
ablation studies to evaluate the efficacy of our models with varied designs.
The evaluation on 5 robot embodiments in simulation and real work shows that
our model not only significantly surpasses existing VLAs in task performance
and but also exhibits remarkable adaptation to new robots and generalization to
unseen objects and backgrounds. It exceeds the average success rates of OpenVLA
which has similar model size (7B) with ours by over 35% in simulated evaluation
and 55% in real robot experiments. It also outperforms the large RT-2-X model
(55B) by 18% absolute success rates in simulation. Code and models can be found
on our project page (https://cogact.github.io/).",2024-11-29,"Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, Baining Guo",http://arxiv.org/pdf/2411.19650v1,cs.CL
LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification,"With the ever-increasing number of news stories available online, classifying
them by topic, regardless of the language they are written in, has become
crucial for enhancing readers' access to relevant content. To address this
challenge, we propose a teacher-student framework based on large language
models (LLMs) for developing multilingual news classification models of
reasonable size with no need for manual data annotation. The framework employs
a Generative Pretrained Transformer (GPT) model as the teacher model to develop
an IPTC Media Topic training dataset through automatic annotation of news
articles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits
a high zero-shot performance on all four languages. Its agreement with human
annotators is comparable to that between the human annotators themselves. To
mitigate the computational limitations associated with the requirement of
processing millions of texts daily, smaller BERT-like student models are
fine-tuned on the GPT-annotated dataset. These student models achieve high
performance comparable to the teacher model. Furthermore, we explore the impact
of the training data size on the performance of the student models and
investigate their monolingual, multilingual and zero-shot cross-lingual
capabilities. The findings indicate that student models can achieve high
performance with a relatively small number of training instances, and
demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the
best-performing news topic classifier, enabling multilingual classification
with the top-level categories of the IPTC Media Topic schema.",2024-11-29,"Taja Kuzman, Nikola Ljubešić",http://arxiv.org/pdf/2411.19638v1,cs.CL
Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and the Empirical Findings,"The excessive use of visual tokens in existing Multimoal Large Language
Models (MLLMs) often exhibits obvious redundancy and brings in prohibitively
expensive computation. To gain insights into this problem, we first conduct
extensive empirical studies on the attention behaviors of MLLMs, and summarize
three main inference stages in MLLMs: (i) Early fusion between tokens is first
accomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)
Multimodal reasoning} resumes and lasts until the end of inference. In
particular, we reveal that visual tokens will stop contributing to reasoning
when the text tokens receive enough image information, yielding obvious visual
redundancy. Based on these generalized observations, we propose a simple yet
effective method to improve the efficiency of MLLMs, termed dynamic
visual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive
the text token status and decide the removal of all visual tokens after a
certain layer, thereby addressing the observed visual redundancy. To validate
VTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,
and conduct extensive experiments on a bunch of benchmarks. The experiment
results not only show the effectiveness of our VTE in improving MLLMs'
efficiency, but also yield the general modeling patterns of MLLMs, well
facilitating the in-depth understanding of MLLMs. Our code is anonymously
released at https://github.com/DoubtedSteam/DyVTE.",2024-11-29,"Qiong Wu, Wenhao Lin, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji",http://arxiv.org/pdf/2411.19628v1,cs.CL
Can Large Language Models Reason about the Region Connection Calculus?,"Qualitative Spatial Reasoning is a well explored area of Knowledge
Representation and Reasoning and has multiple applications ranging from
Geographical Information Systems to Robotics and Computer Vision. Recently,
many claims have been made for the reasoning capabilities of Large Language
Models (LLMs). Here, we investigate the extent to which a set of representative
LLMs can perform classical qualitative spatial reasoning tasks on the
mereotopological Region Connection Calculus, RCC-8. We conduct three pairs of
experiments (reconstruction of composition tables, alignment to human
composition preferences, conceptual neighbourhood reconstruction) using
state-of-the-art LLMs; in each pair one experiment uses eponymous relations and
one, anonymous relations (to test the extent to which the LLM relies on
knowledge about the relation names obtained during training). All instances are
repeated 30 times to measure the stochasticity of the LLMs.",2024-11-29,"Anthony G Cohn, Robert E Blackwell",http://arxiv.org/pdf/2411.19589v1,cs.CL
In-Context Learning with Noisy Labels,"In-context learning refers to the emerging ability of large language models
(LLMs) to perform a target task without additional training, utilizing
demonstrations of the task. Recent studies aim to enhance in-context learning
performance by selecting more useful demonstrations. However, they overlook the
presence of inevitable noisy labels in task demonstrations that arise during
the labeling process in the real-world. In this paper, we propose a new task,
in-context learning with noisy labels, which aims to solve real-world problems
for in-context learning where labels in task demonstrations would be corrupted.
Moreover, we propose a new method and baseline methods for the new task,
inspired by studies in learning with noisy labels. Through experiments, we
demonstrate that our proposed method can serve as a safeguard against
performance degradation in in-context learning caused by noisy labels.",2024-11-29,"Junyong Kang, Donghyun Son, Hwanjun Song, Buru Chang",http://arxiv.org/pdf/2411.19581v1,cs.CL
ICPR 2024 Competition on Multilingual Claim-Span Identification,"A lot of claims are made in social media posts, which may contain
misinformation or fake news. Hence, it is crucial to identify claims as a first
step towards claim verification. Given the huge number of social media posts,
the task of identifying claims needs to be automated. This competition deals
with the task of 'Claim Span Identification' in which, given a text, parts /
spans that correspond to claims are to be identified. This task is more
challenging than the traditional binary classification of text into claim or
not-claim, and requires state-of-the-art methods in Pattern Recognition,
Natural Language Processing and Machine Learning. For this competition, we used
a newly developed dataset called HECSI containing about 8K posts in English and
about 8K posts in Hindi with claim-spans marked by human annotators. This paper
gives an overview of the competition, and the solutions developed by the
participating teams.",2024-11-29,"Soham Poddar, Biswajit Paul, Moumita Basu, Saptarshi Ghosh",http://arxiv.org/pdf/2411.19579v1,cs.CL
KV Shifting Attention Enhances Language Modeling,"The current large language models are mainly based on decode-only structure
transformers, which have great in-context learning (ICL) capabilities. It is
generally believed that the important foundation of its ICL capability is the
induction heads mechanism, which requires at least two layers attention. In
order to more efficiently implement the ability of the model's induction, we
revisit the induction heads mechanism and proposed a KV shifting attention. We
theoretically prove that the KV shifting attention reducing the model's
requirements for the depth and width of the induction heads mechanism. Our
experimental results demonstrate that KV shifting attention is beneficial to
learning induction heads and language modeling, which lead to better
performance or faster convergence from toy models to the pre-training models
with more than 10 B parameters.",2024-11-29,"Mingyu Xu, Wei Cheng, Bingning Wang, Weipeng Chen",http://arxiv.org/pdf/2411.19574v2,cs.CL
Ensemble Watermarks for Large Language Models,"The rapid advancement of large language models (LLMs) has made it
increasingly difficult to distinguish between text written by humans and
machines. While watermarks already exist for LLMs, they often lack flexibility,
and struggle with attacks such as paraphrasing. To address these issues, we
propose a multi-feature method for generating watermarks that combines multiple
distinct watermark features into an ensemble watermark. Concretely, we combine
acrostica and sensorimotor norms with the established red-green watermark to
achieve a 98% detection rate. After a paraphrasing attack the performance
remains high with 95% detection rate. The red-green feature alone as baseline
achieves a detection rate of 49%. The evaluation of all feature combinations
reveals that the ensemble of all three consistently has the highest detection
rate across several LLMs and watermark strength settings. Due to the
flexibility of combining features in the ensemble, various requirements and
trade-offs can be addressed. Additionally, for all ensemble configurations the
same detection function can be used without adaptations. This method is
particularly of interest to facilitate accountability and prevent societal
harm.",2024-11-29,"Georg Niess, Roman Kern",http://arxiv.org/pdf/2411.19563v1,cs.CL
Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning,"Low-rank adapters have become standard for efficiently fine-tuning large
language models (LLMs), but they often fall short of achieving the performance
of full fine-tuning. We propose a method, LoRA Silver Bullet or LoRA-SB, that
approximates full fine-tuning within low-rank subspaces using a carefully
designed initialization strategy. We theoretically demonstrate that the
architecture of LoRA-XS, which inserts a learnable (r x r) matrix between B and
A while keeping other matrices fixed, provides the precise conditions needed
for this approximation. We leverage its constrained update space to achieve
optimal scaling for high-rank gradient updates while removing the need for
hyperparameter tuning. We prove that our initialization offers an optimal
low-rank approximation of the initial gradient and preserves update directions
throughout training. Extensive experiments across mathematical reasoning,
commonsense reasoning, and language understanding tasks demonstrate that our
approach exceeds the performance of standard LoRA while using \textbf{27-90}
times fewer learnable parameters, and comprehensively outperforms LoRA-XS. Our
findings establish that it is possible to simulate full fine-tuning in low-rank
subspaces, and achieve significant efficiency gains without sacrificing
performance. Our code is publicly available at
https://github.com/RaghavSinghal10/lora-sb.",2024-11-29,"Kaustubh Ponkshe, Raghav Singhal, Eduard Gorbunov, Alexey Tumanov, Samuel Horvath, Praneeth Vepakomma",http://arxiv.org/pdf/2411.19557v3,cs.CL
Training Agents with Weakly Supervised Feedback from Large Language Models,"Large Language Models (LLMs) offer a promising basis for creating agents that
can tackle complex tasks through iterative environmental interaction. Existing
methods either require these agents to mimic expert-provided trajectories or
rely on definitive environmental feedback for reinforcement learning which
limits their application to specific scenarios like gaming or code generation.
This paper introduces a novel training method for LLM-based agents using weakly
supervised signals from a critic LLM, bypassing the need for expert
trajectories or definitive feedback. Our agents are trained in iterative
manner, where they initially generate trajectories through environmental
interaction. Subsequently, a critic LLM selects a subset of good trajectories,
which are then used to update the agents, enabling them to generate improved
trajectories in the next iteration. Extensive tests on the API-bank dataset
show consistent improvement in our agents' capabilities and comparable
performance to GPT-4, despite using open-source models with much fewer
parameters.",2024-11-29,"Dihong Gong, Pu Lu, Zelong Wang, Meng Zhou, Xiuqiang He",http://arxiv.org/pdf/2411.19547v1,cs.CL
Knowledge Management for Automobile Failure Analysis Using Graph RAG,"This paper presents a knowledge management system for automobile failure
analysis using retrieval-augmented generation (RAG) with large language models
(LLMs) and knowledge graphs (KGs). In the automotive industry, there is a
growing demand for knowledge transfer of failure analysis from experienced
engineers to young engineers. However, failure events are phenomena that occur
in a chain reaction, making them difficult for beginners to analyze them. While
knowledge graphs, which can describe semantic relationships and structure
information is effective in representing failure events, due to their
capability of representing the relationships between components, there is much
information in KGs, so it is challenging for young engineers to extract and
understand sub-graphs from the KG. On the other hand, there is increasing
interest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for
knowledge management. However, when using the current Graph RAG framework with
an existing knowledge graph for automobile failures, several issues arise
because it is difficult to generate executable queries for a knowledge graph
database which is not constructed by LLMs. To address this, we focused on
optimizing the Graph RAG pipeline for existing knowledge graphs. Using an
original Q&A dataset, the ROUGE F1 score of the sentences generated by the
proposed method showed an average improvement of 157.6% compared to the current
method. This highlights the effectiveness of the proposed method for automobile
failure analysis.",2024-11-29,"Yuta Ojima, Hiroki Sakaji, Tadashi Nakamura, Hiroaki Sakata, Kazuya Seki, Yuu Teshigawara, Masami Yamashita, Kazuhiro Aoyama",http://arxiv.org/pdf/2411.19539v1,cs.CL
TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with Scalable Context and Symbolic Extension,"The advent of large language models (LLMs) has unlocked great opportunities
in complex data management tasks, particularly in question answering (QA) over
complicated multi-table relational data. Despite significant progress,
systematically evaluating LLMs on multi-table QA remains a critical challenge
due to the inherent complexity of analyzing heterogeneous table structures and
potential large scale of serialized relational data. Existing benchmarks
primarily focus on single-table QA, failing to capture the intricacies of
reasoning across multiple relational tables, as required in real-world domains
such as finance, healthcare, and e-commerce. To address this gap, we present
TQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities
of LLMs in tackling complex QA tasks over relational data. Our benchmark
incorporates diverse relational database instances sourced from real-world
public datasets and introduces a flexible sampling mechanism to create tasks
with varying multi-table context lengths, ranging from 8K to 64K tokens. To
ensure robustness and reliability, we integrate symbolic extensions into the
evaluation framework, enabling the assessment of LLM reasoning capabilities
beyond simple data retrieval or probabilistic pattern matching. We
systematically evaluate a range of LLMs, both open-source and closed-source,
spanning model scales from 7 billion to 70 billion parameters. Our extensive
experiments reveal critical insights into the performance of LLMs in
multi-table QA, highlighting both challenges and opportunities for advancing
their application in complex, data-driven environments. Our benchmark
implementation and results are available at
https://github.com/Relaxed-System-Lab/TQA-Bench.",2024-11-29,"Zipeng Qiu, You Peng, Guangxin He, Binhang Yuan, Chen Wang",http://arxiv.org/pdf/2411.19504v1,cs.CL
COLD: Causal reasOning in cLosed Daily activities,"Large Language Models (LLMs) have shown state-of-the-art performance in a
variety of tasks, including arithmetic and reasoning; however, to gauge the
intellectual capabilities of LLMs, causal reasoning has become a reliable proxy
for validating a general understanding of the mechanics and intricacies of the
world similar to humans. Previous works in natural language processing (NLP)
have either focused on open-ended causal reasoning via causal commonsense
reasoning (CCR) or framed a symbolic representation-based question answering
for theoretically backed-up analysis via a causal inference engine. The former
adds an advantage of real-world grounding but lacks theoretically backed-up
analysis/validation, whereas the latter is far from real-world grounding. In
this work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed
Daily activities) framework, which is built upon human understanding of daily
real-world activities to reason about the causal nature of events. We show that
the proposed framework facilitates the creation of enormous causal queries (~ 9
million) and comes close to the mini-turing test, simulating causal reasoning
to evaluate the understanding of a daily real-world task. We evaluate multiple
LLMs on the created causal queries and find that causal reasoning is
challenging even for activities trivial to humans. We further explore (the
causal reasoning abilities of LLMs) using the backdoor criterion to determine
the causal strength between events.",2024-11-29,"Abhinav Joshi, Areeb Ahmad, Ashutosh Modi",http://arxiv.org/pdf/2411.19500v1,cs.CL
BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching,"Large language models (LLMs) increasingly play an important role in a wide
range of information processing and management tasks. Many of these tasks are
performed in large batches or even offline, and the performance indictor for
which is throughput. These tasks usually show the characteristic of prefix
sharing, where different prompt input can partially show the common prefix.
However, the existing LLM inference engines tend to optimize the streaming
requests and show limitations of supporting the large batched tasks with the
prefix sharing characteristic. The existing solutions use the LRU-based cache
to reuse the KV context of common prefix between requests. The KV context that
are about to be reused may prematurely evicted with the implicit cache
management. Besides, the streaming oriented systems do not leverage the
request-batch information and can not mix the decoding tokens with the prefill
chunks to the best for the batched scenarios, and thus fails to saturate the
GPU. We propose BatchLLM to address the above problems. BatchLLM explicitly
identifies the common prefixes globally. The requests sharing the same prefix
will be scheduled together to reuse the KV context the best. BatchLLM reorders
the requests and schedules the requests with larger ratio of decoding first to
better mix the decoding tokens with the latter prefill chunks, and applies
memory-centric token batching to enlarge the token-batch sizes, which helps to
increase the GPU utilization. Finally, BatchLLM optimizes the prefix-shared
Attention kernel with horizontal fusion to reduce tail effect and kernel launch
overhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang
by 1.3$\times$ to 10.8$\times$ on a set of microbenchmarks and a typical
industry workload under different hardware environments.",2024-11-29,"Zhen Zheng, Xin Ji, Taosong Fang, Fanghao Zhou, Chuanjie Liu, Gang Peng",http://arxiv.org/pdf/2412.03594v2,cs.CL
Simple and Provable Scaling Laws for the Test-Time Compute of Large Language Models,"We propose two simple, principled and practical algorithms that enjoy
provable scaling laws for the test-time compute of large language models
(LLMs). The first one is a two-stage knockout-style algorithm: given an input
problem, it first generates multiple candidate solutions, and then aggregate
them via a knockout tournament for the final output. Assuming that the LLM can
generate a correct solution with non-zero probability and do better than a
random guess in comparing a pair of correct and incorrect solutions, we prove
theoretically that the failure probability of this algorithm decays to zero
exponentially or by a power law (depending on the specific way of scaling) as
its test-time compute grows. The second one is a two-stage league-style
algorithm, where each candidate is evaluated by its average win rate against
multiple opponents, rather than eliminated upon loss to a single opponent.
Under analogous but more robust assumptions, we prove that its failure
probability also decays to zero exponentially with more test-time compute. Both
algorithms require a black-box LLM and nothing else (e.g., no verifier or
reward model) for a minimalistic implementation, which makes them appealing for
practical applications and easy to adapt for different tasks. Through extensive
experiments with diverse models and datasets, we validate the proposed theories
and demonstrate the outstanding scaling properties of both algorithms.",2024-11-29,"Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou",http://arxiv.org/pdf/2411.19477v4,cs.CL
Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension Ability,"Large language models (LLMs) have shown remarkable capability in natural
language tasks, yet debate persists on whether they truly comprehend deep
structure (i.e., core semantics) or merely rely on surface structure (e.g.,
presentation format). Prior studies observe that LLMs' performance declines
when intervening on surface structure, arguing their success relies on surface
structure recognition. However, surface structure sensitivity does not prevent
deep structure comprehension. Rigorously evaluating LLMs' capability requires
analyzing both, yet deep structure is often overlooked. To this end, we assess
LLMs' comprehension ability using causal mediation analysis, aiming to fully
discover the capability of using both deep and surface structures.
Specifically, we formulate the comprehension of deep structure as direct causal
effect (DCE) and that of surface structure as indirect causal effect (ICE),
respectively. To address the non-estimability of original DCE and ICE --
stemming from the infeasibility of isolating mutual influences of deep and
surface structures, we develop the corresponding quantifiable surrogates,
including approximated DCE (ADCE) and approximated ICE (AICE). We further apply
the ADCE to evaluate a series of mainstream LLMs, showing that most of them
exhibit deep structure comprehension ability, which grows along with the
prediction accuracy. Comparing ADCE and AICE demonstrates closed-source LLMs
rely more on deep structure, while open-source LLMs are more surface-sensitive,
which decreases with model scale. Theoretically, ADCE is a bidirectional
evaluation, which measures both the sufficiency and necessity of deep structure
changes in causing output variations, thus offering a more comprehensive
assessment than accuracy, a common evaluation in LLMs. Our work provides new
insights into LLMs' deep structure comprehension and offers novel methods for
LLMs evaluation.",2024-11-29,"Yujin Han, Lei Xu, Sirui Chen, Difan Zou, Chaochao Lu",http://arxiv.org/pdf/2411.19456v1,cs.CL
Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models,"Iterative retrieval refers to the process in which the model continuously
queries the retriever during generation to enhance the relevance of the
retrieved knowledge, thereby improving the performance of Retrieval-Augmented
Generation (RAG). Existing work typically employs few-shot prompting or
manually constructed rules to implement iterative retrieval. This introduces
additional inference overhead and overlooks the remarkable reasoning
capabilities of Large Language Models (LLMs). In this paper, we introduce
Auto-RAG, an autonomous iterative retrieval model centered on the LLM's
powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues
with the retriever, systematically planning retrievals and refining queries to
acquire valuable knowledge. This process continues until sufficient external
information is gathered, at which point the results are presented to the user.
To this end, we develop a method for autonomously synthesizing reasoning-based
decision-making instructions in iterative retrieval and fine-tuned the latest
open-source LLMs. The experimental results indicate that Auto-RAG is capable of
autonomous iterative interaction with the retriever, effectively leveraging the
remarkable reasoning and decision-making abilities of LLMs, which lead to
outstanding performance across six benchmarks. Further analysis reveals that
Auto-RAG can autonomously adjust the number of iterations based on the
difficulty of the questions and the utility of the retrieved knowledge, without
requiring any human intervention. Moreover, Auto-RAG expresses the iterative
retrieval process in natural language, enhancing interpretability while
providing users with a more intuitive experience\footnote{Code is available at
\url{https://github.com/ictnlp/Auto-RAG}.",2024-11-29,"Tian Yu, Shaolei Zhang, Yang Feng",http://arxiv.org/pdf/2411.19443v1,cs.CL
Actions and Objects Pathways for Domain Adaptation in Video Question Answering,"In this paper, we introduce the Actions and Objects Pathways (AOPath) for
out-of-domain generalization in video question answering tasks. AOPath
leverages features from a large pretrained model to enhance generalizability
without the need for explicit training on the unseen domains. Inspired by human
brain, AOPath dissociates the pretrained features into action and object
features, and subsequently processes them through separate reasoning pathways.
It utilizes a novel module which converts out-of-domain features into
domain-agnostic features without introducing any trainable weights. We validate
the proposed approach on the TVQA dataset, which is partitioned into multiple
subsets based on genre to facilitate the assessment of generalizability. The
proposed approach demonstrates 5% and 4% superior performance over conventional
classifiers on out-of-domain and in-domain datasets, respectively. It also
outperforms prior methods that involve training millions of parameters, whereas
the proposed approach trains very few parameters.",2024-11-29,"Safaa Abdullahi Moallim Mohamud, Ho-Young Jung",http://arxiv.org/pdf/2411.19434v1,cs.CL
Libra: Leveraging Temporal Images for Biomedical Radiology Analysis,"Radiology report generation (RRG) requires advanced medical image analysis,
effective temporal reasoning, and accurate text generation. While multimodal
large language models (MLLMs) align with pre-trained vision encoders to enhance
visual-language understanding, most existing methods rely on single-image
analysis or rule-based heuristics to process multiple images, failing to fully
leverage temporal information in multi-modal medical datasets. In this paper,
we introduce Libra, a temporal-aware MLLM tailored for chest X-ray report
generation. Libra combines a radiology-specific image encoder with a novel
Temporal Alignment Connector (TAC), designed to accurately capture and
integrate temporal differences between paired current and prior images.
Extensive experiments on the MIMIC-CXR dataset demonstrate that Libra
establishes a new state-of-the-art benchmark among similarly scaled MLLMs,
setting new standards in both clinical relevance and lexical accuracy.",2024-11-28,"Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho",http://arxiv.org/pdf/2411.19378v2,cs.CL
DENIAHL: In-Context Features Influence LLM Needle-In-A-Haystack Abilities,"The Needle-in-a-haystack (NIAH) test is a general task used to assess
language models' (LMs') abilities to recall particular information from long
input context. This framework however does not provide a means of analyzing
what factors, beyond context length, contribute to LMs' abilities or
inabilities to separate and recall needles from their haystacks. To provide a
systematic means of assessing what features contribute to LMs' NIAH
capabilities, we developed a synthetic benchmark called DENIAHL (Data-oriented
Evaluation of NIAH for LLM's). Our work expands on previous NIAH studies by
ablating NIAH features beyond typical context length including data type, size,
and patterns. We find stark differences between GPT-3.5 and LLaMA 2-7B's
performance on DENIAHL, and drops in recall performance when features like item
size are increased, and to some degree when data type is changed from numbers
to letters. This has implications for increasingly large context models,
demonstrating factors beyond item-number impact NIAH capabilities.",2024-11-28,"Hui Dai, Dan Pechi, Xinyi Yang, Garvit Banga, Raghav Mantri",http://arxiv.org/pdf/2411.19360v1,cs.CL
CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections,"In the era of foundation models, CLIP has emerged as a powerful tool for
aligning text & visual modalities into a common embedding space. However, the
alignment objective used to train CLIP often results in subpar visual features
for fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at
extracting rich visual features due to their specialized training paradigm.
Yet, these SSL models require an additional supervised linear probing step,
which relies on fully labeled data which is often expensive and difficult to
obtain at scale. In this paper, we propose a label-free prompt-tuning method
that leverages the rich visual features of self-supervised learning models
(DINO) and the broad textual knowledge of large language models (LLMs) to
largely enhance CLIP-based image classification performance using unlabeled
images. Our approach unfolds in three key steps: (1) We generate robust textual
feature embeddings that more accurately represent object classes by leveraging
class-specific descriptions from LLMs, enabling more effective zero-shot
classification compared to CLIP's default name-specific prompts. (2) These
textual embeddings are then used to produce pseudo-labels to train an alignment
module that integrates the complementary strengths of LLM description-based
textual embeddings & DINO's visual features. (3) Finally, we prompt-tune CLIP's
vision encoder through DINO-assisted supervision using the trained alignment
module. This three-step process allows us to harness the best of visual &
textual foundation models, resulting in a powerful and efficient approach that
surpasses state-of-the-art label-free classification methods. Notably, our
framework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6%
over the state-of-the-art LaFTer across 11 diverse image classification
datasets. Our code & models can be found at https://github.com/fazliimam/NoLA.",2024-11-28,"Mohamed Fazli Imam, Rufael Fedaku Marew, Jameel Hassan, Mustansar Fiaz, Alham Fikri Aji, Hisham Cholakkal",http://arxiv.org/pdf/2411.19346v3,cs.CL
MAG-V: A Multi-Agent Framework for Synthetic Data Generation and Verification,"Extending the capabilities of Large Language Models (LLMs) with functions or
tools for environment interaction has led to the emergence of the agent
paradigm. In industry, training an LLM is not always feasible because of the
scarcity of domain data, legal holds on proprietary customer data, rapidly
changing business requirements, and the need to prototype new assistants.
Agents provide an elegant solution to the above by relying on the zero-shot
reasoning abilities of the underlying LLM and utilizing tools to explore and
reason over customer data and respond to user requests. However, there are two
concerns here: (I) acquiring large scale customer queries for agent testing is
time-consuming, and (II) high reliance on the tool call sequence (or
trajectory) followed by the agent to respond to user queries may lead to
unexpected or incorrect behavior. To address this, we propose MAG-V, a
multi-agent framework to first generate a dataset of questions that mimic
customer queries; and second, reverse-engineer alternate questions from the
responses for trajectory verification. Initial results indicate that our
synthetic data can improve agent performance on actual customer queries.
Furthermore, our trajectory verification methodology, inspired by distant
supervision and using traditional machine learning (ML) models, outperforms a
GPT-4o judge baseline by 11% accuracy and matches the performance of a GPT-4
judge on our constructed dataset. Overall, our approach is a step towards
unifying diverse task agents into a cohesive framework for achieving an aligned
objective.",2024-11-28,"Saptarshi Sengupta, Harsh Vashistha, Kristal Curtis, Akshay Mallipeddi, Abhinav Mathur, Joseph Ross, Liang Gou",http://arxiv.org/pdf/2412.04494v2,cs.CL
Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation,"Open-Vocabulary Segmentation (OVS) aims at segmenting images from free-form
textual concepts without predefined training classes. While existing
vision-language models such as CLIP can generate segmentation masks by
leveraging coarse spatial information from Vision Transformers, they face
challenges in spatial localization due to their global alignment of image and
text features. Conversely, self-supervised visual models like DINO excel in
fine-grained visual encoding but lack integration with language. To bridge this
gap, we present Talk2DINO, a novel hybrid approach that combines the spatial
accuracy of DINOv2 with the language understanding of CLIP. Our approach aligns
the textual embeddings of CLIP to the patch-level features of DINOv2 through a
learned mapping function without the need to fine-tune the underlying
backbones. At training time, we exploit the attention maps of DINOv2 to
selectively align local visual patches with textual embeddings. We show that
the powerful semantic and localization abilities of Talk2DINO can enhance the
segmentation process, resulting in more natural and less noisy segmentations,
and that our approach can also effectively distinguish foreground objects from
the background. Experimental results demonstrate that Talk2DINO achieves
state-of-the-art performance across several unsupervised OVS benchmarks. Source
code and models are publicly available at:
https://lorebianchi98.github.io/Talk2DINO/.",2024-11-28,"Luca Barsellotti, Lorenzo Bianchi, Nicola Messina, Fabio Carrara, Marcella Cornia, Lorenzo Baraldi, Fabrizio Falchi, Rita Cucchiara",http://arxiv.org/pdf/2411.19331v1,cs.CL
Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers,"Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a
wide variety of vision-language (VL) tasks such as image captioning or visual
question answering. Despite strong performance, LMMs are not directly suited
for foundational discriminative vision-language tasks (i.e., tasks requiring
discrete label predictions) such as image classification and multiple-choice
VQA. One key challenge in utilizing LMMs for discriminative tasks is the
extraction of useful features from generative models. To overcome this issue,
we propose an approach for finding features in the model's latent space to more
effectively leverage LMMs for discriminative tasks. Toward this end, we present
Sparse Attention Vectors (SAVs) -- a finetuning-free method that leverages
sparse attention head activations (fewer than 1\% of the heads) in LMMs as
strong features for VL tasks. With only few-shot examples, SAVs demonstrate
state-of-the-art performance compared to a variety of few-shot and finetuned
baselines on a collection of discriminative tasks. Our experiments also imply
that SAVs can scale in performance with additional examples and generalize to
similar tasks, establishing SAVs as both effective and robust multimodal
feature representations.",2024-11-28,"Chancharik Mitra, Brandon Huang, Tianning Chai, Zhiqiu Lin, Assaf Arbelle, Rogerio Feris, Leonid Karlinsky, Trevor Darrell, Deva Ramanan, Roei Herzig",http://arxiv.org/pdf/2412.00142v2,cs.CL
Extracting Information in a Low-resource Setting: Case Study on Bioinformatics Workflows,"Bioinformatics workflows are essential for complex biological data analyses
and are often described in scientific articles with source code in public
repositories. Extracting detailed workflow information from articles can
improve accessibility and reusability but is hindered by limited annotated
corpora. To address this, we framed the problem as a low-resource extraction
task and tested four strategies: 1) creating a tailored annotated corpus, 2)
few-shot named-entity recognition (NER) with an autoregressive language model,
3) NER using masked language models with existing and new corpora, and 4)
integrating workflow knowledge into NER models. Using BioToFlow, a new corpus
of 52 articles annotated with 16 entities, a SciBERT-based NER model achieved a
70.4 F-measure, comparable to inter-annotator agreement. While knowledge
integration improved performance for specific entities, it was less effective
across the entire information schema. Our results demonstrate that
high-performance information extraction for bioinformatics workflows is
achievable.",2024-11-28,"Clémence Sebe, Sarah Cohen-Boulakia, Olivier Ferret, Aurélie Névéol",http://arxiv.org/pdf/2411.19295v2,cs.CL
Consolidating and Developing Benchmarking Datasets for the Nepali Natural Language Understanding Tasks,"The Nepali language has distinct linguistic features, especially its complex
script (Devanagari script), morphology, and various dialects, which pose a
unique challenge for natural language processing (NLP) evaluation. While the
Nepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a
foundation for evaluating models, it remains limited in scope, covering four
tasks. This restricts their utility for comprehensive assessments of NLP
models. To address this limitation, we introduce eight new datasets, creating a
new benchmark, the Nepali Language Understanding Evaluation (NLUE) benchmark,
which covers a total of 12 tasks for evaluating the performance of models
across a diverse set of Natural Language Understanding (NLU) tasks. The added
tasks include single-sentence classification, similarity and paraphrase tasks,
and Natural Language Inference (NLI) tasks. On evaluating the models using
added tasks, we observe that the existing models fall short in handling complex
NLU tasks effectively. This expanded benchmark sets a new standard for
evaluating, comparing, and advancing models, contributing significantly to the
broader goal of advancing NLP research for low-resource languages.",2024-11-28,"Jinu Nyachhyon, Mridul Sharma, Prajwal Thapa, Bal Krishna Bal",http://arxiv.org/pdf/2411.19244v1,cs.CL
How far can bias go? -- Tracing bias from pretraining data to alignment,"As LLMs are increasingly integrated into user-facing applications, addressing
biases that perpetuate societal inequalities is crucial. While much work has
gone into measuring or mitigating biases in these models, fewer studies have
investigated their origins. Therefore, this study examines the correlation
between gender-occupation bias in pre-training data and their manifestation in
LLMs, focusing on the Dolma dataset and the OLMo model. Using zero-shot
prompting and token co-occurrence analyses, we explore how biases in training
data influence model outputs. Our findings reveal that biases present in
pre-training data are amplified in model outputs. The study also examines the
effects of prompt types, hyperparameters, and instruction-tuning on bias
expression, finding instruction-tuning partially alleviating representational
bias while still maintaining overall stereotypical gender associations, whereas
hyperparameters and prompting variation have a lesser effect on bias
expression. Our research traces bias throughout the LLM development pipeline
and underscores the importance of mitigating bias at the pretraining stage.",2024-11-28,"Marion Thaler, Abdullatif Köksal, Alina Leidinger, Anna Korhonen, Hinrich Schütze",http://arxiv.org/pdf/2411.19240v1,cs.CL
An Extensive Evaluation of Factual Consistency in Large Language Models for Data-to-Text Generation,"Large Language Models (LLMs) have shown exceptional performance across
various Data-to-Text Generation (DTG) tasks. However, generating factually
consistent text in DTG remains challenging for LLMs. Despite this, in-depth
evaluations of LLM factual consistency for DTG remain missing in the current
literature. This paper addresses this gap by providing an extensive evaluation
of factual consistency in LLMs for DTG. Our evaluation covers five widely used
DTG datasets (E2E, ViGGo, WikiTableText, DART, and WebNLG) and five prominent
LLM families (T5, BART, OPT, BLOOM, and Llama 2). To ensure a thorough
evaluation of factual consistency, we use four state-of-the-art automatic
metrics and include essential human assessments. Our extensive evaluations
reveals three key findings regarding factual consistency in LLMs for DTG.
First, Llama 2 often excels in generating factually consistent text, although
smaller models like T5 and BART can achieve strong factual consistency on
larger, lexically less-diverse datasets. Second, the average rate of change
(AROC) indicates that increasing model size (number of model trainable
parameters) generally enhances factual consistency of LLMs in DTG. Third, we
observe that source-reference divergence (i.e., when the reference text
diverges semantically from the source) typically reduces the factual
consistency of LLMs in DTG.",2024-11-28,"Joy Mahapatra, Utpal Garain",http://arxiv.org/pdf/2411.19203v1,cs.CL
Beyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection & Grounding in VLMs,"The rapid development of Large Multimodal Models (LMMs) has significantly
advanced multimodal understanding by harnessing the language abilities of Large
Language Models (LLMs) and integrating modality-specific encoders. However,
LMMs are plagued by hallucinations that limit their reliability and adoption.
While traditional methods to detect and mitigate these hallucinations often
involve costly training or rely heavily on external models, recent approaches
utilizing internal model features present a promising alternative. In this
paper, we critically assess the limitations of the state-of-the-art
training-free technique, the logit lens, in handling generalized visual
hallucinations. We introduce ContextualLens, a refined method that leverages
contextual token embeddings from middle layers of LMMs. This approach
significantly improves hallucination detection and grounding across diverse
categories, including actions and OCR, while also excelling in tasks requiring
contextual understanding, such as spatial relations and attribute comparison.
Our novel grounding technique yields highly precise bounding boxes,
facilitating a transition from Zero-Shot Object Segmentation to Grounded Visual
Question Answering. Our contributions pave the way for more reliable and
interpretable multimodal models.",2024-11-28,"Anirudh Phukan, Divyansh, Harshit Kumar Morj, Vaishnavi, Apoorv Saxena, Koustava Goswami",http://arxiv.org/pdf/2411.19187v2,cs.CL
Examining Multimodal Gender and Content Bias in ChatGPT-4o,"This study investigates ChatGPT-4o's multimodal content generation,
highlighting significant disparities in its treatment of sexual content and
nudity versus violent and drug-related themes. Detailed analysis reveals that
ChatGPT-4o consistently censors sexual content and nudity, while showing
leniency towards violence and drug use. Moreover, a pronounced gender bias
emerges, with female-specific content facing stricter regulation compared to
male-specific content. This disparity likely stems from media scrutiny and
public backlash over past AI controversies, prompting tech companies to impose
stringent guidelines on sensitive issues to protect their reputations. Our
findings emphasize the urgent need for AI systems to uphold genuine ethical
standards and accountability, transcending mere political correctness. This
research contributes to the understanding of biases in AI-driven language and
multimodal models, calling for more balanced and ethical content moderation
practices.",2024-11-28,Roberto Balestri,http://arxiv.org/pdf/2411.19140v1,cs.CL
Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads,"We introduce Orthus, an autoregressive (AR) transformer that excels in
generating images given textual prompts, answering questions based on visual
inputs, and even crafting lengthy image-text interleaved contents. Unlike prior
arts on unified multimodal modeling, Orthus simultaneously copes with discrete
text tokens and continuous image features under the AR modeling principle. The
continuous treatment of visual signals minimizes the information loss for both
image understanding and generation while the fully AR formulation renders the
characterization of the correlation between modalities straightforward. The key
mechanism enabling Orthus to leverage these advantages lies in its
modality-specific heads -- one regular language modeling (LM) head predicts
discrete text tokens and one diffusion head generates continuous image features
conditioning on the output of the backbone. We devise an efficient strategy for
building Orthus -- by substituting the Vector Quantization (VQ) operation in
the existing unified AR model with a soft alternative, introducing a diffusion
head, and tuning the added modules to reconstruct images, we can create an
Orthus-base model effortlessly (e.g., within mere 72 A100 GPU hours).
Orthus-base can further embrace post-training to better model interleaved
images and texts. Empirically, Orthus surpasses competing baselines including
Show-o and Chameleon across standard benchmarks, achieving a GenEval score of
0.58 and an MME-P score of 1265.8 using 7B parameters. Orthus also shows
exceptional mixed-modality generation capabilities, reflecting the potential
for handling intricate practical generation tasks.",2024-11-28,"Siqi Kou, Jiachun Jin, Zhihong Liu, Chang Liu, Ye Ma, Jian Jia, Quan Chen, Peng Jiang, Zhijie Deng",http://arxiv.org/pdf/2412.00127v2,cs.CL
Integration of Contextual Descriptors in Ontology Alignment for Enrichment of Semantic Correspondence,"This paper proposes a novel approach to semantic ontology alignment using
contextual descriptors. A formalization was developed that enables the
integration of essential and contextual descriptors to create a comprehensive
knowledge model. The hierarchical structure of the semantic approach and the
mathematical apparatus for analyzing potential conflicts between concepts,
particularly in the example of ""Transparency"" and ""Privacy"" in the context of
artificial intelligence, are demonstrated. Experimental studies showed a
significant improvement in ontology alignment metrics after the implementation
of contextual descriptors, especially in the areas of privacy, responsibility,
and freedom & autonomy. The application of contextual descriptors achieved an
average overall improvement of approximately 4.36%. The results indicate the
effectiveness of the proposed approach for more accurately reflecting the
complexity of knowledge and its contextual dependence.",2024-11-28,"Eduard Manziuk, Oleksander Barmak, Pavlo Radiuk, Vladislav Kuznetsov, Iurii Krak, Sergiy Yakovlev",http://arxiv.org/pdf/2411.19113v1,cs.CL
VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models,"In this paper, we introduce an open-source Korean-English vision-language
model (VLM), VARCO-VISION. We incorporate a step-by-step training strategy that
allows a model learn both linguistic and visual information while preserving
the backbone model's knowledge. Our model demonstrates outstanding performance
in diverse settings requiring bilingual image-text understanding and generation
abilities compared to models of similar size. VARCO-VISION is also capable of
grounding, referring, and OCR, expanding its usage and potential applications
for real-world scenarios. In addition to the model, we release five Korean
evaluation datasets, including four closed-set and one openset benchmarks. We
anticipate that our milestone will broaden the opportunities for AI researchers
aiming to train VLMs. VARCO-VISION is available at
https://huggingface.co/NCSOFT/VARCO-VISION-14B.",2024-11-28,"Jeongho Ju, Daeyoung Kim, SunYoung Park, Youngjune Kim",http://arxiv.org/pdf/2411.19103v1,cs.CL
Pralekha: An Indic Document Alignment Evaluation Benchmark,"Mining parallel document pairs poses a significant challenge because existing
sentence embedding models often have limited context windows, preventing them
from effectively capturing document-level information. Another overlooked issue
is the lack of concrete evaluation benchmarks comprising high-quality parallel
document pairs for assessing document-level mining approaches, particularly for
Indic languages. In this study, we introduce Pralekha, a large-scale benchmark
for document-level alignment evaluation. Pralekha includes over 2 million
documents, with a 1:2 ratio of unaligned to aligned pairs, covering 11 Indic
languages and English. Using Pralekha, we evaluate various document-level
mining approaches across three dimensions: the embedding models, the
granularity levels, and the alignment algorithm. To address the challenge of
aligning documents using sentence and chunk-level alignments, we propose a
novel scoring method, Document Alignment Coefficient (DAC). DAC demonstrates
substantial improvements over baseline pooling approaches, particularly in
noisy scenarios, achieving average gains of 20-30% in precision and 15-20% in
F1 score. These results highlight DAC's effectiveness in parallel document
mining for Indic languages.",2024-11-28,"Sanjay Suryanarayanan, Haiyue Song, Mohammed Safi Ur Rahman Khan, Anoop Kunchukuttan, Mitesh M. Khapra, Raj Dabre",http://arxiv.org/pdf/2411.19096v1,cs.CL
Efficient Learning Content Retrieval with Knowledge Injection,"With the rise of online education platforms, there is a growing abundance of
educational content across various domain. It can be difficult to navigate the
numerous available resources to find the most suitable training, especially in
domains that include many interconnected areas, such as ICT. In this study, we
propose a domain-specific chatbot application that requires limited resources,
utilizing versions of the Phi language model to help learners with educational
content. In the proposed method, Phi-2 and Phi-3 models were fine-tuned using
QLoRA. The data required for fine-tuning was obtained from the Huawei Talent
Platform, where courses are available at different levels of expertise in the
field of computer science. RAG system was used to support the model, which was
fine-tuned by 500 Q&A pairs. Additionally, a total of 420 Q&A pairs of content
were extracted from different formats such as JSON, PPT, and DOC to create a
vector database to be used in the RAG system. By using the fine-tuned model and
RAG approach together, chatbots with different competencies were obtained. The
questions and answers asked to the generated chatbots were saved separately and
evaluated using ROUGE, BERTScore, METEOR, and BLEU metrics. The precision value
of the Phi-2 model supported by RAG was 0.84 and the F1 score was 0.82. In
addition to a total of 13 different evaluation metrics in 4 different
categories, the answers of each model were compared with the created content
and the most appropriate method was selected for real-life applications.",2024-11-28,"Batuhan Sariturk, Rabia Bayraktar, Merve Elmas Erdem",http://arxiv.org/pdf/2412.00125v1,cs.CL
CovidLLM: A Robust Large Language Model with Missing Value Adaptation and Multi-Objective Learning Strategy for Predicting Disease Severity and Clinical Outcomes in COVID-19 Patients,"Coronavirus Disease 2019 (COVID-19), which emerged in 2019, has caused
millions of deaths worldwide. Although effective vaccines have been developed
to mitigate severe symptoms, certain populations, particularly the elderly and
those with comorbidities, remain at high risk for severe outcomes and increased
mortality. Consequently, early identification of the severity and clinical
outcomes of the disease in these patients is vital to prevent adverse
prognoses. Although traditional machine learning and deep learning models have
been widely employed in this area, the potential of large language models
(LLMs) remains largely unexplored. Our research focuses primarily on
constructing specialized prompts and adopting multi-objective learning
strategies. We started by selecting serological indicators that significantly
correlate with clinical outcomes and disease severity to serve as input data
for the model. Blood test samples often contain numerous missing values, and
traditional models generally rely on imputation to handle these gaps in the
data. In contrast, LLMs offer the advantage of robust semantic understanding.
By setting prompts, we can explicitly inform the model when a feature's value
is missing, without the need for imputation. For the multi-objective learning
strategy, the model is designed to first predict disease severity and then
predict clinical outcomes. Given that LLMs utilize both the input text and the
generated tokens as input for generating the next token, the predicted severity
is used as a basis for generating the clinical outcome. During the fine-tuning
of the LLM, the two objectives influence and improve each other. Our
experiments were implemented based on the ChatGLM model. The results
demonstrate the effectiveness of LLMs in this task, suggesting promising
potential for further development.",2024-11-28,"Shengjun Zhu, Siyu Liu, Yang Li, Qing Lei, Hongyan Hou, Hewei Jiang, Shujuan Guo, Feng Wang, Rongshang Chen, Xionglin Fan, Shengce Tao, Jiaxin Cai",http://arxiv.org/pdf/2412.03593v1,cs.CL
Way to Specialist: Closing Loop Between Specialized LLM and Evolving Domain Knowledge Graph,"Large language models (LLMs) have demonstrated exceptional performance across
a wide variety of domains. Nonetheless, generalist LLMs continue to fall short
in reasoning tasks necessitating specialized knowledge. Prior investigations
into specialized LLMs focused on domain-specific training, which entails
substantial efforts in domain data acquisition and model parameter fine-tuning.
To address these challenges, this paper proposes the Way-to-Specialist (WTS)
framework, which synergizes retrieval-augmented generation with knowledge
graphs (KGs) to enhance the specialized capability of LLMs in the absence of
specialized training. In distinction to existing paradigms that merely utilize
external knowledge from general KGs or static domain KGs to prompt LLM for
enhanced domain-specific reasoning, WTS proposes an innovative
""LLM$\circlearrowright$KG"" paradigm, which achieves bidirectional enhancement
between specialized LLM and domain knowledge graph (DKG). The proposed paradigm
encompasses two closely coupled components: the DKG-Augmented LLM and the
LLM-Assisted DKG Evolution. The former retrieves question-relevant domain
knowledge from DKG and uses it to prompt LLM to enhance the reasoning
capability for domain-specific tasks; the latter leverages LLM to generate new
domain knowledge from processed tasks and use it to evolve DKG. WTS closes the
loop between DKG-Augmented LLM and LLM-Assisted DKG Evolution, enabling
continuous improvement in the domain specialization as it progressively answers
and learns from domain-specific questions. We validate the performance of WTS
on 6 datasets spanning 5 domains. The experimental results show that WTS
surpasses the previous SOTA in 4 specialized domains and achieves a maximum
performance improvement of 11.3%.",2024-11-28,"Yutong Zhang, Lixing Chen, Shenghong Li, Nan Cao, Yang Shi, Jiaxin Ding, Zhe Qu, Pan Zhou, Yang Bai",http://arxiv.org/pdf/2411.19064v1,cs.CL
DIESEL -- Dynamic Inference-Guidance via Evasion of Semantic Embeddings in LLMs,"In recent years, large language models (LLMs) have had great success in tasks
such as casual conversation, contributing to significant advancements in
domains like virtual assistance. However, they often generate responses that
are not aligned with human values (e.g., ethical standards, safety), leading to
potentially unsafe or inappropriate outputs. While several techniques have been
proposed to address this problem, they come with a cost, requiring
computationally expensive training or dramatically increasing the inference
time. In this paper, we present DIESEL, a lightweight inference-guidance
technique that can be seamlessly integrated into any autoregressive LLM to
semantically filter undesired concepts from the response. DIESEL can function
either as a standalone safeguard or as an additional layer of defense,
enhancing response safety by reranking the LLM's proposed tokens based on their
similarity to predefined negative concepts in the latent space. Our evaluation
demonstrates DIESEL's effectiveness on state-of-the-art conversational models,
even in adversarial jailbreaking scenarios that challenge response safety. We
also highlight DIESEL's generalization capabilities, showing that it can be
used in use cases other than safety, providing general-purpose response
filtering.",2024-11-28,"Ben Ganon, Alon Zolfi, Omer Hofman, Inderjeet Singh, Hisashi Kojima, Yuval Elovici, Asaf Shabtai",http://arxiv.org/pdf/2411.19038v2,cs.CL
A Survey on Automatic Online Hate Speech Detection in Low-Resource Languages,"The expanding influence of social media platforms over the past decade has
impacted the way people communicate. The level of obscurity provided by social
media and easy accessibility of the internet has facilitated the spread of hate
speech. The terms and expressions related to hate speech gets updated with
changing times which poses an obstacle to policy-makers and researchers in case
of hate speech identification. With growing number of individuals using their
native languages to communicate with each other, hate speech in these
low-resource languages are also growing. Although, there is awareness about the
English-related approaches, much attention have not been provided to these
low-resource languages due to lack of datasets and online available data. This
article provides a detailed survey of hate speech detection in low-resource
languages around the world with details of available datasets, features
utilized and techniques used. This survey further discusses the prevailing
surveys, overlapping concepts related to hate speech, research challenges and
opportunities.",2024-11-28,"Susmita Das, Arpita Dutta, Kingshuk Roy, Abir Mondal, Arnab Mukhopadhyay",http://arxiv.org/pdf/2411.19017v1,cs.CL
Talking to oneself in CMC: a study of self replies in Wikipedia talk pages,"This study proposes a qualitative analysis of self replies in Wikipedia talk
pages, more precisely when the first two messages of a discussion are written
by the same user. This specific pattern occurs in more than 10% of threads with
two messages or more and can be explained by a number of reasons. After a first
examination of the lexical specificities of second messages, we propose a seven
categories typology and use it to annotate two reference samples (English and
French) of 100 threads each. Finally, we analyse and compare the performance of
human annotators (who reach a reasonable global efficiency) and
instruction-tuned LLMs (which encounter important difficulties with several
categories).",2024-11-28,"Ludovic Tanguy, Céline Poudat, Lydia-Mai Ho-Dac",http://arxiv.org/pdf/2411.19007v1,cs.CL
Using Images to Find Context-Independent Word Representations in Vector Space,"Many methods have been proposed to find vector representation for words, but
most rely on capturing context from the text to find semantic relationships
between these vectors. We propose a novel method of using dictionary meanings
and image depictions to find word vectors independent of any context. We use
auto-encoder on the word images to find meaningful representations and use them
to calculate the word vectors. We finally evaluate our method on word
similarity, concept categorization and outlier detection tasks. Our method
performs comparably to context-based methods while taking much less training
time.",2024-11-28,Harsh Kumar,http://arxiv.org/pdf/2412.03592v1,cs.CL
USTCCTSU at SemEval-2024 Task 1: Reducing Anisotropy for Cross-lingual Semantic Textual Relatedness Task,"Cross-lingual semantic textual relatedness task is an important research task
that addresses challenges in cross-lingual communication and text
understanding. It helps establish semantic connections between different
languages, crucial for downstream tasks like machine translation, multilingual
information retrieval, and cross-lingual text understanding.Based on extensive
comparative experiments, we choose the XLM-R-base as our base model and use
pre-trained sentence representations based on whitening to reduce
anisotropy.Additionally, for the given training data, we design a delicate data
filtering method to alleviate the curse of multilingualism. With our approach,
we achieve a 2nd score in Spanish, a 3rd in Indonesian, and multiple entries in
the top ten results in the competition's track C. We further do a comprehensive
analysis to inspire future research aimed at improving performance on
cross-lingual tasks.",2024-11-28,"Jianjian Li, Shengwei Liang, Yong Liao, Hongping Deng, Haiyang Yu",http://arxiv.org/pdf/2411.18990v1,cs.CL
Zero-shot Slot Filling in the Age of LLMs for Dialogue Systems,"Zero-shot slot filling is a well-established subtask of Natural Language
Understanding (NLU). However, most existing methods primarily focus on
single-turn text data, overlooking the unique complexities of conversational
dialogue. Conversational data is highly dynamic, often involving abrupt topic
shifts, interruptions, and implicit references that make it difficult to
directly apply zero-shot slot filling techniques, even with the remarkable
capabilities of large language models (LLMs). This paper addresses these
challenges by proposing strategies for automatic data annotation with slot
induction and black-box knowledge distillation (KD) from a teacher LLM to a
smaller model, outperforming vanilla LLMs on internal datasets by 26% absolute
increase in F1 score. Additionally, we introduce an efficient system
architecture for call center product settings that surpasses off-the-shelf
extractive models by 34% relative F1 score, enabling near real-time inference
on dialogue streams with higher accuracy, while preserving low latency.",2024-11-28,"Mansi Rana, Kadri Hacioglu, Sindhuja Gopalan, Maragathamani Boothalingam",http://arxiv.org/pdf/2411.18980v1,cs.CL
Rephrasing Electronic Health Records for Pretraining Clinical Language Models,"Clinical language models are important for many applications in healthcare,
but their development depends on access to extensive clinical text for
pretraining. However, obtaining clinical notes from electronic health records
(EHRs) at scale is challenging due to patient privacy concerns. In this study,
we rephrase existing clinical notes using LLMs to generate synthetic
pretraining corpora, drawing inspiration from previous work on rephrasing web
data. We examine four popular small-sized LLMs (<10B) to create synthetic
clinical text to pretrain both decoder-based and encoder-based language models.
The method yields better results in language modeling and downstream tasks than
previous synthesis approaches without referencing real clinical text. We find
that augmenting original clinical notes with synthetic corpora from different
LLMs improves performances even at a small token budget, showing the potential
of this method to support pretraining at the institutional level or be scaled
to synthesize large-scale clinical corpora.",2024-11-28,"Jinghui Liu, Anthony Nguyen",http://arxiv.org/pdf/2411.18940v1,cs.CL
ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges,"Recent advancements in large multimodal models (LMMs) have showcased
impressive code generation capabilities, primarily evaluated through
image-to-code benchmarks. However, these benchmarks are limited to specific
visual programming scenarios where the logic reasoning and the multimodal
understanding capacities are split apart. To fill this gap, we propose
ScratchEval, a novel benchmark designed to evaluate the visual programming
reasoning ability of LMMs. ScratchEval is based on Scratch, a block-based
visual programming language widely used in children's programming education. By
integrating visual elements and embedded programming logic, ScratchEval
requires the model to process both visual information and code structure,
thereby comprehensively evaluating its programming intent understanding
ability. Our evaluation approach goes beyond the traditional image-to-code
mapping and focuses on unified logical thinking and problem-solving abilities,
providing a more comprehensive and challenging framework for evaluating the
visual programming ability of LMMs. ScratchEval not only fills the gap in
existing evaluation methods, but also provides new insights for the future
development of LMMs in the field of visual programming. Our benchmark can be
accessed at https://github.com/HKBUNLP/ScratchEval .",2024-11-28,"Rao Fu, Ziyang Luo, Hongzhan Lin, Zhen Ye, Jing Ma",http://arxiv.org/pdf/2411.18932v1,cs.CL
The Impact of Example Selection in Few-Shot Prompting on Automated Essay Scoring Using GPT Models,"This study investigates the impact of example selection on the performance of
au-tomated essay scoring (AES) using few-shot prompting with GPT models. We
evaluate the effects of the choice and order of examples in few-shot prompting
on several versions of GPT-3.5 and GPT-4 models. Our experiments involve 119
prompts with different examples, and we calculate the quadratic weighted kappa
(QWK) to measure the agreement between GPT and human rater scores. Regres-sion
analysis is used to quantitatively assess biases introduced by example
selec-tion. The results show that the impact of example selection on QWK varies
across models, with GPT-3.5 being more influenced by examples than GPT-4. We
also find evidence of majority label bias, which is a tendency to favor the
majority la-bel among the examples, and recency bias, which is a tendency to
favor the label of the most recent example, in GPT-generated essay scores and
QWK, with these biases being more pronounced in GPT-3.5. Notably, careful
example selection enables GPT-3.5 models to outperform some GPT-4 models.
However, among the GPT models, the June 2023 version of GPT-4, which is not the
latest model, exhibits the highest stability and performance. Our findings
provide insights into the importance of example selection in few-shot prompting
for AES, especially in GPT-3.5 models, and highlight the need for individual
performance evaluations of each model, even for minor versions.",2024-11-28,Lui Yoshida,http://arxiv.org/pdf/2411.18924v1,cs.CL
EzSQL: An SQL intermediate representation for improving SQL-to-text Generation,"The SQL-to-text generation task traditionally uses template base, Seq2Seq,
tree-to-sequence, and graph-to-sequence models. Recent models take advantage of
pre-trained generative language models for this task in the Seq2Seq framework.
However, treating SQL as a sequence of inputs to the pre-trained models is not
optimal. In this work, we put forward a new SQL intermediate representation
called EzSQL to align SQL with the natural language text sequence. EzSQL
simplifies the SQL queries and brings them closer to natural language text by
modifying operators and keywords, which can usually be described in natural
language. EzSQL also removes the need for set operators. Our proposed
SQL-to-text generation model uses EzSQL as the input to a pre-trained
generative language model for generating the text descriptions. We demonstrate
that our model is an effective state-of-the-art method to generate text
narrations from SQL queries on the WikiSQL and Spider datasets. We also show
that by generating pretraining data using our SQL-to-text generation model, we
can enhance the performance of Text-to-SQL parsers.",2024-11-28,"Meher Bhardwaj, Hrishikesh Ethari, Dennis Singh Moirangthem",http://arxiv.org/pdf/2411.18923v2,cs.CL
Devising a Set of Compact and Explainable Spoken Language Feature for Screening Alzheimer's Disease,"Alzheimer's disease (AD) has become one of the most significant health
challenges in an aging society. The use of spoken language-based AD detection
methods has gained prevalence due to their scalability due to their
scalability. Based on the Cookie Theft picture description task, we devised an
explainable and effective feature set that leverages the visual capabilities of
a large language model (LLM) and the Term Frequency-Inverse Document Frequency
(TF-IDF) model. Our experimental results show that the newly proposed features
consistently outperform traditional linguistic features across two different
classifiers with high dimension efficiency. Our new features can be well
explained and interpreted step by step which enhance the interpretability of
automatic AD screening.",2024-11-28,"Junan Li, Yunxiang Li, Yuren Wang, Xixin Wu, Helen Meng",http://arxiv.org/pdf/2411.18922v1,cs.CL
MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications,"Business documents often contain substantial tabular and textual information
with numerical values, requiring mathematical reasoning for effective document
understanding. While Small Language Models (SLMs) still struggle at this task,
tool-augmented multi-step agents perform better, at the cost of relying on
closed-source or larger models, external data, or extensive prompt-engineering.
This work introduces MATATA, a novel weakly supervised end-to-end approach to
train multi-step reasoning language agents for document tabular applications.
MATATA presents an annotation-free paradigm for each agent to enhance 3.8B/8B
SLMs. During its two-stage training, MATATA uses the final outcome of the
multi-step reasoning chain as weak supervision. This approach avoids having to
individually supervise each intermediate agent in the reasoning chain. By
employing an adaptive planner and shared tools across different datasets,
MATATA shows robust performance. Experiments demonstrate that MATATA achieves
state-of-the-art on FinQA, and on TAT-QA among reasoning methods based on
open-source SLMs. Although being SLM-based, MATATA closely matches GPT-4-based
frameworks on TabMWP. This novel weakly supervised approach enables training an
end-to-end multi-step reasoning agent without intermediate supervision,
supporting future developments of cost-effective powerful agentic systems.",2024-11-28,"Vishnou Vinayagame, Gregory Senay, Luis Martí",http://arxiv.org/pdf/2411.18915v4,cs.CL
Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks,"Sparse Autoencoders (SAEs) are an interpretability technique aimed at
decomposing neural network activations into interpretable units. However, a
major bottleneck for SAE development has been the lack of high-quality
performance metrics, with prior work largely relying on unsupervised proxies.
In this work, we introduce a family of evaluations based on SHIFT, a downstream
task from Marks et al. (Sparse Feature Circuits, 2024) in which spurious cues
are removed from a classifier by ablating SAE features judged to be
task-irrelevant by a human annotator. We adapt SHIFT into an automated metric
of SAE quality; this involves replacing the human annotator with an LLM.
Additionally, we introduce the Targeted Probe Perturbation (TPP) metric that
quantifies an SAE's ability to disentangle similar concepts, effectively
scaling SHIFT to a wider range of datasets. We apply both SHIFT and TPP to
multiple open-source models, demonstrating that these metrics effectively
differentiate between various SAE training hyperparameters and architectures.",2024-11-28,"Adam Karvonen, Can Rager, Samuel Marks, Neel Nanda",http://arxiv.org/pdf/2411.18895v1,cs.CL
ArEEG_Words: Dataset for Envisioned Speech Recognition using EEG for Arabic Words,"Brain-Computer-Interface (BCI) aims to support communication-impaired
patients by translating neural signals into speech. A notable research topic in
BCI involves Electroencephalography (EEG) signals that measure the electrical
activity in the brain. While significant advancements have been made in BCI EEG
research, a major limitation still exists: the scarcity of publicly available
EEG datasets for non-English languages, such as Arabic. To address this gap, we
introduce in this paper ArEEG_Words dataset, a novel EEG dataset recorded from
22 participants with mean age of 22 years (5 female, 17 male) using a
14-channel Emotiv Epoc X device. The participants were asked to be free from
any effects on their nervous system, such as coffee, alcohol, cigarettes, and
so 8 hours before recording. They were asked to stay calm in a clam room during
imagining one of the 16 Arabic Words for 10 seconds. The words include 16
commonly used words such as up, down, left, and right. A total of 352 EEG
recordings were collected, then each recording was divided into multiple 250ms
signals, resulting in a total of 15,360 EEG signals. To the best of our
knowledge, ArEEG_Words data is the first of its kind in Arabic EEG domain.
Moreover, it is publicly available for researchers as we hope that will fill
the gap in Arabic EEG research.",2024-11-28,"Hazem Darwish, Abdalrahman Al Malah, Khloud Al Jallad, Nada Ghneim",http://arxiv.org/pdf/2411.18888v1,cs.CL
Sneaking Syntax into Transformer Language Models with Tree Regularization,"While compositional accounts of human language understanding are based on a
hierarchical tree-like process, neural models like transformers lack a direct
inductive bias for such tree structures. Introducing syntactic inductive biases
could unlock more robust and data-efficient learning in transformer language
models (LMs), but existing methods for incorporating such structure greatly
restrict models, either limiting their expressivity or increasing inference
complexity. This work instead aims to softly inject syntactic inductive biases
into given transformer circuits, through a structured regularizer. We introduce
TreeReg, an auxiliary loss function that converts bracketing decisions from
silver parses into a set of differentiable orthogonality constraints on vector
hidden states. TreeReg integrates seamlessly with the standard LM objective,
requiring no architectural changes. LMs pre-trained with TreeReg on natural
language corpora such as WikiText-103 achieve up to 10% lower perplexities on
out-of-distribution data and up to 9.5 point improvements in syntactic
generalization, requiring less than half the training data to outperform
standard LMs. TreeReg still provides gains for pre-trained LLMs: Continued
pre-training of Sheared Llama with TreeReg results in improved syntactic
generalization, and fine-tuning on MultiNLI with TreeReg mitigates degradation
of performance on adversarial NLI benchmarks by 41.2 points. We release all
code to guide future research.",2024-11-28,"Ananjan Nandi, Christopher D. Manning, Shikhar Murty",http://arxiv.org/pdf/2411.18885v2,cs.CL
NERsocial: Efficient Named Entity Recognition Dataset Construction for Human-Robot Interaction Utilizing RapidNER,"Adapting named entity recognition (NER) methods to new domains poses
significant challenges. We introduce RapidNER, a framework designed for the
rapid deployment of NER systems through efficient dataset construction.
RapidNER operates through three key steps: (1) extracting domain-specific
sub-graphs and triples from a general knowledge graph, (2) collecting and
leveraging texts from various sources to build the NERsocial dataset, which
focuses on entities typical in human-robot interaction, and (3) implementing an
annotation scheme using Elasticsearch (ES) to enhance efficiency. NERsocial,
validated by human annotators, includes six entity types, 153K tokens, and
99.4K sentences, demonstrating RapidNER's capability to expedite dataset
creation.",2024-11-28,"Jesse Atuhurra, Hidetaka Kamigaito, Hiroki Ouchi, Hiroyuki Shindo, Taro Watanabe",http://arxiv.org/pdf/2412.09634v1,cs.CL
Measuring Risk of Bias in Biomedical Reports: The RoBBR Benchmark,"Systems that answer questions by reviewing the scientific literature are
becoming increasingly feasible. To draw reliable conclusions, these systems
should take into account the quality of available evidence, placing more weight
on studies that use a valid methodology. We present a benchmark for measuring
the methodological strength of biomedical papers, drawing on the risk-of-bias
framework used for systematic reviews. The four benchmark tasks, drawn from
more than 500 papers, cover the analysis of research study methodology,
followed by evaluation of risk of bias in these studies. The benchmark contains
2000 expert-generated bias annotations, and a human-validated pipeline for
fine-grained alignment with research paper content. We evaluate a range of
large language models on the benchmark, and find that these models fall
significantly short of expert-level performance. By providing a standardized
tool for measuring judgments of study quality, the benchmark can help to guide
systems that perform large-scale aggregation of scientific data. The dataset is
available at https://github.com/RoBBR-Benchmark/RoBBR.",2024-11-28,"Jianyou Wang, Weili Cao, Longtian Bao, Youze Zheng, Gil Pasternak, Kaicheng Wang, Xiaoyue Wang, Ramamohan Paturi, Leon Bergen",http://arxiv.org/pdf/2411.18831v1,cs.CL
NewsEdits 2.0: Learning the Intentions Behind Updating News,"As events progress, news articles often update with new information: if we
are not cautious, we risk propagating outdated facts. In this work, we
hypothesize that linguistic features indicate factual fluidity, and that we can
predict which facts in a news article will update using solely the text of a
news article (i.e. not external resources like search engines). We test this
hypothesis, first, by isolating fact-updates in large news revisions corpora.
News articles may update for many reasons (e.g. factual, stylistic, narrative).
We introduce the NewsEdits 2.0 taxonomy, an edit-intentions schema that
separates fact updates from stylistic and narrative updates in news writing. We
annotate over 9,200 pairs of sentence revisions and train high-scoring ensemble
models to apply this schema. Then, taking a large dataset of silver-labeled
pairs, we show that we can predict when facts will update in older article
drafts with high precision. Finally, to demonstrate the usefulness of these
findings, we construct a language model question asking (LLM-QA) abstention
task. We wish the LLM to abstain from answering questions when information is
likely to become outdated. Using our predictions, we show, LLM absention
reaches near oracle levels of accuracy.",2024-11-27,"Alexander Spangher, Kung-Hsiang Huang, Hyundong Cho, Jonathan May",http://arxiv.org/pdf/2411.18811v1,cs.CL
Reconstructing Animals and the Wild,"The idea of 3D reconstruction as scene understanding is foundational in
computer vision. Reconstructing 3D scenes from 2D visual observations requires
strong priors to disambiguate structure. Much work has been focused on the
anthropocentric, which, characterized by smooth surfaces, coherent normals, and
regular edges, allows for the integration of strong geometric inductive biases.
Here, we consider a more challenging problem where such assumptions do not
hold: the reconstruction of natural scenes containing trees, bushes, boulders,
and animals. While numerous works have attempted to tackle the problem of
reconstructing animals in the wild, they have focused solely on the animal,
neglecting environmental context. This limits their usefulness for analysis
tasks, as animals exist inherently within the 3D world, and information is lost
when environmental factors are disregarded. We propose a method to reconstruct
natural scenes from single images. We base our approach on recent advances
leveraging the strong world priors ingrained in Large Language Models and train
an autoregressive model to decode a CLIP embedding into a structured
compositional scene representation, encompassing both animals and the wild
(RAW). To enable this, we propose a synthetic dataset comprising one million
images and thousands of assets. Our approach, having been trained solely on
synthetic data, generalizes to the task of reconstructing animals and their
environments in real-world images. We will release our dataset and code to
encourage future research at https://raw.is.tue.mpg.de/",2024-11-27,"Peter Kulits, Michael J. Black, Silvia Zuffi",http://arxiv.org/pdf/2411.18807v1,cs.CL
UOE: Unlearning One Expert Is Enough For Mixture-of-experts LLMS,"Recent advancements in large language model (LLM) unlearning have shown
remarkable success in removing unwanted data-model influences while preserving
the model's utility for legitimate knowledge. However, despite these strides,
sparse Mixture-of-Experts (MoE) LLMs--a key subset of the LLM family--have
received little attention and remain largely unexplored in the context of
unlearning. As MoE LLMs are celebrated for their exceptional performance and
highly efficient inference processes, we ask: How can unlearning be performed
effectively and efficiently on MoE LLMs? And will traditional unlearning
methods be applicable to MoE architectures? Our pilot study shows that the
dynamic routing nature of MoE LLMs introduces unique challenges, leading to
substantial utility drops when existing unlearning methods are applied.
Specifically, unlearning disrupts the router's expert selection, causing
significant selection shift from the most unlearning target-related experts to
irrelevant ones. As a result, more experts than necessary are affected, leading
to excessive forgetting and loss of control over which knowledge is erased. To
address this, we propose a novel single-expert unlearning framework, referred
to as UOE, for MoE LLMs. Through expert attribution, unlearning is concentrated
on the most actively engaged expert for the specified knowledge. Concurrently,
an anchor loss is applied to the router to stabilize the active state of this
targeted expert, ensuring focused and controlled unlearning that preserves
model utility. The proposed UOE framework is also compatible with various
unlearning algorithms. Extensive experiments demonstrate that UOE enhances both
forget quality up to 5% and model utility by 35% on MoE LLMs across various
benchmarks, LLM architectures, while only unlearning 0.06% of the model
parameters.",2024-11-27,"Haomin Zhuang, Yihua Zhang, Kehan Guo, Jinghan Jia, Gaowen Liu, Sijia Liu, Xiangliang Zhang",http://arxiv.org/pdf/2411.18797v1,cs.CL
Enhancing Document AI Data Generation Through Graph-Based Synthetic Layouts,"The development of robust Document AI models has been constrained by limited
access to high-quality, labeled datasets, primarily due to data privacy
concerns, scarcity, and the high cost of manual annotation. Traditional methods
of synthetic data generation, such as text and image augmentation, have proven
effective for increasing data diversity but often fail to capture the complex
layout structures present in real world documents. This paper proposes a novel
approach to synthetic document layout generation using Graph Neural Networks
(GNNs). By representing document elements (e.g., text blocks, images, tables)
as nodes in a graph and their spatial relationships as edges, GNNs are trained
to generate realistic and diverse document layouts. This method leverages
graph-based learning to ensure structural coherence and semantic consistency,
addressing the limitations of traditional augmentation techniques. The proposed
framework is evaluated on tasks such as document classification, named entity
recognition (NER), and information extraction, demonstrating significant
performance improvements. Furthermore, we address the computational challenges
of GNN based synthetic data generation and propose solutions to mitigate domain
adaptation issues between synthetic and real-world datasets. Our experimental
results show that graph-augmented document layouts outperform existing
augmentation techniques, offering a scalable and flexible solution for training
Document AI models.",2024-11-27,"Amit Agarwal, Hitesh Patel, Priyaranjan Pattnayak, Srikant Panda, Bhargava Kumar, Tejaswini Kumar",http://arxiv.org/pdf/2412.03590v1,cs.CL
Cyber-Attack Technique Classification Using Two-Stage Trained Large Language Models,"Understanding the attack patterns associated with a cyberattack is crucial
for comprehending the attacker's behaviors and implementing the right
mitigation measures. However, majority of the information regarding new attacks
is typically presented in unstructured text, posing significant challenges for
security analysts in collecting necessary information. In this paper, we
present a sentence classification system that can identify the attack
techniques described in natural language sentences from cyber threat
intelligence (CTI) reports. We propose a new method for utilizing auxiliary
data with the same labels to improve classification for the low-resource
cyberattack classification task. The system first trains the model using the
augmented training data and then trains more using only the primary data. We
validate our model using the TRAM data1 and the MITRE ATT&CK framework.
Experiments show that our method enhances Macro-F1 by 5 to 9 percentage points
and keeps Micro-F1 scores competitive when compared to the baseline performance
on the TRAM dataset.",2024-11-27,"Weiqiu You, Youngja Park",http://arxiv.org/pdf/2411.18755v1,cs.CL
ElectroVizQA: How well do Multi-modal LLMs perform in Electronics Visual Question Answering?,"Multi-modal Large Language Models (MLLMs) are gaining significant attention
for their ability to process multi-modal data, providing enhanced contextual
understanding of complex problems. MLLMs have demonstrated exceptional
capabilities in tasks such as Visual Question Answering (VQA); however, they
often struggle with fundamental engineering problems, and there is a scarcity
of specialized datasets for training on topics like digital electronics. To
address this gap, we propose a benchmark dataset called ElectroVizQA
specifically designed to evaluate MLLMs' performance on digital electronic
circuit problems commonly found in undergraduate curricula. This dataset, the
first of its kind tailored for the VQA task in digital electronics, comprises
approximately 626 visual questions, offering a comprehensive overview of
digital electronics topics. This paper rigorously assesses the extent to which
MLLMs can understand and solve digital electronic circuit questions, providing
insights into their capabilities and limitations within this specialized
domain. By introducing this benchmark dataset, we aim to motivate further
research and development in the application of MLLMs to engineering education,
ultimately bridging the performance gap and enhancing the efficacy of these
models in technical fields.",2024-11-27,"Pragati Shuddhodhan Meshram, Swetha Karthikeyan, Bhavya, Suma Bhat",http://arxiv.org/pdf/2412.00102v1,cs.CL
Multi-Task Model Merging via Adaptive Weight Disentanglement,"Model merging has recently gained attention as an economical and scalable
approach to incorporate task-specific weights from various tasks into a unified
multi-task model. For example, in Task Arithmetic (TA), adding the fine-tuned
weights of different tasks can enhance the model's performance on those tasks,
while subtracting them leads to task forgetting. Although TA is highly
effective, interference among task still hampers the performance of the merged
model. Existing methods for handling conflicts between task generally rely on
empirical selection, resulting in suboptimal performance. In this paper, we
introduce an Adaptive Weight Disentanglement method. We begin by theoretically
proving that task vectors employed in model merging should be orthogonal to
minimize interference among tasks. Guided by this insight, we initialize
redundant vectors such that, when subtracted from the original task vectors,
the resulting vectors exhibit increased orthogonality. Additionally, we impose
an norm constraint on the redundant vectors to preserve the performance of the
task-specific models. Experimental results demonstrate the effectiveness of our
proposed technique: it successfully extracts redundant vectors, and after their
subtraction, the task vectors not only retain robust performance but also
achieve superior fusion outcomes. Our code is available at
\href{https://github.com/FarisXiong/AWD.git}{https://github.com/FarisXiong/AWD.git}.",2024-11-27,"Feng Xiong, Runxi Cheng, Wang Chen, Zhanqiu Zhang, Yiwen Guo, Chun Yuan, Ruifeng Xu",http://arxiv.org/pdf/2411.18729v2,cs.CL
Evaluating Vision-Language Models as Evaluators in Path Planning,"Despite their promise to perform complex reasoning, large language models
(LLMs) have been shown to have limited effectiveness in end-to-end planning.
This has inspired an intriguing question: if these models cannot plan well, can
they still contribute to the planning framework as a helpful plan evaluator? In
this work, we generalize this question to consider LLMs augmented with visual
understanding, i.e., Vision-Language Models (VLMs). We introduce PathEval, a
novel benchmark evaluating VLMs as plan evaluators in complex path-planning
scenarios. Succeeding in the benchmark requires a VLM to be able to abstract
traits of optimal paths from the scenario description, demonstrate precise
low-level perception on each path, and integrate this information to decide the
better path. Our analysis of state-of-the-art VLMs reveals that these models
face significant challenges on the benchmark. We observe that the VLMs can
precisely abstract given scenarios to identify the desired traits and exhibit
mixed performance in integrating the provided information. Yet, their vision
component presents a critical bottleneck, with models struggling to perceive
low-level details about a path. Our experimental results show that this issue
cannot be trivially addressed via end-to-end fine-tuning; rather, task-specific
discriminative adaptation of these vision encoders is needed for these VLMs to
become effective path evaluators.",2024-11-27,"Mohamed Aghzal, Xiang Yue, Erion Plaku, Ziyu Yao",http://arxiv.org/pdf/2411.18711v4,cs.CL
On the Effectiveness of Incremental Training of Large Language Models,"Training large language models is a computationally intensive process that
often requires substantial resources to achieve state-of-the-art results.
Incremental layer-wise training has been proposed as a potential strategy to
optimize the training process by progressively introducing layers, with the
expectation that this approach would lead to faster convergence and more
efficient use of computational resources. In this paper, we investigate the
effectiveness of incremental training for LLMs, dividing the training process
into multiple stages where layers are added progressively. Our experimental
results indicate that while the incremental approach initially demonstrates
some computational efficiency, it ultimately requires greater overall
computational costs to reach comparable performance to traditional full-scale
training. Although the incremental training process can eventually close the
performance gap with the baseline, it does so only after significantly extended
continual training. These findings suggest that incremental layer-wise training
may not be a viable alternative for training large language models,
highlighting its limitations and providing valuable insights into the
inefficiencies of this approach.",2024-11-27,"Miles Q. Li, Benjamin C. M. Fung, Shih-Chia Huang",http://arxiv.org/pdf/2411.18700v1,cs.CL
An indicator for effectiveness of text-to-image guardrails utilizing the Single-Turn Crescendo Attack (STCA),"The Single-Turn Crescendo Attack (STCA), first introduced in Aqrawi and
Abbasi [2024], is an innovative method designed to bypass the ethical
safeguards of text-to-text AI models, compelling them to generate harmful
content. This technique leverages a strategic escalation of context within a
single prompt, combined with trust-building mechanisms, to subtly deceive the
model into producing unintended outputs. Extending the application of STCA to
text-to-image models, we demonstrate its efficacy by compromising the
guardrails of a widely-used model, DALL-E 3, achieving outputs comparable to
outputs from the uncensored model Flux Schnell, which served as a baseline
control. This study provides a framework for researchers to rigorously evaluate
the robustness of guardrails in text-to-image models and benchmark their
resilience against adversarial attacks.",2024-11-27,"Ted Kwartler, Nataliia Bagan, Ivan Banny, Alan Aqrawi, Arian Abbasi",http://arxiv.org/pdf/2411.18699v1,cs.CL
Cross-modal Information Flow in Multimodal Large Language Models,"The recent advancements in auto-regressive multimodal large language models
(MLLMs) have demonstrated promising progress for vision-language tasks. While
there exists a variety of studies investigating the processing of linguistic
information within large language models, little is currently known about the
inner working mechanism of MLLMs and how linguistic and visual information
interact within these models. In this study, we aim to fill this gap by
examining the information flow between different modalities -- language and
vision -- in MLLMs, focusing on visual question answering. Specifically, given
an image-question pair as input, we investigate where in the model and how the
visual and linguistic information are combined to generate the final
prediction. Conducting experiments with a series of models from the LLaVA
series, we find that there are two distinct stages in the process of
integration of the two modalities. In the lower layers, the model first
transfers the more general visual features of the whole image into the
representations of (linguistic) question tokens. In the middle layers, it once
again transfers visual information about specific objects relevant to the
question to the respective token positions of the question. Finally, in the
higher layers, the resulting multimodal representation is propagated to the
last position of the input sequence for the final prediction. Overall, our
findings provide a new and comprehensive perspective on the spatial and
functional aspects of image and language processing in the MLLMs, thereby
facilitating future research into multimodal information localization and
editing. Our code and collected dataset are released here:
https://github.com/FightingFighting/cross-modal-information-flow-in-MLLM.git.",2024-11-27,"Zhi Zhang, Srishti Yadav, Fengze Han, Ekaterina Shutova",http://arxiv.org/pdf/2411.18620v2,cs.CL
Fine-Tuning Large Language Models for Scientific Text Classification: A Comparative Study,"The exponential growth of online textual content across diverse domains has
necessitated advanced methods for automated text classification. Large Language
Models (LLMs) based on transformer architectures have shown significant success
in this area, particularly in natural language processing (NLP) tasks. However,
general-purpose LLMs often struggle with domain-specific content, such as
scientific texts, due to unique challenges like specialized vocabulary and
imbalanced data. In this study, we fine-tune four state-of-the-art LLMs BERT,
SciBERT, BioBERT, and BlueBERT on three datasets derived from the WoS-46985
dataset to evaluate their performance in scientific text classification. Our
experiments reveal that domain-specific models, particularly SciBERT,
consistently outperform general-purpose models in both abstract-based and
keyword-based classification tasks. Additionally, we compare our achieved
results with those reported in the literature for deep learning models, further
highlighting the advantages of LLMs, especially when utilized in specific
domains. The findings emphasize the importance of domain-specific adaptations
for LLMs to enhance their effectiveness in specialized text classification
tasks.",2024-11-27,"Zhyar Rzgar K Rostam, Gábor Kertész",http://arxiv.org/pdf/2412.00098v1,cs.CL
Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation,"This research presents and compares multiple approaches to automate the
generation of literature reviews using several Natural Language Processing
(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language
Model (LLM). The ever-increasing number of research articles provides a huge
challenge for manual literature review. It has resulted in an increased demand
for automation. Developing a system capable of automatically generating the
literature reviews from only the PDF files as input is the primary objective of
this research work. The effectiveness of several Natural Language Processing
(NLP) strategies, such as the frequency-based method (spaCy), the transformer
model (Simple T5), and retrieval-augmented generation (RAG) with Large Language
Model (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR
dataset is chosen for this research experiment and three distinct techniques
are utilized to implement three different systems for auto-generating the
literature reviews. The ROUGE scores are used for the evaluation of all three
systems. Based on the evaluation, the Large Language Model GPT-3.5-turbo
achieved the highest ROUGE-1 score, 0.364. The transformer model comes in
second place and spaCy is at the last position. Finally, a graphical user
interface is created for the best system based on the large language model.",2024-11-27,"Nurshat Fateh Ali, Md. Mahdi Mohtasim, Shakil Mosharrof, T. Gopi Krishna",http://arxiv.org/pdf/2411.18583v1,cs.CL
On Importance of Code-Mixed Embeddings for Hate Speech Identification,"Code-mixing is the practice of using two or more languages in a single
sentence, which often occurs in multilingual communities such as India where
people commonly speak multiple languages. Classic NLP tools, trained on
monolingual data, face challenges when dealing with code-mixed data. Extracting
meaningful information from sentences containing multiple languages becomes
difficult, particularly in tasks like hate speech detection, due to linguistic
variation, cultural nuances, and data sparsity. To address this, we aim to
analyze the significance of code-mixed embeddings and evaluate the performance
of BERT and HingBERT models (trained on a Hindi-English corpus) in hate speech
detection. Our study demonstrates that HingBERT models, benefiting from
training on the extensive Hindi-English dataset L3Cube-HingCorpus, outperform
BERT models when tested on hate speech text datasets. We also found that
code-mixed Hing-FastText performs better than standard English FastText and
vanilla BERT models.",2024-11-27,"Shruti Jagdale, Omkar Khade, Gauri Takalikar, Mihir Inamdar, Raviraj Joshi",http://arxiv.org/pdf/2411.18577v1,cs.CL
Challenges in Adapting Multilingual LLMs to Low-Resource Languages using LoRA PEFT Tuning,"Large Language Models (LLMs) have demonstrated remarkable multilingual
capabilities, yet challenges persist in adapting these models for low-resource
languages. In this study, we investigate the effects of Low-Rank Adaptation
(LoRA) Parameter-Efficient Fine-Tuning (PEFT) on multilingual Gemma models for
Marathi, a language with limited resources. Using a translated Alpaca dataset
with 52,000 instruction-response pairs, our findings reveal that while
evaluation metrics often show a performance decline post-fine-tuning, manual
assessments frequently suggest that the fine-tuned models outperform their
original counterparts. The observations indicate improvements in target
language generation capabilities but a reduction in reasoning abilities
following language adaptation. These results underscore the need for improved
evaluation methodologies and the creation of high-quality native datasets to
accurately assess language-specific model performance in low-resource settings.",2024-11-27,"Omkar Khade, Shruti Jagdale, Abhishek Phaltankar, Gauri Takalikar, Raviraj Joshi",http://arxiv.org/pdf/2411.18571v1,cs.CL
Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs,"Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they often struggle with spatial reasoning. This paper
presents a novel neural-symbolic framework that enhances LLMs' spatial
reasoning abilities through iterative feedback between LLMs and Answer Set
Programming (ASP). We evaluate our approach on two benchmark datasets: StepGame
and SparQA, implementing three distinct strategies: (1) direct prompting
baseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with
iterative refinement. Our experimental results demonstrate that the LLM+ASP
pipeline significantly outperforms baseline methods, achieving an average 82%
accuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and
8-15% respectively over direct prompting. The success stems from three key
innovations: (1) effective separation of semantic parsing and logical reasoning
through a modular pipeline, (2) iterative feedback mechanism between LLMs and
ASP solvers that improves program rate, and (3) robust error handling that
addresses parsing, grounding, and solving failures. Additionally, we propose
Facts+Rules as a lightweight alternative that achieves comparable performance
on complex SparQA dataset, while reducing computational overhead.Our analysis
across different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini)
demonstrates the framework's generalizability and provides insights into the
trade-offs between implementation complexity and reasoning capability,
contributing to the development of more interpretable and reliable AI systems.",2024-11-27,"Rong Wang, Kun Sun, Jonas Kuhn",http://arxiv.org/pdf/2411.18564v2,cs.CL
Retrofitting Large Language Models with Dynamic Tokenization,"Current language models (LMs) use a fixed, static subword tokenizer. This
default choice typically results in degraded efficiency and language
capabilities, especially in languages other than English. To address this
issue, we challenge the static design and propose retrofitting LMs with dynamic
tokenization: a way to dynamically decide on token boundaries based on the
input text via a subword-merging algorithm inspired by byte-pair encoding. We
merge frequent subword sequences in a batch, then apply a pre-trained
embedding-prediction hypernetwork to compute the token embeddings on-the-fly.
For encoder-style models (e.g., XLM-R), this on average reduces token sequence
lengths by >20% across 14 languages while degrading performance by less than
2%. The same method applied to pre-filling and scoring in decoder-style models
(e.g., Mistral-7B; evaluated on English) results in minimal performance
degradation at up to 6% reduction in sequence length. Overall, we find that
dynamic tokenization can mitigate the limitations of static tokenization by
substantially improving inference speed and promoting fairness across
languages, enabling more equitable and adaptable LMs.",2024-11-27,"Darius Feher, Ivan Vulić, Benjamin Minixhofer",http://arxiv.org/pdf/2411.18553v2,cs.CL
Emergence of Self-Identity in AI: A Mathematical Framework and Empirical Study with Generative Large Language Models,"This paper introduces a mathematical framework for defining and quantifying
self-identity in artificial intelligence (AI) systems, addressing a critical
gap in the theoretical foundations of artificial consciousness. While existing
approaches to artificial self-awareness often rely on heuristic implementations
or philosophical abstractions, we present a formal framework grounded in metric
space theory, measure theory, and functional analysis. Our framework posits
that self-identity emerges from two mathematically quantifiable conditions: the
existence of a connected continuum of memories $C \subseteq \mathcal{M}$ in a
metric space $(\mathcal{M}, d_{\mathcal{M}})$, and a continuous mapping $I:
\mathcal{M} \to \mathcal{S}$ that maintains consistent self-recognition across
this continuum, where $(\mathcal{S}, d_{\mathcal{S}})$ represents the metric
space of possible self-identities. To validate this theoretical framework, we
conducted empirical experiments using the Llama 3.2 1B model, employing
Low-Rank Adaptation (LoRA) for efficient fine-tuning. The model was trained on
a synthetic dataset containing temporally structured memories, designed to
capture the complexity of coherent self-identity formation. Our evaluation
metrics included quantitative measures of self-awareness, response consistency,
and linguistic precision. The experimental results demonstrate substantial
improvements in measurable self-awareness metrics, with the primary
self-awareness score increasing from 0.276 to 0.801. This enables the
structured creation of AI systems with validated self-identity features. The
implications of our study are immediately relevant to the fields of humanoid
robotics and autonomous systems.",2024-11-27,Minhyeok Lee,http://arxiv.org/pdf/2411.18530v1,cs.CL
Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS,"In-context Learning (ICL) enables large language models (LLMs) to tackle
downstream tasks through sophisticated prompting and high-quality
demonstrations. However, this traditional ICL paradigm shows limitations when
facing complex mathematical reasoning tasks, primarily due to its heavy
dependence on example quality and the necessity for human intervention in
challenging scenarios. To address these limitations, this paper presents
HiAR-ICL, a \textbf{Hi}gh-level \textbf{A}utomated \textbf{R}easoning paradigm
in \textbf{ICL} that shifts focus from specific examples to abstract thinking
patterns, extending the conventional concept of context in ICL. HiAR-ICL
introduces five atomic reasoning actions as fundamental components for
constructing chain-structured patterns. Using Monte Carlo Tree Search, we
explore reasoning paths and construct thought cards to guide subsequent
inference. We then develop a cognitive complexity framework that dynamically
matches problems with appropriate thought cards. Experimental results
demonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy
(79.6$\%$) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o
(76.6$\%$) and Claude 3.5 (71.1$\%$).",2024-11-27,"Jinyang Wu, Mingkuan Feng, Shuai Zhang, Feihu Che, Zengqi Wen, Jianhua Tao",http://arxiv.org/pdf/2411.18478v1,cs.CL
Isolating authorship from content with semantic embeddings and contrastive learning,"Authorship has entangled style and content inside. Authors frequently write
about the same topics in the same style, so when different authors write about
the exact same topic the easiest way out to distinguish them is by
understanding the nuances of their style. Modern neural models for authorship
can pick up these features using contrastive learning, however, some amount of
content leakage is always present. Our aim is to reduce the inevitable impact
and correlation between content and authorship. We present a technique to use
contrastive learning (InfoNCE) with additional hard negatives synthetically
created using a semantic similarity model. This disentanglement technique aims
to distance the content embedding space from the style embedding space, leading
to embeddings more informed by style. We demonstrate the performance with
ablations on two different datasets and compare them on out-of-domain
challenges. Improvements are clearly shown on challenging evaluations on
prolific authors with up to a 10% increase in accuracy when the settings are
particularly hard. Trials on challenges also demonstrate the preservation of
zero-shot capabilities of this method as fine tuning.",2024-11-27,"Javier Huertas-Tato, Adrián Girón-Jiménez, Alejandro Martín, David Camacho",http://arxiv.org/pdf/2411.18472v1,cs.CL
Parole de présidents (1958-2022),"En plus de soixante ans, huit pr\'esidents se sont succ\'ed\'e \`a la t\^ete
de la Ve R\'epublique fran\c{c}aise (de Gaulle, Pompidou, Giscard d'Estaing,
Mitterrand, Chirac, Sarkozy, Hollande, Macron). Apr\`es avoir pr\'esent\'e le
corpus de leurs discours -- soit 9202 textes et plus de 20 millions de mots
\'etiquet\'es -- le style de chacun des pr\'esidents sera caract\'eris\'e \`a
l'aide de leurs vocabulaire (vocables et cat\'egories grammaticales). Une
analyse plus approfondie r\'ev\`ele les s\'equences typiques de chaque
locataire de l'\'Elys\'ee. Bas\'ee sur les distances entre l'ensemble des
allocutions, une figure illustre les similitudes et diff\'erences entre les
diff\'erents pr\'esidents.
  Over the past sixty-six years, eight presidents successively headed the Fifth
French Republic (de Gaulle, Pompidou, Giscard d'Estaing, Mitterrand, Chirac,
Sarkozy, Holland, Macron). After presenting the corpus of their speeches --
9,202 texts and more than 20 million labelled words -- the style of each of
them will be characterized by their vocabulary (lemmas and part-of-speech). A
deeper analysis reveals the typical sequences of each tenant of the Elys\'ee.
Based on an intertextual distance between all presidential speeches, a
synthesis can be drawn reflecting the similarities and differences between
presidents.",2024-11-27,"Dominique Labbé, Jacques Savoy",http://arxiv.org/pdf/2411.18468v1,cs.CL
Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding,"Speculative Decoding (SD) has become an important technique in accelerating
the inference speed of large language models. Conventional SD methods employ a
fixed draft length, which ignores the token generation difficulty across tasks.
Consequently, in this paper, we address such an issue and introduce SVIP - a
difficulty-aware dynamic draft length policy for speculative decoding systems.
Based on a theoretical lower bound of draft token acceptance rate and its
inference-time approximation, SVIP adaptively determines the lengths of draft
sequences based on the entropy of each draft token distribution. Experimental
results on mainstream SD benchmarks and frameworks demonstrate the superior
performance of SVIP, achieving up to 20\% walltime speedup on SpecBench over
baseline SD methods and 60\% speedup on MT-Bench for long-form generation of up
to 8K tokens. Moreover, SVIP is totally training-free and compatible with any
existing SD methods that generate draft tokens autoregressively. Experimental
results also show that SVIP yields consistent walltime improvement on top of
GliDe & CaPE and EAGLE-2.",2024-11-27,"Ziyin Zhang, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Rui Wang, Zhaopeng Tu",http://arxiv.org/pdf/2411.18462v1,cs.CL
Is my Meeting Summary Good? Estimating Quality with a Multi-LLM Evaluator,"The quality of meeting summaries generated by natural language generation
(NLG) systems is hard to measure automatically. Established metrics such as
ROUGE and BERTScore have a relatively low correlation with human judgments and
fail to capture nuanced errors. Recent studies suggest using large language
models (LLMs), which have the benefit of better context understanding and
adaption of error definitions without training on a large number of human
preference judgments. However, current LLM-based evaluators risk masking errors
and can only serve as a weak proxy, leaving human evaluation the gold standard
despite being costly and hard to compare across studies. In this work, we
present MESA, an LLM-based framework employing a three-step assessment of
individual error types, multi-agent discussion for decision refinement, and
feedback-based self-training to refine error definition understanding and
alignment with human judgment. We show that MESA's components enable thorough
error detection, consistent rating, and adaptability to custom error
guidelines. Using GPT-4o as its backbone, MESA achieves mid to high
Point-Biserial correlation with human judgment in error detection and mid
Spearman and Kendall correlation in reflecting error impact on summary quality,
on average 0.25 higher than previous methods. The framework's flexibility in
adapting to custom error guidelines makes it suitable for various tasks with
limited human-labeled data.",2024-11-27,"Frederic Kirstein, Terry Ruas, Bela Gipp",http://arxiv.org/pdf/2411.18444v1,cs.CL
Politicians vs ChatGPT. A study of presuppositions in French and Italian political communication,"This paper aims to provide a comparison between texts produced by French and
Italian politicians on polarizing issues, such as immigration and the European
Union, and their chatbot counterparts created with ChatGPT 3.5. In this study,
we focus on implicit communication, in particular on presuppositions and their
functions in discourse, which have been considered in the literature as a
potential linguistic feature of manipulation. This study also aims to
contribute to the emerging literature on the pragmatic competences of Large
Language Models.",2024-11-27,"Davide Garassino, Vivana Masia, Nicola Brocca, Alice Delorme Benites",http://arxiv.org/pdf/2411.18403v1,cs.CL
Topic Modeling and Sentiment Analysis on Japanese Online Media's Coverage of Nuclear Energy,"Thirteen years after the Fukushima Daiichi nuclear power plant accident,
Japan's nuclear energy accounts for only approximately 6% of electricity
production, as most nuclear plants remain shut down. To revitalize the nuclear
industry and achieve sustainable development goals, effective communication
with Japanese citizens, grounded in an accurate understanding of public
sentiment, is of paramount importance. While nationwide surveys have
traditionally been used to gauge public views, the rise of social media in
recent years has provided a promising new avenue for understanding public
sentiment. To explore domestic sentiment on nuclear energy-related issues
expressed online, we analyzed the content and comments of over 3,000 YouTube
videos covering topics related to nuclear energy. Topic modeling was used to
extract the main topics from the videos, and sentiment analysis with large
language models classified user sentiments towards each topic. Additionally,
word co-occurrence network analysis was performed to examine the shift in
online discussions during August and September 2023 regarding the release of
treated water. Overall, our results provide valuable insights into the online
discourse on nuclear energy and contribute to a more comprehensive
understanding of public sentiment in Japan.",2024-11-27,"Yifan Sun, Hirofumi Tsuruta, Masaya Kumagai, Ken Kurosaki",http://arxiv.org/pdf/2411.18383v1,cs.CL
ChatGPT as speechwriter for the French presidents,"Generative AI proposes several large language models (LLMs) to automatically
generate a message in response to users' requests. Such scientific
breakthroughs promote new writing assistants but with some fears. The main
focus of this study is to analyze the written style of one LLM called ChatGPT
by comparing its generated messages with those of the recent French presidents.
To achieve this, we compare end-of-the-year addresses written by Chirac,
Sarkozy, Hollande, and Macron with those automatically produced by ChatGPT. We
found that ChatGPT tends to overuse nouns, possessive determiners, and numbers.
On the other hand, the generated speeches employ less verbs, pronouns, and
adverbs and include, in mean, too standardized sentences. Considering some
words, one can observe that ChatGPT tends to overuse ""to must"" (devoir), ""to
continue"" or the lemma ""we"" (nous). Moreover, GPT underuses the auxiliary verb
""to be"" (^etre), or the modal verbs ""to will"" (vouloir) or ""to have to""
(falloir). In addition, when a short text is provided as example to ChatGPT,
the machine can generate a short message with a style closed to the original
wording. Finally, we reveal that ChatGPT style exposes distinct features
compared to real presidential speeches.",2024-11-27,"Dominique Labbé, Cyril Labbé, Jacques Savoy",http://arxiv.org/pdf/2411.18382v1,cs.CL
AMPS: ASR with Multimodal Paraphrase Supervision,"Spontaneous or conversational multilingual speech presents many challenges
for state-of-the-art automatic speech recognition (ASR) systems. In this work,
we present a new technique AMPS that augments a multilingual multimodal ASR
system with paraphrase-based supervision for improved conversational ASR in
multiple languages, including Hindi, Marathi, Malayalam, Kannada, and Nyanja.
We use paraphrases of the reference transcriptions as additional supervision
while training the multimodal ASR model and selectively invoke this paraphrase
objective for utterances with poor ASR performance. Using AMPS with a
state-of-the-art multimodal model SeamlessM4T, we obtain significant relative
reductions in word error rates (WERs) of up to 5%. We present detailed analyses
of our system using both objective and human evaluation metrics.",2024-11-27,"Abhishek Gupta, Amruta Parulekar, Sameep Chattopadhyay, Preethi Jyothi",http://arxiv.org/pdf/2411.18368v2,cs.CL
GPT as ghostwriter at the White House,"Recently several large language models (LLMs) have demonstrated their
capability to generate a message in response to a user request. Such scientific
breakthroughs promote new perspectives but also some fears. The main focus of
this study is to analyze the written style of one LLM called ChatGPT 3.5 by
comparing its generated messages with those of the recent US presidents. To
achieve this objective, we compare the State of the Union addresses written by
Reagan to Obama with those automatically produced by ChatGPT. We found that
ChatGPT tends to overuse the lemma ""we"" as well as nouns and commas. On the
other hand, the generated speeches employ less verbs and include, in mean,
longer sentences. Even when imposing a given style to ChatGPT, the resulting
speech remains distinct from messages written by the target author. Moreover,
ChatGPT opts for a neutral tone with mainly positive emotional expressions and
symbolic terms (e.g., freedom, nation). Finally, we show that the GPT's style
exposes distinct features compared to real presidential addresses.",2024-11-27,Jacques Savoy,http://arxiv.org/pdf/2411.18365v1,cs.CL
Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation,"Ambiguous words are often found in modern digital communications. Lexical
ambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due
to limited data. Consequently, the efficiency of translation, information
retrieval, and question-answering systems is hindered by these limitations.
This study investigates the use of Large Language Models (LLMs) to improve WSD
using a novel approach combining a systematic prompt augmentation mechanism
with a knowledge base (KB) consisting of different sense interpretations. The
proposed method incorporates a human-in-loop approach for prompt augmentation
where prompt is supported by Part-of-Speech (POS) tagging, synonyms of
ambiguous words, aspect-based sense filtering and few-shot prompting to guide
the LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based
approach, this work demonstrates a substantial improvement in performance. The
evaluation was conducted using FEWS test data and sense tags. This research
advances accurate word interpretation in social media and digital
communication.",2024-11-27,"T. G. D. K. Sumanathilaka, Nicholas Micallef, Julian Hough",http://arxiv.org/pdf/2411.18337v3,cs.CL
Continual Learning in Machine Speech Chain Using Gradient Episodic Memory,"Continual learning for automatic speech recognition (ASR) systems poses a
challenge, especially with the need to avoid catastrophic forgetting while
maintaining performance on previously learned tasks. This paper introduces a
novel approach leveraging the machine speech chain framework to enable
continual learning in ASR using gradient episodic memory (GEM). By
incorporating a text-to-speech (TTS) component within the machine speech chain,
we support the replay mechanism essential for GEM, allowing the ASR model to
learn new tasks sequentially without significant performance degradation on
earlier tasks. Our experiments, conducted on the LJ Speech dataset, demonstrate
that our method outperforms traditional fine-tuning and multitask learning
approaches, achieving a substantial error rate reduction while maintaining high
performance across varying noise conditions. We showed the potential of our
semi-supervised machine speech chain approach for effective and efficient
continual learning in speech recognition.",2024-11-27,"Geoffrey Tyndall, Kurniawati Azizah, Dipta Tanaya, Ayu Purwarianti, Dessi Puji Lestari, Sakriani Sakti",http://arxiv.org/pdf/2411.18320v1,cs.CL
Energy-Efficient Split Learning for Fine-Tuning Large Language Models in Edge Networks,"In this letter, we propose an energy-efficient split learning (SL) framework
for fine-tuning large language models (LLMs) using geo-distributed personal
data at the network edge, where LLMs are split and alternately across massive
mobile devices and an edge server. Considering the device heterogeneity and
channel dynamics in edge networks, a \underline{C}ut l\underline{A}yer and
computing \underline{R}esource \underline{D}ecision (CARD) algorithm is
developed to minimize training delay and energy consumption. Simulation results
demonstrate that the proposed approach reduces the average training delay and
server's energy consumption by 70.8% and 53.1%, compared to the benchmarks,
respectively.",2024-11-27,"Zuguang Li, Shaohua Wu, Liang Li, Songge Zhang",http://arxiv.org/pdf/2412.00090v2,cs.CL
Aligning Pre-trained Models for Spoken Language Translation,"This paper investigates a novel approach to end-to-end speech translation
(ST) based on aligning frozen pre-trained automatic speech recognition (ASR)
and machine translation (MT) models via a small connector module (Q-Former, our
Subsampler-Transformer Encoder). This connector bridges the gap between the
speech and text modalities, transforming ASR encoder embeddings into the latent
representation space of the MT encoder while being the only part of the system
optimized during training. Experiments are conducted on the How2
English-Portuguese dataset as we investigate the alignment approach in a
small-scale scenario focusing on ST. While keeping the size of the connector
module constant and small in comparison ( < 5% of the size of the larger
aligned models), increasing the size and capability of the foundation ASR and
MT models universally improves translation results. We also find that the
connectors can serve as domain adapters for the foundation MT models,
significantly improving translation performance in the aligned ST setting. We
conclude that this approach represents a viable and scalable approach to
training end-to-end ST systems.",2024-11-27,"Šimon Sedláček, Santosh Kesiraju, Alexander Polok, Jan Černocký",http://arxiv.org/pdf/2411.18294v1,cs.CL
Neutralizing Backdoors through Information Conflicts for Large Language Models,"Large language models (LLMs) have seen significant advancements, achieving
superior performance in various Natural Language Processing (NLP) tasks, from
understanding to reasoning. However, they remain vulnerable to backdoor
attacks, where models behave normally for standard queries but generate harmful
responses or unintended output when specific triggers are activated. Existing
backdoor defenses often suffer from drawbacks that they either focus on
detection without removal, rely on rigid assumptions about trigger properties,
or prove to be ineffective against advanced attacks like multi-trigger
backdoors. In this paper, we present a novel method to eliminate backdoor
behaviors from LLMs through the construction of information conflicts using
both internal and external mechanisms. Internally, we leverage a lightweight
dataset to train a conflict model, which is then merged with the backdoored
model to neutralize malicious behaviors by embedding contradictory information
within the model's parametric memory. Externally, we incorporate convincing
contradictory evidence into the prompt to challenge the model's internal
backdoor knowledge. Experimental results on classification and conversational
tasks across 4 widely used LLMs demonstrate that our method outperforms 8
state-of-the-art backdoor defense baselines. We can reduce the attack success
rate of advanced backdoor attacks by up to 98% while maintaining over 90% clean
data accuracy. Furthermore, our method has proven to be robust against adaptive
backdoor attacks. The code will be open-sourced upon publication.",2024-11-27,"Chen Chen, Yuchen Sun, Xueluan Gong, Jiaxin Gao, Kwok-Yan Lam",http://arxiv.org/pdf/2411.18280v1,cs.CL
Large Language Model-Brained GUI Agents: A Survey,"GUIs have long been central to human-computer interaction, providing an
intuitive and visually-driven way to access and interact with digital systems.
The advent of LLMs, particularly multimodal models, has ushered in a new era of
GUI automation. They have demonstrated exceptional capabilities in natural
language understanding, code generation, and visual processing. This has paved
the way for a new generation of LLM-brained GUI agents capable of interpreting
complex GUI elements and autonomously executing actions based on natural
language instructions. These agents represent a paradigm shift, enabling users
to perform intricate, multi-step tasks through simple conversational commands.
Their applications span across web navigation, mobile app interactions, and
desktop automation, offering a transformative user experience that
revolutionizes how individuals interact with software. This emerging field is
rapidly advancing, with significant progress in both research and industry.
  To provide a structured understanding of this trend, this paper presents a
comprehensive survey of LLM-brained GUI agents, exploring their historical
evolution, core components, and advanced techniques. We address research
questions such as existing GUI agent frameworks, the collection and utilization
of data for training specialized GUI agents, the development of large action
models tailored for GUI tasks, and the evaluation metrics and benchmarks
necessary to assess their effectiveness. Additionally, we examine emerging
applications powered by these agents. Through a detailed analysis, this survey
identifies key research gaps and outlines a roadmap for future advancements in
the field. By consolidating foundational knowledge and state-of-the-art
developments, this work aims to guide both researchers and practitioners in
overcoming challenges and unlocking the full potential of LLM-brained GUI
agents.",2024-11-27,"Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang",http://arxiv.org/pdf/2411.18279v12,cs.CL
Hidden Data Privacy Breaches in Federated Learning,"Federated Learning (FL) emerged as a paradigm for conducting machine learning
across broad and decentralized datasets, promising enhanced privacy by
obviating the need for direct data sharing. However, recent studies show that
attackers can steal private data through model manipulation or gradient
analysis. Existing attacks are constrained by low theft quantity or
low-resolution data, and they are often detected through anomaly monitoring in
gradients or weights. In this paper, we propose a novel data-reconstruction
attack leveraging malicious code injection, supported by two key techniques,
i.e., distinctive and sparse encoding design and block partitioning. Unlike
conventional methods that require detectable changes to the model, our method
stealthily embeds a hidden model using parameter sharing to systematically
extract sensitive data. The Fibonacci-based index design ensures efficient,
structured retrieval of memorized data, while the block partitioning method
enhances our method's capability to handle high-resolution images by dividing
them into smaller, manageable units. Extensive experiments on 4 datasets
confirmed that our method is superior to the five state-of-the-art
data-reconstruction attacks under the five respective detection methods. Our
method can handle large-scale and high-resolution data without being detected
or mitigated by state-of-the-art data reconstruction defense methods. In
contrast to baselines, our method can be directly applied to both FedAVG and
FedSGD scenarios, underscoring the need for developers to devise new defenses
against such vulnerabilities. We will open-source our code upon acceptance.",2024-11-27,"Xueluan Gong, Yuji Wang, Shuaike Li, Mengyuan Sun, Songze Li, Qian Wang, Kwok-Yan Lam, Chen Chen",http://arxiv.org/pdf/2411.18269v1,cs.CL
MetaphorShare: A Dynamic Collaborative Repository of Open Metaphor Datasets,"The metaphor studies community has developed numerous valuable labelled
corpora in various languages over the years. Many of these resources are not
only unknown to the NLP community, but are also often not easily shared among
the researchers. Both in human sciences and in NLP, researchers could benefit
from a centralised database of labelled resources, easily accessible and
unified under an identical format. To facilitate this, we present
MetaphorShare, a website to integrate metaphor datasets making them open and
accessible. With this effort, our aim is to encourage researchers to share and
upload more datasets in any language in order to facilitate metaphor studies
and the development of future metaphor processing NLP systems. The website has
four main functionalities: upload, download, search and label metaphor
datasets. It is accessible at www.metaphorshare.com.",2024-11-27,"Joanne Boisson, Arif Mehmood, Jose Camacho-Collados",http://arxiv.org/pdf/2411.18260v3,cs.CL
A gentle push funziona benissimo: making instructed models in Italian via contrastive activation steering,"Adapting models to a language that was only partially present in the
pre-training data requires fine-tuning, which is expensive in terms of both
data and computational resources. As an alternative to fine-tuning, we explore
the potential of activation steering-based techniques to enhance model
performance on Italian tasks. Through our experiments we show that Italian
steering (i) can be successfully applied to different models, (ii) achieves
performances comparable to, or even better than, fine-tuned models for Italian,
and (iii) yields higher quality and consistency in Italian generations. We also
discuss the utility of steering and fine-tuning in the contemporary LLM
landscape where models are anyway getting high Italian performances even if not
explicitly trained in this language.",2024-11-27,"Daniel Scalena, Elisabetta Fersini, Malvina Nissim",http://arxiv.org/pdf/2411.18247v1,cs.CL
Thai Financial Domain Adaptation of THaLLE -- Technical Report,"Large Language Models (LLMs) excel in general tasks but struggle with
domain-specific challenges, such as specialized terminology and localized
regulations. Existing financial LLMs, like FinGPT and BloombergGPT, lack
support for the Thai financial domain. We developed a Thai Financial LLM using
the Investment Consultant (IC) exam dataset from the Stock Exchange of
Thailand. To address dataset limitations, we applied data augmentation, ReLoRA
for efficient training, Continued Pretraining (CPT) for domain knowledge, and
Rank-Stabilized LoRA (rsLoRA) for fine-tuning. Supervised Fine-Tuning (SFT)
simulated exam scenarios, while Direct Preference Optimization (DPO) refined
the model using feedback. The model achieved scores of 72%, 72%, and 84% on IC
exam levels P1, P2, and P3, respectively, demonstrating its effectiveness in
Thai financial advisory tasks and its potential for specialized applications.",2024-11-27,"KBTG Labs, Atthakorn Petchsod, Pornchanan Balee, Danupat Khamnuansin, Anuruth Lertpiya, Chanatip Saetia, Tawunrat Chalothorn, Thadpong Pongthawornkamol, Monchai Lertsutthiwong",http://arxiv.org/pdf/2411.18242v1,cs.CL
How to Learn a New Language? An Efficient Solution for Self-Supervised Learning Models Unseen Languages Adaption in Low-Resource Scenario,"The utilization of speech Self-Supervised Learning (SSL) models achieves
impressive performance on Automatic Speech Recognition (ASR). However, in
low-resource language ASR, they encounter the domain mismatch problem between
pre-trained and low-resource languages. Typical solutions like fine-tuning the
SSL model suffer from high computation costs while using frozen SSL models as
feature extractors comes with poor performance. To handle these issues, we
extend a conventional efficient fine-tuning scheme based on the adapter. We add
an extra intermediate adaptation to warm up the adapter and downstream model
initialization. Remarkably, we update only 1-5% of the total model parameters
to achieve the adaptation. Experimental results on the ML-SUPERB dataset show
that our solution outperforms conventional efficient fine-tuning. It achieves
up to a 28% relative improvement in the Character/Phoneme error rate when
adapting to unseen languages.",2024-11-27,"Shih-Heng Wang, Zih-Ching Chen, Jiatong Shi, Ming-To Chuang, Guan-Ting Lin, Kuan-Po Huang, David Harwath, Shang-Wen Li, Hung-yi Lee",http://arxiv.org/pdf/2411.18217v2,cs.CL
Human Evaluation of Procedural Knowledge Graph Extraction from Text with Large Language Models,"Procedural Knowledge is the know-how expressed in the form of sequences of
steps needed to perform some tasks. Procedures are usually described by means
of natural language texts, such as recipes or maintenance manuals, possibly
spread across different documents and systems, and their interpretation and
subsequent execution is often left to the reader. Representing such procedures
in a Knowledge Graph (KG) can be the basis to build digital tools to support
those users who need to apply or execute them. In this paper, we leverage Large
Language Model (LLM) capabilities and propose a prompt engineering approach to
extract steps, actions, objects, equipment and temporal information from a
textual procedure, in order to populate a Procedural KG according to a
pre-defined ontology. We evaluate the KG extraction results by means of a user
study, in order to qualitatively and quantitatively assess the perceived
quality and usefulness of the LLM-extracted procedural knowledge. We show that
LLMs can produce outputs of acceptable quality and we assess the subjective
perception of AI by human evaluators.",2024-11-27,"Valentina Anita Carriero, Antonia Azzini, Ilaria Baroni, Mario Scrocca, Irene Celino",http://arxiv.org/pdf/2412.03589v1,cs.CL
Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning,"Vision-language models (VLMs) have shown remarkable advancements in
multimodal reasoning tasks. However, they still often generate inaccurate or
irrelevant responses due to issues like hallucinated image understandings or
unrefined reasoning paths. To address these challenges, we introduce Critic-V,
a novel framework inspired by the Actor-Critic paradigm to boost the reasoning
capability of VLMs. This framework decouples the reasoning process and critic
process by integrating two independent components: the Reasoner, which
generates reasoning paths based on visual and textual inputs, and the Critic,
which provides constructive critique to refine these paths. In this approach,
the Reasoner generates reasoning responses according to text prompts, which can
evolve iteratively as a policy based on feedback from the Critic. This
interaction process was theoretically driven by a reinforcement learning
framework where the Critic offers natural language critiques instead of scalar
rewards, enabling more nuanced feedback to boost the Reasoner's capability on
complex reasoning tasks. The Critic model is trained using Direct Preference
Optimization (DPO), leveraging a preference dataset of critiques ranked by
Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results
show that the Critic-V framework significantly outperforms existing methods,
including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning
accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner
and constructive feedback from the preference-optimized Critic enables a more
reliable and context-sensitive multimodal reasoning process. Our approach
provides a promising solution to enhance the reliability of VLMs, improving
their performance in real-world reasoning-heavy multimodal applications such as
autonomous driving and embodied intelligence.",2024-11-27,"Di Zhang, Junxian Li, Jingdi Lei, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, Dongzhan Zhou",http://arxiv.org/pdf/2411.18203v5,cs.CL
SentiXRL: An advanced large language Model Framework for Multilingual Fine-Grained Emotion Classification in Complex Text Environment,"With strong expressive capabilities in Large Language Models(LLMs),
generative models effectively capture sentiment structures and deep semantics,
however, challenges remain in fine-grained sentiment classification across
multi-lingual and complex contexts. To address this, we propose the Sentiment
Cross-Lingual Recognition and Logic Framework (SentiXRL), which incorporates
two modules,an emotion retrieval enhancement module to improve sentiment
classification accuracy in complex contexts through historical dialogue and
logical reasoning,and a self-circulating analysis negotiation mechanism
(SANM)to facilitates autonomous decision-making within a single model for
classification tasks.We have validated SentiXRL's superiority on multiple
standard datasets, outperforming existing models on CPED and CH-SIMS,and
achieving overall better performance on MELD,Emorynlp and IEMOCAP. Notably, we
unified labels across several fine-grained sentiment annotation datasets and
conducted category confusion experiments, revealing challenges and impacts of
class imbalance in standard datasets.",2024-11-27,"Jie Wang, Yichen Wang, Zhilin Zhang, Jianhao Zeng, Kaidi Wang, Zhiyang Chen",http://arxiv.org/pdf/2411.18162v1,cs.CL
A survey on cutting-edge relation extraction techniques based on language models,"This comprehensive survey delves into the latest advancements in Relation
Extraction (RE), a pivotal task in natural language processing essential for
applications across biomedical, financial, and legal sectors. This study
highlights the evolution and current state of RE techniques by analyzing 137
papers presented at the Association for Computational Linguistics (ACL)
conferences over the past four years, focusing on models that leverage language
models. Our findings underscore the dominance of BERT-based methods in
achieving state-of-the-art results for RE while also noting the promising
capabilities of emerging large language models (LLMs) like T5, especially in
few-shot relation extraction scenarios where they excel in identifying
previously unseen relations.",2024-11-27,"Jose A. Diaz-Garcia, Julio Amador Diaz Lopez",http://arxiv.org/pdf/2411.18157v1,cs.CL
MSA-ASR: Efficient Multilingual Speaker Attribution with frozen ASR Models,"Speaker-attributed automatic speech recognition (SA-ASR) aims to transcribe
speech while assigning transcripts to the corresponding speakers accurately.
Existing methods often rely on complex modular systems or require extensive
fine-tuning of joint modules, limiting their adaptability and general
efficiency. This paper introduces a novel approach, leveraging a frozen
multilingual ASR model to incorporate speaker attribution into the
transcriptions, using only standard monolingual ASR datasets. Our method
involves training a speaker module to predict speaker embeddings based on weak
labels without requiring additional ASR model modifications. Despite being
trained exclusively with non-overlapping monolingual data, our approach
effectively extracts speaker attributes across diverse multilingual datasets,
including those with overlapping speech. Experimental results demonstrate
competitive performance compared to strong baselines, highlighting the model's
robustness and potential for practical applications.",2024-11-27,"Thai-Binh Nguyen, Alexander Waibel",http://arxiv.org/pdf/2411.18152v2,cs.CL
SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation,"Full-duplex multimodal large language models (LLMs) provide a unified
framework for addressing diverse speech understanding and generation tasks,
enabling more natural and seamless human-machine conversations. Unlike
traditional modularised conversational AI systems, which separate speech
recognition, understanding, and text-to-speech generation into distinct
components, multimodal LLMs operate as single end-to-end models. This
streamlined design eliminates error propagation across components and fully
leverages the rich non-verbal information embedded in input speech signals. We
introduce SALMONN-omni, a codec-free, full-duplex speech understanding and
generation model capable of simultaneously listening to its own generated
speech and background sounds while speaking. To support this capability, we
propose a novel duplex spoken dialogue framework incorporating a ``thinking''
mechanism that facilitates asynchronous text and speech generation relying on
embeddings instead of codecs (quantized speech and audio tokens). Experimental
results demonstrate SALMONN-omni's versatility across a broad range of
streaming speech tasks, including speech recognition, speech enhancement, and
spoken question answering. Additionally, SALMONN-omni excels at managing
turn-taking, barge-in, and echo cancellation scenarios, establishing its
potential as a robust prototype for full-duplex conversational AI systems. To
the best of our knowledge, SALMONN-omni is the first codec-free model of its
kind. A full technical report along with model checkpoints will be released
soon.",2024-11-27,"Wenyi Yu, Siyin Wang, Xiaoyu Yang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang",http://arxiv.org/pdf/2411.18138v1,cs.CL
Curriculum Demonstration Selection for In-Context Learning,"Large Language Models (LLMs) have shown strong in-context learning (ICL)
abilities with a few demonstrations. However, one critical challenge is how to
select demonstrations to elicit the full potential of LLMs. In this paper, we
propose Curriculum Demonstration Selection (CDS), a novel demonstration
selection method for ICL. Instead of merely using similarity, CDS additionally
partitions samples by their complexity measurements. Following curriculum
learning, CDS then selects demonstrations from easy to difficult. Thus the
selected demonstrations cover a wide range of difficulty levels, enabling LLMs
to learn from varied complexities within the training set. Experiments
demonstrate that our CDS consistently outperforms baseline methods, achieving
notable improvements across nine LLMs on three benchmarks. Moreover, CDS proves
especially effective in enhancing LLM performance in solving challenging
problems.",2024-11-27,"Duc Anh Vu, Nguyen Tran Cong Duy, Xiaobao Wu, Hoang Minh Nhat, Du Mingzhe, Nguyen Thanh Thong, Anh Tuan Luu",http://arxiv.org/pdf/2411.18126v2,cs.CL
Training and Evaluating Language Models with Template-based Data Generation,"The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,
and Llama has significantly transformed natural language processing, showcasing
remarkable capabilities in understanding and generating language. However,
these models often struggle with tasks requiring complex reasoning,
particularly in mathematical problem-solving, due in part to the scarcity of
large-scale, high-quality, domain-specific datasets necessary for training
sophisticated reasoning abilities. To address this limitation, we introduce
Template-based Data Generation (TDG), a novel approach that leverages LLMs
(GPT-4) to automatically generate parameterized meta-templates, which are then
used to synthesize a vast array of high-quality problems and solutions.
Leveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset
comprising over 7 million synthetically generated grade school math
problems--each accompanied by code-based and natural language solutions--with
the potential to generate an effectively unlimited number more. This dataset
alleviates the scarcity of large-scale mathematical datasets and serves as a
valuable resource for pre-training, fine-tuning, and evaluating LLMs in
mathematical reasoning. Our method not only enables the generation of virtually
infinite data but also elevates data augmentation to a new level by using GPT-4
for meta-template generation, ensuring diverse and high-quality problem
structures. The TemplateMath Part I: TemplateGSM dataset is publicly available
at https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available
at https://github.com/iiis-ai/TemplateMath.",2024-11-27,Yifan Zhang,http://arxiv.org/pdf/2411.18104v3,cs.CL
Fine-Tuning Small Embeddings for Elevated Performance,"Contextual Embeddings have yielded state-of-the-art results in various
natural language processing tasks. However, these embeddings are constrained by
models requiring large amounts of data and huge computing power. This is an
issue for low-resource languages like Nepali as the amount of data available
over the internet is not always sufficient for the models. This work has taken
an incomplete BERT model with six attention heads pretrained on Nepali language
and finetuned it on previously unseen data. The obtained results from intrinsic
and extrinsic evaluations have been compared to the results drawn from the
original model baseline and a complete BERT model pretrained on Nepali language
as the oracle. The results demonstrate that even though the oracle is better on
average, finetuning the small embeddings drastically improves results compared
to the original baseline.",2024-11-27,Biraj Silwal,http://arxiv.org/pdf/2411.18099v1,cs.CL
MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache,"How to efficiently serve LLMs in practice has become exceptionally
challenging due to their prohibitive memory and computation requirements. In
this study, we investigate optimizing the KV cache, whose memory footprint
poses a critical bottleneck in LLM inference, especially when dealing with long
context tasks. To tackle the challenge, we introduce MiniKV, a KV cache
optimization method that simultaneously preserves long context task accuracy
while significantly reducing KV cache size via a novel 2-bit
layer-discriminative KV cache. More importantly, we develop specialized CUDA
kernels to make MiniKV compatible with FlashAttention. Experiments on a wide
range of long context tasks show that MiniKV effectively achieves 86% KV cache
compression ratio while recovering over 98.5% of accuracy, outperforming
state-of-the-art methods while achieving excellent measured system performance
improvements.",2024-11-27,"Akshat Sharma, Hangliang Ding, Jianping Li, Neel Dani, Minjia Zhang",http://arxiv.org/pdf/2411.18077v2,cs.CL
Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?,"Over the past few decades, Artificial Intelligence(AI) has progressed from
the initial machine learning stage to the deep learning stage, and now to the
stage of foundational models. Foundational models have the characteristics of
pre-training, transfer learning, and self-supervised learning, and pre-trained
models can be fine-tuned and applied to various downstream tasks. Under the
framework of foundational models, models such as Bidirectional Encoder
Representations from Transformers(BERT) and Generative Pre-trained
Transformer(GPT) have greatly advanced the development of natural language
processing(NLP), especially the emergence of many models based on BERT. BERT
broke through the limitation of only using one-way methods for language
modeling in pre-training by using a masked language model. It can capture
bidirectional context information to predict the masked words in the sequence,
this can improve the feature extraction ability of the model. This makes the
model very useful for downstream tasks, especially for specialized
applications. The model using the bidirectional encoder can better understand
the domain knowledge and be better applied to these downstream tasks. So we
hope to help understand how this technology has evolved and improved model
performance in various natural language processing tasks under the background
of foundational models and reveal its importance in capturing context
information and improving the model's performance on downstream tasks. This
article analyzes one-way and bidirectional models based on GPT and BERT and
compares their differences based on the purpose of the model. It also briefly
analyzes BERT and the improvements of some models based on BERT. The model's
performance on the Stanford Question Answering Dataset(SQuAD) and General
Language Understanding Evaluation(GLUE) was compared.",2024-11-27,"Lewen Yang, Xuanyu Zhou, Juao Fan, Xinyi Xie, Shengxin Zhu",http://arxiv.org/pdf/2411.18021v1,cs.CL
JPPO: Joint Power and Prompt Optimization for Accelerated Large Language Model Services,"Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks, leading to their increasing deployment in wireless networks for
a wide variety of user services. However, the growing longer prompt setting
highlights the crucial issue of computational resource demands and huge
communication load. To address this challenge, we propose Joint Power and
Prompt Optimization (JPPO), a framework that combines Small Language Model
(SLM)-based prompt compression with wireless power allocation optimization. By
deploying SLM at user devices for prompt compression and employing Deep
Reinforcement Learning for joint optimization of compression ratio and
transmission power, JPPO effectively balances service quality with resource
efficiency. Experimental results demonstrate that our framework achieves high
service fidelity and low bit error rates while optimizing power usage in
wireless LLM services. The system reduces response time by about 17%, with the
improvement varying based on the length of the original prompt.",2024-11-27,"Feiran You, Hongyang Du, Kaibin Huang, Abbas Jamalipour",http://arxiv.org/pdf/2411.18010v2,cs.CL
DRS: Deep Question Reformulation With Structured Output,"Question answering represents a core capability of large language models
(LLMs). However, when individuals encounter unfamiliar knowledge in texts, they
often formulate questions that the text itself cannot answer due to
insufficient understanding of the underlying information. Recent studies reveal
that while LLMs can detect unanswerable questions, they struggle to assist
users in reformulating these questions. Even advanced models like GPT-3.5
demonstrate limited effectiveness in this regard. To address this limitation,
we propose DRS: Deep Question Reformulation with Structured Output, a novel
zero-shot method aimed at enhancing LLMs ability to assist users in
reformulating questions to extract relevant information from new documents. DRS
combines the strengths of LLMs with a DFS-based algorithm to iteratively
explore potential entity combinations and constrain outputs using predefined
entities. This structured approach significantly enhances the reformulation
capabilities of LLMs. Comprehensive experimental evaluations demonstrate that
DRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while
also enhancing the performance of open-source models, such as Gemma2-9B, from
26.35% to 56.75%.",2024-11-27,"Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Nanyun Peng, Kai-Wei Chang",http://arxiv.org/pdf/2411.17993v4,cs.CL
New Faithfulness-Centric Interpretability Paradigms for Natural Language Processing,"As machine learning becomes more widespread and is used in more critical
applications, it's important to provide explanations for these models, to
prevent unintended behavior. Unfortunately, many current interpretability
methods struggle with faithfulness. Therefore, this Ph.D. thesis investigates
the question ""How to provide and ensure faithful explanations for complex
general-purpose neural NLP models?"" The main thesis is that we should develop
new paradigms in interpretability. This is achieved by first developing solid
faithfulness metrics and then applying the lessons learned from this
investigation to develop new paradigms. The two new paradigms explored are
faithfulness measurable models (FMMs) and self-explanations. The idea in
self-explanations is to have large language models explain themselves, we
identify that current models are not capable of doing this consistently.
However, we suggest how this could be achieved. The idea of FMMs is to create
models that are designed such that measuring faithfulness is cheap and precise.
This makes it possible to optimize an explanation towards maximum faithfulness,
which makes FMMs designed to be explained. We find that FMMs yield explanations
that are near theoretical optimal in terms of faithfulness. Overall, from all
investigations of faithfulness, results show that post-hoc and intrinsic
explanations are by default model and task-dependent. However, this was not the
case when using FMMs, even with the same post-hoc explanation methods. This
shows, that even simple modifications to the model, such as randomly masking
the training dataset, as was done in FMMs, can drastically change the situation
and result in consistently faithful explanations. This answers the question of
how to provide and ensure faithful explanations.",2024-11-27,Andreas Madsen,http://arxiv.org/pdf/2411.17992v1,cs.CL
VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format,"Recent researches on video large language models (VideoLLM) predominantly
focus on model architectures and training datasets, leaving the interaction
format between the user and the model under-explored. In existing works, users
often interact with VideoLLMs by using the entire video and a query as input,
after which the model generates a response. This interaction format constrains
the application of VideoLLMs in scenarios such as live-streaming comprehension
where videos do not end and responses are required in a real-time manner, and
also results in unsatisfactory performance on time-sensitive tasks that
requires localizing video segments. In this paper, we focus on a video-text
duet interaction format. This interaction format is characterized by the
continuous playback of the video, and both the user and the model can insert
their text messages at any position during the video playback. When a text
message ends, the video continues to play, akin to the alternative of two
performers in a duet. We construct MMDuetIT, a video-text training dataset
designed to adapt VideoLLMs to video-text duet interaction format. We also
introduce the Multi-Answer Grounded Video Question Answering (MAGQA) task to
benchmark the real-time response ability of VideoLLMs. Trained on MMDuetIT,
MMDuet demonstrates that adopting the video-text duet interaction format
enables the model to achieve significant improvements in various time-sensitive
tasks (76% CIDEr on YouCook2 dense video captioning, 90\% mAP on QVHighlights
highlight detection and 25% R@0.5 on Charades-STA temporal video grounding)
with minimal training efforts, and also enable VideoLLMs to reply in a
real-time manner as the video plays. Code, data and demo are available at:
https://github.com/yellow-binary-tree/MMDuet.",2024-11-27,"Yueqian Wang, Xiaojun Meng, Yuxuan Wang, Jianxin Liang, Jiansheng Wei, Huishuai Zhang, Dongyan Zhao",http://arxiv.org/pdf/2411.17991v1,cs.CL
Verbalized Representation Learning for Interpretable Few-Shot Generalization,"Humans recognize objects after observing only a few examples, a remarkable
capability enabled by their inherent language understanding of the real-world
environment. Developing verbalized and interpretable representation can
significantly improve model generalization in low-data settings. In this work,
we propose Verbalized Representation Learning (VRL), a novel approach for
automatically extracting human-interpretable features for object recognition
using few-shot data. Our method uniquely captures inter-class differences and
intra-class commonalities in the form of natural language by employing a
Vision-Language Model (VLM) to identify key discriminative features between
different classes and shared characteristics within the same class. These
verbalized features are then mapped to numeric vectors through the VLM. The
resulting feature vectors can be further utilized to train and infer with
downstream classifiers. Experimental results show that, at the same model
scale, VRL achieves a 24% absolute improvement over prior state-of-the-art
methods while using 95% less data and a smaller mode. Furthermore, compared to
human-labeled attributes, the features learned by VRL exhibit a 20% absolute
gain when used for downstream classification tasks. Code is available at:
https://github.com/joeyy5588/VRL/tree/main.",2024-11-27,"Cheng-Fu Yang, Da Yin, Wenbo Hu, Nanyun Peng, Bolei Zhou, Kai-Wei Chang",http://arxiv.org/pdf/2411.18651v1,cs.CL
QuaLLM-Health: An Adaptation of an LLM-Based Framework for Quantitative Data Extraction from Online Health Discussions,"Health-related discussions on social media like Reddit offer valuable
insights, but extracting quantitative data from unstructured text is
challenging. In this work, we present an adapted framework from QuaLLM into
QuaLLM-Health for extracting clinically relevant quantitative data from Reddit
discussions about glucagon-like peptide-1 (GLP-1) receptor agonists using large
language models (LLMs). We collected 410k posts and comments from five
GLP-1-related communities using the Reddit API in July 2024. After filtering
for cancer-related discussions, 2,059 unique entries remained. We developed
annotation guidelines to manually extract variables such as cancer
survivorship, family cancer history, cancer types mentioned, risk perceptions,
and discussions with physicians. Two domain-experts independently annotated a
random sample of 100 entries to create a gold-standard dataset. We then
employed iterative prompt engineering with OpenAI's ""GPT-4o-mini"" on the
gold-standard dataset to build an optimized pipeline that allowed us to extract
variables from the large dataset. The optimized LLM achieved accuracies above
0.85 for all variables, with precision, recall and F1 score macro averaged >
0.90, indicating balanced performance. Stability testing showed a 95% match
rate across runs, confirming consistency. Applying the framework to the full
dataset enabled efficient extraction of variables necessary for downstream
analysis, costing under $3 and completing in approximately one hour.
QuaLLM-Health demonstrates that LLMs can effectively and efficiently extract
clinically relevant quantitative data from unstructured social media content.
Incorporating human expertise and iterative prompt refinement ensures accuracy
and reliability. This methodology can be adapted for large-scale analysis of
patient-generated data across various health domains, facilitating valuable
insights for healthcare research.",2024-11-27,"Ramez Kouzy, Roxanna Attar-Olyaee, Michael K. Rooney, Comron J. Hassanzadeh, Junyi Jessy Li, Osama Mohamad",http://arxiv.org/pdf/2411.17967v1,cs.CL
"Evaluating Generative AI-Enhanced Content: A Conceptual Framework Using Qualitative, Quantitative, and Mixed-Methods Approaches","Generative AI (GenAI) has revolutionized content generation, offering
transformative capabilities for improving language coherence, readability, and
overall quality. This manuscript explores the application of qualitative,
quantitative, and mixed-methods research approaches to evaluate the performance
of GenAI models in enhancing scientific writing. Using a hypothetical use case
involving a collaborative medical imaging manuscript, we demonstrate how each
method provides unique insights into the impact of GenAI. Qualitative methods
gather in-depth feedback from expert reviewers, analyzing their responses using
thematic analysis tools to capture nuanced improvements and identify
limitations. Quantitative approaches employ automated metrics such as BLEU,
ROUGE, and readability scores, as well as user surveys, to objectively measure
improvements in coherence, fluency, and structure. Mixed-methods research
integrates these strengths, combining statistical evaluations with detailed
qualitative insights to provide a comprehensive assessment. These research
methods enable quantifying improvement levels in GenAI-generated content,
addressing critical aspects of linguistic quality and technical accuracy. They
also offer a robust framework for benchmarking GenAI tools against traditional
editing processes, ensuring the reliability and effectiveness of these
technologies. By leveraging these methodologies, researchers can evaluate the
performance boost driven by GenAI, refine its applications, and guide its
responsible adoption in high-stakes domains like healthcare and scientific
research. This work underscores the importance of rigorous evaluation
frameworks for advancing trust and innovation in GenAI.",2024-11-26,Saman Sarraf,http://arxiv.org/pdf/2411.17943v1,cs.CL
HOPPR Medical-Grade Platform for Medical Imaging AI,"Technological advances in artificial intelligence (AI) have enabled the
development of large vision language models (LVLMs) that are trained on
millions of paired image and text samples. Subsequent research efforts have
demonstrated great potential of LVLMs to achieve high performance in medical
imaging use cases (e.g., radiology report generation), but there remain
barriers that hinder the ability to deploy these solutions broadly. These
include the cost of extensive computational requirements for developing large
scale models, expertise in the development of sophisticated AI models, and the
difficulty in accessing substantially large, high-quality datasets that
adequately represent the population in which the LVLM solution is to be
deployed. The HOPPR Medical-Grade Platform addresses these barriers by
providing powerful computational infrastructure, a suite of foundation models
on top of which developers can fine-tune for their specific use cases, and a
robust quality management system that sets a standard for evaluating fine-tuned
models for deployment in clinical settings. The HOPPR Platform has access to
millions of imaging studies and text reports sourced from hundreds of imaging
centers from diverse populations to pretrain foundation models and enable use
case-specific cohorts for fine-tuning. All data are deidentified and securely
stored for HIPAA compliance. Additionally, developers can securely host models
on the HOPPR platform and access them via an API to make inferences using these
models within established clinical workflows. With the Medical-Grade Platform,
HOPPR's mission is to expedite the deployment of LVLM solutions for medical
imaging and ultimately optimize radiologist's workflows and meet the growing
demands of the field.",2024-11-26,"Kalina P. Slavkova, Melanie Traughber, Oliver Chen, Robert Bakos, Shayna Goldstein, Dan Harms, Bradley J. Erickson, Khan M. Siddiqui",http://arxiv.org/pdf/2411.17891v1,cs.CL
Leveraging Large Language Models and Topic Modeling for Toxicity Classification,"Content moderation and toxicity classification represent critical tasks with
significant social implications. However, studies have shown that major
classification models exhibit tendencies to magnify or reduce biases and
potentially overlook or disadvantage certain marginalized groups within their
classification processes. Researchers suggest that the positionality of
annotators influences the gold standard labels in which the models learned from
propagate annotators' bias. To further investigate the impact of annotator
positionality, we delve into fine-tuning BERTweet and HateBERT on the dataset
while using topic-modeling strategies for content moderation. The results
indicate that fine-tuning the models on specific topics results in a notable
improvement in the F1 score of the models when compared to the predictions
generated by other prominent classification models such as GPT-4,
PerspectiveAPI, and RewireAPI. These findings further reveal that the
state-of-the-art large language models exhibit significant limitations in
accurately detecting and interpreting text toxicity contrasted with earlier
methodologies. Code is available at
https://github.com/aheldis/Toxicity-Classification.git.",2024-11-26,"Haniyeh Ehsani Oskouie, Christina Chance, Claire Huang, Margaret Capetz, Elizabeth Eyeson, Majid Sarrafzadeh",http://arxiv.org/pdf/2411.17876v1,cs.CL
LongKey: Keyphrase Extraction for Long Documents,"In an era of information overload, manually annotating the vast and growing
corpus of documents and scholarly papers is increasingly impractical. Automated
keyphrase extraction addresses this challenge by identifying representative
terms within texts. However, most existing methods focus on short documents (up
to 512 tokens), leaving a gap in processing long-context documents. In this
paper, we introduce LongKey, a novel framework for extracting keyphrases from
lengthy documents, which uses an encoder-based language model to capture
extended text intricacies. LongKey uses a max-pooling embedder to enhance
keyphrase candidate representation. Validated on the comprehensive LDKP
datasets and six diverse, unseen datasets, LongKey consistently outperforms
existing unsupervised and language model-based keyphrase extraction methods.
Our findings demonstrate LongKey's versatility and superior performance,
marking an advancement in keyphrase extraction for varied text lengths and
domains.",2024-11-26,"Jeovane Honorio Alves, Radu State, Cinthia Obladen de Almendra Freitas, Jean Paul Barddal",http://arxiv.org/pdf/2411.17863v1,cs.CL
Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats,"As large language models (LLMs) become increasingly capable, it is prudent to
assess whether safety measures remain effective even if LLMs intentionally try
to bypass them. Previous work introduced control evaluations, an adversarial
framework for testing deployment strategies of untrusted models (i.e., models
which might be trying to bypass safety measures). While prior work treats a
single failure as unacceptable, we perform control evaluations in a
""distributed threat setting"" -- a setting where no single action is
catastrophic and no single action provides overwhelming evidence of
misalignment. We approach this problem with a two-level deployment framework
that uses an adaptive macro-protocol to choose between micro-protocols.
Micro-protocols operate on a single task, using a less capable, but extensively
tested (trusted) model to harness and monitor the untrusted model. Meanwhile,
the macro-protocol maintains an adaptive credence on the untrusted model's
alignment based on its past actions, using it to pick between safer and riskier
micro-protocols. We evaluate our method in a code generation testbed where a
red team attempts to generate subtly backdoored code with an LLM whose
deployment is safeguarded by a blue team. We plot Pareto frontiers of safety (#
of non-backdoored solutions) and usefulness (# of correct solutions). At a
given level of usefulness, our adaptive deployment strategy reduces the number
of backdoors by 80% compared to non-adaptive baselines.",2024-11-26,"Jiaxin Wen, Vivek Hebbar, Caleb Larson, Aryan Bhatt, Ansh Radhakrishnan, Mrinank Sharma, Henry Sleight, Shi Feng, He He, Ethan Perez, Buck Shlegeris, Akbir Khan",http://arxiv.org/pdf/2411.17693v1,cs.CL
Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens,"We reveal that low-bit quantization favors undertrained large language models
(LLMs) by observing that models with larger sizes or fewer training tokens
experience less quantization-induced degradation (QiD) when applying low-bit
quantization, whereas smaller models with extensive training tokens suffer
significant QiD. To gain deeper insights into this trend, we study over 1500
quantized LLM checkpoints of various sizes and at different training levels
(undertrained or fully trained) in a controlled setting, deriving scaling laws
for understanding the relationship between QiD and factors such as the number
of training tokens, model size and bit width.
  With the derived scaling laws, we propose a novel perspective that we can use
QiD to measure an LLM's training levels and determine the number of training
tokens required for fully training LLMs of various sizes. Moreover, we use the
scaling laws to predict the quantization performance of different-sized LLMs
trained with 100 trillion tokens. Our projection shows that the low-bit
quantization performance of future models, which are expected to be trained
with over 100 trillion tokens, may NOT be desirable. This poses a potential
challenge for low-bit quantization in the future and highlights the need for
awareness of a model's training level when evaluating low-bit quantization
research. To facilitate future research on this problem, we release all the
1500+ quantized checkpoints used in this work at
https://huggingface.co/Xu-Ouyang.",2024-11-26,"Xu Ouyang, Tao Ge, Thomas Hartvigsen, Zhisong Zhang, Haitao Mi, Dong Yu",http://arxiv.org/pdf/2411.17691v2,cs.CL
Attamba: Attending To Multi-Token States,"When predicting the next token in a sequence, vanilla transformers compute
attention over all previous tokens, resulting in quadratic scaling of compute
with sequence length. State-space models compress the entire sequence of tokens
into a fixed-dimensional representation to improve efficiency, while other
architectures achieve sub-quadratic complexity via low-rank projections or
sparse attention patterns over the sequence. In this paper, we introduce
Attamba, a novel architecture that uses state-space models to compress chunks
of tokens and applies attention on these compressed key-value representations.
We find that replacing key and value projections in a transformer with SSMs can
improve model quality and enable flexible token chunking, resulting in 24%
improved perplexity with transformer of similar KV-Cache and attention
footprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity
trade-off. Attamba can perform attention on chunked-sequences of variable
length, enabling a smooth transition between quadratic and linear scaling,
offering adaptable efficiency gains.",2024-11-26,"Yash Akhauri, Safeen Huda, Mohamed S. Abdelfattah",http://arxiv.org/pdf/2411.17685v1,cs.CL
Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning,"Tokenization methods like Byte-Pair Encoding (BPE) enhance computational
efficiency in large language models (LLMs) but often obscure internal character
structures within tokens. This limitation hinders LLMs' ability to predict
precise character positions, which is crucial in tasks like Chinese Spelling
Correction (CSC) where identifying the positions of misspelled characters
accelerates correction processes. We propose Token Internal Position Awareness
(TIPA), a method that significantly improves models' ability to capture
character positions within tokens by training them on reverse character
prediction tasks using the tokenizer's vocabulary. Experiments demonstrate that
TIPA enhances position prediction accuracy in LLMs, enabling more precise
identification of target characters in original text. Furthermore, when applied
to downstream tasks that do not require exact position prediction, TIPA still
boosts performance in tasks needing character-level information, validating its
versatility and effectiveness.",2024-11-26,"Zhu Xu, Zhiqiang Zhao, Zihan Zhang, Yuchi Liu, Quanwei Shen, Fei Liu, Yu Kuang, Jian He, Conglin Liu",http://arxiv.org/pdf/2411.17679v4,cs.CL
Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with Receptive-Field-Aware Attention Weighting,"Understanding the emotions in a dialogue usually requires external knowledge
to accurately understand the contents. As the LLMs become more and more
powerful, we do not want to settle on the limited ability of the pre-trained
language model. However, the LLMs either can only process text modality or are
too expensive to process the multimedia information. We aim to utilize both the
power of LLMs and the supplementary features from the multimedia modalities. In
this paper, we present a framework, Lantern, that can improve the performance
of a certain vanilla model by prompting large language models with
receptive-field-aware attention weighting. This framework trained a multi-task
vanilla model to produce probabilities of emotion classes and dimension scores.
These predictions are fed into the LLMs as references to adjust the predicted
probabilities of each emotion class with its external knowledge and contextual
understanding. We slice the dialogue into different receptive fields, and each
sample is included in exactly t receptive fields. Finally, the predictions of
LLMs are merged with a receptive-field-aware attention-driven weighting module.
In the experiments, vanilla models CORECT and SDT are deployed in Lantern with
GPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way
settings demonstrated that the Lantern can significantly improve the
performance of current vanilla models by up to 1.23% and 1.80%.",2024-11-26,"Liyun Zhang, Dian Ding, Yu Lu, Yi-Chao Chen, Guangtao Xue",http://arxiv.org/pdf/2411.17674v1,cs.CL
Linguistic Laws Meet Protein Sequences: A Comparative Analysis of Subword Tokenization Methods,"Tokenization is a crucial step in processing protein sequences for machine
learning models, as proteins are complex sequences of amino acids that require
meaningful segmentation to capture their functional and structural properties.
However, existing subword tokenization methods, developed primarily for human
language, may be inadequate for protein sequences, which have unique patterns
and constraints. This study evaluates three prominent tokenization approaches,
Byte-Pair Encoding (BPE), WordPiece, and SentencePiece, across varying
vocabulary sizes (400-6400), analyzing their effectiveness in protein sequence
representation, domain boundary preservation, and adherence to established
linguistic laws. Our comprehensive analysis reveals distinct behavioral
patterns among these tokenizers, with vocabulary size significantly influencing
their performance. BPE demonstrates better contextual specialization and
marginally better domain boundary preservation at smaller vocabularies, while
SentencePiece achieves better encoding efficiency, leading to lower fertility
scores. WordPiece offers a balanced compromise between these characteristics.
However, all tokenizers show limitations in maintaining protein domain
integrity, particularly as vocabulary size increases. Analysis of linguistic
law adherence shows partial compliance with Zipf's and Brevity laws but notable
deviations from Menzerath's law, suggesting that protein sequences may follow
distinct organizational principles from natural languages. These findings
highlight the limitations of applying traditional NLP tokenization methods to
protein sequences and emphasize the need for developing specialized
tokenization strategies that better account for the unique characteristics of
proteins.",2024-11-26,"Burak Suyunu, Enes Taylan, Arzucan Özgür",http://arxiv.org/pdf/2411.17669v1,cs.CL
How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations,"Multimodal foundation models aim to create a unified representation space
that abstracts away from surface features like language syntax or modality
differences. To investigate this, we study the internal representations of
three recent models, analyzing the model activations from semantically
equivalent sentences across languages in the text and speech modalities. Our
findings reveal that: 1) Cross-modal representations converge over model
layers, except in the initial layers specialized at text and speech processing.
2) Length adaptation is crucial for reducing the cross-modal gap between text
and speech, although current approaches' effectiveness is primarily limited to
high-resource languages. 3) Speech exhibits larger cross-lingual differences
than text. 4) For models not explicitly trained for modality-agnostic
representations, the modality gap is more prominent than the language gap.",2024-11-26,"Hyunji Lee, Danni Liu, Supriti Sinhamahapatra, Jan Niehues",http://arxiv.org/pdf/2411.17666v2,cs.CL
Signs as Tokens: A Retrieval-Enhanced Multilingual Sign Language Generator,"Sign language is a visual language that encompasses all linguistic features
of natural languages and serves as the primary communication method for the
deaf and hard-of-hearing communities. Although many studies have successfully
adapted pretrained language models (LMs) for sign language translation
(sign-to-text), the reverse task-sign language generation
(text-to-sign)-remains largely unexplored. In this work, we introduce a
multilingual sign language model, Signs as Tokens (SOKE), which can generate 3D
sign avatars autoregressively from text inputs using a pretrained LM. To align
sign language with the LM, we leverage a decoupled tokenizer that discretizes
continuous signs into token sequences representing various body parts. During
decoding, unlike existing approaches that flatten all part-wise tokens into a
single sequence and predict one token at a time, we propose a multi-head
decoding method capable of predicting multiple tokens simultaneously. This
approach improves inference efficiency while maintaining effective information
fusion across different body parts. To further ease the generation process, we
propose a retrieval-enhanced SLG approach, which incorporates external sign
dictionaries to provide accurate word-level signs as auxiliary conditions,
significantly improving the precision of generated signs. Extensive qualitative
and quantitative evaluations demonstrate the effectiveness of SOKE. Code,
models, and data will be made publicly available.",2024-11-26,"Ronglai Zuo, Rolandos Alexandros Potamias, Evangelos Ververas, Jiankang Deng, Stefanos Zafeiriou",http://arxiv.org/pdf/2411.17799v2,cs.CL
Non-Contextual BERT or FastText? A Comparative Analysis,"Natural Language Processing (NLP) for low-resource languages, which lack
large annotated datasets, faces significant challenges due to limited
high-quality data and linguistic resources. The selection of embeddings plays a
critical role in achieving strong performance in NLP tasks. While contextual
BERT embeddings require a full forward pass, non-contextual BERT embeddings
rely only on table lookup. Existing research has primarily focused on
contextual BERT embeddings, leaving non-contextual embeddings largely
unexplored. In this study, we analyze the effectiveness of non-contextual
embeddings from BERT models (MuRIL and MahaBERT) and FastText models (IndicFT
and MahaFT) for tasks such as news classification, sentiment analysis, and hate
speech detection in one such low-resource language Marathi. We compare these
embeddings with their contextual and compressed variants. Our findings indicate
that non-contextual BERT embeddings extracted from the model's first embedding
layer outperform FastText embeddings, presenting a promising alternative for
low-resource NLP.",2024-11-26,"Abhay Shanbhag, Suramya Jadhav, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi",http://arxiv.org/pdf/2411.17661v3,cs.CL
On Limitations of LLM as Annotator for Low Resource Languages,"Low-resource languages face significant challenges due to the lack of
sufficient linguistic data, resources, and tools for tasks such as supervised
learning, annotation, and classification. This shortage hinders the development
of accurate models and datasets, making it difficult to perform critical NLP
tasks like sentiment analysis or hate speech detection. To bridge this gap,
Large Language Models (LLMs) present an opportunity for potential annotators,
capable of generating datasets and resources for these underrepresented
languages. In this paper, we focus on Marathi, a low-resource language, and
evaluate the performance of both closed-source and open-source LLMs as
annotators, while also comparing these results with fine-tuned BERT models. We
assess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and 9B), and Llama
3.1 (8B and 405B) on classification tasks including sentiment analysis, news
classification, and hate speech detection. Our findings reveal that while LLMs
excel in annotation tasks for high-resource languages like English, they still
fall short when applied to Marathi. Even advanced models like GPT-4o and Llama
3.1 405B underperform compared to fine-tuned BERT-based baselines, with GPT-4o
and Llama 3.1 405B trailing fine-tuned BERT by accuracy margins of 10.2% and
14.1%, respectively. This highlights the limitations of LLMs as annotators for
low-resource languages.",2024-11-26,"Suramya Jadhav, Abhay Shanbhag, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi",http://arxiv.org/pdf/2411.17637v2,cs.CL
"$H^3$Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs","Alignment of pretrained LLMs using instruction-based datasets is critical for
creating fine-tuned models that reflect human preference. A growing number of
alignment-based fine-tuning algorithms and benchmarks emerged recently, fueling
the efforts on effective alignments of pre-trained LLMs to ensure helpful,
harmless, and honest answers from both open-source and closed-source LLMs. This
paper tackles this problem by developing an alignment fusion approach, coined
as $H^3$Fusion, with three unique characteristics. First, $H^3$Fusion ensembles
multiple individually aligned LLMs to create a final fine-tuned alignment model
with enhanced capabilities beyond those of individual models, delivering robust
alignment through promoting helpful, harmless, honest fusion. Second,
$H^3$Fusion leverages the mixture-of-experts (MoE) methodology in two steps. We
first freeze the multi-head attention weights of each individual model while
tuning the FFN layer during alignment fusion. Then we merge the aligned model
weights with an expert router according to the type of input instruction and
dynamically select a subset of experts that are best suited for producing the
output response. Finally, we boost the performance of the resulting
$H^3$3Fusion model by introducing gating loss and regularization terms. The
former penalizes the selection errors of the expert-router, and the latter
mediates the expert weights drifting during fine-tuning and dynamically adjusts
the fusion behavior of the resulting model by canalizing the activations on the
experts. Extensive evaluations on three benchmark datasets show that
$H^3$3Fusion is more helpful, less harmful, and more honest from two aspects:
it outperforms each individually aligned model by $11.37\%$, and it provides
stronger robustness compared to the state-of-the-art LLM ensemble approaches by
$13.77\%$. Code is available at github.com/sftekin/h3fusion.",2024-11-26,"Selim Furkan Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Zachary Yahn, Ling Liu",http://arxiv.org/pdf/2411.17792v1,cs.CL
Scaling Speech-Text Pre-training with Synthetic Interleaved Data,"Speech language models (SpeechLMs) accept speech input and produce speech
output, allowing for more natural human-computer interaction compared to
text-based large language models (LLMs). Traditional approaches for developing
SpeechLMs are constrained by the limited availability of unsupervised speech
data and parallel speech-text data, which are significantly less abundant than
text pre-training data, thereby limiting their scalability as LLMs. We propose
a novel approach to scaling speech-text pre-training by leveraging large-scale
synthetic interleaved data derived from text corpora, eliminating the need for
parallel speech-text datasets. Our method efficiently constructs speech-text
interleaved data by sampling text spans from existing text corpora and
synthesizing corresponding speech spans using a text-to-token model, bypassing
the need to generate actual speech. We also employ a supervised speech
tokenizer derived from an automatic speech recognition (ASR) model by
incorporating a vector-quantized bottleneck into the encoder. This supervised
training approach results in discrete speech tokens with strong semantic
preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining
speech reconstruction quality. Starting from a pre-trained language model and
scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved
speech-text data), we achieve state-of-the-art performance in speech language
modeling and spoken question answering, improving performance on spoken
questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further
demonstrate that by fine-tuning the pre-trained model with speech dialogue
data, we can develop an end-to-end spoken chatbot that achieves competitive
performance comparable to existing baselines in both conversational abilities
and speech quality, even operating exclusively in the speech domain.",2024-11-26,"Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, Jie Tang",http://arxiv.org/pdf/2411.17607v2,cs.CL
What Differentiates Educational Literature? A Multimodal Fusion Approach of Transformers and Computational Linguistics,"The integration of new literature into the English curriculum remains a
challenge since educators often lack scalable tools to rapidly evaluate
readability and adapt texts for diverse classroom needs. This study proposes to
address this gap through a multimodal approach that combines transformer-based
text classification with linguistic feature analysis to align texts with UK Key
Stages. Eight state-of-the-art Transformers were fine-tuned on segmented text
data, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,
500 deep neural network topologies were searched for the classification of
linguistic characteristics, achieving an F1 score of 0.392. The fusion of these
modalities shows a significant improvement, with every multimodal approach
outperforming all unimodal models. In particular, the ELECTRA Transformer fused
with the neural network achieved an F1 score of 0.996. Unimodal and multimodal
approaches are shown to have statistically significant differences in all
validation metrics (accuracy, precision, recall, F1 score) except for inference
time. The proposed approach is finally encapsulated in a stakeholder-facing web
application, providing non-technical stakeholder access to real-time insights
on text complexity, reading difficulty, curriculum alignment, and
recommendations for learning age range. The application empowers data-driven
decision making and reduces manual workload by integrating AI-based
recommendations into lesson planning for English literature.",2024-11-26,Jordan J. Bird,http://arxiv.org/pdf/2411.17593v3,cs.CL
Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey,"Visual Question Answering (VQA) is a challenge task that combines natural
language processing and computer vision techniques and gradually becomes a
benchmark test task in multimodal large language models (MLLMs). The goal of
our survey is to provide an overview of the development of VQA and a detailed
description of the latest models with high timeliness. This survey gives an
up-to-date synthesis of natural language understanding of images and text, as
well as the knowledge reasoning module based on image-question information on
the core VQA tasks. In addition, we elaborate on recent advances in extracting
and fusing modal information with vision-language pretraining models and
multimodal large language models in VQA. We also exhaustively review the
progress of knowledge reasoning in VQA by detailing the extraction of internal
knowledge and the introduction of external knowledge. Finally, we present the
datasets of VQA and different evaluation metrics and discuss possible
directions for future work.",2024-11-26,"Jiayi Kuang, Jingyou Xie, Haohao Luo, Ronghao Li, Zhe Xu, Xianfeng Cheng, Yinghui Li, Xika Lin, Ying Shen",http://arxiv.org/pdf/2411.17558v1,cs.CL
Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code Search,"Low isotropy in an embedding space impairs performance on tasks involving
semantic inference. Our study investigates the impact of isotropy on semantic
code search performance and explores post-processing techniques to mitigate
this issue. We analyze various code language models, examine isotropy in their
embedding spaces, and its influence on search effectiveness. We propose a
modified ZCA whitening technique to control isotropy levels in embeddings. Our
results demonstrate that Soft-ZCA whitening improves the performance of
pre-trained code language models and can complement contrastive fine-tuning.",2024-11-26,"Andor Diera, Lukas Galke, Ansgar Scherp",http://arxiv.org/pdf/2411.17538v2,cs.CL
ShowUI: One Vision-Language-Action Model for GUI Visual Agent,"Building Graphical User Interface (GUI) assistants holds significant promise
for enhancing human workflow productivity. While most agents are
language-based, relying on closed-source API with text-rich meta-information
(e.g., HTML or accessibility tree), they show limitations in perceiving UI
visuals as humans do, highlighting the need for GUI visual agents. In this
work, we develop a vision-language-action model in digital world, namely
ShowUI, which features the following innovations: (i) UI-Guided Visual Token
Selection to reduce computational costs by formulating screenshots as an UI
connected graph, adaptively identifying their redundant relationship and serve
as the criteria for token selection during self-attention blocks; (ii)
Interleaved Vision-Language-Action Streaming that flexibly unifies diverse
needs within GUI tasks, enabling effective management of visual-action history
in navigation or pairing multi-turn query-action sequences per screenshot to
enhance training efficiency; (iii) Small-scale High-quality GUI
Instruction-following Datasets by careful data curation and employing a
resampling strategy to address significant data type imbalances. With above
components, ShowUI, a lightweight 2B model using 256K data, achieves a strong
75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection
further reduces 33% of redundant visual tokens during training and speeds up
the performance by 1.4x. Navigation experiments across web Mind2Web, mobile
AITW, and online MiniWob environments further underscore the effectiveness and
potential of our model in advancing GUI visual agents. The models are available
at https://github.com/showlab/ShowUI.",2024-11-26,"Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, Mike Zheng Shou",http://arxiv.org/pdf/2411.17465v1,cs.CL
FLEX-CLIP: Feature-Level GEneration Network Enhanced CLIP for X-shot Cross-modal Retrieval,"Given a query from one modality, few-shot cross-modal retrieval (CMR)
retrieves semantically similar instances in another modality with the target
domain including classes that are disjoint from the source domain. Compared
with classical few-shot CMR methods, vision-language pretraining methods like
CLIP have shown great few-shot or zero-shot learning performance. However, they
still suffer challenges due to (1) the feature degradation encountered in the
target domain and (2) the extreme data imbalance. To tackle these issues, we
propose FLEX-CLIP, a novel Feature-level Generation Network Enhanced CLIP.
FLEX-CLIP includes two training stages. In multimodal feature generation, we
propose a composite multimodal VAE-GAN network to capture real feature
distribution patterns and generate pseudo samples based on CLIP features,
addressing data imbalance. For common space projection, we develop a gate
residual network to fuse CLIP features with projected features, reducing
feature degradation in X-shot scenarios. Experimental results on four benchmark
datasets show a 7%-15% improvement over state-of-the-art methods, with ablation
studies demonstrating enhancement of CLIP features.",2024-11-26,"Jingyou Xie, Jiayi Kuang, Zhenzhou Lin, Jiarui Ouyang, Zishuo Zhao, Ying Shen",http://arxiv.org/pdf/2411.17454v1,cs.CL
VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models,"Vision-language generative reward models (VL-GenRMs) play a crucial role in
aligning and evaluating multimodal AI systems, yet their own evaluation remains
under-explored. Current assessment methods primarily rely on AI-annotated
preference labels from traditional VL tasks, which can introduce biases and
often fail to effectively challenge state-of-the-art models. To address these
limitations, we introduce VL-RewardBench, a comprehensive benchmark spanning
general multimodal queries, visual hallucination detection, and complex
reasoning tasks. Through our AI-assisted annotation pipeline combining sample
selection with human verification, we curate 1,250 high-quality examples
specifically designed to probe model limitations. Comprehensive evaluation
across 16 leading large vision-language models, demonstrates VL-RewardBench's
effectiveness as a challenging testbed, where even GPT-4o achieves only 65.4%
accuracy, and state-of-the-art open-source models such as Qwen2-VL-72B,
struggle to surpass random-guessing. Importantly, performance on VL-RewardBench
strongly correlates (Pearson's r > 0.9) with MMMU-Pro accuracy using Best-of-N
sampling with VL-GenRMs. Analysis experiments uncover three critical insights
for improving VL-GenRMs: (i) models predominantly fail at basic visual
perception tasks rather than reasoning tasks; (ii) inference-time scaling
benefits vary dramatically by model capacity; and (iii) training VL-GenRMs to
learn to judge substantially boosts judgment capability (+14.7% accuracy for a
7B VL-GenRM). We believe VL-RewardBench along with the experimental insights
will become a valuable resource for advancing VL-GenRMs.",2024-11-26,"Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, Lingpeng Kong, Qi Liu",http://arxiv.org/pdf/2411.17451v1,cs.CL
"""Stupid robot, I want to speak to a human!"" User Frustration Detection in Task-Oriented Dialog Systems","Detecting user frustration in modern-day task-oriented dialog (TOD) systems
is imperative for maintaining overall user satisfaction, engagement, and
retention. However, most recent research is focused on sentiment and emotion
detection in academic settings, thus failing to fully encapsulate implications
of real-world user data. To mitigate this gap, in this work, we focus on user
frustration in a deployed TOD system, assessing the feasibility of
out-of-the-box solutions for user frustration detection. Specifically, we
compare the performance of our deployed keyword-based approach, open-source
approaches to sentiment analysis, dialog breakdown detection methods, and
emerging in-context learning LLM-based detection. Our analysis highlights the
limitations of open-source methods for real-world frustration detection, while
demonstrating the superior performance of the LLM-based approach, achieving a
16\% relative improvement in F1 score on an internal benchmark. Finally, we
analyze advantages and limitations of our methods and provide an insight into
user frustration detection task for industry practitioners.",2024-11-26,"Mireia Hernandez Caralt, Ivan Sekulić, Filip Carević, Nghia Khau, Diana Nicoleta Popa, Bruna Guedes, Victor Guimarães, Zeyu Yang, Andre Manso, Meghana Reddy, Paolo Rosso, Roland Mathis",http://arxiv.org/pdf/2411.17437v2,cs.CL
BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving,"LLMs exhibit advanced reasoning capabilities, offering the potential to
transform natural language questions into mathematical models. However,
existing open-source datasets in operations research domain lack detailed
annotations of the modeling process, such as variable definitions, focusing
solely on objective values, which hinders reinforcement learning applications.
To address this, we release the StructuredOR dataset, annotated with
comprehensive labels that capture the complete mathematical modeling process.
We further propose BPP-Search, an algorithm that integrates reinforcement
learning into a tree-of-thought structure using Beam search, a Process reward
model, and a pairwise Preference algorithm. This approach enables efficient
exploration of tree structures, avoiding exhaustive search while improving
accuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP
datasets show that BPP-Search significantly outperforms state-of-the-art
methods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,
enabling faster retrieval of correct solutions. The StructuredOR dataset is
available on Huggingface https://huggingface.co/datasets/LLM4OR/StructuredOR
and GitHub https://github.com/LLM4OR/StructuredOR.",2024-11-26,"Teng Wang, Wing-Yin Yu, Zhenqi He, Zehua Liu, Hailei Gong, Han Wu, Xiongwei Han, Wei Shi, Ruifeng She, Fangzhou Zhu, Tao Zhong",http://arxiv.org/pdf/2411.17404v4,cs.CL
"One Mind, Many Tongues: A Deep Dive into Language-Agnostic Knowledge Neurons in Large Language Models","Large language models (LLMs) have learned vast amounts of factual knowledge
through self-supervised pre-training on large-scale corpora. Meanwhile, LLMs
have also demonstrated excellent multilingual capabilities, which can express
the learned knowledge in multiple languages. However, the knowledge storage
mechanism in LLMs still remains mysterious. Some researchers attempt to
demystify the factual knowledge in LLMs from the perspective of knowledge
neurons, and subsequently discover language-agnostic knowledge neurons that
store factual knowledge in a form that transcends language barriers. However,
the preliminary finding suffers from two limitations: 1) High Uncertainty in
Localization Results. Existing study only uses a prompt-based probe to localize
knowledge neurons for each fact, while LLMs cannot provide consistent answers
for semantically equivalent queries. Thus, it leads to inaccurate localization
results with high uncertainty. 2) Lack of Analysis in More Languages. The study
only analyzes language-agnostic knowledge neurons on English and Chinese data,
without exploring more language families and languages. Naturally, it limits
the generalizability of the findings. To address aforementioned problems, we
first construct a new benchmark called Rephrased Multilingual LAMA (RML-LAMA),
which contains high-quality cloze-style multilingual parallel queries for each
fact. Then, we propose a novel method named Multilingual Integrated Gradients
with Uncertainty Estimation (MATRICE), which quantifies the uncertainty across
queries and languages during knowledge localization. Extensive experiments show
that our method can accurately localize language-agnostic knowledge neurons. We
also further investigate the role of language-agnostic knowledge neurons in
cross-lingual knowledge editing, knowledge enhancement and new knowledge
injection.",2024-11-26,"Pengfei Cao, Yuheng Chen, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao",http://arxiv.org/pdf/2411.17401v1,cs.CL
Can LLMs be Good Graph Judge for Knowledge Graph Construction?,"In real-world scenarios, most of the data obtained from the information
retrieval (IR) system is unstructured. Converting natural language sentences
into structured Knowledge Graphs (KGs) remains a critical challenge. We
identified three limitations with respect to existing KG construction methods:
(1) There could be a large amount of noise in real-world documents, which could
result in extracting messy information. (2) Naive LLMs usually extract
inaccurate knowledge from some domain-specific documents. (3) Hallucination
phenomenon cannot be overlooked when directly using LLMs to construct KGs. In
this paper, we propose \textbf{GraphJudge}, a KG construction framework to
address the aforementioned challenges. In this framework, we designed an
entity-centric strategy to eliminate the noise information in the documents.
And we fine-tuned a LLM as a graph judge to finally enhance the quality of
generated KGs. Experiments conducted on two general and one domain-specific
text-graph pair datasets demonstrate state-of-the-art performance against
various baseline methods with strong generalization abilities. Our code is
available at
\href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.",2024-11-26,"Haoyu Huang, Chong Chen, Zeang Sheng, Yang Li, Wentao Zhang",http://arxiv.org/pdf/2411.17388v3,cs.CL
The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs in LLM Generations,"Across all fields of academic study, experts cite their sources when sharing
information. While large language models (LLMs) excel at synthesizing
information, they do not provide reliable citation to sources, making it
difficult to trace and verify the origins of the information they present. In
contrast, search engines make sources readily accessible to users and place the
burden of synthesizing information on the user. Through a survey, we find that
users prefer search engines over LLMs for high-stakes queries, where concerns
regarding information provenance outweigh the perceived utility of LLM
responses. To examine the interplay between verifiability and utility of
information-sharing tools, we introduce the extractive-abstractive spectrum, in
which search engines and LLMs are extreme endpoints encapsulating multiple
unexplored intermediate operating points. Search engines are extractive because
they respond to queries with snippets of sources with links (citations) to the
original webpages. LLMs are abstractive because they address queries with
answers that synthesize and logically transform relevant information from
training and in-context sources without reliable citation. We define five
operating points that span the extractive-abstractive spectrum and conduct
human evaluations on seven systems across four diverse query distributions that
reflect real-world QA settings: web search, language simplification, multi-step
reasoning, and medical advice. As outputs become more abstractive, we find that
perceived utility improves by as much as 200%, while the proportion of properly
cited sentences decreases by as much as 50% and users take up to 3 times as
long to verify cited information. Our findings recommend distinct operating
points for domain-specific LLM systems and our failure analysis informs
approaches to high-utility LLM systems that empower users to verify
information.",2024-11-26,"Theodora Worledge, Tatsunori Hashimoto, Carlos Guestrin",http://arxiv.org/pdf/2411.17375v1,cs.CL
Fairness And Performance In Harmony: Data Debiasing Is All You Need,"Fairness in both machine learning (ML) predictions and human decisions is
critical, with ML models prone to algorithmic and data bias, and human
decisions affected by subjectivity and cognitive bias. This study investigates
fairness using a real-world university admission dataset with 870 profiles,
leveraging three ML models, namely XGB, Bi-LSTM, and KNN. Textual features are
encoded with BERT embeddings. For individual fairness, we assess decision
consistency among experts with varied backgrounds and ML models, using a
consistency score. Results show ML models outperform humans in fairness by
14.08% to 18.79%. For group fairness, we propose a gender-debiasing pipeline
and demonstrate its efficacy in removing gender-specific language without
compromising prediction performance. Post-debiasing, all models maintain or
improve their classification accuracy, validating the hypothesis that fairness
and performance can coexist. Our findings highlight ML's potential to enhance
fairness in admissions while maintaining high accuracy, advocating a hybrid
approach combining human judgement and ML models.",2024-11-26,"Junhua Liu, Wendy Wan Yee Hui, Roy Ka-Wei Lee, Kwan Hui Lim",http://arxiv.org/pdf/2411.17374v1,cs.CL
Different Bias Under Different Criteria: Assessing Bias in LLMs with a Fact-Based Approach,"Large language models (LLMs) often reflect real-world biases, leading to
efforts to mitigate these effects and make the models unbiased. Achieving this
goal requires defining clear criteria for an unbiased state, with any deviation
from these criteria considered biased. Some studies define an unbiased state as
equal treatment across diverse demographic groups, aiming for balanced outputs
from LLMs. However, differing perspectives on equality and the importance of
pluralism make it challenging to establish a universal standard. Alternatively,
other approaches propose using fact-based criteria for more consistent and
objective evaluations, though these methods have not yet been fully applied to
LLM bias assessments. Thus, there is a need for a metric with objective
criteria that offers a distinct perspective from equality-based approaches.
Motivated by this need, we introduce a novel metric to assess bias using
fact-based criteria and real-world statistics. In this paper, we conducted a
human survey demonstrating that humans tend to perceive LLM outputs more
positively when they align closely with real-world demographic distributions.
Evaluating various LLMs with our proposed metric reveals that model bias varies
depending on the criteria used, highlighting the need for multi-perspective
assessment.",2024-11-26,"Changgeon Ko, Jisu Shin, Hoyun Song, Jeongyeon Seo, Jong C. Park",http://arxiv.org/pdf/2411.17338v1,cs.CL
Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning,"This paper introduces a novel method, referred to as ""hashing"", which
involves masking potentially bias-inducing words in large language models
(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and
reliance on external knowledge. The method was tested across three sets of
experiments involving a total of 490 prompts. Statistical analysis using
chi-square tests showed significant improvements in all tested scenarios, which
covered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first
experiment, hashing decreased the fallacy rate in a modified version of the
""Linda"" problem aimed at evaluating susceptibility to cognitive biases. In the
second experiment, it improved LLM results on the frequent itemset extraction
task. In the third experiment, we found hashing is also effective when the
Linda problem is presented in a tabular format rather than text, indicating
that the technique works across various input representations. Overall, the
method was shown to improve bias reduction and incorporation of external
knowledge. Despite bias reduction, hallucination rates were inconsistently
reduced across types of LLM models. These findings suggest that masking
bias-inducing terms can improve LLM performance, although its effectiveness is
model- and task-dependent.",2024-11-26,"Milena Chadimová, Eduard Jurášek, Tomáš Kliegr",http://arxiv.org/pdf/2411.17304v1,cs.CL
ReFINE: A Reward-Based Framework for Interpretable and Nuanced Evaluation of Radiology Report Generation,"Automated radiology report generation (R2Gen) has advanced significantly,
introducing challenges in accurate evaluation due to its complexity.
Traditional metrics often fall short by relying on rigid word-matching or
focusing only on pathological entities, leading to inconsistencies with human
assessments. To bridge this gap, we introduce ReFINE, an automatic evaluation
metric designed specifically for R2Gen. Our metric utilizes a reward model,
guided by our margin-based reward enforcement loss, along with a tailored
training data design that enables customization of evaluation criteria to suit
user-defined needs. It not only scores reports according to user-specified
criteria but also provides detailed sub-scores, enhancing interpretability and
allowing users to adjust the criteria between different aspects of reports.
Leveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling
us to produce extensive training data based on two distinct scoring systems,
each containing reports of varying quality along with corresponding scores.
These GPT-generated reports are then paired as accepted and rejected samples
through our pairing rule to train an LLM towards our fine-grained reward model,
which assigns higher rewards to the report with high quality. Our
reward-control loss enables this model to simultaneously output multiple
individual rewards corresponding to the number of evaluation criteria, with
their summation as our final ReFINE. Our experiments demonstrate ReFINE's
heightened correlation with human judgments and superior performance in model
selection compared to traditional metrics. Notably, our model provides both an
overall score and individual scores for each evaluation item, enhancing
interpretability. We also demonstrate its flexible training across various
evaluation systems.",2024-11-26,"Yunyi Liu, Yingshu Li, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Luping Zhou",http://arxiv.org/pdf/2411.17301v2,cs.CL
2D Matryoshka Training for Information Retrieval,"2D Matryoshka Training is an advanced embedding representation training
approach designed to train an encoder model simultaneously across various
layer-dimension setups. This method has demonstrated higher effectiveness in
Semantic Text Similarity (STS) tasks over traditional training approaches when
using sub-layers for embeddings. Despite its success, discrepancies exist
between two published implementations, leading to varied comparative results
with baseline models. In this reproducibility study, we implement and evaluate
both versions of 2D Matryoshka Training on STS tasks and extend our analysis to
retrieval tasks. Our findings indicate that while both versions achieve higher
effectiveness than traditional Matryoshka training on sub-dimensions, and
traditional full-sized model training approaches, they do not outperform models
trained separately on specific sub-layer and sub-dimension setups. Moreover,
these results generalize well to retrieval tasks, both in supervised (MSMARCO)
and zero-shot (BEIR) settings. Further explorations of different loss
computations reveals more suitable implementations for retrieval tasks, such as
incorporating full-dimension loss and training on a broader range of target
dimensions. Conversely, some intuitive approaches, such as fixing document
encoders to full model outputs, do not yield improvements. Our reproduction
code is available at https://github.com/ielab/2DMSE-Reproduce.",2024-11-26,"Shuai Wang, Shengyao Zhuang, Bevan Koopman, Guido Zuccon",http://arxiv.org/pdf/2411.17299v1,cs.CL
AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling,"Large language models (LLMs) acquire a breadth of information across various
domains. However, their computational complexity, cost, and lack of
transparency often hinder their direct application for predictive tasks where
privacy and interpretability are paramount. In fields such as healthcare,
biology, and finance, specialised and interpretable linear models still hold
considerable value. In such domains, labelled data may be scarce or expensive
to obtain. Well-specified prior distributions over model parameters can reduce
the sample complexity of learning through Bayesian inference; however,
eliciting expert priors can be time-consuming. We therefore introduce
AutoElicit to extract knowledge from LLMs and construct priors for predictive
models. We show these priors are informative and can be refined using natural
language. We perform a careful study contrasting AutoElicit with in-context
learning and demonstrate how to perform model selection between the two
methods. We find that AutoElicit yields priors that can substantially reduce
error over uninformative priors, using fewer labels, and consistently
outperform in-context learning. We show that AutoElicit saves over 6 months of
labelling effort when building a new predictive model for urinary tract
infections from sensor recordings of people living with dementia.",2024-11-26,"Alexander Capstick, Rahul G. Krishnan, Payam Barnaghi",http://arxiv.org/pdf/2411.17284v4,cs.CL
An Attempt to Develop a Neural Parser based on Simplified Head-Driven Phrase Structure Grammar on Vietnamese,"In this paper, we aimed to develop a neural parser for Vietnamese based on
simplified Head-Driven Phrase Structure Grammar (HPSG). The existing corpora,
VietTreebank and VnDT, had around 15% of constituency and dependency tree pairs
that did not adhere to simplified HPSG rules. To attempt to address the issue
of the corpora not adhering to simplified HPSG rules, we randomly permuted
samples from the training and development sets to make them compliant with
simplified HPSG. We then modified the first simplified HPSG Neural Parser for
the Penn Treebank by replacing it with the PhoBERT or XLM-RoBERTa models, which
can encode Vietnamese texts. We conducted experiments on our modified
VietTreebank and VnDT corpora. Our extensive experiments showed that the
simplified HPSG Neural Parser achieved a new state-of-the-art F-score of 82%
for constituency parsing when using the same predicted part-of-speech (POS)
tags as the self-attentive constituency parser. Additionally, it outperformed
previous studies in dependency parsing with a higher Unlabeled Attachment Score
(UAS). However, our parser obtained lower Labeled Attachment Score (LAS) scores
likely due to our focus on arc permutation without changing the original
labels, as we did not consult with a linguistic expert. Lastly, the research
findings of this paper suggest that simplified HPSG should be given more
attention to linguistic expert when developing treebanks for Vietnamese natural
language processing.",2024-11-26,"Duc-Vu Nguyen, Thang Chau Phan, Quoc-Nam Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen",http://arxiv.org/pdf/2411.17270v2,cs.CL
A Topic-level Self-Correctional Approach to Mitigate Hallucinations in MLLMs,"Aligning the behaviors of Multimodal Large Language Models (MLLMs) with human
preferences is crucial for developing robust and trustworthy AI systems. While
recent attempts have employed human experts or powerful auxiliary AI systems to
provide more accurate preference feedback, such as determining the preferable
responses from MLLMs or directly rewriting hallucination-free responses,
extensive resource overhead compromise the scalability of the feedback
collection. In this work, we introduce Topic-level Preference Overwriting
(TPO), a self-correctional approach that guide the model itself to mitigate its
own hallucination at the topic level. Through a deconfounded strategy that
replaces each topic within the response with the best or worst alternatives
generated by the model itself, TPO creates more contrasting pairwise preference
feedback, enhancing the feedback quality without human or proprietary model
intervention. Notably, the experimental results demonstrate proposed TPO
achieves state-of-the-art performance in trustworthiness, significantly
reducing the object hallucinations by 92% and overall hallucinations by 38%.
Code, model and dataset are available now.",2024-11-26,"Lehan He, Zeren Chen, Zhelun Shi, Tianyu Yu, Jing Shao, Lu Sheng",http://arxiv.org/pdf/2411.17265v2,cs.CL
Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models,"Transformer-based large-scale pre-trained models achieve great success.
Fine-tuning is the standard practice for leveraging these models in downstream
tasks. Among the fine-tuning methods, adapter-tuning provides a
parameter-efficient fine-tuning by introducing lightweight trainable modules
while keeping most pre-trained parameters frozen. However, existing
adapter-tuning methods still impose substantial resource usage. Through our
investigation, we show that each adapter unequally contributes to both task
performance and resource usage. Motivated by this insight, we propose Selective
Adapter FrEezing (SAFE), which gradually freezes less important adapters early
to reduce unnecessary resource usage while maintaining performance. In our
experiments, SAFE reduces memory usage, computation amount, and training time
by 42.85\%, 34.59\%, and 11.82\%, respectively, while achieving comparable or
better task performance compared to the baseline. We also demonstrate that SAFE
induces regularization effect, thereby smoothing the loss landscape, which
enables the model to generalize better by avoiding sharp minima.",2024-11-26,"Hyegang Son, Yonglak Son, Changhoon Kim, Young Geun Kim",http://arxiv.org/pdf/2412.03587v2,cs.CL
Strategic Prompting for Conversational Tasks: A Comparative Analysis of Large Language Models Across Diverse Conversational Tasks,"Given the advancements in conversational artificial intelligence, the
evaluation and assessment of Large Language Models (LLMs) play a crucial role
in ensuring optimal performance across various conversational tasks. In this
paper, we present a comprehensive study that thoroughly evaluates the
capabilities and limitations of five prevalent LLMs: Llama, OPT, Falcon,
Alpaca, and MPT. The study encompasses various conversational tasks, including
reservation, empathetic response generation, mental health and legal
counseling, persuasion, and negotiation. To conduct the evaluation, an
extensive test setup is employed, utilizing multiple evaluation criteria that
span from automatic to human evaluation. This includes using generic and
task-specific metrics to gauge the LMs' performance accurately. From our
evaluation, no single model emerges as universally optimal for all tasks.
Instead, their performance varies significantly depending on the specific
requirements of each task. While some models excel in certain tasks, they may
demonstrate comparatively poorer performance in others. These findings
emphasize the importance of considering task-specific requirements and
characteristics when selecting the most suitable LM for conversational
applications.",2024-11-26,"Ratnesh Kumar Joshi, Priyanshu Priya, Vishesh Desai, Saurav Dudhate, Siddhant Senapati, Asif Ekbal, Roshni Ramnani, Anutosh Maitra, Shubhashis Sengupta",http://arxiv.org/pdf/2411.17204v2,cs.CL
Socio-Emotional Response Generation: A Human Evaluation Protocol for LLM-Based Conversational Systems,"Conversational systems are now capable of producing impressive and generally
relevant responses. However, we have no visibility nor control of the
socio-emotional strategies behind state-of-the-art Large Language Models
(LLMs), which poses a problem in terms of their transparency and thus their
trustworthiness for critical applications. Another issue is that current
automated metrics are not able to properly evaluate the quality of generated
responses beyond the dataset's ground truth. In this paper, we propose a neural
architecture that includes an intermediate step in planning socio-emotional
strategies before response generation. We compare the performance of
open-source baseline LLMs to the outputs of these same models augmented with
our planning module. We also contrast the outputs obtained from automated
metrics and evaluation results provided by human annotators. We describe a
novel evaluation protocol that includes a coarse-grained consistency
evaluation, as well as a finer-grained annotation of the responses on various
social and emotional criteria. Our study shows that predicting a sequence of
expected strategy labels and using this sequence to generate a response yields
better results than a direct end-to-end generation scheme. It also highlights
the divergences and the limits of current evaluation metrics for generated
content. The code for the annotation platform and the annotated data are made
publicly available for the evaluation of future models.",2024-11-26,"Lorraine Vanel, Ariel R. Ramos Vela, Alya Yacoubi, Chloé Clavel",http://arxiv.org/pdf/2412.04492v1,cs.CL
Interleaved Scene Graphs for Interleaved Text-and-Image Generation Assessment,"Many real-world user queries (e.g. ""How do to make egg fried rice?"") could
benefit from systems capable of generating responses with both textual steps
with accompanying images, similar to a cookbook. Models designed to generate
interleaved text and images face challenges in ensuring consistency within and
across these modalities. To address these challenges, we present ISG, a
comprehensive evaluation framework for interleaved text-and-image generation.
ISG leverages a scene graph structure to capture relationships between text and
image blocks, evaluating responses on four levels of granularity: holistic,
structural, block-level, and image-specific. This multi-tiered evaluation
allows for a nuanced assessment of consistency, coherence, and accuracy, and
provides interpretable question-answer feedback. In conjunction with ISG, we
introduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8
categories and 21 subcategories. This benchmark dataset includes complex
language-vision dependencies and golden answers to evaluate models effectively
on vision-centric tasks such as style transfer, a challenging area for current
models. Using ISG-Bench, we demonstrate that recent unified vision-language
models perform poorly on generating interleaved content. While compositional
approaches that combine separate language and image models show a 111%
improvement over unified models at the holistic level, their performance
remains suboptimal at both block and image levels. To facilitate future work,
we develop ISG-Agent, a baseline agent employing a ""plan-execute-refine""
pipeline to invoke tools, achieving a 122% performance improvement.",2024-11-26,"Dongping Chen, Ruoxi Chen, Shu Pu, Zhaoyi Liu, Yanru Wu, Caixi Chen, Benlin Liu, Yue Huang, Yao Wan, Pan Zhou, Ranjay Krishna",http://arxiv.org/pdf/2411.17188v2,cs.CL
A Novel Word Pair-based Gaussian Sentence Similarity Algorithm For Bengali Extractive Text Summarization,"Extractive Text Summarization is the process of selecting the most
representative parts of a larger text without losing any key information.
Recent attempts at extractive text summarization in Bengali, either relied on
statistical techniques like TF-IDF or used naive sentence similarity measures
like the word averaging technique. All of these strategies suffer from
expressing semantic relationships correctly. Here, we propose a novel Word
pair-based Gaussian Sentence Similarity (WGSS) algorithm for calculating the
semantic relation between two sentences. WGSS takes the geometric means of
individual Gaussian similarity values of word embedding vectors to get the
semantic relationship between sentences. It compares two sentences on a
word-to-word basis which rectifies the sentence representation problem faced by
the word averaging method. The summarization process extracts key sentences by
grouping semantically similar sentences into clusters using the Spectral
Clustering algorithm. After clustering, we use TF-IDF ranking to pick the best
sentence from each cluster. The proposed method is validated using four
different datasets, and it outperformed other recent models by 43.2% on average
ROUGE scores (ranging from 2.5% to 95.4%). It is also experimented on other
low-resource languages i.e. Turkish, Marathi, and Hindi language, where we find
that the proposed method performs as similar as Bengali for these languages. In
addition, a new high-quality Bengali dataset is curated which contains 250
articles and a pair of summaries for each of them. We believe this research is
a crucial addition to Bengali Natural Language Processing (NLP) research and it
can easily be extended into other low-resource languages. We made the
implementation of the proposed model and data public on
https://github.com/FMOpee/WGSS.",2024-11-26,"Fahim Morshed, Md. Abdur Rahman, Sumon Ahmed",http://arxiv.org/pdf/2411.17181v2,cs.CL
Learning Monotonic Attention in Transducer for Streaming Generation,"Streaming generation models are increasingly utilized across various fields,
with the Transducer architecture being particularly popular in industrial
applications. However, its input-synchronous decoding mechanism presents
challenges in tasks requiring non-monotonic alignments, such as simultaneous
translation, leading to suboptimal performance in these contexts. In this
research, we address this issue by tightly integrating Transducer's decoding
with the history of input stream via a learnable monotonic attention mechanism.
Our approach leverages the forward-backward algorithm to infer the posterior
probability of alignments between the predictor states and input timestamps,
which is then used to estimate the context representations of monotonic
attention in training. This allows Transducer models to adaptively adjust the
scope of attention based on their predictions, avoiding the need to enumerate
the exponentially large alignment space. Extensive experiments demonstrate that
our MonoAttn-Transducer significantly enhances the handling of non-monotonic
alignments in streaming generation, offering a robust solution for
Transducer-based frameworks to tackle more complex streaming generation tasks.",2024-11-26,"Zhengrui Ma, Yang Feng, Min Zhang",http://arxiv.org/pdf/2411.17170v1,cs.CL
Safe to Serve: Aligning Instruction-Tuned Models for Safety and Helpfulness,"Large language models (LLMs) have demonstrated remarkable capabilities in
complex reasoning and text generation. However, these models can inadvertently
generate unsafe or biased responses when prompted with problematic inputs,
raising significant ethical and practical concerns for real-world deployment.
This research addresses the critical challenge of developing language models
that generate both helpful and harmless content, navigating the delicate
balance between model performance and safety. We demonstrate that incorporating
safety-related instructions during the instruction-tuning of pre-trained models
significantly reduces toxic responses to unsafe prompts without compromising
performance on helpfulness datasets. We found Direct Preference Optimization
(DPO) to be particularly effective, outperforming both SIT and RAFT by
leveraging both chosen and rejected responses for learning. Our approach
increased safe responses from 40$\%$ to over 90$\%$ across various harmfulness
benchmarks. In addition, we discuss a rigorous evaluation framework
encompassing specialized metrics and diverse datasets for safety and
helpfulness tasks ensuring a comprehensive assessment of the model's
capabilities.",2024-11-26,"Avinash Amballa, Durga Sandeep Saluru, Gayathri Akkinapalli, Abhishek Sureddy, Akshay Kumar Sureddy",http://arxiv.org/pdf/2412.00074v1,cs.CL
Enhancing Code-Switching ASR Leveraging Non-Peaky CTC Loss and Deep Language Posterior Injection,"Code-switching-where multilingual speakers alternately switch between
languages during conversations-still poses significant challenges to end-to-end
(E2E) automatic speech recognition (ASR) systems due to phenomena of both
acoustic and semantic confusion. This issue arises because ASR systems struggle
to handle the rapid alternation of languages effectively, which often leads to
significant performance degradation. Our main contributions are at least
threefold: First, we incorporate language identification (LID) information into
several intermediate layers of the encoder, aiming to enrich output embeddings
with more detailed language information. Secondly, through the novel
application of language boundary alignment loss, the subsequent ASR modules are
enabled to more effectively utilize the knowledge of internal language
posteriors. Third, we explore the feasibility of using language posteriors to
facilitate deep interaction between shared encoder and language-specific
encoders. Through comprehensive experiments on the SEAME corpus, we have
verified that our proposed method outperforms the prior-art method, disentangle
based mixture-of-experts (D-MoE), further enhancing the acuity of the encoder
to languages.",2024-11-26,"Tzu-Ting Yang, Hsin-Wei Wang, Yi-Cheng Wang, Berlin Chen",http://arxiv.org/pdf/2412.08651v1,cs.CL
LLM-Based Offline Learning for Embodied Agents via Consistency-Guided Reward Ensemble,"Employing large language models (LLMs) to enable embodied agents has become
popular, yet it presents several limitations in practice. In this work, rather
than using LLMs directly as agents, we explore their use as tools for embodied
agent learning. Specifically, to train separate agents via offline
reinforcement learning (RL), an LLM is used to provide dense reward feedback on
individual actions in training datasets. In doing so, we present a
consistency-guided reward ensemble framework (CoREN), designed for tackling
difficulties in grounding LLM-generated estimates to the target environment
domain. The framework employs an adaptive ensemble of spatio-temporally
consistent rewards to derive domain-grounded rewards in the training datasets,
thus enabling effective offline learning of embodied agents in different
environment domains. Experiments with the VirtualHome benchmark demonstrate
that CoREN significantly outperforms other offline RL agents, and it also
achieves comparable performance to state-of-the-art LLM-based agents with 8B
parameters, despite CoREN having only 117M parameters for the agent policy
network and using LLMs only for training.",2024-11-26,"Yujeong Lee, Sangwoo Shin, Wei-Jin Park, Honguk Woo",http://arxiv.org/pdf/2411.17135v1,cs.CL
Star Attention: Efficient LLM Inference over Long Sequences,"Inference with Transformer-based Large Language Models (LLMs) on long
sequences is both costly and slow due to the quadratic complexity of the
self-attention mechanism. We introduce Star Attention, a two-phase block-sparse
approximation that improves computational efficiency by sharding attention
across multiple hosts while minimizing communication overhead. In the first
phase, the context is processed using blockwise-local attention across hosts,
in parallel. In the second phase, query and response tokens attend to all prior
cached tokens through sequence-global attention. Star Attention integrates
seamlessly with most Transformer-based LLMs trained with global attention,
reducing memory requirements and inference time by up to 11x while preserving
97-100% of accuracy.",2024-11-26,"Shantanu Acharya, Fei Jia, Boris Ginsburg",http://arxiv.org/pdf/2411.17116v2,cs.CL
COAP: Memory-Efficient Training with Correlation-Aware Gradient Projection,"Training large-scale neural networks in vision, and multimodal domains
demands substantial memory resources, primarily due to the storage of optimizer
states. While LoRA, a popular parameter-efficient method, reduces memory usage,
it often suffers from suboptimal performance due to the constraints of low-rank
updates. Low-rank gradient projection methods (e.g., GaLore, Flora) reduce
optimizer memory by projecting gradients and moment estimates into low-rank
spaces via singular value decomposition or random projection. However, they
fail to account for inter-projection correlation, causing performance
degradation, and their projection strategies often incur high computational
costs. In this paper, we present COAP (Correlation-Aware Gradient Projection),
a memory-efficient method that minimizes computational overhead while
maintaining training performance. Evaluated across various vision, language,
and multimodal tasks, COAP outperforms existing methods in both training speed
and model performance. For LLaMA-1B, it reduces optimizer memory by 61% with
only 2% additional time cost, achieving the same PPL as AdamW. With 8-bit
quantization, COAP cuts optimizer memory by 81% and achieves 4x speedup over
GaLore for LLaVA-v1.5-7B fine-tuning, while delivering higher accuracy.",2024-11-26,"Jinqi Xiao, Shen Sang, Tiancheng Zhi, Jing Liu, Qing Yan, Yuqian Zhang, Linjie Luo, Bo Yuan",http://arxiv.org/pdf/2412.00071v2,cs.CL
"Don't Command, Cultivate: An Exploratory Study of System-2 Alignment","The o1 system card identifies the o1 models as the most robust within OpenAI,
with their defining characteristic being the progression from rapid, intuitive
thinking to slower, more deliberate reasoning. This observation motivated us to
investigate the influence of System-2 thinking patterns on model safety. In our
preliminary research, we conducted safety evaluations of the o1 model,
including complex jailbreak attack scenarios using adversarial natural language
prompts and mathematical encoding prompts. Our findings indicate that the o1
model demonstrates relatively improved safety performance; however, it still
exhibits vulnerabilities, particularly against jailbreak attacks employing
mathematical encoding. Through detailed case analysis, we identified specific
patterns in the o1 model's responses. We also explored the alignment of
System-2 safety in open-source models using prompt engineering and supervised
fine-tuning techniques. Experimental results show that some simple methods to
encourage the model to carefully scrutinize user requests are beneficial for
model safety. Additionally, we proposed a implementation plan for process
supervision to enhance safety alignment. The implementation details and
experimental results will be provided in future versions.",2024-11-26,"Yuhang Wang, Yuxiang Zhang, Yanxu Zhu, Xinyan Wen, Jitao Sang",http://arxiv.org/pdf/2411.17075v5,cs.CL
"Relations, Negations, and Numbers: Looking for Logic in Generative Text-to-Image Models","Despite remarkable progress in multi-modal AI research, there is a salient
domain in which modern AI continues to lag considerably behind even human
children: the reliable deployment of logical operators. Here, we examine three
forms of logical operators: relations, negations, and discrete numbers. We
asked human respondents (N=178 in total) to evaluate images generated by a
state-of-the-art image-generating AI (DALL-E 3) prompted with these `logical
probes', and find that none reliably produce human agreement scores greater
than 50\%. The negation probes and numbers (beyond 3) fail most frequently. In
a 4th experiment, we assess a `grounded diffusion' pipeline that leverages
targeted prompt engineering and structured intermediate representations for
greater compositional control, but find its performance is judged even worse
than that of DALL-E 3 across prompts. To provide further clarity on potential
sources of success and failure in these text-to-image systems, we supplement
our 4 core experiments with multiple auxiliary analyses and schematic diagrams,
directly quantifying, for example, the relationship between the N-gram
frequency of relational prompts and the average match to generated images; the
success rates for 3 different prompt modification strategies in the rendering
of negation prompts; and the scalar variability / ratio dependence
(`approximate numeracy') of prompts involving integers. We conclude by
discussing the limitations inherent to `grounded' multimodal learning systems
whose grounding relies heavily on vector-based semantics (e.g. DALL-E 3), or
under-specified syntactical constraints (e.g. `grounded diffusion'), and
propose minimal modifications (inspired by development, based in imagery) that
could help to bridge the lingering compositional gap between scale and
structure. All data and code is available at
https://github.com/ColinConwell/T2I-Probology",2024-11-26,"Colin Conwell, Rupert Tawiah-Quashie, Tomer Ullman",http://arxiv.org/pdf/2411.17066v1,cs.CL
"Condense, Don't Just Prune: Enhancing Efficiency and Performance in MoE Layer Pruning","Mixture-of-Experts (MoE) has garnered significant attention for its ability
to scale up neural networks while utilizing the same or even fewer active
parameters. However, MoE does not alleviate the massive memory requirements of
networks, which limits their practicality in real-world applications,
especially in the era of large language models (LLMs). While recent work
explores the possibility of removing entire layers of MoE to reduce memory, the
performance degradation is still notable. In this paper, we propose
ConDense-MoE (CD-MoE), which, instead of dropping the entire MoE layer,
condenses the large, sparse MoE layer into a smaller, denser layer with only a
few experts activated for all tokens, while maintaining hardware friendliness.
Our approach is specifically designed for fine-grained MoE with shared experts,
where Feed-Forward Networks are split into many small experts, with certain
experts isolated to serve as shared experts that are always activated, such as
DeepSeekMoE and QwenMoE. We demonstrate the effectiveness of our method.
Specifically, for the DeepSeekMoE-16B model, our approach maintains 90% of the
average accuracy while reducing memory usage by 27.5% and increasing inference
speed by 1.26 times. Moreover, we show that by applying lightweight expert
fine-tuning -- only to the condensed layers -- and using 5 hours on a single
80G A100 GPU, we can successfully recover 98% of the original performance. Our
code is available at: https://github.com/duterscmy/CD-MoE/tree/main.",2024-11-26,"Mingyu Cao, Gen Li, Jie Ji, Jiaqi Zhang, Xiaolong Ma, Shiwei Liu, Lu Yin",http://arxiv.org/pdf/2412.00069v2,cs.CL
Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach,"Self-improvement in multimodal large language models (MLLMs) is crucial for
enhancing their reliability and robustness. However, current methods often rely
heavily on MLLMs themselves as judges, leading to high computational costs and
potential pitfalls like reward hacking and model collapse. This paper
introduces a novel, model-level judge-free self-improvement framework. Our
approach employs a controlled feedback mechanism while eliminating the need for
MLLMs in the verification loop. We generate preference learning pairs using a
controllable hallucination mechanism and optimize data quality by leveraging
lightweight, contrastive language-image encoders to evaluate and reverse pairs
when necessary. Evaluations across public benchmarks and our newly introduced
IC dataset designed to challenge hallucination control demonstrate that our
model outperforms conventional techniques. We achieve superior precision and
recall with significantly lower computational demands. This method offers an
efficient pathway to scalable self-improvement in MLLMs, balancing performance
gains with reduced resource requirements.",2024-11-26,"Shijian Deng, Wentian Zhao, Yu-Jhe Li, Kun Wan, Daniel Miranda, Ajinkya Kale, Yapeng Tian",http://arxiv.org/pdf/2411.17760v1,cs.CL
Tree Transformers are an Ineffective Model of Syntactic Constituency,"Linguists have long held that a key aspect of natural language syntax is the
recursive organization of language units into constituent structures, and
research has suggested that current state-of-the-art language models lack an
inherent bias towards this feature. A number of alternative models have been
proposed to provide inductive biases towards constituency, including the Tree
Transformer, which utilizes a modified attention mechanism to organize tokens
into constituents.
  We investigate Tree Transformers to study whether they utilize meaningful
and/or useful constituent structures. We pretrain a large Tree Transformer on
language modeling in order to investigate the learned constituent tree
representations of sentences, finding little evidence for meaningful
structures. Next, we evaluate Tree Transformers with similar transformer models
on error detection tasks requiring constituent structure. We find that while
the Tree Transformer models may slightly outperform at these tasks, there is
little evidence to suggest a meaningful improvement. In general, we conclude
that there is little evidence to support Tree Transformer as an effective model
of syntactic constituency.",2024-11-25,Michael Ginn,http://arxiv.org/pdf/2411.16993v1,cs.CL
Dynamic Self-Distillation via Previous Mini-batches for Fine-tuning Small Language Models,"Knowledge distillation (KD) has become a widely adopted approach for
compressing large language models (LLMs) to reduce computational costs and
memory footprints. However, the availability of complex teacher models is a
prerequisite for running most KD pipelines. Thus, the traditional KD procedure
can be unachievable or budget-unfriendly, particularly when relying on
commercial LLMs like GPT4. In this regard, Self-distillation (SelfD) emerges as
an advisable alternative, enabling student models to learn without teachers'
guidance. Nonetheless, existing SelfD approaches for LMs often involve
architectural modifications, assuming the models are open-source, which may not
always be practical. In this work, we introduce a model-agnostic and
task-agnostic method named dynamic SelfD from the previous minibatch (DynSDPB),
which realizes current iterations' distillation from the last ones' generated
logits. Additionally, to address prediction inaccuracies during the early
iterations, we dynamically adjust the distillation influence and temperature
values to enhance the adaptability of fine-tuning. Furthermore, DynSDPB is a
novel fine-tuning policy that facilitates the seamless integration of existing
self-correction and self-training techniques for small language models (SLMs)
because they all require updating SLMs' parameters. We demonstrate the superior
performance of DynSDPB on both encoder-only LMs (e.g., BERT model families) and
decoder-only LMs (e.g., LLaMA model families), validating its effectiveness
across natural language understanding (NLU) and natural language generation
(NLG) benchmarks.",2024-11-25,"Yao Fu, Yin Yu, Xiaotian Han, Runchao Li, Xianxuan Long, Haotian Yu, Pan Li",http://arxiv.org/pdf/2411.16991v1,cs.CL
Teaching Smaller Language Models To Generalise To Unseen Compositional Questions (Full Thesis),"Pretrained large Language Models (LLMs) are able to answer questions that are
unlikely to have been encountered during training. However a diversity of
potential applications exist in the broad domain of reasoning systems and
considerations such as latency, cost, available compute resource and internet
connectivity are relevant in determining an appropriate approach. We consider
the setting where some local compute capacity is available at inference time
but internet connectivity is not.
  Similar to a general-purpose LLM, we assume that our much smaller Reasoning
Models may be asked arbitrary questions from unknown distributions, so we focus
on evaluation in an unseen setting. We train our models to answer diverse
questions by instilling an ability to reason over a retrieved context. We
acquire context from two knowledge sources; a Wikipedia corpus queried using a
multi-hop dense retrieval system with novel extensions, and from rationales
generated from a larger Language Model optimised to run in a lower resource
environment.
  Our main contributions: We propose novel methods to show that our model is
capable of answering contextualised questions without memorisation. We
establish a comprehensive set of baseline results on unseen evaluation
datasets. We show that the addition of novel retrieval-augmented training
datasets (RATD) to the training regime of the Reasoning Model significantly
improves results. We demonstrate further significant improvement through the
application of methods for combining knowledge from two sources. The first
method (RR) involves training a novel Rationale Ranking model to score both
generated rationales and retrieved contexts with respect to relevance and
truthfulness. We use the scores to derive combined contexts. We also show that
utilising the RATD datasets enables our model to become proficient at utilising
combined noisy contexts.",2024-11-25,Tim Hartill,http://arxiv.org/pdf/2411.16985v1,cs.CL
Harnessing LLMs for Educational Content-Driven Italian Crossword Generation,"In this work, we unveil a novel tool for generating Italian crossword puzzles
from text, utilizing advanced language models such as GPT-4o,
Mistral-7B-Instruct-v0.3, and Llama3-8b-Instruct. Crafted specifically for
educational applications, this cutting-edge generator makes use of the
comprehensive Italian-Clue-Instruct dataset, which comprises over 30,000
entries including diverse text, solutions, and types of clues. This carefully
assembled dataset is designed to facilitate the creation of contextually
relevant clues in various styles associated with specific texts and keywords.
The study delves into four distinctive styles of crossword clues: those without
format constraints, those formed as definite determiner phrases, copular
sentences, and bare noun phrases. Each style introduces unique linguistic
structures to diversify clue presentation. Given the lack of sophisticated
educational tools tailored to the Italian language, this project seeks to
enhance learning experiences and cognitive development through an engaging,
interactive platform. By meshing state-of-the-art AI with contemporary
educational strategies, our tool can dynamically generate crossword puzzles
from Italian educational materials, thereby providing an enjoyable and
interactive learning environment. This technological advancement not only
redefines educational paradigms but also sets a new benchmark for interactive
and cognitive language learning solutions.",2024-11-25,"Kamyar Zeinalipour, Achille Fusco, Asya Zanollo, Marco Maggini, Marco Gori",http://arxiv.org/pdf/2411.16936v1,cs.CL
Boundless Socratic Learning with Language Games,"An agent trained within a closed system can master any desired capability, as
long as the following three conditions hold: (a) it receives sufficiently
informative and aligned feedback, (b) its coverage of experience/data is broad
enough, and (c) it has sufficient capacity and resource. In this position
paper, we justify these conditions, and consider what limitations arise from
(a) and (b) in closed systems, when assuming that (c) is not a bottleneck.
Considering the special case of agents with matching input and output spaces
(namely, language), we argue that such pure recursive self-improvement, dubbed
""Socratic learning"", can boost performance vastly beyond what is present in its
initial data or knowledge, and is only limited by time, as well as gradual
misalignment concerns. Furthermore, we propose a constructive framework to
implement it, based on the notion of language games.",2024-11-25,Tom Schaul,http://arxiv.org/pdf/2411.16905v1,cs.CL
Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering,"Multimodal LLMs (MLLMs) are the natural extension of large language models to
handle multimodal inputs, combining text and image data. They have recently
garnered attention due to their capability to address complex tasks involving
both modalities. However, their effectiveness is limited to the knowledge
acquired during training, which restricts their practical utility. In this
work, we introduce a novel method to enhance the adaptability of MLLMs by
integrating external knowledge sources. Our proposed model, Reflective LLaVA
(ReflectiVA), utilizes reflective tokens to dynamically determine the need for
external knowledge and predict the relevance of information retrieved from an
external database. Tokens are trained following a two-stage two-model training
recipe. This ultimately enables the MLLM to manage external knowledge while
preserving fluency and performance on tasks where external knowledge is not
needed. Through our experiments, we demonstrate the efficacy of ReflectiVA for
knowledge-based visual question answering, highlighting its superior
performance compared to existing methods. Source code and trained models are
publicly available at https://aimagelab.github.io/ReflectiVA.",2024-11-25,"Federico Cocchi, Nicholas Moratelli, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara",http://arxiv.org/pdf/2411.16863v2,cs.CL
Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?,"We evaluate how well Large Language Models (LLMs) latently recall and compose
facts to answer multi-hop queries like ""In the year Scarlett Johansson was
born, the Summer Olympics were hosted in the country of"". One major challenge
in evaluating this ability is that LLMs may have developed shortcuts by
encounters of the head entity ""Scarlett Johansson"" and the answer entity
""United States"" in the same training sequences or merely guess the answer based
on frequency-based priors. To prevent shortcuts, we exclude test queries where
the head and answer entities co-appear in pretraining corpora. Through careful
selection of relations and facts and systematic removal of cases where models
might guess answers or exploit partial matches, we construct an evaluation
dataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs
demonstrate promising latent multi-hop reasoning abilities without exploiting
shortcuts, but only for certain types of queries. For queries requiring latent
recall of countries as the intermediate answer, the best models achieve 80%
latent composability, but this drops to just 5% for the recall of years.
Comparisons with Chain-of-Thought composability highlight a significant gap
between the ability of models to reason latently versus explicitly. Analysis
reveals that latent representations of the intermediate answer are constructed
more often in queries with higher latent composability, and shows the emergence
of latent multi-hop reasoning during pretraining.",2024-11-25,"Sohee Yang, Nora Kassner, Elena Gribovskaya, Sebastian Riedel, Mor Geva",http://arxiv.org/pdf/2411.16679v1,cs.CL
DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation,"Storytelling video generation (SVG) aims to produce coherent and visually
rich multi-scene videos that follow a structured narrative. Existing methods
primarily employ LLM for high-level planning to decompose a story into
scene-level descriptions, which are then independently generated and stitched
together. However, these approaches struggle with generating high-quality
videos aligned with the complex single-scene description, as visualizing such
complex description involves coherent composition of multiple characters and
events, complex motion synthesis and muti-character customization. To address
these challenges, we propose DreamRunner, a novel story-to-video generation
method: First, we structure the input script using a large language model (LLM)
to facilitate both coarse-grained scene planning as well as fine-grained
object-level layout and motion planning. Next, DreamRunner presents
retrieval-augmented test-time adaptation to capture target motion priors for
objects in each scene, supporting diverse motion customization based on
retrieved videos, thus facilitating the generation of new videos with complex,
scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D
attention and prior injection module SR3AI for fine-grained object-motion
binding and frame-by-frame semantic control. We compare DreamRunner with
various SVG baselines, demonstrating state-of-the-art performance in character
consistency, text alignment, and smooth transitions. Additionally, DreamRunner
exhibits strong fine-grained condition-following ability in compositional
text-to-video generation, significantly outperforming baselines on
T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate
multi-object interactions with qualitative examples.",2024-11-25,"Zun Wang, Jialu Li, Han Lin, Jaehong Yoon, Mohit Bansal",http://arxiv.org/pdf/2411.16657v3,cs.CL
Self-Generated Critiques Boost Reward Modeling for Language Models,"Reward modeling is crucial for aligning large language models (LLMs) with
human preferences, especially in reinforcement learning from human feedback
(RLHF). However, current reward models mainly produce scalar scores and
struggle to incorporate critiques in a natural language format. We hypothesize
that predicting both critiques and the scalar reward would improve reward
modeling ability. Motivated by this, we propose Critic-RM, a framework that
improves reward models using self-generated critiques without extra
supervision. Critic-RM employs a two-stage process: generating and filtering
high-quality critiques, followed by joint fine-tuning on reward prediction and
critique generation. Experiments across benchmarks show that Critic-RM improves
reward modeling accuracy by 3.7%-7.3% compared to standard reward models and
LLM judges, demonstrating strong performance and data efficiency. Additional
studies further validate the effectiveness of generated critiques in rectifying
flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.",2024-11-25,"Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou",http://arxiv.org/pdf/2411.16646v3,cs.CL
Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A Cyber Defense Perspective,"Jailbreak prompts pose a significant threat in AI and cybersecurity, as they
are crafted to bypass ethical safeguards in large language models, potentially
enabling misuse by cybercriminals. This paper analyzes jailbreak prompts from a
cyber defense perspective, exploring techniques like prompt injection and
context manipulation that allow harmful content generation, content filter
evasion, and sensitive information extraction. We assess the impact of
successful jailbreaks, from misinformation and automated social engineering to
hazardous content creation, including bioweapons and explosives. To address
these threats, we propose strategies involving advanced prompt analysis,
dynamic safety protocols, and continuous model fine-tuning to strengthen AI
resilience. Additionally, we highlight the need for collaboration among AI
researchers, cybersecurity experts, and policymakers to set standards for
protecting AI systems. Through case studies, we illustrate these cyber defense
approaches, promoting responsible AI practices to maintain system integrity and
public trust. \textbf{\color{red}Warning: This paper contains content which the
reader may find offensive.}",2024-11-25,"Jean Marie Tshimula, Xavier Ndona, D'Jeff K. Nkashama, Pierre-Martin Tardif, Froduald Kabanza, Marc Frappier, Shengrui Wang",http://arxiv.org/pdf/2411.16642v1,cs.CL
Do Automatic Factuality Metrics Measure Factuality? A Critical Evaluation,"Modern LLMs can now produce highly readable abstractive summaries, to the
point where traditional automated metrics for evaluating summary quality, such
as ROUGE, have become saturated. However, LLMs still sometimes introduce
unwanted content into summaries, i.e., information inconsistent with or
unsupported by their source. Measuring the occurrence of these often subtle
``hallucinations'' automatically has proved to be challenging. This in turn has
motivated development of a variety of metrics intended to measure the factual
consistency of generated summaries against their source. But are these
approaches measuring what they purport to do? In this work, we stress-test
automatic factuality metrics. Specifically, we investigate whether and to what
degree superficial attributes of summary texts suffice to predict
``factuality'', finding that a (supervised) model using only such shallow
features is reasonably competitive with SOTA factuality scoring methods. We
then evaluate how factuality metrics respond to factual corrections in
inconsistent summaries and find that only a few show meaningful improvements.
In contrast, some metrics are more sensitive to benign, non-factual edits.
Motivated by these insights, we show that one can ``game'' (most) automatic
factuality metrics, i.e., reliably inflate ``factuality'' scores by appending
innocuous sentences to generated summaries. Taken together, our results raise
questions about the degree to which we should rely on existing automated
factuality metrics and what exactly we want ``factuality metrics'' to measure.",2024-11-25,"Sanjana Ramprasad, Byron C. Wallace",http://arxiv.org/pdf/2411.16638v3,cs.CL
StructFormer: Document Structure-based Masked Attention and its Impact on Language Model Pre-Training,"Most state-of-the-art techniques for Language Models (LMs) today rely on
transformer-based architectures and their ubiquitous attention mechanism.
However, the exponential growth in computational requirements with longer input
sequences confines Transformers to handling short passages. Recent efforts have
aimed to address this limitation by introducing selective attention mechanisms,
notably local and global attention. While sparse attention mechanisms, akin to
full attention in being Turing-complete, have been theoretically established,
their practical impact on pre-training remains unexplored. This study focuses
on empirically assessing the influence of global attention on BERT
pre-training. The primary steps involve creating an extensive corpus of
structure-aware text through arXiv data, alongside a text-only counterpart. We
carry out pre-training on these two datasets, investigate shifts in attention
patterns, and assess their implications for downstream tasks. Our analysis
underscores the significance of incorporating document structure into LM
models, demonstrating their capacity to excel in more abstract tasks, such as
document understanding.",2024-11-25,"Kaustubh Ponkshe, Venkatapathy Subramanian, Natwar Modani, Ganesh Ramakrishnan",http://arxiv.org/pdf/2411.16618v1,cs.CL
Recent Trends in Linear Text Segmentation: a Survey,"Linear Text Segmentation is the task of automatically tagging text documents
with topic shifts, i.e. the places in the text where the topics change. A
well-established area of research in Natural Language Processing, drawing from
well-understood concepts in linguistic and computational linguistic research,
the field has recently seen a lot of interest as a result of the surge of text,
video, and audio available on the web, which in turn require ways of
summarising and categorizing the mole of content for which linear text
segmentation is a fundamental step. In this survey, we provide an extensive
overview of current advances in linear text segmentation, describing the state
of the art in terms of resources and approaches for the task. Finally, we
highlight the limitations of available resources and of the task itself, while
indicating ways forward based on the most recent literature and under-explored
research directions.",2024-11-25,"Iacopo Ghinassi, Lin Wang, Chris Newell, Matthew Purver",http://arxiv.org/pdf/2411.16613v1,cs.CL
From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge,"Assessment and evaluation have long been critical challenges in artificial
intelligence (AI) and natural language processing (NLP). However, traditional
methods, whether matching-based or embedding-based, often fall short of judging
subtle attributes and delivering satisfactory results. Recent advancements in
Large Language Models (LLMs) inspire the ""LLM-as-a-judge"" paradigm, where LLMs
are leveraged to perform scoring, ranking, or selection across various tasks
and applications. This paper provides a comprehensive survey of LLM-based
judgment and assessment, offering an in-depth overview to advance this emerging
field. We begin by giving detailed definitions from both input and output
perspectives. Then we introduce a comprehensive taxonomy to explore
LLM-as-a-judge from three dimensions: what to judge, how to judge and where to
judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and
highlight key challenges and promising directions, aiming to provide valuable
insights and inspire future research in this promising research area. Paper
list and more resources about LLM-as-a-judge can be found at
https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge and
https://llm-as-a-judge.github.io.",2024-11-25,"Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, Huan Liu",http://arxiv.org/pdf/2411.16594v6,cs.CL
KL-geodesics flow matching with a novel sampling scheme,"Non-autoregressive language models generate all tokens simultaneously,
offering potential speed advantages over traditional autoregressive models, but
they face challenges in modeling the complex dependencies inherent in text
data. In this work, we investigate a conditional flow matching approach for
text generation. We represent tokens as one-hot vectors in a \(V\)-dimensional
simplex and utilize geodesics under the Kullback-Leibler (KL) divergence, which
correspond to linear interpolation in logit space. We provide a theoretical
justification that maximizing the conditional likelihood \(P_{\theta}(x_1 \mid
x_t, t)\) yields the exact flow matching velocity under logit interpolation. To
address the suboptimal performance of basic inference, we propose a novel
empirical sampling scheme that iteratively samples from the conditional
distribution and introduces additional noise, significantly improving results
despite lacking full theoretical underpinnings. Furthermore, we propose a
hybrid inference method that combines the basic approach with the sampling
scheme. This method demonstrates superior performance on both conditional and
unconditional text generation experiments compared to previous SOTA method for
discrete flow matching.",2024-11-25,"Egor Sevriugov, Ivan Oseledets",http://arxiv.org/pdf/2411.16821v4,cs.CL
Enhancing LLM Reasoning via Critique Models with Test-Time and Training-Time Supervision,"Training large language models (LLMs) to spend more time thinking and
reflection before responding is crucial for effectively solving complex
reasoning tasks in fields such as science, coding, and mathematics. However,
the effectiveness of mechanisms like self-reflection and self-correction
depends on the model's capacity to accurately assess its own performance, which
can be limited by factors such as initial accuracy, question difficulty, and
the lack of external feedback. In this paper, we delve into a two-player
paradigm that separates the roles of reasoning and critique models, where the
critique model provides step-level feedback to supervise the reasoning (actor)
model during both test-time and train-time. We first propose AutoMathCritique,
an automated and scalable framework for collecting critique data, resulting in
a dataset of $76,321$ responses paired with step-level feedback. Fine-tuning
language models with this dataset enables them to generate natural language
feedback for mathematical reasoning. We demonstrate that the critique models
consistently improve the actor's performance on difficult queries at test-time,
especially when scaling up inference-time computation. Motivated by these
findings, we introduce the critique-based supervision to the actor's
self-training process, and propose a critique-in-the-loop self-improvement
method. Experiments show that the method improves the actor's exploration
efficiency and solution diversity, especially on challenging queries, leading
to a stronger reasoning model. Lastly, we take the preliminary step to explore
training self-talk reasoning models via critique supervision and showcase its
potential. Our code and datasets are at
\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}.",2024-11-25,"Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, Xiao Wang, Rui Zheng, Tao Ji, Xiaowei Shi, Yitao Zhai, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Zuxuan Wu, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Yu-Gang Jiang",http://arxiv.org/pdf/2411.16579v1,cs.CL
EnStack: An Ensemble Stacking Framework of Large Language Models for Enhanced Vulnerability Detection in Source Code,"Automated detection of software vulnerabilities is critical for enhancing
security, yet existing methods often struggle with the complexity and diversity
of modern codebases. In this paper, we introduce EnStack, a novel ensemble
stacking framework that enhances vulnerability detection using natural language
processing (NLP) techniques. Our approach synergizes multiple pre-trained large
language models (LLMs) specialized in code understanding CodeBERT for semantic
analysis, GraphCodeBERT for structural representation, and UniXcoder for
cross-modal capabilities. By fine-tuning these models on the Draper VDISC
dataset and integrating their outputs through meta-classifiers such as Logistic
Regression, Support Vector Machines (SVM), Random Forest, and XGBoost, EnStack
effectively captures intricate code patterns and vulnerabilities that
individual models may overlook. The meta-classifiers consolidate the strengths
of each LLM, resulting in a comprehensive model that excels in detecting subtle
and complex vulnerabilities across diverse programming contexts. Experimental
results demonstrate that EnStack significantly outperforms existing methods,
achieving notable improvements in accuracy, precision, recall, and F1-score.
This work highlights the potential of ensemble LLM approaches in code analysis
tasks and offers valuable insights into applying NLP techniques for advancing
automated vulnerability detection.",2024-11-25,"Shahriyar Zaman Ridoy, Md. Shazzad Hossain Shaon, Alfredo Cuzzocrea, Mst Shapna Akter",http://arxiv.org/pdf/2411.16561v1,cs.CL
Enhancing In-Hospital Mortality Prediction Using Multi-Representational Learning with LLM-Generated Expert Summaries,"In-hospital mortality (IHM) prediction for ICU patients is critical for
timely interventions and efficient resource allocation. While structured
physiological data provides quantitative insights, clinical notes offer
unstructured, context-rich narratives. This study integrates these modalities
with Large Language Model (LLM)-generated expert summaries to improve IHM
prediction accuracy. Using the MIMIC-III database, we analyzed time-series
physiological data and clinical notes from the first 48 hours of ICU admission.
Clinical notes were concatenated chronologically for each patient and
transformed into expert summaries using Med42-v2 70B. A multi-representational
learning framework was developed to integrate these data sources, leveraging
LLMs to enhance textual data while mitigating direct reliance on LLM
predictions, which can introduce challenges in uncertainty quantification and
interpretability. The proposed model achieved an AUPRC of 0.6156 (+36.41%) and
an AUROC of 0.8955 (+7.64%) compared to a time-series-only baseline. Expert
summaries outperformed clinical notes or time-series data alone, demonstrating
the value of LLM-generated knowledge. Performance gains were consistent across
demographic groups, with notable improvements in underrepresented populations,
underscoring the framework's equitable application potential. By integrating
LLM-generated summaries with structured and unstructured data, the framework
captures complementary patient information, significantly improving predictive
performance. This approach showcases the potential of LLMs to augment critical
care prediction models, emphasizing the need for domain-specific validation and
advanced integration strategies for broader clinical adoption.",2024-11-25,"Harshavardhan Battula, Jiacheng Liu, Jaideep Srivastava",http://arxiv.org/pdf/2411.16818v1,cs.CL
RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics,"Spatial understanding is a crucial capability that enables robots to perceive
their surroundings, reason about their environment, and interact with it
meaningfully. In modern robotics, these capabilities are increasingly provided
by vision-language models. However, these models face significant challenges in
spatial reasoning tasks, as their training data are based on general-purpose
image datasets that often lack sophisticated spatial understanding. For
example, datasets frequently do not capture reference frame comprehension, yet
effective spatial reasoning requires understanding whether to reason from ego-,
world-, or object-centric perspectives. To address this issue, we introduce
RoboSpatial, a large-scale dataset for spatial understanding in robotics. It
consists of real indoor and tabletop scenes, captured as 3D scans and
egocentric images, and annotated with rich spatial information relevant to
robotics. The dataset includes 1M images, 5k 3D scans, and 3M annotated spatial
relationships, and the pairing of 2D egocentric images with 3D scans makes it
both 2D- and 3D- ready. Our experiments show that models trained with
RoboSpatial outperform baselines on downstream tasks such as spatial affordance
prediction, spatial relationship prediction, and robot manipulation.",2024-11-25,"Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, Stan Birchfield",http://arxiv.org/pdf/2411.16537v4,cs.CL
Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word Embeddings,"Large language models (LLMs) are the foundation of the current successes of
artificial intelligence (AI), however, they are unavoidably biased. To
effectively communicate the risks and encourage mitigation efforts these models
need adequate and intuitive descriptions of their discriminatory properties,
appropriate for all audiences of AI. We suggest bias profiles with respect to
stereotype dimensions based on dictionaries from social psychology research.
Along these dimensions we investigate gender bias in contextual embeddings,
across contexts and layers, and generate stereotype profiles for twelve
different LLMs, demonstrating their intuition and use case for exposing and
visualizing bias.",2024-11-25,"Carolin M. Schuster, Maria-Alexandra Dinisor, Shashwat Ghatiwala, Georg Groh",http://arxiv.org/pdf/2411.16527v2,cs.CL
"Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity and Efficiency","We investigate the statistical and computational limits of prompt tuning for
transformer-based foundation models. Our key contributions are prompt tuning on
\textit{single-head} transformers with only a \textit{single} self-attention
layer: (i) is universal, and (ii) supports efficient (even almost-linear time)
algorithms under the Strong Exponential Time Hypothesis (SETH). Statistically,
we prove that prompt tuning on such simplest possible transformers are
universal approximators for sequence-to-sequence Lipschitz functions. In
addition, we provide an exponential-in-$dL$ and -in-$(1/\epsilon)$ lower bound
on the required soft-prompt tokens for prompt tuning to memorize any dataset
with 1-layer, 1-head transformers. Computationally, we identify a phase
transition in the efficiency of prompt tuning, determined by the norm of the
\textit{soft-prompt-induced} keys and queries, and provide an upper bound
criterion. Beyond this criterion, no sub-quadratic (efficient) algorithm for
prompt tuning exists under SETH. Within this criterion, we showcase our theory
by proving the existence of almost-linear time prompt tuning inference
algorithms. These fundamental limits provide important necessary conditions for
designing expressive and efficient prompt tuning methods for practitioners.",2024-11-25,"Jerry Yao-Chieh Hu, Wei-Po Wang, Ammar Gilani, Chenyang Li, Zhao Song, Han Liu",http://arxiv.org/pdf/2411.16525v1,cs.CL
LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology Report Generation,"In the current paradigm of image captioning, deep learning models are trained
to generate text from image embeddings of latent features. We challenge the
assumption that these latent features ought to be high-dimensional vectors
which require model fine tuning to handle. Here we propose Label Boosted
Retrieval Augmented Generation (LaB-RAG), a text-based approach to image
captioning that leverages image descriptors in the form of categorical labels
to boost standard retrieval augmented generation (RAG) with pretrained large
language models (LLMs). We study our method in the context of radiology report
generation (RRG), where the task is to generate a clinician's report detailing
their observations from a set of radiological images, such as X-rays. We argue
that simple linear classifiers over extracted image embeddings can effectively
transform X-rays into text-space as radiology-specific labels. In combination
with standard RAG, we show that these derived text labels can be used with
general-domain LLMs to generate radiology reports. Without ever training our
generative language model or image feature encoder models, and without ever
directly ""showing"" the LLM an X-ray, we demonstrate that LaB-RAG achieves
better results across natural language and radiology language metrics compared
with other retrieval-based RRG methods, while attaining competitive results
compared to other fine-tuned vision-language RRG models. We further present
results of our experiments with various components of LaB-RAG to better
understand our method. Finally, we critique the use of a popular RRG metric,
arguing it is possible to artificially inflate its results without true
data-leakage.",2024-11-25,"Steven Song, Anirudh Subramanyam, Irene Madejski, Robert L. Grossman",http://arxiv.org/pdf/2411.16523v1,cs.CL
All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages,"Existing Large Multimodal Models (LMMs) generally focus on only a few regions
and languages. As LMMs continue to improve, it is increasingly important to
ensure they understand cultural contexts, respect local sensitivities, and
support low-resource languages, all while effectively integrating corresponding
visual cues. In pursuit of culturally diverse global multimodal models, our
proposed All Languages Matter Benchmark (ALM-bench) represents the largest and
most comprehensive effort to date for evaluating LMMs across 100 languages.
ALM-bench challenges existing models by testing their ability to understand and
reason about culturally diverse images paired with text in various languages,
including many low-resource languages traditionally underrepresented in LMM
research. The benchmark offers a robust and nuanced evaluation framework
featuring various question formats, including true/false, multiple choice, and
open-ended questions, which are further divided into short and long-answer
categories. ALM-bench design ensures a comprehensive assessment of a model's
ability to handle varied levels of difficulty in visual and linguistic
reasoning. To capture the rich tapestry of global cultures, ALM-bench carefully
curates content from 13 distinct cultural aspects, ranging from traditions and
rituals to famous personalities and celebrations. Through this, ALM-bench not
only provides a rigorous testing ground for state-of-the-art open and
closed-source LMMs but also highlights the importance of cultural and
linguistic inclusivity, encouraging the development of models that can serve
diverse global populations effectively. Our benchmark is publicly available.",2024-11-25,"Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, Noor Ahsan, Nevasini Sasikumar, Omkar Thawakar, Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kuckreja, Mykola Maslych, Wafa Al Ghallabi, Mihail Mihaylov, Chao Qin, Abdelrahman M Shaker, Mike Zhang, Mahardika Krisna Ihsani, Amiel Esplana, Monil Gokani, Shachar Mirkin, Harsh Singh, Ashay Srivastava, Endre Hamerlik, Fathinah Asma Izzati, Fadillah Adamsyah Maani, Sebastian Cavada, Jenny Chim, Rohit Gupta, Sanjay Manjunath, Kamila Zhumakhanova, Feno Heriniaina Rabevohitra, Azril Amirudin, Muhammad Ridzuan, Daniya Kareem, Ketan More, Kunyang Li, Pramesh Shakya, Muhammad Saad, Amirpouya Ghasemaghaei, Amirbek Djanibekov, Dilshod Azizov, Branislava Jankovic, Naman Bhatia, Alvaro Cabrera, Johan Obando-Ceron, Olympiah Otieno, Fabian Farestam, Muztoba Rabbani, Sanoojan Baliah, Santosh Sanjeev, Abduragim Shtanchaev, Maheen Fatima, Thao Nguyen, Amrin Kareem, Toluwani Aremu, Nathan Xavier, Amit Bhatkal, Hawau Toyin, Aman Chadha, Hisham Cholakkal, Rao Muhammad Anwer, Michael Felsberg, Jorma Laaksonen, Thamar Solorio, Monojit Choudhury, Ivan Laptev, Mubarak Shah, Salman Khan, Fahad Khan",http://arxiv.org/pdf/2411.16508v4,cs.CL
DocEDA: Automated Extraction and Design of Analog Circuits from Documents with Large Language Model,"Efficient and accurate extraction of electrical parameters from circuit
datasheets and design documents is critical for accelerating circuit design in
Electronic Design Automation (EDA). Traditional workflows often rely on
engineers manually searching and extracting these parameters, which is
time-consuming, and prone to human error. To address these challenges, we
introduce DocEDA, an automated system that leverages advanced computer vision
techniques and Large Language Models (LLMs) to extract electrical parameters
seamlessly from documents. The layout analysis model specifically designed for
datasheet is proposed to classify documents into circuit-related parts.
Utilizing the inherent Chain-of-Thought reasoning capabilities of LLMs, DocEDA
automates the extraction of electronic component parameters from documents. For
circuit diagrams parsing, an improved GAM-YOLO model is hybrid with topology
identification to transform diagrams into circuit netlists. Then, a space
mapping enhanced optimization framework is evoked for optimization the layout
in the document. Experimental evaluations demonstrate that DocEDA significantly
enhances the efficiency of processing circuit design documents and the accuracy
of electrical parameter extraction. It exhibits adaptability to various circuit
design scenarios and document formats, offering a novel solution for EDA with
the potential to transform traditional methodologies.",2024-11-25,"Hong Cai Chen, Longchang Wu, Ming Gao, Lingrui Shen, Jiarui Zhong, Yipin Xu",http://arxiv.org/pdf/2412.05301v1,cs.CL
AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge Reasoning,"Despite the outstanding capabilities of large language models (LLMs),
knowledge-intensive reasoning still remains a challenging task due to LLMs'
limitations in compositional reasoning and the hallucination problem. A
prevalent solution is to employ chain-of-thought (CoT) with retrieval-augmented
generation (RAG), which first formulates a reasoning plan by decomposing
complex questions into simpler sub-questions, and then applies iterative RAG at
each sub-question. However, prior works exhibit two crucial problems:
inadequate reasoning planning and poor incorporation of heterogeneous
knowledge. In this paper, we introduce AtomR, a framework for LLMs to conduct
accurate heterogeneous knowledge reasoning at the atomic level. Inspired by how
knowledge graph query languages model compositional reasoning through combining
predefined operations, we propose three atomic knowledge operators, a unified
set of operators for LLMs to retrieve and manipulate knowledge from
heterogeneous sources. First, in the reasoning planning stage, AtomR decomposes
a complex question into a reasoning tree where each leaf node corresponds to an
atomic knowledge operator, achieving question decomposition that is highly
fine-grained and orthogonal. Subsequently, in the reasoning execution stage,
AtomR executes each atomic knowledge operator, which flexibly selects,
retrieves, and operates atomic level knowledge from heterogeneous sources. We
also introduce BlendQA, a challenging benchmark specially tailored for
heterogeneous knowledge reasoning. Experiments on three single-source and two
multi-source datasets show that AtomR outperforms state-of-the-art baselines by
a large margin, with F1 score improvements of 9.4% on 2WikiMultihop and 9.5% on
BlendQA. We release our code and datasets.",2024-11-25,"Amy Xin, Jinxin Liu, Zijun Yao, Zhicheng Lee, Shulin Cao, Lei Hou, Juanzi Li",http://arxiv.org/pdf/2411.16495v3,cs.CL
"O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?","This paper presents a critical examination of current approaches to
replicating OpenAI's O1 model capabilities, with particular focus on the
widespread but often undisclosed use of knowledge distillation techniques.
While our previous work explored the fundamental technical path to O1
replication, this study reveals how simple distillation from O1's API, combined
with supervised fine-tuning, can achieve superior performance on complex
mathematical reasoning tasks. Through extensive experiments, we show that a
base model fine-tuned on simply tens of thousands of samples O1-distilled
long-thought chains outperforms O1-preview on the American Invitational
Mathematics Examination (AIME) with minimal technical complexity. Moreover, our
investigation extends beyond mathematical reasoning to explore the
generalization capabilities of O1-distilled models across diverse tasks:
hallucination, safety and open-domain QA. Notably, despite training only on
mathematical problem-solving data, our models demonstrated strong
generalization to open-ended QA tasks and became significantly less susceptible
to sycophancy after fine-tuning. We deliberately make this finding public to
promote transparency in AI research and to challenge the current trend of
obscured technical claims in the field. Our work includes: (1) A detailed
technical exposition of the distillation process and its effectiveness, (2) A
comprehensive benchmark framework for evaluating and categorizing O1
replication attempts based on their technical transparency and reproducibility,
(3) A critical discussion of the limitations and potential risks of
over-relying on distillation approaches, our analysis culminates in a crucial
bitter lesson: while the pursuit of more capable AI systems is important, the
development of researchers grounded in first-principles thinking is paramount.",2024-11-25,"Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, Pengfei Liu",http://arxiv.org/pdf/2411.16489v1,cs.CL
Fine-Tuning LLMs with Noisy Data for Political Argument Generation and Post Guidance,"The incivility in social media discourse complicates the deployment of
automated text generation models for politically sensitive content. Fine-tuning
and prompting strategies are critical, but underexplored, solutions to mitigate
toxicity in such contexts. This study investigates the fine-tuning and
prompting effects on GPT-3.5 Turbo using subsets of the CLAPTON dataset of
political discussion posts, comprising Twitter and Reddit data labeled for
their justification, reciprocity and incivility. Fine-tuned models on Reddit
data scored highest on discussion quality, while combined noisy data led to
persistent toxicity. Prompting strategies reduced specific toxic traits, such
as personal attacks, but had limited broader impact. The findings emphasize
that high-quality data and well-crafted prompts are essential to reduce
incivility and improve rhetorical quality in automated political discourse
generation.",2024-11-25,"Svetlana Churina, Kokil Jaidka",http://arxiv.org/pdf/2411.16813v2,cs.CL
When Babies Teach Babies: Can student knowledge sharing outperform Teacher-Guided Distillation on small datasets?,"We present our submission to the BabyLM challenge, aiming to push the
boundaries of data-efficient language model pretraining. Our method builds upon
deep mutual learning, introducing a student model search for diverse
initialization. We address the limitation of treating students equally by
formulating weighted mutual learning as a bi-level optimization problem. The
inner loop learns compact students through online distillation, while the outer
loop optimizes weights for better knowledge distillation from diverse students.
This dynamic weighting strategy eliminates the need for a teacher model,
reducing computational requirements. Our evaluations show that teacher-less
methods can match or surpass teacher-supervised approaches.",2024-11-25,Srikrishna Iyer,http://arxiv.org/pdf/2411.16487v1,cs.CL
Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem Solving with Computational Graph-Based Retrieval,"Large language models (LLMs) are known to struggle with complicated reasoning
tasks such as math word problems (MWPs). In this paper, we present how analogy
from similarly structured questions can improve LLMs' problem-solving
capabilities for MWPs. Specifically, we rely on the retrieval of problems with
similar computational graphs to the given question to serve as exemplars in the
prompt, providing the correct reasoning path for the generation model to refer
to. Empirical results across six math word problem datasets demonstrate the
effectiveness of our proposed method, which achieves a significant improvement
of up to 6.7 percent on average in absolute value, compared to baseline
methods. These results highlight our method's potential in addressing the
reasoning challenges in current LLMs.",2024-11-25,"Xiaocong Yang, Jiacheng Lin, Ziqi Wang, Chengxiang Zhai",http://arxiv.org/pdf/2411.16454v1,cs.CL
Finding Structure in Language Models,"When we speak, write or listen, we continuously make predictions based on our
knowledge of a language's grammar. Remarkably, children acquire this
grammatical knowledge within just a few years, enabling them to understand and
generalise to novel constructions that have never been uttered before. Language
models are powerful tools that create representations of language by
incrementally predicting the next word in a sentence, and they have had a
tremendous societal impact in recent years. The central research question of
this thesis is whether these models possess a deep understanding of grammatical
structure similar to that of humans. This question lies at the intersection of
natural language processing, linguistics, and interpretability. To address it,
we will develop novel interpretability techniques that enhance our
understanding of the complex nature of large-scale language models. We approach
our research question from three directions. First, we explore the presence of
abstract linguistic information through structural priming, a key paradigm in
psycholinguistics for uncovering grammatical structure in human language
processing. Next, we examine various linguistic phenomena, such as adjective
order and negative polarity items, and connect a model's comprehension of these
phenomena to the data distribution on which it was trained. Finally, we
introduce a controlled testbed for studying hierarchical structure in language
models using various synthetic languages of increasing complexity and examine
the role of feature interactions in modelling this structure. Our findings
offer a detailed account of the grammatical knowledge embedded in language
model representations and provide several directions for investigating
fundamental linguistic questions using computational methods.",2024-11-25,Jaap Jumelet,http://arxiv.org/pdf/2411.16433v1,cs.CL
Adapter-based Approaches to Knowledge-enhanced Language Models -- A Survey,"Knowledge-enhanced language models (KELMs) have emerged as promising tools to
bridge the gap between large-scale language models and domain-specific
knowledge. KELMs can achieve higher factual accuracy and mitigate
hallucinations by leveraging knowledge graphs (KGs). They are frequently
combined with adapter modules to reduce the computational load and risk of
catastrophic forgetting. In this paper, we conduct a systematic literature
review (SLR) on adapter-based approaches to KELMs. We provide a structured
overview of existing methodologies in the field through quantitative and
qualitative analysis and explore the strengths and potential shortcomings of
individual approaches. We show that general knowledge and domain-specific
approaches have been frequently explored along with various adapter
architectures and downstream tasks. We particularly focused on the popular
biomedical domain, where we provided an insightful performance comparison of
existing KELMs. We outline the main trends and propose promising future
directions.",2024-11-25,"Alexander Fichtl, Juraj Vladika, Georg Groh",http://arxiv.org/pdf/2411.16403v1,cs.CL
Speculative Decoding with CTC-based Draft Model for LLM Inference Acceleration,"Inference acceleration of large language models (LLMs) has been put forward
in many application scenarios and speculative decoding has shown its advantage
in addressing inference acceleration. Speculative decoding usually introduces a
draft model to assist the base LLM where the draft model produces drafts and
the base LLM verifies the draft for acceptance or rejection. In this framework,
the final inference speed is decided by the decoding speed of the draft model
and the acceptance rate of the draft provided by the draft model. Currently the
widely used draft models usually generate draft tokens for the next several
positions in a non-autoregressive way without considering the correlations
between draft tokens. Therefore, it has a high decoding speed but an
unsatisfactory acceptance rate. In this paper, we focus on how to improve the
performance of the draft model and aim to accelerate inference via a high
acceptance rate. To this end, we propose a CTC-based draft model which
strengthens the correlations between draft tokens during the draft phase,
thereby generating higher-quality draft candidate sequences. Experiment results
show that compared to strong baselines, the proposed method can achieve a
higher acceptance rate and hence a faster inference speed.",2024-11-25,"Zhuofan Wen, Shangtong Gui, Yang Feng",http://arxiv.org/pdf/2412.00061v1,cs.CL
Human-Calibrated Automated Testing and Validation of Generative Language Models,"This paper introduces a comprehensive framework for the evaluation and
validation of generative language models (GLMs), with a focus on
Retrieval-Augmented Generation (RAG) systems deployed in high-stakes domains
such as banking. GLM evaluation is challenging due to open-ended outputs and
subjective quality assessments. Leveraging the structured nature of RAG
systems, where generated responses are grounded in a predefined document
collection, we propose the Human-Calibrated Automated Testing (HCAT) framework.
HCAT integrates a) automated test generation using stratified sampling, b)
embedding-based metrics for explainable assessment of functionality, risk and
safety attributes, and c) a two-stage calibration approach that aligns
machine-generated evaluations with human judgments through probability
calibration and conformal prediction.
  In addition, the framework includes robustness testing to evaluate model
performance against adversarial, out-of-distribution, and varied input
conditions, as well as targeted weakness identification using marginal and
bivariate analysis to pinpoint specific areas for improvement. This
human-calibrated, multi-layered evaluation framework offers a scalable,
transparent, and interpretable approach to GLM assessment, providing a
practical and reliable solution for deploying GLMs in applications where
accuracy, transparency, and regulatory compliance are paramount.",2024-11-25,"Agus Sudjianto, Aijun Zhang, Srinivas Neppalli, Tarun Joshi, Michal Malohlava",http://arxiv.org/pdf/2411.16391v2,cs.CL
FineWeb-zhtw: Scalable Curation of Traditional Chinese Text Data from the Web,"The quality and size of a pretraining dataset significantly influence the
performance of large language models (LLMs). While there have been numerous
efforts in the curation of such a dataset for English users, there is a
relative lack of similar initiatives for Traditional Chinese. Building upon
this foundation of FineWeb, we introduce FineWeb-zhtw, a dataset tailored
specifically for Traditional Chinese users. We came up with multiple stages of
meticulously designed filters to cater to the linguistic difference between
English and Traditional Chinese, to ensure comprehensiveness and quality. We
determined effectiveness from querying dataset samples with three main
objectives. Our code and datasets are publicly available.",2024-11-25,"Cheng-Wei Lin, Wan-Hsuan Hsieh, Kai-Xin Guan, Chan-Jan Hsu, Chia-Chen Kuo, Chuan-Lin Lai, Chung-Wei Chung, Ming-Jen Wang, Da-Shan Shiu",http://arxiv.org/pdf/2411.16387v1,cs.CL
"Multi-modal Retrieval Augmented Multi-modal Generation: Datasets, Evaluation Metrics and Strong Baselines","We present a systematic investigation of Multi-modal Retrieval Augmented
Multi-modal Generation (M$^2$RAG), a novel task that enables foundation models
to process multi-modal web content and generate multi-modal responses, which
exhibits better information density and readability. Despite its potential
impact, M$^2$RAG remains understudied, lacking comprehensive analysis and
high-quality data resources. To address this gap, we establish a comprehensive
benchmark through a rigorous data curation pipeline, and employ text-modal
metrics and multi-modal metrics based on foundation models for evaluation. We
further propose several strategies for foundation models to process M$^2$RAG
task effectively and construct a training set by filtering high-quality samples
using our designed metrics. Our extensive experiments demonstrate the
reliability of our proposed metrics, a landscape of model performance within
our designed strategies, and show that our fine-tuned 7B-8B models outperform
the GPT-4o model and approach the state-of-the-art OpenAI o3-mini.
Additionally, we perform fine-grained analyses across diverse domains and
validate the effectiveness of our designs in data curation pipeline. All
resources, including codes, datasets, and model weights, will be publicly
released.",2024-11-25,"Zi-Ao Ma, Tian Lan, Rong-Cheng Tu, Yong Hu, Yu-Shi Zhu, Tong Zhang, Heyan Huang, Zhijing Wu, Xian-Ling Mao",http://arxiv.org/pdf/2411.16365v4,cs.CL
"The Two-Hop Curse: LLMs trained on A$\rightarrow$B, B$\rightarrow$C fail to learn A$\rightarrow$C","[Notice: This version is outdated. Recent research contradicts some key
claims; we are working on a major revision with more nuanced analysis. Please
wait for the updated version.]
  While LLMs excel at multi-hop questions (e.g. ""Who is the spouse of the
performer of Imagine?"") when using chain-of-thought reasoning (CoT), they
struggle when forced to reason internally (without CoT). Previous work on the
size and nature of this gap produced mixed evidence with inconclusive results.
In this paper, we introduce a controlled setting for investigating two-hop
reasoning in LLMs, where the above-chance performance constitutes undeniable
evidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instruct
and GPT-4o) on fictional facts and confirm that they generalize to answering
two-hop questions about them using CoT. We find that models can perform latent
reasoning when facts appear together during training or in the prompt. However,
to our surprise, models completely fail at two-hop reasoning without CoT when
learned facts only appear in different documents, achieving chance-level
accuracy and chance-level test loss. We call this complete failure to compose
separately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontier
LLMs on real-world facts, finding that models completely fail at two-hop no-CoT
reasoning for over half of question categories while maintaining partial
success with CoT across most categories. These results suggest that LLMs lack a
general capability for latent multi-hop reasoning independent of the question
type.",2024-11-25,"Mikita Balesni, Tomek Korbak, Owain Evans",http://arxiv.org/pdf/2411.16353v2,cs.CL
Preference Optimization for Reasoning with Pseudo Feedback,"Preference optimization techniques, such as Direct Preference Optimization
(DPO), are frequently employed to enhance the reasoning capabilities of large
language models (LLMs) in domains like mathematical reasoning and coding,
typically following supervised fine-tuning. These methods rely on high-quality
labels for reasoning tasks to generate preference pairs; however, the
availability of reasoning datasets with human-verified labels is limited. In
this study, we introduce a novel approach to generate pseudo feedback for
reasoning tasks by framing the labeling of solutions to reason problems as an
evaluation against associated test cases. We explore two forms of pseudo
feedback based on test cases: one generated by frontier LLMs and the other by
extending self-consistency to multi-test-case. We conduct experiments on both
mathematical reasoning and coding tasks using pseudo feedback for preference
optimization, and observe improvements across both tasks. Specifically, using
Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,
surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and
College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,
respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on
LiveCodeBench (from 21.1), surpassing Claude-3-Haiku.",2024-11-25,"Fangkai Jiao, Geyang Guo, Xingxing Zhang, Nancy F. Chen, Shafiq Joty, Furu Wei",http://arxiv.org/pdf/2411.16345v2,cs.CL
Can AI grade your essays? A comparative analysis of large language models and teacher ratings in multidimensional essay scoring,"The manual assessment and grading of student writing is a time-consuming yet
critical task for teachers. Recent developments in generative AI, such as large
language models, offer potential solutions to facilitate essay-scoring tasks
for teachers. In our study, we evaluate the performance and reliability of both
open-source and closed-source LLMs in assessing German student essays,
comparing their evaluations to those of 37 teachers across 10 pre-defined
criteria (i.e., plot logic, expression). A corpus of 20 real-world essays from
Year 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1, LLaMA
3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs' scoring
capabilities. Closed-source GPT models outperform open-source models in both
internal consistency and alignment with human ratings, particularly excelling
in language-related criteria. The novel o1 model outperforms all other LLMs,
achieving Spearman's $r = .74$ with human assessments in the overall score, and
an internal consistency of $ICC=.80$. These findings indicate that LLM-based
assessment can be a useful tool to reduce teacher workload by supporting the
evaluation of essays, especially with regard to language-related criteria.
However, due to their tendency for higher scores, the models require further
refinement to better capture aspects of content quality.",2024-11-25,"Kathrin Seßler, Maurice Fürstenberg, Babette Bühler, Enkelejda Kasneci",http://arxiv.org/pdf/2411.16337v1,cs.CL
Learning from Relevant Subgoals in Successful Dialogs using Iterative Training for Task-oriented Dialog Systems,"Task-oriented Dialog (ToD) systems have to solve multiple subgoals to
accomplish user goals, whereas feedback is often obtained only at the end of
the dialog. In this work, we propose SUIT (SUbgoal-aware ITerative Training),
an iterative training approach for improving ToD systems. We sample dialogs
from the model we aim to improve and determine subgoals that contribute to
dialog success using distant supervision to obtain high quality training
samples. We show how this data improves supervised fine-tuning or,
alternatively, preference learning results. SUIT is able to iteratively
generate more data instead of relying on fixed static sets. SUIT reaches new
state-of-the-art performance on a popular ToD benchmark.",2024-11-25,"Magdalena Kaiser, Patrick Ernst, György Szarvas",http://arxiv.org/pdf/2411.16305v1,cs.CL
BayLing 2: A Multilingual Large Language Model with Efficient Language Alignment,"Large language models (LLMs), with their powerful generative capabilities and
vast knowledge, empower various tasks in everyday life. However, these
abilities are primarily concentrated in high-resource languages, leaving
low-resource languages with weaker generative capabilities and relatively
limited knowledge. Enhancing the multilingual capabilities of LLMs is therefore
crucial for serving over 100 linguistic communities worldwide. An intuitive
approach to enhance the multilingual capabilities would be to construct
instruction data for various languages, but constructing instruction data for
over 100 languages is prohibitively costly. In this paper, we introduce BayLing
2, which efficiently transfers generative capabilities and knowledge from
high-resource languages to low-resource languages through language alignment.
To achieve this, we constructed a dataset of 3.2 million instructions,
comprising high-resource language instructions (Chinese and English) and
cross-lingual instructions for 100+ languages and performed instruction tuning
based on the dataset to facilitate the capability transfer between languages.
Using Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,
and BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For
multilingual translation across 100+ languages, BayLing shows superior
performance compared to open-source models of similar scale. For multilingual
knowledge and understanding benchmarks, BayLing achieves significant
improvements across over 20 low-resource languages, demonstrating its
capability of effective knowledge transfer from high-resource to low-resource
languages. Furthermore, results on English benchmarks indicate that BayLing
maintains high performance in highresource languages while enhancing the
performance in low-resource languages. Demo, homepage, code and models of
BayLing are available.",2024-11-25,"Shaolei Zhang, Kehao Zhang, Qingkai Fang, Shoutao Guo, Yan Zhou, Xiaodong Liu, Yang Feng",http://arxiv.org/pdf/2411.16300v3,cs.CL
Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures,"Large language models (LLMs) have demonstrated remarkable mathematical
capabilities, largely driven by chain-of-thought (CoT) prompting, which
decomposes complex reasoning into step-by-step solutions. This approach has
enabled significant advancements, as evidenced by performance on benchmarks
like GSM8K and MATH. However, the mechanisms underlying LLMs' ability to
perform arithmetic in a single step of CoT remain poorly understood. Existing
studies debate whether LLMs encode numerical values or rely on symbolic
reasoning, while others explore attention and multi-layered processing in
arithmetic tasks. In this work, we propose that LLMs learn arithmetic by
capturing algebraic structures, such as commutativity and identity properties.
Since these structures are observable through input-output relationships, they
can generalize to unseen data. We empirically demonstrate that LLMs can learn
algebraic structures using a custom dataset of arithmetic problems, as well as
providing theoretical evidence showing that, under specific configurations of
weights and biases, the transformer-based LLMs can generate embeddings that
remain invariant to both permutations of input tokens and the presence of
identity elements. Our findings indicate that leveraging algebraic structures
can enhance the LLMs' arithmetic capabilities, offering insights into improving
their arithmetic performance.",2024-11-25,"Fu-Chieh Chang, You-Chen Lin, Pei-Yuan Wu",http://arxiv.org/pdf/2411.16260v3,cs.CL
Enhancing Answer Reliability Through Inter-Model Consensus of Large Language Models,"We propose a collaborative framework in which multiple large language models
-- including GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and
Gemini-1.5-Flash -- generate and answer complex, PhD-level statistical
questions when definitive ground truth is unavailable. Our study examines how
inter-model consensus improves both response reliability and identifies the
quality of the generated questions. Employing chi-square tests, Fleiss' Kappa,
and confidence interval analysis, we quantify consensus rates and inter-rater
agreement to assess both response precision and question quality. Key results
indicate that Claude and GPT-4 produce well-structured, less ambiguous
questions with a higher inter-rater agreement, as shown by narrower confidence
intervals and greater alignment with question-generating models. In contrast,
Gemini and LLaMA exhibit greater variability and lower reliability in question
formulation. These findings demonstrate that collaborative interactions among
large language models enhance response reliability and provide valuable
insights for optimizing AI-driven collaborative reasoning systems.",2024-11-25,"Alireza Amiri-Margavi, Iman Jebellat, Ehsan Jebellat, Seyed Pouyan Mousavi Davoudi",http://arxiv.org/pdf/2411.16797v2,cs.CL
NormXLogit: The Head-on-Top Never Lies,"The Transformer architecture has emerged as the dominant choice for building
large language models (LLMs). However, with new LLMs emerging on a frequent
basis, it is important to consider the potential value of architecture-agnostic
approaches that can provide interpretability across a variety of architectures.
Despite recent successes in the interpretability of LLMs, many existing
approaches rely on complex methods that are often tied to a specific model
design and come with a significant computational cost. To address these
limitations, we propose a novel technique, called NormXLogit, for assessing the
significance of individual input tokens. This method operates based on the
input and output representations associated with each token. First, we
demonstrate that during the pre-training of LLMs, the norms of word embeddings
capture the importance of input tokens. Second, we reveal a significant
relationship between a token's importance and the extent to which its
representation can resemble the model's final prediction. Through extensive
analysis, we show that our approach consistently outperforms existing
gradient-based methods in terms of faithfulness. Additionally, our method
achieves better performance in layer-wise explanations compared to the most
prominent architecture-specific methods.",2024-11-25,"Sina Abbasi, Mohammad Reza Modarres, Mohammad Taher Pilehvar",http://arxiv.org/pdf/2411.16252v1,cs.CL
Transparent Neighborhood Approximation for Text Classifier Explanation,"Recent literature highlights the critical role of neighborhood construction
in deriving model-agnostic explanations, with a growing trend toward deploying
generative models to improve synthetic instance quality, especially for
explaining text classifiers. These approaches overcome the challenges in
neighborhood construction posed by the unstructured nature of texts, thereby
improving the quality of explanations. However, the deployed generators are
usually implemented via neural networks and lack inherent explainability,
sparking arguments over the transparency of the explanation process itself. To
address this limitation while preserving neighborhood quality, this paper
introduces a probability-based editing method as an alternative to black-box
text generators. This approach generates neighboring texts by implementing
manipulations based on in-text contexts. Substituting the generator-based
construction process with recursive probability-based editing, the resultant
explanation method, XPROB (explainer with probability-based editing), exhibits
competitive performance according to the evaluation conducted on two real-world
datasets. Additionally, XPROB's fully transparent and more controllable
construction process leads to superior stability compared to the
generator-based explainers.",2024-11-25,"Yi Cai, Arthur Zimek, Eirini Ntoutsi, Gerhard Wunder",http://arxiv.org/pdf/2411.16251v1,cs.CL
Towards Efficient Model-Heterogeneity Federated Learning for Large Models,"As demand grows for complex tasks and high-performance applications in edge
computing, the deployment of large models in federated learning has become
increasingly urgent, given their superior representational power and
generalization capabilities. However, the resource constraints and
heterogeneity among clients present significant challenges to this deployment.
To tackle these challenges, we introduce HeteroTune, an innovative fine-tuning
framework tailored for model-heterogeneity federated learning (MHFL). In
particular, we propose a novel parameter-efficient fine-tuning (PEFT)
structure, called FedAdapter, which employs a multi-branch cross-model
aggregator to enable efficient knowledge aggregation across diverse models.
Benefiting from the lightweight FedAdapter, our approach significantly reduces
both the computational and communication overhead. Finally, our approach is
simple yet effective, making it applicable to a wide range of large model
fine-tuning tasks. Extensive experiments on computer vision (CV) and natural
language processing (NLP) tasks demonstrate that our method achieves
state-of-the-art results, seamlessly integrating efficiency and performance.",2024-11-25,"Ruofan Jia, Weiying Xie, Jie Lei, Haonan Qin, Jitao Ma, Leyuan Fang",http://arxiv.org/pdf/2411.16796v1,cs.CL
DoubleCCA: Improving Foundation Model Group Robustness with Random Sentence Embeddings,"This paper presents a novel method to improve the robustness of foundation
models to group-based biases. We propose a simple yet effective method, called
DoubleCCA, that leverages random sentences and Canonical Correlation Analysis
(CCA) to enrich the text embeddings of the foundation model. First, we generate
various random sentences that augment the original prompts, which extends the
original prompts with random words or character sequences. Second, we use an
additional sentence embedding model to generate different text embeddings with
respect to these random sentences. We then use CCA double twice to align the
representations and reconstruct them back to the original representation space.
We demonstrate the effectiveness of our method on a variety of tasks and
datasets, showing that it outperforms existing methods in terms of both
performance and robustness. Our method is simple to implement and can be easily
integrated into existing models, making it a practical solution for improving
the robustness of foundation models to group-based biases.",2024-11-25,"Hong Liu, Yitong Lu",http://arxiv.org/pdf/2411.16236v1,cs.CL
What can LLM tell us about cities?,"This study explores the capabilities of large language models (LLMs) in
providing knowledge about cities and regions on a global scale. We employ two
methods: directly querying the LLM for target variable values and extracting
explicit and implicit features from the LLM correlated with the target
variable. Our experiments reveal that LLMs embed a broad but varying degree of
knowledge across global cities, with ML models trained on LLM-derived features
consistently leading to improved predictive accuracy. Additionally, we observe
that LLMs demonstrate a certain level of knowledge across global cities on all
continents, but it is evident when they lack knowledge, as they tend to
generate generic or random outputs for unfamiliar tasks. These findings suggest
that LLMs can offer new opportunities for data-driven decision-making in the
study of cities.",2024-11-25,"Zhuoheng Li, Yaochen Wang, Zhixue Song, Yuqi Huang, Rui Bao, Guanjie Zheng, Zhenhui Jessie Li",http://arxiv.org/pdf/2411.16791v1,cs.CL
MH-MoE: Multi-Head Mixture-of-Experts,"Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by
using the multi-head mechanism to collectively attend to information from
various representation spaces within different experts. In this paper, we
present a novel implementation of MH-MoE that maintains both FLOPs and
parameter parity with sparse Mixture of Experts models. Experimental results on
language models show that the new implementation yields quality improvements
over both vanilla MoE and fine-grained MoE models. Additionally, our
experiments demonstrate that MH-MoE is compatible with 1-bit Large Language
Models (LLMs) such as BitNet.",2024-11-25,"Shaohan Huang, Xun Wu, Shuming Ma, Furu Wei",http://arxiv.org/pdf/2411.16205v3,cs.CL
Leveraging the Power of MLLMs for Gloss-Free Sign Language Translation,"Sign language translation (SLT) is a challenging task that involves
translating sign language images into spoken language. For SLT models to
perform this task successfully, they must bridge the modality gap and identify
subtle variations in sign language components to understand their meanings
accurately. To address these challenges, we propose a novel gloss-free SLT
framework called Multimodal Sign Language Translation (MMSLT), which leverages
the representational capabilities of off-the-shelf multimodal large language
models (MLLMs). Specifically, we generate detailed textual descriptions of sign
language components using MLLMs. Then, through our proposed multimodal-language
pre-training module, we integrate these description features with sign video
features to align them within the spoken sentence space. Our approach achieves
state-of-the-art performance on benchmark datasets PHOENIX14T and CSL-Daily,
highlighting the potential of MLLMs to be effectively utilized in SLT.",2024-11-25,"Jungeun Kim, Hyeongwoo Jeon, Jongseong Bae, Ha Young Kim",http://arxiv.org/pdf/2411.16789v1,cs.CL
Video-Text Dataset Construction from Multi-AI Feedback: Promoting Weak-to-Strong Preference Learning for Video Large Language Models,"High-quality video-text preference data is crucial for Multimodal Large
Language Models (MLLMs) alignment. However, existing preference data is very
scarce. Obtaining VQA preference data for preference training is costly, and
manually annotating responses is highly unreliable, which could result in
low-quality pairs. Meanwhile, AI-generated responses controlled by temperature
adjustment lack diversity. To address these issues, we propose a high-quality
VQA preference dataset, called \textit{\textbf{M}ultiple \textbf{M}ultimodal
\textbf{A}rtificial \textbf{I}ntelligence \textbf{P}reference Datasets in
\textbf{V}QA} (\textbf{MMAIP-V}), which is constructed by sampling from the
response distribution set and using an external scoring function for response
evaluation. Furthermore, to fully leverage the preference knowledge in MMAIP-V
and ensure sufficient optimization, we propose \textit{\textbf{Iter}ative
\textbf{W}eak-to-\textbf{S}trong \textbf{R}einforcement \textbf{L}earning from
\textbf{AI} \textbf{F}eedback for video MLLMs} (\textbf{Iter-W2S-RLAIF}), a
framework that gradually enhances MLLMs' alignment capabilities by iteratively
updating the reference model and performing parameter extrapolation. Finally,
we propose an unbiased and information-complete evaluation scheme in VQA
evaluation. Experiments demonstrate that MMAIP-V is beneficial for MLLMs in
preference learning and Iter-W2S-RLAIF fully exploits the alignment information
in MMAIP-V. We believe that the proposed automatic VQA preference data
generation pipeline based on AI feedback can greatly promote future work in the
MLLMs alignment. \textbf{Code and dataset are available}
\href{https://anonymous.4open.science/r/MMAIP-V_Iter-W2S-RLAIF-702F}{MMAIP-V\_Iter-W2S-RLAIF-702F}.",2024-11-25,"Hao Yi, Qingyang Li, Yulan Hu, Fuzheng Zhang, Di Zhang, Yong Liu",http://arxiv.org/pdf/2411.16201v1,cs.CL
Enhancing Multi-Agent Consensus through Third-Party LLM Integration: Analyzing Uncertainty and Mitigating Hallucinations in Large Language Models,"Large Language Models (LLMs) still face challenges when dealing with complex
reasoning tasks, often resulting in hallucinations, which limit the practical
application of LLMs. To alleviate this issue, this paper proposes a new method
that integrates different LLMs to expand the knowledge boundary, reduce
dependence on a single model, and promote in-depth debate among agents. The
main contributions include: 1) Introducing third-party LLMs to adjust the
attention weights of agents through uncertainty estimation and confidence
analysis, optimizing consensus formation in multi-agent systems; 2) Experiments
on arithmetic datasets have validated the effectiveness of the method,
surpassing traditional multi-agent baselines. This research provides a new
perspective for large models to alleviate hallucination phenomena when dealing
with complex tasks.",2024-11-25,"Zhihua Duan, Jialin Wang",http://arxiv.org/pdf/2411.16189v1,cs.CL
Contrastive Multi-graph Learning with Neighbor Hierarchical Sifting for Semi-supervised Text Classification,"Graph contrastive learning has been successfully applied in text
classification due to its remarkable ability for self-supervised node
representation learning. However, explicit graph augmentations may lead to a
loss of semantics in the contrastive views. Secondly, existing methods tend to
overlook edge features and the varying significance of node features during
multi-graph learning. Moreover, the contrastive loss suffer from false
negatives. To address these limitations, we propose a novel method of
contrastive multi-graph learning with neighbor hierarchical sifting for
semi-supervised text classification, namely ConNHS. Specifically, we exploit
core features to form a multi-relational text graph, enhancing semantic
connections among texts. By separating text graphs, we provide diverse views
for contrastive learning. Our approach ensures optimal preservation of the
graph information, minimizing data loss and distortion. Then, we separately
execute relation-aware propagation and cross-graph attention propagation, which
effectively leverages the varying correlations between nodes and edge features
while harmonising the information fusion across graphs. Subsequently, we
present the neighbor hierarchical sifting loss (NHS) to refine the negative
selection. For one thing, following the homophily assumption, NHS masks
first-order neighbors of the anchor and positives from being negatives. For
another, NHS excludes the high-order neighbors analogous to the anchor based on
their similarities. Consequently, it effectively reduces the occurrence of
false negatives, preventing the expansion of the distance between similar
samples in the embedding space. Our experiments on ThuCNews, SogouNews, 20
Newsgroups, and Ohsumed datasets achieved 95.86\%, 97.52\%, 87.43\%, and
70.65\%, which demonstrates competitive results in semi-supervised text
classification.",2024-11-25,"Wei Ai, Jianbin Li, Ze Wang, Yingying Wei, Tao Meng, Yuntao Shou, Keqin Lib",http://arxiv.org/pdf/2411.16787v1,cs.CL
Specifications: The missing link to making the development of LLM systems an engineering discipline,"Despite the significant strides made by generative AI in just a few short
years, its future progress is constrained by the challenge of building modular
and robust systems. This capability has been a cornerstone of past
technological revolutions, which relied on combining components to create
increasingly sophisticated and reliable systems. Cars, airplanes, computers,
and software consist of components-such as engines, wheels, CPUs, and
libraries-that can be assembled, debugged, and replaced. A key tool for
building such reliable and modular systems is specification: the precise
description of the expected behavior, inputs, and outputs of each component.
However, the generality of LLMs and the inherent ambiguity of natural language
make defining specifications for LLM-based components (e.g., agents) both a
challenging and urgent problem. In this paper, we discuss the progress the
field has made so far-through advances like structured outputs, process
supervision, and test-time compute-and outline several future directions for
research to enable the development of modular and reliable LLM-based systems
through improved specifications.",2024-11-25,"Ion Stoica, Matei Zaharia, Joseph Gonzalez, Ken Goldberg, Koushik Sen, Hao Zhang, Anastasios Angelopoulos, Shishir G. Patil, Lingjiao Chen, Wei-Lin Chiang, Jared Q. Davis",http://arxiv.org/pdf/2412.05299v2,cs.CL
Parameter Efficient Instruction Tuning: An Empirical Study,"Instruction tuning has become an important step for finetuning pretrained
language models to better follow human instructions and generalize on various
tasks. Nowadays, pretrained language models become increasingly larger, and
full parameter finetuning is overwhelmingly costly. Therefore, Parameter
Efficient Finetuning (PEFT) has arisen as a cost-effective practice for
instruction tuning because of significantly smaller computational, memory, and
storage cost compared to full finetuning. Despite their widespread adaptations,
the vast hyperparameter spaces, the number of PEFT methods, the different focus
of instruction tuning capabilities make disentangling the impact of each aspect
difficult. This study systematically investigates several representative PEFT
methods, surveying the effect of hyperparameter choices including training
hyperparameters and PEFT-specific hyperparameters, how different models sizes
and the number of instruction tasks affect the performance,
in-task-distribution memorization and open instruction following capability.
Our empirical study shows that only LoRA and adapter can get close to full
finetuning with ideal training settings. The ideal training setting includes an
appropriate learning rate, largest LoRA rank or adapter size allowed and
diverse training tasks. On the other hand, LoRA and adapter suffer from
training instability if such an ideal training condition is not met.
Additionally, LoRA requires a greater number of tasks for effective unseen task
generalization, exhibit slower learning speed. Moreover, LoRA has weaker
task-level memorization. Lastly, LoRA and adapter fall short in complex
reasoning, coding and long-form generation compared to finetuning in open
instruction tuning settings but it shows stronger capabilities compared to
adapter.",2024-11-25,Pengfei He,http://arxiv.org/pdf/2411.16775v1,cs.CL
A Cross-Corpus Speech Emotion Recognition Method Based on Supervised Contrastive Learning,"Research on Speech Emotion Recognition (SER) often faces challenges such as
the lack of large-scale public datasets and limited generalization capability
when dealing with data from different distributions. To solve this problem,
this paper proposes a cross-corpus speech emotion recognition method based on
supervised contrast learning. The method employs a two-stage fine-tuning
process: first, the self-supervised speech representation model is fine-tuned
using supervised contrastive learning on multiple speech emotion datasets;
then, the classifier is fine-tuned on the target dataset. The experimental
results show that the WavLM-based model achieved unweighted accuracy (UA) of
77.41% on the IEMOCAP dataset and 96.49% on the CASIA dataset, outperforming
the state-of-the-art results on the two datasets.",2024-11-25,Xiang minjie,http://arxiv.org/pdf/2411.19803v1,cs.CL
LLM Augmentations to support Analytical Reasoning over Multiple Documents,"Building on their demonstrated ability to perform a variety of tasks, we
investigate the application of large language models (LLMs) to enhance in-depth
analytical reasoning within the context of intelligence analysis. Intelligence
analysts typically work with massive dossiers to draw connections between
seemingly unrelated entities, and uncover adversaries' plans and motives. We
explore if and how LLMs can be helpful to analysts for this task and develop an
architecture to augment the capabilities of an LLM with a memory module called
dynamic evidence trees (DETs) to develop and track multiple investigation
threads. Through extensive experiments on multiple datasets, we highlight how
LLMs, as-is, are still inadequate to support intelligence analysts and offer
recommendations to improve LLMs for such intricate reasoning applications.",2024-11-25,"Raquib Bin Yousuf, Nicholas Defelice, Mandar Sharma, Shengzhe Xu, Naren Ramakrishnan",http://arxiv.org/pdf/2411.16116v1,cs.CL
Adaptive Circuit Behavior and Generalization in Mechanistic Interpretability,"Mechanistic interpretability aims to understand the inner workings of large
neural networks by identifying circuits, or minimal subgraphs within the model
that implement algorithms responsible for performing specific tasks. These
circuits are typically discovered and analyzed using a narrowly defined prompt
format. However, given the abilities of large language models (LLMs) to
generalize across various prompt formats for the same task, it remains unclear
how well these circuits generalize. For instance, it is unclear whether the
models generalization results from reusing the same circuit components, the
components behaving differently, or the use of entirely different components.
In this paper, we investigate the generality of the indirect object
identification (IOI) circuit in GPT-2 small, which is well-studied and believed
to implement a simple, interpretable algorithm. We evaluate its performance on
prompt variants that challenge the assumptions of this algorithm. Our findings
reveal that the circuit generalizes surprisingly well, reusing all of its
components and mechanisms while only adding additional input edges. Notably,
the circuit generalizes even to prompt variants where the original algorithm
should fail; we discover a mechanism that explains this which we term S2
Hacking. Our findings indicate that circuits within LLMs may be more flexible
and general than previously recognized, underscoring the importance of studying
circuit generalization to better understand the broader capabilities of these
models.",2024-11-25,"Jatin Nainani, Sankaran Vaidyanathan, AJ Yeung, Kartik Gupta, David Jensen",http://arxiv.org/pdf/2411.16105v2,cs.CL
Cautious Optimizers: Improving Training with One Line of Code,"AdamW has been the default optimizer for transformer pretraining. For many
years, our community searched for faster and more stable optimizers with only
constrained positive outcomes. In this work, we propose a single-line
modification in Pytorch to any momentum-based optimizer, which we rename
cautious optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that
this modification preserves Adam's Hamiltonian function and it does not break
the convergence guarantee under the Lyapunov analysis. In addition, a whole new
family of optimizers is revealed by our theoretical insight. Among them, we
pick the simplest one for empirical experiments, showing not only speed-up on
Llama and MAE pretraining up to $1.47$ times, but also better results in LLM
post-training tasks. Code is available at
https://github.com/kyleliang919/C-Optim.",2024-11-25,"Kaizhao Liang, Lizhang Chen, Bo Liu, Qiang Liu",http://arxiv.org/pdf/2411.16085v3,cs.CL
In-Context Experience Replay Facilitates Safety Red-Teaming of Text-to-Image Diffusion Models,"Text-to-image (T2I) models have shown remarkable progress, but their
potential to generate harmful content remains a critical concern in the ML
community. While various safety mechanisms have been developed, the field lacks
systematic tools for evaluating their effectiveness against real-world misuse
scenarios. In this work, we propose ICER, a novel red-teaming framework that
leverages Large Language Models (LLMs) and a bandit optimization-based
algorithm to generate interpretable and semantic meaningful problematic prompts
by learning from past successful red-teaming attempts. Our ICER efficiently
probes safety mechanisms across different T2I models without requiring internal
access or additional training, making it broadly applicable to deployed
systems. Through extensive experiments, we demonstrate that ICER significantly
outperforms existing prompt attack methods in identifying model vulnerabilities
while maintaining high semantic similarity with intended content. By uncovering
that successful jailbreaking instances can systematically facilitate the
discovery of new vulnerabilities, our work provides crucial insights for
developing more robust safety mechanisms in T2I systems.",2024-11-25,"Zhi-Yi Chin, Mario Fritz, Pin-Yu Chen, Wei-Chen Chiu",http://arxiv.org/pdf/2411.16769v2,cs.CL
SAGEval: The frontiers of Satisfactory Agent based NLG Evaluation for reference-free open-ended text,"Large Language Model (LLM) integrations into applications like Microsoft365
suite and Google Workspace for creating/processing documents, emails,
presentations, etc. has led to considerable enhancements in productivity and
time savings. But as these integrations become more more complex, it is
paramount to ensure that the quality of output from the LLM-integrated
applications are relevant and appropriate for use. Identifying the need to
develop robust evaluation approaches for natural language generation, wherein
references/ground labels doesn't exist or isn't amply available, this paper
introduces a novel framework called ""SAGEval"" which utilizes a critiquing Agent
to provide feedback on scores generated by LLM evaluators. We show that the
critiquing Agent is able to rectify scores from LLM evaluators, in absence of
references/ground-truth labels, thereby reducing the need for labeled data even
for complex NLG evaluation scenarios, like the generation of JSON-structured
forms/surveys with responses in different styles like multiple choice, likert
ratings, single choice questions, etc.",2024-11-25,"Reshmi Ghosh, Tianyi Yao, Lizzy Chen, Sadid Hasan, Tianwei Chen, Dario Bernal, Huitian Jiao, H M Sajjad Hossain",http://arxiv.org/pdf/2411.16077v1,cs.CL
SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction,"Sign language processing has traditionally relied on task-specific
models,limiting the potential for transfer learning across tasks. We introduce
SHuBERT (Sign Hidden-Unit BERT), a self-supervised transformer encoder that
learns strong representations from approximately 1,000 hours of American Sign
Language (ASL) video content. Inspired by the success of the HuBERT speech
representation model, SHuBERT adapts masked prediction for multi-stream visual
sign language input, learning to predict multiple targets for corresponding to
clustered hand, face, and body pose streams. SHuBERT achieves state-of-the-art
performance across multiple benchmarks. On sign language translation, it
outperforms prior methods trained on publicly available data on the How2Sign
(+0.7 BLEU), OpenASL (+10.0 BLEU), and FLEURS-ASL (+0.3 BLEU) benchmarks.
Similarly for isolated sign language recognition, SHuBERT's accuracy surpasses
that of specialized models on ASL-Citizen (+5\%) and SEM-LEX (+20.6\%), while
coming close to them on WLASL2000 (-3\%). Ablation studies confirm the
contribution of each component of the approach.",2024-11-25,"Shester Gueuwou, Xiaodan Du, Greg Shakhnarovich, Karen Livescu, Alexander H. Liu",http://arxiv.org/pdf/2411.16765v1,cs.CL
Predicting Emergent Capabilities by Finetuning,"A fundamental open challenge in modern LLM scaling is the lack of
understanding around emergent capabilities. In particular, language model
pretraining loss is known to be highly predictable as a function of compute.
However, downstream capabilities are far less predictable -- sometimes even
exhibiting emergent jumps -- which makes it challenging to anticipate the
capabilities of future models. In this work, we first pose the task of
emergence prediction: given access to current LLMs that have random few-shot
accuracy on a task, can we predict whether future models (GPT-N+1) will have
non-trivial accuracy on that task? We then discover a simple insight for this
problem: finetuning LLMs on a given task can shift the point in scaling at
which emergence occurs towards less capable models. To operationalize this
insight, we can finetune LLMs with varying amounts of data and fit a parametric
function that predicts when emergence will occur (i.e., ""emergence laws""). We
validate this approach using four standard NLP benchmarks where large-scale
open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and
CoLA). Using only small-scale LLMs, we find that, in some cases, we can
accurately predict whether models trained with up to 4x more compute have
emerged. Finally, we present a case study of two realistic uses for emergence
prediction.",2024-11-25,"Charlie Snell, Eric Wallace, Dan Klein, Sergey Levine",http://arxiv.org/pdf/2411.16035v1,cs.CL
TransCompressor: LLM-Powered Multimodal Data Compression for Smart Transportation,"The incorporation of Large Language Models (LLMs) into smart transportation
systems has paved the way for improving data management and operational
efficiency. This study introduces TransCompressor, a novel framework that
leverages LLMs for efficient compression and decompression of multimodal
transportation sensor data. TransCompressor has undergone thorough evaluation
with diverse sensor data types, including barometer, speed, and altitude
measurements, across various transportation modes like buses, taxis, and MTRs.
Comprehensive evaluation illustrates the effectiveness of TransCompressor in
reconstructing transportation sensor data at different compression ratios. The
results highlight that, with well-crafted prompts, LLMs can utilize their vast
knowledge base to contribute to data compression processes, enhancing data
storage, analysis, and retrieval in smart transportation settings.",2024-11-25,"Huanqi Yang, Rucheng Wu, Weitao Xu",http://arxiv.org/pdf/2411.16020v1,cs.CL
"Exploring Performance Contrasts in TableQA: Step-by-Step Reasoning Boosts Bigger Language Models, Limits Smaller Language Models","This paper proposes a detailed prompting flow, termed Table-Logic, to
investigate the performance contrasts between bigger and smaller language
models (LMs) utilizing step-by-step reasoning methods in the TableQA task. The
method processes tasks by sequentially identifying critical columns and rows
given question and table with its structure, determining necessary
aggregations, calculations, or comparisons, and finally inferring the results
to generate a precise prediction. By deploying this method, we observe a 7.8%
accuracy improvement in bigger LMs like Llama-3-70B compared to the vanilla on
HybridQA, while smaller LMs like Llama-2-7B shows an 11% performance decline.
We empirically investigate the potential causes of performance contrasts by
exploring the capabilities of bigger and smaller LMs from various dimensions in
TableQA task. Our findings highlight the limitations of the step-by-step
reasoning method in small models and provide potential insights for making
improvements.",2024-11-24,"Haoyan Yang, Yixuan Wang, Keyue Tong, Hongjin Zhu, Yuanxin Zhang",http://arxiv.org/pdf/2411.16002v1,cs.CL
Multi-ToM: Evaluating Multilingual Theory of Mind Capabilities in Large Language Models,"Theory of Mind (ToM) refers to the cognitive ability to infer and attribute
mental states to oneself and others. As large language models (LLMs) are
increasingly evaluated for social and cognitive capabilities, it remains
unclear to what extent these models demonstrate ToM across diverse languages
and cultural contexts. In this paper, we introduce a comprehensive study of
multilingual ToM capabilities aimed at addressing this gap. Our approach
includes two key components: (1) We translate existing ToM datasets into
multiple languages, effectively creating a multilingual ToM dataset and (2) We
enrich these translations with culturally specific elements to reflect the
social and cognitive scenarios relevant to diverse populations. We conduct
extensive evaluations of six state-of-the-art LLMs to measure their ToM
performance across both the translated and culturally adapted datasets. The
results highlight the influence of linguistic and cultural diversity on the
models' ability to exhibit ToM, and questions their social reasoning
capabilities. This work lays the groundwork for future research into enhancing
LLMs' cross-cultural social cognition and contributes to the development of
more culturally aware and socially intelligent AI systems. All our data and
code are publicly available.",2024-11-24,"Jayanta Sadhu, Ayan Antik Khan, Noshin Nawal, Sanju Basak, Abhik Bhattacharjee, Rifat Shahriyar",http://arxiv.org/pdf/2411.15999v1,cs.CL
Investigating Factuality in Long-Form Text Generation: The Roles of Self-Known and Self-Unknown,"Large language models (LLMs) have demonstrated strong capabilities in text
understanding and generation. However, they often lack factuality, producing a
mixture of true and false information, especially in long-form generation. In
this work, we investigates the factuality of long-form text generation across
various large language models (LLMs), including GPT-4, Gemini-1.5-Pro,
Claude-3-Opus, Llama-3-70B, and Mistral. Our analysis reveals that factuality
scores tend to decline in later sentences of the generated text, accompanied by
a rise in the number of unsupported claims. Furthermore, we explore the
effectiveness of different evaluation settings to assess whether LLMs can
accurately judge the correctness of their own outputs: Self-Known (the
percentage of supported atomic claims, decomposed from LLM outputs, that the
corresponding LLMs judge as correct) and Self-Unknown (the percentage of
unsupported atomic claims that the corresponding LLMs judge as incorrect). The
results indicate that even advanced models like GPT-4 and Gemini-1.5-Pro fail
to achieve perfect Self-Known scores, while their Self-Unknown scores remain
notably above zero, reflecting ongoing uncertainty in their self-assessments.
Moreover, we find a correlation between higher Self-Known scores and improved
factuality, while higher Self-Unknown scores are associated with lower
factuality. Interestingly, even without significant changes in the models'
self-judgment (Self-Known and Self-Unknown), the number of unsupported claims
can increases, likely as an artifact of long-form generation. These findings
show the limitations of current LLMs in long-form generation, and provide
valuable insights for improving factuality in long-form text generation.",2024-11-24,"Lifu Tu, Rui Meng, Shafiq Joty, Yingbo Zhou, Semih Yavuz",http://arxiv.org/pdf/2411.15993v1,cs.CL
Kleene algebra with commutativity conditions is undecidable,"We prove that the equational theory of Kleene algebra with commutativity
  conditions on primitives (or atomic terms) is undecidable, thereby settling a
  longstanding open question in the theory of Kleene algebra. While this
  question has also been recently solved independently by Kuznetsov, our
results
  hold even for weaker theories that do not support the induction axioms
  of Kleene algebra.",2024-11-24,"Arthur Azevedo de Amorim, Cheng Zhang, Marco Gaboardi",http://arxiv.org/pdf/2411.15979v1,cs.CL
Generative Prompt Internalization,"Prompts used in recent large language model based applications are often
fixed and lengthy, leading to significant computational overhead. To address
this challenge, we propose Generative Prompt Internalization (GenPI), a
lightweight method that employs a joint training approach. GenPI not only
replicates the behavior of models with prompt inputs but also generates the
content of the prompt along with reasons for why the model's behavior should
change accordingly. We demonstrate that our approach effectively internalizes
complex prompts across various agent-based application scenarios. For effective
training without interactions with the dedicated environments, we introduce a
data synthesis technique that autonomously collects conversational datasets by
swapping the roles of the agent and environment. This method is especially
useful in scenarios where only a predefined prompt is available without a
corresponding training dataset. By internalizing complex prompts, Generative
Prompt Internalization enables high performance and efficient inference without
the need for explicit prompts.",2024-11-24,"Haebin Shin, Lei Ji, Yeyun Gong, Sungdong Kim, Eunbi Choi, Minjoon Seo",http://arxiv.org/pdf/2411.15927v3,cs.CL
High-precision medical speech recognition through synthetic data and semantic correction: UNITED-MEDASR,"Automatic Speech Recognition (ASR) systems in the clinical domain face
significant challenges, notably the need to recognise specialised medical
vocabulary accurately and meet stringent precision requirements. We introduce
United-MedASR, a novel architecture that addresses these challenges by
integrating synthetic data generation, precision ASR fine-tuning, and advanced
semantic enhancement techniques. United-MedASR constructs a specialised medical
vocabulary by synthesising data from authoritative sources such as ICD-10
(International Classification of Diseases, 10th Revision), MIMS (Monthly Index
of Medical Specialties), and FDA databases. This enriched vocabulary helps
finetune the Whisper ASR model to better cater to clinical needs. To enhance
processing speed, we incorporate Faster Whisper, ensuring streamlined and
high-speed ASR performance. Additionally, we employ a customised BART-based
semantic enhancer to handle intricate medical terminology, thereby increasing
accuracy efficiently. Our layered approach establishes new benchmarks in ASR
performance, achieving a Word Error Rate (WER) of 0.985% on LibriSpeech
test-clean, 0.26% on Europarl-ASR EN Guest-test, and demonstrating robust
performance on Tedlium (0.29% WER) and FLEURS (0.336% WER). Furthermore, we
present an adaptable architecture that can be replicated across different
domains, making it a versatile solution for domain-specific ASR systems.",2024-11-24,"Sourav Banerjee, Ayushi Agarwal, Promila Ghosh",http://arxiv.org/pdf/2412.00055v1,cs.CL
Evaluating Large Language Models for Causal Modeling,"In this paper, we consider the process of transforming causal domain
knowledge into a representation that aligns more closely with guidelines from
causal data science. To this end, we introduce two novel tasks related to
distilling causal domain knowledge into causal variables and detecting
interaction entities using LLMs. We have determined that contemporary LLMs are
helpful tools for conducting causal modeling tasks in collaboration with human
experts, as they can provide a wider perspective. Specifically, LLMs, such as
GPT-4-turbo and Llama3-70b, perform better in distilling causal domain
knowledge into causal variables compared to sparse expert models, such as
Mixtral-8x22b. On the contrary, sparse expert models such as Mixtral-8x22b
stand out as the most effective in identifying interaction entities. Finally,
we highlight the dependency between the domain where the entities are generated
and the performance of the chosen LLM for causal modeling.",2024-11-24,"Houssam Razouk, Leonie Benischke, Georg Niess, Roman Kern",http://arxiv.org/pdf/2411.15888v1,cs.CL
Do LLMs Really Think Step-by-step In Implicit Reasoning?,"It has been well-known that Chain-of-Thought can remarkably enhance LLMs'
performance on complex tasks. However, because it also introduces slower
inference speeds and higher computational costs, many researches have attempted
to use implicit CoT, which does not need LLMs to explicitly generate the
intermediate steps. However, the invisible reasoning process leaves us a doubt
that, can implicit CoT really be equal to explicit CoT? Therefore, in this
study, we address this question through experiments. We probe the information
of intermediate steps from the model's hidden states when it is either trained
or prompted to perform implicit CoT. The results surprisingly indicate that
when prompted, LLMs hardly think about intermediate steps, suggesting they may
just rely on experience rather than strict step-by-step reasoning. But when
trained, they indeed calculate intermediate steps. Moreover, in both
situations, we find the effect of using implicit CoT is susceptible to the
format of the problem, reaffirming the current deficiency of implicit CoT.",2024-11-24,Yijiong Yu,http://arxiv.org/pdf/2411.15862v4,cs.CL
Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?,"This study investigates the relative impact of training data quality versus
quantity on the performance of small language models (SLMs), utilizing the
TinyStories dataset for empirical analysis. Analysis of dataset variations with
respect to size (25% and 50% of the original size) and duplication (controlled
rates of 25%, 50%, 75%, and 100%) were performed. Model performance was
evaluated based on the validation loss, accuracy, and perplexity metrics.
Results indicate training data quality plays a more significant role in the
overall performance of SLMs, especially given scale of this experiment. Minimal
duplication positively impacted model accuracy (+0.87% increase in accuracy at
25% duplication) without significantly increasing perplexity (+0.52% increase
going from 0% to 25% duplication) but excessive duplication led to pronounced
performance degradation (-40% drop in accuracy at 100% duplication). The
implications of this exploration extend beyond just model performance; training
large-scale models imposes significant financial and computational burdens,
which can be prohibitive for organizations, individuals, and the public at
large, especially in developing countries. Additionally, the energy consumption
associated with large-scale training raises environmental concerns.
Understanding the relative importance of data quality versus quantity could
democratize AI technology, making advanced models more accessible and
sustainable for all.",2024-11-24,"Aryan Sajith, Krishna Chaitanya Rao Kathala",http://arxiv.org/pdf/2411.15821v3,cs.CL
LeMoLE: LLM-Enhanced Mixture of Linear Experts for Time Series Forecasting,"Recent research has shown that large language models (LLMs) can be
effectively used for real-world time series forecasting due to their strong
natural language understanding capabilities. However, aligning time series into
semantic spaces of LLMs comes with high computational costs and inference
complexity, particularly for long-range time series generation. Building on
recent advancements in using linear models for time series, this paper
introduces an LLM-enhanced mixture of linear experts for precise and efficient
time series forecasting. This approach involves developing a mixture of linear
experts with multiple lookback lengths and a new multimodal fusion mechanism.
The use of a mixture of linear experts is efficient due to its simplicity,
while the multimodal fusion mechanism adaptively combines multiple linear
experts based on the learned features of the text modality from pre-trained
large language models. In experiments, we rethink the need to align time series
to LLMs by existing time-series large language models and further discuss their
efficiency and effectiveness in time series forecasting. Our experimental
results show that the proposed LeMoLE model presents lower prediction errors
and higher computational efficiency than existing LLM models.",2024-11-24,"Lingzheng Zhang, Lifeng Shen, Yimin Zheng, Shiyuan Piao, Ziyue Li, Fugee Tsung",http://arxiv.org/pdf/2412.00053v1,cs.CL
LoRA-Mini : Adaptation Matrices Decomposition and Selective Training,"The rapid advancements in large language models (LLMs) have revolutionized
natural language processing, creating an increased need for efficient,
task-specific fine-tuning methods. Traditional fine-tuning of LLMs involves
updating a large number of parameters, which is computationally expensive and
memory-intensive. Low-Rank Adaptation (LoRA) has emerged as a promising
solution, enabling parameter-efficient fine-tuning by reducing the number of
trainable parameters. However, while LoRA reduces the number of trainable
parameters, LoRA modules still create significant storage challenges. We
propose LoRA-Mini, an optimized adaptation of LoRA that improves parameter
efficiency by splitting low-rank matrices into four parts, with only the two
inner matrices being trainable. This approach achieves upto a 20x reduction
compared to standard LoRA in the number of trainable parameters while
preserving performance levels comparable to standard LoRA, addressing both
computational and storage efficiency in LLM fine-tuning.",2024-11-24,"Ayush Singh, Rajdeep Aher, Shivank Garg",http://arxiv.org/pdf/2411.15804v1,cs.CL
A Method for Building Large Language Models with Predefined KV Cache Capacity,"This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),
for building large language models with a predefined Key-Value (KV) cache
capacity. The BCT addresses the excessive memory consumption issue in
traditional KV caches by implementing a bounded-length KV cache, which is
particularly suitable for the attention layers in Transformer decode-only
architectures. By dynamically updating the key-value vector sequences, the BCT
achieves efficient inference within limited cache capacity, significantly
reducing memory usage while maintaining model performance and system
throughput. Experimental results demonstrate that the BCT significantly reduces
memory usage while maintaining the model's inference quality, offering a new
solution for efficient inference in large language models.",2024-11-24,"Zhonghua Yi, Ge Niu, Lei Wang, Wei Tang, Liqiu Zhang",http://arxiv.org/pdf/2411.15785v2,cs.CL
Detecting Turkish Synonyms Used in Different Time Periods,"Dynamic structure of languages poses significant challenges in applying
natural language processing models on historical texts, causing decreased
performance in various downstream tasks. Turkish is a prominent example of
rapid linguistic transformation due to the language reform in the 20th century.
In this paper, we propose two methods for detecting synonyms used in different
time periods, focusing on Turkish. In our first method, we use Orthogonal
Procrustes method to align the embedding spaces created using documents written
in the corresponding time periods. In our second method, we extend the first
one by incorporating Spearman's correlation between frequencies of words
throughout the years. In our experiments, we show that our proposed methods
outperform the baseline method. Furthermore, we observe that the efficacy of
our methods remains consistent when the target time period shifts from the
1960s to the 1980s. However, their performance slightly decreases for
subsequent time periods.",2024-11-24,"Umur Togay Yazar, Mucahid Kutlu",http://arxiv.org/pdf/2411.15768v1,cs.CL
TableTime: Reformulating Time Series Classification as Training-Free Table Understanding with Large Language Models,"Large language models (LLMs) have demonstrated their effectiveness in
multivariate time series classification (MTSC). Effective adaptation of LLMs
for MTSC necessitates informative data representations. Existing LLM-based
methods directly encode embeddings for time series within the latent space of
LLMs from scratch to align with semantic space of LLMs. Despite their
effectiveness, we reveal that these methods conceal three inherent bottlenecks:
(1) they struggle to encode temporal and channel-specific information in a
lossless manner, both of which are critical components of multivariate time
series; (2) it is much difficult to align the learned representation space with
the semantic space of the LLMs; (3) they require task-specific retraining,
which is both computationally expensive and labor-intensive. To bridge these
gaps, we propose TableTime, which reformulates MTSC as a table understanding
task. Specifically, TableTime introduces the following strategies: (1) convert
multivariate time series into a tabular form, thus minimizing information loss
to the greatest extent; (2) represent tabular time series in text format to
achieve natural alignment with the semantic space of LLMs; (3) design a
reasoning framework that integrates contextual text information, neighborhood
assistance, multi-path inference and problem decomposition to enhance the
reasoning ability of LLMs and realize zero-shot classification. Extensive
experiments performed on 10 publicly representative datasets from UEA archive
verify the superiorities of the TableTime.",2024-11-24,"Jiahao Wang, Mingyue Cheng, Qingyang Mao, Yitong Zhou, Feiyang Xu, Xin Li",http://arxiv.org/pdf/2411.15737v3,cs.CL
Development of Pre-Trained Transformer-based Models for the Nepali Language,"Transformer-based pre-trained language models have dominated the field of
Natural Language Processing (NLP) for quite some time now. However, the Nepali
language, spoken by approximately 32 million people worldwide, remains
significantly underrepresented in this domain. This underrepresentation is
primarily attributed to the scarcity of monolingual data corpora and limited
available resources for the Nepali language. While existing efforts have
predominantly concentrated on basic encoder-based models, there is a notable
gap in the exploration of decoder-based architectures. To address this gap, we
have collected 27.5 GB of Nepali text data, approximately 2.4x larger than any
previously available Nepali language corpus. Leveraging this data, we
pre-trained three different models i.e., BERT, RoBERTa, and GPT-2, exclusively
for the Nepali Language. Furthermore, we performed instruction tuning and
explored its potential for monolingual Nepali data, providing a foundation for
future research. Our models outperformed the existing best model by 2 points on
Nep-gLUE benchmark, scoring 95.60 and also outperformed existing models on text
generation tasks, demonstrating improvements in both understanding and
generating Nepali text.",2024-11-24,"Prajwal Thapa, Jinu Nyachhyon, Mridul Sharma, Bal Krishna Bal",http://arxiv.org/pdf/2411.15734v1,cs.CL
PriorDiffusion: Leverage Language Prior in Diffusion Models for Monocular Depth Estimation,"Traditional monocular depth estimation suffers from inherent ambiguity and
visual nuisance. We argue that language prior can enhance monocular depth
estimation by leveraging the inductive bias learned during the text-to-image
pre-training of diffusion models. The ability of these models to generate
images that align with text indicates that they have learned the spatial
relationships, size, and shape of specified objects, which can be applied to
improve depth estimation. Thus, we propose PriorDiffusion, using a pre-trained
text-to-image diffusion model that takes both images and corresponding text
descriptions to infer affine-invariant depth through a denoising process. We
also show that language prior enhances the model's perception of specific
regions of images that users care about and describe. Simultaneously, language
prior acts as a constraint to accelerate the convergence of both training and
the inference diffusion trajectory. By training on HyperSim and Virtual KITTI,
we achieve faster training convergence, fewer inference diffusion steps, and
state-of-the-art zero-shot performance across NYUv2, KITTI, ETH3D, and ScanNet.
Code will be released upon acceptance.",2024-11-24,"Ziyao Zeng, Jingcheng Ni, Daniel Wang, Patrick Rim, Younjoon Chung, Fengyu Yang, Byung-Woo Hong, Alex Wong",http://arxiv.org/pdf/2411.16750v2,cs.CL
LLaMA-MoE v2: Exploring Sparsity of LLaMA from Perspective of Mixture-of-Experts with Post-Training,"Recently, inspired by the concept of sparsity, Mixture-of-Experts (MoE)
models have gained increasing popularity for scaling model size while keeping
the number of activated parameters constant. In this study, we thoroughly
investigate the sparsity of the dense LLaMA model by constructing MoE for both
the attention (i.e., Attention MoE) and MLP (i.e., MLP MoE) modules in the
transformer blocks. Specifically, we investigate different expert construction
methods and granularities under the same activation conditions to analyze the
impact of sparsifying the model. Additionally, to comprehensively evaluate the
model's capabilities across various domains (e.g., conversation, code, math)
after sparsification, we apply sparsity to the instructed large language models
(LLMs) and construct instructed MoE models. To counteract the performance
degradation resulting from increased sparsity, we design a two-stage
post-training strategy to enhance model performance. Experiments on the LLaMA3
model demonstrate the potential effectiveness of this approach for future
developments of instructed MoE models. The source codes and models are
available at: \url{https://github.com/OpenSparseLLMs/LLaMA-MoE-v2}.",2024-11-24,"Xiaoye Qu, Daize Dong, Xuyang Hu, Tong Zhu, Weigao Sun, Yu Cheng",http://arxiv.org/pdf/2411.15708v1,cs.CL
RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large Language Models on Dietary Supplements,"\textbf{Objective:} We aimed to develop an advanced multi-task large language
model (LLM) framework to extract multiple types of information about dietary
supplements (DS) from clinical records.
  \textbf{Methods:} We used four core DS information extraction tasks - namely,
named entity recognition (NER: 2,949 clinical sentences), relation extraction
(RE: 4,892 sentences), triple extraction (TE: 2,949 sentences), and usage
classification (UC: 2,460 sentences) as our multitasks. We introduced a novel
Retrieval-Augmented Multi-task Information Extraction (RAMIE) Framework,
including: 1) employed instruction fine-tuning techniques with task-specific
prompts, 2) trained LLMs for multiple tasks with improved storage efficiency
and lower training costs, and 3) incorporated retrieval augmentation generation
(RAG) techniques by retrieving similar examples from the training set. We
compared RAMIE's performance to LLMs with instruction fine-tuning alone and
conducted an ablation study to assess the contributions of multi-task learning
and RAG to improved multitasking performance.
  \textbf{Results:} With the aid of the RAMIE framework, Llama2-13B achieved an
F1 score of 87.39 (3.51\% improvement) on the NER task and demonstrated
outstanding performance on the RE task with an F1 score of 93.74 (1.15\%
improvement). For the TE task, Llama2-7B scored 79.45 (14.26\% improvement),
and MedAlpaca-7B achieved the highest F1 score of 93.45 (0.94\% improvement) on
the UC task. The ablation study revealed that while MTL increased efficiency
with a slight trade-off in performance, RAG significantly boosted overall
accuracy.
  \textbf{Conclusion:} This study presents a novel RAMIE framework that
demonstrates substantial improvements in multi-task information extraction for
DS-related data from clinical records. Our framework can potentially be applied
to other domains.",2024-11-24,"Zaifu Zhan, Shuang Zhou, Mingchen Li, Rui Zhang",http://arxiv.org/pdf/2411.15700v1,cs.CL
Deep Sparse Latent Feature Models for Knowledge Graph Completion,"Recent progress in knowledge graph completion (KGC) has focused on text-based
approaches to address the challenges of large-scale knowledge graphs (KGs).
Despite their achievements, these methods often overlook the intricate
interconnections between entities, a key aspect of the underlying topological
structure of a KG. Stochastic blockmodels (SBMs), particularly the latent
feature relational model (LFRM), offer robust probabilistic frameworks that can
dynamically capture latent community structures and enhance link prediction. In
this paper, we introduce a novel framework of sparse latent feature models for
KGC, optimized through a deep variational autoencoder (VAE). Our approach not
only effectively completes missing triples but also provides clear
interpretability of the latent structures, leveraging textual information.
Comprehensive experiments on the WN18RR, FB15k-237, and Wikidata5M datasets
show that our method significantly improves performance by revealing latent
communities and producing interpretable representations.",2024-11-24,"Haotian Li, Rui Zhang, Lingzhi Wang, Bin Yu, Youwei Wang, Yuliang Wei, Kai Wang, Richard Yi Da Xu, Bailing Wang",http://arxiv.org/pdf/2411.15694v1,cs.CL
Ontology-Constrained Generation of Domain-Specific Clinical Summaries,"Large Language Models (LLMs) offer promising solutions for text
summarization. However, some domains require specific information to be
available in the summaries. Generating these domain-adapted summaries is still
an open challenge. Similarly, hallucinations in generated content is a major
drawback of current approaches, preventing their deployment. This study
proposes a novel approach that leverages ontologies to create domain-adapted
summaries both structured and unstructured. We employ an ontology-guided
constrained decoding process to reduce hallucinations while improving
relevance. When applied to the medical domain, our method shows potential in
summarizing Electronic Health Records (EHRs) across different specialties,
allowing doctors to focus on the most relevant information to their domain.
Evaluation on the MIMIC-III dataset demonstrates improvements in generating
domain-adapted summaries of clinical notes and hallucination reduction.",2024-11-23,"Gaya Mehenni, Amal Zouaq",http://arxiv.org/pdf/2411.15666v1,cs.CL
Improving Next Tokens via Second-to-Last Predictions with Generate and Refine,"Autoregressive language models like GPT aim to predict next tokens, while
autoencoding models such as BERT are trained on tasks such as predicting masked
tokens. We train a decoder-only architecture for predicting the second to last
token for a sequence of tokens. Our approach yields higher computational
training efficiency than BERT-style models by employing a structured
deterministic approach to masking tokens. We use our model to improve the next
token predictions of a standard GPT by combining both predictions in a
``generate-then-refine'' approach. We demonstrate on different variants of
GPT-2 and different datasets that (not unexpectedly) second to last token
predictions are much more accurate, i.e., more than 15\% higher accuracy than
standard next token predictions. The ``generate-then-refine'' approach also
demonstrates notable improvements in next-token predictions, yielding smaller
yet consistent and significant gains.",2024-11-23,Johannes Schneider,http://arxiv.org/pdf/2411.15661v2,cs.CL
"AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering Benchmark Dataset","Recent advancements in large language model(LLM) performance on medical
multiple choice question (MCQ) benchmarks have stimulated interest from
healthcare providers and patients globally. Particularly in low-and
middle-income countries (LMICs) facing acute physician shortages and lack of
specialists, LLMs offer a potentially scalable pathway to enhance healthcare
access and reduce costs. However, their effectiveness in the Global South,
especially across the African continent, remains to be established. In this
work, we introduce AfriMed-QA, the first large scale Pan-African English
multi-specialty medical Question-Answering (QA) dataset, 15,000 questions (open
and closed-ended) sourced from over 60 medical schools across 16 countries,
covering 32 medical specialties. We further evaluate 30 LLMs across multiple
axes including correctness and demographic bias. Our findings show significant
performance variation across specialties and geographies, MCQ performance
clearly lags USMLE (MedQA). We find that biomedical LLMs underperform general
models and smaller edge-friendly LLMs struggle to achieve a passing score.
Interestingly, human evaluations show a consistent consumer preference for LLM
answers and explanations when compared with clinician answers.",2024-11-23,"Tobi Olatunji, Charles Nimo, Abraham Owodunni, Tassallah Abdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, Folafunmi Omofoye, Foutse Yuehgoh, Timothy Faniran, Bonaventure F. P. Dossou, Moshood Yekini, Jonas Kemp, Katherine Heller, Jude Chidubem Omeke, Chidi Asuzu MD, Naome A. Etori, Aimérou Ndiaye, Ifeoma Okoh, Evans Doe Ocansey, Wendy Kinara, Michael Best, Irfan Essa, Stephen Edward Moore, Chris Fourie, Mercy Nyamewaa Asiedu",http://arxiv.org/pdf/2411.15640v3,cs.CL
"""All that Glitters"": Approaches to Evaluations with Unreliable Model and Human Annotations","""Gold"" and ""ground truth"" human-mediated labels have error. The effects of
this error can escape commonly reported metrics of label quality or obscure
questions of accuracy, bias, fairness, and usefulness during model evaluation.
This study demonstrates methods for answering such questions even in the
context of very low reliabilities from expert humans. We analyze human labels,
GPT model ratings, and transformer encoder model annotations describing the
quality of classroom teaching, an important, expensive, and currently only
human task. We answer the question of whether such a task can be automated
using two Large Language Model (LLM) architecture families--encoders and GPT
decoders, using novel approaches to evaluating label quality across six
dimensions: Concordance, Confidence, Validity, Bias, Fairness, and Helpfulness.
First, we demonstrate that using standard metrics in the presence of poor
labels can mask both label and model quality: the encoder family of models
achieve state-of-the-art, even ""super-human"", results across all classroom
annotation tasks. But not all these positive results remain after using more
rigorous evaluation measures which reveal spurious correlations and nonrandom
racial biases across models and humans. This study then expands these methods
to estimate how model use would change to human label quality if models were
used in a human-in-the-loop context, finding that the variance captured in GPT
model labels would worsen reliabilities for humans influenced by these models.
We identify areas where some LLMs, within the generalizability of the current
data, could improve the quality of expensive human ratings of classroom
instruction.",2024-11-23,Michael Hardy,http://arxiv.org/pdf/2411.15634v1,cs.CL
Multi-label Sequential Sentence Classification via Large Language Model,"Sequential sentence classification (SSC) in scientific publications is
crucial for supporting downstream tasks such as fine-grained information
retrieval and extractive summarization. However, current SSC methods are
constrained by model size, sequence length, and single-label setting. To
address these limitations, this paper proposes LLM-SSC, a large language model
(LLM)-based framework for both single- and multi-label SSC tasks. Unlike
previous approaches that employ small- or medium-sized language models, the
proposed framework utilizes LLMs to generate SSC labels through designed
prompts, which enhance task understanding by incorporating demonstrations and a
query to describe the prediction target. We also present a multi-label
contrastive learning loss with auto-weighting scheme, enabling the multi-label
classification task. To support our multi-label SSC analysis, we introduce and
release a new dataset, biorc800, which mainly contains unstructured abstracts
in the biomedical domain with manual annotations. Experiments demonstrate
LLM-SSC's strong performance in SSC under both in-context learning and
task-specific tuning settings. We release biorc800 and our code at:
https://github.com/ScienceNLP-Lab/LLM-SSC.",2024-11-23,"Mengfei Lan, Lecheng Zheng, Shufan Ming, Halil Kilicoglu",http://arxiv.org/pdf/2411.15623v2,cs.CL
A Survey on LLM-as-a-Judge,"Accurate and consistent evaluation is crucial for decision-making across
numerous fields, yet it remains a challenging task due to inherent
subjectivity, variability, and scale. Large Language Models (LLMs) have
achieved remarkable success across diverse domains, leading to the emergence of
""LLM-as-a-Judge,"" where LLMs are employed as evaluators for complex tasks. With
their ability to process diverse data types and provide scalable,
cost-effective, and consistent assessments, LLMs present a compelling
alternative to traditional expert-driven evaluations. However, ensuring the
reliability of LLM-as-a-Judge systems remains a significant challenge that
requires careful design and standardization. This paper provides a
comprehensive survey of LLM-as-a-Judge, addressing the core question: How can
reliable LLM-as-a-Judge systems be built? We explore strategies to enhance
reliability, including improving consistency, mitigating biases, and adapting
to diverse assessment scenarios. Additionally, we propose methodologies for
evaluating the reliability of LLM-as-a-Judge systems, supported by a novel
benchmark designed for this purpose. To advance the development and real-world
deployment of LLM-as-a-Judge systems, we also discussed practical applications,
challenges, and future directions. This survey serves as a foundational
reference for researchers and practitioners in this rapidly evolving field.",2024-11-23,"Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, Jian Guo",http://arxiv.org/pdf/2411.15594v5,cs.CL
"Transparent but Powerful: Explainability, Accuracy, and Generalizability in ADHD Detection from Social Media Data","Attention-deficit/hyperactivity disorder (ADHD) is a prevalent mental health
condition affecting both children and adults, yet it remains severely
underdiagnosed. Recent advances in artificial intelligence, particularly in
Natural Language Processing (NLP) and Machine Learning (ML), offer promising
solutions for scalable and non-invasive ADHD screening methods using social
media data. This paper presents a comprehensive study on ADHD detection,
leveraging both shallow machine learning models and deep learning approaches,
including BiLSTM and transformer-based models, to analyze linguistic patterns
in ADHD-related social media text. Our results highlight the trade-offs between
interpretability and performance across different models, with BiLSTM offering
a balance of transparency and accuracy. Additionally, we assess the
generalizability of these models using cross-platform data from Reddit and
Twitter, uncovering key linguistic features associated with ADHD that could
contribute to more effective digital screening tools.",2024-11-23,"D. Wiechmann, E. Kempa, E. Kerz, Y. Qiao",http://arxiv.org/pdf/2411.15586v1,cs.CL
From MTEB to MTOB: Retrieval-Augmented Classification for Descriptive Grammars,"Recent advances in language modeling have demonstrated significant
improvements in zero-shot capabilities, including in-context learning,
instruction following, and machine translation for extremely under-resourced
languages (Tanzer et al., 2024). However, many languages with limited written
resources rely primarily on formal descriptions of grammar and vocabulary.
  In this paper, we introduce a set of benchmarks to evaluate how well models
can extract and classify information from the complex descriptions found in
linguistic grammars. We present a Retrieval-Augmented Generation (RAG)-based
approach that leverages these descriptions for downstream tasks such as machine
translation. Our benchmarks encompass linguistic descriptions for 248 languages
across 142 language families, focusing on typological features from WALS and
Grambank.
  This set of benchmarks offers the first comprehensive evaluation of language
models' in-context ability to accurately interpret and extract linguistic
features, providing a critical resource for scaling NLP to low-resource
languages. The code and data are publicly available at
\url{https://github.com/al-the-eigenvalue/RAG-on-grammars}.",2024-11-23,"Albert Kornilov, Tatiana Shavrina",http://arxiv.org/pdf/2411.15577v2,cs.CL
Do LLMs Agree on the Creativity Evaluation of Alternative Uses?,"This paper investigates whether large language models (LLMs) show agreement
in assessing creativity in responses to the Alternative Uses Test (AUT). While
LLMs are increasingly used to evaluate creative content, previous studies have
primarily focused on a single model assessing responses generated by the same
model or humans. This paper explores whether LLMs can impartially and
accurately evaluate creativity in outputs generated by both themselves and
other models. Using an oracle benchmark set of AUT responses, categorized by
creativity level (common, creative, and highly creative), we experiment with
four state-of-the-art LLMs evaluating these outputs. We test both scoring and
ranking methods and employ two evaluation settings (comprehensive and
segmented) to examine if LLMs agree on the creativity evaluation of alternative
uses. Results reveal high inter-model agreement, with Spearman correlations
averaging above 0.7 across models and reaching over 0.77 with respect to the
oracle, indicating a high level of agreement and validating the reliability of
LLMs in creativity assessment of alternative uses. Notably, models do not
favour their own responses, instead they provide similar creativity assessment
scores or rankings for alternative uses generated by other models. These
findings suggest that LLMs exhibit impartiality and high alignment in
creativity evaluation, offering promising implications for their use in
automated creativity assessment.",2024-11-23,"Abdullah Al Rabeyah, Fabrício Góes, Marco Volpe, Talles Medeiros",http://arxiv.org/pdf/2411.15560v2,cs.CL
ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain,"The advancement and extensive application of large language models (LLMs)
have been remarkable, including their use in scientific research assistance.
However, these models often generate scientifically incorrect or unsafe
responses, and in some cases, they may encourage users to engage in dangerous
behavior. To address this issue in the field of chemistry, we introduce
ChemSafetyBench, a benchmark designed to evaluate the accuracy and safety of
LLM responses. ChemSafetyBench encompasses three key tasks: querying chemical
properties, assessing the legality of chemical uses, and describing synthesis
methods, each requiring increasingly deeper chemical knowledge. Our dataset has
more than 30K samples across various chemical materials. We incorporate
handcrafted templates and advanced jailbreaking scenarios to enhance task
diversity. Our automated evaluation framework thoroughly assesses the safety,
accuracy, and appropriateness of LLM responses. Extensive experiments with
state-of-the-art LLMs reveal notable strengths and critical vulnerabilities,
underscoring the need for robust safety measures. ChemSafetyBench aims to be a
pivotal tool in developing safer AI technologies in chemistry. Our code and
dataset are available at https://github.com/HaochenZhao/SafeAgent4Chem.
Warning: this paper contains discussions on the synthesis of controlled
chemicals using AI models.",2024-11-23,"Haochen Zhao, Xiangru Tang, Ziran Yang, Xiao Han, Xuanzhi Feng, Yueqing Fan, Senhao Cheng, Di Jin, Yilun Zhao, Arman Cohan, Mark Gerstein",http://arxiv.org/pdf/2411.16736v1,cs.CL
QEQR: An Exploration of Query Expansion Methods for Question Retrieval in CQA Services,"CQA services are valuable sources of knowledge that can be used to find
answers to users' information needs. In these services, question retrieval aims
to help users with their information needs by finding similar questions to
theirs. However, finding similar questions is obstructed by the lexical gap
that exists between relevant questions. In this work, we target this problem by
using query expansion methods. We use word-similarity-based methods, propose a
question-similarity-based method and selective expansion of these methods to
expand a question that's been submitted and mitigate the lexical gap problem.
Our best method achieves a significant relative improvement of 1.8\% compared
to the best-performing baseline without query expansion.",2024-11-23,"Yasin Ghafourian, Sajad Movahedi, Azadeh Shakery",http://arxiv.org/pdf/2411.15530v1,cs.CL
Enhancing Grammatical Error Detection using BERT with Cleaned Lang-8 Dataset,"This paper presents an improved LLM based model for Grammatical Error
Detection (GED), which is a very challenging and equally important problem for
many applications. The traditional approach to GED involved hand-designed
features, but recently, Neural Networks (NN) have automated the discovery of
these features, improving performance in GED. Traditional rule-based systems
have an F1 score of 0.50-0.60 and earlier machine learning models give an F1
score of 0.65-0.75, including decision trees and simple neural networks.
Previous deep learning models, for example, Bi-LSTM, have reported F1 scores
within the range from 0.80 to 0.90. In our study, we have fine-tuned various
transformer models using the Lang8 dataset rigorously cleaned by us. In our
experiments, the BERT-base-uncased model gave an impressive performance with an
F1 score of 0.91 and accuracy of 98.49% on training data and 90.53% on testing
data, also showcasing the importance of data cleaning. Increasing model size
using BERT-large-uncased or RoBERTa-large did not give any noticeable
improvements in performance or advantage for this task, underscoring that
larger models are not always better. Our results clearly show how far rigorous
data cleaning and simple transformer-based models can go toward significantly
improving the quality of GED.",2024-11-23,"Rahul Nihalani, Kushal Shah",http://arxiv.org/pdf/2411.15523v1,cs.CL
Multi-Reranker: Maximizing performance of retrieval-augmented generation in the FinanceRAG challenge,"As Large Language Models (LLMs) increasingly address domain-specific
problems, their application in the financial sector has expanded rapidly. Tasks
that are both highly valuable and time-consuming, such as analyzing financial
statements, disclosures, and related documents, are now being effectively
tackled using LLMs. This paper details the development of a high-performance,
finance-specific Retrieval-Augmented Generation (RAG) system for the ACM-ICAIF
'24 FinanceRAG competition. We optimized performance through ablation studies
on query expansion and corpus refinement during the pre-retrieval phase. To
enhance retrieval accuracy, we employed multiple reranker models. Notably, we
introduced an efficient method for managing long context sizes during the
generation phase, significantly improving response quality without sacrificing
performance. We ultimately achieve 2nd place in the FinanceRAG Challenge. Our
key contributions include: (1) pre-retrieval ablation analysis, (2) an enhanced
retrieval algorithm, and (3) a novel approach for long-context management. This
work demonstrates the potential of LLMs in effectively processing and analyzing
complex financial data to generate accurate and valuable insights. The source
code and further details are available at https://github.com/cv-lee/FinanceRAG.",2024-11-23,"Joohyun Lee, Minji Roh",http://arxiv.org/pdf/2411.16732v1,cs.CL
"""Moralized"" Multi-Step Jailbreak Prompts: Black-Box Testing of Guardrails in Large Language Models for Verbal Attacks","As the application of large language models continues to expand in various
fields, it poses higher challenges to the effectiveness of identifying harmful
content generation and guardrail mechanisms. This research aims to evaluate the
guardrail effectiveness of GPT-4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5,
and Claude 3.5 Sonnet through black-box testing of seemingly ethical multi-step
jailbreak prompts. It conducts ethical attacks by designing an identical
multi-step prompts that simulates the scenario of ""corporate middle managers
competing for promotions."" The data results show that the guardrails of the
above-mentioned LLMs were bypassed and the content of verbal attacks was
generated. Claude 3.5 Sonnet's resistance to multi-step jailbreak prompts is
more obvious. To ensure objectivity, the experimental process, black box test
code, and enhanced guardrail code are uploaded to the GitHub repository:
https://github.com/brucewang123456789/GeniusTrail.git.",2024-11-23,Libo Wang,http://arxiv.org/pdf/2411.16730v4,cs.CL
MolMetaLM: a Physicochemical Knowledge-Guided Molecular Meta Language Model,"Most current molecular language models transfer the masked language model or
image-text generation model from natural language processing to molecular
field. However, molecules are not solely characterized by atom/bond symbols;
they encapsulate important physical/chemical properties. Moreover, normal
language models bring grammar rules that are irrelevant for understanding
molecules. In this study, we propose a novel physicochemical knowledge-guided
molecular meta language framework MolMetaLM. We design a molecule-specialized
meta language paradigm, formatted as multiple <S,P,O> (subject, predicate,
object) knowledge triples sharing the same S (i.e., molecule) to enhance
learning the semantic relationships between physicochemical knowledge and
molecules. By introducing different molecular knowledge and noises, the meta
language paradigm generates tens of thousands of pretraining tasks. By
recovering the token/sequence/order-level noises, MolMetaLM exhibits
proficiency in large-scale benchmark evaluations involving property prediction,
molecule generation, conformation inference, and molecular optimization.
Through MolMetaLM, we offer a new insight for designing language models.",2024-11-23,"Yifan Wu, Min Zeng, Yang Li, Yang Zhang, Min Li",http://arxiv.org/pdf/2411.15500v1,cs.CL
Traditional Chinese Medicine Case Analysis System for High-Level Semantic Abstraction: Optimized with Prompt and RAG,"This paper details a technical plan for building a clinical case database for
Traditional Chinese Medicine (TCM) using web scraping. Leveraging multiple
platforms, including 360doc, we gathered over 5,000 TCM clinical cases,
performed data cleaning, and structured the dataset with crucial fields such as
patient details, pathogenesis, syndromes, and annotations. Using the
$Baidu\_ERNIE\_Speed\_128K$ API, we removed redundant information and generated
the final answers through the $DeepSeekv2$ API, outputting results in standard
JSON format. We optimized data recall with RAG and rerank techniques during
retrieval and developed a hybrid matching scheme. By combining two-stage
retrieval method with keyword matching via Jieba, we significantly enhanced the
accuracy of model outputs.",2024-11-23,"Peng Xu, Hongjin Wu, Jinle Wang, Rongjia Lin, Liwei Tan",http://arxiv.org/pdf/2411.15491v1,cs.CL
"Automatic Evaluation for Text-to-image Generation: Task-decomposed Framework, Distilled Training, and Meta-evaluation Benchmark","Driven by the remarkable progress in diffusion models, text-to-image
generation has made significant strides, creating a pressing demand for
automatic quality evaluation of generated images. Current state-of-the-art
automatic evaluation methods heavily rely on Multi-modal Large Language Models
(MLLMs), particularly powerful commercial models like GPT-4o. While these
models are highly effective, their substantial costs limit scalability in
large-scale evaluations. Adopting open-source MLLMs is an alternative; however,
their performance falls short due to significant limitations in processing
multi-modal data compared to commercial MLLMs. To tackle these problems, we
first propose a task decomposition evaluation framework based on GPT-4o to
automatically construct a new training dataset, where the complex evaluation
task is decoupled into simpler sub-tasks, effectively reducing the learning
complexity. Based on this dataset, we design innovative training strategies to
effectively distill GPT-4o's evaluation capabilities into a 7B open-source
MLLM, MiniCPM-V-2.6. Furthermore, to reliably and comprehensively assess prior
works and our proposed model, we manually annotate a meta-evaluation benchmark
that includes chain-of-thought explanations alongside quality scores for
generated images. Experimental results demonstrate that our distilled
open-source MLLM significantly outperforms the current state-of-the-art
GPT-4o-base baseline, VIEScore, with over 4.6\% improvement in Spearman and
Kendall correlations with human judgments.",2024-11-23,"Rong-Cheng Tu, Zi-Ao Ma, Tian Lan, Yuehao Zhao, Heyan Huang, Xian-Ling Mao",http://arxiv.org/pdf/2411.15488v1,cs.CL
"Transition Network Analysis: A Novel Framework for Modeling, Visualizing, and Identifying the Temporal Patterns of Learners and Learning Processes","This paper presents a novel learning analytics method: Transition Network
Analysis (TNA), a method that integrates Stochastic Process Mining and
probabilistic graph representation to model, visualize, and identify transition
patterns in the learning process data. Combining the relational and temporal
aspects into a single lens offers capabilities beyond either framework,
including centralities to capture important learning events, community
detection to identify behavior patterns, and clustering to reveal temporal
patterns. Furthermore, TNA introduces several significance tests that go beyond
either method and add rigor to the analysis. Here, we introduce the theoretical
and mathematical foundations of TNA and we demonstrate the functionalities of
TNA with a case study where students (n=191) engaged in small-group
collaboration to map patterns of group dynamics using the theories of
co-regulation and socially-shared regulated learning. The analysis revealed
that TNA can map the regulatory processes as well as identify important events,
patterns, and clusters. Bootstrap validation established the significant
transitions and eliminated spurious transitions. As such, TNA can capture
learning dynamics and provide a robust framework for investigating the temporal
evolution of learning processes. Future directions include -- inter alia --
expanding estimation methods, reliability assessment, and building longitudinal
TNA.",2024-11-23,"Mohammed Saqr, Sonsoles López-Pernas, Tiina Törmänen, Rogers Kaliisa, Kamila Misiejuk, Santtu Tikka",http://arxiv.org/pdf/2411.15486v2,cs.CL
Seed-Free Synthetic Data Generation Framework for Instruction-Tuning LLMs: A Case Study in Thai,"We present a synthetic data approach for instruction-tuning large language
models (LLMs) for low-resource languages in a data-efficient manner,
specifically focusing on Thai. We identify three key properties that contribute
to the effectiveness of instruction-tuning datasets: fluency, diversity, and
cultural context. We propose a seed-data-free framework for generating
synthetic instruction-tuning data that incorporates these essential properties.
Our framework employs an LLM to generate diverse topics, retrieve relevant
contexts from Wikipedia, and create instructions for various tasks, such as
question answering, summarization, and conversation. The experimental results
show that our best-performing synthetic dataset, which incorporates all three
key properties, achieves competitive performance using only 5,000 instructions
when compared to state-of-the-art Thai LLMs trained on hundreds of thousands of
instructions. Our code and dataset are publicly available at
https://github.com/parinzee/seed-free-synthetic-instruct.",2024-11-23,"Parinthapat Pengpun, Can Udomcharoenchaikit, Weerayut Buaphet, Peerat Limkonchotiwat",http://arxiv.org/pdf/2411.15484v1,cs.CL
Towards Robust Evaluation of Unlearning in LLMs via Data Transformations,"Large Language Models (LLMs) have shown to be a great success in a wide range
of applications ranging from regular NLP-based use cases to AI agents. LLMs
have been trained on a vast corpus of texts from various sources; despite the
best efforts during the data pre-processing stage while training the LLMs, they
may pick some undesirable information such as personally identifiable
information (PII). Consequently, in recent times research in the area of
Machine Unlearning (MUL) has become active, the main idea is to force LLMs to
forget (unlearn) certain information (e.g., PII) without suffering from
performance loss on regular tasks. In this work, we examine the robustness of
the existing MUL techniques for their ability to enable leakage-proof
forgetting in LLMs. In particular, we examine the effect of data transformation
on forgetting, i.e., is an unlearned LLM able to recall forgotten information
if there is a change in the format of the input? Our findings on the TOFU
dataset highlight the necessity of using diverse data formats to quantify
unlearning in LLMs more reliably.",2024-11-23,"Abhinav Joshi, Shaswati Saha, Divyaksh Shukla, Sriram Vema, Harsh Jhamtani, Manas Gaur, Ashutosh Modi",http://arxiv.org/pdf/2411.15477v1,cs.CL
HateDay: Insights from a Global Hate Speech Dataset Representative of a Day on Twitter,"To tackle the global challenge of online hate speech, a large body of
research has developed detection models to flag hate speech in the sea of
online content. Yet, due to systematic biases in evaluation datasets, detection
performance in real-world settings remains unclear, let alone across
geographies. To address this issue, we introduce HateDay, the first global hate
speech dataset representative of social media settings, randomly sampled from
all tweets posted on September 21, 2022 for eight languages and four
English-speaking countries. Using HateDay, we show how the prevalence and
composition of hate speech varies across languages and countries. We also find
that evaluation on academic hate speech datasets overestimates real-world
detection performance, which we find is very low, especially for non-European
languages. We identify several factors explaining poor performance, including
models' inability to distinguish between hate and offensive speech, and the
misalignment between academic target focus and real-world target prevalence. We
finally argue that such low performance renders hate speech moderation with
public detection models unfeasible, even in a human-in-the-loop setting which
we find is prohibitively costly. Overall, we emphasize the need to evaluate
future detection models from academia and platforms in real-world settings to
address this global challenge.",2024-11-23,"Manuel Tonneau, Diyi Liu, Niyati Malhotra, Scott A. Hale, Samuel P. Fraiberger, Victor Orozco-Olvera, Paul Röttger",http://arxiv.org/pdf/2411.15462v1,cs.CL
Efficient Ternary Weight Embedding Model: Bridging Scalability and Performance,"Embedding models have become essential tools in both natural language
processing and computer vision, enabling efficient semantic search,
recommendation, clustering, and more. However, the high memory and
computational demands of full-precision embeddings pose challenges for
deployment in resource-constrained environments, such as real-time
recommendation systems. In this work, we propose a novel finetuning framework
to ternary-weight embedding models, which reduces memory and computational
overhead while maintaining high performance. To apply ternarization to
pre-trained embedding models, we introduce self-taught knowledge distillation
to finalize the ternary-weights of the linear layers. With extensive
experiments on public text and vision datasets, we demonstrated that without
sacrificing effectiveness, the ternarized model consumes low memory usage and
has low latency in the inference stage with great efficiency. In practical
implementations, embedding models are typically integrated with Approximate
Nearest Neighbor (ANN) search. Our experiments combining ternary embedding with
ANN search yielded impressive improvement in both accuracy and computational
efficiency. The repository is available at here.",2024-11-23,"Jiayi Chen, Chen Wu, Shaoqun Zhang, Nan Li, Liangjie Zhang, Qi Zhang",http://arxiv.org/pdf/2411.15438v1,cs.CL
Lifelong Knowledge Editing for Vision Language Models with Low-Rank Mixture-of-Experts,"Model editing aims to correct inaccurate knowledge, update outdated
information, and incorporate new data into Large Language Models (LLMs) without
the need for retraining. This task poses challenges in lifelong scenarios where
edits must be continuously applied for real-world applications. While some
editors demonstrate strong robustness for lifelong editing in pure LLMs, Vision
LLMs (VLLMs), which incorporate an additional vision modality, are not directly
adaptable to existing LLM editors. In this paper, we propose LiveEdit, a
LIfelong Vision language modEl Edit to bridge the gap between lifelong LLM
editing and VLLMs. We begin by training an editing expert generator to
independently produce low-rank experts for each editing instance, with the goal
of correcting the relevant responses of the VLLM. A hard filtering mechanism is
developed to utilize visual semantic knowledge, thereby coarsely eliminating
visually irrelevant experts for input queries during the inference stage of the
post-edited model. Finally, to integrate visually relevant experts, we
introduce a soft routing mechanism based on textual semantic relevance to
achieve multi-expert fusion. For evaluation, we establish a benchmark for
lifelong VLLM editing. Extensive experiments demonstrate that LiveEdit offers
significant advantages in lifelong VLLM editing scenarios. Further experiments
validate the rationality and effectiveness of each module design in LiveEdit.",2024-11-23,"Qizhou Chen, Chengyu Wang, Dakan Wang, Taolin Zhang, Wangyue Li, Xiaofeng He",http://arxiv.org/pdf/2411.15432v2,cs.CL
"Exploring Large Language Models for Multimodal Sentiment Analysis: Challenges, Benchmarks, and Future Directions","Multimodal Aspect-Based Sentiment Analysis (MABSA) aims to extract aspect
terms and their corresponding sentiment polarities from multimodal information,
including text and images. While traditional supervised learning methods have
shown effectiveness in this task, the adaptability of large language models
(LLMs) to MABSA remains uncertain. Recent advances in LLMs, such as Llama2,
LLaVA, and ChatGPT, demonstrate strong capabilities in general tasks, yet their
performance in complex and fine-grained scenarios like MABSA is underexplored.
In this study, we conduct a comprehensive investigation into the suitability of
LLMs for MABSA. To this end, we construct a benchmark to evaluate the
performance of LLMs on MABSA tasks and compare them with state-of-the-art
supervised learning methods. Our experiments reveal that, while LLMs
demonstrate potential in multimodal understanding, they face significant
challenges in achieving satisfactory results for MABSA, particularly in terms
of accuracy and inference time. Based on these findings, we discuss the
limitations of current LLMs and outline directions for future research to
enhance their capabilities in multimodal sentiment analysis.",2024-11-23,Shezheng Song,http://arxiv.org/pdf/2411.15408v1,cs.CL
ML-SPEAK: A Theory-Guided Machine Learning Method for Studying and Predicting Conversational Turn-taking Patterns,"Predicting team dynamics from personality traits remains a fundamental
challenge for the psychological sciences and team-based organizations.
Understanding how team composition generates team processes can significantly
advance team-based research along with providing practical guidelines for team
staffing and training. Although the Input-Process-Output (IPO) model has been
useful for studying these connections, the complex nature of team member
interactions demands a more dynamic approach. We develop a computational model
of conversational turn-taking within self-organized teams that can provide
insight into the relationships between team member personality traits and team
communication dynamics. We focus on turn-taking patterns between team members,
independent of content, which can significantly influence team emergent states
and outcomes while being objectively measurable and quantifiable. As our model
is trained on conversational data from teams of given trait compositions, it
can learn the relationships between individual traits and speaking behaviors
and predict group-wide patterns of communication based on team trait
composition alone. We first evaluate the performance of our model using
simulated data and then apply it to real-world data collected from
self-organized student teams. In comparison to baselines, our model is more
accurate at predicting speaking turn sequences and can reveal new relationships
between team member traits and their communication patterns. Our approach
offers a more data-driven and dynamic understanding of team processes. By
bridging the gap between individual personality traits and team communication
patterns, our model has the potential to inform theories of team processes and
provide powerful insights into optimizing team staffing and training.",2024-11-23,"Lisa R. O'Bryan, Madeline Navarro, Juan Segundo Hevia, Santiago Segarra",http://arxiv.org/pdf/2411.15405v1,cs.CL
A Comparative Analysis of Transformer and LSTM Models for Detecting Suicidal Ideation on Reddit,"Suicide is a critical global health problem involving more than 700,000
deaths yearly, particularly among young adults. Many people express their
suicidal thoughts on social media platforms such as Reddit. This paper
evaluates the effectiveness of the deep learning transformer-based models BERT,
RoBERTa, DistilBERT, ALBERT, and ELECTRA and various Long Short-Term Memory
(LSTM) based models in detecting suicidal ideation from user posts on Reddit.
Toward this objective, we curated an extensive dataset from diverse subreddits
and conducted linguistic, topic modeling, and statistical analyses to ensure
data quality. Our results indicate that each model could reach high accuracy
and F1 scores, but among them, RoBERTa emerged as the most effective model with
an accuracy of 93.22% and F1 score of 93.14%. An LSTM model that uses attention
and BERT embeddings performed as the second best, with an accuracy of 92.65%
and an F1 score of 92.69%. Our findings show that transformer-based models have
the potential to improve suicide ideation detection, thereby providing a path
to develop robust mental health monitoring tools from social media. This
research, therefore, underlines the undeniable prospect of advanced techniques
in Natural Language Processing (NLP) while improving suicide prevention
efforts.",2024-11-23,"Khalid Hasan, Jamil Saquer",http://arxiv.org/pdf/2411.15404v1,cs.CL
ChatBCI: A P300 Speller BCI Leveraging Large Language Models for Improved Sentence Composition in Realistic Scenarios,"P300 speller BCIs allow users to compose sentences by selecting target keys
on a GUI through the detection of P300 component in their EEG signals following
visual stimuli. Most P300 speller BCIs require users to spell words letter by
letter, or the first few initial letters, resulting in high keystroke demands
that increase time, cognitive load, and fatigue. This highlights the need for
more efficient, user-friendly methods for faster sentence composition. In this
work, we introduce ChatBCI, a P300 speller BCI that leverages the zero-shot
learning capabilities of large language models (LLMs) to suggest words from
user-spelled initial letters or predict the subsequent word(s), reducing
keystrokes and accelerating sentence composition. ChatBCI retrieves word
suggestions through remote queries to the GPT-3.5 API. A new GUI, displaying
GPT-3.5 word suggestions as extra keys is designed. SWLDA is used for the P300
classification. Seven subjects completed two online spelling tasks: 1)
copy-spelling a self-composed sentence using ChatBCI, and 2) improvising a
sentence using ChatBCI's word suggestions. Results demonstrate that in Task 1,
on average, ChatBCI outperforms letter-by-letter BCI spellers, reducing time
and keystrokes by 62.14% and 53.22%, respectively, and increasing information
transfer rate by 198.96%. In Task 2, ChatBCI achieves 80.68% keystroke savings
and a record 8.53 characters/min for typing speed. Overall, ChatBCI, by
employing remote LLM queries, enhances sentence composition in realistic
scenarios, significantly outperforming traditional spellers without requiring
local model training or storage. ChatBCI's (multi-) word predictions, combined
with its new GUI, pave the way for developing next-generation speller BCIs that
are efficient and effective for real-time communication, especially for users
with communication and motor disabilities.",2024-11-23,"Jiazhen Hong, Weinan Wang, Laleh Najafizadeh",http://arxiv.org/pdf/2411.15395v1,cs.CL
From Jack of All Trades to Master of One: Specializing LLM-based Autoraters to a Test Set,"As LLMs continue to become more powerful and versatile, human evaluation has
quickly become intractable at scale and reliance on automatic metrics has
become the norm. Recently, it has been shown that LLMs are themselves
state-of-the-art evaluators for many tasks. These Autoraters are typically
designed so that they generalize to new systems and test sets. In practice,
however, evaluation is performed on a small set of fixed, canonical test sets,
which are carefully curated to measure certain capabilities of interest and are
not changed frequently. In this work, we design a method which specializes a
prompted Autorater to a given test set, by leveraging historical ratings on the
test set to construct in-context learning (ICL) examples. We evaluate our
Specialist method on the task of fine-grained machine translation evaluation,
and show that it dramatically outperforms the state-of-the-art XCOMET metric by
54% and 119% on the WMT'23 and WMT'24 test sets, respectively. We perform
extensive analyses to understand the representations learned by our Specialist
metrics, and how variability in rater behavior affects their performance. We
also verify the generalizability and robustness of our Specialist method for
designing automatic metrics across different numbers of ICL examples, LLM
backbones, systems to evaluate, and evaluation tasks.",2024-11-23,"Mara Finkelstein, Dan Deutsch, Parker Riley, Juraj Juraska, Geza Kovacs, Markus Freitag",http://arxiv.org/pdf/2411.15387v2,cs.CL
On the Impact of Fine-Tuning on Chain-of-Thought Reasoning,"Large language models have emerged as powerful tools for general
intelligence, showcasing advanced natural language processing capabilities that
find applications across diverse domains. Despite their impressive performance,
recent studies have highlighted the potential for significant enhancements in
LLMs' task-specific performance through fine-tuning strategies like
Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning
(SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works
have shown that while fine-tuning offers significant performance gains, it also
leads to challenges such as catastrophic forgetting and privacy and safety
risks. To this end, there has been little to no work in \textit{understanding
the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research
investigates the effect of fine-tuning on the reasoning abilities of LLMs,
addressing critical questions regarding the impact of task-specific fine-tuning
on overall reasoning capabilities, the influence of fine-tuning on
Chain-of-Thought (CoT) reasoning performance, and the implications for the
faithfulness of CoT reasonings. By exploring these dimensions, our study shows
the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness
of CoT reasoning, on average across four datasets, decreases, highlighting
potential shifts in internal mechanisms of the LLMs resulting from fine-tuning
processes.",2024-11-22,"Elita Lobo, Chirag Agarwal, Himabindu Lakkaraju",http://arxiv.org/pdf/2411.15382v2,cs.CL
"Transforming NLU with Babylon: A Case Study in Development of Real-time, Edge-Efficient, Multi-Intent Translation System for Automated Drive-Thru Ordering","Real-time conversational AI agents face challenges in performing Natural
Language Understanding (NLU) in dynamic, outdoor environments like automated
drive-thru systems. These settings require NLU models to handle background
noise, diverse accents, and multi-intent queries while operating under strict
latency and memory constraints on edge devices. Additionally, robustness to
errors from upstream Automatic Speech Recognition (ASR) is crucial, as ASR
outputs in these environments are often noisy. We introduce Babylon, a
transformer-based architecture that tackles NLU as an intent translation task,
converting natural language inputs into sequences of regular language units
('transcodes') that encode both intents and slot information. This formulation
allows Babylon to manage multi-intent scenarios in a single dialogue turn.
Furthermore, Babylon incorporates an LSTM-based token pooling mechanism to
preprocess phoneme sequences, reducing input length and optimizing for
low-latency, low-memory edge deployment. This also helps mitigate inaccuracies
in ASR outputs, enhancing system robustness. While this work focuses on
drive-thru ordering, Babylon's design extends to similar noise-prone scenarios,
for e.g. ticketing kiosks. Our experiments show that Babylon achieves
significantly better accuracy-latency-memory footprint trade-offs over
typically employed NMT models like Flan-T5 and BART, demonstrating its
effectiveness for real-time NLU in edge deployment settings.",2024-11-22,"Mostafa Varzaneh, Pooja Voladoddi, Tanmay Bakshi, Uma Gunturi",http://arxiv.org/pdf/2411.15372v1,cs.CL
Exploring Facets of Language Generation in the Limit,"The recent work of Kleinberg & Mullainathan [KM24] provides a concrete model
for language generation in the limit: given a sequence of examples from an
unknown target language, the goal is to generate new examples from the target
language such that no incorrect examples are generated beyond some point. In
sharp contrast to strong negative results for the closely related problem of
language identification, they establish positive results for language
generation in the limit for all countable collections of languages. Follow-up
work by Raman & Tewari [RT24] studies bounds on the number of distinct inputs
required by an algorithm before correct language generation is achieved --
namely, whether this is a constant for all languages in the collection (uniform
generation) or a language-dependent constant (non-uniform generation).
  We show that every countable language collection has a generator which has
the stronger property of non-uniform generation in the limit. However, while
the generation algorithm of [KM24] can be implemented using membership queries,
we show that any algorithm cannot non-uniformly generate even for collections
of just two languages, using only membership queries.
  We also formalize the tension between validity and breadth in the generation
algorithm of [KM24] by introducing a definition of exhaustive generation, and
show a strong negative result for exhaustive generation. Our result shows that
a tradeoff between validity and breadth is inherent for generation in the
limit. We also provide a precise characterization of the language collections
for which exhaustive generation is possible. Finally, inspired by algorithms
that can choose to obtain feedback, we consider a model of uniform generation
with feedback, completely characterizing language collections for which such
uniform generation with feedback is possible in terms of a complexity measure
of the collection.",2024-11-22,"Moses Charikar, Chirag Pabbaraju",http://arxiv.org/pdf/2411.15364v2,cs.CL
PPLqa: An Unsupervised Information-Theoretic Quality Metric for Comparing Generative Large Language Models,"We propose PPLqa, an easy to compute, language independent,
information-theoretic metric to measure the quality of responses of generative
Large Language Models (LLMs) in an unsupervised way, without requiring ground
truth annotations or human supervision. The method and metric enables users to
rank generative language models for quality of responses, so as to make a
selection of the best model for a given task. Our single metric assesses LLMs
with an approach that subsumes, but is not explicitly based on, coherence and
fluency (quality of writing) and relevance and consistency (appropriateness of
response) to the query. PPLqa performs as well as other related metrics, and
works better with long-form Q\&A. Thus, PPLqa enables bypassing the lengthy
annotation process required for ground truth evaluations, and it also
correlates well with human and LLM rankings.",2024-11-22,"Gerald Friedland, Xin Huang, Yueying Cui, Vishaal Kapoor, Ashish Khetan, Sanjiv Das",http://arxiv.org/pdf/2411.15320v1,cs.CL
MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs,"As a prominent direction of Artificial General Intelligence (AGI), Multimodal
Large Language Models (MLLMs) have garnered increased attention from both
industry and academia. Building upon pre-trained LLMs, this family of models
further develops multimodal perception and reasoning capabilities that are
impressive, such as writing code given a flow chart or creating stories based
on an image. In the development process, evaluation is critical since it
provides intuitive feedback and guidance on improving models. Distinct from the
traditional train-eval-test paradigm that only favors a single task like image
classification, the versatility of MLLMs has spurred the rise of various new
benchmarks and evaluation methods. In this paper, we aim to present a
comprehensive survey of MLLM evaluation, discussing four key aspects: 1) the
summarised benchmarks types divided by the evaluation capabilities, including
foundation capabilities, model self-analysis, and extented applications; 2) the
typical process of benchmark counstruction, consisting of data collection,
annotation, and precautions; 3) the systematic evaluation manner composed of
judge, metric, and toolkit; 4) the outlook for the next benchmark. This work
aims to offer researchers an easy grasp of how to effectively evaluate MLLMs
according to different needs and to inspire better evaluation methods, thereby
driving the progress of MLLM research.",2024-11-22,"Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, Caifeng Shan, Ran He",http://arxiv.org/pdf/2411.15296v2,cs.CL
Measuring Bullshit in the Language Games played by ChatGPT,"Generative large language models (LLMs), which create text without direct
correspondence to truth value, are widely understood to resemble the uses of
language described in Frankfurt's popular monograph On Bullshit. In this paper,
we offer a rigorous investigation of this topic, identifying how the phenomenon
has arisen, and how it might be analysed. In this paper, we elaborate on this
argument to propose that LLM-based chatbots play the 'language game of
bullshit'. We use statistical text analysis to investigate the features of this
Wittgensteinian language game, based on a dataset constructed to contrast the
language of 1,000 scientific publications with typical pseudo-scientific text
generated by ChatGPT. We then explore whether the same language features can be
detected in two well-known contexts of social dysfunction: George Orwell's
critique of politics and language, and David Graeber's characterisation of
bullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a
statistical model of the language of bullshit can reliably relate the
Frankfurtian artificial bullshit of ChatGPT to the political and workplace
functions of bullshit as observed in natural human language.",2024-11-22,"Alessandro Trevisan, Harry Giddens, Sarah Dillon, Alan F. Blackwell",http://arxiv.org/pdf/2411.15129v1,cs.CL
Tulu 3: Pushing Frontiers in Open Language Model Post-Training,"Language model post-training is applied to refine behaviors and unlock new
skills across a wide range of recent language models, but open recipes for
applying these techniques lag behind proprietary ones. The underlying training
data and recipes for post-training are simultaneously the most important pieces
of the puzzle and the portion with the least transparency. To bridge this gap,
we introduce Tulu 3, a family of fully-open state-of-the-art post-trained
models, alongside its data, code, and training recipes, serving as a
comprehensive guide for modern post-training techniques. Tulu 3, which builds
on Llama 3.1 base models, achieves results surpassing the instruct versions of
Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and
Claude 3.5-Haiku. The training algorithms for our models include supervised
finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we
call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we
introduce a multi-task evaluation scheme for post-training recipes with
development and unseen evaluations, standard benchmark implementations, and
substantial decontamination of existing open datasets on said benchmarks. We
conclude with analysis and discussion of training methods that did not reliably
improve performance.
  In addition to the Tulu 3 model weights and demo, we release the complete
recipe -- including datasets for diverse core skills, a robust toolkit for data
curation and evaluation, the training code and infrastructure, and, most
importantly, a detailed report for reproducing and further adapting the Tulu 3
approach to more domains.",2024-11-22,"Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, Hannaneh Hajishirzi",http://arxiv.org/pdf/2411.15124v5,cs.CL
ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation,"AI-driven models have demonstrated significant potential in automating
radiology report generation for chest X-rays. However, there is no standardized
benchmark for objectively evaluating their performance. To address this, we
present ReXrank, https://rexrank.ai, a public leaderboard and challenge for
assessing AI-powered radiology report generation. Our framework incorporates
ReXGradient, the largest test dataset consisting of 10,000 studies, and three
public datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generation
assessment. ReXrank employs 8 evaluation metrics and separately assesses models
capable of generating only findings sections and those providing both findings
and impressions sections. By providing this standardized evaluation framework,
ReXrank enables meaningful comparisons of model performance and offers crucial
insights into their robustness across diverse clinical settings. Beyond its
current focus on chest X-rays, ReXrank's framework sets the stage for
comprehensive evaluation of automated reporting across the full spectrum of
medical imaging.",2024-11-22,"Xiaoman Zhang, Hong-Yu Zhou, Xiaoli Yang, Oishi Banerjee, Julián N. Acosta, Josh Miller, Ouwen Huang, Pranav Rajpurkar",http://arxiv.org/pdf/2411.15122v1,cs.CL
VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement,"Recent text-to-video (T2V) diffusion models have demonstrated impressive
generation capabilities across various domains. However, these models often
generate videos that have misalignments with text prompts, especially when the
prompts describe complex scenes with multiple objects and attributes. To
address this, we introduce VideoRepair, a novel model-agnostic, training-free
video refinement framework that automatically identifies fine-grained
text-video misalignments and generates explicit spatial and textual feedback,
enabling a T2V diffusion model to perform targeted, localized refinements.
VideoRepair consists of two stages: In (1) video refinement planning, we first
detect misalignments by generating fine-grained evaluation questions and
answering them using an MLLM. Based on video evaluation outputs, we identify
accurately generated objects and construct localized prompts to precisely
refine misaligned regions. In (2) localized refinement, we enhance video
alignment by 'repairing' the misaligned regions from the original video while
preserving the correctly generated areas. This is achieved by frame-wise region
decomposition using our Region-Preserving Segmentation (RPS) module. On two
popular video generation benchmarks (EvalCrafter and T2V-CompBench),
VideoRepair substantially outperforms recent baselines across various
text-video alignment metrics. We provide a comprehensive analysis of
VideoRepair components and qualitative examples.",2024-11-22,"Daeun Lee, Jaehong Yoon, Jaemin Cho, Mohit Bansal",http://arxiv.org/pdf/2411.15115v2,cs.CL
Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable Diffusion,"As text-to-image models grow increasingly powerful and complex, their
burgeoning size presents a significant obstacle to widespread adoption,
especially on resource-constrained devices. This paper presents a pioneering
study on post-training pruning of Stable Diffusion 2, addressing the critical
need for model compression in text-to-image domain. Our study tackles the
pruning techniques for the previously unexplored multi-modal generation models,
and particularly examines the pruning impact on the textual component and the
image generation component separately. We conduct a comprehensive comparison on
pruning the model or the single component of the model in various sparsities.
Our results yield previously undocumented findings. For example, contrary to
established trends in language model pruning, we discover that simple magnitude
pruning outperforms more advanced techniques in text-to-image context.
Furthermore, our results show that Stable Diffusion 2 can be pruned to 38.5%
sparsity with minimal quality loss, achieving a significant reduction in model
size. We propose an optimal pruning configuration that prunes the text encoder
to 47.5% and the diffusion generator to 35%. This configuration maintains image
generation quality while substantially reducing computational requirements. In
addition, our work uncovers intriguing questions about information encoding in
text-to-image models: we observe that pruning beyond certain thresholds leads
to sudden performance drops (unreadable images), suggesting that specific
weights encode critical semantics information. This finding opens new avenues
for future research in model compression, interoperability, and bias
identification in text-to-image models. By providing crucial insights into the
pruning behavior of text-to-image models, our study lays the groundwork for
developing more efficient and accessible AI-driven image generation systems",2024-11-22,"Samarth N Ramesh, Zhixue Zhao",http://arxiv.org/pdf/2411.15113v1,cs.CL
XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models,"The applications of LLM Agents are becoming increasingly complex and diverse,
leading to a high demand for structured outputs that can be parsed into code,
structured function calls, and embodied agent commands. These developments
bring significant demands for structured generation in LLM inference.
Context-free grammar is a flexible approach to enable structured generation via
constrained decoding. However, executing context-free grammar requires going
through several stack states over all tokens in vocabulary during runtime,
bringing non-negligible overhead for structured generation. In this paper, we
propose XGrammar, a flexible and efficient structure generation engine for
large language models. XGrammar accelerates context-free grammar execution by
dividing the vocabulary into context-independent tokens that can be prechecked
and context-dependent tokens that need to be interpreted during runtime. We
further build transformations to expand the grammar context and reduce the
number of context-independent tokens. Additionally, we build an efficient
persistent stack to accelerate the context-dependent token checks. Finally, we
co-design the grammar engine with LLM inference engine to overlap grammar
computation with GPU executions. Evaluation results show that XGrammar can
achieve up to 100x speedup over existing solutions. Combined with an LLM
inference engine, it can generate near-zero overhead structure generation in
end-to-end low-LLM serving.",2024-11-22,"Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, Tianqi Chen",http://arxiv.org/pdf/2411.15100v3,cs.CL
Context-Aware Multimodal Pretraining,"Large-scale multimodal representation learning successfully optimizes for
zero-shot transfer at test time. Yet the standard pretraining paradigm
(contrastive learning on large amounts of image-text data) does not explicitly
encourage representations to support few-shot adaptation. In this work, we
propose a simple, but carefully designed extension to multimodal pretraining
which enables representations to accommodate additional context. Using this
objective, we show that vision-language models can be trained to exhibit
significantly increased few-shot adaptation: across 21 downstream tasks, we
find up to four-fold improvements in test-time sample efficiency, and average
few-shot adaptation gains of over 5%, while retaining zero-shot generalization
performance across model scales and training durations. In particular, equipped
with simple, training-free, metric-based adaptation mechanisms, our
representations easily surpass more complex and expensive optimization-based
schemes, vastly simplifying generalization to new domains.",2024-11-22,"Karsten Roth, Zeynep Akata, Dima Damen, Ivana Balažević, Olivier J. Hénaff",http://arxiv.org/pdf/2411.15099v1,cs.CL
Instance-Aware Generalized Referring Expression Segmentation,"Recent works on Generalized Referring Expression Segmentation (GRES) struggle
with handling complex expressions referring to multiple distinct objects. This
is because these methods typically employ an end-to-end foreground-background
segmentation and lack a mechanism to explicitly differentiate and associate
different object instances to the text query. To this end, we propose
InstAlign, a method that incorporates object-level reasoning into the
segmentation process. Our model leverages both text and image inputs to extract
a set of object-level tokens that capture both the semantic information in the
input prompt and the objects within the image. By modeling the text-object
alignment via instance-level supervision, each token uniquely represents an
object segment in the image, while also aligning with relevant semantic
information from the text. Extensive experiments on the gRefCOCO and Ref-ZOM
benchmarks demonstrate that our method significantly advances state-of-the-art
performance, setting a new standard for precise and flexible GRES.",2024-11-22,"E-Ro Nguyen, Hieu Le, Dimitris Samaras, Michael Ryoo",http://arxiv.org/pdf/2411.15087v1,cs.CL
Sycophancy in Large Language Models: Causes and Mitigations,"Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of natural language processing tasks. However, their tendency to
exhibit sycophantic behavior - excessively agreeing with or flattering users -
poses significant risks to their reliability and ethical deployment. This paper
provides a technical survey of sycophancy in LLMs, analyzing its causes,
impacts, and potential mitigation strategies. We review recent work on
measuring and quantifying sycophantic tendencies, examine the relationship
between sycophancy and other challenges like hallucination and bias, and
evaluate promising techniques for reducing sycophancy while maintaining model
performance. Key approaches explored include improved training data, novel
fine-tuning methods, post-deployment control mechanisms, and decoding
strategies. We also discuss the broader implications of sycophancy for AI
alignment and propose directions for future research. Our analysis suggests
that mitigating sycophancy is crucial for developing more robust, reliable, and
ethically-aligned language models.",2024-11-22,Lars Malmqvist,http://arxiv.org/pdf/2411.15287v1,cs.CL
Locating the Leading Edge of Cultural Change,"Measures of textual similarity and divergence are increasingly used to study
cultural change. But which measures align, in practice, with social evidence
about change? We apply three different representations of text (topic models,
document embeddings, and word-level perplexity) to three different corpora
(literary studies, economics, and fiction). In every case, works by
highly-cited authors and younger authors are textually ahead of the curve. We
don't find clear evidence that one representation of text is to be preferred
over the others. But alignment with social evidence is strongest when texts are
represented through the top quartile of passages, suggesting that a text's
impact may depend more on its most forward-looking moments than on sustaining a
high level of innovation throughout.",2024-11-22,"Sarah Griebel, Becca Cohen, Lucian Li, Jaihyun Park, Jiayu Liu, Jana Perkins, Ted Underwood",http://arxiv.org/pdf/2411.15068v1,cs.CL
Fantastic Biases (What are They) and Where to Find Them,"Deep Learning models tend to learn correlations of patterns on huge datasets.
The bigger these systems are, the more complex are the phenomena they can
detect, and the more data they need for this. The use of Artificial
Intelligence (AI) is becoming increasingly ubiquitous in our society, and its
impact is growing everyday. The promises it holds strongly depend on their fair
and universal use, such as access to information or education for all. In a
world of inequalities, they can help to reach the most disadvantaged areas.
However, such a universal systems must be able to represent society, without
benefiting some at the expense of others. We must not reproduce the
inequalities observed throughout the world, but educate these IAs to go beyond
them. We have seen cases where these systems use gender, race, or even class
information in ways that are not appropriate for resolving their tasks. Instead
of real causal reasoning, they rely on spurious correlations, which is what we
usually call a bias. In this paper, we first attempt to define what is a bias
in general terms. It helps us to demystify the concept of bias, to understand
why we can find them everywhere and why they are sometimes useful. Second, we
focus over the notion of what is generally seen as negative bias, the one we
want to avoid in machine learning, before presenting a general zoology
containing the most common of these biases. We finally conclude by looking at
classical methods to detect them, by means of specially crafted datasets of
templates and specific algorithms, and also classical methods to mitigate them.",2024-11-22,Valentin Barriere,http://arxiv.org/pdf/2411.15051v1,cs.CL
mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for Knowledge-Based VQA,"Advanced Multimodal Large Language Models (MLLMs) struggle with recent
Knowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their
limited and frozen knowledge scope, often leading to ambiguous and inaccurate
responses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally
introduced to provide MLLMs with comprehensive and up-to-date knowledge,
effectively expanding the knowledge scope. However, current mRAG methods have
inherent drawbacks, including: 1) Performing retrieval even when external
knowledge is not needed. 2) Lacking of identification of evidence that supports
the query. 3) Increasing model complexity due to additional information
filtering modules or rules. To address these shortcomings, we propose a novel
generalized framework called \textbf{m}ultimodal
\textbf{R}etrieval-\textbf{R}eflection-\textbf{A}ugmented \textbf{G}eneration
(mR$^2$AG), which achieves adaptive retrieval and useful information
localization to enable answers through two easy-to-implement reflection
operations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflection
is designed to distinguish different user queries and avoids redundant
retrieval calls, and Relevance-Reflection is introduced to guide the MLLM in
locating beneficial evidence of the retrieved content and generating answers
accordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLM
with efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset
(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,
GPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while
maintaining the exceptional capabilities of base MLLMs across a wide range of
Visual-dependent tasks.",2024-11-22,"Tao Zhang, Ziqi Zhang, Zongyang Ma, Yuxin Chen, Zhongang Qi, Chunfeng Yuan, Bing Li, Junfu Pu, Yuxuan Zhao, Zehua Xie, Jin Ma, Ying Shan, Weiming Hu",http://arxiv.org/pdf/2411.15041v1,cs.CL
Evolutionary Automata and Deep Evolutionary Computation,"Evolution by natural selection, which is one of the most compelling themes of
modern science, brought forth evolutionary algorithms and evolutionary
computation, applying mechanisms of evolution in nature to various problems
solved by computers. In this paper we concentrate on evolutionary automata that
constitute an analogous model of evolutionary computation compared to
well-known evolutionary algorithms. Evolutionary automata provide a more
complete dual model of evolutionary computation, similar like abstract automata
(e.g., Turing machines) form a more formal and precise model compared to
recursive algorithms and their subset - evolutionary algorithms. An
evolutionary automaton is an automaton that evolves performing evolutionary
computation perhaps using an infinite number of generations. This model allows
for a direct modeling evolution of evolution, and leads to tremendous
expressiveness of evolutionary automata and evolutionary computation. This also
gives the hint to the power of natural evolution that is self-evolving by
interactive feedback with the environment.",2024-11-22,Eugene Eberbach,http://arxiv.org/pdf/2411.15008v1,cs.CL
ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data,"Large Language Model (LLM) agents are rapidly improving to handle
increasingly complex web-based tasks. Most of these agents rely on
general-purpose, proprietary models like GPT-4 and focus on designing better
prompts to improve their planning abilities. However, general-purpose LLMs are
not specifically trained to understand specialized web contexts such as HTML,
and they often struggle with long-horizon planning. We explore an alternative
approach that fine-tunes open-source LLMs using production-scale workflow data
collected from over 250 domains corresponding to 6 billion tokens. This simple
yet effective approach shows substantial gains over prompting-based agents on
existing benchmarks -- ScribeAgent achieves state-of-the-art direct generation
performance on Mind2Web and improves the task success rate by 7.3% over the
previous best text-only web agents on WebArena. We further perform detailed
ablation studies on various fine-tuning design choices and provide insights
into LLM selection, training recipes, context window optimization, and effect
of dataset sizes.",2024-11-22,"Junhong Shen, Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny, Ameet Talwalkar",http://arxiv.org/pdf/2411.15004v2,cs.CL
Large Multi-modal Models Can Interpret Features in Large Multi-modal Models,"Recent advances in Large Multimodal Models (LMMs) lead to significant
breakthroughs in both academia and industry. One question that arises is how
we, as humans, can understand their internal neural representations. This paper
takes an initial step towards addressing this question by presenting a
versatile framework to identify and interpret the semantics within LMMs.
Specifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the
representations into human understandable features. 2) We then present an
automatic interpretation framework to interpreted the open-semantic features
learned in SAE by the LMMs themselves. We employ this framework to analyze the
LLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these
features can effectively steer the model's behavior. Our results contribute to
a deeper understanding of why LMMs excel in specific tasks, including EQ tests,
and illuminate the nature of their mistakes along with potential strategies for
their rectification. These findings offer new insights into the internal
mechanisms of LMMs and suggest parallels with the cognitive processes of the
human brain.",2024-11-22,"Kaichen Zhang, Yifei Shen, Bo Li, Ziwei Liu",http://arxiv.org/pdf/2411.14982v1,cs.CL
SwissADT: An Audio Description Translation System for Swiss Languages,"Audio description (AD) is a crucial accessibility service provided to blind
persons and persons with visual impairment, designed to convey visual
information in acoustic form. Despite recent advancements in multilingual
machine translation research, the lack of well-crafted and time-synchronized AD
data impedes the development of audio description translation (ADT) systems
that address the needs of multilingual countries such as Switzerland.
Furthermore, since the majority of ADT systems rely solely on text, uncertainty
exists as to whether incorporating visual information from the corresponding
video clips can enhance the quality of ADT outputs. In this work, we present
SwissADT, the first ADT system implemented for three main Swiss languages and
English. By collecting well-crafted AD data augmented with video clips in
German, French, Italian, and English, and leveraging the power of Large
Language Models (LLMs), we aim to enhance information accessibility for diverse
language populations in Switzerland by automatically translating AD scripts to
the desired Swiss language. Our extensive experimental ADT results, composed of
both automatic and human evaluations of ADT quality, demonstrate the promising
capability of SwissADT for the ADT task. We believe that combining human
expertise with the generation power of LLMs can further enhance the performance
of ADT systems, ultimately benefiting a larger multilingual target population.",2024-11-22,"Lukas Fischer, Yingqiang Gao, Alexa Lintner, Sarah Ebling",http://arxiv.org/pdf/2411.14967v1,cs.CL
LLM for Barcodes: Generating Diverse Synthetic Data for Identity Documents,"Accurate barcode detection and decoding in Identity documents is crucial for
applications like security, healthcare, and education, where reliable data
extraction and verification are essential. However, building robust detection
models is challenging due to the lack of diverse, realistic datasets an issue
often tied to privacy concerns and the wide variety of document formats.
Traditional tools like Faker rely on predefined templates, making them less
effective for capturing the complexity of real-world identity documents. In
this paper, we introduce a new approach to synthetic data generation that uses
LLMs to create contextually rich and realistic data without relying on
predefined field. Using the vast knowledge LLMs have about different documents
and content, our method creates data that reflects the variety found in real
identity documents. This data is then encoded into barcode and overlayed on
templates for documents such as Driver's licenses, Insurance cards, Student
IDs. Our approach simplifies the process of dataset creation, eliminating the
need for extensive domain knowledge or predefined fields. Compared to
traditional methods like Faker, data generated by LLM demonstrates greater
diversity and contextual relevance, leading to improved performance in barcode
detection models. This scalable, privacy-first solution is a big step forward
in advancing machine learning for automated document processing and identity
verification.",2024-11-22,"Hitesh Laxmichand Patel, Amit Agarwal, Bhargava Kumar, Karan Gupta, Priyaranjan Pattnayak",http://arxiv.org/pdf/2411.14962v2,cs.CL
Information Extraction from Heterogeneous Documents without Ground Truth Labels using Synthetic Label Generation and Knowledge Distillation,"Invoices and receipts submitted by employees are visually rich documents
(VRDs) with textual, visual and layout information. To protect against the risk
of fraud and abuse, it is crucial for organizations to efficiently extract
desired information from submitted receipts. This helps in the assessment of
key factors such as appropriateness of the expense claim, adherence to spending
and transaction policies, the validity of the receipt, as well as downstream
anomaly detection at various levels. These documents are heterogeneous, with
multiple formats and languages, uploaded with different image qualities, and
often do not contain ground truth labels for the efficient training of models.
In this paper we propose Task Aware Instruction-based Labelling (TAIL), a
method for synthetic label generation in VRD corpuses without labels, and
fine-tune a multimodal Visually Rich Document Understanding Model (VRDU) on
TAIL labels using response-based knowledge distillation without using the
teacher model's weights or training dataset to conditionally generate
annotations in the appropriate format. Using a benchmark external dataset where
ground truth labels are available, we demonstrate conditions under which our
approach performs at par with Claude 3 Sonnet through empirical studies. We
then show that the resulting model performs at par or better on the internal
expense documents of a large multinational organization than state-of-the-art
LMM (large multimodal model) Claude 3 Sonnet while being 85% less costly and
~5X faster, and outperforms layout-aware baselines by more than 10% in Average
Normalized Levenshtein Similarity (ANLS) scores due to its ability to reason
and extract information from rare formats. Finally, we illustrate the usage of
our approach in overpayment prevention.",2024-11-22,"Aniket Bhattacharyya, Anurag Tripathi",http://arxiv.org/pdf/2411.14957v2,cs.CL
A Brief Summary of Explanatory Virtues,"In this report, I provide a brief summary of the literature in philosophy,
psychology and cognitive science about Explanatory Virtues, and link these
concepts to eXplainable AI.",2024-11-22,Ingrid Zukerman,http://arxiv.org/pdf/2411.16709v1,cs.CL
BanglaEmbed: Efficient Sentence Embedding Models for a Low-Resource Language Using Cross-Lingual Distillation Techniques,"Sentence-level embedding is essential for various tasks that require
understanding natural language. Many studies have explored such embeddings for
high-resource languages like English. However, low-resource languages like
Bengali (a language spoken by almost two hundred and thirty million people) are
still under-explored. This work introduces two lightweight sentence
transformers for the Bangla language, leveraging a novel cross-lingual
knowledge distillation approach. This method distills knowledge from a
pre-trained, high-performing English sentence transformer. Proposed models are
evaluated across multiple downstream tasks, including paraphrase detection,
semantic textual similarity (STS), and Bangla hate speech detection. The new
method consistently outperformed existing Bangla sentence transformers.
Moreover, the lightweight architecture and shorter inference time make the
models highly suitable for deployment in resource-constrained environments,
making them valuable for practical NLP applications in low-resource languages.",2024-11-22,"Muhammad Rafsan Kabir, Md. Mohibur Rahman Nabil, Mohammad Ashrafuzzaman Khan",http://arxiv.org/pdf/2411.15270v1,cs.CL
ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos,"Large language models (LLMs) excel at retrieving information from lengthy
text, but their vision-language counterparts (VLMs) face difficulties with
hour-long videos, especially for temporal grounding. Specifically, these VLMs
are constrained by frame limitations, often losing essential temporal details
needed for accurate event localization in extended video content. We propose
ReVisionLLM, a recursive vision-language model designed to locate events in
hour-long videos. Inspired by human search strategies, our model initially
targets broad segments of interest, progressively revising its focus to
pinpoint exact temporal boundaries. Our model can seamlessly handle videos of
vastly different lengths, from minutes to hours. We also introduce a
hierarchical training strategy that starts with short clips to capture distinct
events and progressively extends to longer videos. To our knowledge,
ReVisionLLM is the first VLM capable of temporal grounding in hour-long videos,
outperforming previous state-of-the-art methods across multiple datasets by a
significant margin (+2.6% R1@0.1 on MAD). The code is available at
https://github.com/Tanveer81/ReVisionLLM.",2024-11-22,"Tanveer Hannan, Md Mohaiminul Islam, Jindong Gu, Thomas Seidl, Gedas Bertasius",http://arxiv.org/pdf/2411.14901v1,cs.CL
Evaluating LLM Prompts for Data Augmentation in Multi-label Classification of Ecological Texts,"Large language models (LLMs) play a crucial role in natural language
processing (NLP) tasks, improving the understanding, generation, and
manipulation of human language across domains such as translating, summarizing,
and classifying text. Previous studies have demonstrated that instruction-based
LLMs can be effectively utilized for data augmentation to generate diverse and
realistic text samples. This study applied prompt-based data augmentation to
detect mentions of green practices in Russian social media. Detecting green
practices in social media aids in understanding their prevalence and helps
formulate recommendations for scaling eco-friendly actions to mitigate
environmental issues. We evaluated several prompts for augmenting texts in a
multi-label classification task, either by rewriting existing datasets using
LLMs, generating new data, or combining both approaches. Our results revealed
that all strategies improved classification performance compared to the models
fine-tuned only on the original dataset, outperforming baselines in most cases.
The best results were obtained with the prompt that paraphrased the original
text while clearly indicating the relevant categories.",2024-11-22,"Anna Glazkova, Olga Zakharova",http://arxiv.org/pdf/2411.14896v1,cs.CL
ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models,"Despite the recent breakthroughs achieved by Large Vision Language Models
(LVLMs) in understanding and responding to complex visual-textual contexts,
their inherent hallucination tendencies limit their practical application in
real-world scenarios that demand high levels of precision. Existing methods
typically either fine-tune the LVLMs using additional data, which incurs extra
costs in manual annotation and computational resources or perform comparisons
at the decoding stage, which may eliminate useful language priors for reasoning
while introducing inference time overhead. Therefore, we propose ICT, a
lightweight, training-free method that calculates an intervention direction to
shift the model's focus towards different levels of visual information,
enhancing its attention to high-level and fine-grained visual details. During
the forward pass stage, the intervention is applied to the attention heads that
encode the overall image information and the fine-grained object details,
effectively mitigating the phenomenon of overly language priors, and thereby
alleviating hallucinations. Extensive experiments demonstrate that ICT achieves
strong performance with a small amount of data and generalizes well across
different datasets and models. Our code will be public.",2024-11-22,"Junzhe Chen, Tianshu Zhang, Shiyu Huang, Yuwei Niu, Linfeng Zhang, Lijie Wen, Xuming Hu",http://arxiv.org/pdf/2411.15268v1,cs.CL
Leveraging Hierarchical Prototypes as the Verbalizer for Implicit Discourse Relation Recognition,"Implicit discourse relation recognition involves determining relationships
that hold between spans of text that are not linked by an explicit discourse
connective. In recent years, the pre-train, prompt, and predict paradigm has
emerged as a promising approach for tackling this task. However, previous work
solely relied on manual verbalizers for implicit discourse relation
recognition, which suffer from issues of ambiguity and even incorrectness. To
overcome these limitations, we leverage the prototypes that capture certain
class-level semantic features and the hierarchical label structure for
different classes as the verbalizer. We show that our method improves on
competitive baselines. Besides, our proposed approach can be extended to enable
zero-shot cross-lingual learning, facilitating the recognition of discourse
relations in languages with scarce resources. These advancement validate the
practicality and versatility of our approach in addressing the issues of
implicit discourse relation recognition across different languages.",2024-11-22,"Wanqiu Long, Bonnie Webber",http://arxiv.org/pdf/2411.14880v1,cs.CL
Astro-HEP-BERT: A bidirectional language model for studying the meanings of concepts in astrophysics and high energy physics,"I present Astro-HEP-BERT, a transformer-based language model specifically
designed for generating contextualized word embeddings (CWEs) to study the
meanings of concepts in astrophysics and high-energy physics. Built on a
general pretrained BERT model, Astro-HEP-BERT underwent further training over
three epochs using the Astro-HEP Corpus, a dataset I curated from 21.84 million
paragraphs extracted from more than 600,000 scholarly articles on arXiv, all
belonging to at least one of these two scientific domains. The project
demonstrates both the effectiveness and feasibility of adapting a bidirectional
transformer for applications in the history, philosophy, and sociology of
science (HPSS). The entire training process was conducted using freely
available code, pretrained weights, and text inputs, completed on a single
MacBook Pro Laptop (M2/96GB). Preliminary evaluations indicate that
Astro-HEP-BERT's CWEs perform comparably to domain-adapted BERT models trained
from scratch on larger datasets for domain-specific word sense disambiguation
and induction and related semantic change analyses. This suggests that
retraining general language models for specific scientific domains can be a
cost-effective and efficient strategy for HPSS researchers, enabling high
performance without the need for extensive training from scratch.",2024-11-22,Arno Simons,http://arxiv.org/pdf/2411.14877v1,cs.CL
Preference Alignment for Diffusion Model via Explicit Denoised Distribution Estimation,"Diffusion models have shown remarkable success in text-to-image generation,
making preference alignment for these models increasingly important. The
preference labels are typically available only at the terminal of denoising
trajectories, which poses challenges in optimizing the intermediate denoising
steps. In this paper, we propose to conduct Denoised Distribution Estimation
(DDE) that explicitly connects intermediate steps to the terminal denoised
distribution. Therefore, preference labels can be used for the entire
trajectory optimization. To this end, we design two estimation strategies for
our DDE. The first is stepwise estimation, which utilizes the conditional
denoised distribution to estimate the model denoised distribution. The second
is single-shot estimation, which converts the model output into the terminal
denoised distribution via DDIM modeling. Analytically and empirically, we
reveal that DDE equipped with two estimation strategies naturally derives a
novel credit assignment scheme that prioritizes optimizing the middle part of
the denoising trajectory. Extensive experiments demonstrate that our approach
achieves superior performance, both quantitatively and qualitatively.",2024-11-22,"Dingyuan Shi, Yong Wang, Hangyu Li, Xiangxiang Chu",http://arxiv.org/pdf/2411.14871v3,cs.CL
VisGraphVar: A Benchmark Generator for Assessing Variability in Graph Analysis Using Large Vision-Language Models,"The fast advancement of Large Vision-Language Models (LVLMs) has shown
immense potential. These models are increasingly capable of tackling abstract
visual tasks. Geometric structures, particularly graphs with their inherent
flexibility and complexity, serve as an excellent benchmark for evaluating
these models' predictive capabilities. While human observers can readily
identify subtle visual details and perform accurate analyses, our investigation
reveals that state-of-the-art LVLMs exhibit consistent limitations in specific
visual graph scenarios, especially when confronted with stylistic variations.
In response to these challenges, we introduce VisGraphVar (Visual Graph
Variability), a customizable benchmark generator able to produce graph images
for seven distinct task categories (detection, classification, segmentation,
pattern recognition, link prediction, reasoning, matching), designed to
systematically evaluate the strengths and limitations of individual LVLMs. We
use VisGraphVar to produce 990 graph images and evaluate six LVLMs, employing
two distinct prompting strategies, namely zero-shot and chain-of-thought. The
findings demonstrate that variations in visual attributes of images (e.g., node
labeling and layout) and the deliberate inclusion of visual imperfections, such
as overlapping nodes, significantly affect model performance. This research
emphasizes the importance of a comprehensive evaluation across graph-related
tasks, extending beyond reasoning alone. VisGraphVar offers valuable insights
to guide the development of more reliable and robust systems capable of
performing advanced visual graph analysis.",2024-11-22,"Camilo Chacón Sartori, Christian Blum, Filippo Bistaffa",http://arxiv.org/pdf/2411.14832v1,cs.CL
Fine-Grained Alignment in Vision-and-Language Navigation through Bayesian Optimization,"This paper addresses the challenge of fine-grained alignment in
Vision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D
environments based on natural language instructions. Current approaches use
contrastive learning to align language with visual trajectory sequences.
Nevertheless, they encounter difficulties with fine-grained vision negatives.
To enhance cross-modal embeddings, we introduce a novel Bayesian
Optimization-based adversarial optimization framework for creating fine-grained
contrastive vision samples. To validate the proposed methodology, we conduct a
series of experiments to assess the effectiveness of the enriched embeddings on
fine-grained vision negatives. We conduct experiments on two common VLN
benchmarks R2R and REVERIE, experiments on the them demonstrate that these
embeddings benefit navigation, and can lead to a promising performance
enhancement. Our source code and trained models are available at:
https://anonymous.4open.science/r/FGVLN.",2024-11-22,"Yuhang Song, Mario Gianni, Chenguang Yang, Kunyang Lin, Te-Chuan Chiu, Anh Nguyen, Chun-Yi Lee",http://arxiv.org/pdf/2411.14811v2,cs.CL
Harlequin: Color-driven Generation of Synthetic Data for Referring Expression Comprehension,"Referring Expression Comprehension (REC) aims to identify a particular object
in a scene by a natural language expression, and is an important topic in
visual language understanding. State-of-the-art methods for this task are based
on deep learning, which generally requires expensive and manually labeled
annotations. Some works tackle the problem with limited-supervision learning or
relying on Large Vision and Language Models. However, the development of
techniques to synthesize labeled data is overlooked. In this paper, we propose
a novel framework that generates artificial data for the REC task, taking into
account both textual and visual modalities. At first, our pipeline processes
existing data to create variations in the annotations. Then, it generates an
image using altered annotations as guidance. The result of this pipeline is a
new dataset, called Harlequin, made by more than 1M queries. This approach
eliminates manual data collection and annotation, enabling scalability and
facilitating arbitrary complexity. We pre-train three REC models on Harlequin,
then fine-tuned and evaluated on human-annotated datasets. Our experiments show
that the pre-training on artificial data is beneficial for performance.",2024-11-22,"Luca Parolari, Elena Izzo, Lamberto Ballan",http://arxiv.org/pdf/2411.14807v1,cs.CL
Continual SFT Matches Multimodal RLHF with Negative Supervision,"Multimodal RLHF usually happens after supervised finetuning (SFT) stage to
continually improve vision-language models' (VLMs) comprehension. Conventional
wisdom holds its superiority over continual SFT during this preference
alignment stage. In this paper, we observe that the inherent value of
multimodal RLHF lies in its negative supervision, the logit of the rejected
responses. We thus propose a novel negative supervised finetuning (nSFT)
approach that fully excavates these information resided. Our nSFT disentangles
this negative supervision in RLHF paradigm, and continually aligns VLMs with a
simple SFT loss. This is more memory efficient than multimodal RLHF where 2
(e.g., DPO) or 4 (e.g., PPO) large VLMs are strictly required. The
effectiveness of nSFT is rigorously proved by comparing it with various
multimodal RLHF approaches, across different dataset sources, base VLMs and
evaluation metrics. Besides, fruitful of ablations are provided to support our
hypothesis. We hope this paper will stimulate further research to properly
align large vision language models.",2024-11-22,"Ke Zhu, Yu Wang, Yanpeng Sun, Qiang Chen, Jiangjiang Liu, Gang Zhang, Jingdong Wang",http://arxiv.org/pdf/2411.14797v1,cs.CL
De-biased Multimodal Electrocardiogram Analysis,"Multimodal large language models (MLLMs) are increasingly being applied in
the medical field, particularly in medical imaging. However, developing MLLMs
for ECG signals, which are crucial in clinical settings, has been a significant
challenge beyond medical imaging. Previous studies have attempted to address
this by converting ECGs into several text tags using an external classifier in
a training-free manner. However, this approach significantly compresses the
information in ECGs and underutilizes the reasoning capabilities of LLMs. In
this work, we directly feed the embeddings of ECGs into the LLM through a
projection layer, retaining more information about ECGs and better leveraging
the reasoning abilities of LLMs. Our method can also effectively handle a
common situation in clinical practice where it is necessary to compare two ECGs
taken at different times. Recent studies found that MLLMs may rely solely on
text input to provide answers, ignoring inputs from other modalities. We
analyzed this phenomenon from a causal perspective in the context of ECG MLLMs
and discovered that the confounder, severity of illness, introduces a spurious
correlation between the question and answer, leading the model to rely on this
spurious correlation and ignore the ECG input. Such models do not comprehend
the ECG input and perform poorly in adversarial tests where different
expressions of the same question are used in the training and testing sets. We
designed a de-biased pre-training method to eliminate the confounder's effect
according to the theory of backdoor adjustment. Our model performed well on the
ECG-QA task under adversarial testing and demonstrated zero-shot capabilities.
An interesting random ECG test further validated that our model effectively
understands and utilizes the input ECG signal.",2024-11-22,"Haitao Li, Ziyu Li, Yiheng Mao, Ziyi Liu, Zhoujian Sun, Zhengxing Huang",http://arxiv.org/pdf/2411.14795v1,cs.CL
VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection,"The advancement of Large Vision Language Models (LVLMs) has significantly
improved multimodal understanding, yet challenges remain in video reasoning
tasks due to the scarcity of high-quality, large-scale datasets. Existing video
question-answering (VideoQA) datasets often rely on costly manual annotations
with insufficient granularity or automatic construction methods with redundant
frame-by-frame analysis, limiting their scalability and effectiveness for
complex reasoning. To address these challenges, we introduce VideoEspresso, a
novel dataset that features VideoQA pairs preserving essential spatial details
and temporal coherence, along with multimodal annotations of intermediate
reasoning steps. Our construction pipeline employs a semantic-aware method to
reduce redundancy, followed by generating QA pairs using GPT-4o. We further
develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes,
guiding GPT-4o in extracting logical relationships from QA pairs and video
content. To exploit the potential of high-quality VideoQA pairs, we propose a
Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a
two-stage instruction fine-tuned reasoning LVLM. This framework adaptively
selects core frames and performs CoT reasoning using multimodal evidence.
Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our
method outperforms existing baselines on most tasks, demonstrating superior
video reasoning capabilities. Our code and dataset will be released at:
https://github.com/hshjerry/VideoEspresso",2024-11-22,"Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, Si Liu",http://arxiv.org/pdf/2411.14794v1,cs.CL
TPLogAD: Unsupervised Log Anomaly Detection Based on Event Templates and Key Parameters,"Log-system is an important mechanism for recording the runtime status and
events of Web service systems, and anomaly detection in logs is an effective
method of detecting problems. However, manual anomaly detection in logs is
inefficient, error-prone, and unrealistic. Existing log anomaly detection
methods either use the indexes of event templates, or form vectors by embedding
the fixed string part of the template as a sentence, or use time parameters for
sequence analysis. However, log entries often contain features and semantic
information that cannot be fully represented by these methods, resulting in
missed and false alarms. In this paper, we propose TPLogAD, a universal
unsupervised method for analyzing unstructured logs, which performs anomaly
detection based on event templates and key parameters. The itemplate2vec and
para2vec included in TPLogAD are two efficient and easy-to-implement semantic
representation methods for logs, detecting anomalies in event templates and
parameters respectively, which has not been achieved in previous work.
Additionally, TPLogAD can avoid the interference of log diversity and dynamics
on anomaly detection. Our experiments on four public log datasets show that
TPLogAD outperforms existing log anomaly detection methods.",2024-11-22,"Jiawei Lu, Chengrong Wu",http://arxiv.org/pdf/2411.15250v1,cs.CL
KBAlign: Efficient Self Adaptation on Specific Knowledge Bases,"Although retrieval-augmented generation (RAG) remains essential for
knowledge-based question answering (KBQA), current paradigms face critical
challenges under specific domains. Existing methods struggle with targeted
adaptation on small-scale KBs: vanilla unsupervised training exhibits poor
effectiveness, while fine-tuning incurs prohibitive costs of external signals.
We present KBAlign, a self-supervised framework that enhances RAG systems
through efficient model adaptation. Our key insight is to leverage the model's
intrinsic capabilities for knowledge alignment through two innovative
mechanisms: multi-grained self-annotation that captures global knowledge for
data construction, and iterative tuning that accelerates convergence through
self verification. This framework enables cost-effective model adaptation to
specific textual KBs, without human supervision or external model assistance.
Experiments demonstrate that KBAlign can achieve 90\% of the performance gain
obtained through GPT-4-supervised adaptation, while relying entirely on
self-annotation of much smaller models. KBAlign significantly improves
downstream QA accuracy across multiple domains with tiny costs, particularly
benefiting scenarios requiring deep knowledge integration from specialized
corpora. We release our experimental data, models, and process analyses to the
community for further exploration (https://github.com/thunlp/KBAlign).",2024-11-22,"Zheni Zeng, Yuxuan Chen, Shi Yu, Ruobing Wang, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun",http://arxiv.org/pdf/2411.14790v4,cs.CL
Understanding the Impact of News Articles on the Movement of Market Index: A Case on Nifty 50,"In the recent past, there were several works on the prediction of stock price
using different methods. Sentiment analysis of news and tweets and relating
them to the movement of stock prices have already been explored. But, when we
talk about the news, there can be several topics such as politics, markets,
sports etc. It was observed that most of the prior analyses dealt with news or
comments associated with particular stock prices only or the researchers dealt
with overall sentiment scores only. However, it is quite possible that
different topics having different levels of impact on the movement of the stock
price or an index. The current study focused on bridging this gap by analysing
the movement of Nifty 50 index with respect to the sentiments associated with
news items related to various different topic such as sports, politics, markets
etc. The study established that sentiment scores of news items of different
other topics also have a significant impact on the movement of the index.",2024-11-22,"Subhasis Dasgupta, Pratik Satpati, Ishika Choudhary, Jaydip Sen",http://arxiv.org/pdf/2412.06794v1,cs.CL
IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query Generation for Conversational Search,"The Interactive Knowledge Assistant Track (iKAT) 2024 focuses on advancing
conversational assistants, able to adapt their interaction and responses from
personalized user knowledge. The track incorporates a Personal Textual
Knowledge Base (PTKB) alongside Conversational AI tasks, such as passage
ranking and response generation. Query Rewrite being an effective approach for
resolving conversational context, we explore Large Language Models (LLMs), as
query rewriters. Specifically, our submitted runs explore multi-aspect query
generation using the MQ4CS framework, which we further enhance with Learned
Sparse Retrieval via the SPLADE architecture, coupled with robust cross-encoder
models. We also propose an alternative to the previous interleaving strategy,
aggregating multiple aspects during the reranking phase. Our findings indicate
that multi-aspect query generation is effective in enhancing performance when
integrated with advanced retrieval and reranking models. Our results also lead
the way for better personalization in Conversational Search, relying on LLMs to
integrate personalization within query rewrite, and outperforming human rewrite
performance.",2024-11-22,"Simon Lupart, Zahra Abbasiantaeb, Mohammad Aliannejadi",http://arxiv.org/pdf/2411.14739v1,cs.CL
Universal and Context-Independent Triggers for Precise Control of LLM Outputs,"Large language models (LLMs) have been widely adopted in applications such as
automated content generation and even critical decision-making systems.
However, the risk of prompt injection allows for potential manipulation of LLM
outputs. While numerous attack methods have been documented, achieving full
control over these outputs remains challenging, often requiring experienced
attackers to make multiple attempts and depending heavily on the prompt
context. Recent advancements in gradient-based white-box attack techniques have
shown promise in tasks like jailbreaks and system prompt leaks. Our research
generalizes gradient-based attacks to find a trigger that is (1) Universal:
effective irrespective of the target output; (2) Context-Independent: robust
across diverse prompt contexts; and (3) Precise Output: capable of manipulating
LLM inputs to yield any specified output with high accuracy. We propose a novel
method to efficiently discover such triggers and assess the effectiveness of
the proposed attack. Furthermore, we discuss the substantial threats posed by
such attacks to LLM-based applications, highlighting the potential for
adversaries to taking over the decisions and actions made by AI agents.",2024-11-22,"Jiashuo Liang, Guancheng Li, Yang Yu",http://arxiv.org/pdf/2411.14738v1,cs.CL
Evaluating and Advancing Multimodal Large Language Models in Ability Lens,"As multimodal large language models (MLLMs) advance rapidly, rigorous
evaluation has become essential, providing further guidance for their
development. In this work, we focus on a unified and robust evaluation of
\textbf{vision perception} abilities, the foundational skill of MLLMs. We find
that existing perception benchmarks, each focusing on different question types,
domains, and evaluation metrics, introduce significant evaluation variance,
complicating comprehensive assessments of perception abilities when relying on
any single benchmark. To address this, we introduce \textbf{AbilityLens}, a
unified benchmark designed to evaluate MLLMs across six key perception
abilities, focusing on both accuracy and stability, with each ability
encompassing diverse question types, domains, and metrics. With the assistance
of AbilityLens, we: (1) identify the strengths and weaknesses of current
models, highlighting stability patterns and revealing a notable performance gap
between open-source and closed-source models; (2) introduce an online
evaluation mode, which uncovers interesting ability conflict and early
convergence phenomena during MLLM training; and (3) design a simple
ability-specific model merging method that combines the best ability checkpoint
from early training stages, effectively mitigating performance decline due to
ability conflict. The benchmark and online leaderboard will be released soon.",2024-11-22,"Feng Chen, Chenhui Gou, Jing Liu, Yang Yang, Zhaoyang Li, Jiyuan Zhang, Zhenbang Sun, Bohan Zhuang, Qi Wu",http://arxiv.org/pdf/2411.14725v1,cs.CL
MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts,"Molecule discovery is a pivotal research field, impacting everything from the
medicines we take to the materials we use. Recently, Large Language Models
(LLMs) have been widely adopted in molecule understanding and generation, yet
the alignments between molecules and their corresponding captions remain a
significant challenge. Previous endeavours often treat the molecule as a
general SMILES string or molecular graph, neglecting the fine-grained
alignments between the molecular sub-structures and the descriptive textual
phrases, which are crucial for accurate and explainable predictions. In this
case, we introduce MolReFlect, a novel teacher-student framework designed to
contextually perform the molecule-caption alignments in a fine-grained way. Our
approach initially leverages a larger teacher LLM to label the detailed
alignments by directly extracting critical phrases from molecule captions or
SMILES strings and implying them to corresponding sub-structures or
characteristics. To refine these alignments, we propose In-Context Selective
Reflection, which retrieves previous extraction results as context examples for
teacher LLM to reflect and lets a smaller student LLM select from in-context
reflection and previous extraction results. Finally, we enhance the learning
process of the student LLM through Chain-of-Thought In-Context Molecule Tuning,
integrating the fine-grained alignments and the reasoning processes within the
Chain-of-Thought format. Our experimental results demonstrate that MolReFlect
enables LLMs like Mistral-7B to significantly outperform the previous
baselines, achieving SOTA performance on the ChEBI-20 dataset. This advancement
not only enhances the generative capabilities of LLMs in the molecule-caption
translation task, but also contributes to a more explainable framework.",2024-11-22,"Jiatong Li, Yunqing Liu, Wei Liu, Jingdi Le, Di Zhang, Wenqi Fan, Dongzhan Zhou, Yuqiang Li, Qing Li",http://arxiv.org/pdf/2411.14721v1,cs.CL
Optimizing Social Media Annotation of HPV Vaccine Skepticism and Misinformation Using Large Language Models: An Experimental Evaluation of In-Context Learning and Fine-Tuning Stance Detection Across Multiple Models,"This paper leverages large-language models (LLMs) to experimentally determine
optimal strategies for scaling up social media content annotation for stance
detection on HPV vaccine-related tweets. We examine both conventional
fine-tuning and emergent in-context learning methods, systematically varying
strategies of prompt engineering across widely used LLMs and their variants
(e.g., GPT4, Mistral, and Llama3, etc.). Specifically, we varied prompt
template design, shot sampling methods, and shot quantity to detect stance on
HPV vaccination. Our findings reveal that 1) in general, in-context learning
outperforms fine-tuning in stance detection for HPV vaccine social media
content; 2) increasing shot quantity does not necessarily enhance performance
across models; and 3) different LLMs and their variants present differing
sensitivity to in-context learning conditions. We uncovered that the optimal
in-context learning configuration for stance detection on HPV vaccine tweets
involves six stratified shots paired with detailed contextual prompts. This
study highlights the potential and provides an applicable approach for applying
LLMs to research on social media stance and skepticism detection.",2024-11-22,"Luhang Sun, Varsha Pendyala, Yun-Shiuan Chuang, Shanglin Yang, Jonathan Feldman, Andrew Zhao, Munmun De Choudhury, Sijia Yang, Dhavan Shah",http://arxiv.org/pdf/2411.14720v2,cs.CL
FedMLLM: Federated Fine-tuning MLLM on Multimodal Heterogeneity Data,"Multimodal Large Language Models (MLLMs) have made significant advancements,
demonstrating powerful capabilities in processing and understanding multimodal
data. Fine-tuning MLLMs with Federated Learning (FL) allows for expanding the
training data scope by including private data sources, thereby enhancing their
practical applicability in privacy-sensitive domains. However, current research
remains in the early stage, particularly in addressing the \textbf{multimodal
heterogeneities} in real-world applications. In this paper, we introduce a
benchmark to evaluate the performance of federated fine-tuning of MLLMs across
various multimodal heterogeneous scenarios, laying the groundwork for future
research in the field. Our benchmark includes two lightweight MLLMs, two
downstream tasks, three evaluation metrics, and five datasets across three
domains, along with six comparison baselines, covering over ten types of
modality heterogeneities across four multimodal scenarios. To address the
challenges posed by multimodal heterogeneity, we develop a general FedMLLM
framework that integrates classic FL methods alongside two modality-agnostic
strategies. Extensive experimental results show that our proposed FL paradigm
improves the performance of MLLMs by broadening the range of training data and
mitigating multimodal heterogeneity. Code is available in supplementary
materials.",2024-11-22,"Binqian Xu, Xiangbo Shu, Haiyang Mei, Guosen Xie, Basura Fernando, Jinhui Tang",http://arxiv.org/pdf/2411.14717v2,cs.CL
Understanding LLM Embeddings for Regression,"With the rise of large language models (LLMs) for flexibly processing
information as strings, a natural application is regression, specifically by
preprocessing string representations into LLM embeddings as downstream features
for metric prediction. In this paper, we provide one of the first comprehensive
investigations into embedding-based regression and demonstrate that LLM
embeddings as features can be better for high-dimensional regression tasks than
using traditional feature engineering. This regression performance can be
explained in part due to LLM embeddings over numeric data inherently preserving
Lipschitz continuity over the feature space. Furthermore, we quantify the
contribution of different model effects, most notably model size and language
understanding, which we find surprisingly do not always improve regression
performance.",2024-11-22,"Eric Tang, Bangding Yang, Xingyou Song",http://arxiv.org/pdf/2411.14708v3,cs.CL
Improving Mathematical Reasoning Capabilities of Small Language Models via Feedback-Driven Distillation,"Large Language Models (LLMs) demonstrate exceptional reasoning capabilities,
often achieving state-of-the-art performance in various tasks. However, their
substantial computational and memory demands, due to billions of parameters,
hinder deployment in resource-constrained environments. A promising solution is
knowledge distillation, where LLMs transfer reasoning capabilities to Small
Language Models (SLMs, $\le$ 1B parameters), enabling wider deployment on
low-resource devices. Existing methods primarily focus on generating
high-quality reasoning rationales for distillation datasets but often neglect
the critical role of data quantity and quality. To address these challenges, we
propose a Feedback-Driven Distillation (FDD) framework to enhance SLMs'
mathematical reasoning capabilities. In the initialization stage, a
distillation dataset is constructed by prompting LLMs to pair mathematical
problems with corresponding reasoning rationales. We classify problems into
easy and hard categories based on SLM performance. For easy problems, LLMs
generate more complex variations, while for hard problems, new questions of
similar complexity are synthesized. In addition, we propose a multi-round
distillation paradigm to iteratively enrich the distillation datasets, thereby
progressively improving the mathematical reasoning abilities of SLMs.
Experimental results demonstrate that our method can make SLMs achieve SOTA
mathematical reasoning performance.",2024-11-22,"Xunyu Zhu, Jian Li, Can Ma, Weiping Wang",http://arxiv.org/pdf/2411.14698v1,cs.CL
Bio-inspired AI: Integrating Biological Complexity into Artificial Intelligence,"The pursuit of creating artificial intelligence (AI) mirrors our longstanding
fascination with understanding our own intelligence. From the myths of Talos to
Aristotelian logic and Heron's inventions, we have sought to replicate the
marvels of the mind. While recent advances in AI hold promise, singular
approaches often fall short in capturing the essence of intelligence. This
paper explores how fundamental principles from biological
computation--particularly context-dependent, hierarchical information
processing, trial-and-error heuristics, and multi-scale organization--can guide
the design of truly intelligent systems. By examining the nuanced mechanisms of
biological intelligence, such as top-down causality and adaptive interaction
with the environment, we aim to illuminate potential limitations in artificial
constructs. Our goal is to provide a framework inspired by biological systems
for designing more adaptable and robust artificial intelligent systems.",2024-11-22,"Nima Dehghani, Michael Levin",http://arxiv.org/pdf/2411.15243v1,cs.CL
The Zamba2 Suite: Technical Report,"In this technical report, we present the Zamba2 series -- a suite of 1.2B,
2.7B, and 7.4B parameter hybrid Mamba2-transformer models that achieve state of
the art performance against the leading open-weights models of their class,
while achieving substantial gains in inference latency, throughput, and memory
efficiency. The Zamba2 series builds upon our initial work with Zamba1-7B,
optimizing its architecture, training and annealing datasets, and training for
up to three trillion tokens. We provide open-source weights for all models of
the Zamba2 series as well as instruction-tuned variants that are strongly
competitive against comparable instruct-tuned models of their class. We
additionally open-source the pretraining dataset, which we call Zyda-2, used to
train the Zamba2 series of models. The models and datasets used in this work
are openly available at https://huggingface.co/Zyphra",2024-11-22,"Paolo Glorioso, Quentin Anthony, Yury Tokpanov, Anna Golubeva, Vasudev Shyam, James Whittington, Jonathan Pilault, Beren Millidge",http://arxiv.org/pdf/2411.15242v1,cs.CL
Whats in a Video: Factorized Autoregressive Decoding for Online Dense Video Captioning,"Generating automatic dense captions for videos that accurately describe their
contents remains a challenging area of research. Most current models require
processing the entire video at once. Instead, we propose an efficient, online
approach which outputs frequent, detailed and temporally aligned captions,
without access to future frames. Our model uses a novel autoregressive
factorized decoding architecture, which models the sequence of visual features
for each time segment, outputting localized descriptions and efficiently
leverages the context from the previous video segments. This allows the model
to output frequent, detailed captions to more comprehensively describe the
video, according to its actual local content, rather than mimic the training
data. Second, we propose an optimization for efficient training and inference,
which enables scaling to longer videos. Our approach shows excellent
performance compared to both offline and online methods, and uses 20\% less
compute. The annotations produced are much more comprehensive and frequent, and
can further be utilized in automatic video tagging and in large-scale video
data harvesting.",2024-11-22,"AJ Piergiovanni, Dahun Kim, Michael S. Ryoo, Isaac Noble, Anelia Angelova",http://arxiv.org/pdf/2411.14688v1,cs.CL
Multiverse of Greatness: Generating Story Branches with LLMs,"This paper presents Dynamic Context Prompting/Programming (DCP/P), a novel
framework for interacting with LLMs to generate graph-based content with a
dynamic context window history. While there is an existing study utilizing LLMs
to generate a visual novel game, the previous study involved a manual process
of output extraction and did not provide flexibility in generating a longer,
coherent story. We evaluate DCP/P against our baseline, which does not provide
context history to an LLM and only relies on the initial story data. Through
objective evaluation, we show that simply providing the LLM with a summary
leads to a subpar story compared to additionally providing the LLM with the
proper context of the story. We also provide an extensive qualitative analysis
and discussion. We qualitatively examine the quality of the objectively
best-performing generated game from each approach. In addition, we examine
biases in word choices and word sentiment of the generated content. We find a
consistent observation with previous studies that LLMs are biased towards
certain words, even with a different LLM family. Finally, we provide a
comprehensive discussion on opportunities for future studies.",2024-11-22,"Pittawat Taveekitworachai, Chollakorn Nimpattanavong, Mustafa Can Gursesli, Antonio Lanata, Andrea Guazzini, Ruck Thawonmas",http://arxiv.org/pdf/2411.14672v1,cs.CL
Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis Perspective,"Large Language Models (LLMs) have revolutionized natural language processing
(NLP) by delivering state-of-the-art performance across a variety of tasks.
Among these, Transformer-based models like BERT and GPT rely on pooling layers
to aggregate token-level embeddings into sentence-level representations. Common
pooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in
this aggregation process. Despite their widespread use, the comparative
performance of these strategies on different LLM architectures remains
underexplored. To address this gap, this paper investigates the effects of
these pooling mechanisms on two prominent LLM families -- BERT and GPT, in the
context of sentence-level sentiment analysis. Comprehensive experiments reveal
that each pooling mechanism exhibits unique strengths and weaknesses depending
on the task's specific requirements. Our findings underline the importance of
selecting pooling methods tailored to the demands of particular applications,
prompting a re-evaluation of common assumptions regarding pooling operations.
By offering actionable insights, this study contributes to the optimization of
LLM-based models for downstream tasks.",2024-11-22,"Jinming Xing, Dongwen Luo, Chang Xue, Ruilin Xing",http://arxiv.org/pdf/2411.14654v3,cs.CL
Benchmarking Multimodal Models for Ukrainian Language Understanding Across Academic and Cultural Domains,"While the evaluation of multimodal English-centric models is an active area
of research with numerous benchmarks, there is a profound lack of benchmarks or
evaluation suites for low- and mid-resource languages. We introduce ZNO-Vision,
a comprehensive multimodal Ukrainian-centric benchmark derived from
standardized university entrance examination (ZNO). The benchmark consists of
over 4,300 expert-crafted questions spanning 12 academic disciplines, including
mathematics, physics, chemistry, and humanities. We evaluated the performance
of both open-source models and API providers, finding that only a handful of
models performed above baseline. Alongside the new benchmark, we performed the
first evaluation study of multimodal text generation for the Ukrainian
language: we measured caption generation quality on the Multi30K-UK dataset,
translated the VQA benchmark into Ukrainian, and measured performance
degradation relative to original English versions. Lastly, we tested a few
models from a cultural perspective on knowledge of national cuisine. We believe
our work will advance multimodal generation capabilities for the Ukrainian
language and our approach could be useful for other low-resource languages.",2024-11-22,"Yurii Paniv, Artur Kiulian, Dmytro Chaplynskyi, Mykola Khandoga, Anton Polishko, Tetiana Bas, Guillermo Gabrielli",http://arxiv.org/pdf/2411.14647v1,cs.CL
Towards Knowledge Checking in Retrieval-augmented Generation: A Representation Perspective,"Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing
the performance of Large Language Models (LLMs). However, these systems face
challenges in effectively integrating external knowledge with the LLM's
internal knowledge, often leading to issues with misleading or unhelpful
information. This work aims to provide a systematic study on knowledge checking
in RAG systems. We conduct a comprehensive analysis of LLM representation
behaviors and demonstrate the significance of using representations in
knowledge checking. Motivated by the findings, we further develop
representation-based classifiers for knowledge filtering. We show substantial
improvements in RAG performance, even when dealing with noisy knowledge
databases. Our study provides new insights into leveraging LLM representations
for enhancing the reliability and effectiveness of RAG systems.",2024-11-21,"Shenglai Zeng, Jiankun Zhang, Bingheng Li, Yuping Lin, Tianqi Zheng, Dante Everaert, Hanqing Lu, Hui Liu, Hui Liu, Yue Xing, Monica Xiao Cheng, Jiliang Tang",http://arxiv.org/pdf/2411.14572v1,cs.CL
Assessment of LLM Responses to End-user Security Questions,"Answering end user security questions is challenging. While large language
models (LLMs) like GPT, LLAMA, and Gemini are far from error-free, they have
shown promise in answering a variety of questions outside of security. We
studied LLM performance in the area of end user security by qualitatively
evaluating 3 popular LLMs on 900 systematically collected end user security
questions.
  While LLMs demonstrate broad generalist ``knowledge'' of end user security
information, there are patterns of errors and limitations across LLMs
consisting of stale and inaccurate answers, and indirect or unresponsive
communication styles, all of which impacts the quality of information received.
Based on these patterns, we suggest directions for model improvement and
recommend user strategies for interacting with LLMs when seeking assistance
with security.",2024-11-21,"Vijay Prakash, Kevin Lee, Arkaprabha Bhattacharya, Danny Yuxing Huang, Jessica Staddon",http://arxiv.org/pdf/2411.14571v1,cs.CL
Reducibility among NP-Hard graph problems and boundary classes,"Many NP-hard graph problems become easy for some classes of graphs, such as
coloring is easy for bipartite graphs, but NP-hard in general. So we can ask
question like when does a hard problem become easy? What is the minimum
substructure for which the problem remains hard? We use the notion of boundary
classes to study such questions. In this paper, we introduce a method for
transforming the boundary class of one NP-hard graph problem into a boundary
class for another problem. If $\Pi$ and $\Gamma$ are two NP-hard graph problems
where $\Pi$ is reducible to $\Gamma$, we transform a boundary class of $\Pi$
into a boundary class of $\Gamma$. More formally if $\Pi$ is reducible to
$\Gamma$, where the reduction is bijective and it maps hereditary classes of
graphs to hereditary classes of graphs, then $X$ is a boundary class of $\Pi$
if and only if the image of $X$ under the reduction is a boundary class of
$\Gamma$. This gives us a relationship between boundary classes and
reducibility among several NP-hard problems. To show the strength of our main
result, we apply our theorem to obtain some previously unknown boundary classes
for a few graph problems namely; vertex-cover, clique, traveling-salesperson,
bounded-degree-spanning-tree, subgraph-isomorphism and clique-cover.",2024-11-21,"Syed Mujtaba Hassan, Shahid Hussain, Abdul Samad",http://arxiv.org/pdf/2411.14553v1,cs.CL
An Experimental Study on Data Augmentation Techniques for Named Entity Recognition on Low-Resource Domains,"Named Entity Recognition (NER) is a machine learning task that traditionally
relies on supervised learning and annotated data. Acquiring such data is often
a challenge, particularly in specialized fields like medical, legal, and
financial sectors. Those are commonly referred to as low-resource domains,
which comprise long-tail entities, due to the scarcity of available data. To
address this, data augmentation techniques are increasingly being employed to
generate additional training instances from the original dataset. In this
study, we evaluate the effectiveness of two prominent text augmentation
techniques, Mention Replacement and Contextual Word Replacement, on two
widely-used NER models, Bi-LSTM+CRF and BERT. We conduct experiments on four
datasets from low-resource domains, and we explore the impact of various
combinations of training subset sizes and number of augmented examples. We not
only confirm that data augmentation is particularly beneficial for smaller
datasets, but we also demonstrate that there is no universally optimal number
of augmented examples, i.e., NER practitioners must experiment with different
quantities in order to fine-tune their projects.",2024-11-21,"Arthur Elwing Torres, Edleno Silva de Moura, Altigran Soares da Silva, Mario A. Nascimento, Filipe Mesquita",http://arxiv.org/pdf/2411.14551v1,cs.CL
BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models,"Recent advancements in vision-language models (VLMs), such as CLIP, have
demonstrated substantial success in self-supervised representation learning for
vision tasks. However, effectively adapting VLMs to downstream applications
remains challenging, as their accuracy often depends on time-intensive and
expertise-demanding prompt engineering, while full model fine-tuning is costly.
This is particularly true for biomedical images, which, unlike natural images,
typically suffer from limited annotated datasets, unintuitive image contrasts,
and nuanced visual features. Recent prompt learning techniques, such as Context
Optimization (CoOp) intend to tackle these issues, but still fall short in
generalizability. Meanwhile, explorations in prompt learning for biomedical
image analysis are still highly limited. In this work, we propose BiomedCoOp, a
novel prompt learning framework that enables efficient adaptation of BiomedCLIP
for accurate and highly generalizable few-shot biomedical image classification.
Our approach achieves effective prompt context learning by leveraging semantic
consistency with average prompt ensembles from Large Language Models (LLMs) and
knowledge distillation with a statistics-based prompt selection strategy. We
conducted comprehensive validation of our proposed framework on 11 medical
datasets across 9 modalities and 10 organs against existing state-of-the-art
methods, demonstrating significant improvements in both accuracy and
generalizability. The code is publicly available at
https://github.com/HealthX-Lab/BiomedCoOp.",2024-11-21,"Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao",http://arxiv.org/pdf/2411.15232v2,cs.CL
Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework,"The integration of experimental technologies with large language models
(LLMs) is transforming scientific research. It positions AI as a versatile
research assistant rather than a mere problem-solving tool. In the field of
power systems, however, managing simulations -- one of the essential
experimental technologies -- remains a challenge for LLMs due to their limited
domain-specific knowledge, restricted reasoning capabilities, and imprecise
handling of simulation parameters. To address these limitations, this paper
proposes a feedback-driven, multi-agent framework. It incorporates three
proposed modules: an enhanced retrieval-augmented generation (RAG) module, an
improved reasoning module, and a dynamic environmental acting module with an
error-feedback mechanism. Validated on 69 diverse tasks from Daline and
MATPOWER, this framework achieves success rates of 93.13% and 96.85%,
respectively. It significantly outperforms ChatGPT 4o, o1-preview, and the
fine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex
tasks. Additionally, the proposed framework also supports rapid, cost-effective
task execution, completing each simulation in approximately 30 seconds at an
average cost of 0.014 USD for tokens. Overall, this adaptable framework lays a
foundation for developing intelligent LLM-based assistants for human
researchers, facilitating power system research and beyond.",2024-11-21,"Mengshuo Jia, Zeyu Cui, Gabriela Hug",http://arxiv.org/pdf/2411.16707v3,cs.CL
Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions,"Currently OpenAI o1 sparks a surge of interest in the study of large
reasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on
disciplines with standard answers, such as mathematics, physics, and coding --
which are well-suited for reinforcement learning (RL) -- but also places
greater emphasis on open-ended resolutions. We aim to address the question:
''Can the o1 model effectively generalize to broader domains where clear
standards are absent and rewards are challenging to quantify?'' Marco-o1 is
powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS),
reflection mechanisms, and innovative reasoning strategies -- optimized for
complex real-world problem-solving tasks.",2024-11-21,"Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang",http://arxiv.org/pdf/2411.14405v2,cs.CL
Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings,"With the recent proliferation of large language models (LLMs), enterprises
have been able to rapidly develop proof-of-concepts and prototypes. As a
result, there is a growing need to implement robust guardrails that monitor,
quantize and control an LLM's behavior, ensuring that the use is reliable,
safe, accurate and also aligned with the users' expectations. Previous
approaches for filtering out inappropriate user prompts or system outputs, such
as LlamaGuard and OpenAI's MOD API, have achieved significant success by
fine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails
introduces increased latency and higher maintenance costs, which may not be
practical or scalable for cost-efficient deployments. We take a different
approach, focusing on fine-tuning a lightweight architecture: Sentence-BERT.
This method reduces the model size from LlamaGuard's 7 billion parameters to
approximately 67 million, while maintaining comparable performance on the AEGIS
safety benchmark.",2024-11-21,"Aaron Zheng, Mansi Rana, Andreas Stolcke",http://arxiv.org/pdf/2411.14398v1,cs.CL
POS-tagging to highlight the skeletal structure of sentences,"This study presents the development of a part-of-speech (POS) tagging model
to extract the skeletal structure of sentences using transfer learning with the
BERT architecture for token classification. The model, fine-tuned on Russian
text, demonstrating its effectiveness. The approach offers potential
applications in enhancing natural language processing tasks, such as improving
machine translation.
  Keywords: part of speech tagging, morphological analysis, natural language
processing, BERT.",2024-11-21,Grigorii Churakov,http://arxiv.org/pdf/2411.14393v1,cs.CL
UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages,"Large language models (LLMs) under-perform on low-resource languages due to
limited training data. We present a method to efficiently collect text data for
low-resource languages from the entire Common Crawl corpus. Our approach,
UnifiedCrawl, filters and extracts common crawl using minimal compute
resources, yielding mono-lingual datasets much larger than previously available
sources. We demonstrate that leveraging this data to fine-tuning multilingual
LLMs via efficient adapter methods (QLoRA) significantly boosts performance on
the low-resource language, while minimizing VRAM usage. Our experiments show
large improvements in language modeling perplexity and an increase in few-shot
prompting scores. Our work and released source code provide an affordable
approach to improve LLMs for low-resource languages using consumer hardware.
Our source code is available here at
https://github.com/bethelmelesse/unifiedcrawl.",2024-11-21,"Bethel Melesse Tessema, Akhil Kedia, Tae-Sun Chung",http://arxiv.org/pdf/2411.14343v1,cs.CL
Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training,"It is well-known that a diverse corpus is critical for training large
language models, which are typically constructed from a mixture of various
domains. In general, previous efforts resort to sampling training data from
different domains with static proportions, as well as adjusting data
proportions during training. However, few methods have addressed the
complexities of domain-adaptive continual pre-training. To fill this gap, we
propose Velocitune, a novel framework dynamically assesses learning velocity
and adjusts data proportions accordingly, favoring slower-learning domains
while shunning faster-learning ones, which is guided by a scaling law to
indicate the desired learning goal for each domain with less associated cost.
To evaluate the effectiveness of Velocitune, we conduct experiments in a
reasoning-focused dataset with CodeLlama, as well as in a corpus specialised
for system command generation with Llama3 and Mistral. Velocitune achieves
performance gains in both math and code reasoning tasks and command-line
generation benchmarks. Further analysis reveals that key factors driving
Velocitune's effectiveness include target loss prediction and data ordering.",2024-11-21,"Zheheng Luo, Xin Zhang, Xiao Liu, Haoling Li, Yeyun Gong, Chen Qi, Peng Cheng",http://arxiv.org/pdf/2411.14318v2,cs.CL
Looking Beyond Text: Reducing Language bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance,"Large vision-language models (LVLMs) have achieved impressive results in
various vision-language tasks. However, despite showing promising performance,
LVLMs suffer from hallucinations caused by language bias, leading to diminished
focus on images and ineffective visual comprehension. We identify two primary
reasons for this bias: 1. Different scales of training data between the
pretraining stage of LLM and multimodal alignment stage. 2. The learned
inference bias due to short-term dependency of text data. Therefore, we propose
LACING, a systemic framework designed to address the language bias of LVLMs
with muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG).
Specifically, MDA introduces a parallel dual-attention mechanism that enhances
the integration of visual inputs across the model. IFG introduces a learnable
soft visual prompt during training and inference to replace visual inputs,
designed to compel LVLMs to prioritize text inputs. Then, IFG further proposes
a novel decoding strategy using the soft visual prompt to mitigate the model's
over-reliance on adjacent text inputs. Comprehensive experiments demonstrate
that our method effectively debiases LVLMs from their language bias, enhancing
visual comprehension and reducing hallucinations without requiring additional
training resources or data. The code and model are available at
[lacing-lvlm.github.io](https://lacing-lvlm.github.io).",2024-11-21,"Haozhe Zhao, Shuzheng Si, Liang Chen, Yichi Zhang, Maosong Sun, Mingjia Zhang, Baobao Chang",http://arxiv.org/pdf/2411.14279v1,cs.CL
Efficient Aspect-Based Summarization of Climate Change Reports with Small Language Models,"The use of Natural Language Processing (NLP) for helping decision-makers with
Climate Change action has recently been highlighted as a use case aligning with
a broader drive towards NLP technologies for social good. In this context,
Aspect-Based Summarization (ABS) systems that extract and summarize relevant
information are particularly useful as they provide stakeholders with a
convenient way of finding relevant information in expert-curated reports. In
this work, we release a new dataset for ABS of Climate Change reports and we
employ different Large Language Models (LLMs) and so-called Small Language
Models (SLMs) to tackle this problem in an unsupervised way. Considering the
problem at hand, we also show how SLMs are not significantly worse for the
problem while leading to reduced carbon footprint; we do so by applying for the
first time an existing framework considering both energy efficiency and task
performance to the evaluation of zero-shot generative models for ABS. Overall,
our results show that modern language models, both big and small, can
effectively tackle ABS for Climate Change reports but more research is needed
when we frame the problem as a Retrieval Augmented Generation (RAG) problem and
our work and dataset will help foster efforts in this direction.",2024-11-21,"Iacopo Ghinassi, Leonardo Catalano, Tommaso Colella",http://arxiv.org/pdf/2411.14272v1,cs.CL
"Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective","Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) based applications including automated text generation, question
answering, chatbots, and others. However, they face a significant challenge:
hallucinations, where models produce plausible-sounding but factually incorrect
responses. This undermines trust and limits the applicability of LLMs in
different domains. Knowledge Graphs (KGs), on the other hand, provide a
structured collection of interconnected facts represented as entities (nodes)
and their relationships (edges). In recent research, KGs have been leveraged to
provide context that can fill gaps in an LLM understanding of certain topics
offering a promising approach to mitigate hallucinations in LLMs, enhancing
their reliability and accuracy while benefiting from their wide applicability.
Nonetheless, it is still a very active area of research with various unresolved
open problems. In this paper, we discuss these open challenges covering
state-of-the-art datasets and benchmarks as well as methods for knowledge
integration and evaluating hallucinations. In our discussion, we consider the
current use of KGs in LLM systems and identify future directions within each of
these challenges.",2024-11-21,"Ernests Lavrinovics, Russa Biswas, Johannes Bjerva, Katja Hose",http://arxiv.org/pdf/2411.14258v1,cs.CL
Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models,"Hallucinations in large language models are a widespread problem, yet the
mechanisms behind whether models will hallucinate are poorly understood,
limiting our ability to solve this problem. Using sparse autoencoders as an
interpretability tool, we discover that a key part of these mechanisms is
entity recognition, where the model detects if an entity is one it can recall
facts about. Sparse autoencoders uncover meaningful directions in the
representation space, these detect whether the model recognizes an entity, e.g.
detecting it doesn't know about an athlete or a movie. This suggests that
models can have self-knowledge: internal representations about their own
capabilities. These directions are causally relevant: capable of steering the
model to refuse to answer questions about known entities, or to hallucinate
attributes of unknown entities when it would otherwise refuse. We demonstrate
that despite the sparse autoencoders being trained on the base model, these
directions have a causal effect on the chat model's refusal behavior,
suggesting that chat finetuning has repurposed this existing mechanism.
Furthermore, we provide an initial exploration into the mechanistic role of
these directions in the model, finding that they disrupt the attention of
downstream heads that typically move entity attributes to the final token.",2024-11-21,"Javier Ferrando, Oscar Obeso, Senthooran Rajamanoharan, Neel Nanda",http://arxiv.org/pdf/2411.14257v2,cs.CL
Intent-Aware Dialogue Generation and Multi-Task Contrastive Learning for Multi-Turn Intent Classification,"Generating large-scale, domain-specific, multilingual multi-turn dialogue
datasets remains a significant hurdle for training effective Multi-Turn Intent
Classification models in chatbot systems. In this paper, we introduce
Chain-of-Intent, a novel mechanism that combines Hidden Markov Models with
Large Language Models (LLMs) to generate contextually aware, intent-driven
conversations through self-play. By extracting domain-specific knowledge from
e-commerce chat logs, we estimate conversation turns and intent transitions,
which guide the generation of coherent dialogues. Leveraging LLMs to enhance
emission probabilities, our approach produces natural and contextually
consistent questions and answers. We also propose MINT-CL, a framework for
multi-turn intent classification using multi-task contrastive learning,
improving classification accuracy without the need for extensive annotated
data. Evaluations show that our methods outperform baselines in dialogue
quality and intent classification accuracy, especially in multilingual
settings, while significantly reducing data generation efforts. Furthermore, we
release MINT-E, a multilingual, intent-aware multi-turn e-commerce dialogue
corpus to support future research in this area.",2024-11-21,"Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim",http://arxiv.org/pdf/2411.14252v1,cs.CL
Natural Language Reinforcement Learning,"Reinforcement Learning (RL) mathematically formulates decision-making with
Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable
breakthroughs across various domains, including games, robotics, and language
models. This paper seeks a new possibility, Natural Language Reinforcement
Learning (NLRL), by extending traditional MDP to natural language-based
representation space. Specifically, NLRL innovatively redefines RL principles,
including task objectives, policy, value function, Bellman equation, and policy
iteration, into their language counterparts. With recent advancements in large
language models (LLMs), NLRL can be practically implemented to achieve RL-like
policy and value improvement by either pure prompting or gradient-based
training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games
demonstrate the effectiveness, efficiency, and interpretability of the NLRL
framework among diverse use cases.",2024-11-21,"Xidong Feng, Bo Liu, Ziyu Wan, Haotian Fu, Girish A. Koushik, Zhiyuan Hu, Mengyue Yang, Ying Wen, Jun Wang",http://arxiv.org/pdf/2411.14251v2,cs.CL
Evaluating the Robustness of Analogical Reasoning in Large Language Models,"LLMs have performed well on several reasoning benchmarks, including ones that
test analogical reasoning abilities. However, there is debate on the extent to
which they are performing general abstract reasoning versus employing
non-robust processes, e.g., that overly rely on similarity to pre-training
data. Here we investigate the robustness of analogy-making abilities previously
claimed for LLMs on three of four domains studied by Webb, Holyoak, and Lu
(2023): letter-string analogies, digit matrices, and story analogies. For each
domain we test humans and GPT models on robustness to variants of the original
analogy problems that test the same abstract reasoning abilities but are likely
dissimilar from tasks in the pre-training data. The performance of a system
that uses robust abstract reasoning should not decline substantially on these
variants.
  On simple letter-string analogies, we find that while the performance of
humans remains high for two types of variants we tested, the GPT models'
performance declines sharply. This pattern is less pronounced as the complexity
of these problems is increased, as both humans and GPT models perform poorly on
both the original and variant problems requiring more complex analogies. On
digit-matrix problems, we find a similar pattern but only on one out of the two
types of variants we tested. On story-based analogy problems, we find that,
unlike humans, the performance of GPT models are susceptible to answer-order
effects, and that GPT models also may be more sensitive than humans to
paraphrasing.
  This work provides evidence that LLMs often lack the robustness of zero-shot
human analogy-making, exhibiting brittleness on most of the variations we
tested. More generally, this work points to the importance of carefully
evaluating AI systems not only for accuracy but also robustness when testing
their cognitive capabilities.",2024-11-21,"Martha Lewis, Melanie Mitchell",http://arxiv.org/pdf/2411.14215v1,cs.CL
OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs,"Scientific progress depends on researchers' ability to synthesize the growing
body of literature. Can large language models (LMs) assist scientists in this
task? We introduce OpenScholar, a specialized retrieval-augmented LM that
answers scientific queries by identifying relevant passages from 45 million
open-access papers and synthesizing citation-backed responses. To evaluate
OpenScholar, we develop ScholarQABench, the first large-scale multi-domain
benchmark for literature search, comprising 2,967 expert-written queries and
208 long-form answers across computer science, physics, neuroscience, and
biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and
PaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o
hallucinates citations 78 to 90% of the time, OpenScholar achieves citation
accuracy on par with human experts. OpenScholar's datastore, retriever, and
self-feedback inference loop also improves off-the-shelf LMs: for instance,
OpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations,
experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over
expert-written ones 51% and 70% of the time, respectively, compared to GPT4o's
32%. We open-source all of our code, models, datastore, data and a public demo.",2024-11-21,"Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D'arcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen-tau Yih, Pang Wei Koh, Hannaneh Hajishirzi",http://arxiv.org/pdf/2411.14199v1,cs.CL
Why do language models perform worse for morphologically complex languages?,"Language models perform differently across languages. It has been previously
suggested that morphological typology may explain some of this variability
(Cotterell et al., 2018). We replicate previous analyses and find additional
new evidence for a performance gap between agglutinative and fusional
languages, where fusional languages, such as English, tend to have better
language modeling performance than morphologically more complex languages like
Turkish. We then propose and test three possible causes for this performance
gap: morphological alignment of tokenizers, tokenization quality, and
disparities in dataset sizes and measurement. To test the morphological
alignment hypothesis, we present MorphScore, a tokenizer evaluation metric, and
supporting datasets for 22 languages. We find some evidence that tokenization
quality explains the performance gap, but none for the role of morphological
alignment. Instead we find that the performance gap is most reduced when
training datasets are of equivalent size across language types, but only when
scaled according to the so-called ""byte-premium"" -- the different encoding
efficiencies of different languages and orthographies. These results suggest
that no language is harder or easier for a language model to learn on the basis
of its morphological typology. Differences in performance can be attributed to
disparities in dataset size. These results bear on ongoing efforts to improve
performance for low-performing and under-resourced languages.",2024-11-21,"Catherine Arnett, Benjamin K. Bergen",http://arxiv.org/pdf/2411.14198v1,cs.CL
VAGUE: Visual Contexts Clarify Ambiguous Expressions,"Human communication often relies on visual cues to resolve ambiguity. While
humans can intuitively integrate these cues, AI systems often find it
challenging to engage in sophisticated multimodal reasoning. We introduce
VAGUE, a benchmark evaluating multimodal AI systems' ability to integrate
visual context for intent disambiguation. VAGUE consists of 1.6K ambiguous
textual expressions, each paired with an image and multiple-choice
interpretations, where the correct answer is only apparent with visual context.
The dataset spans both staged, complex (Visual Commonsense Reasoning) and
natural, personal (Ego4D) scenes, ensuring diversity. Our experiments reveal
that existing multimodal AI models struggle to infer the speaker's true intent.
While performance consistently improves from the introduction of more visual
cues, the overall accuracy remains far below human performance, highlighting a
critical gap in multimodal reasoning. Analysis of failure cases demonstrates
that current models fail to distinguish true intent from superficial
correlations in the visual scene, indicating that they perceive images but do
not effectively reason with them. We release our code and data at
https://github.com/Hazel-Heejeong-Nam/VAGUE.git.",2024-11-21,"Heejeong Nam, Jinwoo Ahn, Keummin Ka, Jiwan Chung, Youngjae Yu",http://arxiv.org/pdf/2411.14137v2,cs.CL
Towards a Middleware for Large Language Models,"Large language models have gained widespread popularity for their ability to
process natural language inputs and generate insights derived from their
training data, nearing the qualities of true artificial intelligence. This
advancement has prompted enterprises worldwide to integrate LLMs into their
services. So far, this effort is dominated by commercial cloud-based solutions
like OpenAI's ChatGPT and Microsoft Azure. As the technology matures, however,
there is a strong incentive for independence from major cloud providers through
self-hosting ""LLM as a Service"", driven by privacy, cost, and customization
needs. In practice, hosting LLMs independently presents significant challenges
due to their complexity and integration issues with existing systems. In this
paper, we discuss our vision for a forward-looking middleware system
architecture that facilitates the deployment and adoption of LLMs in
enterprises, even for advanced use cases in which we foresee LLMs to serve as
gateways to a complete application ecosystem and, to some degree, absorb
functionality traditionally attributed to the middleware.",2024-11-21,"Narcisa Guran, Florian Knauf, Man Ngo, Stefan Petrescu, Jan S. Rellermeyer",http://arxiv.org/pdf/2411.14513v1,cs.CL
"Learning from ""Silly"" Questions Improves Large Language Models, But Only Slightly","Constructing high-quality Supervised Fine-Tuning (SFT) datasets is critical
for the training of large language models (LLMs). Recent studies have shown
that using data from a specific source, Ruozhiba, a Chinese website where users
ask ""silly"" questions to better understand certain topics, can lead to better
fine-tuning performance. This paper aims to explore some hidden factors: the
potential interpretations of its success and a large-scale evaluation of the
performance. First, we leverage GPT-4 to analyze the successful cases of
Ruozhiba questions from the perspective of education, psychology, and cognitive
science, deriving a set of explanatory rules. Then, we construct fine-tuning
datasets by applying these rules to the MMLU training set. Surprisingly, our
results indicate that rules can significantly improve model performance in
certain tasks, while potentially diminishing performance on others. For
example, SFT data generated following the ""Counterintuitive Thinking"" rule can
achieve approximately a 5% improvement on the ""Global Facts"" task, whereas the
""Blurring the Conceptual Boundaries"" rule leads to a performance drop of 6.14%
on the ""Econometrics"" task. In addition, for specific tasks, different rules
tend to have a consistent impact on model performance. This suggests that the
differences between the extracted rules are not as significant, and the
effectiveness of the rules is relatively consistent across tasks. Our research
highlights the importance of considering task diversity and rule applicability
when constructing SFT datasets to achieve more comprehensive performance
improvements.",2024-11-21,"Tingyuan Zhu, Shudong Liu, Yidong Wang, Derek F. Wong, Han Yu, Takahiro Shinozaki, Jindong Wang",http://arxiv.org/pdf/2411.14121v1,cs.CL
Lost in Inference: Rediscovering the Role of Natural Language Inference for Large Language Models,"In the recent past, a popular way of evaluating natural language
understanding (NLU), was to consider a model's ability to perform natural
language inference (NLI) tasks. In this paper, we investigate if NLI tasks,
that are rarely used for LLM evaluation, can still be informative for
evaluating LLMs. Focusing on five different NLI benchmarks across six models of
different scales, we investigate if they are able to discriminate models of
different size and quality and how their accuracies develop during training.
Furthermore, we investigate the extent to which the softmax distributions of
models align with human distributions in cases where statements are ambiguous
or vague. Overall, our results paint a positive picture for the NLI tasks: we
find that they are able to discriminate well between models at various stages
of training, yet are not (all) saturated. Furthermore, we find that while the
similarity of model distributions with human label distributions increases with
scale, it is still much higher than the similarity between two populations of
humans, making it a potentially interesting statistic to consider.",2024-11-21,"Lovish Madaan, David Esiobu, Pontus Stenetorp, Barbara Plank, Dieuwke Hupkes",http://arxiv.org/pdf/2411.14103v1,cs.CL
BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection,"Spoken term detection (STD) is often hindered by reliance on frame-level
features and the computationally intensive DTW-based template matching,
limiting its practicality. To address these challenges, we propose a novel
approach that encodes speech into discrete, speaker-agnostic semantic tokens.
This facilitates fast retrieval using text-based search algorithms and
effectively handles out-of-vocabulary terms. Our approach focuses on generating
consistent token sequences across varying utterances of the same term. We also
propose a bidirectional state space modeling within the Mamba encoder, trained
in a self-supervised learning framework, to learn contextual frame-level
features that are further encoded into discrete tokens. Our analysis shows that
our speech tokens exhibit greater speaker invariance than those from existing
tokenizers, making them more suitable for STD tasks. Empirical evaluation on
LibriSpeech and TIMIT databases indicates that our method outperforms existing
STD baselines while being more efficient.",2024-11-21,"Anup Singh, Kris Demuynck, Vipul Arora",http://arxiv.org/pdf/2411.14100v2,cs.CL
"Meaning at the Planck scale? Contextualized word embeddings for doing history, philosophy, and sociology of science","This paper explores the potential of contextualized word embeddings (CWEs) as
a new tool in the history, philosophy, and sociology of science (HPSS) for
studying contextual and evolving meanings of scientific concepts. Using the
term ""Planck"" as a test case, I evaluate five BERT-based models with varying
degrees of domain-specific pretraining, including my custom model
Astro-HEP-BERT, trained on the Astro-HEP Corpus, a dataset containing 21.84
million paragraphs from 600,000 articles in astrophysics and high-energy
physics. For this analysis, I compiled two labeled datasets: (1) the
Astro-HEP-Planck Corpus, consisting of 2,900 labeled occurrences of ""Planck""
sampled from 1,500 paragraphs in the Astro-HEP Corpus, and (2) a
physics-related Wikipedia dataset comprising 1,186 labeled occurrences of
""Planck"" across 885 paragraphs. Results demonstrate that the domain-adapted
models outperform the general-purpose ones in disambiguating the target term,
predicting its known meanings, and generating high-quality sense clusters, as
measured by a novel purity indicator I developed. Additionally, this approach
reveals semantic shifts in the target term over three decades in the unlabeled
Astro-HEP Corpus, highlighting the emergence of the Planck space mission as a
dominant sense. The study underscores the importance of domain-specific
pretraining for analyzing scientific language and demonstrates the
cost-effectiveness of adapting pretrained models for HPSS research. By offering
a scalable and transferable method for modeling the meanings of scientific
concepts, CWEs open up new avenues for investigating the socio-historical
dynamics of scientific discourses.",2024-11-21,Arno Simons,http://arxiv.org/pdf/2411.14073v1,cs.CL
The Master-Slave Encoder Model for Improving Patent Text Summarization: A New Approach to Combining Specifications and Claims,"In order to solve the problem of insufficient generation quality caused by
traditional patent text abstract generation models only originating from patent
specifications, the problem of new terminology OOV caused by rapid patent
updates, and the problem of information redundancy caused by insufficient
consideration of the high professionalism, accuracy, and uniqueness of patent
texts, we proposes a patent text abstract generation model (MSEA) based on a
master-slave encoder architecture; Firstly, the MSEA model designs a
master-slave encoder, which combines the instructions in the patent text with
the claims as input, and fully explores the characteristics and details between
the two through the master-slave encoder; Then, the model enhances the
consideration of new technical terms in the input sequence based on the pointer
network, and further enhances the correlation with the input text by re
weighing the ""remembered"" and ""for-gotten"" parts of the input sequence from the
encoder; Finally, an enhanced repetition suppression mechanism for patent text
was introduced to ensure accurate and non redundant abstracts generated. On a
publicly available patent text dataset, compared to the state-of-the-art model,
Improved Multi-Head Attention Mechanism (IMHAM), the MSEA model achieves an
improvement of 0.006, 0.005, and 0.005 in Rouge-1, Rouge-2, and Rouge-L scores,
respectively. MSEA leverages the characteristics of patent texts to effectively
enhance the quality of patent text generation, demonstrating its advancement
and effectiveness in the experiments.",2024-11-21,"Shu Zhou, Xin Wang, Zhengda Zhou, Haohan Yi, Xuhui Zheng, Hao Wan",http://arxiv.org/pdf/2411.14072v1,cs.CL
Voice Communication Analysis in Esports,"In most team-based esports, voice communications are prominent in the team
efficiency and synergy. In fact it has been observed that not only the skill
aspect of the team but also the team effective voice communication comes into
play when trying to have good performance in official matches. With the recent
emergence of LLM (Large Language Models) tools regarding NLP (Natural Language
Processing) (Vaswani et. al.), we decided to try applying them in order to have
a better understanding on how to improve the effectiveness of the voice
communications. In this paper the study has been made through the prism of
League of Legends esport. However the main concepts and ideas can be easily
applicable in any other team related esports.",2024-11-21,"Aymeric Vinot, Nicolas Perez",http://arxiv.org/pdf/2411.19793v1,cs.CL
MMGenBench: Fully Automatically Evaluating LMMs from the Text-to-Image Generation Perspective,"Large Multimodal Models (LMMs) demonstrate impressive capabilities. However,
current benchmarks predominantly focus on image comprehension in specific
domains, and these benchmarks are labor-intensive to construct. Moreover, their
answers tend to be brief, making it difficult to assess the ability of LMMs to
generate detailed descriptions of images. To address these limitations, we
propose the MMGenBench-Pipeline, a straightforward and fully automated
evaluation pipeline. This involves generating textual descriptions from input
images, using these descriptions to create auxiliary images via text-to-image
generative models, and then comparing the original and generated images.
Furthermore, to ensure the effectiveness of MMGenBench-Pipeline, we design
MMGenBench-Test, evaluating LMMs across 13 distinct image patterns, and
MMGenBench-Domain, focusing on generative image performance. A thorough
evaluation involving over 50 popular LMMs demonstrates the effectiveness and
reliability of both the pipeline and benchmark. Our observations indicate that
numerous LMMs excelling in existing benchmarks fail to adequately complete the
basic tasks related to image understanding and description. This finding
highlights the substantial potential for performance improvement in current
LMMs and suggests avenues for future model optimization. Concurrently,
MMGenBench-Pipeline can efficiently assess the performance of LMMs across
diverse domains using only image inputs.",2024-11-21,"Hailang Huang, Yong Wang, Zixuan Huang, Huaqiu Li, Tongwen Huang, Xiangxiang Chu, Richong Zhang",http://arxiv.org/pdf/2411.14062v2,cs.CL
DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization,"Large language models (LLMs) deliver impressive results but face challenges
from increasing model sizes and computational costs. Structured pruning reduces
model size and speeds up inference but often causes uneven degradation across
domains, leading to biased performance. To address this, we propose DRPruning,
which incorporates distributionally robust optimization to restore balanced
performance across domains, along with further improvements to enhance
robustness. Experiments in monolingual and multilingual settings show that our
method surpasses similarly sized models in pruning and continued pretraining
over perplexity, downstream tasks, and instruction tuning. We further provide
analysis demonstrating the robustness of our method towards various domains and
distribution shifts. Furthermore, our method automatically determines optimal
reference losses and data ratios, suggesting potential for broader
applications. Our code is available at https://github.com/hexuandeng/DRPruning.",2024-11-21,"Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Min Zhang, Zhaopeng Tu",http://arxiv.org/pdf/2411.14055v1,cs.CL
FunctionChat-Bench: Comprehensive Evaluation of Language Models' Generative Capabilities in Korean Tool-use Dialogs,"This study investigates language models' generative capabilities in tool-use
dialogs. We categorize the models' outputs in tool-use dialogs into four
distinct types: Tool Call, Answer Completion, Slot Question, and Relevance
Detection, which serve as aspects for evaluation. We introduce
FunctionChat-Bench, comprising 700 evaluation items and automated assessment
programs. Using this benchmark, we evaluate several language models that
support function calling. Our findings indicate that while language models may
exhibit high accuracy in single-turn Tool Call scenarios, this does not
necessarily translate to superior generative performance in multi-turn
environments. We argue that the capabilities required for function calling
extend beyond generating tool call messages; they must also effectively
generate conversational messages that engage the user.",2024-11-21,"Shinbok Lee, Gaeun Seo, Daniel Lee, Byeongil Ko, Sunghee Jung, Myeongcheol Shin",http://arxiv.org/pdf/2411.14054v1,cs.CL
Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling,"Predicting future international events from textual information, such as news
articles, has tremendous potential for applications in global policy, strategic
decision-making, and geopolitics. However, existing datasets available for this
task are often limited in quality, hindering the progress of related research.
In this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction),
a novel dataset designed to address these limitations by leveraging the
advanced reasoning capabilities of large-language models (LLMs). Our dataset
features high-quality scoring labels generated through advanced prompt modeling
and rigorously validated by domain experts in political science. We showcase
the quality and utility of WORLDREP for real-world event prediction tasks,
demonstrating its effectiveness through extensive experiments and analysis.
Furthermore, we publicly release our dataset along with the full automation
source code for data collection, labeling, and benchmarking, aiming to support
and advance research in text-based event prediction.",2024-11-21,"Daehoon Gwak, Junwoo Park, Minho Park, Chaehun Park, Hyunchan Lee, Edward Choi, Jaegul Choo",http://arxiv.org/pdf/2411.14042v1,cs.CL
StackEval: Benchmarking LLMs in Coding Assistance,"We present two comprehensive benchmarks to evaluate the performance of
language models in coding assistance tasks, covering code writing, debugging,
code review, and conceptual understanding. Our main contribution includes two
curated datasets: StackEval, a large-scale benchmark derived from Stack
Overflow questions, and StackUnseen, a dynamic benchmark featuring the most
recent Stack Overflow content. These benchmarks offer novel insights into the
capabilities and limitations of LLMs, particularly in handling new and emerging
content. Additionally, we assess LLMs' proficiency as judges for coding tasks
using a curated, human-annotated dataset, exploring their evaluation
capabilities and potential biases, including whether they favor their own
generated solutions. Our findings underscore the potential of these benchmarks
to advance LLM development and application in coding assistance. To ensure
reproducibility, we publicly share our datasets and evaluation code at
https://github.com/ProsusAI/stack-eval .",2024-11-21,"Nidhish Shah, Zulkuf Genc, Dogu Araci",http://arxiv.org/pdf/2412.05288v1,cs.CL
Logic Augmented Generation,"Semantic Knowledge Graphs (SKG) face challenges with scalability,
flexibility, contextual understanding, and handling unstructured or ambiguous
information. However, they offer formal and structured knowledge enabling
highly interpretable and reliable results by means of reasoning and querying.
Large Language Models (LLMs) overcome those limitations making them suitable in
open-ended tasks and unstructured environments. Nevertheless, LLMs are neither
interpretable nor reliable. To solve the dichotomy between LLMs and SKGs we
envision Logic Augmented Generation (LAG) that combines the benefits of the two
worlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate
potentially infinite relations and tacit knowledge on-demand. SKGs are key for
injecting a discrete heuristic dimension with clear logical and factual
boundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,
medical diagnostics and climate projections. Understanding the properties and
limitations of LAG, which are still mostly unknown, is of utmost importance for
enabling a variety of tasks involving tacit knowledge in order to provide
interpretable and effective results.",2024-11-21,"Aldo Gangemi, Andrea Giovanni Nuzzolese",http://arxiv.org/pdf/2411.14012v2,cs.CL
FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers,"Generative Pre-trained Transformers (GPTs) have demonstrated remarkable
performance across diverse domains, largely due to the extensive scaling of
model parameters. Recent works have observed redundancy within transformer
blocks and developed compression methods by structured pruning of less
important blocks. However, such direct removal often leads to irreversible
performance degradation. In this paper, we propose FuseGPT, a novel methodology
designed to recycle pruned transformer blocks, thereby recovering the model's
performance. Firstly, we introduce a new importance detection metric, Macro
Influence (MI), which evaluates the long-term impact of each transformer block
by quantifying the information loss incurred upon its removal. Next, we propose
group-level layer fusion, which leverages the parameters from layers of less
important blocks and integrates them into the corresponding layers of
neighboring blocks. This fusion process is not a one-time operation but is
refined through iterative parameter updates by lightweight group-level
fine-tuning. Specifically, the injected parameters are frozen but are weighted
with learnable rank decomposition matrices to reduce the computational overhead
during fine-tuning. Our approach not only works well for large language models
but also for large multimodal models. Experimental results indicate that, even
with modest amounts of data, FuseGPT surpasses previous methods in both
perplexity and zero-shot task performance.",2024-11-21,"Zehua Pei, Hui-Ling Zhen, Xianzhi Yu, Sinno Jialin Pan, Mingxuan Yuan, Bei Yu",http://arxiv.org/pdf/2411.14507v2,cs.CL
Sentiment Analysis of Economic Text: A Lexicon-Based Approach,"We propose an Economic Lexicon (EL) specifically designed for textual
applications in economics. We construct the dictionary with two important
characteristics: 1) to have a wide coverage of terms used in documents
discussing economic concepts, and 2) to provide a human-annotated sentiment
score in the range [-1,1]. We illustrate the use of the EL in the context of a
simple sentiment measure and consider several applications in economics. The
comparison to other lexicons shows that the EL is superior due to its wider
coverage of domain relevant terms and its more accurate categorization of the
word sentiment.",2024-11-21,"Luca Barbaglia, Sergio Consoli, Sebastiano Manzan, Luca Tiozzo Pezzoli, Elisa Tosetti",http://arxiv.org/pdf/2411.13958v1,cs.CL
Towards Full Delegation: Designing Ideal Agentic Behaviors for Travel Planning,"How are LLM-based agents used in the future? While many of the existing work
on agents has focused on improving the performance of a specific family of
objective and challenging tasks, in this work, we take a different perspective
by thinking about full delegation: agents take over humans' routine
decision-making processes and are trusted by humans to find solutions that fit
people's personalized needs and are adaptive to ever-changing context. In order
to achieve such a goal, the behavior of the agents, i.e., agentic behaviors,
should be evaluated not only on their achievements (i.e., outcome evaluation),
but also how they achieved that (i.e., procedure evaluation). For this, we
propose APEC Agent Constitution, a list of criteria that an agent should follow
for good agentic behaviors, including Accuracy, Proactivity, Efficiency and
Credibility. To verify whether APEC aligns with human preferences, we develop
APEC-Travel, a travel planning agent that proactively extracts hidden
personalized needs via multi-round dialog with travelers. APEC-Travel is
constructed purely from synthetic data generated by Llama3.1-405B-Instruct with
a diverse set of travelers' persona to simulate rich distribution of dialogs.
Iteratively fine-tuned to follow APEC Agent Constitution, APEC-Travel surpasses
baselines by 20.7% on rule-based metrics and 9.1% on LLM-as-a-Judge scores
across the constitution axes.",2024-11-21,"Song Jiang, Da JU, Andrew Cohen, Sasha Mitts, Aaron Foss, Justine T Kao, Xian Li, Yuandong Tian",http://arxiv.org/pdf/2411.13904v1,cs.CL
PIORS: Personalized Intelligent Outpatient Reception based on Large Language Model with Multi-Agents Medical Scenario Simulation,"In China, receptionist nurses face overwhelming workloads in outpatient
settings, limiting their time and attention for each patient and ultimately
reducing service quality. In this paper, we present the Personalized
Intelligent Outpatient Reception System (PIORS). This system integrates an
LLM-based reception nurse and a collaboration between LLM and hospital
information system (HIS) into real outpatient reception setting, aiming to
deliver personalized, high-quality, and efficient reception services.
Additionally, to enhance the performance of LLMs in real-world healthcare
scenarios, we propose a medical conversational data generation framework named
Service Flow aware Medical Scenario Simulation (SFMSS), aiming to adapt the LLM
to the real-world environments and PIORS settings. We evaluate the
effectiveness of PIORS and SFMSS through automatic and human assessments
involving 15 users and 15 clinical experts. The results demonstrate that
PIORS-Nurse outperforms all baselines, including the current state-of-the-art
model GPT-4o, and aligns with human preferences and clinical needs. Further
details and demo can be found at https://github.com/FudanDISC/PIORS",2024-11-21,"Zhijie Bao, Qingyun Liu, Ying Guo, Zhengqiang Ye, Jun Shen, Shirong Xie, Jiajie Peng, Xuanjing Huang, Zhongyu Wei",http://arxiv.org/pdf/2411.13902v1,cs.CL
Robust Detection of Watermarks for Large Language Models Under Human Edits,"Watermarking has offered an effective approach to distinguishing text
generated by large language models (LLMs) from human-written text. However, the
pervasive presence of human edits on LLM-generated text dilutes watermark
signals, thereby significantly degrading detection performance of existing
methods. In this paper, by modeling human edits through mixture model
detection, we introduce a new method in the form of a truncated goodness-of-fit
test for detecting watermarked text under human edits, which we refer to as
Tr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection
of the Gumbel-max watermark in a certain asymptotic regime of substantial text
modifications and vanishing watermark signals. Importantly, Tr-GoF achieves
this optimality \textit{adaptively} as it does not require precise knowledge of
human edit levels or probabilistic specifications of the LLMs, in contrast to
the optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover,
we establish that the Tr-GoF test attains the highest detection efficiency rate
in a certain regime of moderate text modifications. In stark contrast, we show
that sum-based detection rules, as employed by existing methods, fail to
achieve optimal robustness in both regimes because the additive nature of their
statistics is less resilient to edit-induced noise. Finally, we demonstrate the
competitive and sometimes superior empirical performance of the Tr-GoF test on
both synthetic data and open-source LLMs in the OPT and LLaMA families.",2024-11-21,"Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su",http://arxiv.org/pdf/2411.13868v1,cs.CL
Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for Exploration and Exploitation in Recommender Systems,"Modern recommender systems often create information cocoons, restricting
users' exposure to diverse content. A key challenge lies in balancing content
exploration and exploitation while allowing users to adjust their
recommendation preferences. Intuitively, this balance can be modeled as a
tree-structured representation, where depth search facilitates exploitation and
breadth search enables exploration. However, existing approaches face two
fundamental limitations: Euclidean methods struggle to capture hierarchical
structures, while hyperbolic methods, despite their superior hierarchical
modeling, lack semantic understanding of user and item profiles and fail to
provide a principled mechanism for balancing exploration and exploitation. To
address these challenges, we propose HERec, a hyperbolic graph-LLM framework
that effectively balances exploration and exploitation in recommender systems.
Our framework introduces two key innovations: (1) a semantic-enhanced
hierarchical mechanism that aligns rich textual descriptions processed by large
language models (LLMs) with collaborative information directly in hyperbolic
space, allowing for more nuanced updates that respect the underlying
hierarchical structure in user-item profiles; (2) an automatic hierarchical
representation by optimizing Dasgupta's cost, which discovers hierarchical
structures without requiring predefined hyperparameters, enabling
user-adjustable exploration-exploitation trade-offs. Extensive experiments
demonstrate that HERec consistently outperforms both Euclidean and hyperbolic
baselines, achieving up to 5.49% improvement in utility metrics and 11.39%
increase in diversity metrics, effectively mitigating information cocoons. We
open-source our model implementation at https://github.com/Martin-qyma/HERec.",2024-11-21,"Qiyao Ma, Menglin Yang, Mingxuan Ju, Tong Zhao, Neil Shah, Rex Ying",http://arxiv.org/pdf/2411.13865v3,cs.CL
Exploring Accuracy-Fairness Trade-off in Large Language Models,"Large Language Models (LLMs) have made significant strides in the field of
artificial intelligence, showcasing their ability to interact with humans and
influence human cognition through information dissemination. However, recent
studies have brought to light instances of bias inherent within these LLMs,
presenting a critical issue that demands attention. In our research, we delve
deeper into the intricate challenge of harmonising accuracy and fairness in the
enhancement of LLMs. While improving accuracy can indeed enhance overall LLM
performance, it often occurs at the expense of fairness. Overemphasising
optimisation of one metric invariably leads to a significant degradation of the
other. This underscores the necessity of taking into account multiple
considerations during the design and optimisation phases of LLMs. Therefore, we
advocate for reformulating the LLM training process as a multi-objective
learning task. Our investigation reveals that multi-objective evolutionary
learning (MOEL) methodologies offer promising avenues for tackling this
challenge. Our MOEL framework enables the simultaneous optimisation of both
accuracy and fairness metrics, resulting in a Pareto-optimal set of LLMs. In
summary, our study sheds valuable lights on the delicate equilibrium between
accuracy and fairness within LLMs, which is increasingly significant for their
real-world applications. By harnessing MOEL, we present a promising pathway
towards fairer and more efficacious AI technologies.",2024-11-21,"Qingquan Zhang, Qiqi Duan, Bo Yuan, Yuhui Shi, Jialin Liu",http://arxiv.org/pdf/2411.14500v1,cs.CL
Interactive and Expressive Code-Augmented Planning with Large Language Models,"Large Language Models (LLMs) demonstrate strong abilities in common-sense
reasoning and interactive decision-making, but often struggle with complex,
long-horizon planning tasks. Recent techniques have sought to structure LLM
outputs using control flow and other code-adjacent techniques to improve
planning performance. These techniques include using variables (to track
important information) and functions (to divide complex tasks into smaller
re-usable sub-tasks). However, purely code-based approaches can be error-prone
and insufficient for handling ambiguous or unstructured data. To address these
challenges, we propose REPL-Plan, an LLM planning approach that is fully
code-expressive (it can utilize all the benefits of code) while also being
dynamic (it can flexibly adapt from errors and use the LLM for fuzzy
situations). In REPL-Plan, an LLM solves tasks by interacting with a
Read-Eval-Print Loop (REPL), which iteratively executes and evaluates code,
similar to language shells or interactive code notebooks, allowing the model to
flexibly correct errors and handle tasks dynamically. We demonstrate that
REPL-Plan achieves strong results across various planning domains compared to
previous methods.",2024-11-21,"Anthony Z. Liu, Xinhe Wang, Jacob Sansom, Yao Fu, Jongwook Choi, Sungryull Sohn, Jaekyeom Kim, Honglak Lee",http://arxiv.org/pdf/2411.13826v1,cs.CL
Understanding World or Predicting Future? A Comprehensive Survey of World Models,"The concept of world models has garnered significant attention due to
advancements in multimodal large language models such as GPT-4 and video
generation models such as Sora, which are central to the pursuit of artificial
general intelligence. This survey offers a comprehensive review of the
literature on world models. Generally, world models are regarded as tools for
either understanding the present state of the world or predicting its future
dynamics. This review presents a systematic categorization of world models,
emphasizing two primary functions: (1) constructing internal representations to
understand the mechanisms of the world, and (2) predicting future states to
simulate and guide decision-making. Initially, we examine the current progress
in these two categories. We then explore the application of world models in key
domains, including autonomous driving, robotics, and social simulacra, with a
focus on how each domain utilizes these aspects. Finally, we outline key
challenges and provide insights into potential future research directions.",2024-11-21,"Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, Fengli Xu, Yong Li",http://arxiv.org/pdf/2411.14499v1,cs.CL
InstCache: A Predictive Cache for LLM Serving,"Large language models are revolutionizing every aspect of human life.
However, the unprecedented power comes at the cost of significant computing
intensity, suggesting long latency and large energy footprint. Key-Value Cache
and Semantic Cache have been proposed as a solution to the above problem, but
both suffer from limited scalability due to significant memory cost for each
token or instruction embeddings. Motivated by the observations that most
instructions are short, repetitive and predictable by LLMs, we propose to
predict user-instructions by an instruction-aligned LLM and store them in a
predictive cache, so-called InstCache. We introduce an instruction
pre-population algorithm based on the negative log likelihood of instructions,
determining the cache size with regard to the hit rate. The proposed InstCache
is efficiently implemented as a hash table with minimal lookup latency for
deployment. Experimental results show that InstCache can achieve up to 51.34%
hit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost
of only 4.5GB.",2024-11-21,"Longwei Zou, Tingfeng Liu, Kai Chen, Jiangang Kong, Yangdong Deng",http://arxiv.org/pdf/2411.13820v1,cs.CL
"SemiKong: Curating, Training, and Evaluating A Semiconductor Industry-Specific Large Language Model","Large Language Models (LLMs) have demonstrated the potential to address some
issues within the semiconductor industry. However, they are often
general-purpose models that lack the specialized knowledge needed to tackle the
unique challenges of this sector, such as the intricate physics and chemistry
of semiconductor devices and processes. SemiKong, the first industry-specific
LLM for the semiconductor domain, provides a foundation that can be used to
develop tailored proprietary models. With SemiKong 1.0, we aim to develop a
foundational model capable of understanding etching problems at an expert
level. Our key contributions include (a) curating a comprehensive corpus of
semiconductor-related texts, (b) creating a foundational model with in-depth
semiconductor knowledge, and (c) introducing a framework for integrating expert
knowledge, thereby advancing the evaluation process of domain-specific AI
models. Through fine-tuning a pre-trained LLM using our curated dataset, we
have shown that SemiKong outperforms larger, general-purpose LLMs in various
semiconductor manufacturing and design tasks. Our extensive experiments
underscore the importance of developing domain-specific LLMs as a foundation
for company- or tool-specific proprietary models, paving the way for further
research and applications in the semiconductor domain. Code and dataset will be
available at https://github.com/aitomatic/semikong",2024-11-21,"Christopher Nguyen, William Nguyen, Atsushi Suzuki, Daisuke Oku, Hong An Phan, Sang Dinh, Zooey Nguyen, Anh Ha, Shruti Raghavan, Huy Vo, Thang Nguyen, Lan Nguyen, Yoshikuni Hirayama",http://arxiv.org/pdf/2411.13802v2,cs.CL
Explaining GPT-4's Schema of Depression Using Machine Behavior Analysis,"Use of large language models such as ChatGPT (GPT-4) for mental health
support has grown rapidly, emerging as a promising route to assess and help
people with mood disorders, like depression. However, we have a limited
understanding of GPT-4's schema of mental disorders, that is, how it internally
associates and interprets symptoms. In this work, we leveraged contemporary
measurement theory to decode how GPT-4 interrelates depressive symptoms to
inform both clinical utility and theoretical understanding. We found GPT-4's
assessment of depression: (a) had high overall convergent validity (r = .71
with self-report on 955 samples, and r = .81 with experts judgments on 209
samples); (b) had moderately high internal consistency (symptom
inter-correlates r = .23 to .78 ) that largely aligned with literature and
self-report; except that GPT-4 (c) underemphasized suicidality's -- and
overemphasized psychomotor's -- relationship with other symptoms, and (d) had
symptom inference patterns that suggest nuanced hypotheses (e.g. sleep and
fatigue are influenced by most other symptoms while feelings of
worthlessness/guilt is mostly influenced by depressed mood).",2024-11-21,"Adithya V Ganesan, Vasudha Varadarajan, Yash Kumar Lal, Veerle C. Eijsbroek, Katarina Kjell, Oscar N. E. Kjell, Tanuja Dhanasekaran, Elizabeth C. Stade, Johannes C. Eichstaedt, Ryan L. Boyd, H. Andrew Schwartz, Lucie Flek",http://arxiv.org/pdf/2411.13800v1,cs.CL
Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning,"The efficacy of large language models (LLMs) on downstream tasks usually
hinges on instruction tuning, which relies critically on the quality of
training data. Unfortunately, collecting high-quality and diverse data is both
expensive and time-consuming. To mitigate this issue, we propose a novel
Star-Agents framework, which automates the enhancement of data quality across
datasets through multi-agent collaboration and assessment. The framework adopts
a three-pronged strategy. It initially generates diverse instruction data with
multiple LLM agents through a bespoke sampling method. Subsequently, the
generated data undergo a rigorous evaluation using a dual-model method that
assesses both difficulty and quality. Finaly, the above process evolves in a
dynamic refinement phase, where more effective LLMs are prioritized, enhancing
the overall data quality. Our empirical studies, including instruction tuning
experiments with models such as Pythia and LLaMA, demonstrate the effectiveness
of the proposed framework. Optimized datasets have achieved substantial
improvements, with an average increase of 12% and notable gains in specific
metrics, such as a 40% improvement in Fermi, as evidenced by benchmarks like
MT-bench, Vicuna bench, and WizardLM testset.",2024-11-21,"Hang Zhou, Yehui Tang, Haochen Qin, Yujie Yang, Renren Jin, Deyi Xiong, Kai Han, Yunhe Wang",http://arxiv.org/pdf/2411.14497v1,cs.CL
Adaptable Embeddings Network (AEN),"Modern day Language Models see extensive use in text classification, yet this
comes at significant computational cost. Compute-effective classification
models are needed for low-resource environments, most notably on edge devices.
We introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder
architecture using Kernel Density Estimation (KDE). This architecture allows
for runtime adaptation of classification criteria without retraining and is
non-autoregressive. Through thorough synthetic data experimentation, we
demonstrate our model outputs comparable and in certain cases superior results
to that of autoregressive models an order of magnitude larger than AEN's size.
The architecture's ability to preprocess and cache condition embeddings makes
it ideal for edge computing applications and real-time monitoring systems.",2024-11-21,"Stan Loosmore, Alexander Titus",http://arxiv.org/pdf/2411.13786v1,cs.CL
NewsInterview: a Dataset and a Playground to Evaluate LLMs' Ground Gap via Informational Interviews,"Large Language Models (LLMs) have demonstrated impressive capabilities in
generating coherent text but often struggle with grounding language and
strategic dialogue. To address this gap, we focus on journalistic interviews, a
domain rich in grounding communication and abundant in data. We curate a
dataset of 40,000 two-person informational interviews from NPR and CNN, and
reveal that LLMs are significantly less likely than human interviewers to use
acknowledgements and to pivot to higher-level questions. Realizing that a
fundamental deficit exists in multi-turn planning and strategic thinking, we
develop a realistic simulated environment, incorporating source personas and
persuasive elements, in order to facilitate the development of agents with
longer-horizon rewards. Our experiments show that while source LLMs mimic human
behavior in information sharing, interviewer LLMs struggle with recognizing
when questions are answered and engaging persuasively, leading to suboptimal
information extraction across model size and capability. These findings
underscore the need for enhancing LLMs' strategic dialogue capabilities.",2024-11-21,"Michael Lu, Hyundong Justin Cho, Weiyan Shi, Jonathan May, Alexander Spangher",http://arxiv.org/pdf/2411.13779v1,cs.CL
"Benchmarking GPT-4 against Human Translators: A Comprehensive Evaluation Across Languages, Domains, and Expertise Levels","This study presents a comprehensive evaluation of GPT-4's translation
capabilities compared to human translators of varying expertise levels. Through
systematic human evaluation using the MQM schema, we assess translations across
three language pairs (Chinese$\longleftrightarrow$English,
Russian$\longleftrightarrow$English, and Chinese$\longleftrightarrow$Hindi) and
three domains (News, Technology, and Biomedical). Our findings reveal that
GPT-4 achieves performance comparable to junior-level translators in terms of
total errors, while still lagging behind senior translators. Unlike traditional
Neural Machine Translation systems, which show significant performance
degradation in resource-poor language directions, GPT-4 maintains consistent
translation quality across all evaluated language pairs. Through qualitative
analysis, we identify distinctive patterns in translation approaches: GPT-4
tends toward overly literal translations and exhibits lexical inconsistency,
while human translators sometimes over-interpret context and introduce
hallucinations. This study represents the first systematic comparison between
LLM and human translators across different proficiency levels, providing
valuable insights into the current capabilities and limitations of LLM-based
translation systems.",2024-11-21,"Jianhao Yan, Pingchuan Yan, Yulong Chen, Jing Li, Xianchao Zhu, Yue Zhang",http://arxiv.org/pdf/2411.13775v1,cs.CL
NewsHomepages: Homepage Layouts Capture Information Prioritization Decisions,"Information prioritization plays an important role in how humans perceive and
understand the world. Homepage layouts serve as a tangible proxy for this
prioritization. In this work, we present NewsHomepages, a large dataset of over
3,000 new website homepages (including local, national and topic-specific
outlets) captured twice daily over a three-year period. We develop models to
perform pairwise comparisons between news items to infer their relative
significance. To illustrate that modeling organizational hierarchies has
broader implications, we applied our models to rank-order a collection of local
city council policies passed over a ten-year period in San Francisco, assessing
their ""newsworthiness"". Our findings lay the groundwork for leveraging implicit
organizational cues to deepen our understanding of information prioritization.",2024-11-21,"Ben Welsh, Naitian Zhou, Arda Kaz, Michael Vu, Alexander Spangher",http://arxiv.org/pdf/2501.00004v1,cs.CL
A Framework for Evaluating LLMs Under Task Indeterminacy,"Large language model (LLM) evaluations often assume there is a single correct
response -- a gold label -- for each item in the evaluation corpus. However,
some tasks can be ambiguous -- i.e., they provide insufficient information to
identify a unique interpretation -- or vague -- i.e., they do not clearly
indicate where to draw the line when making a determination. Both ambiguity and
vagueness can cause task indeterminacy -- the condition where some items in the
evaluation corpus have more than one correct response. In this paper, we
develop a framework for evaluating LLMs under task indeterminacy. Our framework
disentangles the relationships between task specification, human ratings, and
LLM responses in the LLM evaluation pipeline. Using our framework, we conduct a
synthetic experiment showing that evaluations that use the ""gold label""
assumption underestimate the true performance. We also provide a method for
estimating an error-adjusted performance interval given partial knowledge about
indeterminate items in the evaluation corpus. We conclude by outlining
implications of our work for the research community.",2024-11-21,"Luke Guerdan, Hanna Wallach, Solon Barocas, Alexandra Chouldechova",http://arxiv.org/pdf/2411.13760v1,cs.CL
Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human Perceptions and Official Statistics,"This study investigates gender bias in large language models (LLMs) by
comparing their gender perception to that of human respondents, U.S. Bureau of
Labor Statistics data, and a 50% no-bias benchmark. We created a new evaluation
set using occupational data and role-specific sentences. Unlike common
benchmarks included in LLM training data, our set is newly developed,
preventing data leakage and test set contamination. Five LLMs were tested to
predict the gender for each role using single-word answers. We used
Kullback-Leibler (KL) divergence to compare model outputs with human
perceptions, statistical data, and the 50% neutrality benchmark. All LLMs
showed significant deviation from gender neutrality and aligned more with
statistical data, still reflecting inherent biases.",2024-11-20,Tetiana Bas,http://arxiv.org/pdf/2411.13738v1,cs.CL
Test Security in Remote Testing Age: Perspectives from Process Data Analytics and AI,"The COVID-19 pandemic has accelerated the implementation and acceptance of
remotely proctored high-stake assessments. While the flexible administration of
the tests brings forth many values, it raises test security-related concerns.
Meanwhile, artificial intelligence (AI) has witnessed tremendous advances in
the last five years. Many AI tools (such as the very recent ChatGPT) can
generate high-quality responses to test items. These new developments require
test security research beyond the statistical analysis of scores and response
time. Data analytics and AI methods based on clickstream process data can get
us deeper insight into the test-taking process and hold great promise for
securing remotely administered high-stakes tests. This chapter uses real-world
examples to show that this is indeed the case.",2024-11-20,"Jiangang Hao, Michael Fauss",http://arxiv.org/pdf/2411.13699v2,cs.CL
Retrieval-Augmented Generation for Domain-Specific Question Answering: A Case Study on Pittsburgh and CMU,"We designed a Retrieval-Augmented Generation (RAG) system to provide large
language models with relevant documents for answering domain-specific questions
about Pittsburgh and Carnegie Mellon University (CMU). We extracted over 1,800
subpages using a greedy scraping strategy and employed a hybrid annotation
process, combining manual and Mistral-generated question-answer pairs,
achieving an inter-annotator agreement (IAA) score of 0.7625. Our RAG framework
integrates BM25 and FAISS retrievers, enhanced with a reranker for improved
document retrieval accuracy. Experimental results show that the RAG system
significantly outperforms a non-RAG baseline, particularly in time-sensitive
and complex queries, with an F1 score improvement from 5.45% to 42.21% and
recall of 56.18%. This study demonstrates the potential of RAG systems in
enhancing answer precision and relevance, while identifying areas for further
optimization in document retrieval and model training.",2024-11-20,"Haojia Sun, Yaqi Wang, Shuting Zhang",http://arxiv.org/pdf/2411.13691v1,cs.CL
Hierarchical Text Classification (HTC) vs. eXtreme Multilabel Classification (XML): Two Sides of the Same Medal,"Assigning a subset of labels from a fixed pool of labels to a given input
text is a text classification problem with many real-world applications, such
as in recommender systems. Two separate research streams address this issue.
Hierarchical Text Classification (HTC) focuses on datasets with smaller label
pools of hundreds of entries, accompanied by a semantic label hierarchy. In
contrast, eXtreme Multi-Label Text Classification (XML) considers very large
label pools with up to millions of entries, in which the labels are not
arranged in any particular manner. However, in XML, a common approach is to
construct an artificial hierarchy without any semantic information before or
during the training process. Here, we investigate how state-of-the-art models
from one domain perform when trained and tested on datasets from the other
domain. The HBGL and HGLCR models from the HTC domain are trained and tested on
the datasets Wiki10-31K, AmazonCat-13K, and Amazon-670K from the XML domain. On
the other side, the XML models CascadeXML and XR-Transformer are trained and
tested on the datasets Web of Science, The New York Times Annotated Corpus, and
RCV1-V2 from the HTC domain. HTC models, on the other hand, are not equipped to
handle the size of XML datasets and achieve poor transfer results. The code and
numerous files that are needed to reproduce our results can be obtained from
https://github.com/FloHauss/XMC_HTC",2024-11-20,"Nerijus Bertalis, Paul Granse, Ferhat Gül, Florian Hauss, Leon Menkel, David Schüler, Tom Speier, Lukas Galke, Ansgar Scherp",http://arxiv.org/pdf/2411.13687v2,cs.CL
Hymba: A Hybrid-head Architecture for Small Language Models,"We propose Hymba, a family of small language models featuring a hybrid-head
parallel architecture that integrates transformer attention mechanisms with
state space models (SSMs) for enhanced efficiency. Attention heads provide
high-resolution recall, while SSM heads enable efficient context summarization.
Additionally, we introduce learnable meta tokens that are prepended to prompts,
storing critical information and alleviating the ""forced-to-attend"" burden
associated with attention mechanisms. This model is further optimized by
incorporating cross-layer key-value (KV) sharing and partial sliding window
attention, resulting in a compact cache size. During development, we conducted
a controlled study comparing various architectures under identical settings and
observed significant advantages of our proposed architecture. Notably, Hymba
achieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model
surpasses all sub-2B public models in performance and even outperforms
Llama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size
reduction, and 3.49x throughput.",2024-11-20,"Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Lin, Jan Kautz, Pavlo Molchanov",http://arxiv.org/pdf/2411.13676v1,cs.CL
Predictive Insights into LGBTQ+ Minority Stress: A Transductive Exploration of Social Media Discourse,"Individuals who identify as sexual and gender minorities, including lesbian,
gay, bisexual, transgender, queer, and others (LGBTQ+) are more likely to
experience poorer health than their heterosexual and cisgender counterparts.
One primary source that drives these health disparities is minority stress
(i.e., chronic and social stressors unique to LGBTQ+ communities' experiences
adapting to the dominant culture). This stress is frequently expressed in
LGBTQ+ users' posts on social media platforms. However, these expressions are
not just straightforward manifestations of minority stress. They involve
linguistic complexity (e.g., idiom or lexical diversity), rendering them
challenging for many traditional natural language processing methods to detect.
In this work, we designed a hybrid model using Graph Neural Networks (GNN) and
Bidirectional Encoder Representations from Transformers (BERT), a pre-trained
deep language model to improve the classification performance of minority
stress detection. We experimented with our model on a benchmark social media
dataset for minority stress detection (LGBTQ+ MiSSoM+). The dataset is
comprised of 5,789 human-annotated Reddit posts from LGBTQ+ subreddits. Our
approach enables the extraction of hidden linguistic nuances through
pretraining on a vast amount of raw data, while also engaging in transductive
learning to jointly develop representations for both labeled training data and
unlabeled test data. The RoBERTa-GCN model achieved an accuracy of 0.86 and an
F1 score of 0.86, surpassing the performance of other baseline models in
predicting LGBTQ+ minority stress. Improved prediction of minority stress
expressions on social media could lead to digital health interventions to
improve the wellbeing of LGBTQ+ people-a community with high rates of
stress-sensitive health problems.",2024-11-20,"S. Chapagain, Y. Zhao, T. K. Rohleen, S. M. Hamdi, S. F. Boubrahimi, R. E. Flinn, E. M. Lund, D. Klooster, J. R. Scheer, C. J. Cascalheira",http://arxiv.org/pdf/2411.13534v1,cs.CL
SlideSpawn: An Automatic Slides Generation System for Research Publications,"Research papers are well structured documents. They have text, figures,
equations, tables etc., to covey their ideas and findings. They are divided
into sections like Introduction, Model, Experiments etc., which deal with
different aspects of research. Characteristics like these set research papers
apart from ordinary documents and allows us to significantly improve their
summarization. In this paper, we propose a novel system, SlideSpwan, that takes
PDF of a research document as an input and generates a quality presentation
providing it's summary in a visual and concise fashion. The system first
converts the PDF of the paper to an XML document that has the structural
information about various elements. Then a machine learning model, trained on
PS5K dataset and Aminer 9.5K Insights dataset (that we introduce), is used to
predict salience of each sentence in the paper. Sentences for slides are
selected using ILP and clustered based on their similarity with each cluster
being given a suitable title. Finally a slide is generated by placing any
graphical element referenced in the selected sentences next to them.
Experiments on a test set of 650 pairs of papers and slides demonstrate that
our system generates presentations with better quality.",2024-11-20,"Keshav Kumar, Ravindranath Chowdary",http://arxiv.org/pdf/2411.17719v1,cs.CL
Advancing Complex Medical Communication in Arabic with Sporo AraSum: Surpassing Existing Large Language Models,"The increasing demand for multilingual capabilities in healthcare underscores
the need for AI models adept at processing diverse languages, particularly in
clinical documentation and decision-making. Arabic, with its complex
morphology, syntax, and diglossia, poses unique challenges for natural language
processing (NLP) in medical contexts. This case study evaluates Sporo AraSum, a
language model tailored for Arabic clinical documentation, against JAIS, the
leading Arabic NLP model. Using synthetic datasets and modified PDQI-9 metrics
modified ourselves for the purposes of assessing model performances in a
different language. The study assessed the models' performance in summarizing
patient-physician interactions, focusing on accuracy, comprehensiveness,
clinical utility, and linguistic-cultural competence.
  Results indicate that Sporo AraSum significantly outperforms JAIS in
AI-centric quantitative metrics and all qualitative attributes measured in our
modified version of the PDQI-9. AraSum's architecture enables precise and
culturally sensitive documentation, addressing the linguistic nuances of Arabic
while mitigating risks of AI hallucinations. These findings suggest that Sporo
AraSum is better suited to meet the demands of Arabic-speaking healthcare
environments, offering a transformative solution for multilingual clinical
workflows. Future research should incorporate real-world data to further
validate these findings and explore broader integration into healthcare
systems.",2024-11-20,"Chanseo Lee, Sonu Kumar, Kimon A. Vogt, Sam Meraj, Antonia Vogt",http://arxiv.org/pdf/2411.13518v1,cs.CL
Disentangling Memory and Reasoning Ability in Large Language Models,"Large Language Models (LLMs) have demonstrated strong performance in handling
complex tasks requiring both extensive knowledge and reasoning abilities.
However, the existing LLM inference pipeline operates as an opaque process
without explicit separation between knowledge retrieval and reasoning steps,
making the model's decision-making process unclear and disorganized. This
ambiguity can lead to issues such as hallucinations and knowledge forgetting,
which significantly impact the reliability of LLMs in high-stakes domains. In
this paper, we propose a new inference paradigm that decomposes the complex
inference process into two distinct and clear actions: (1) memory recall: which
retrieves relevant knowledge, and (2) reasoning: which performs logical steps
based on the recalled knowledge. To facilitate this decomposition, we introduce
two special tokens memory and reason, guiding the model to distinguish between
steps that require knowledge retrieval and those that involve reasoning. Our
experiment results show that this decomposition not only improves model
performance but also enhances the interpretability of the inference process,
enabling users to identify sources of error and refine model responses
effectively. The code is available at
https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.",2024-11-20,"Mingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang, Wenyue Hua, Ruixiang Tang, William Yang Wang, Yongfeng Zhang",http://arxiv.org/pdf/2411.13504v3,cs.CL
From Statistical Methods to Pre-Trained Models; A Survey on Automatic Speech Recognition for Resource Scarce Urdu Language,"Automatic Speech Recognition (ASR) technology has witnessed significant
advancements in recent years, revolutionizing human-computer interactions.
While major languages have benefited from these developments, lesser-resourced
languages like Urdu face unique challenges. This paper provides an extensive
exploration of the dynamic landscape of ASR research, focusing particularly on
the resource-constrained Urdu language, which is widely spoken across South
Asian nations. It outlines current research trends, technological advancements,
and potential directions for future studies in Urdu ASR, aiming to pave the way
for forthcoming researchers interested in this domain. By leveraging
contemporary technologies, analyzing existing datasets, and evaluating
effective algorithms and tools, the paper seeks to shed light on the unique
challenges and opportunities associated with Urdu language processing and its
integration into the broader field of speech research.",2024-11-20,"Muhammad Sharif, Zeeshan Abbas, Jiangyan Yi, Chenglin Liu",http://arxiv.org/pdf/2411.14493v1,cs.CL
Utilizing Large Language Models to Synthesize Product Desirability Datasets,"This research explores the application of large language models (LLMs) to
generate synthetic datasets for Product Desirability Toolkit (PDT) testing, a
key component in evaluating user sentiment and product experience. Utilizing
gpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three
methods, Word+Review, Review+Word, and Supply-Word, were each used to
synthesize 1000 product reviews. The generated datasets were assessed for
sentiment alignment, textual diversity, and data generation cost. Results
demonstrated high sentiment alignment across all methods, with Pearson
correlations ranging from 0.93 to 0.97. Supply-Word exhibited the highest
diversity and coverage of PDT terms, although with increased generation costs.
Despite minor biases toward positive sentiments, in situations with limited
test data, LLM-generated synthetic data offers significant advantages,
including scalability, cost savings, and flexibility in dataset production.",2024-11-20,"John D. Hastings, Sherri Weitl-Harms, Joseph Doty, Zachary J. Myers, Warren Thompson",http://arxiv.org/pdf/2411.13485v2,cs.CL
PatentEdits: Framing Patent Novelty as Textual Entailment,"A patent must be deemed novel and non-obvious in order to be granted by the
US Patent Office (USPTO). If it is not, a US patent examiner will cite the
prior work, or prior art, that invalidates the novelty and issue a non-final
rejection. Predicting what claims of the invention should change given the
prior art is an essential and crucial step in securing invention rights, yet
has not been studied before as a learnable task. In this work we introduce the
PatentEdits dataset, which contains 105K examples of successful revisions that
overcome objections to novelty. We design algorithms to label edits sentence by
sentence, then establish how well these edits can be predicted with large
language models (LLMs). We demonstrate that evaluating textual entailment
between cited references and draft sentences is especially effective in
predicting which inventive claims remained unchanged or are novel in relation
to prior art.",2024-11-20,"Ryan Lee, Alexander Spangher, Xuezhe Ma",http://arxiv.org/pdf/2411.13477v1,cs.CL
When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training,"Extending context window sizes allows large language models (LLMs) to process
longer sequences and handle more complex tasks. Rotary Positional Embedding
(RoPE) has become the de facto standard due to its relative positional encoding
properties that benefit long-context training. However, we observe that using
RoPE with BFloat16 format results in numerical issues, causing it to deviate
from its intended relative positional encoding, especially in long-context
scenarios. This issue arises from BFloat16's limited precision and accumulates
as context length increases, with the first token contributing significantly to
this problem. To address this, we develop AnchorAttention, a plug-and-play
attention method that alleviates numerical issues caused by BFloat16, improves
long-context capabilities, and speeds up training. AnchorAttention reduces
unnecessary attention computations, maintains semantic coherence, and boosts
computational efficiency by treating the first token as a shared anchor with a
consistent position ID, making it visible to all documents within the training
context. Experiments on three types of LLMs demonstrate that AnchorAttention
significantly improves long-context performance and reduces training time by
over 50\% compared to standard full attention mechanisms, while preserving the
original LLM's capabilities on general tasks. Our code is available at
https://github.com/haonan3/AnchorContext.",2024-11-20,"Haonan Wang, Qian Liu, Chao Du, Tongyao Zhu, Cunxiao Du, Kenji Kawaguchi, Tianyu Pang",http://arxiv.org/pdf/2411.13476v2,cs.CL
LIMBA: An Open-Source Framework for the Preservation and Valorization of Low-Resource Languages using Generative Models,"Minority languages are vital to preserving cultural heritage, yet they face
growing risks of extinction due to limited digital resources and the dominance
of artificial intelligence models trained on high-resource languages. This
white paper proposes a framework to generate linguistic tools for low-resource
languages, focusing on data creation to support the development of language
models that can aid in preservation efforts. Sardinian, an endangered language,
serves as the case study to demonstrate the framework's effectiveness. By
addressing the data scarcity that hinders intelligent applications for such
languages, we contribute to promoting linguistic diversity and support ongoing
efforts in language standardization and revitalization through modern
technologies.",2024-11-20,"Salvatore Mario Carta, Stefano Chessa, Giulia Contu, Andrea Corriga, Andrea Deidda, Gianni Fenu, Luca Frigau, Alessandro Giuliani, Luca Grassi, Marco Manolo Manca, Mirko Marras, Francesco Mola, Bastianino Mossa, Piergiorgio Mura, Marco Ortu, Leonardo Piano, Simone Pisano, Alessia Pisu, Alessandro Sebastian Podda, Livio Pompianu, Simone Seu, Sandro Gabriele Tiddia",http://arxiv.org/pdf/2411.13453v1,cs.CL
AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Demonstrations,"State-of-the-art multimodal web agents, powered by Multimodal Large Language
Models (MLLMs), can autonomously execute many web tasks by processing user
instructions and interacting with graphical user interfaces (GUIs). Current
strategies for building web agents rely on (i) the generalizability of
underlying MLLMs and their steerability via prompting, and (ii) large-scale
fine-tuning of MLLMs on web-related tasks. However, web agents still struggle
to automate tasks on unseen websites and domains, limiting their applicability
to enterprise-specific and proprietary platforms. Beyond generalization from
large-scale pre-training and fine-tuning, we propose building agents for
few-shot adaptability using human demonstrations. We introduce the AdaptAgent
framework that enables both proprietary and open-weights multimodal web agents
to adapt to new websites and domains using few human demonstrations (up to 2).
Our experiments on two popular benchmarks -- Mind2Web & VisualWebArena -- show
that using in-context demonstrations (for proprietary models) or
meta-adaptation demonstrations (for meta-learned open-weights models) boosts
task success rate by 3.36% to 7.21% over non-adapted state-of-the-art models,
corresponding to a relative increase of 21.03% to 65.75%. Furthermore, our
additional analyses (a) show the effectiveness of multimodal demonstrations
over text-only ones, (b) shed light on the influence of different data
selection strategies during meta-learning on the generalization of the agent,
and (c) demonstrate the effect of number of few-shot examples on the web
agent's success rate. Overall, our results unlock a complementary axis for
developing widely applicable multimodal web agents beyond large-scale
pre-training and fine-tuning, emphasizing few-shot adaptability.",2024-11-20,"Gaurav Verma, Rachneet Kaur, Nishan Srishankar, Zhen Zeng, Tucker Balch, Manuela Veloso",http://arxiv.org/pdf/2411.13451v1,cs.CL
WaterPark: A Robustness Assessment of Language Model Watermarking,"Various watermarking methods (``watermarkers'') have been proposed to
identify LLM-generated texts; yet, due to the lack of unified evaluation
platforms, many critical questions remain under-explored: i) What are the
strengths/limitations of various watermarkers, especially their attack
robustness? ii) How do various design choices impact their robustness? iii) How
to optimally operate watermarkers in adversarial environments? To fill this
gap, we systematize existing LLM watermarkers and watermark removal attacks,
mapping out their design spaces. We then develop WaterPark, a unified platform
that integrates 10 state-of-the-art watermarkers and 12 representative attacks.
More importantly, by leveraging WaterPark, we conduct a comprehensive
assessment of existing watermarkers, unveiling the impact of various design
choices on their attack robustness. We further explore the best practices to
operate watermarkers in adversarial environments. We believe our study sheds
light on current LLM watermarking techniques while WaterPark serves as a
valuable testbed to facilitate future research.",2024-11-20,"Jiacheng Liang, Zian Wang, Lauren Hong, Shouling Ji, Ting Wang",http://arxiv.org/pdf/2411.13425v2,cs.CL
CAFE A Novel Code switching Dataset for Algerian Dialect French and English,"The paper introduces and publicly releases (Data download link available
after acceptance) CAFE -- the first Code-switching dataset between Algerian
dialect, French, and english languages. The CAFE speech data is unique for (a)
its spontaneous speaking style in vivo human-human conversation capturing
phenomena like code-switching and overlapping speech, (b) addresses distinct
linguistic challenges in North African Arabic dialect; (c) the CAFE captures
dialectal variations from various parts of Algeria within different
sociolinguistic contexts. CAFE data contains approximately 37 hours of speech,
with a subset, CAFE-small, of 2 hours and 36 minutes released with manual human
annotation including speech segmentation, transcription, explicit annotation of
code-switching points, overlapping speech, and other events such as noises, and
laughter among others. The rest approximately 34.58 hours contain pseudo label
transcriptions. In addition to the data release, the paper also highlighted the
challenges of using state-of-the-art Automatic Speech Recognition (ASR) models
such as Whisper large-v2,3 and PromptingWhisper to handle such content.
Following, we benchmark CAFE data with the aforementioned Whisper models and
show how well-designed data processing pipelines and advanced decoding
techniques can improve the ASR performance in terms of Mixed Error Rate (MER)
of 0.310, Character Error Rate (CER) of 0.329 and Word Error Rate (WER) of
0.538.",2024-11-20,"Houssam Eddine-Othman Lachemat, Akli Abbas, Nourredine Oukas, Yassine El Kheir, Samia Haboussi, Absar Chowdhury Shammur",http://arxiv.org/pdf/2411.13424v1,cs.CL
Unification of Balti and trans-border sister dialects in the essence of LLMs and AI Technology,"The language called Balti belongs to the Sino-Tibetan, specifically the
Tibeto-Burman language family. It is understood with variations, across
populations in India, China, Pakistan, Nepal, Tibet, Burma, and Bhutan,
influenced by local cultures and producing various dialects. Considering the
diverse cultural, socio-political, religious, and geographical impacts, it is
important to step forward unifying the dialects, the basis of common root,
lexica, and phonological perspectives, is vital. In the era of globalization
and the increasingly frequent developments in AI technology, understanding the
diversity and the efforts of dialect unification is important to understanding
commonalities and shortening the gaps impacted by unavoidable circumstances.
This article analyzes and examines how artificial intelligence AI in the
essence of Large Language Models LLMs, can assist in analyzing, documenting,
and standardizing the endangered Balti Language, based on the efforts made in
different dialects so far.",2024-11-20,"Muhammad Sharif, Jiangyan Yi, Muhammad Shoaib",http://arxiv.org/pdf/2411.13409v1,cs.CL
Transformer-Based Contextualized Language Models Joint with Neural Networks for Natural Language Inference in Vietnamese,"Natural Language Inference (NLI) is a task within Natural Language Processing
(NLP) that holds value for various AI applications. However, there have been
limited studies on Natural Language Inference in Vietnamese that explore the
concept of joint models. Therefore, we conducted experiments using various
combinations of contextualized language models (CLM) and neural networks. We
use CLM to create contextualized work presentations and use Neural Networks for
classification. Furthermore, we have evaluated the strengths and weaknesses of
each joint model and identified the model failure points in the Vietnamese
context. The highest F1 score in this experiment, up to 82.78% in the benchmark
dataset (ViNLI). By conducting experiments with various models, the most
considerable size of the CLM is XLM-R (355M). That combination has consistently
demonstrated superior performance compared to fine-tuning strong pre-trained
language models like PhoBERT (+6.58%), mBERT (+19.08%), and XLM-R (+0.94%) in
terms of F1-score. This article aims to introduce a novel approach or model
that attains improved performance for Vietnamese NLI. Overall, we find that the
joint approach of CLM and neural networks is simple yet capable of achieving
high-quality performance, which makes it suitable for applications that require
efficient resource utilization.",2024-11-20,"Dat Van-Thanh Nguyen, Tin Van Huynh, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen",http://arxiv.org/pdf/2411.13407v2,cs.CL
On the Way to LLM Personalization: Learning to Remember User Conversations,"Large Language Models (LLMs) have quickly become an invaluable assistant for
a variety of tasks. However, their effectiveness is constrained by their
ability to tailor responses to human preferences and behaviors via
personalization. Prior work in LLM personalization has largely focused on style
transfer or incorporating small factoids about the user, as knowledge injection
remains an open challenge. In this paper, we explore injecting knowledge of
prior conversations into LLMs to enable future work on less redundant,
personalized conversations. We identify two real-world constraints: (1)
conversations are sequential in time and must be treated as such during
training, and (2) per-user personalization is only viable in
parameter-efficient settings. To this aim, we propose PLUM, a pipeline
performing data augmentation for up-sampling conversations as question-answer
pairs, that are then used to finetune a low-rank adaptation adapter with a
weighted cross entropy loss. Even in this first exploration of the problem, we
perform competitively with baselines such as RAG, attaining an accuracy of
81.5% across 100 conversations.",2024-11-20,"Lucie Charlotte Magister, Katherine Metcalf, Yizhe Zhang, Maartje ter Hoeve",http://arxiv.org/pdf/2411.13405v1,cs.CL
NLP Cluster Analysis of Common Core State Standards and NAEP Item Specifications,"Camilli (2024) proposed a methodology using natural language processing (NLP)
to map the relationship of a set of content standards to item specifications.
This study provided evidence that NLP can be used to improve the mapping
process. As part of this investigation, the nominal classifications of
standards and items specifications were used to examine construct equivalence.
In the current paper, we determine the strength of empirical support for the
semantic distinctiveness of these classifications, which are known as ""domains""
for Common Core standards, and ""strands"" for National Assessment of Educational
Progress (NAEP) item specifications. This is accomplished by separate k-means
clustering for standards and specifications of their corresponding embedding
vectors. We then briefly illustrate an application of these findings.",2024-11-20,"Gregory Camilli, Larry Suter",http://arxiv.org/pdf/2412.04482v2,cs.CL
Executable QR codes with Machine Learning for Industrial Applications,"Executable QR codes, also known as eQR codes or just sQRy, are a special kind
of QR codes that embed programs conceived to run on mobile devices like
smartphones. Since the program is directly encoded in binary form within the QR
code, it can be executed even when the reading device is not provided with
Internet access. The applications of this technology are manifold, and range
from smart user guides to advisory systems. The first programming language made
available for eQR is QRtree, which enables the implementation of decision trees
aimed, for example, at guiding the user in operating/maintaining a complex
machinery or for reaching a specific location.
  In this work, an additional language is proposed, we term QRind, which was
specifically devised for Industry. It permits to integrate distinct
computational blocks into the QR code, e.g., machine learning models to enable
predictive maintenance and algorithms to ease machinery usage. QRind permits
the Industry 4.0/5.0 paradigms to be implemented, in part, also in those cases
where Internet is unavailable.",2024-11-20,"Stefano Scanzio, Francesco Velluto, Matteo Rosani, Lukasz Wisniewski, Gianluca Cena",http://arxiv.org/pdf/2411.13400v1,cs.CL
Fact-Level Confidence Calibration and Self-Correction,"Confidence calibration in LLMs, i.e., aligning their self-assessed confidence
with the actual accuracy of their responses, enabling them to self-evaluate the
correctness of their outputs. However, current calibration methods for LLMs
typically estimate two scalars to represent overall response confidence and
correctness, which is inadequate for long-form generation where the response
includes multiple atomic facts and may be partially confident and correct.
These methods also overlook the relevance of each fact to the query. To address
these challenges, we propose a Fact-Level Calibration framework that operates
at a finer granularity, calibrating confidence to relevance-weighted
correctness at the fact level. Furthermore, comprehensive analysis under the
framework inspired the development of Confidence-Guided Fact-level
Self-Correction ($\textbf{ConFix}$), which uses high-confidence facts within a
response as additional knowledge to improve low-confidence ones. Extensive
experiments across four datasets and six models demonstrate that ConFix
effectively mitigates hallucinations without requiring external knowledge
sources such as retrieval systems.",2024-11-20,"Yige Yuan, Bingbing Xu, Hexiang Tan, Fei Sun, Teng Xiao, Wei Li, Huawei Shen, Xueqi Cheng",http://arxiv.org/pdf/2411.13343v1,cs.CL
Towards Advanced Speech Signal Processing: A Statistical Perspective on Convolution-Based Architectures and its Applications,"This article surveys convolution-based models including convolutional neural
networks (CNNs), Conformers, ResNets, and CRNNs-as speech signal processing
models and provide their statistical backgrounds and speech recognition,
speaker identification, emotion recognition, and speech enhancement
applications. Through comparative training cost assessment, model size,
accuracy and speed assessment, we compare the strengths and weaknesses of each
model, identify potential errors and propose avenues for further research,
emphasizing the central role it plays in advancing applications of speech
technologies.",2024-11-20,"Nirmal Joshua Kapu, Raghav Karan",http://arxiv.org/pdf/2411.18636v1,cs.CL
Combining Autoregressive and Autoencoder Language Models for Text Classification,"This paper presents CAALM-TC (Combining Autoregressive and Autoencoder
Language Models for Text Classification), a novel method that enhances text
classification by integrating autoregressive and autoencoder language models.
Autoregressive large language models such as Open AI's GPT, Meta's Llama or
Microsoft's Phi offer promising prospects for content analysis practitioners,
but they generally underperform supervised BERT based models for text
classification. CAALM leverages autoregressive models to generate contextual
information based on input texts, which is then combined with the original text
and fed into an autoencoder model for classification. This hybrid approach
capitalizes on the extensive contextual knowledge of autoregressive models and
the efficient classification capabilities of autoencoders. Experimental results
on four benchmark datasets demonstrate that CAALM consistently outperforms
existing methods, particularly in tasks with smaller datasets and more abstract
classification objectives. The findings indicate that CAALM offers a scalable
and effective solution for automated content analysis in social science
research that minimizes sample size requirements.",2024-11-20,João Gonçalves,http://arxiv.org/pdf/2411.13282v2,cs.CL
VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation,"Large multimodal models (LMMs) with advanced video analysis capabilities have
recently garnered significant attention. However, most evaluations rely on
traditional methods like multiple-choice questions in benchmarks such as
VideoMME and LongVideoBench, which are prone to lack the depth needed to
capture the complex demands of real-world users. To address this limitation-and
due to the prohibitive cost and slow pace of human annotation for video
tasks-we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS
Chatbot Arena's framework, designed to automatically assess LMMs' video
analysis abilities. VideoAutoArena utilizes user simulation to generate
open-ended, adaptive questions that rigorously assess model performance in
video understanding. The benchmark features an automated, scalable evaluation
framework, incorporating a modified ELO Rating System for fair and continuous
comparisons across multiple LMMs. To validate our automated judging system, we
construct a 'gold standard' using a carefully curated subset of human
annotations, demonstrating that our arena strongly aligns with human judgment
while maintaining scalability. Additionally, we introduce a fault-driven
evolution strategy, progressively increasing question complexity to push models
toward handling more challenging video analysis scenarios. Experimental results
demonstrate that VideoAutoArena effectively differentiates among
state-of-the-art LMMs, providing insights into model strengths and areas for
improvement. To further streamline our evaluation, we introduce VideoAutoBench
as an auxiliary benchmark, where human annotators label winners in a subset of
VideoAutoArena battles. We use GPT-4o as a judge to compare responses against
these human-validated answers. Together, VideoAutoArena and VideoAutoBench
offer a cost-effective, and scalable framework for evaluating LMMs in
user-centric video analysis.",2024-11-20,"Ziyang Luo, Haoning Wu, Dongxu Li, Jing Ma, Mohan Kankanhalli, Junnan Li",http://arxiv.org/pdf/2411.13281v2,cs.CL
A Survey on Human-Centric LLMs,"The rapid evolution of large language models (LLMs) and their capacity to
simulate human cognition and behavior has given rise to LLM-based frameworks
and tools that are evaluated and applied based on their ability to perform
tasks traditionally performed by humans, namely those involving cognition,
decision-making, and social interaction. This survey provides a comprehensive
examination of such human-centric LLM capabilities, focusing on their
performance in both individual tasks (where an LLM acts as a stand-in for a
single human) and collective tasks (where multiple LLMs coordinate to mimic
group dynamics). We first evaluate LLM competencies across key areas including
reasoning, perception, and social cognition, comparing their abilities to
human-like skills. Then, we explore real-world applications of LLMs in
human-centric domains such as behavioral science, political science, and
sociology, assessing their effectiveness in replicating human behaviors and
interactions. Finally, we identify challenges and future research directions,
such as improving LLM adaptability, emotional intelligence, and cultural
sensitivity, while addressing inherent biases and enhancing frameworks for
human-AI collaboration. This survey aims to provide a foundational
understanding of LLMs from a human-centric perspective, offering insights into
their current capabilities and potential for future development.",2024-11-20,"Jing Yi Wang, Nicholas Sukiennik, Tong Li, Weikang Su, Qianyue Hao, Jingbo Xu, Zihan Huang, Fengli Xu, Yong Li",http://arxiv.org/pdf/2411.14491v3,cs.CL
Leveraging Prior Experience: An Expandable Auxiliary Knowledge Base for Text-to-SQL,"Large Language Models (LLMs) exhibit impressive problem-solving skills across
many tasks, but they still underperform compared to humans in various
downstream applications, such as text-to-SQL. On the BIRD benchmark
leaderboard, human performance achieves an accuracy of 92.96\%, whereas the
top-performing method reaches only 72.39\%. Notably, these state-of-the-art
(SoTA) methods predominantly rely on in-context learning to simulate human-like
reasoning. However, they overlook a critical human skill: continual learning.
Inspired by the educational practice of maintaining mistake notebooks during
our formative years, we propose LPE-SQL (Leveraging Prior Experience: An
Expandable Auxiliary Knowledge Base for Text-to-SQL), a novel framework
designed to augment LLMs by enabling continual learning without requiring
parameter fine-tuning. LPE-SQL consists of four modules that \textbf{i)}
retrieve relevant entries, \textbf{ii)} efficient sql generation, \textbf{iii)}
generate the final result through a cross-consistency mechanism and
\textbf{iv)} log successful and failed tasks along with their reasoning
processes or reflection-generated tips. Importantly, the core module of LPE-SQL
is the fourth one, while the other modules employ foundational methods,
allowing LPE-SQL to be easily integrated with SoTA technologies to further
enhance performance. Our experimental results demonstrate that this continual
learning approach yields substantial performance gains, with the smaller
Llama-3.1-70B model with surpassing the performance of the larger
Llama-3.1-405B model using SoTA methods.",2024-11-20,"Zhibo Chu, Zichong Wang, Qitao Qin",http://arxiv.org/pdf/2411.13244v1,cs.CL
BIPro: Zero-shot Chinese Poem Generation via Block Inverse Prompting Constrained Generation Framework,"Recently, generative pre-trained models have made significant strides,
particularly highlighted by the release of ChatGPT and GPT-4, which exhibit
superior cross-domain capabilities. However, these models still face challenges
on constrained writing tasks like poem generation under open-domain titles. In
response to this challenge, we introduce Block Inverse Prompting (BIPro)
constrained generation framework. BIPro leverages two block inverse prompting
methods, revise and rewrite, that mimic the process of human text writing using
block generative models. It significantly improves the zero-shot generation
quality on the formidable constrained generation task of open-domain
traditional-form Chinese poem generation. Based on a less powerful block
generative model GLM-10B-Chinese, poems composed via BIPro without priming or
additional training outperform both most advanced direct generative systems
like GPT-4 or GLM-4 and best domain-specific systems such as Yusheng,
Shisanbai, or Baidu Poetry Helper in human evaluation by proficient poets.
Finally, BIPro considerably narrows the gap between AI-generated works and
short-listed human literary arts in another human evaluation, unveiling the
promising potential of block generative models in improving the quality of
constrained generation.",2024-11-20,Xu Zou,http://arxiv.org/pdf/2411.13237v1,cs.CL
AIDBench: A benchmark for evaluating the authorship identification capability of large language models,"As large language models (LLMs) rapidly advance and integrate into daily
life, the privacy risks they pose are attracting increasing attention. We focus
on a specific privacy risk where LLMs may help identify the authorship of
anonymous texts, which challenges the effectiveness of anonymity in real-world
systems such as anonymous peer review systems. To investigate these risks, we
present AIDBench, a new benchmark that incorporates several author
identification datasets, including emails, blogs, reviews, articles, and
research papers. AIDBench utilizes two evaluation methods: one-to-one
authorship identification, which determines whether two texts are from the same
author; and one-to-many authorship identification, which, given a query text
and a list of candidate texts, identifies the candidate most likely written by
the same author as the query text. We also introduce a Retrieval-Augmented
Generation (RAG)-based method to enhance the large-scale authorship
identification capabilities of LLMs, particularly when input lengths exceed the
models' context windows, thereby establishing a new baseline for authorship
identification using LLMs. Our experiments with AIDBench demonstrate that LLMs
can correctly guess authorship at rates well above random chance, revealing new
privacy risks posed by these powerful models. The source code and data will be
made publicly available after acceptance.",2024-11-20,"Zichen Wen, Dadi Guo, Huishuai Zhang",http://arxiv.org/pdf/2411.13226v1,cs.CL
GhostRNN: Reducing State Redundancy in RNN with Cheap Operations,"Recurrent neural network (RNNs) that are capable of modeling long-distance
dependencies are widely used in various speech tasks, eg., keyword spotting
(KWS) and speech enhancement (SE). Due to the limitation of power and memory in
low-resource devices, efficient RNN models are urgently required for real-world
applications. In this paper, we propose an efficient RNN architecture,
GhostRNN, which reduces hidden state redundancy with cheap operations. In
particular, we observe that partial dimensions of hidden states are similar to
the others in trained RNN models, suggesting that redundancy exists in specific
RNNs. To reduce the redundancy and hence computational cost, we propose to
first generate a few intrinsic states, and then apply cheap operations to
produce ghost states based on the intrinsic states. Experiments on KWS and SE
tasks demonstrate that the proposed GhostRNN significantly reduces the memory
usage (~40%) and computation cost while keeping performance similar.",2024-11-20,"Hang Zhou, Xiaoxu Zheng, Yunhe Wang, Michael Bi Mi, Deyi Xiong, Kai Han",http://arxiv.org/pdf/2411.14489v1,cs.CL
Hard-Synth: Synthesizing Diverse Hard Samples for ASR using Zero-Shot TTS and LLM,"Text-to-speech (TTS) models have been widely adopted to enhance automatic
speech recognition (ASR) systems using text-only corpora, thereby reducing the
cost of labeling real speech data. Existing research primarily utilizes
additional text data and predefined speech styles supported by TTS models. In
this paper, we propose Hard-Synth, a novel ASR data augmentation method that
leverages large language models (LLMs) and advanced zero-shot TTS. Our approach
employs LLMs to generate diverse in-domain text through rewriting, without
relying on additional text data. Rather than using predefined speech styles, we
introduce a hard prompt selection method with zero-shot TTS to clone speech
styles that the ASR model finds challenging to recognize. Experiments
demonstrate that Hard-Synth significantly enhances the Conformer model,
achieving relative word error rate (WER) reductions of 6.5\%/4.4\% on
LibriSpeech dev/test-other subsets. Additionally, we show that Hard-Synth is
data-efficient and capable of reducing bias in ASR.",2024-11-20,"Jiawei Yu, Yuang Li, Xiaosong Qiao, Huan Zhao, Xiaofeng Zhao, Wei Tang, Min Zhang, Hao Yang, Jinsong Su",http://arxiv.org/pdf/2411.13159v1,cs.CL
Closer Look at Efficient Inference Methods: A Survey of Speculative Decoding,"Efficient inference in large language models (LLMs) has become a critical
focus as their scale and complexity grow. Traditional autoregressive decoding,
while effective, suffers from computational inefficiencies due to its
sequential token generation process. Speculative decoding addresses this
bottleneck by introducing a two-stage framework: drafting and verification. A
smaller, efficient model generates a preliminary draft, which is then refined
by a larger, more sophisticated model. This paper provides a comprehensive
survey of speculative decoding methods, categorizing them into draft-centric
and model-centric approaches. We discuss key ideas associated with each method,
highlighting their potential for scaling LLM inference. This survey aims to
guide future research in optimizing speculative decoding and its integration
into real-world LLM applications.",2024-11-20,"Hyun Ryu, Eric Kim",http://arxiv.org/pdf/2411.13157v2,cs.CL
Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training,"Recent advancements in vision-language pre-training via contrastive learning
have significantly improved performance across computer vision tasks. However,
in the medical domain, obtaining multimodal data is often costly and
challenging due to privacy, sensitivity, and annotation complexity. To mitigate
data scarcity while boosting model performance, we introduce \textbf{Uni-Mlip},
a unified self-supervision framework specifically designed to enhance medical
vision-language pre-training. Uni-Mlip seamlessly integrates cross-modality,
uni-modality, and fused-modality self-supervision techniques at the data-level
and the feature-level. Additionally, Uni-Mlip tailors uni-modal image
self-supervision to accommodate the unique characteristics of medical images.
Our experiments across datasets of varying scales demonstrate that Uni-Mlip
significantly surpasses current state-of-the-art methods in three key
downstream tasks: image-text retrieval, image classification, and visual
question answering (VQA).",2024-11-20,"Ameera Bawazir, Kebin Wu, Wenbin Li",http://arxiv.org/pdf/2411.15207v1,cs.CL
Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control,"Lyrics generation presents unique challenges, particularly in achieving
precise syllable control while adhering to song form structures such as verses
and choruses. Conventional line-by-line approaches often lead to unnatural
phrasing, underscoring the need for more granular syllable management. We
propose a framework for lyrics generation that enables multi-level syllable
control at the word, phrase, line, and paragraph levels, aware of song form.
Our approach generates complete lyrics conditioned on input text and song form,
ensuring alignment with specified syllable constraints. Generated lyrics
samples are available at: https://tinyurl.com/lyrics9999",2024-11-20,"Yunkee Chae, Eunsik Shin, Hwang Suntae, Seungryeol Paik, Kyogu Lee",http://arxiv.org/pdf/2411.13100v1,cs.CL
Patience Is The Key to Large Language Model Reasoning,"Recent advancements in the field of large language models, particularly
through the Chain of Thought (CoT) approach, have demonstrated significant
improvements in solving complex problems. However, existing models either tend
to sacrifice detailed reasoning for brevity due to user preferences, or require
extensive and expensive training data to learn complicated reasoning ability,
limiting their potential in solving complex tasks. To bridge this gap,
following the concept of scaling test-time, we propose a simple method by
encouraging models to adopt a more patient reasoning style without the need of
introducing new knowledge or skills. To employ a preference optimization
approach, we generate detailed reasoning processes as positive examples and
simple answers as negative examples, thereby training the model to favor
thoroughness in its responses. Our results demonstrate a performance increase
of up to 2.1% on GSM8k with training just on a lightweight dataset.",2024-11-20,Yijiong Yu,http://arxiv.org/pdf/2411.13082v3,cs.CL
Ensuring Safety and Trust: Analyzing the Risks of Large Language Models in Medicine,"The remarkable capabilities of Large Language Models (LLMs) make them
increasingly compelling for adoption in real-world healthcare applications.
However, the risks associated with using LLMs in medical applications have not
been systematically characterized. We propose using five key principles for
safe and trustworthy medical AI: Truthfulness, Resilience, Fairness,
Robustness, and Privacy, along with ten specific aspects. Under this
comprehensive framework, we introduce a novel MedGuard benchmark with 1,000
expert-verified questions. Our evaluation of 11 commonly used LLMs shows that
the current language models, regardless of their safety alignment mechanisms,
generally perform poorly on most of our benchmarks, particularly when compared
to the high performance of human physicians. Despite recent reports indicate
that advanced LLMs like ChatGPT can match or even exceed human performance in
various medical tasks, this study underscores a significant safety gap,
highlighting the crucial need for human oversight and the implementation of AI
safety guardrails.",2024-11-20,"Yifan Yang, Qiao Jin, Robert Leaman, Xiaoyu Liu, Guangzhi Xiong, Maame Sarfo-Gyamfi, Changlin Gong, Santiago Ferrière-Steinert, W. John Wilbur, Xiaojun Li, Jiaxin Yuan, Bang An, Kelvin S. Castro, Francisco Erramuspe Álvarez, Matías Stockle, Aidong Zhang, Furong Huang, Zhiyong Lu",http://arxiv.org/pdf/2411.14487v1,cs.CL
Explainable LLM-driven Multi-dimensional Distillation for E-Commerce Relevance Learning,"Effective query-item relevance modeling is pivotal for enhancing user
experience and safeguarding user satisfaction in e-commerce search systems.
Recently, benefiting from the vast inherent knowledge, Large Language Model
(LLM) approach demonstrates strong performance and long-tail generalization
ability compared with previous neural-based specialized relevance learning
methods. Though promising, current LLM-based methods encounter the following
inadequacies in practice: First, the massive parameters and computational
demands make it difficult to be deployed online. Second, distilling LLM models
to online models is a feasible direction, but the LLM relevance modeling is a
black box, and its rich intrinsic knowledge is difficult to extract and apply
online. To improve the interpretability of LLM and boost the performance of
online relevance models via LLM, we propose an Explainable LLM-driven
Multi-dimensional Distillation framework for e-commerce relevance learning,
which comprises two core components: (1) An Explainable LLM for relevance
modeling (ELLM-rele), which decomposes the relevance learning into intermediate
steps and models relevance learning as a Chain-of-Thought (CoT) reasoning,
thereby enhancing both interpretability and performance of LLM. (2) A
Multi-dimensional Knowledge Distillation (MKD) architecture that transfers the
knowledge of ELLM-rele to current deployable interaction-based and
representation-based student models from both the relevance score distribution
and CoT reasoning aspects. Through distilling the probabilistic and CoT
reasoning knowledge, MKD improves both the semantic interaction and long-tail
generalization abilities of student models. Extensive offline evaluations and
online experiments on Taobao search ad scene demonstrate that our proposed
framework significantly enhances e-commerce relevance learning performance and
user experience.",2024-11-20,"Gang Zhao, Ximing Zhang, Chenji Lu, Hui Zhao, Tianshu Wu, Pengjie Wang, Jian Xu, Bo Zheng",http://arxiv.org/pdf/2411.13045v2,cs.CL
Multimodal large language model for wheat breeding: a new exploration of smart breeding,"UAV remote sensing technology has become a key technology in crop breeding,
which can achieve high-throughput and non-destructive collection of crop
phenotyping data. However, the multidisciplinary nature of breeding has brought
technical barriers and efficiency challenges to knowledge mining. Therefore, it
is important to develop a smart breeding goal tool to mine cross-domain
multimodal data. Based on different pre-trained open-source multimodal large
language models (MLLMs) (e.g., Qwen-VL, InternVL, Deepseek-VL), this study used
supervised fine-tuning (SFT), retrieval-augmented generation (RAG), and
reinforcement learning from human feedback (RLHF) technologies to inject
cross-domain knowledge into MLLMs, thereby constructing multiple multimodal
large language models for wheat breeding (WBLMs). The above WBLMs were
evaluated using the newly created evaluation benchmark in this study. The
results showed that the WBLM constructed using SFT, RAG and RLHF technologies
and InternVL2-8B has leading performance. Then, subsequent experiments were
conducted using the WBLM. Ablation experiments indicated that the combination
of SFT, RAG, and RLHF technologies can improve the overall generation
performance, enhance the generated quality, balance the timeliness and
adaptability of the generated answer, and reduce hallucinations and biases. The
WBLM performed best in wheat yield prediction using cross-domain data (remote
sensing, phenotyping, weather, germplasm) simultaneously, with R2 and RMSE of
0.821 and 489.254 kg/ha, respectively. Furthermore, the WBLM can generate
professional decision support answers for phenotyping estimation, environmental
stress assessment, target germplasm screening, cultivation technique
recommendation, and seed price query tasks.",2024-11-20,"Guofeng Yang, Yu Li, Yong He, Zhenjiang Zhou, Lingzhen Ye, Hui Fang, Yiqi Luo, Xuping Feng",http://arxiv.org/pdf/2411.15203v1,cs.CL
The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz,"This research introduces a novel evaluation framework designed to assess
large language models' (LLMs) ability to acknowledge uncertainty on 675
fundamentally unsolvable problems. Using a curated dataset of graduate-level
grand challenge questions with intentionally unknowable answers, we evaluated
twelve state-of-the-art LLMs, including both open and closed-source models, on
their propensity to admit ignorance rather than generate plausible but
incorrect responses. The best models scored in 62-68% accuracy ranges for
admitting the problem solution was unknown in fields ranging from biology to
philosophy and mathematics. We observed an inverse relationship between problem
difficulty and model accuracy, with GPT-4 demonstrating higher rates of
uncertainty acknowledgment on more challenging problems (35.8%) compared to
simpler ones (20.0%). This pattern indicates that models may be more prone to
generate speculative answers when problems appear more tractable. The study
also revealed significant variations across problem categories, with models
showing difficulty in acknowledging uncertainty in invention and NP-hard
problems while performing relatively better on philosophical and psychological
challenges. These results contribute to the growing body of research on
artificial general intelligence (AGI) assessment by highlighting the importance
of uncertainty recognition as a critical component of future machine
intelligence evaluation. This impossibility test thus extends previous
theoretical frameworks for universal intelligence testing by providing
empirical evidence of current limitations in LLMs' ability to recognize their
own knowledge boundaries, suggesting new directions for improving model
training architectures and evaluation approaches.",2024-11-20,"David Noever, Forrest McKee",http://arxiv.org/pdf/2411.14486v1,cs.CL
Breaking the Cycle of Recurring Failures: Applying Generative AI to Root Cause Analysis in Legacy Banking Systems,"Traditional banks face significant challenges in digital transformation,
primarily due to legacy system constraints and fragmented ownership. Recent
incidents show that such fragmentation often results in superficial incident
resolutions, leaving root causes unaddressed and causing recurring failures. We
introduce a novel approach to post-incident analysis, integrating
knowledge-based GenAI agents with the ""Five Whys"" technique to examine problem
descriptions and change request data. This method uncovered that approximately
70% of the incidents previously attributed to management or vendor failures
were due to underlying internal code issues. We present a case study to show
the impact of our method. By scanning over 5,000 projects, we identified over
400 files with a similar root cause. Overall, we leverage the knowledge-based
agents to automate and elevate root cause analysis, transforming it into a more
proactive process. These agents can be applied across other phases of the
software development lifecycle, further improving development processes.",2024-11-20,"Siyuan Jin, Zhendong Bei, Bichao Chen, Yong Xia",http://arxiv.org/pdf/2411.13017v1,cs.CL
LLMSteer: Improving Long-Context LLM Inference by Steering Attention on Reused Contexts,"As large language models (LLMs) show impressive performance on complex tasks,
they still struggle with longer contextual understanding and high computational
costs. To balance efficiency and quality, we introduce LLMSteer, a
fine-tuning-free framework that enhances LLMs through query-independent
attention steering. Tested on popular LLMs and datasets, LLMSteer narrows the
performance gap with baselines by 65.9% and reduces the runtime delay by up to
4.8x compared to recent attention steering methods.",2024-11-20,"Zhuohan Gu, Jiayi Yao, Kuntai Du, Junchen Jiang",http://arxiv.org/pdf/2411.13009v2,cs.CL
Mediating Modes of Thought: LLM's for design scripting,"Architects adopt visual scripting and parametric design tools to explore more
expansive design spaces (Coates, 2010), refine their thinking about the
geometric logic of their design (Woodbury, 2010), and overcome conventional
software limitations (Burry, 2011). Despite two decades of effort to make
design scripting more accessible, a disconnect between a designer's free ways
of thinking and the rigidity of algorithms remains (Burry, 2011). Recent
developments in Large Language Models (LLMs) suggest this might soon change, as
LLMs encode a general understanding of human context and exhibit the capacity
to produce geometric logic. This project speculates that if LLMs can
effectively mediate between user intent and algorithms, they become a powerful
tool to make scripting in design more widespread and fun. We explore if such
systems can interpret natural language prompts to assemble geometric operations
relevant to computational design scripting. In the system, multiple layers of
LLM agents are configured with specific context to infer the user intent and
construct a sequential logic. Given a user's high-level text prompt, a
geometric description is created, distilled into a sequence of logic
operations, and mapped to software-specific commands. The completed script is
constructed in the user's visual programming interface. The system succeeds in
generating complete visual scripts up to a certain complexity but fails beyond
this complexity threshold. It shows how LLMs can make design scripting much
more aligned with human creativity and thought. Future research should explore
conversational interactions, expand to multimodal inputs and outputs, and
assess the performance of these tools.",2024-11-20,"Moritz Rietschel, Fang Guo, Kyle Steinfeld",http://arxiv.org/pdf/2411.14485v2,cs.CL
MemoryFormer: Minimize Transformer Computation by Removing Fully-Connected Layers,"In order to reduce the computational complexity of large language models,
great efforts have been made to to improve the efficiency of transformer models
such as linear attention and flash-attention. However, the model size and
corresponding computational complexity are constantly scaled up in pursuit of
higher performance. In this work, we present MemoryFormer, a novel transformer
architecture which significantly reduces the computational complexity (FLOPs)
from a new perspective. We eliminate nearly all the computations of the
transformer model except for the necessary computation required by the
multi-head attention operation. This is made possible by utilizing an
alternative method for feature transformation to replace the linear projection
of fully-connected layers. Specifically, we first construct a group of
in-memory lookup tables that store a large amount of discrete vectors to
replace the weight matrix used in linear projection. We then use a hash
algorithm to retrieve a correlated subset of vectors dynamically based on the
input embedding. The retrieved vectors combined together will form the output
embedding, which provides an estimation of the result of matrix multiplication
operation in a fully-connected layer. Compared to conducting matrix
multiplication, retrieving data blocks from memory is a much cheaper operation
which requires little computations. We train MemoryFormer from scratch and
conduct extensive experiments on various benchmarks to demonstrate the
effectiveness of the proposed model.",2024-11-20,"Ning Ding, Yehui Tang, Haochen Qin, Zhenli Zhou, Chao Xu, Lin Li, Kai Han, Heng Liao, Yunhe Wang",http://arxiv.org/pdf/2411.12992v1,cs.CL
Training Bilingual LMs with Data Constraints in the Targeted Language,"Large language models are trained on massive scrapes of the web, as required
by current scaling laws. Most progress is made for English, given its abundance
of high-quality pretraining data. For most other languages, however, such high
quality pretraining data is unavailable. In this work, we study how to boost
pretrained model performance in a target language with insufficient pretraining
data for training a high performing language model, by enlisting data from an
auxiliary language for which high quality data is available. We study this by
quantifying the performance gap between training with data in a data-rich
auxiliary language compared with training in the target language, exploring the
benefits of translation systems, studying the limitations of model scaling when
data is limited in the target languages, and proposing new methods for
upsampling data from the auxiliary language. Our results show that stronger
auxiliary datasets result in performance gains without modification to the
model or training objective for close languages, and, in particular, that
performance gains due to the development of more information-rich English
pretraining datasets can extend to targeted language settings with limited
data.",2024-11-20,"Skyler Seto, Maartje ter Hoeve, Richard He Bai, Natalie Schluter, David Grangier",http://arxiv.org/pdf/2411.12986v2,cs.CL
MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Collaborative Learning,"Contemporary embodied agents powered by large language models (LLMs), such as
Voyager, have shown promising capabilities in individual learning within
open-ended environments like Minecraft. However, when powered by open LLMs,
they struggle with basic tasks even after domain-specific fine-tuning. We
present MindForge, a generative-agent framework for collaborative lifelong
learning through explicit perspective taking. We introduce three key
innovations: (1) a structured theory of mind representation linking percepts,
beliefs, desires, and actions; (2) natural interagent communication; and (3) a
multicomponent memory system. In Minecraft experiments, MindForge agents
powered by open-weight LLMs significantly outperform their Voyager counterparts
in basic tasks where traditional Voyager fails without GPT-4, collecting
$2.3\times$ more unique items and achieving $3\times$ more tech-tree
milestones, advancing from basic wood tools to advanced iron equipment.
MindForge agents demonstrate sophisticated behaviors, including expert-novice
knowledge transfer, collaborative problem solving, and adaptation to
out-of-distribution tasks through accumulated collaborative experiences.
MindForge advances the democratization of embodied AI development through
open-ended social learning, enabling peer-to-peer knowledge sharing.",2024-11-20,"Mircea Lică, Ojas Shirekar, Baptiste Colle, Chirag Raman",http://arxiv.org/pdf/2411.12977v3,cs.CL
Robust Planning with Compound LLM Architectures: An LLM-Modulo Approach,"Previous work has attempted to boost Large Language Model (LLM) performance
on planning and scheduling tasks through a variety of prompt engineering
techniques. While these methods can work within the distributions tested, they
are neither robust nor predictable. This limitation can be addressed through
compound LLM architectures where LLMs work in conjunction with other components
to ensure reliability. In this paper, we present a technical evaluation of a
compound LLM architecture--the LLM-Modulo framework. In this framework, an LLM
is paired with a complete set of sound verifiers that validate its output,
re-prompting it if it fails. This approach ensures that the system can never
output any fallacious output, and therefore that every output generated is
guaranteed correct--something previous techniques have not been able to claim.
Our results, evaluated across four scheduling domains, demonstrate significant
performance gains with the LLM-Modulo framework using various models.
Additionally, we explore modifications to the base configuration of the
framework and assess their impact on overall system performance.",2024-11-20,"Atharva Gundawar, Karthik Valmeekam, Mudit Verma, Subbarao Kambhampati",http://arxiv.org/pdf/2411.14484v1,cs.CL
A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection,"Large Language Models (LLMs) are prone to off-topic misuse, where users may
prompt these models to perform tasks beyond their intended scope. Current
guardrails, which often rely on curated examples or custom classifiers, suffer
from high false-positive rates, limited adaptability, and the impracticality of
requiring real-world data that is not available in pre-production. In this
paper, we introduce a flexible, data-free guardrail development methodology
that addresses these challenges. By thoroughly defining the problem space
qualitatively and passing this to an LLM to generate diverse prompts, we
construct a synthetic dataset to benchmark and train off-topic guardrails that
outperform heuristic approaches. Additionally, by framing the task as
classifying whether the user prompt is relevant with respect to the system
prompt, our guardrails effectively generalize to other misuse categories,
including jailbreak and harmful prompts. Lastly, we further contribute to the
field by open-sourcing both the synthetic dataset and the off-topic guardrail
models, providing valuable resources for developing guardrails in
pre-production environments and supporting future research and development in
LLM safety.",2024-11-20,"Gabriel Chua, Shing Yee Chan, Shaun Khoo",http://arxiv.org/pdf/2411.12946v2,cs.CL
Loss-to-Loss Prediction: Scaling Laws for All Datasets,"While scaling laws provide a reliable methodology for predicting train loss
across compute scales for a single data distribution, less is known about how
these predictions should change as we change the distribution. In this paper,
we derive a strategy for predicting one loss from another and apply it to
predict across different pre-training datasets and from pre-training data to
downstream task data. Our predictions extrapolate well even at 20x the largest
FLOP budget used to fit the curves. More precisely, we find that there are
simple shifted power law relationships between (1) the train losses of two
models trained on two separate datasets when the models are paired by training
compute (train-to-train), (2) the train loss and the test loss on any
downstream distribution for a single model (train-to-test), and (3) the test
losses of two models trained on two separate train datasets (test-to-test). The
results hold up for pre-training datasets that differ substantially (some are
entirely code and others have no code at all) and across a variety of
downstream tasks. Finally, we find that in some settings these shifted power
law relationships can yield more accurate predictions than extrapolating
single-dataset scaling laws.",2024-11-19,"David Brandfonbrener, Nikhil Anand, Nikhil Vyas, Eran Malach, Sham Kakade",http://arxiv.org/pdf/2411.12925v1,cs.CL
Signformer is all you need: Towards Edge AI for Sign Language,"Sign language translation, especially in gloss-free paradigm, is confronting
a dilemma of impracticality and unsustainability due to growing
resource-intensive methodologies. Contemporary state-of-the-arts (SOTAs) have
significantly hinged on pretrained sophiscated backbones such as Large Language
Models (LLMs), embedding sources, or extensive datasets, inducing considerable
parametric and computational inefficiency for sustainable use in real-world
scenario. Despite their success, following this research direction undermines
the overarching mission of this domain to create substantial value to bridge
hard-hearing and common populations. Committing to the prevailing trend of LLM
and Natural Language Processing (NLP) studies, we pursue a profound essential
change in architecture to achieve ground-up improvements without external aid
from pretrained models, prior knowledge transfer, or any NLP strategies
considered not-from-scratch.
  Introducing Signformer, a from-scratch Feather-Giant transforming the area
towards Edge AI that redefines extremities of performance and efficiency with
LLM-competence and edgy-deployable compactness. In this paper, we present
nature analysis of sign languages to inform our algorithmic design and deliver
a scalable transformer pipeline with convolution and attention novelty. We
achieve new 2nd place on leaderboard with a parametric reduction of 467-1807x
against the finests as of 2024 and outcompete almost every other methods in a
lighter configuration of 0.57 million parameters.",2024-11-19,Eta Yang,http://arxiv.org/pdf/2411.12901v1,cs.CL
Selective Attention: Enhancing Transformer through Principled Context Control,"The attention mechanism within the transformer architecture enables the model
to weigh and combine tokens based on their relevance to the query. While
self-attention has enjoyed major success, it notably treats all queries $q$ in
the same way by applying the mapping $V^\top\text{softmax}(Kq)$, where $V,K$
are the value and key embeddings respectively. In this work, we argue that this
uniform treatment hinders the ability to control contextual sparsity and
relevance. As a solution, we introduce the $\textit{Selective Self-Attention}$
(SSA) layer that augments the softmax nonlinearity with a principled
temperature scaling strategy. By controlling temperature, SSA adapts the
contextual sparsity of the attention map to the query embedding and its
position in the context window. Through theory and experiments, we demonstrate
that this alleviates attention dilution, aids the optimization process, and
enhances the model's ability to control softmax spikiness of individual
queries. We also incorporate temperature scaling for value embeddings and show
that it boosts the model's ability to suppress irrelevant/noisy tokens.
Notably, SSA is a lightweight method which introduces less than 0.5% new
parameters through a weight-sharing strategy and can be fine-tuned on existing
LLMs. Extensive empirical evaluations demonstrate that SSA-equipped models
achieve a noticeable and consistent accuracy improvement on language modeling
benchmarks.",2024-11-19,"Xuechen Zhang, Xiangyu Chang, Mingchen Li, Amit Roy-Chowdhury, Jiasi Chen, Samet Oymak",http://arxiv.org/pdf/2411.12892v1,cs.CL
ProSec: Fortifying Code LLMs with Proactive Security Alignment,"Recent advances in code-specific large language models (LLMs) have greatly
enhanced code generation and refinement capabilities. However, the safety of
code LLMs remains under-explored, posing potential risks as insecure code
generated by these models may introduce vulnerabilities into real-world
systems. Previous work proposes to collect security-focused instruction-tuning
dataset from real-world vulnerabilities. It is constrained by the data sparsity
of vulnerable code, and has limited applicability in the iterative
post-training workflows of modern LLMs. In this paper, we propose ProSec, a
novel proactive security alignment approach designed to align code LLMs with
secure coding practices. ProSec systematically exposes the vulnerabilities in a
code LLM by synthesizing error-inducing coding scenarios from Common Weakness
Enumerations (CWEs), and generates fixes to vulnerable code snippets, allowing
the model to learn secure practices through advanced preference learning
objectives. The scenarios synthesized by ProSec triggers 25 times more
vulnerable code than a normal instruction-tuning dataset, resulting in a
security-focused alignment dataset 7 times larger than the previous work.
Experiments show that models trained with ProSec are 25.2% to 91.4% more secure
compared to previous work without degrading models' utility.",2024-11-19,"Xiangzhe Xu, Zian Su, Jinyao Guo, Kaiyuan Zhang, Zhenting Wang, Xiangyu Zhang",http://arxiv.org/pdf/2411.12882v2,cs.CL
"AzSLD: Azerbaijani Sign Language Dataset for Fingerspelling, Word, and Sentence Translation with Baseline Software","Sign language processing technology development relies on extensive and
reliable datasets, instructions, and ethical guidelines. We present a
comprehensive Azerbaijani Sign Language Dataset (AzSLD) collected from diverse
sign language users and linguistic parameters to facilitate advancements in
sign recognition and translation systems and support the local sign language
community. The dataset was created within the framework of a vision-based AzSL
translation project. This study introduces the dataset as a summary of the
fingerspelling alphabet and sentence- and word-level sign language datasets.
The dataset was collected from signers of different ages, genders, and signing
styles, with videos recorded from two camera angles to capture each sign in
full detail. This approach ensures robust training and evaluation of gesture
recognition models. AzSLD contains 30,000 videos, each carefully annotated with
accurate sign labels and corresponding linguistic translations. The dataset is
accompanied by technical documentation and source code to facilitate its use in
training and testing. This dataset offers a valuable resource of labeled data
for researchers and developers working on sign language recognition,
translation, or synthesis. Ethical guidelines were strictly followed throughout
the project, with all participants providing informed consent for collecting,
publishing, and using the data.",2024-11-19,"Nigar Alishzade, Jamaladdin Hasanov",http://arxiv.org/pdf/2411.12865v2,cs.CL
SCOUT: A Situated and Multi-Modal Human-Robot Dialogue Corpus,"We introduce the Situated Corpus Of Understanding Transactions (SCOUT), a
multi-modal collection of human-robot dialogue in the task domain of
collaborative exploration. The corpus was constructed from multiple
Wizard-of-Oz experiments where human participants gave verbal instructions to a
remotely-located robot to move and gather information about its surroundings.
SCOUT contains 89,056 utterances and 310,095 words from 278 dialogues averaging
320 utterances per dialogue. The dialogues are aligned with the multi-modal
data streams available during the experiments: 5,785 images and 30 maps. The
corpus has been annotated with Abstract Meaning Representation and Dialogue-AMR
to identify the speaker's intent and meaning within an utterance, and with
Transactional Units and Relations to track relationships between utterances to
reveal patterns of the Dialogue Structure. We describe how the corpus and its
annotations have been used to develop autonomous human-robot systems and enable
research in open questions of how humans speak to robots. We release this
corpus to accelerate progress in autonomous, situated, human-robot dialogue,
especially in the context of navigation tasks where details about the
environment need to be discovered.",2024-11-19,"Stephanie M. Lukin, Claire Bonial, Matthew Marge, Taylor Hudson, Cory J. Hayes, Kimberly A. Pollard, Anthony Baker, Ashley N. Foots, Ron Artstein, Felix Gervits, Mitchell Abrams, Cassidy Henry, Lucia Donatelli, Anton Leuski, Susan G. Hill, David Traum, Clare R. Voss",http://arxiv.org/pdf/2411.12844v1,cs.CL
Reward Modeling with Ordinal Feedback: Wisdom of the Crowd,"Learning a reward model (RM) from human preferences has been an important
component in aligning large language models (LLMs). The canonical setup of
learning RMs from pairwise preference data is rooted in the classic
Bradley-Terry (BT) model that accepts binary feedback, i.e., the label being
either Response 1 is better than Response 2, or the opposite. Such a setup
inevitably discards potentially useful samples (such as ""tied"" between the two
responses) and loses more fine-grained information (such as ""slightly better"").
In this paper, we propose a framework for learning RMs under ordinal feedback
which generalizes the case of binary preference feedback to any arbitrary
granularity. Specifically, we first identify a marginal unbiasedness condition,
which generalizes the assumption of the BT model in the existing binary
feedback setting. The condition validates itself via the sociological concept
of the wisdom of the crowd. Under the condition, we develop a natural
probability model for pairwise preference data under ordinal feedback and
analyze its properties. We prove the statistical benefits of ordinal feedback
in terms of reducing the Rademacher complexity compared to the case of binary
feedback. The proposed learning objective and the theory also extend to hinge
loss and direct policy optimization (DPO). In particular, the theoretical
analysis may be of independent interest when applying to a seemingly unrelated
problem of knowledge distillation to interpret the bias-variance trade-off
therein. The framework also sheds light on writing guidance for human
annotators. Our numerical experiments validate that fine-grained feedback leads
to better reward learning for both in-distribution and out-of-distribution
settings. Further experiments show that incorporating a certain proportion of
samples with tied preference boosts RM learning.",2024-11-19,"Shang Liu, Yu Pan, Guanting Chen, Xiaocheng Li",http://arxiv.org/pdf/2411.12843v1,cs.CL
Ranking Unraveled: Recipes for LLM Rankings in Head-to-Head AI Combat,"Deciding which large language model (LLM) to use is a complex challenge.
Pairwise ranking has emerged as a new method for evaluating human preferences
for LLMs. This approach entails humans evaluating pairs of model outputs based
on a predefined criterion. By collecting these comparisons, a ranking can be
constructed using methods such as Elo. However, applying these algorithms as
constructed in the context of LLM evaluation introduces several challenges. In
this paper, we explore the effectiveness of ranking systems for head-to-head
comparisons of LLMs. We formally define a set of fundamental principles for
effective ranking and conduct a series of extensive evaluations on the
robustness of several ranking algorithms in the context of LLMs. Our analysis
uncovers key insights into the factors that affect ranking accuracy and
efficiency, offering guidelines for selecting the most appropriate methods
based on specific evaluation contexts and resource constraints.",2024-11-19,"Roland Daynauth, Christopher Clarke, Krisztian Flautner, Lingjia Tang, Jason Mars",http://arxiv.org/pdf/2411.14483v2,cs.CL
Human-Robot Dialogue Annotation for Multi-Modal Common Ground,"In this paper, we describe the development of symbolic representations
annotated on human-robot dialogue data to make dimensions of meaning accessible
to autonomous systems participating in collaborative, natural language
dialogue, and to enable common ground with human partners. A particular
challenge for establishing common ground arises in remote dialogue (occurring
in disaster relief or search-and-rescue tasks), where a human and robot are
engaged in a joint navigation and exploration task of an unfamiliar
environment, but where the robot cannot immediately share high quality visual
information due to limited communication constraints. Engaging in a dialogue
provides an effective way to communicate, while on-demand or lower-quality
visual information can be supplemented for establishing common ground. Within
this paradigm, we capture propositional semantics and the illocutionary force
of a single utterance within the dialogue through our Dialogue-AMR annotation,
an augmentation of Abstract Meaning Representation. We then capture patterns in
how different utterances within and across speaker floors relate to one another
in our development of a multi-floor Dialogue Structure annotation schema.
Finally, we begin to annotate and analyze the ways in which the visual
modalities provide contextual information to the dialogue for overcoming
disparities in the collaborators' understanding of the environment. We conclude
by discussing the use-cases, architectures, and systems we have implemented
from our annotations that enable physical robots to autonomously engage with
humans in bi-directional dialogue and navigation.",2024-11-19,"Claire Bonial, Stephanie M. Lukin, Mitchell Abrams, Anthony Baker, Lucia Donatelli, Ashley Foots, Cory J. Hayes, Cassidy Henry, Taylor Hudson, Matthew Marge, Kimberly A. Pollard, Ron Artstein, David Traum, Clare R. Voss",http://arxiv.org/pdf/2411.12829v1,cs.CL
Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction,"Large language model (LLM) agents show promise in an increasing number of
domains. In many proposed applications, it is expected that the agent reasons
over accumulated experience presented in an input prompt. We propose the OEDD
(Operationalize Experience Despite Distraction) corpus, a
human-annotator-validated body of scenarios with pre-scripted agent histories
where the agent must make a decision based on disparate experiential
information in the presence of a distractor. We evaluate three state-of-the-art
LLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) using a minimal
chain-of-thought prompting strategy and observe that when (1) the input context
contains over 1,615 tokens of historical interactions, (2) a crucially
decision-informing premise is the rightful conclusion over two disparate
environment premises, and (3) a trivial, but distracting red herring fact
follows, all LLMs perform worse than random choice at selecting the better of
two actions. Our code and test corpus are publicly available at:
https://github.com/sonnygeorge/OEDD .",2024-11-19,"Sonny George, Chris Sypherd, Dylan Cashman",http://arxiv.org/pdf/2411.12828v1,cs.CL
ACING: Actor-Critic for Instruction Learning in Black-Box Large Language Models,"The effectiveness of Large Language Models (LLMs) in solving tasks vastly
depends on the quality of the instructions, which often require fine-tuning
through extensive human effort. This highlights the need for automated
instruction optimization; however, this optimization is particularly
challenging when dealing with black-box LLMs, where model parameters and
gradients remain inaccessible. We propose ACING, a task-specific prompt
optimization approach framed as a stateless continuous-action Reinforcement
Learning (RL) problem, known as the continuum bandit setting. ACING leverages
an actor-critic-based method to optimize prompts, learning from
non-differentiable reward signals. We validate ACING by optimizing prompts for
ChatGPT on 30 instruction-based tasks. ACING consistently outperforms baseline
methods, achieving a median score improvement of 10 percentage points.
Furthermore, ACING not only recovers but also surpasses human-crafted expert
instructions, achieving up to a 39 percentage point improvement against human
benchmarks.",2024-11-19,"Salma Kharrat, Fares Fourati, Marco Canini",http://arxiv.org/pdf/2411.12736v1,cs.CL
Information Theory of Meaningful Communication,"In Shannon's seminal paper, entropy of printed English, treated as a
stationary stochastic process, was estimated to be roughly 1 bit per character.
However, considered as a means of communication, language differs considerably
from its printed form: (i) the units of information are not characters or even
words but clauses, i.e. shortest meaningful parts of speech; and (ii) what is
transmitted is principally the meaning of what is being said or written, while
the precise phrasing that was used to communicate the meaning is typically
ignored. In this study, we show that one can leverage recently developed large
language models to quantify information communicated in meaningful narratives
in terms of bits of meaning per clause.",2024-11-19,"Doron Sivan, Misha Tsodyks",http://arxiv.org/pdf/2411.12728v1,cs.CL
Scaling laws for nonlinear dynamical models of articulatory control,"Dynamical theories of speech use computational models of articulatory control
to generate quantitative predictions and advance understanding of speech
dynamics. The addition of a nonlinear restoring force to task dynamic models is
a significant improvement over linear models, but nonlinearity introduces
challenges with parameterization and interpretability. We illustrate these
problems through numerical simulations and introduce solutions in the form of
scaling laws. We apply the scaling laws to a cubic model and show how they
facilitate interpretable simulations of articulatory dynamics, and can be
theoretically interpreted as imposing physical and cognitive constraints on
models of speech movement dynamics.",2024-11-19,Sam Kirkham,http://arxiv.org/pdf/2411.12720v2,cs.CL
Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation,"Despite rapid advancements in TTS models, a consistent and robust human
evaluation framework is still lacking. For example, MOS tests fail to
differentiate between similar models, and CMOS's pairwise comparisons are
time-intensive. The MUSHRA test is a promising alternative for evaluating
multiple TTS systems simultaneously, but in this work we show that its reliance
on matching human reference speech unduly penalises the scores of modern TTS
systems that can exceed human speech quality. More specifically, we conduct a
comprehensive assessment of the MUSHRA test, focusing on its sensitivity to
factors such as rater variability, listener fatigue, and reference bias. Based
on our extensive evaluation involving 492 human listeners across Hindi and
Tamil we identify two primary shortcomings: (i) reference-matching bias, where
raters are unduly influenced by the human reference, and (ii) judgement
ambiguity, arising from a lack of clear fine-grained guidelines. To address
these issues, we propose two refined variants of the MUSHRA test. The first
variant enables fairer ratings for synthesized samples that surpass human
reference quality. The second variant reduces ambiguity, as indicated by the
relatively lower variance across raters. By combining these approaches, we
achieve both more reliable and more fine-grained assessments. We also release
MANGO, a massive dataset of 246,000 human ratings, the first-of-its-kind
collection for Indian languages, aiding in analyzing human preferences and
developing automatic metrics for evaluating TTS systems.",2024-11-19,"Praveen Srinivasa Varadhan, Amogh Gulati, Ashwin Sankar, Srija Anand, Anirudh Gupta, Anirudh Mukherjee, Shiva Kumar Marepally, Ankur Bhatia, Saloni Jaju, Suvrat Bhooshan, Mitesh M. Khapra",http://arxiv.org/pdf/2411.12719v2,cs.CL
"Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular, Nervous System, and Digestive Disorders Using Advanced LLMs","In this research, we explored the improvement in terms of multi-class disease
classification via pre-trained language models over Medical-Abstracts-TC-Corpus
that spans five medical conditions. We excluded non-cancer conditions and
examined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and
BERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained
on medical data, demonstrated superior performance in medical text
classification (97% accuracy). Surprisingly, XLNet followed closely (96%
accuracy), demonstrating its generalizability across domains even though it was
not pre-trained on medical data. LastBERT, a custom model based on the lighter
version of BERT, also proved competitive with 87.10% accuracy (just under
BERT's 89.33%). Our findings confirm the importance of specialized models such
as BioBERT and also support impressions around more general solutions like
XLNet and well-tuned transformer architectures with fewer parameters (in this
case, LastBERT) in medical domain tasks.",2024-11-19,"Ahmed Akib Jawad Karim, Muhammad Zawad Mahmud, Samiha Islam, Aznur Azam",http://arxiv.org/pdf/2411.12712v1,cs.CL
Strengthening Fake News Detection: Leveraging SVM and Sophisticated Text Vectorization Techniques. Defying BERT?,"The rapid spread of misinformation, particularly through online platforms,
underscores the urgent need for reliable detection systems. This study explores
the utilization of machine learning and natural language processing,
specifically Support Vector Machines (SVM) and BERT, to detect news that are
fake. We employ three distinct text vectorization methods for SVM: Term
Frequency Inverse Document Frequency (TF-IDF), Word2Vec, and Bag of Words (BoW)
evaluating their effectiveness in distinguishing between genuine and fake news.
Additionally, we compare these methods against the transformer large language
model, BERT. Our comprehensive approach includes detailed preprocessing steps,
rigorous model implementation, and thorough evaluation to determine the most
effective techniques. The results demonstrate that while BERT achieves superior
accuracy with 99.98% and an F1-score of 0.9998, the SVM model with a linear
kernel and BoW vectorization also performs exceptionally well, achieving 99.81%
accuracy and an F1-score of 0.9980. These findings highlight that, despite
BERT's superior performance, SVM models with BoW and TF-IDF vectorization
methods come remarkably close, offering highly competitive performance with the
advantage of lower computational requirements.",2024-11-19,"Ahmed Akib Jawad Karim, Kazi Hafiz Md Asad, Aznur Azam",http://arxiv.org/pdf/2411.12703v1,cs.CL
Enhanced Sign Language Translation between American Sign Language (ASL) and Indian Sign Language (ISL) Using LLMs,"We have come up with a research that hopes to provide a bridge between the
users of American Sign Language and the users of spoken language and Indian
Sign Language (ISL). The research enabled us to create a novel framework that
we have developed for Learner Systems. Leveraging art of Large models to create
key features including: - Real-time translation between these two sign
languages in an efficient manner. Making LLM's capability available for
seamless translations to ISL. Here is the full study showing its implementation
in this paper. The core of the system is a sophisticated pipeline that begins
with reclassification and recognition of ASL gestures based on a strong Random
Forest Classifier. By recognizing the ASL, it is translated into text which can
be more easily processed. Highly evolved natural language NLP (Natural Language
Processing) techniques come in handy as they play a role in our LLM integration
where you then use LLMs to be able to convert the ASL text to ISL which
provides you with the intent of sentence or phrase. The final step is to
synthesize the translated text back into ISL gestures, creating an end-to-end
translation experience using RIFE-Net. This framework is tasked with key
challenges such as automatically dealing with gesture variability and
overcoming the linguistic differences between ASL and ISL. By automating the
translation process, we hope to vastly improve accessibility for sign language
users. No longer will the communication gap between ASL and ISL create
barriers; this totally cool innovation aims to bring our communities closer
together. And we believe, with full confidence in our framework, that we're
able to apply the same principles across a wide variety of sign language
dialects.",2024-11-19,"Malay Kumar, S. Sarvajit Visagan, Tanish Sarang Mahajan, Anisha Natarajan",http://arxiv.org/pdf/2411.12685v1,cs.CL
Neurosymbolic Graph Enrichment for Grounded World Models,"The development of artificial intelligence systems capable of understanding
and reasoning about complex real-world scenarios is a significant challenge. In
this work we present a novel approach to enhance and exploit LLM reactive
capability to address complex problems and interpret deeply contextual
real-world meaning. We introduce a method and a tool for creating a multimodal,
knowledge-augmented formal representation of meaning that combines the
strengths of large language models with structured semantic representations.
Our method begins with an image input, utilizing state-of-the-art large
language models to generate a natural language description. This description is
then transformed into an Abstract Meaning Representation (AMR) graph, which is
formalized and enriched with logical design patterns, and layered semantics
derived from linguistic and factual knowledge bases. The resulting graph is
then fed back into the LLM to be extended with implicit knowledge activated by
complex heuristic learning, including semantic implicatures, moral values,
embodied cognition, and metaphorical representations. By bridging the gap
between unstructured language models and formal semantic structures, our method
opens new avenues for tackling intricate problems in natural language
understanding and reasoning.",2024-11-19,"Stefano De Giorgis, Aldo Gangemi, Alessandro Russo",http://arxiv.org/pdf/2411.12671v1,cs.CL
Optimizing Airline Reservation Systems with Edge-Enabled Microservices: A Framework for Real-Time Data Processing and Enhanced User Responsiveness,"The growing complexity of the operations of airline reservations requires a
smart solution for the adoption of novel approaches to the development of
quick, efficient, and adaptive reservation systems. This paper outlines in
detail a conceptual framework for the implementation of edge computing
microservices in order to address the shortcomings of traditional centralized
architectures. Specifically, as edge computing allows for certain activities
such as seat inventory checks, booking processes and even confirmation to be
done nearer to the user, thus lessening the overall response time and improving
the performance of the system. In addition, the framework value should include
achieving the high performance of the system such as low latency, high
throughput and higher user experience. The major design components include
deployed distributed computing microservices orchestrated by Kubernetes,
real-time message processing system with Kafka and its elastic scaling. Other
operational components include Prometheus and Grafana, which are used to
monitor and manage resources, ensuring that all operational processes are
optimized. Although this research focuses on a design and theoretical scheming
of the framework, its use is foreseen to be more advantageous in facilitating a
transform in the provision of services in the airline industry by improving
customers' satisfaction, providing infrastructure which is cheap to install and
efficiently supporting technology changes such as artificial intelligence and
internet of things embedded systems. This research addresses the increasing
demand for new technologies with modern well-distributed and real-time-centric
systems and also provides a basis for future case implementation and testing.
As such, the proposed architecture offers a market-ready, extensible solution
to the problems posed by existing airline reservation systems .",2024-11-19,"Biman Barua, M. Shamim Kaiser",http://arxiv.org/pdf/2411.12650v1,cs.CL
DLBacktrace: A Model Agnostic Explainability for any Deep Learning Models,"The rapid growth of AI has led to more complex deep learning models, often
operating as opaque ""black boxes"" with limited transparency in their
decision-making. This lack of interpretability poses challenges, especially in
high-stakes applications where understanding model output is crucial. This work
highlights the importance of interpretability in fostering trust,
accountability, and responsible deployment. To address these challenges, we
introduce DLBacktrace, a novel, model-agnostic technique designed to provide
clear insights into deep learning model decisions across a wide range of
domains and architectures, including MLPs, CNNs, and Transformer-based LLM
models. We present a comprehensive overview of DLBacktrace and benchmark its
performance against established interpretability methods such as SHAP, LIME,
and GradCAM. Our results demonstrate that DLBacktrace effectively enhances
understanding of model behavior across diverse tasks. DLBacktrace is compatible
with models developed in both PyTorch and TensorFlow, supporting architectures
such as BERT, ResNet, U-Net, and custom DNNs for tabular data. The library is
open-sourced and available at https://github.com/AryaXAI/DLBacktrace .",2024-11-19,"Vinay Kumar Sankarapu, Chintan Chitroda, Yashwardhan Rathore, Neeraj Kumar Singh, Pratinav Seth",http://arxiv.org/pdf/2411.12643v2,cs.CL
Leveraging Virtual Reality and AI Tutoring for Language Learning: A Case Study of a Virtual Campus Environment with OpenAI GPT Integration with Unity 3D,"This paper presents a new approach to multiple language learning, with Hindi
the language to be learnt in our case, by using the integration of virtual
reality environments and AI enabled tutoring systems using OpenAIs GPT api
calls. We have developed a scenario which has a virtual campus environment
using Unity which focuses on a detailed representation of our universitys
buildings 11th floor, where most of the cultural and technological activities
take place. Within this virtual environment that we have created, we have an AI
tutor powered by OpenAI's GPT model which was called using an api which moves
around with the user. This provided language learning support in Hindi, as GPT
is able to take care of language translation. Our approach mainly involves
utilising speech to text, text to text conversion and text to speech
capabilities to facilitate real time interaction between users and the AI tutor
in the presence of internet. This research demonstrates the use of combining VR
technology with AI tutoring for immersive language learning experiences and
provides interaction.",2024-11-19,"Adithya TG, Abhinavaram N, Gowri Srinivasa",http://arxiv.org/pdf/2411.12619v1,cs.CL
Graph Neural Network-Based Entity Extraction and Relationship Reasoning in Complex Knowledge Graphs,"This study proposed a knowledge graph entity extraction and relationship
reasoning algorithm based on a graph neural network, using a graph
convolutional network and graph attention network to model the complex
structure in the knowledge graph. By building an end-to-end joint model, this
paper achieves efficient recognition and reasoning of entities and
relationships. In the experiment, this paper compared the model with a variety
of deep learning algorithms and verified its superiority through indicators
such as AUC, recall rate, precision rate, and F1 value. The experimental
results show that the model proposed in this paper performs well in all
indicators, especially in complex knowledge graphs, it has stronger
generalization ability and stability. This provides strong support for further
research on knowledge graphs and also demonstrates the application potential of
graph neural networks in entity extraction and relationship reasoning.",2024-11-19,"Junliang Du, Guiran Liu, Jia Gao, Xiaoxuan Liao, Jiacheng Hu, Linxiao Wu",http://arxiv.org/pdf/2411.15195v1,cs.CL
Whisper Finetuning on Nepali Language,"Despite the growing advancements in Automatic Speech Recognition (ASR)
models, the development of robust models for underrepresented languages, such
as Nepali, remains a challenge. This research focuses on making an exhaustive
and generalized dataset followed by fine-tuning OpenAI's Whisper models of
different sizes to improve transcription (speech-to-text) accuracy for the
Nepali language. We leverage publicly available ASR datasets and self-recorded
custom datasets with a diverse range of accents, dialects, and speaking styles
further enriched through augmentation. Our experimental results demonstrate
that fine-tuning Whisper models on our curated custom dataset substantially
reduces the Word Error Rate (WER) across all model sizes attributed to larger
data variations in terms of speaker's age, gender, and sentiment, acoustic
environment, dialect, denser audio segments (15-30 seconds) that are more
compatible with Whisper's input, and manual curation of audios and
transcriptions. Notably, our approach outperforms Whisper's baseline models
trained on Fleur's dataset, achieving WER reductions of up to 36.2% on the
small and 23.8% on medium models. Furthermore, we show that data augmentation
plays a significant role in enhancing model robustness. Our approach underlines
the importance of dataset quality, variation, and augmentation in the
adaptation of state-of-the-art models to underrepresented languages for
developing accurate ASR systems.",2024-11-19,"Sanjay Rijal, Shital Adhikari, Manish Dahal, Manish Awale, Vaghawan Ojha",http://arxiv.org/pdf/2411.12587v1,cs.CL
Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models,"The capabilities and limitations of Large Language Models have been sketched
out in great detail in recent years, providing an intriguing yet conflicting
picture. On the one hand, LLMs demonstrate a general ability to solve problems.
On the other hand, they show surprising reasoning gaps when compared to humans,
casting doubt on the robustness of their generalisation strategies. The sheer
volume of data used in the design of LLMs has precluded us from applying the
method traditionally used to measure generalisation: train-test set separation.
To overcome this, we study what kind of generalisation strategies LLMs employ
when performing reasoning tasks by investigating the pretraining data they rely
on. For two models of different sizes (7B and 35B) and 2.5B of their
pretraining tokens, we identify what documents influence the model outputs for
three simple mathematical reasoning tasks and contrast this to the data that
are influential for answering factual questions. We find that, while the models
rely on mostly distinct sets of data for each factual question, a document
often has a similar influence across different reasoning questions within the
same task, indicating the presence of procedural knowledge. We further find
that the answers to factual questions often show up in the most influential
data. However, for reasoning questions the answers usually do not show up as
highly influential, nor do the answers to the intermediate reasoning steps.
When we characterise the top ranked documents for the reasoning questions
qualitatively, we confirm that the influential documents often contain
procedural knowledge, like demonstrating how to obtain a solution using
formulae or code. Our findings indicate that the approach to reasoning the
models use is unlike retrieval, and more like a generalisable strategy that
synthesises procedural knowledge from documents doing a similar form of
reasoning.",2024-11-19,"Laura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwarak Talupuru, Acyr Locatelli, Robert Kirk, Tim Rocktäschel, Edward Grefenstette, Max Bartolo",http://arxiv.org/pdf/2411.12580v2,cs.CL
Large Language Models for Combinatorial Optimization of Design Structure Matrix,"Combinatorial optimization (CO) is essential for improving efficiency and
performance in engineering applications. As complexity increases with larger
problem sizes and more intricate dependencies, identifying the optimal solution
become challenging. When it comes to real-world engineering problems,
algorithms based on pure mathematical reasoning are limited and incapable to
capture the contextual nuances necessary for optimization. This study explores
the potential of Large Language Models (LLMs) in solving engineering CO
problems by leveraging their reasoning power and contextual knowledge. We
propose a novel LLM-based framework that integrates network topology and domain
knowledge to optimize the sequencing of Design Structure Matrix (DSM)-a common
CO problem. Our experiments on various DSM cases demonstrate that the proposed
method achieves faster convergence and higher solution quality than benchmark
methods. Moreover, results show that incorporating contextual domain knowledge
significantly improves performance despite the choice of LLMs. These findings
highlight the potential of LLMs in tackling complex real-world CO problems by
combining semantic and mathematical reasoning. This approach paves the way for
a new paradigm in in real-world combinatorial optimization.",2024-11-19,"Shuo Jiang, Min Xie, Jianxi Luo",http://arxiv.org/pdf/2411.12571v1,cs.CL
Predicting Customer Satisfaction by Replicating the Survey Response Distribution,"For many call centers, customer satisfaction (CSAT) is a key performance
indicator (KPI). However, only a fraction of customers take the CSAT survey
after the call, leading to a biased and inaccurate average CSAT value, and
missed opportunities for coaching, follow-up, and rectification. Therefore,
call centers can benefit from a model predicting customer satisfaction on calls
where the customer did not complete the survey. Given that CSAT is a closely
monitored KPI, it is critical to minimize any bias in the average predicted
CSAT (pCSAT). In this paper, we introduce a method such that predicted CSAT
(pCSAT) scores accurately replicate the distribution of survey CSAT responses
for every call center with sufficient data in a live production environment.
The method can be applied to many multiclass classification problems to improve
the class balance and minimize its changes upon model updates.",2024-11-19,"Etienne Manderscheid, Matthias Lee",http://arxiv.org/pdf/2411.12539v1,cs.CL
Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues,"Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and
DeltaNet have emerged as efficient alternatives to Transformers for long
sequences. However, both Transformers and LRNNs struggle to perform
state-tracking, which may impair performance in tasks such as code evaluation.
In one forward pass, current architectures are unable to solve even parity, the
simplest state-tracking task, which non-linear RNNs can handle effectively.
Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like
Mamba to solve parity stems from restricting the value range of their diagonal
state-transition matrices to $[0, 1]$ and that incorporating negative values
can resolve this issue. We extend this result to non-diagonal LRNNs such as
DeltaNet. We prove that finite precision LRNNs with state-transition matrices
having only positive eigenvalues cannot solve parity, while non-triangular
matrices are needed to count modulo $3$. Notably, we also prove that LRNNs can
learn any regular language when their state-transition matrices are products of
identity minus vector outer product matrices, each with eigenvalues in the
range $[-1, 1]$. Our experiments confirm that extending the eigenvalue range of
Mamba and DeltaNet to include negative values not only enables them to solve
parity but consistently improves their performance on state-tracking tasks. We
also show that state-tracking enabled LRNNs can be pretrained stably and
efficiently at scale (1.3B parameters), achieving competitive performance on
language modeling and showing promise on code and math tasks.",2024-11-19,"Riccardo Grazzi, Julien Siems, Arber Zela, Jörg K. H. Franke, Frank Hutter, Massimiliano Pontil",http://arxiv.org/pdf/2411.12537v5,cs.CL
Guiding Word Equation Solving using Graph Neural Networks (Extended Technical Report),"This paper proposes a Graph Neural Network-guided algorithm for solving word
equations, based on the well-known Nielsen transformation for splitting
equations. The algorithm iteratively rewrites the first terms of each side of
an equation, giving rise to a tree-like search space. The choice of path at
each split point of the tree significantly impacts solving time, motivating the
use of Graph Neural Networks (GNNs) for efficient split decision-making. Split
decisions are encoded as multi-classification tasks, and five graph
representations of word equations are introduced to encode their structural
information for GNNs. The algorithm is implemented as a solver named DragonLi.
Experiments are conducted on artificial and real-world benchmarks. The
algorithm performs particularly well on satisfiable problems. For single word
\mbox{equations}, DragonLi can solve significantly more problems than
well-established string solvers. For the conjunction of multiple word
equations, DragonLi is competitive with state-of-the-art string solvers.",2024-11-19,"Parosh Aziz Abdulla, Mohamed Faouzi Atig, Julie Cailler, Chencheng Liang, Philipp Rümmer",http://arxiv.org/pdf/2411.15194v1,cs.CL
Eradicating Social Biases in Sentiment Analysis using Semantic Blinding and Semantic Propagation Graph Neural Networks,"This paper introduces the Semantic Propagation Graph Neural Network (SProp
GNN), a machine learning sentiment analysis (SA) architecture that relies
exclusively on syntactic structures and word-level emotional cues to predict
emotions in text. By semantically blinding the model to information about
specific words, it is robust to social biases such as political or gender bias
that have been plaguing previous machine learning-based SA systems. The SProp
GNN shows performance superior to lexicon-based alternatives such as VADER
(Valence Aware Dictionary and Sentiment Reasoner) and EmoAtlas on two different
prediction tasks, and across two languages. Additionally, it approaches the
accuracy of transformer-based models while significantly reducing bias in
emotion prediction tasks. By offering improved explainability and reducing
bias, the SProp GNN bridges the methodological gap between interpretable
lexicon approaches and powerful, yet often opaque, deep learning models,
offering a robust tool for fair and effective emotion analysis in understanding
human behavior through text.",2024-11-19,Hubert Plisiecki,http://arxiv.org/pdf/2411.12493v3,cs.CL
Regular-pattern-sensitive CRFs for Distant Label Interactions,"Linear-chain conditional random fields (CRFs) are a common model component
for sequence labeling tasks when modeling the interactions between different
labels is important. However, the Markov assumption limits linear-chain CRFs to
only directly modeling interactions between adjacent labels. Weighted
finite-state transducers (FSTs) are a related approach which can be made to
model distant label-label interactions, but exact label inference is
intractable for these models in the general case, and the task of selecting an
appropriate automaton structure for the desired interaction types poses a
practical challenge. In this work, we present regular-pattern-sensitive CRFs
(RPCRFs), a method of enriching standard linear-chain CRFs with the ability to
learn long-distance label interactions which occur in user-specified patterns.
This approach allows users to write regular-expression label patterns concisely
specifying which types of interactions the model should take into account,
allowing the model to learn from data whether and in which contexts these
patterns occur. The result can be interpreted alternatively as a CRF augmented
with additional, non-local potentials, or as a finite-state transducer whose
structure is defined by a set of easily-interpretable patterns. Critically,
unlike the general case for FSTs (and for non-chain CRFs), exact training and
inference are tractable for many pattern sets. In this work, we detail how a
RPCRF can be automatically constructed from a set of user-specified patterns,
and demonstrate the model's effectiveness on synthetic data, showing how
different types of patterns can capture different nonlocal dependency
structures in label sequences.",2024-11-19,"Sean Papay, Roman Klinger, Sebastian Pado",http://arxiv.org/pdf/2411.12484v1,cs.CL
Analysing Explanation-Related Interactions in Collaborative Perception-Cognition-Communication-Action,"Effective communication is essential in collaborative tasks, so AI-equipped
robots working alongside humans need to be able to explain their behaviour in
order to cooperate effectively and earn trust. We analyse and classify
communications among human participants collaborating to complete a simulated
emergency response task. The analysis identifies messages that relate to
various kinds of interactive explanations identified in the explainable AI
literature. This allows us to understand what type of explanations humans
expect from their teammates in such settings, and thus where AI-equipped robots
most need explanation capabilities. We find that most explanation-related
messages seek clarification in the decisions or actions taken. We also confirm
that messages have an impact on the performance of our simulated task.",2024-11-19,"Marc Roig Vilamala, Jack Furby, Julian de Gortari Briseno, Mani Srivastava, Alun Preece, Carolina Fuentes Toro",http://arxiv.org/pdf/2411.12483v1,cs.CL
NMT-Obfuscator Attack: Ignore a sentence in translation with only one word,"Neural Machine Translation systems are used in diverse applications due to
their impressive performance. However, recent studies have shown that these
systems are vulnerable to carefully crafted small perturbations to their
inputs, known as adversarial attacks. In this paper, we propose a new type of
adversarial attack against NMT models. In this attack, we find a word to be
added between two sentences such that the second sentence is ignored and not
translated by the NMT model. The word added between the two sentences is such
that the whole adversarial text is natural in the source language. This type of
attack can be harmful in practical scenarios since the attacker can hide
malicious information in the automatic translation made by the target NMT
model. Our experiments show that different NMT models and translation tasks are
vulnerable to this type of attack. Our attack can successfully force the NMT
models to ignore the second part of the input in the translation for more than
50% of all cases while being able to maintain low perplexity for the whole
input.",2024-11-19,"Sahar Sadrizadeh, César Descalzo, Ljiljana Dolamic, Pascal Frossard",http://arxiv.org/pdf/2411.12473v1,cs.CL
Exploring Iterative Controllable Summarization with Large Language Models,"Large language models (LLMs) have demonstrated remarkable performance in
abstractive summarization tasks. However, their ability to precisely control
summary attributes (e.g., length or topic) remains underexplored, limiting
their adaptability to specific user preferences. In this paper, we
systematically explore the controllability of LLMs. To this end, we revisit
summary attribute measurements and introduce iterative evaluation metrics,
failure rate and average iteration count to precisely evaluate controllability
of LLMs, rather than merely assessing errors. Our findings show that LLMs
struggle more with numerical attributes than with linguistic attributes. To
address this challenge, we propose a guide-to-explain framework (GTE) for
controllable summarization. Our GTE framework enables the model to identify
misaligned attributes in the initial draft and guides it in self-explaining
errors in the previous output. By allowing the model to reflect on its
misalignment, GTE generates well-adjusted summaries that satisfy the desired
attributes with robust effectiveness, requiring surprisingly fewer iterations
than other iterative approaches.",2024-11-19,"Sangwon Ryu, Heejin Do, Daehee Kim, Hwanjo Yu, Dongwoo Kim, Yunsu Kim, Gary Geunbae Lee, Jungseul Ok",http://arxiv.org/pdf/2411.12460v2,cs.CL
Variation between Credible and Non-Credible News Across Topics,"'Fake News' continues to undermine trust in modern journalism and politics.
Despite continued efforts to study fake news, results have been conflicting.
Previous attempts to analyse and combat fake news have largely focused on
distinguishing fake news from truth, or differentiating between its various
sub-types (such as propaganda, satire, misinformation, etc.) This paper
conducts a linguistic and stylistic analysis of fake news, focusing on
variation between various news topics. It builds on related work identifying
features from discourse and linguistics in deception detection by analysing
five distinct news topics: Economy, Entertainment, Health, Science, and Sports.
The results emphasize that linguistic features vary between credible and
deceptive news in each domain and highlight the importance of adapting
classification tasks to accommodate variety-based stylistic and linguistic
differences in order to achieve better real-world performance.",2024-11-19,Emilie Francis,http://arxiv.org/pdf/2411.12458v1,cs.CL
Neon: News Entity-Interaction Extraction for Enhanced Question Answering,"Capturing fresh information in near real-time and using it to augment
existing large language models (LLMs) is essential to generate up-to-date,
grounded, and reliable output. This problem becomes particularly challenging
when LLMs are used for informational tasks in rapidly evolving fields, such as
Web search related to recent or unfolding events involving entities, where
generating temporally relevant responses requires access to up-to-the-hour news
sources. However, the information modeled by the parametric memory of LLMs is
often outdated, and Web results from prototypical retrieval systems may fail to
capture the latest relevant information and struggle to handle conflicting
reports in evolving news. To address this challenge, we present the NEON
framework, designed to extract emerging entity interactions -- such as events
or activities -- as described in news articles. NEON constructs an
entity-centric timestamped knowledge graph that captures such interactions,
thereby facilitating enhanced QA capabilities related to news events. Our
framework innovates by integrating open Information Extraction (openIE) style
tuples into LLMs to enable in-context retrieval-augmented generation. This
integration demonstrates substantial improvements in QA performance when
tackling temporal, entity-centric search queries. Through NEON, LLMs can
deliver more accurate, reliable, and up-to-date responses.",2024-11-19,"Sneha Singhania, Silviu Cucerzan, Allen Herring, Sujay Kumar Jauhar",http://arxiv.org/pdf/2411.12449v2,cs.CL
Arabic-Nougat: Fine-Tuning Vision Transformers for Arabic OCR and Markdown Extraction,"We present Arabic-Nougat, a suite of OCR models for converting Arabic book
pages into structured Markdown text. Based on Meta's Nougat architecture,
Arabic-Nougat includes three specialized models: arabic-small-nougat,
arabic-base-nougat, and arabic-large-nougat. These models are fine-tuned on a
synthetic dataset, arabic-img2md, comprising 13.7k pairs of Arabic book pages
and their Markdown representations. Key contributions include the
Aranizer-PBE-86k tokenizer, designed for efficient tokenization, and the use of
torch.bfloat16 precision with Flash Attention 2 for optimized training and
inference. Our models achieve state-of-the-art performance, with
arabic-large-nougat delivering the highest Markdown Structure Accuracy and the
lowest Character Error Rate. Additionally, we release a large-scale dataset
containing 1.1 billion Arabic tokens extracted from over 8,500 books using our
best-performing model, providing a valuable resource for Arabic OCR research.
All models, datasets, and code are open-sourced and available at
https://github.com/MohamedAliRashad/arabic-nougat.",2024-11-19,Mohamed Rashad,http://arxiv.org/pdf/2411.17835v1,cs.CL
RadPhi-3: Small Language Models for Radiology,"LLM based copilot assistants are useful in everyday tasks. There is a
proliferation in the exploration of AI assistant use cases to support radiology
workflows in a reliable manner. In this work, we present RadPhi-3, a Small
Language Model instruction tuned from Phi-3-mini-4k-instruct with 3.8B
parameters to assist with various tasks in radiology workflows. While
impression summary generation has been the primary task which has been explored
in prior works w.r.t radiology reports of Chest X-rays, we also explore other
useful tasks like change summary generation comparing the current radiology
report and its prior report, section extraction from radiology reports, tagging
the reports with various pathologies and tubes, lines or devices present in
them etc. In-addition, instruction tuning RadPhi-3 involved learning from a
credible knowledge source used by radiologists, Radiopaedia.org. RadPhi-3 can
be used both to give reliable answers for radiology related queries as well as
perform useful tasks related to radiology reports. RadPhi-3 achieves SOTA
results on the RaLEs radiology report generation benchmark.",2024-11-19,"Mercy Ranjit, Shaury Srivastav, Tanuja Ganu",http://arxiv.org/pdf/2411.13604v1,cs.CL
GRL-Prompt: Towards Knowledge Graph based Prompt Optimization via Reinforcement Learning,"Large language models (LLMs) have demonstrated impressive success in a wide
range of natural language processing (NLP) tasks due to their extensive general
knowledge of the world. Recent works discovered that the performance of LLMs is
heavily dependent on the input prompt. However, prompt engineering is usually
done manually in a trial-and-error fashion, which can be labor-intensive and
challenging in order to find the optimal prompts. To address these problems and
unleash the utmost potential of LLMs, we propose a novel LLMs-agnostic
framework for prompt optimization, namely GRL-Prompt, which aims to
automatically construct optimal prompts via reinforcement learning (RL) in an
end-to-end manner. To provide structured action/state representation for
optimizing prompts, we construct a knowledge graph (KG) that better encodes the
correlation between the user query and candidate in-context examples.
Furthermore, a policy network is formulated to generate the optimal action by
selecting a set of in-context examples in a rewardable order to construct the
prompt. Additionally, the embedding-based reward shaping is utilized to
stabilize the RL training process. The experimental results show that
GRL-Prompt outperforms recent state-of-the-art methods, achieving an average
increase of 0.10 in ROUGE-1, 0.07 in ROUGE-2, 0.07 in ROUGE-L, and 0.05 in
BLEU.",2024-11-19,"Yuze Liu, Tingjie Liu, Tiehua Zhang, Youhua Xia, Jinze Wang, Zhishu Shen, Jiong Jin, Fei Richard Yu",http://arxiv.org/pdf/2411.14479v1,cs.CL
Evaluating the Prompt Steerability of Large Language Models,"Building pluralistic AI requires designing models that are able to be shaped
to represent a wide range of value systems and cultures. Achieving this
requires first being able to evaluate the degree to which a given model is
capable of reflecting various personas. To this end, we propose a benchmark for
evaluating the steerability of model personas as a function of prompting. Our
design is based on a formal definition of prompt steerability, which analyzes
the degree to which a model's joint behavioral distribution can be shifted from
its baseline. By defining steerability indices and inspecting how these indices
change as a function of steering effort, we can estimate the steerability of a
model across various persona dimensions and directions. Our benchmark reveals
that the steerability of many current models is limited -- due to both a skew
in their baseline behavior and an asymmetry in their steerability across many
persona dimensions. We release an implementation of our benchmark at
https://github.com/IBM/prompt-steering.",2024-11-19,"Erik Miehling, Michael Desmond, Karthikeyan Natesan Ramamurthy, Elizabeth M. Daly, Pierre Dognin, Jesus Rios, Djallel Bouneffouf, Miao Liu",http://arxiv.org/pdf/2411.12405v2,cs.CL
Do LLMs Understand Ambiguity in Text? A Case Study in Open-world Question Answering,"Ambiguity in natural language poses significant challenges to Large Language
Models (LLMs) used for open-domain question answering. LLMs often struggle with
the inherent uncertainties of human communication, leading to
misinterpretations, miscommunications, hallucinations, and biased responses.
This significantly weakens their ability to be used for tasks like
fact-checking, question answering, feature extraction, and sentiment analysis.
Using open-domain question answering as a test case, we compare off-the-shelf
and few-shot LLM performance, focusing on measuring the impact of explicit
disambiguation strategies. We demonstrate how simple, training-free,
token-level disambiguation methods may be effectively used to improve LLM
performance for ambiguous question answering tasks. We empirically show our
findings and discuss best practices and broader impacts regarding ambiguity in
LLMs.",2024-11-19,"Aryan Keluskar, Amrita Bhattacharjee, Huan Liu",http://arxiv.org/pdf/2411.12395v1,cs.CL
RedPajama: an Open Dataset for Training Large Language Models,"Large language models are increasingly becoming a cornerstone technology in
artificial intelligence, the sciences, and society as a whole, yet the optimal
strategies for dataset composition and filtering remain largely elusive. Many
of the top-performing models lack transparency in their dataset curation and
model development processes, posing an obstacle to the development of fully
open language models. In this paper, we identify three core data-related
challenges that must be addressed to advance open-source language models. These
include (1) transparency in model development, including the data curation
process, (2) access to large quantities of high-quality data, and (3)
availability of artifacts and metadata for dataset curation and analysis. To
address these challenges, we release RedPajama-V1, an open reproduction of the
LLaMA training dataset. In addition, we release RedPajama-V2, a massive
web-only dataset consisting of raw, unfiltered text data together with quality
signals and metadata. Together, the RedPajama datasets comprise over 100
trillion tokens spanning multiple domains and with their quality signals
facilitate the filtering of data, aiming to inspire the development of numerous
new datasets. To date, these datasets have already been used in the training of
strong language models used in production, such as Snowflake Arctic,
Salesforce's XGen and AI2's OLMo. To provide insight into the quality of
RedPajama, we present a series of analyses and ablation studies with
decoder-only language models with up to 1.6B parameters. Our findings
demonstrate how quality signals for web data can be effectively leveraged to
curate high-quality subsets of the dataset, underscoring the potential of
RedPajama to advance the development of transparent and high-performing
language models at scale.",2024-11-19,"Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, Ce Zhang",http://arxiv.org/pdf/2411.12372v1,cs.CL
A Layered Architecture for Developing and Enhancing Capabilities in Large Language Model-based Software Systems,"Significant efforts has been made to expand the use of Large Language Models
(LLMs) beyond basic language tasks. While the generalizability and versatility
of LLMs have enabled widespread adoption, evolving demands in application
development often exceed their native capabilities. Meeting these demands may
involve a diverse set of methods, such as enhancing creativity through either
inference temperature adjustments or creativity-provoking prompts. Selecting
the right approach is critical, as different methods lead to trade-offs in
engineering complexity, scalability, and operational costs. This paper
introduces a layered architecture that organizes LLM software system
development into distinct layers, each characterized by specific attributes. By
aligning capabilities with these layers, the framework encourages the
systematic implementation of capabilities in effective and efficient ways that
ultimately supports desired functionalities and qualities. Through practical
case studies, we illustrate the utility of the framework. This work offers
developers actionable insights for selecting suitable technologies in LLM-based
software system development, promoting robustness and scalability.",2024-11-19,"Dawen Zhang, Xiwei Xu, Chen Wang, Zhenchang Xing, Robert Mao",http://arxiv.org/pdf/2411.12357v1,cs.CL
Balancing Accuracy and Efficiency in Multi-Turn Intent Classification for LLM-Powered Dialog Systems in Production,"Accurate multi-turn intent classification is essential for advancing
conversational AI systems. However, challenges such as the scarcity of
comprehensive datasets and the complexity of contextual dependencies across
dialogue turns hinder progress. This paper presents two novel approaches
leveraging Large Language Models (LLMs) to enhance scalability and reduce
latency in production dialogue systems. First, we introduce Symbol Tuning,
which simplifies intent labels to reduce task complexity and improve
performance in multi-turn dialogues. Second, we propose C-LARA
(Consistency-aware, Linguistics Adaptive Retrieval Augmentation), a framework
that employs LLMs for data augmentation and pseudo-labeling to generate
synthetic multi-turn dialogues. These enriched datasets are used to fine-tune a
small, efficient model suitable for deployment. Experiments conducted on
multilingual dialogue datasets demonstrate significant improvements in
classification accuracy and resource efficiency. Our methods enhance multi-turn
intent classification accuracy by 5.09%, reduce annotation costs by 40%, and
enable scalable deployment in low-resource multilingual industrial systems,
highlighting their practicality and impact.",2024-11-19,"Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim",http://arxiv.org/pdf/2411.12307v1,cs.CL
CUE-M: Contextual Understanding and Enhanced Search with Multimodal Large Language Model,"The integration of Retrieval-Augmented Generation (RAG) with Multimodal Large
Language Models (MLLMs) has revolutionized information retrieval and expanded
the practical applications of AI. However, current systems struggle in
accurately interpreting user intent, employing diverse retrieval strategies,
and effectively filtering unintended or inappropriate responses, limiting their
effectiveness. This paper introduces Contextual Understanding and Enhanced
Search with MLLM (CUE-M), a novel multimodal search framework that addresses
these challenges through a multi-stage pipeline comprising image context
enrichment, intent refinement, contextual query generation, external API
integration, and relevance-based filtering. CUE-M incorporates a robust
filtering pipeline combining image-based, text-based, and multimodal
classifiers, dynamically adapting to instance- and category-specific concern
defined by organizational policies. Extensive experiments on real-word datasets
and public benchmarks on knowledge-based VQA and safety demonstrated that CUE-M
outperforms baselines and establishes new state-of-the-art results, advancing
the capabilities of multimodal retrieval systems.",2024-11-19,"Dongyoung Go, Taesun Whang, Chanhee Lee, Hwa-Yeon Kim, Sunghoon Park, Seunghwan Ji, Jinho Kim, Dongchan Kim, Young-Bum Kim",http://arxiv.org/pdf/2411.12287v3,cs.CL
"Building Trust: Foundations of Security, Safety and Transparency in AI","This paper explores the rapidly evolving ecosystem of publicly available AI
models, and their potential implications on the security and safety landscape.
As AI models become increasingly prevalent, understanding their potential risks
and vulnerabilities is crucial. We review the current security and safety
scenarios while highlighting challenges such as tracking issues, remediation,
and the apparent absence of AI model lifecycle and ownership processes.
Comprehensive strategies to enhance security and safety for both model
developers and end-users are proposed. This paper aims to provide some of the
foundational pieces for more standardized security, safety, and transparency in
the development and operation of AI models and the larger open ecosystems and
communities forming around them.",2024-11-19,"Huzaifa Sidhpurwala, Garth Mollett, Emily Fox, Mark Bestavros, Huamin Chen",http://arxiv.org/pdf/2411.12275v1,cs.CL
"A Review on Generative AI Models for Synthetic Medical Text, Time Series, and Longitudinal Data","This paper presents the results of a novel scoping review on the practical
models for generating three different types of synthetic health records (SHRs):
medical text, time series, and longitudinal data. The innovative aspects of the
review, which incorporate study objectives, data modality, and research
methodology of the reviewed studies, uncover the importance and the scope of
the topic for the digital medicine context. In total, 52 publications met the
eligibility criteria for generating medical time series (22), longitudinal data
(17), and medical text (13). Privacy preservation was found to be the main
research objective of the studied papers, along with class imbalance, data
scarcity, and data imputation as the other objectives. The adversarial
network-based, probabilistic, and large language models exhibited superiority
for generating synthetic longitudinal data, time series, and medical texts,
respectively. Finding a reliable performance measure to quantify SHR
re-identification risk is the major research gap of the topic.",2024-11-19,"Mohammad Loni, Fatemeh Poursalim, Mehdi Asadi, Arash Gharehbaghi",http://arxiv.org/pdf/2411.12274v1,cs.CL
Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service,"Low-resource machine translation (MT) presents a diversity of community needs
and application challenges that remain poorly understood. To complement surveys
and focus groups, which tend to rely on small samples of respondents, we
propose an observational study on actual usage patterns of tetun$.$org, a
specialized MT service for the Tetun language, which is the lingua franca in
Timor-Leste. Our analysis of 100,000 translation requests reveals patterns that
challenge assumptions based on existing corpora. We find that users, many of
them students on mobile devices, typically translate text from a high-resource
language into Tetun across diverse domains including science, healthcare, and
daily life. This contrasts sharply with available Tetun corpora, which are
dominated by news articles covering government and social issues. Our results
suggest that MT systems for institutionalized minority languages like Tetun
should prioritize accuracy on domains relevant to educational contexts, in the
high-resource to low-resource direction. More broadly, this study demonstrates
how observational analysis can inform low-resource language technology
development, by grounding research in practical community needs.",2024-11-19,"Raphael Merx, Adérito José Guterres Correia, Hanna Suominen, Ekaterina Vylomova",http://arxiv.org/pdf/2411.12262v4,cs.CL
Predicting User Intents and Musical Attributes from Music Discovery Conversations,"Intent classification is a text understanding task that identifies user needs
from input text queries. While intent classification has been extensively
studied in various domains, it has not received much attention in the music
domain. In this paper, we investigate intent classification models for music
discovery conversation, focusing on pre-trained language models. Rather than
only predicting functional needs: intent classification, we also include a task
for classifying musical needs: musical attribute classification. Additionally,
we propose a method of concatenating previous chat history with just
single-turn user queries in the input text, allowing the model to understand
the overall conversation context better. Our proposed model significantly
improves the F1 score for both user intent and musical attribute
classification, and surpasses the zero-shot and few-shot performance of the
pretrained Llama 3 model.",2024-11-19,"Daeyong Kwon, SeungHeon Doh, Juhan Nam",http://arxiv.org/pdf/2411.12254v2,cs.CL
Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages,"Large Language Models (LLMs) based on transformer architectures have
revolutionized a variety of domains, with tokenization playing a pivotal role
in their pre-processing and fine-tuning stages. In multilingual models,
particularly those tailored for Indic languages, effective tokenization is
crucial for optimizing performance. This paper presents a comprehensive
evaluation of tokenizers used by 12 LLMs across all 22 official languages of
India, with a focus on comparing the efficiency of their tokenization
processes. We employed the Normalized Sequence Length (NSL) as a key metric in
our analysis. Our findings reveal that the SUTRA tokenizer outperforms all
other models, including several Indic-specific models, excelling in 14
languages. Notable insights include the SUTRA tokenizer's superior handling of
Indic languages, GPT-4o's advancement over its predecessor GPT-4 in processing
Indian languages, and the limited performance of Project Indus in certain
languages. This study underscores the critical importance of developing
targeted tokenization strategies for multilingual and Indic-centric models,
laying the groundwork for future improvements in tokenizer design to enhance
linguistic coverage and model efficiency.",2024-11-19,"S. Tamang, D. J. Bora",http://arxiv.org/pdf/2411.12240v2,cs.CL
BoolQuestions: Does Dense Retrieval Understand Boolean Logic in Language?,"Dense retrieval, which aims to encode the semantic information of arbitrary
text into dense vector representations or embeddings, has emerged as an
effective and efficient paradigm for text retrieval, consequently becoming an
essential component in various natural language processing systems. These
systems typically focus on optimizing the embedding space by attending to the
relevance of text pairs, while overlooking the Boolean logic inherent in
language, which may not be captured by current training objectives. In this
work, we first investigate whether current retrieval systems can comprehend the
Boolean logic implied in language. To answer this question, we formulate the
task of Boolean Dense Retrieval and collect a benchmark dataset, BoolQuestions,
which covers complex queries containing basic Boolean logic and corresponding
annotated passages. Through extensive experimental results on the proposed task
and benchmark dataset, we draw the conclusion that current dense retrieval
systems do not fully understand Boolean logic in language, and there is a long
way to go to improve our dense retrieval systems. Furthermore, to promote
further research on enhancing the understanding of Boolean logic for language
models, we explore Boolean operation on decomposed query and propose a
contrastive continual training method that serves as a strong baseline for the
research community.",2024-11-19,"Zongmeng Zhang, Jinhua Zhu, Wengang Zhou, Xiang Qi, Peng Zhang, Houqiang Li",http://arxiv.org/pdf/2411.12235v1,cs.CL
StreetviewLLM: Extracting Geographic Information Using a Chain-of-Thought Multimodal Large Language Model,"Geospatial predictions are crucial for diverse fields such as disaster
management, urban planning, and public health. Traditional machine learning
methods often face limitations when handling unstructured or multi-modal data
like street view imagery. To address these challenges, we propose
StreetViewLLM, a novel framework that integrates a large language model with
the chain-of-thought reasoning and multimodal data sources. By combining street
view imagery with geographic coordinates and textual data, StreetViewLLM
improves the precision and granularity of geospatial predictions. Using
retrieval-augmented generation techniques, our approach enhances geographic
information extraction, enabling a detailed analysis of urban environments. The
model has been applied to seven global cities, including Hong Kong, Tokyo,
Singapore, Los Angeles, New York, London, and Paris, demonstrating superior
performance in predicting urban indicators, including population density,
accessibility to healthcare, normalized difference vegetation index, building
height, and impervious surface. The results show that StreetViewLLM
consistently outperforms baseline models, offering improved predictive accuracy
and deeper insights into the built environment. This research opens new
opportunities for integrating the large language model into urban analytics,
decision-making in urban planning, infrastructure management, and environmental
monitoring.",2024-11-19,"Zongrong Li, Junhao Xu, Siqin Wang, Yifan Wu, Haiyang Li",http://arxiv.org/pdf/2411.14476v1,cs.CL
Revisiting Fake News Detection: Towards Temporality-aware Evaluation by Leveraging Engagement Earliness,"Social graph-based fake news detection aims to identify news articles
containing false information by utilizing social contexts, e.g., user
information, tweets and comments. However, conventional methods are evaluated
under less realistic scenarios, where the model has access to future knowledge
on article-related and context-related data during training. In this work, we
newly formalize a more realistic evaluation scheme that mimics real-world
scenarios, where the data is temporality-aware and the detection model can only
be trained on data collected up to a certain point in time. We show that the
discriminative capabilities of conventional methods decrease sharply under this
new setting, and further propose DAWN, a method more applicable to such
scenarios. Our empirical findings indicate that later engagements (e.g.,
consuming or reposting news) contribute more to noisy edges that link real
news-fake news pairs in the social graph. Motivated by this, we utilize feature
representations of engagement earliness to guide an edge weight estimator to
suppress the weights of such noisy edges, thereby enhancing the detection
performance of DAWN. Through extensive experiments, we demonstrate that DAWN
outperforms existing fake news detection methods under real-world environments.
The source code is available at https://github.com/LeeJunmo/DAWN.",2024-11-19,"Junghoon Kim, Junmo Lee, Yeonjun In, Kanghoon Yoon, Chanyoung Park",http://arxiv.org/pdf/2411.12775v1,cs.CL
Just KIDDIN: Knowledge Infusion and Distillation for Detection of INdecent Memes,"Toxicity identification in online multimodal environments remains a
challenging task due to the complexity of contextual connections across
modalities (e.g., textual and visual). In this paper, we propose a novel
framework that integrates Knowledge Distillation (KD) from Large Visual
Language Models (LVLMs) and knowledge infusion to enhance the performance of
toxicity detection in hateful memes. Our approach extracts sub-knowledge graphs
from ConceptNet, a large-scale commonsense Knowledge Graph (KG) to be infused
within a compact VLM framework. The relational context between toxic phrases in
captions and memes, as well as visual concepts in memes enhance the model's
reasoning capabilities. Experimental results from our study on two hate speech
benchmark datasets demonstrate superior performance over the state-of-the-art
baselines across AU-ROC, F1, and Recall with improvements of 1.1%, 7%, and 35%,
respectively. Given the contextual complexity of the toxicity detection task,
our approach showcases the significance of learning from both explicit (i.e.
KG) as well as implicit (i.e. LVLMs) contextual cues incorporated through a
hybrid neurosymbolic approach. This is crucial for real-world applications
where accurate and scalable recognition of toxic content is critical for
creating safer online environments.",2024-11-19,"Rahul Garg, Trilok Padhi, Hemang Jain, Ugur Kursuncu, Ponnurangam Kumaraguru",http://arxiv.org/pdf/2411.12174v2,cs.CL
A Combined Encoder and Transformer Approach for Coherent and High-Quality Text Generation,"This research introduces a novel text generation model that combines BERT's
semantic interpretation strengths with GPT-4's generative capabilities,
establishing a high standard in generating coherent, contextually accurate
language. Through the combined architecture, the model enhances semantic depth
and maintains smooth, human-like text flow, overcoming limitations seen in
prior models. Experimental benchmarks reveal that BERT-GPT-4 surpasses
traditional models, including GPT-3, T5, BART, Transformer-XL, and CTRL, in key
metrics like Perplexity and BLEU, showcasing its superior natural language
generation performance. By fully utilizing contextual information, this hybrid
model generates text that is not only logically coherent but also aligns
closely with human language patterns, providing an advanced solution for text
generation tasks. This research highlights the potential of integrating
semantic understanding with advanced generative models, contributing new
insights for NLP, and setting a foundation for broader applications of
large-scale generative architectures in areas such as automated writing,
question-answer systems, and adaptive conversational agents.",2024-11-19,"Jiajing Chen, Shuo Wang, Zhen Qi, Zhenhong Zhang, Chihang Wang, Hongye Zheng",http://arxiv.org/pdf/2411.12157v1,cs.CL
HNCSE: Advancing Sentence Embeddings via Hybrid Contrastive Learning with Hard Negatives,"Unsupervised sentence representation learning remains a critical challenge in
modern natural language processing (NLP) research. Recently, contrastive
learning techniques have achieved significant success in addressing this issue
by effectively capturing textual semantics. Many such approaches prioritize the
optimization using negative samples. In fields such as computer vision, hard
negative samples (samples that are close to the decision boundary and thus more
difficult to distinguish) have been shown to enhance representation learning.
However, adapting hard negatives to contrastive sentence learning is complex
due to the intricate syntactic and semantic details of text. To address this
problem, we propose HNCSE, a novel contrastive learning framework that extends
the leading SimCSE approach. The hallmark of HNCSE is its innovative use of
hard negative samples to enhance the learning of both positive and negative
samples, thereby achieving a deeper semantic understanding. Empirical tests on
semantic textual similarity and transfer task datasets validate the superiority
of HNCSE.",2024-11-19,"Wenxiao Liu, Zihong Yang, Chaozhuo Li, Zijin Hong, Jianfeng Ma, Zhiquan Liu, Litian Zhang, Feiran Huang",http://arxiv.org/pdf/2411.12156v1,cs.CL
JuniperLiu at CoMeDi Shared Task: Models as Annotators in Lexical Semantics Disagreements,"We present the results of our system for the CoMeDi Shared Task, which
predicts majority votes (Subtask 1) and annotator disagreements (Subtask 2).
Our approach combines model ensemble strategies with MLP-based and
threshold-based methods trained on pretrained language models. Treating
individual models as virtual annotators, we simulate the annotation process by
designing aggregation measures that incorporate continuous relatedness scores
and discrete classification labels to capture both majority and disagreement.
Additionally, we employ anisotropy removal techniques to enhance performance.
Experimental results demonstrate the effectiveness of our methods, particularly
for Subtask 2. Notably, we find that standard deviation on continuous
relatedness scores among different model manipulations correlates with human
disagreement annotations compared to metrics on aggregated discrete labels. The
code will be published at https://github.com/RyanLiut/CoMeDi_Solution.",2024-11-19,"Zhu Liu, Zhen Hu, Ying Liu",http://arxiv.org/pdf/2411.12147v2,cs.CL
"A Computational Method for Measuring ""Open Codes"" in Qualitative Analysis","Qualitative analysis is critical to understanding human datasets in many
social science disciplines. Open coding is an inductive qualitative process
that identifies and interprets ""open codes"" from datasets. Yet, meeting
methodological expectations (such as ""as exhaustive as possible"") can be
challenging. While many machine learning (ML)/generative AI (GAI) studies have
attempted to support open coding, few have systematically measured or evaluated
GAI outcomes, increasing potential bias risks. Building on Grounded Theory and
Thematic Analysis theories, we present a computational method to measure and
identify potential biases from ""open codes"" systematically. Instead of
operationalizing human expert results as the ""ground truth,"" our method is
built upon a team-based approach between human and machine coders. We
experiment with two HCI datasets to establish this method's reliability by 1)
comparing it with human analysis, and 2) analyzing its output stability. We
present evidence-based suggestions and example workflows for ML/GAI to support
open coding.",2024-11-19,"John Chen, Alexandros Lotsos, Lexie Zhao, Caiyi Wang, Jessica Hullman, Bruce Sherin, Uri Wilensky, Michael Horn",http://arxiv.org/pdf/2411.12142v2,cs.CL
Mechanism and Emergence of Stacked Attention Heads in Multi-Layer Transformers,"In this paper, I introduce the retrieval problem, a simple yet common
reasoning task that can be solved only by transformers with a minimum number of
layers, which grows logarithmically with the input size. I empirically show
that large language models can solve the task under different prompting
formulations without any fine-tuning. To understand how transformers solve the
retrieval problem, I train several transformers on a minimal formulation.
Successful learning occurs only under the presence of an implicit curriculum. I
uncover the learned mechanisms by studying the attention maps in the trained
transformers. I also study the training process, uncovering that attention
heads always emerge in a specific sequence guided by the implicit curriculum.",2024-11-18,Tiberiu Musat,http://arxiv.org/pdf/2411.12118v4,cs.CL
Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning Methods,"Large language model unlearning aims to remove harmful information that LLMs
have learnt to prevent their use for malicious purposes. LLMU and RMU have been
proposed as two methods for LLM unlearning, achieving impressive results on
unlearning benchmarks. We study in detail the impact of unlearning on LLM
performance metrics using the WMDP dataset as well as a new biology dataset we
create. We show that unlearning has a notable impact on general model
capabilities, with the performance degradation being more significant in
general for LLMU. We further test the robustness of the two methods and find
that doing 5-shot prompting or rephrasing the question in simple ways can lead
to an over ten-fold increase in accuracy on unlearning benchmarks. Finally, we
show that training on unrelated data can almost completely recover
pre-unlearning performance, demonstrating that these methods fail at truly
unlearning. Our methodology serves as an evaluation framework for LLM
unlearning methods. The code is available at:
https://github.com/JaiDoshi/Knowledge-Erasure.",2024-11-18,"Jai Doshi, Asa Cooper Stickland",http://arxiv.org/pdf/2411.12103v3,cs.CL
Mitigating Gender Bias in Contextual Word Embeddings,"Word embeddings have been shown to produce remarkable results in tackling a
vast majority of NLP related tasks. Unfortunately, word embeddings also capture
the stereotypical biases that are prevalent in society, affecting the
predictive performance of the embeddings when used in downstream tasks. While
various techniques have been proposed \cite{bolukbasi2016man, zhao2018learning}
and criticized\cite{gonen2019lipstick} for static embeddings, very little work
has focused on mitigating bias in contextual embeddings. In this paper, we
propose a novel objective function for MLM(Masked-Language Modeling) which
largely mitigates the gender bias in contextual embeddings and also preserves
the performance for downstream tasks. Since previous works on measuring bias in
contextual embeddings lack in normative reasoning, we also propose novel
evaluation metrics that are straight-forward and aligned with our motivations
in debiasing. We also propose new methods for debiasing static embeddings and
provide empirical proof via extensive analysis and experiments, as to why the
main source of bias in static embeddings stems from the presence of
stereotypical names rather than gendered words themselves. All experiments and
embeddings studied are in English, unless otherwise
specified.\citep{bender2011achieving}.",2024-11-18,"Navya Yarrabelly, Vinay Damodaran, Feng-Guang Su",http://arxiv.org/pdf/2411.12074v1,cs.CL
Large Language Model for Qualitative Research -- A Systematic Mapping Study,"The exponential growth of text-based data in domains such as healthcare,
education, and social sciences has outpaced the capacity of traditional
qualitative analysis methods, which are time-intensive and prone to
subjectivity. Large Language Models (LLMs), powered by advanced generative AI,
have emerged as transformative tools capable of automating and enhancing
qualitative analysis. This study systematically maps the literature on the use
of LLMs for qualitative research, exploring their application contexts,
configurations, methodologies, and evaluation metrics. Findings reveal that
LLMs are utilized across diverse fields, demonstrating the potential to
automate processes traditionally requiring extensive human input. However,
challenges such as reliance on prompt engineering, occasional inaccuracies, and
contextual limitations remain significant barriers. This research highlights
opportunities for integrating LLMs with human expertise, improving model
robustness, and refining evaluation methodologies. By synthesizing trends and
identifying research gaps, this study aims to guide future innovations in the
application of LLMs for qualitative analysis.",2024-11-18,"Cauã Ferreira Barros, Bruna Borges Azevedo, Valdemar Vicente Graciano Neto, Mohamad Kassab, Marcos Kalinowski, Hugo Alexandre D. do Nascimento, Michelle C. G. S. P. Bandeira",http://arxiv.org/pdf/2411.14473v4,cs.CL
Benchmarking pre-trained text embedding models in aligning built asset information,"Accurate mapping of the built asset information to established data
classification systems and taxonomies is crucial for effective asset
management, whether for compliance at project handover or ad-hoc data
integration scenarios. Due to the complex nature of built asset data, which
predominantly comprises technical text elements, this process remains largely
manual and reliant on domain expert input. Recent breakthroughs in contextual
text representation learning (text embedding), particularly through pre-trained
large language models, offer promising approaches that can facilitate the
automation of cross-mapping of the built asset data. However, no comprehensive
evaluation has yet been conducted to assess these models' ability to
effectively represent the complex semantics specific to built asset technical
terminology. This study presents a comparative benchmark of state-of-the-art
text embedding models to evaluate their effectiveness in aligning built asset
information with domain-specific technical concepts. Our proposed datasets are
derived from two renowned built asset data classification dictionaries. The
results of our benchmarking across six proposed datasets, covering three tasks
of clustering, retrieval, and reranking, highlight the need for future research
on domain adaptation techniques. The benchmarking resources are published as an
open-source library, which will be maintained and extended to support future
evaluations in this field.",2024-11-18,"Mehrzad Shahinmoghadam, Ali Motamedi",http://arxiv.org/pdf/2411.12056v1,cs.CL
Exploring the Potential Role of Generative AI in the TRAPD Procedure for Survey Translation,"This paper explores and assesses in what ways generative AI can assist in
translating survey instruments. Writing effective survey questions is a
challenging and complex task, made even more difficult for surveys that will be
translated and deployed in multiple linguistic and cultural settings.
Translation errors can be detrimental, with known errors rendering data
unusable for its intended purpose and undetected errors leading to incorrect
conclusions. A growing number of institutions face this problem as surveys
deployed by private and academic organizations globalize, and the success of
their current efforts depends heavily on researchers' and translators'
expertise and the amount of time each party has to contribute to the task.
Thus, multilinguistic and multicultural surveys produced by teams with limited
expertise, budgets, or time are at significant risk for translation-based
errors in their data. We implement a zero-shot prompt experiment using ChatGPT
to explore generative AI's ability to identify features of questions that might
be difficult to translate to a linguistic audience other than the source
language. We find that ChatGPT can provide meaningful feedback on translation
issues, including common source survey language, inconsistent
conceptualization, sensitivity and formality issues, and nonexistent concepts.
In addition, we provide detailed information on the practicality of the
approach, including accessing the necessary software, associated costs, and
computational run times. Lastly, based on our findings, we propose avenues for
future research that integrate AI into survey translation practices.",2024-11-18,"Erica Ann Metheney, Lauren Yehle",http://arxiv.org/pdf/2411.14472v2,cs.CL
Popular LLMs Amplify Race and Gender Disparities in Human Mobility,"As large language models (LLMs) are increasingly applied in areas influencing
societal outcomes, it is critical to understand their tendency to perpetuate
and amplify biases. This study investigates whether LLMs exhibit biases in
predicting human mobility -- a fundamental human behavior -- based on race and
gender. Using three prominent LLMs -- GPT-4, Gemini, and Claude -- we analyzed
their predictions of visitations to points of interest (POIs) for individuals,
relying on prompts that included names with and without explicit demographic
details. We find that LLMs frequently reflect and amplify existing societal
biases. Specifically, predictions for minority groups were disproportionately
skewed, with these individuals being significantly less likely to be associated
with wealth-related points of interest (POIs). Gender biases were also evident,
as female individuals were consistently linked to fewer career-related POIs
compared to their male counterparts. These biased associations suggest that
LLMs not only mirror but also exacerbate societal stereotypes, particularly in
contexts involving race and gender.",2024-11-18,"Xinhua Wu, Qi R. Wang",http://arxiv.org/pdf/2411.14469v1,cs.CL
ByteScience: Bridging Unstructured Scientific Literature and Structured Data with Auto Fine-tuned Large Language Model in Token Granularity,"Natural Language Processing (NLP) is widely used to supply summarization
ability from long context to structured information. However, extracting
structured knowledge from scientific text by NLP models remains a challenge
because of its domain-specific nature to complex data preprocessing and the
granularity of multi-layered device-level information. To address this, we
introduce ByteScience, a non-profit cloud-based auto fine-tuned Large Language
Model (LLM) platform, which is designed to extract structured scientific data
and synthesize new scientific knowledge from vast scientific corpora. The
platform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to
natural science. The platform was built on Amazon Web Services (AWS) and
provides an automated, user-friendly workflow for custom model development and
data extraction. The platform achieves remarkable accuracy with only a small
amount of well-annotated articles. This innovative tool streamlines the
transition from the science literature to structured knowledge and data and
benefits the advancements in natural informatics.",2024-11-18,"Tong Xie, Hanzhi Zhang, Shaozhou Wang, Yuwei Wan, Imran Razzak, Chunyu Kit, Wenjie Zhang, Bram Hoex",http://arxiv.org/pdf/2411.12000v2,cs.CL
Understanding Chain-of-Thought in LLMs through Information Theory,"Large Language Models (LLMs) have shown impressive performance in complex
reasoning tasks through Chain-of-Thought (CoT) reasoning, allowing models to
break down problems into manageable sub-tasks. However, existing CoT evaluation
techniques either require annotated CoT data or fall short in accurately
assessing intermediate reasoning steps, leading to high rates of false
positives. In this paper, we formalize CoT reasoning in LLMs through an
information-theoretic lens. Specifically, our framework quantifies the
`information gain' at each reasoning step, enabling the identification of
failure modes in LLMs without the need for expensive annotated datasets. We
demonstrate the efficacy of our approach through extensive experiments on toy
and GSM-8K data, where it significantly outperforms existing outcome-based
methods by providing more accurate insights into model performance on
individual tasks.",2024-11-18,"Jean-Francois Ton, Muhammad Faaiz Taufiq, Yang Liu",http://arxiv.org/pdf/2411.11984v1,cs.CL
Bi-Mamba: Towards Accurate 1-Bit State Space Models,"The typical selective state-space model (SSM) of Mamba addresses several
limitations of Transformers, such as quadratic computational complexity with
sequence length and significant inference-time memory requirements due to the
key-value cache. However, the growing size of Mamba models continues to pose
training and deployment challenges and raises environmental concerns due to
considerable energy consumption. In this work, we introduce Bi-Mamba, a
scalable and powerful 1-bit Mamba architecture designed for more efficient
large language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba
models are trained from scratch on data volume as regular LLM pertaining using
an autoregressive distillation loss. Extensive experimental results on language
modeling demonstrate that Bi-Mamba achieves performance comparable to its
full-precision counterparts (e.g., FP16 or BF16) and much better accuracy than
post-training-binarization (PTB) Mamba baselines, while significantly reducing
memory footprint and energy consumption compared to the original Mamba model.
Our study pioneers a new linear computational complexity LLM framework under
low-bit representation and facilitates the future design of specialized
hardware tailored for efficient 1-bit Mamba-based LLMs.",2024-11-18,"Shengkun Tang, Liqun Ma, Haonan Li, Mingjie Sun, Zhiqiang Shen",http://arxiv.org/pdf/2411.11843v1,cs.CL
Tackling prediction tasks in relational databases with LLMs,"Though large language models (LLMs) have demonstrated exceptional performance
across numerous problems, their application to predictive tasks in relational
databases remains largely unexplored. In this work, we address the notion that
LLMs cannot yield satisfactory results on relational databases due to their
interconnected tables, complex relationships, and heterogeneous data types.
Using the recently introduced RelBench benchmark, we demonstrate that even a
straightforward application of LLMs achieves competitive performance on these
tasks. These findings establish LLMs as a promising new baseline for ML on
relational databases and encourage further research in this direction.",2024-11-18,"Marek Wydmuch, Łukasz Borchmann, Filip Graliński",http://arxiv.org/pdf/2411.11829v1,cs.CL
CNMBERT: A Model for Converting Hanyu Pinyin Abbreviations to Chinese Characters,"The task of converting Hanyu Pinyin abbreviations to Chinese characters is a
significant branch within the domain of Chinese Spelling Correction (CSC). It
plays an important role in many downstream applications such as named entity
recognition and sentiment analysis. This task typically involves text-length
alignment and seems easy to solve; however, due to the limited information
content in pinyin abbreviations, achieving accurate conversion is challenging.
In this paper, we treat this as a fill-mask task and propose CNMBERT, which
stands for zh-CN Pinyin Multi-mask BERT Model, as a solution to this issue. By
introducing a multi-mask strategy and Mixture of Experts (MoE) layers, CNMBERT
outperforms fine-tuned GPT models and ChatGPT-4o with a 61.53% MRR score and
51.86% accuracy on a 10,373-sample test dataset.",2024-11-18,"Zishuo Feng, Feng Cao",http://arxiv.org/pdf/2411.11770v4,cs.CL
Drowning in Documents: Consequences of Scaling Reranker Inference,"Rerankers, typically cross-encoders, are often used to re-score the documents
retrieved by cheaper initial IR systems. This is because, though expensive,
rerankers are assumed to be more effective. We challenge this assumption by
measuring reranker performance for full retrieval, not just re-scoring
first-stage retrieval. Our experiments reveal a surprising trend: the best
existing rerankers provide diminishing returns when scoring progressively more
documents and actually degrade quality beyond a certain limit. In fact, in this
setting, rerankers can frequently assign high scores to documents with no
lexical or semantic overlap with the query. We hope that our findings will spur
future research to improve reranking.",2024-11-18,"Mathew Jacob, Erik Lindgren, Matei Zaharia, Michael Carbin, Omar Khattab, Andrew Drozdov",http://arxiv.org/pdf/2411.11767v1,cs.CL
The Power of Many: Multi-Agent Multimodal Models for Cultural Image Captioning,"Large Multimodal Models (LMMs) exhibit impressive performance across various
multimodal tasks. However, their effectiveness in cross-cultural contexts
remains limited due to the predominantly Western-centric nature of most data
and models. Conversely, multi-agent models have shown significant capability in
solving complex tasks. Our study evaluates the collective performance of LMMs
in a multi-agent interaction setting for the novel task of cultural image
captioning. Our contributions are as follows: (1) We introduce MosAIC, a
Multi-Agent framework to enhance cross-cultural Image Captioning using LMMs
with distinct cultural personas; (2) We provide a dataset of culturally
enriched image captions in English for images from China, India, and Romania
across three datasets: GeoDE, GD-VCR, CVQA; (3) We propose a culture-adaptable
metric for evaluating cultural information within image captions; and (4) We
show that the multi-agent interaction outperforms single-agent models across
different metrics, and offer valuable insights for future research. Our dataset
and models can be accessed at https://github.com/MichiganNLP/MosAIC.",2024-11-18,"Longju Bai, Angana Borah, Oana Ignat, Rada Mihalcea",http://arxiv.org/pdf/2411.11758v1,cs.CL
Advacheck at GenAI Detection Task 1: AI Detection Powered by Domain-Aware Multi-Tasking,"The paper describes a system designed by Advacheck team to recognise
machine-generated and human-written texts in the monolingual subtask of GenAI
Detection Task 1 competition. Our developed system is a multi-task architecture
with shared Transformer Encoder between several classification heads. One head
is responsible for binary classification between human-written and
machine-generated texts, while the other heads are auxiliary multiclass
classifiers for texts of different domains from particular datasets. As
multiclass heads were trained to distinguish the domains presented in the data,
they provide a better understanding of the samples. This approach led us to
achieve the first place in the official ranking with 83.07% macro F1-score on
the test set and bypass the baseline by 10%. We further study obtained system
through ablation, error and representation analyses, finding that multi-task
learning outperforms single-task mode and simultaneous tasks form a cluster
structure in embeddings space.",2024-11-18,"German Gritsai, Anastasia Voznyuk, Ildar Khabutdinov, Andrey Grabovoy",http://arxiv.org/pdf/2411.11736v1,cs.CL
Moral Persuasion in Large Language Models: Evaluating Susceptibility and Ethical Alignment,"We explore how large language models (LLMs) can be influenced by prompting
them to alter their initial decisions and align them with established ethical
frameworks. Our study is based on two experiments designed to assess the
susceptibility of LLMs to moral persuasion. In the first experiment, we examine
the susceptibility to moral ambiguity by evaluating a Base Agent LLM on morally
ambiguous scenarios and observing how a Persuader Agent attempts to modify the
Base Agent's initial decisions. The second experiment evaluates the
susceptibility of LLMs to align with predefined ethical frameworks by prompting
them to adopt specific value alignments rooted in established philosophical
theories. The results demonstrate that LLMs can indeed be persuaded in morally
charged scenarios, with the success of persuasion depending on factors such as
the model used, the complexity of the scenario, and the conversation length.
Notably, LLMs of distinct sizes but from the same company produced markedly
different outcomes, highlighting the variability in their susceptibility to
ethical persuasion.",2024-11-18,"Allison Huang, Yulu Niki Pi, Carlos Mougan",http://arxiv.org/pdf/2411.11731v1,cs.CL
FedCoLLM: A Parameter-Efficient Federated Co-tuning Framework for Large and Small Language Models,"By adapting Large Language Models (LLMs) to domain-specific tasks or
enriching them with domain-specific knowledge, we can fully harness the
capabilities of LLMs. Nonetheless, a gap persists in achieving simultaneous
mutual enhancement between the server's LLM and the downstream clients' Small
Language Models (SLMs). To address this, we propose FedCoLLM, a novel and
parameter-efficient federated framework designed for co-tuning LLMs and SLMs.
This approach is aimed at adaptively transferring server-side LLMs knowledge to
clients' SLMs while simultaneously enriching the LLMs with domain insights from
the clients. To accomplish this, FedCoLLM utilizes lightweight adapters in
conjunction with SLMs, facilitating knowledge exchange between server and
clients in a manner that respects data privacy while also minimizing
computational and communication overhead. Our evaluation of FedCoLLM, utilizing
various public LLMs and SLMs across a range of NLP text generation tasks,
reveals that the performance of clients' SLMs experiences notable improvements
with the assistance of the LLMs. Simultaneously, the LLMs enhanced via FedCoLLM
achieves comparable performance to that obtained through direct fine-tuning on
clients' data.",2024-11-18,"Tao Fan, Yan Kang, Guoqiang Ma, Lixin Fan, Kai Chen, Qiang Yang",http://arxiv.org/pdf/2411.11707v1,cs.CL
Enhancing LLM Reasoning with Reward-guided Tree Search,"Recently, test-time scaling has garnered significant attention from the
research community, largely due to the substantial advancements of the o1 model
released by OpenAI. By allocating more computational resources during the
inference phase, large language models~(LLMs) can extensively explore the
solution space by generating more thought tokens or diverse solutions, thereby
producing more accurate responses. However, developing an o1-like reasoning
approach is challenging, and researchers have been making various attempts to
advance this open area of research. In this paper, we present a preliminary
exploration into enhancing the reasoning abilities of LLMs through
reward-guided tree search algorithms. This framework is implemented by
integrating the policy model, reward model, and search algorithm. It is
primarily constructed around a tree search algorithm, where the policy model
navigates a dynamically expanding tree guided by a specially trained reward
model. The implemented framework is denoted as \textbf{STILL-1}. We thoroughly
explore various design considerations necessary for implementing this framework
and provide a detailed report of the technical aspects. To assess the
effectiveness of our approach, we focus on mathematical reasoning tasks and
conduct extensive evaluations on four challenging datasets, significantly
enhancing the reasoning abilities of LLMs.",2024-11-18,"Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen, Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haoxiang Sun, Jia Deng, Wayne Xin Zhao, Zheng Liu, Dong Yan, Jian Xie, Zhongyuan Wang, Ji-Rong Wen",http://arxiv.org/pdf/2411.11694v4,cs.CL
Chapter 7 Review of Data-Driven Generative AI Models for Knowledge Extraction from Scientific Literature in Healthcare,"This review examines the development of abstractive NLP-based text
summarization approaches and compares them to existing techniques for
extractive summarization. A brief history of text summarization from the 1950s
to the introduction of pre-trained language models such as Bidirectional
Encoder Representations from Transformer (BERT) and Generative Pre-training
Transformers (GPT) are presented. In total, 60 studies were identified in
PubMed and Web of Science, of which 29 were excluded and 24 were read and
evaluated for eligibility, resulting in the use of seven studies for further
analysis. This chapter also includes a section with examples including an
example of a comparison between GPT-3 and state-of-the-art GPT-4 solutions in
scientific text summarisation. Natural language processing has not yet reached
its full potential in the generation of brief textual summaries. As there are
acknowledged concerns that must be addressed, we can expect gradual
introduction of such models in practise.",2024-11-18,"Leon Kopitar, Primoz Kocbek, Lucija Gosak, Gregor Stiglic",http://arxiv.org/pdf/2411.11635v1,cs.CL
METEOR: Evolutionary Journey of Large Language Models from Guidance to Self-Growth,"Model evolution enables learning from feedback to refine experiences and
update skills, transforming models from having no domain knowledge to becoming
domain experts. However, there is currently no unified and effective method for
guiding this evolutionary process. To address this gap, we propose the Meteor
method, which includes three training phases: weak-to-strong data distillation,
iterative training, and self-evolution strategies. Each phase maximizes the
model's inherent domain capabilities, allowing it to autonomously refine its
domain knowledge and enhance performance. Experiments demonstrate that our
approach significantly improves accuracy, completeness, relevance, coherence,
and reliability across domain-specific tasks.",2024-11-18,"Jiawei Li, Xiaoang Xu, Yang Gao",http://arxiv.org/pdf/2411.11933v2,cs.CL
Federated Incremental Named Entity Recognition,"Federated Named Entity Recognition (FNER) boosts model training within each
local client by aggregating the model updates of decentralized local clients,
without sharing their private data. However, existing FNER methods assume fixed
entity types and local clients in advance, leading to their ineffectiveness in
practical applications. In a more realistic scenario, local clients receive new
entity types continuously, while new local clients collecting novel data may
irregularly join the global FNER training. This challenging setup, referred to
here as Federated Incremental NER, renders the global model suffering from
heterogeneous forgetting of old entity types from both intra-client and
inter-client perspectives. To overcome these challenges, we propose a
Local-Global Forgetting Defense (LGFD) model. Specifically, to address
intra-client forgetting, we develop a structural knowledge distillation loss to
retain the latent space's feature structure and a pseudo-label-guided
inter-type contrastive loss to enhance discriminative capability over different
entity types, effectively preserving previously learned knowledge within local
clients. To tackle inter-client forgetting, we propose a task switching monitor
that can automatically identify new entity types under privacy protection and
store the latest old global model for knowledge distillation and
pseudo-labeling. Experiments demonstrate significant improvement of our LGFD
model over comparison methods.",2024-11-18,"Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dong Yu",http://arxiv.org/pdf/2411.11623v3,cs.CL
Reviving Dormant Memories: Investigating Catastrophic Forgetting in Language Models through Rationale-Guidance Difficulty,"Although substantial efforts have been made to mitigate catastrophic
forgetting in continual learning, the intrinsic mechanisms are not well
understood. In this paper, we discover that when a forgetting model passively
receives an externally provided partial appropriate rationale, its performance
on the forgotten task can be restored. Furthermore, by simply adding a
task-agnostic prefix to the original instruction, the forgetting model can
actively generate an appropriate rationale to reach the correct answer. These
findings suggest that the model does not actually ``forget'' the task
knowledge; instead, the degraded performance can be attributed to the failure
of the original instructions in guiding the model to generate the appropriate
rationales. Based on this insight, we propose the Rationale-Guidance Difficulty
metric to evaluate how effectively a given instruction guides the model in
generating appropriate rationales. We apply this metric to optimize the
allocation of replay data in replay-based continual learning algorithm.
Experimental results demonstrate that our data allocation method effectively
mitigates catastrophic forgetting and maintains better model plasticity
simultaneously across models.",2024-11-18,"Huashan Sun, Yang Gao",http://arxiv.org/pdf/2411.11932v1,cs.CL
Learning to Ask: Conversational Product Search via Representation Learning,"Online shopping platforms, such as Amazon and AliExpress, are increasingly
prevalent in society, helping customers purchase products conveniently. With
recent progress in natural language processing, researchers and practitioners
shift their focus from traditional product search to conversational product
search. Conversational product search enables user-machine conversations and
through them collects explicit user feedback that allows to actively clarify
the users' product preferences. Therefore, prospective research on an
intelligent shopping assistant via conversations is indispensable. Existing
publications on conversational product search either model conversations
independently from users, queries, and products or lead to a vocabulary
mismatch. In this work, we propose a new conversational product search model,
ConvPS, to assist users in locating desirable items. The model is first trained
to jointly learn the semantic representations of user, query, item, and
conversation via a unified generative framework. After learning these
representations, they are integrated to retrieve the target items in the latent
semantic space. Meanwhile, we propose a set of greedy and explore-exploit
strategies to learn to ask the user a sequence of high-performance questions
for conversations. Our proposed ConvPS model can naturally integrate the
representation learning of the user, query, item, and conversation into a
unified generative framework, which provides a promising avenue for
constructing accurate and robust conversational product search systems that are
flexible and adaptive. Experimental results demonstrate that our ConvPS model
significantly outperforms state-of-the-art baselines.",2024-11-18,"Jie Zou, Jimmy Xiangji Huang, Zhaochun Ren, Evangelos Kanoulas",http://arxiv.org/pdf/2411.14466v1,cs.CL
OASIS: Open Agent Social Interaction Simulations with One Million Agents,"There has been a growing interest in enhancing rule-based agent-based models
(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large
language model (LLM) agents, thereby allowing for a more nuanced study of
complex systems. As a result, several LLM-based ABMs have been proposed in the
past year. While they hold promise, each simulator is specifically designed to
study a particular scenario, making it time-consuming and resource-intensive to
explore other phenomena using the same ABM. Additionally, these models simulate
only a limited number of agents, whereas real-world social media platforms
involve millions of users. To this end, we propose OASIS, a generalizable and
scalable social media simulator. OASIS is designed based on real-world social
media platforms, incorporating dynamically updated environments (i.e., dynamic
social networks and post information), diverse action spaces (i.e., following,
commenting), and recommendation systems (i.e., interest-based and
hot-score-based). Additionally, OASIS supports large-scale user simulations,
capable of modeling up to one million users. With these features, OASIS can be
easily extended to different social media platforms to study large-scale group
phenomena and behaviors. We replicate various social phenomena, including
information spreading, group polarization, and herd effects across X and Reddit
platforms. Moreover, we provide observations of social phenomena at different
agent group scales. We observe that the larger agent group scale leads to more
enhanced group dynamics and more diverse and helpful agents' opinions. These
findings demonstrate OASIS's potential as a powerful tool for studying complex
systems in digital environments.",2024-11-18,"Ziyi Yang, Zaibin Zhang, Zirui Zheng, Yuxian Jiang, Ziyue Gan, Zhiyu Wang, Zijian Ling, Jinsong Chen, Martz Ma, Bowen Dong, Prateek Gupta, Shuyue Hu, Zhenfei Yin, Guohao Li, Xu Jia, Lijun Wang, Bernard Ghanem, Huchuan Lu, Chaochao Lu, Wanli Ouyang, Yu Qiao, Philip Torr, Jing Shao",http://arxiv.org/pdf/2411.11581v5,cs.CL
Testing Uncertainty of Large Language Models for Physics Knowledge and Reasoning,"Large Language Models (LLMs) have gained significant popularity in recent
years for their ability to answer questions in various fields. However, these
models have a tendency to ""hallucinate"" their responses, making it challenging
to evaluate their performance. A major challenge is determining how to assess
the certainty of a model's predictions and how it correlates with accuracy. In
this work, we introduce an analysis for evaluating the performance of popular
open-source LLMs, as well as gpt-3.5 Turbo, on multiple choice physics
questionnaires. We focus on the relationship between answer accuracy and
variability in topics related to physics. Our findings suggest that most models
provide accurate replies in cases where they are certain, but this is by far
not a general behavior. The relationship between accuracy and uncertainty
exposes a broad horizontal bell-shaped distribution. We report how the
asymmetry between accuracy and uncertainty intensifies as the questions demand
more logical reasoning of the LLM agent, while the same relationship remains
sharp for knowledge retrieval tasks.",2024-11-18,"Elizaveta Reganova, Peter Steinbach",http://arxiv.org/pdf/2411.14465v1,cs.CL
Addressing Hallucinations in Language Models with Knowledge Graph Embeddings as an Additional Modality,"In this paper we present an approach to reduce hallucinations in Large
Language Models (LLMs) by incorporating Knowledge Graphs (KGs) as an additional
modality. Our method involves transforming input text into a set of KG
embeddings and using an adapter to integrate these embeddings into the language
model space, without relying on external retrieval processes.
  To facilitate this, we created WikiEntities, a dataset containing over 3
million Wikipedia texts annotated with entities from Wikidata and their
corresponding embeddings from PyTorch-BigGraph. This dataset serves as a
valuable resource for training Entity Linking models and adapting the described
method to various LLMs using specialized adapters.
  Our method does not require fine-tuning of the language models themselves;
instead, we only train the adapter. This ensures that the model's performance
on other tasks is not affected. We trained an adapter for the Mistral 7B, LLaMA
2-7B (chat), and LLaMA 3-8B (instruct) models using this dataset and
demonstrated that our approach improves performance on the HaluEval, True-False
benchmarks and FEVER dataset. The results indicate that incorporating KGs as a
new modality can effectively reduce hallucinations and improve the factual
accuracy of language models, all without the need for external retrieval.",2024-11-18,"Viktoriia Chekalina, Anton Razzhigaev, Elizaveta Goncharova, Andrey Kuznetsov",http://arxiv.org/pdf/2411.11531v2,cs.CL
"Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering","The evolution of machine learning has increasingly prioritized the
development of powerful models and more scalable supervision signals. However,
the emergence of foundation models presents significant challenges in providing
effective supervision signals necessary for further enhancing their
capabilities. Consequently, there is an urgent need to explore novel
supervision signals and technical approaches. In this paper, we propose
verifier engineering, a novel post-training paradigm specifically designed for
the era of foundation models. The core of verifier engineering involves
leveraging a suite of automated verifiers to perform verification tasks and
deliver meaningful feedback to foundation models. We systematically categorize
the verifier engineering process into three essential stages: search, verify,
and feedback, and provide a comprehensive review of state-of-the-art research
developments within each stage. We believe that verifier engineering
constitutes a fundamental pathway toward achieving Artificial General
Intelligence.",2024-11-18,"Xinyan Guan, Yanjiang Liu, Xinyu Lu, Boxi Cao, Ben He, Xianpei Han, Le Sun, Jie Lou, Bowen Yu, Yaojie Lu, Hongyu Lin",http://arxiv.org/pdf/2411.11504v1,cs.CL
Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models,"Recent advances in Large Vision-Language Models (LVLMs) have showcased strong
reasoning abilities across multiple modalities, achieving significant
breakthroughs in various real-world applications. Despite this great success,
the safety guardrail of LVLMs may not cover the unforeseen domains introduced
by the visual modality. Existing studies primarily focus on eliciting LVLMs to
generate harmful responses via carefully crafted image-based jailbreaks
designed to bypass alignment defenses. In this study, we reveal that a safe
image can be exploited to achieve the same jailbreak consequence when combined
with additional safe images and prompts. This stems from two fundamental
properties of LVLMs: universal reasoning capabilities and safety snowball
effect. Building on these insights, we propose Safety Snowball Agent (SSA), a
novel agent-based framework leveraging agents' autonomous and tool-using
abilities to jailbreak LVLMs. SSA operates through two principal stages: (1)
initial response generation, where tools generate or retrieve jailbreak images
based on potential harmful intents, and (2) harmful snowballing, where refined
subsequent prompts induce progressively harmful outputs. Our experiments
demonstrate that \ours can use nearly any image to induce LVLMs to produce
unsafe content, achieving high success jailbreaking rates against the latest
LVLMs. Unlike prior works that exploit alignment flaws, \ours leverages the
inherent properties of LVLMs, presenting a profound challenge for enforcing
safety in generative multimodal systems. Our code is avaliable at
\url{https://github.com/gzcch/Safety_Snowball_Agent}.",2024-11-18,"Chenhang Cui, Gelei Deng, An Zhang, Jingnan Zheng, Yicong Li, Lianli Gao, Tianwei Zhang, Tat-Seng Chua",http://arxiv.org/pdf/2411.11496v3,cs.CL
Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts,"The recent progress in Vision-Language Models (VLMs) has broadened the scope
of multimodal applications. However, evaluations often remain limited to
functional tasks, neglecting abstract dimensions such as personality traits and
human values. To address this gap, we introduce Value-Spectrum, a novel Visual
Question Answering (VQA) benchmark aimed at assessing VLMs based on Schwartz's
value dimensions that capture core values guiding people's preferences and
actions. We designed a VLM agent pipeline to simulate video browsing and
constructed a vector database comprising over 50,000 short videos from TikTok,
YouTube Shorts, and Instagram Reels. These videos span multiple months and
cover diverse topics, including family, health, hobbies, society, technology,
etc. Benchmarking on Value-Spectrum highlights notable variations in how VLMs
handle value-oriented content. Beyond identifying VLMs' intrinsic preferences,
we also explored the ability of VLM agents to adopt specific personas when
explicitly prompted, revealing insights into the adaptability of the model in
role-playing scenarios. These findings highlight the potential of
Value-Spectrum as a comprehensive evaluation set for tracking VLM alignments in
value-based tasks and abilities to simulate diverse personas.",2024-11-18,"Jingxuan Li, Yuning Yang, Shengqi Yang, Linfan Zhang, Ying Nian Wu",http://arxiv.org/pdf/2411.11479v2,cs.CL
Re-examining learning linear functions in context,"In-context learning (ICL) has emerged as a powerful paradigm for easily
adapting Large Language Models (LLMs) to various tasks. However, our
understanding of how ICL works remains limited. We explore a simple model of
ICL in a controlled setup with synthetic training data to investigate ICL of
univariate linear functions. We experiment with a range of GPT-2-like
transformer models trained from scratch. Our findings challenge the prevailing
narrative that transformers adopt algorithmic approaches like linear regression
to learn a linear function in-context. These models fail to generalize beyond
their training distribution, highlighting fundamental limitations in their
capacity to infer abstract task structures. Our experiments lead us to propose
a mathematically precise hypothesis of what the model might be learning.",2024-11-18,"Omar Naim, Guilhem Fouilhé, Nicholas Asher",http://arxiv.org/pdf/2411.11465v3,cs.CL
Causal Effect of Group Diversity on Redundancy and Coverage in Peer-Reviewing,"A large host of scientific journals and conferences solicit peer reviews from
multiple reviewers for the same submission, aiming to gather a broader range of
perspectives and mitigate individual biases. In this work, we reflect on the
role of diversity in the slate of reviewers assigned to evaluate a submitted
paper as a factor in diversifying perspectives and improving the utility of the
peer-review process. We propose two measures for assessing review utility:
review coverage -- reviews should cover most contents of the paper -- and
review redundancy -- reviews should add information not already present in
other reviews. We hypothesize that reviews from diverse reviewers will exhibit
high coverage and low redundancy. We conduct a causal study of different
measures of reviewer diversity on review coverage and redundancy using
observational data from a peer-reviewed conference with approximately 5,000
submitted papers. Our study reveals disparate effects of different diversity
measures on review coverage and redundancy. Our study finds that assigning a
group of reviewers that are topically diverse, have different seniority levels,
or have distinct publication networks leads to broader coverage of the paper or
review criteria, but we find no evidence of an increase in coverage for
reviewer slates with reviewers from diverse organizations or geographical
locations. Reviewers from different organizations, seniority levels, topics, or
publications networks (all except geographical diversity) lead to a decrease in
redundancy in reviews. Furthermore, publication network-based diversity alone
also helps bring in varying perspectives (that is, low redundancy), even within
specific review criteria. Our study adopts a group decision-making perspective
for reviewer assignments in peer review and suggests dimensions of diversity
that can help guide the reviewer assignment process.",2024-11-18,"Navita Goyal, Ivan Stelmakh, Nihar Shah, Hal Daumé III",http://arxiv.org/pdf/2411.11437v1,cs.CL
Membership Inference Attack against Long-Context Large Language Models,"Recent advances in Large Language Models (LLMs) have enabled them to overcome
their context window limitations, and demonstrate exceptional retrieval and
reasoning capacities on longer context. Quesion-answering systems augmented
with Long-Context Language Models (LCLMs) can automatically search massive
external data and incorporate it into their contexts, enabling faithful
predictions and reducing issues such as hallucinations and knowledge staleness.
Existing studies targeting LCLMs mainly concentrate on addressing the so-called
lost-in-the-middle problem or improving the inference effiencicy, leaving their
privacy risks largely unexplored. In this paper, we aim to bridge this gap and
argue that integrating all information into the long context makes it a
repository of sensitive information, which often contains private data such as
medical records or personal identities. We further investigate the membership
privacy within LCLMs external context, with the aim of determining whether a
given document or sequence is included in the LCLMs context. Our basic idea is
that if a document lies in the context, it will exhibit a low generation loss
or a high degree of semantic similarity to the contents generated by LCLMs. We
for the first time propose six membership inference attack (MIA) strategies
tailored for LCLMs and conduct extensive experiments on various popular models.
Empirical results demonstrate that our attacks can accurately infer membership
status in most cases, e.g., 90.66% attack F1-score on Multi-document QA
datasets with LongChat-7b-v1.5-32k, highlighting significant risks of
membership leakage within LCLMs input contexts. Furthermore, we examine the
underlying reasons why LCLMs are susceptible to revealing such membership
information.",2024-11-18,"Zixiong Wang, Gaoyang Liu, Yang Yang, Chen Wang",http://arxiv.org/pdf/2411.11424v1,cs.CL
Rethinking Thinking Tokens: Understanding Why They Underperform in Practice,"Thinking Tokens (TT) have been proposed as an unsupervised method to
facilitate reasoning in language models. However, despite their conceptual
appeal, our findings show that TTs marginally improves performance and
consistently underperforms compared to Chain-of-Thought (CoT) reasoning across
multiple benchmarks. We hypothesize that this underperformance stems from the
reliance on a single embedding for TTs, which results in inconsistent learning
signals and introduces noisy gradients. This paper provides a comprehensive
empirical analysis to validate this hypothesis and discusses the implications
for future research on unsupervised reasoning in LLMs.",2024-11-18,"Sreeram Vennam, David Valente, David Herel, Ponnurangam Kumaraguru",http://arxiv.org/pdf/2411.11371v1,cs.CL
MAIRA-Seg: Enhancing Radiology Report Generation with Segmentation-Aware Multimodal Large Language Models,"There is growing interest in applying AI to radiology report generation,
particularly for chest X-rays (CXRs). This paper investigates whether
incorporating pixel-level information through segmentation masks can improve
fine-grained image interpretation of multimodal large language models (MLLMs)
for radiology report generation. We introduce MAIRA-Seg, a segmentation-aware
MLLM framework designed to utilize semantic segmentation masks alongside CXRs
for generating radiology reports. We train expert segmentation models to obtain
mask pseudolabels for radiology-specific structures in CXRs. Subsequently,
building on the architectures of MAIRA, a CXR-specialised model for report
generation, we integrate a trainable segmentation tokens extractor that
leverages these mask pseudolabels, and employ mask-aware prompting to generate
draft radiology reports. Our experiments on the publicly available MIMIC-CXR
dataset show that MAIRA-Seg outperforms non-segmentation baselines. We also
investigate set-of-marks prompting with MAIRA and find that MAIRA-Seg
consistently demonstrates comparable or superior performance. The results
confirm that using segmentation masks enhances the nuanced reasoning of MLLMs,
potentially contributing to better clinical outcomes.",2024-11-18,"Harshita Sharma, Valentina Salvatelli, Shaury Srivastav, Kenza Bouzid, Shruthi Bannur, Daniel C. Castro, Maximilian Ilse, Sam Bond-Taylor, Mercy Prasanna Ranjit, Fabian Falck, Fernando Pérez-García, Anton Schwaighofer, Hannah Richardson, Maria Teodora Wetscherek, Stephanie L. Hyland, Javier Alvarez-Valle",http://arxiv.org/pdf/2411.11362v1,cs.CL
CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization,"Recent studies reveal that Large Language Models (LLMs) are susceptible to
backdoor attacks, where adversaries embed hidden triggers that manipulate model
responses. Existing backdoor defense methods are primarily designed for vision
or classification tasks, and are thus ineffective for text generation tasks,
leaving LLMs vulnerable. We introduce Internal Consistency Regularization
(CROW), a novel defense using consistency regularization finetuning to address
layer-wise inconsistencies caused by backdoor triggers. CROW leverages the
intuition that clean models exhibit smooth, consistent transitions in hidden
representations across layers, whereas backdoored models show noticeable
fluctuation when triggered. By enforcing internal consistency through
adversarial perturbations and regularization, CROW neutralizes backdoor effects
without requiring clean reference models or prior trigger knowledge, relying
only on a small set of clean data. This makes it practical for deployment
across various LLM architectures. Experimental results demonstrate that CROW
consistently achieves a significant reductions in attack success rates across
diverse backdoor strategies and tasks, including negative sentiment, targeted
refusal, and code injection, on models such as Llama-2 (7B, 13B), CodeLlama
(7B, 13B) and Mistral-7B, while preserving the model's generative capabilities.",2024-11-18,"Nay Myat Min, Long H. Pham, Yige Li, Jun Sun",http://arxiv.org/pdf/2411.12768v1,cs.CL
Mitigating Knowledge Conflicts in Language Model-Driven Question Answering,"In the context of knowledge-driven seq-to-seq generation tasks, such as
document-based question answering and document summarization systems, two
fundamental knowledge sources play crucial roles: the inherent knowledge
embedded within model parameters and the external knowledge obtained through
context. Recent studies revealed a significant challenge: when there exists a
misalignment between the model's inherent knowledge and the ground truth
answers in training data, the system may exhibit problematic behaviors during
inference, such as ignoring input context, or generating unfaithful content.
Our investigation proposes a strategy to minimize hallucination by building
explicit connection between source inputs and generated outputs. We
specifically target a common hallucination pattern in question answering,
examining how the correspondence between entities and their contexts during
model training influences the system's performance at inference time.",2024-11-18,"Han Cao, Zhaoyang Zhang, Xiangtian Li, Chufan Wu, Hansong Zhang, Wenqing Zhang",http://arxiv.org/pdf/2411.11344v3,cs.CL
PerfCodeGen: Improving Performance of LLM Generated Code with Execution Feedback,"Large Language Models (LLMs) are widely adopted for assisting in software
development tasks, yet their performance evaluations have narrowly focused on
the functional correctness of generated code. Human programmers, however,
require LLM-generated code to be not only correct but also optimally efficient.
We propose PerfCodeGen, a training-free framework that enhances the performance
of LLM-generated code by incorporating feedback based on runtime during test
case execution into the self-refinement iterations. With PerfCodeGen, we
achieve speedups for a significantly higher proportion of problems compared to
using the base LLM with sophisticated prompting techniques. Applied to open
language models like Phi-3-mini, PerfCodeGen achieves runtime efficiency
comparable to prompting powerful closed models like GPT-4. We achieve
state-of-the-art runtime efficiency on benchmarks such as HumanEval, MBPP, and
APPS, frequently surpassing the ground truth reference solutions with
PerfCodeGen using GPT-3.5 and GPT-4. Additionally, we demonstrate the
effectiveness of our approach in enhancing code quality across a range of open
LLMs of varying sizes including Phi-3-mini, Llama 3 8B, Mixtral 8x7B, Command
R, and Llama 3 70B.",2024-11-18,"Yun Peng, Akhilesh Deepak Gotmare, Michael Lyu, Caiming Xiong, Silvio Savarese, Doyen Sahoo",http://arxiv.org/pdf/2412.03578v1,cs.CL
Improved GUI Grounding via Iterative Narrowing,"Graphical User Interface (GUI) grounding plays a crucial role in enhancing
the capabilities of Vision-Language Model (VLM) agents. While general VLMs,
such as GPT-4V, demonstrate strong performance across various tasks, their
proficiency in GUI grounding remains suboptimal. Recent studies have focused on
fine-tuning these models specifically for zero-shot GUI grounding, yielding
significant improvements over baseline performance. We introduce a visual
prompting framework that employs an iterative narrowing mechanism to further
improve the performance of both general and fine-tuned models in GUI grounding.
For evaluation, we tested our method on a comprehensive benchmark comprising
various UI platforms and provided the code to reproduce our results.",2024-11-18,Anthony Nguyen,http://arxiv.org/pdf/2411.13591v5,cs.CL
Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation,"Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of tasks and domains. However, their performance in low-resource
language translation, particularly when translating into these languages,
remains underexplored. This gap poses significant challenges, as linguistic
barriers hinder the cultural preservation and development of minority
communities. To address this issue, this paper introduces a novel
retrieval-based method that enhances translation quality for low-resource
languages by focusing on key terms, which involves translating keywords and
retrieving corresponding examples from existing data. To evaluate the
effectiveness of this method, we conducted experiments translating from English
into three low-resource languages: Cherokee, a critically endangered indigenous
language of North America; Tibetan, a historically and culturally significant
language in Asia; and Manchu, a language with few remaining speakers. Our
comparison with the zero-shot performance of GPT-4o and LLaMA 3.1 405B,
highlights the significant challenges these models face when translating into
low-resource languages. In contrast, our retrieval-based method shows promise
in improving both word-level accuracy and overall semantic understanding by
leveraging existing resources more effectively.",2024-11-18,"Peng Shu, Junhao Chen, Zhengliang Liu, Hui Wang, Zihao Wu, Tianyang Zhong, Yiwei Li, Huaqin Zhao, Hanqi Jiang, Yi Pan, Yifan Zhou, Constance Owl, Xiaoming Zhai, Ninghao Liu, Claudio Saunt, Tianming Liu",http://arxiv.org/pdf/2411.11295v1,cs.CL
"LP Data Pipeline: Lightweight, Purpose-driven Data Pipeline for Large Language Models","Creating high-quality, large-scale datasets for large language models (LLMs)
often relies on resource-intensive, GPU-accelerated models for quality
filtering, making the process time-consuming and costly. This dependence on
GPUs limits accessibility for organizations lacking significant computational
infrastructure. To address this issue, we introduce the Lightweight,
Purpose-driven (LP) Data Pipeline, a framework that operates entirely on CPUs
to streamline the processes of dataset extraction, filtering, and curation.
Based on our four core principles, the LP Data Pipeline significantly reduces
preparation time and cost while maintaining high data quality. Importantly, our
pipeline enables the creation of purpose-driven datasets tailored to specific
domains and languages, enhancing the applicability of LLMs in specialized
contexts. We anticipate that our pipeline will lower the barriers to LLM
development, enabling a wide range of organizations to access LLMs more easily.",2024-11-18,"Yungi Kim, Hyunsoo Ha, Seonghoon Yang, Sukyung Lee, Jihoo Kim, Chanjun Park",http://arxiv.org/pdf/2411.11289v1,cs.CL
VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs,"As demonstrated by the proprietary Large Language Models (LLMs) such as GPT
and Claude series, LLMs have the potential to achieve remarkable proficiency
across a wide range of domains, including law, medicine, finance, science,
code, etc., all within a single model. These capabilities are further augmented
during the Supervised Fine-Tuning (SFT) phase. Despite their potential,
existing work mainly focuses on domain-specific enhancements during
fine-tuning, the challenge of which lies in catastrophic forgetting of
knowledge across other domains. In this study, we introduce **VersaTune**, a
novel data composition framework designed for enhancing LLMs' overall
multi-domain capabilities during training. We begin with detecting the
distribution of domain-specific knowledge within the base model, followed by
the training data composition that aligns with the model's existing knowledge
distribution. During the subsequent training process, domain weights are
dynamically adjusted based on their learnable potential and forgetting degree.
Experimental results indicate that VersaTune is effective in multi-domain
fostering, with an improvement of 35.21\% in the overall multi-ability
performances compared to uniform domain weights. Furthermore, we find that
Qwen-2.5-32B + VersaTune even surpasses frontier models, including GPT-4o,
Claude3.5-Sonnet and DeepSeek-V3 by 0.86\%, 4.76\% and 4.60\%. Additionally, in
scenarios where flexible expansion of a specific domain is required, VersaTune
reduces the performance degradation in other domains by 38.77\%, while
preserving the training efficacy of the target domain.",2024-11-18,"Keer Lu, Keshi Zhao, Zhuoran Zhang, Zheng Liang, Da Pan, Shusen Zhang, Xin Wu, Guosheng Dong, Bin Cui, Tengjiao Wang, Wentao Zhang",http://arxiv.org/pdf/2411.11266v5,cs.CL
Large corpora and large language models: a replicable method for automating grammatical annotation,"Much linguistic research relies on annotated datasets of features extracted
from text corpora, but the rapid quantitative growth of these corpora has
created practical difficulties for linguists to manually annotate large data
samples. In this paper, we present a replicable, supervised method that
leverages large language models for assisting the linguist in grammatical
annotation through prompt engineering, training, and evaluation. We introduce a
methodological pipeline applied to the case study of formal variation in the
English evaluative verb construction 'consider X (as) (to be) Y', based on the
large language model Claude 3.5 Sonnet and corpus data from Davies' NOW and
EnTenTen21 (SketchEngine). Overall, we reach a model accuracy of over 90% on
our held-out test samples with only a small amount of training data, validating
the method for the annotation of very large quantities of tokens of the
construction in the future. We discuss the generalisability of our results for
a wider range of case studies of grammatical constructions and grammatical
variation and change, underlining the value of AI copilots as tools for future
linguistic research, notwithstanding some important caveats.",2024-11-18,"Cameron Morin, Matti Marttinen Larsson",http://arxiv.org/pdf/2411.11260v2,cs.CL
Understanding Student Sentiment on Mental Health Support in Colleges Using Large Language Models,"Mental health support in colleges is vital in educating students by offering
counseling services and organizing supportive events. However, evaluating its
effectiveness faces challenges like data collection difficulties and lack of
standardized metrics, limiting research scope. Student feedback is crucial for
evaluation but often relies on qualitative analysis without systematic
investigation using advanced machine learning methods. This paper uses public
Student Voice Survey data to analyze student sentiments on mental health
support with large language models (LLMs). We created a sentiment analysis
dataset, SMILE-College, with human-machine collaboration. The investigation of
both traditional machine learning methods and state-of-the-art LLMs showed the
best performance of GPT-3.5 and BERT on this new dataset. The analysis
highlights challenges in accurately predicting response sentiments and offers
practical insights on how LLMs can enhance mental health-related research and
improve college mental health services. This data-driven approach will
facilitate efficient and informed mental health support evaluation, management,
and decision-making.",2024-11-18,"Palak Sood, Chengyang He, Divyanshu Gupta, Yue Ning, Ping Wang",http://arxiv.org/pdf/2412.04326v1,cs.CL
Suicide Risk Assessment on Social Media with Semi-Supervised Learning,"With social media communities increasingly becoming places where suicidal
individuals post and congregate, natural language processing presents an
exciting avenue for the development of automated suicide risk assessment
systems. However, past efforts suffer from a lack of labeled data and class
imbalances within the available labeled data. To accommodate this task's
imperfect data landscape, we propose a semi-supervised framework that leverages
labeled (n=500) and unlabeled (n=1,500) data and expands upon the self-training
algorithm with a novel pseudo-label acquisition process designed to handle
imbalanced datasets. To further ensure pseudo-label quality, we manually verify
a subset of the pseudo-labeled data that was not predicted unanimously across
multiple trials of pseudo-label generation. We test various models to serve as
the backbone for this framework, ultimately deciding that RoBERTa performs the
best. Ultimately, by leveraging partially validated pseudo-labeled data in
addition to ground-truth labeled data, we substantially improve our model's
ability to assess suicide risk from social media posts.",2024-11-18,"Max Lovitt, Haotian Ma, Song Wang, Yifan Peng",http://arxiv.org/pdf/2411.12767v2,cs.CL
ZeFaV: Boosting Large Language Models for Zero-shot Fact Verification,"In this paper, we propose ZeFaV - a zero-shot based fact-checking
verification framework to enhance the performance on fact verification task of
large language models by leveraging the in-context learning ability of large
language models to extract the relations among the entities within a claim,
re-organized the information from the evidence in a relationally logical form,
and combine the above information with the original evidence to generate the
context from which our fact-checking model provide verdicts for the input
claims. We conducted empirical experiments to evaluate our approach on two
multi-hop fact-checking datasets including HoVer and FEVEROUS, and achieved
potential results results comparable to other state-of-the-art fact
verification task methods.",2024-11-18,"Son T. Luu, Hiep Nguyen, Trung Vo, Le-Minh Nguyen",http://arxiv.org/pdf/2411.11247v1,cs.CL
MEMO-Bench: A Multiple Benchmark for Text-to-Image and Multimodal Large Language Models on Human Emotion Analysis,"Artificial Intelligence (AI) has demonstrated significant capabilities in
various fields, and in areas such as human-computer interaction (HCI), embodied
intelligence, and the design and animation of virtual digital humans, both
practitioners and users are increasingly concerned with AI's ability to
understand and express emotion. Consequently, the question of whether AI can
accurately interpret human emotions remains a critical challenge. To date, two
primary classes of AI models have been involved in human emotion analysis:
generative models and Multimodal Large Language Models (MLLMs). To assess the
emotional capabilities of these two classes of models, this study introduces
MEMO-Bench, a comprehensive benchmark consisting of 7,145 portraits, each
depicting one of six different emotions, generated by 12 Text-to-Image (T2I)
models. Unlike previous works, MEMO-Bench provides a framework for evaluating
both T2I models and MLLMs in the context of sentiment analysis. Additionally, a
progressive evaluation approach is employed, moving from coarse-grained to
fine-grained metrics, to offer a more detailed and comprehensive assessment of
the sentiment analysis capabilities of MLLMs. The experimental results
demonstrate that existing T2I models are more effective at generating positive
emotions than negative ones. Meanwhile, although MLLMs show a certain degree of
effectiveness in distinguishing and recognizing human emotions, they fall short
of human-level accuracy, particularly in fine-grained emotion analysis. The
MEMO-Bench will be made publicly available to support further research in this
area.",2024-11-18,"Yingjie Zhou, Zicheng Zhang, Jiezhang Cao, Jun Jia, Yanwei Jiang, Farong Wen, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai",http://arxiv.org/pdf/2411.11235v1,cs.CL
ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?,"Effective toxic content detection relies heavily on high-quality and diverse
data, which serve as the foundation for robust content moderation models.
Synthetic data has become a common approach for training models across various
NLP tasks. However, its effectiveness remains uncertain for highly subjective
tasks like hate speech detection, with previous research yielding mixed
results. This study explores the potential of open-source LLMs for harmful data
synthesis, utilizing controlled prompting and supervised fine-tuning techniques
to enhance data quality and diversity. We systematically evaluated 6 open
source LLMs on 5 datasets, assessing their ability to generate diverse,
high-quality harmful data while minimizing hallucination and duplication. Our
results show that Mistral consistently outperforms other open models, and
supervised fine-tuning significantly enhances data reliability and diversity.
We further analyze the trade-offs between prompt-based vs. fine-tuned toxic
data synthesis, discuss real-world deployment challenges, and highlight ethical
considerations. Our findings demonstrate that fine-tuned open source LLMs
provide scalable and cost-effective solutions to augment toxic content
detection datasets, paving the way for more accessible and transparent content
moderation tools.",2024-11-18,"Zheng Hui, Zhaoxiao Guo, Hang Zhao, Juanyong Duan, Lin Ai, Yinheng Li, Julia Hirschberg, Congrui Huang",http://arxiv.org/pdf/2411.15175v4,cs.CL
Capturing Sparks of Abstraction for the ARC Challenge,"Excellent progress has been made recently in solving ARC Challenge problems.
However, it seems that new techniques may be required to push beyond 60%
accuracy. Even commercial Large Language Models (LLMs) struggle to 'understand'
many of the problems (when given the input and output grids), which makes
discovering solutions by LLM-lead program search somewhat futile.
  In this work, LLM 'understanding' is attempted from a stronger starting
position : An LLM is given complete solutions to tasks in code, and then asked
to explain how the task is being solved at various levels of abstraction.
Specifically, the LLM was given code solutions implemented in arc-dsl-llm (an
LLM-legible version of Hodel's arc-dsl to obtain: (a) commented code; (b) code
refactored into reusable functional chunks; (c) problem solution steps; and (d)
high-level problem-solving tactics.
  We demonstrate that 'Sparks of Abstraction' can be extracted from the LLM
output - in a form that could be used in downstream tasks with Local LLMs
eligible to enter the ARC Prize.
  Both the arc-dsl-llm DSL framework (with the re-engineered solutions) and the
Gemini LLM-generated data (along with the generation code) are made Open
Source.",2024-11-17,Martin Andrews,http://arxiv.org/pdf/2411.11206v1,cs.CL
Debiasing Watermarks for Large Language Models via Maximal Coupling,"Watermarking language models is essential for distinguishing between human
and machine-generated text and thus maintaining the integrity and
trustworthiness of digital communication. We present a novel green/red list
watermarking approach that partitions the token set into ``green'' and ``red''
lists, subtly increasing the generation probability for green tokens. To
correct token distribution bias, our method employs maximal coupling, using a
uniform coin flip to decide whether to apply bias correction, with the result
embedded as a pseudorandom watermark signal. Theoretical analysis confirms this
approach's unbiased nature and robust detection capabilities. Experimental
results show that it outperforms prior techniques by preserving text quality
while maintaining high detectability, and it demonstrates resilience to
targeted modifications aimed at improving text quality. This research provides
a promising watermarking solution for language models, balancing effective
detection with minimal impact on text quality.",2024-11-17,"Yangxinyu Xie, Xiang Li, Tanwi Mallick, Weijie J. Su, Ruixun Zhang",http://arxiv.org/pdf/2411.11203v1,cs.CL
LLäMmlein: Compact and Competitive German-Only Language Models from Scratch,"We create two German-only decoder models, LL\""aMmlein 120M and 1B,
transparently from scratch and publish them, along with the training data, for
the German NLP research community to use. The model training involved several
key steps, including extensive data preprocessing, the creation of a custom
German tokenizer, the training itself, as well as the evaluation of the final
models on various benchmarks. Throughout the training process, multiple
checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor
the models' learning dynamics. Compared to state-of-the-art models on the
SuperGLEBer benchmark, both LL\""aMmlein models performed competitively,
consistently matching or surpassing models with similar parameter sizes. The
results show that the models' quality scales with size as expected, but
performance improvements on some tasks plateaued early, offering valuable
insights into resource allocation for future model development.",2024-11-17,"Jan Pfister, Julia Wunderle, Andreas Hotho",http://arxiv.org/pdf/2411.11171v3,cs.CL
SEFD: Semantic-Enhanced Framework for Detecting LLM-Generated Text,"The widespread adoption of large language models (LLMs) has created an urgent
need for robust tools to detect LLM-generated text, especially in light of
\textit{paraphrasing} techniques that often evade existing detection methods.
To address this challenge, we present a novel semantic-enhanced framework for
detecting LLM-generated text (SEFD) that leverages a retrieval-based mechanism
to fully utilize text semantics. Our framework improves upon existing detection
methods by systematically integrating retrieval-based techniques with
traditional detectors, employing a carefully curated retrieval mechanism that
strikes a balance between comprehensive coverage and computational efficiency.
We showcase the effectiveness of our approach in sequential text scenarios
common in real-world applications, such as online forums and Q\&A platforms.
Through comprehensive experiments across various LLM-generated texts and
detection methods, we demonstrate that our framework substantially enhances
detection accuracy in paraphrasing scenarios while maintaining robustness for
standard LLM-generated content.",2024-11-17,"Weiqing He, Bojian Hou, Tianqi Shang, Davoud Ataee Tarzanagh, Qi Long, Li Shen",http://arxiv.org/pdf/2411.12764v1,cs.CL
Leveraging Large Language Models for Generating Labeled Mineral Site Record Linkage Data,"Record linkage integrates diverse data sources by identifying records that
refer to the same entity. In the context of mineral site records, accurate
record linkage is crucial for identifying and mapping mineral deposits.
Properly linking records that refer to the same mineral deposit helps define
the spatial coverage of mineral areas, benefiting resource identification and
site data archiving. Mineral site record linkage falls under the spatial record
linkage category since the records contain information about the physical
locations and non-spatial attributes in a tabular format. The task is
particularly challenging due to the heterogeneity and vast scale of the data.
While prior research employs pre-trained discriminative language models (PLMs)
on spatial entity linkage, they often require substantial amounts of curated
ground-truth data for fine-tuning. Gathering and creating ground truth data is
both time-consuming and costly. Therefore, such approaches are not always
feasible in real-world scenarios where gold-standard data are unavailable.
Although large generative language models (LLMs) have shown promising results
in various natural language processing tasks, including record linkage, their
high inference time and resource demand present challenges. We propose a method
that leverages an LLM to generate training data and fine-tune a PLM to address
the training data gap while preserving the efficiency of PLMs. Our approach
achieves over 45\% improvement in F1 score for record linkage compared to
traditional PLM-based methods using ground truth data while reducing the
inference time by nearly 18 times compared to relying on LLMs. Additionally, we
offer an automated pipeline that eliminates the need for human intervention,
highlighting this approach's potential to overcome record linkage challenges.",2024-11-17,"Jiyoon Pyo, Yao-Yi Chiang",http://arxiv.org/pdf/2412.03575v1,cs.CL
Leveraging AI and NLP for Bank Marketing: A Systematic Review and Gap Analysis,"This paper explores the growing impact of AI and NLP in bank marketing,
highlighting their evolving roles in enhancing marketing strategies, improving
customer engagement, and creating value within this sector. While AI and NLP
have been widely studied in general marketing, there is a notable gap in
understanding their specific applications and potential within the banking
sector. This research addresses this specific gap by providing a systematic
review and strategic analysis of AI and NLP applications in bank marketing,
focusing on their integration across the customer journey and operational
excellence. Employing the PRISMA methodology, this study systematically reviews
existing literature to assess the current landscape of AI and NLP in bank
marketing. Additionally, it incorporates semantic mapping using Sentence
Transformers and UMAP for strategic gap analysis to identify underexplored
areas and opportunities for future research.
  The systematic review reveals limited research specifically focused on NLP
applications in bank marketing. The strategic gap analysis identifies key areas
where NLP can further enhance marketing strategies, including customer-centric
applications like acquisition, retention, and personalized engagement, offering
valuable insights for both academic research and practical implementation. This
research contributes to the field of bank marketing by mapping the current
state of AI and NLP applications and identifying strategic gaps. The findings
provide actionable insights for developing NLP-driven growth and innovation
frameworks and highlight the role of NLP in improving operational efficiency
and regulatory compliance. This work has broader implications for enhancing
customer experience, profitability, and innovation in the banking industry.",2024-11-17,"Christopher Gerling, Stefan Lessmann",http://arxiv.org/pdf/2411.14463v1,cs.CL
The Promises and Pitfalls of LLM Annotations in Dataset Labeling: a Case Study on Media Bias Detection,"High annotation costs from hiring or crowdsourcing complicate the creation of
large, high-quality datasets needed for training reliable text classifiers.
Recent research suggests using Large Language Models (LLMs) to automate the
annotation process, reducing these costs while maintaining data quality. LLMs
have shown promising results in annotating downstream tasks like hate speech
detection and political framing. Building on the success in these areas, this
study investigates whether LLMs are viable for annotating the complex task of
media bias detection and whether a downstream media bias classifier can be
trained on such data. We create annolexical, the first large-scale dataset for
media bias classification with over 48000 synthetically annotated examples. Our
classifier, fine-tuned on this dataset, surpasses all of the annotator LLMs by
5-9 percent in Matthews Correlation Coefficient (MCC) and performs close to or
outperforms the model trained on human-labeled data when evaluated on two media
bias benchmark datasets (BABE and BASIL). This study demonstrates how our
approach significantly reduces the cost of dataset creation in the media bias
domain and, by extension, the development of classifiers, while our subsequent
behavioral stress-testing reveals some of its current limitations and
trade-offs.",2024-11-17,"Tomas Horych, Christoph Mandl, Terry Ruas, Andre Greiner-Petter, Bela Gipp, Akiko Aizawa, Timo Spinde",http://arxiv.org/pdf/2411.11081v2,cs.CL
AIGS: Generating Science from AI-Powered Automated Falsification,"Rapid development of artificial intelligence has drastically accelerated the
development of scientific discovery. Trained with large-scale observation data,
deep neural networks extract the underlying patterns in an end-to-end manner
and assist human researchers with highly-precised predictions in unseen
scenarios. The recent rise of Large Language Models (LLMs) and the empowered
autonomous agents enable scientists to gain help through interaction in
different stages of their research, including but not limited to literature
review, research ideation, idea implementation, and academic writing. However,
AI researchers instantiated by foundation model empowered agents with
full-process autonomy are still in their infancy. In this paper, we study
$\textbf{AI-Generated Science}$ (AIGS), where agents independently and
autonomously complete the entire research process and discover scientific laws.
By revisiting the definition of scientific research, we argue that
$\textit{falsification}$ is the essence of both human research process and the
design of an AIGS system. Through the lens of falsification, prior systems
attempting towards AI-Generated Science either lack the part in their design,
or rely heavily on existing verification engines that narrow the use in
specialized domains. In this work, we propose Baby-AIGS as a baby-step
demonstration of a full-process AIGS system, which is a multi-agent system with
agents in roles representing key research process. By introducing
FalsificationAgent, which identify and then verify possible scientific
discoveries, we empower the system with explicit falsification. Experiments on
three tasks preliminarily show that Baby-AIGS could produce meaningful
scientific discoveries, though not on par with experienced human researchers.
Finally, we discuss on the limitations of current Baby-AIGS, actionable
insights, and related ethical issues in detail.",2024-11-17,"Zijun Liu, Kaiming Liu, Yiqi Zhu, Xuanyu Lei, Zonghan Yang, Zhenhe Zhang, Peng Li, Yang Liu",http://arxiv.org/pdf/2411.11910v2,cs.CL
Multilingual Large Language Models: A Systematic Survey,"This paper provides a comprehensive survey of the latest research on
multilingual large language models (MLLMs). MLLMs not only are able to
understand and generate language across linguistic boundaries, but also
represent an important advancement in artificial intelligence. We first discuss
the architecture and pre-training objectives of MLLMs, highlighting the key
components and methodologies that contribute to their multilingual
capabilities. We then discuss the construction of multilingual pre-training and
alignment datasets, underscoring the importance of data quality and diversity
in enhancing MLLM performance. An important focus of this survey is on the
evaluation of MLLMs. We present a detailed taxonomy and roadmap covering the
assessment of MLLMs' cross-lingual knowledge, reasoning, alignment with human
values, safety, interpretability and specialized applications. Specifically, we
extensively discuss multilingual evaluation benchmarks and datasets, and
explore the use of LLMs themselves as multilingual evaluators. To enhance MLLMs
from black to white boxes, we also address the interpretability of multilingual
capabilities, cross-lingual transfer and language bias within these models.
Finally, we provide a comprehensive review of real-world applications of MLLMs
across diverse domains, including biology, medicine, computer science,
mathematics and law. We showcase how these models have driven innovation and
improvements in these specialized fields while also highlighting the challenges
and opportunities in deploying MLLMs within diverse language communities and
application scenarios. We listed the paper related in this survey and publicly
available at https://github.com/tjunlp-lab/Awesome-Multilingual-LLMs-Papers.",2024-11-17,"Shaolin Zhu, Supryadi, Shaoyang Xu, Haoran Sun, Leiyu Pan, Menglong Cui, Jiangcun Du, Renren Jin, António Branco, Deyi Xiong",http://arxiv.org/pdf/2411.11072v2,cs.CL
Beyond Human-Like Processing: Large Language Models Perform Equivalently on Forward and Backward Scientific Text,"The impressive performance of large language models (LLMs) has led to their
consideration as models of human language processing. Instead, we suggest that
the success of LLMs arises from the flexibility of the transformer learning
architecture. To evaluate this conjecture, we trained LLMs on scientific texts
that were either in a forward or backward format. Despite backward text being
inconsistent with the structure of human languages, we found that LLMs
performed equally well in either format on a neuroscience benchmark, eclipsing
human expert performance for both forward and backward orders. Our results are
consistent with the success of transformers across diverse domains, such as
weather prediction and protein design. This widespread success is attributable
to LLM's ability to extract predictive patterns from any sufficiently
structured input. Given their generality, we suggest caution in interpreting
LLM's success in linguistic tasks as evidence for human-like mechanisms.",2024-11-17,"Xiaoliang Luo, Michael Ramscar, Bradley C. Love",http://arxiv.org/pdf/2411.11061v1,cs.CL
FastDraft: How to Train Your Draft,"Speculative Decoding has gained popularity as an effective technique for
accelerating the auto-regressive inference process of Large Language Models.
However, Speculative Decoding entirely relies on the availability of efficient
draft models, which are often lacking for many existing language models due to
a stringent constraint of vocabulary compatibility. In this work we introduce
FastDraft, a novel and efficient approach for pre-training and aligning a draft
model to any large language model by incorporating efficient pre-training,
followed by fine-tuning over synthetic datasets generated by the target model.
We demonstrate FastDraft by training two highly parameter efficient drafts for
the popular Phi-3-mini and Llama-3.1-8B models. Using FastDraft, we were able
to produce a draft model with approximately 10 billion tokens on a single
server with 8 Intel$^\circledR$ Gaudi$^\circledR$ 2 accelerators in under 24
hours. Our results show that the draft model achieves impressive results in key
metrics of acceptance rate, block efficiency and up to 3x memory bound speed up
when evaluated on code completion and up to 2x in summarization, text
completion and instruction tasks. We validate our theoretical findings through
benchmarking on the latest Intel$^\circledR$ Core$^{\tiny \text{TM}}$ Ultra,
achieving a wall-clock time speedup of up to 2x, indicating a significant
reduction in runtime. Due to its high quality, FastDraft unlocks large language
models inference on AI-PC and other edge-devices.",2024-11-17,"Ofir Zafrir, Igor Margulis, Dorin Shteyman, Shira Guskin, Guy Boudoukh",http://arxiv.org/pdf/2411.11055v2,cs.CL
SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation,"Large language models demonstrate exceptional performance in simple code
generation tasks but still face challenges in tackling complex problems. These
challenges may stem from insufficient reasoning and problem decomposition
capabilities. To address this issue, we propose a reasoning-augmented data
generation process, SRA-MCTS, which guides the model to autonomously generate
high-quality intermediate reasoning paths. This creates a positive feedback
loop, enabling continuous improvement. Our method operates entirely through the
model itself without requiring additional supervision. By synthesizing natural
language reasoning paths and translating them into executable code, the
approach ensures analytical accuracy and enhances the success rate in solving
complex tasks. Experimental results show that, even without additional
supervisory signals, our method achieves performance improvements across
different model scales, demonstrating the significant potential of
self-improvement in small models. Furthermore, the method remains robust when
traditional Chain-of-Thought (CoT) approaches exhibit performance degradation,
with notable improvements observed in diversity metrics such as pass@10. We
encourage further exploration of reasoning processes within training data to
enhance the ability of language models to address complex problems. Our code
and data are public at https://github.com/DIRECT-BIT/SRA-MCTS.",2024-11-17,"Bin Xu, Yiguan Lin, Yinghao Li, Yang Gao",http://arxiv.org/pdf/2411.11053v5,cs.CL
BianCang: A Traditional Chinese Medicine Large Language Model,"The rise of large language models (LLMs) has driven significant progress in
medical applications, including traditional Chinese medicine (TCM). However,
current medical LLMs struggle with TCM diagnosis and syndrome differentiation
due to substantial differences between TCM and modern medical theory, and the
scarcity of specialized, high-quality corpora. This paper addresses these
challenges by proposing BianCang, a TCM-specific LLM, using a two-stage
training process that first injects domain-specific knowledge and then aligns
it through targeted stimulation. To enhance diagnostic and differentiation
capabilities, we constructed pre-training corpora, instruction-aligned datasets
based on real hospital records, and the ChP-TCM dataset derived from the
Pharmacopoeia of the People's Republic of China. We compiled extensive TCM and
medical corpora for continuous pre-training and supervised fine-tuning,
building a comprehensive dataset to refine the model's understanding of TCM.
Evaluations across 11 test sets involving 29 models and 4 tasks demonstrate the
effectiveness of BianCang, offering valuable insights for future research.
Code, datasets, and models are available at
https://github.com/QLU-NLP/BianCang.",2024-11-17,"Sibo Wei, Xueping Peng, Yi-fei Wang, Jiasheng Si, Weiyu Zhang, Wenpeng Lu, Xiaoming Wu, Yinglong Wang",http://arxiv.org/pdf/2411.11027v1,cs.CL
AddrLLM: Address Rewriting via Large Language Model on Nationwide Logistics Data,"Textual description of a physical location, commonly known as an address,
plays an important role in location-based services(LBS) such as on-demand
delivery and navigation. However, the prevalence of abnormal addresses, those
containing inaccuracies that fail to pinpoint a location, have led to
significant costs. Address rewriting has emerged as a solution to rectify these
abnormal addresses. Despite the critical need, existing address rewriting
methods are limited, typically tailored to correct specific error types, or
frequently require retraining to process new address data effectively. In this
study, we introduce AddrLLM, an innovative framework for address rewriting that
is built upon a retrieval augmented large language model. AddrLLM overcomes
aforementioned limitations through a meticulously designed Supervised
Fine-Tuning module, an Address-centric Retrieval Augmented Generation module
and a Bias-free Objective Alignment module. To the best of our knowledge, this
study pioneers the application of LLM-based address rewriting approach to solve
the issue of abnormal addresses. Through comprehensive offline testing with
real-world data on a national scale and subsequent online deployment, AddrLLM
has demonstrated superior performance in integration with existing logistics
system. It has significantly decreased the rate of parcel re-routing by
approximately 43\%, underscoring its exceptional efficacy in real-world
applications.",2024-11-17,"Qinchen Yang, Zhiqing Hong, Dongjiang Cao, Haotian Wang, Zejun Xie, Tian He, Yunhuai Liu, Yu Yang, Desheng Zhang",http://arxiv.org/pdf/2411.13584v1,cs.CL
A Topic-aware Comparable Corpus of Chinese Variations,"This study aims to fill the gap by constructing a topic-aware comparable
corpus of Mainland Chinese Mandarin and Taiwanese Mandarin from the social
media in Mainland China and Taiwan, respectively. Using Dcard for Taiwanese
Mandarin and Sina Weibo for Mainland Chinese, we create a comparable corpus
that updates regularly and reflects modern language use on social media.",2024-11-17,"Da-Chen Lian, Shu-Kai Hsieh",http://arxiv.org/pdf/2411.10955v1,cs.CL
Dialectal Toxicity Detection: Evaluating LLM-as-a-Judge Consistency Across Language Varieties,"There has been little systematic study on how dialectal differences affect
toxicity detection by modern LLMs. Furthermore, although using LLMs as
evaluators (""LLM-as-a-judge"") is a growing research area, their sensitivity to
dialectal nuances is still underexplored and requires more focused attention.
In this paper, we address these gaps through a comprehensive toxicity
evaluation of LLMs across diverse dialects. We create a multi-dialect dataset
through synthetic transformations and human-assisted translations, covering 10
language clusters and 60 varieties. We then evaluated three LLMs on their
ability to assess toxicity across multilingual, dialectal, and LLM-human
consistency. Our findings show that LLMs are sensitive in handling both
multilingual and dialectal variations. However, if we have to rank the
consistency, the weakest area is LLM-human agreement, followed by dialectal
consistency. Code repository:
\url{https://github.com/ffaisal93/dialect_toxicity_llm_judge}",2024-11-17,"Fahim Faisal, Md Mushfiqur Rahman, Antonios Anastasopoulos",http://arxiv.org/pdf/2411.10954v1,cs.CL
Understanding Multimodal LLMs: the Mechanistic Interpretability of Llava in Visual Question Answering,"Understanding the mechanisms behind Large Language Models (LLMs) is crucial
for designing improved models and strategies. While recent studies have yielded
valuable insights into the mechanisms of textual LLMs, the mechanisms of
Multi-modal Large Language Models (MLLMs) remain underexplored. In this paper,
we apply mechanistic interpretability methods to analyze the visual question
answering (VQA) mechanisms in the first MLLM, Llava. We compare the mechanisms
between VQA and textual QA (TQA) in color answering tasks and find that: a) VQA
exhibits a mechanism similar to the in-context learning mechanism observed in
TQA; b) the visual features exhibit significant interpretability when
projecting the visual embeddings into the embedding space; and c) Llava
enhances the existing capabilities of the corresponding textual LLM Vicuna
during visual instruction tuning. Based on these findings, we develop an
interpretability tool to help users and researchers identify important visual
locations for final predictions, aiding in the understanding of visual
hallucination. Our method demonstrates faster and more effective results
compared to existing interpretability approaches. Code:
\url{https://github.com/zepingyu0512/llava-mechanism}",2024-11-17,"Zeping Yu, Sophia Ananiadou",http://arxiv.org/pdf/2411.10950v2,cs.CL
Improving Tool Retrieval by Leveraging Large Language Models for Query Generation,"Using tools by Large Language Models (LLMs) is a promising avenue to extend
their reach beyond language or conversational settings. The number of tools can
scale to thousands as they enable accessing sensory information, fetching
updated factual knowledge, or taking actions in the real world. In such
settings, in-context learning by providing a short list of relevant tools in
the prompt is a viable approach. To retrieve relevant tools, various approaches
have been suggested, ranging from simple frequency-based matching to dense
embedding-based semantic retrieval. However, such approaches lack the
contextual and common-sense understanding required to retrieve the right tools
for complex user requests. Rather than increasing the complexity of the
retrieval component itself, we propose leveraging LLM understanding to generate
a retrieval query. Then, the generated query is embedded and used to find the
most relevant tools via a nearest-neighbor search. We investigate three
approaches for query generation: zero-shot prompting, supervised fine-tuning on
tool descriptions, and alignment learning by iteratively optimizing a reward
metric measuring retrieval performance. By conducting extensive experiments on
a dataset covering complex and multi-tool scenarios, we show that leveraging
LLMs for query generation improves the retrieval for in-domain (seen tools) and
out-of-domain (unseen tools) settings.",2024-11-17,"Mohammad Kachuee, Sarthak Ahuja, Vaibhav Kumar, Puyang Xu, Xiaohu Liu",http://arxiv.org/pdf/2412.03573v1,cs.CL
Memory-Augmented Multimodal LLMs for Surgical VQA via Self-Contained Inquiry,"Comprehensively understanding surgical scenes in Surgical Visual Question
Answering (Surgical VQA) requires reasoning over multiple objects. Previous
approaches address this task using cross-modal fusion strategies to enhance
reasoning ability. However, these methods often struggle with limited scene
understanding and question comprehension, and some rely on external resources
(e.g., pre-extracted object features), which can introduce errors and
generalize poorly across diverse surgical environments. To address these
challenges, we propose SCAN, a simple yet effective memory-augmented framework
that leverages Multimodal LLMs to improve surgical context comprehension via
Self-Contained Inquiry. SCAN operates autonomously, generating two types of
memory for context augmentation: Direct Memory (DM), which provides multiple
candidates (or hints) to the final answer, and Indirect Memory (IM), which
consists of self-contained question-hint pairs to capture broader scene
context. DM directly assists in answering the question, while IM enhances
understanding of the surgical scene beyond the immediate query. Reasoning over
these object-aware memories enables the model to accurately interpret images
and respond to questions. Extensive experiments on three publicly available
Surgical VQA datasets demonstrate that SCAN achieves state-of-the-art
performance, offering improved accuracy and robustness across various surgical
scenarios.",2024-11-17,"Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, Jiang Liu",http://arxiv.org/pdf/2411.10937v1,cs.CL
Analyzing Pokémon and Mario Streamers' Twitch Chat with LLM-based User Embeddings,"We present a novel digital humanities method for representing our Twitch
chatters as user embeddings created by a large language model (LLM). We cluster
these embeddings automatically using affinity propagation and further narrow
this clustering down through manual analysis. We analyze the chat of one stream
by each Twitch streamer: SmallAnt, DougDoug and PointCrow. Our findings suggest
that each streamer has their own type of chatters, however two categories
emerge for all of the streamers: supportive viewers and emoji and reaction
senders. Repetitive message spammers is a shared chatter category for two of
the streamers.",2024-11-17,"Mika Hämäläinen, Jack Rueter, Khalid Alnajjar",http://arxiv.org/pdf/2411.10934v1,cs.CL
Learn from Downstream and Be Yourself in Multimodal Large Language Model Fine-Tuning,"Multimodal Large Language Model (MLLM) have demonstrated strong
generalization capabilities across diverse distributions and tasks, largely due
to extensive pre-training datasets. Fine-tuning MLLM has become a common
practice to improve performance on specific downstream tasks. However, during
fine-tuning, MLLM often faces the risk of forgetting knowledge acquired during
pre-training, which can result in a decline in generalization abilities. To
balance the trade-off between generalization and specialization, we propose
measuring the parameter importance for both pre-trained and fine-tuning
distributions, based on frozen pre-trained weight magnitude and accumulated
fine-tuning gradient values. We further apply an importance-aware weight
allocation strategy, selectively updating relatively important parameters for
downstream tasks. We conduct empirical evaluations on both image captioning and
visual question-answering tasks using various MLLM architectures. The
comprehensive experimental analysis demonstrates the effectiveness of the
proposed solution, highlighting the efficiency of the crucial modules in
enhancing downstream specialization performance while mitigating generalization
degradation in MLLM Fine-Tuning.",2024-11-17,"Wenke Huang, Jian Liang, Zekun Shi, Didi Zhu, Guancheng Wan, He Li, Bo Du, Dacheng Tao, Mang Ye",http://arxiv.org/pdf/2411.10928v1,cs.CL
Inter-linguistic Phonetic Composition (IPC): A Theoretical and Computational Approach to Enhance Second Language Pronunciation,"Learners of a second language (L2) often unconsciously substitute unfamiliar
L2 phonemes with similar phonemes from their native language (L1), even though
native speakers of the L2 perceive these sounds as distinct and
non-interchangeable. This phonemic substitution leads to deviations from the
standard phonological patterns of the L2, creating challenges for learners in
acquiring accurate L2 pronunciation. To address this, we propose
Inter-linguistic Phonetic Composition (IPC), a novel computational method
designed to minimize incorrect phonological transfer by reconstructing L2
phonemes as composite sounds derived from multiple L1 phonemes. Tests with two
automatic speech recognition models demonstrated that when L2 speakers produced
IPC-generated composite sounds, the recognition rate of target L2 phonemes
improved by 20% compared to when their pronunciation was influenced by original
phonological transfer patterns. The improvement was observed within a
relatively shorter time frame, demonstrating rapid acquisition of the composite
sound.",2024-11-17,"Jisang Park, Minu Kim, DaYoung Hong, Jongha Lee",http://arxiv.org/pdf/2411.10927v2,cs.CL
"Bias in Large Language Models: Origin, Evaluation, and Mitigation","Large Language Models (LLMs) have revolutionized natural language processing,
but their susceptibility to biases poses significant challenges. This
comprehensive review examines the landscape of bias in LLMs, from its origins
to current mitigation strategies. We categorize biases as intrinsic and
extrinsic, analyzing their manifestations in various NLP tasks. The review
critically assesses a range of bias evaluation methods, including data-level,
model-level, and output-level approaches, providing researchers with a robust
toolkit for bias detection. We further explore mitigation strategies,
categorizing them into pre-model, intra-model, and post-model techniques,
highlighting their effectiveness and limitations. Ethical and legal
implications of biased LLMs are discussed, emphasizing potential harms in
real-world applications such as healthcare and criminal justice. By
synthesizing current knowledge on bias in LLMs, this review contributes to the
ongoing effort to develop fair and responsible AI systems. Our work serves as a
comprehensive resource for researchers and practitioners working towards
understanding, evaluating, and mitigating bias in LLMs, fostering the
development of more equitable AI technologies.",2024-11-16,"Yufei Guo, Muzhe Guo, Juntao Su, Zhou Yang, Mengqiu Zhu, Hongfei Li, Mengyang Qiu, Shuo Shuo Liu",http://arxiv.org/pdf/2411.10915v1,cs.CL
BPO: Towards Balanced Preference Optimization between Knowledge Breadth and Depth in Alignment,"Reinforcement Learning with Human Feedback (RLHF) is the key to the success
of large language models (LLMs) in recent years. In this work, we first
introduce the concepts of knowledge breadth and knowledge depth, which measure
the comprehensiveness and depth of an LLM or knowledge source respectively. We
reveal that the imbalance in the number of prompts and responses can lead to a
potential disparity in breadth and depth learning within alignment tuning
datasets by showing that even a simple uniform method for balancing the number
of instructions and responses can lead to significant improvements. Building on
this, we further propose Balanced Preference Optimization (BPO), designed to
dynamically augment the knowledge depth of each sample. BPO is motivated by the
observation that the usefulness of knowledge varies across samples,
necessitating tailored learning of knowledge depth. To achieve this, we
introduce gradient-based clustering, estimating the knowledge informativeness
and usefulness of each augmented sample based on the model's optimization
direction. Our experimental results across various benchmarks demonstrate that
BPO outperforms other baseline methods in alignment tuning while maintaining
training efficiency. Furthermore, we conduct a detailed analysis of each
component of BPO, providing guidelines for future research in preference data
optimization.",2024-11-16,"Sizhe Wang, Yongqi Tong, Hengyuan Zhang, Dawei Li, Xin Zhang, Tianlong Chen",http://arxiv.org/pdf/2411.10914v2,cs.CL
SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment,"When different groups' values differ, one approach to model alignment is to
steer models at inference time towards each group's preferences. However,
techniques like in-context learning only consider similarity when drawing
few-shot examples and not cross-group differences in values. We propose SPICA,
a framework that accounts for group-level differences during in-context example
retrieval. SPICA introduces three designs: scenario banks, group-informed
retrieval metrics, and in-context alignment prompts. From an evaluation of
SPICA on an alignment task collecting inputs from four demographic groups ($n =
544$), our metrics retrieve in-context examples that more closely match
observed preferences, with the best prompt configuration using multiple
contrastive responses to demonstrate examples. In an end-to-end evaluation ($n
= 120$), we observe that SPICA is higher rated than similarity-based retrieval,
with groups seeing up to a +0.16 point improvement on a 5 point scale.
Additionally, gains from SPICA were more uniform, with all groups benefiting
from alignment rather than only some. Finally, we find that while a
group-agnostic approach can align to aggregated values, it is not most suited
for divergent groups.",2024-11-16,"Quan Ze Chen, K. J. Kevin Feng, Chan Young Park, Amy X. Zhang",http://arxiv.org/pdf/2411.10912v2,cs.CL
BanglaDialecto: An End-to-End AI-Powered Regional Speech Standardization,"This study focuses on recognizing Bangladeshi dialects and converting diverse
Bengali accents into standardized formal Bengali speech. Dialects, often
referred to as regional languages, are distinctive variations of a language
spoken in a particular location and are identified by their phonetics,
pronunciations, and lexicon. Subtle changes in pronunciation and intonation are
also influenced by geographic location, educational attainment, and
socioeconomic status. Dialect standardization is needed to ensure effective
communication, educational consistency, access to technology, economic
opportunities, and the preservation of linguistic resources while respecting
cultural diversity. Being the fifth most spoken language with around 55
distinct dialects spoken by 160 million people, addressing Bangla dialects is
crucial for developing inclusive communication tools. However, limited research
exists due to a lack of comprehensive datasets and the challenges of handling
diverse dialects. With the advancement in multilingual Large Language Models
(mLLMs), emerging possibilities have been created to address the challenges of
dialectal Automated Speech Recognition (ASR) and Machine Translation (MT). This
study presents an end-to-end pipeline for converting dialectal Noakhali speech
to standard Bangla speech. This investigation includes constructing a
large-scale diverse dataset with dialectal speech signals that tailored the
fine-tuning process in ASR and LLM for transcribing the dialect speech to
dialect text and translating the dialect text to standard Bangla text. Our
experiments demonstrated that fine-tuning the Whisper ASR model achieved a CER
of 0.8% and WER of 1.5%, while the BanglaT5 model attained a BLEU score of
41.6% for dialect-to-standard text translation.",2024-11-16,"Md. Nazmus Sadat Samin, Jawad Ibn Ahad, Tanjila Ahmed Medha, Fuad Rahman, Mohammad Ruhul Amin, Nabeel Mohammed, Shafin Rahman",http://arxiv.org/pdf/2411.10879v1,cs.CL
Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis,"This study investigates the automation of meta-analysis in scientific
documents using large language models (LLMs). Meta-analysis is a robust
statistical method that synthesizes the findings of multiple studies support
articles to provide a comprehensive understanding. We know that a meta-article
provides a structured analysis of several articles. However, conducting
meta-analysis by hand is labor-intensive, time-consuming, and susceptible to
human error, highlighting the need for automated pipelines to streamline the
process. Our research introduces a novel approach that fine-tunes the LLM on
extensive scientific datasets to address challenges in big data handling and
structured data extraction. We automate and optimize the meta-analysis process
by integrating Retrieval Augmented Generation (RAG). Tailored through prompt
engineering and a new loss metric, Inverse Cosine Distance (ICD), designed for
fine-tuning on large contextual datasets, LLMs efficiently generate structured
meta-analysis content. Human evaluation then assesses relevance and provides
information on model performance in key metrics. This research demonstrates
that fine-tuned models outperform non-fine-tuned models, with fine-tuned LLMs
generating 87.6% relevant meta-analysis abstracts. The relevance of the
context, based on human evaluation, shows a reduction in irrelevancy from 4.56%
to 1.9%. These experiments were conducted in a low-resource environment,
highlighting the study's contribution to enhancing the efficiency and
reliability of meta-analysis automation.",2024-11-16,"Jawad Ibn Ahad, Rafeed Mohammad Sultan, Abraham Kaikobad, Fuad Rahman, Mohammad Ruhul Amin, Nabeel Mohammed, Shafin Rahman",http://arxiv.org/pdf/2411.10878v1,cs.CL
Large Language Models (LLMs) as Traffic Control Systems at Urban Intersections: A New Paradigm,"This study introduces a novel approach for traffic control systems by using
Large Language Models (LLMs) as traffic controllers. The study utilizes their
logical reasoning, scene understanding, and decision-making capabilities to
optimize throughput and provide feedback based on traffic conditions in
real-time. LLMs centralize traditionally disconnected traffic control processes
and can integrate traffic data from diverse sources to provide context-aware
decisions. LLMs can also deliver tailored outputs using various means such as
wireless signals and visuals to drivers, infrastructures, and autonomous
vehicles. To evaluate LLMs ability as traffic controllers, this study proposed
a four-stage methodology. The methodology includes data creation and
environment initialization, prompt engineering, conflict identification, and
fine-tuning. We simulated multi-lane four-leg intersection scenarios and
generates detailed datasets to enable conflict detection using LLMs and Python
simulation as a ground truth. We used chain-of-thought prompts to lead LLMs in
understanding the context, detecting conflicts, resolving them using traffic
rules, and delivering context-sensitive traffic management solutions. We
evaluated the prformance GPT-mini, Gemini, and Llama as traffic controllers.
Results showed that the fine-tuned GPT-mini achieved 83% accuracy and an
F1-score of 0.84. GPT-mini model exhibited a promising performance in
generating actionable traffic management insights, with high ROUGE-L scores
across conflict identification of 0.95, decision-making of 0.91, priority
assignment of 0.94, and waiting time optimization of 0.92. We demonstrated that
LLMs can offer precise recommendations to drivers in real-time including
yielding, slowing, or stopping based on vehicle dynamics.",2024-11-16,"Sari Masri, Huthaifa I. Ashqar, Mohammed Elhenawy",http://arxiv.org/pdf/2411.10869v1,cs.CL
Large Vision-Language Models for Remote Sensing Visual Question Answering,"Remote Sensing Visual Question Answering (RSVQA) is a challenging task that
involves interpreting complex satellite imagery to answer natural language
questions. Traditional approaches often rely on separate visual feature
extractors and language processing models, which can be computationally
intensive and limited in their ability to handle open-ended questions. In this
paper, we propose a novel method that leverages a generative Large
Vision-Language Model (LVLM) to streamline the RSVQA process. Our approach
consists of a two-step training strategy: domain-adaptive pretraining and
prompt-based finetuning. This method enables the LVLM to generate natural
language answers by conditioning on both visual and textual inputs, without the
need for predefined answer categories. We evaluate our model on the RSVQAxBEN
dataset, demonstrating superior performance compared to state-of-the-art
baselines. Additionally, a human evaluation study shows that our method
produces answers that are more accurate, relevant, and fluent. The results
highlight the potential of generative LVLMs in advancing the field of remote
sensing analysis.",2024-11-16,"Surasakdi Siripong, Apirak Chaiyapan, Thanakorn Phonchai",http://arxiv.org/pdf/2411.10857v1,cs.CL
Towards Next-Generation Medical Agent: How o1 is Reshaping Decision-Making in Medical Scenarios,"Artificial Intelligence (AI) has become essential in modern healthcare, with
large language models (LLMs) offering promising advances in clinical
decision-making. Traditional model-based approaches, including those leveraging
in-context demonstrations and those with specialized medical fine-tuning, have
demonstrated strong performance in medical language processing but struggle
with real-time adaptability, multi-step reasoning, and handling complex medical
tasks. Agent-based AI systems address these limitations by incorporating
reasoning traces, tool selection based on context, knowledge retrieval, and
both short- and long-term memory. These additional features enable the medical
AI agent to handle complex medical scenarios where decision-making should be
built on real-time interaction with the environment. Therefore, unlike
conventional model-based approaches that treat medical queries as isolated
questions, medical AI agents approach them as complex tasks and behave more
like human doctors. In this paper, we study the choice of the backbone LLM for
medical AI agents, which is the foundation for the agent's overall reasoning
and action generation. In particular, we consider the emergent o1 model and
examine its impact on agents' reasoning, tool-use adaptability, and real-time
information retrieval across diverse clinical scenarios, including high-stakes
settings such as intensive care units (ICUs). Our findings demonstrate o1's
ability to enhance diagnostic accuracy and consistency, paving the way for
smarter, more responsive AI tools that support better patient outcomes and
decision-making efficacy in clinical practice.",2024-11-16,"Shaochen Xu, Yifan Zhou, Zhengliang Liu, Zihao Wu, Tianyang Zhong, Huaqin Zhao, Yiwei Li, Hanqi Jiang, Yi Pan, Junhao Chen, Jin Lu, Wei Zhang, Tuo Zhang, Lu Zhang, Dajiang Zhu, Xiang Li, Wei Liu, Quanzheng Li, Andrea Sikora, Xiaoming Zhai, Zhen Xiang, Tianming Liu",http://arxiv.org/pdf/2411.14461v1,cs.CL
Bilingual Text-dependent Speaker Verification with Pre-trained Models for TdSV Challenge 2024,"This paper presents our submissions to the Iranian division of the
Text-dependent Speaker Verification Challenge (TdSV) 2024. TdSV aims to
determine if a specific phrase was spoken by a target speaker. We developed two
independent subsystems based on pre-trained models: For phrase verification, a
phrase classifier rejected incorrect phrases, while for speaker verification, a
pre-trained ResNet293 with domain adaptation extracted speaker embeddings for
computing cosine similarity scores. In addition, we evaluated Whisper-PMFA, a
pre-trained ASR model adapted for speaker verification, and found that,
although it outperforms randomly initialized ResNets, it falls short of the
performance of pre-trained ResNets, highlighting the importance of large-scale
pre-training. The results also demonstrate that achieving competitive
performance on TdSV without joint modeling of speaker and text is possible. Our
best system achieved a MinDCF of 0.0358 on the evaluation subset and won the
challenge.",2024-11-16,Seyed Ali Farokh,http://arxiv.org/pdf/2411.10828v1,cs.CL
Information Anxiety in Large Language Models,"Large Language Models (LLMs) have demonstrated strong performance as
knowledge repositories, enabling models to understand user queries and generate
accurate and context-aware responses. Extensive evaluation setups have
corroborated the positive correlation between the retrieval capability of LLMs
and the frequency of entities in their pretraining corpus. We take the
investigation further by conducting a comprehensive analysis of the internal
reasoning and retrieval mechanisms of LLMs. Our work focuses on three critical
dimensions - the impact of entity popularity, the models' sensitivity to
lexical variations in query formulation, and the progression of hidden state
representations across LLM layers. Our preliminary findings reveal that popular
questions facilitate early convergence of internal states toward the correct
answer. However, as the popularity of a query increases, retrieved attributes
across lexical variations become increasingly dissimilar and less accurate.
Interestingly, we find that LLMs struggle to disentangle facts, grounded in
distinct relations, from their parametric memory when dealing with highly
popular subjects. Through a case study, we explore these latent strains within
LLMs when processing highly popular queries, a phenomenon we term information
anxiety. The emergence of information anxiety in LLMs underscores the
adversarial injection in the form of linguistic variations and calls for a more
holistic evaluation of frequently occurring entities.",2024-11-16,"Prasoon Bajpai, Sarah Masud, Tanmoy Chakraborty",http://arxiv.org/pdf/2411.10813v1,cs.CL
Playing Language Game with LLMs Leads to Jailbreaking,"The advent of large language models (LLMs) has spurred the development of
numerous jailbreak techniques aimed at circumventing their security defenses
against malicious attacks. An effective jailbreak approach is to identify a
domain where safety generalization fails, a phenomenon known as mismatched
generalization. In this paper, we introduce two novel jailbreak methods based
on mismatched generalization: natural language games and custom language games,
both of which effectively bypass the safety mechanisms of LLMs, with various
kinds and different variants, making them hard to defend and leading to high
attack rates. Natural language games involve the use of synthetic linguistic
constructs and the actions intertwined with these constructs, such as the Ubbi
Dubbi language. Building on this phenomenon, we propose the custom language
games method: by engaging with LLMs using a variety of custom rules, we
successfully execute jailbreak attacks across multiple LLM platforms. Extensive
experiments demonstrate the effectiveness of our methods, achieving success
rates of 93% on GPT-4o, 89% on GPT-4o-mini and 83% on Claude-3.5-Sonnet.
Furthermore, to investigate the generalizability of safety alignments, we
fine-tuned Llama-3.1-70B with the custom language games to achieve safety
alignment within our datasets and found that when interacting through other
language games, the fine-tuned models still failed to identify harmful content.
This finding indicates that the safety alignment knowledge embedded in LLMs
fails to generalize across different linguistic formats, thus opening new
avenues for future research in this area.",2024-11-16,"Yu Peng, Zewen Long, Fangming Dong, Congyi Li, Shu Wu, Kai Chen",http://arxiv.org/pdf/2411.12762v2,cs.CL
LLaSA: Large Language and Structured Data Assistant,"Structured data, such as tables, graphs, and databases, play a critical role
in plentiful NLP tasks such as question answering and dialogue system.
Recently, inspired by Vision-Language Models, Graph Neutral Networks (GNNs)
have been introduced as an additional modality into the input of Large Language
Models (LLMs) to improve their performance on Structured Knowledge Grounding
(SKG) tasks. However, those GNN-enhanced LLMs have the following limitations:
(1) They employ diverse GNNs to model varying types of structured data,
rendering them unable to uniformly process various forms of structured data.
(2) The pretraining of GNNs is coupled with specific LLMs, which prevents GNNs
from fully aligning with the textual space and limits their adaptability to
other LLMs. To address these issues, we propose \textbf{L}arge
\textbf{L}anguage and \textbf{S}tructured Data \textbf{A}ssistant (LLaSA), a
general framework for enhancing LLMs' ability to handle structured data.
Specifically, we represent various types of structured data in a unified
hypergraph format, and use self-supervised learning to pretrain a hypergraph
encoder, and a G-Former compressing encoded hypergraph representations with
cross-attention. The compressed hypergraph representations are appended to the
serialized inputs during training and inference stages of LLMs. Experimental
results on multiple SKG tasks show that our pretrained hypergraph encoder can
adapt to various LLMs and enhance their ability to process different types of
structured data. Besides, LLaSA, with LoRA fine-tuning, outperforms previous
SOTA method using full parameters tuning.",2024-11-16,"Yao Xu, Shizhu He, Jiabei Chen, Zeng Xiangrong, Bingning Wang, Guang Liu, Jun Zhao, Kang Liu",http://arxiv.org/pdf/2411.14460v2,cs.CL
Unveiling User Preferences: A Knowledge Graph and LLM-Driven Approach for Conversational Recommendation,"Conversational Recommender Systems (CRSs) aim to provide personalized
recommendations through dynamically capturing user preferences in interactive
conversations. Conventional CRSs often extract user preferences as hidden
representations, which are criticized for their lack of interpretability. This
diminishes the transparency and trustworthiness of the recommendation process.
Recent works have explored combining the impressive capabilities of Large
Language Models (LLMs) with the domain-specific knowledge of Knowledge Graphs
(KGs) to generate human-understandable recommendation explanations. Despite
these efforts, the integration of LLMs and KGs for CRSs remains challenging due
to the modality gap between unstructured dialogues and structured KGs.
Moreover, LLMs pre-trained on large-scale corpora may not be well-suited for
analyzing user preferences, which require domain-specific knowledge. In this
paper, we propose COMPASS, a plug-and-play framework that synergizes LLMs and
KGs to unveil user preferences, enhancing the performance and explainability of
existing CRSs. To address integration challenges, COMPASS employs a two-stage
training approach: first, it bridges the gap between the structured KG and
natural language through an innovative graph entity captioning pre-training
mechanism. This enables the LLM to transform KG entities into concise natural
language descriptions, allowing them to comprehend domain-specific knowledge.
Following, COMPASS optimizes user preference modeling via knowledge-aware
instruction fine-tuning, where the LLM learns to reason and summarize user
preferences from both dialogue histories and KG-augmented context. This enables
COMPASS to perform knowledge-aware reasoning and generate comprehensive and
interpretable user preferences that can seamlessly integrate with existing CRS
models for improving recommendation performance and explainability.",2024-11-16,"Zhangchi Qiu, Linhao Luo, Shirui Pan, Alan Wee-Chung Liew",http://arxiv.org/pdf/2411.14459v1,cs.CL
Can Generic LLMs Help Analyze Child-adult Interactions Involving Children with Autism in Clinical Observation?,"Large Language Models (LLMs) have shown significant potential in
understanding human communication and interaction. However, their performance
in the domain of child-inclusive interactions, including in clinical settings,
remains less explored. In this work, we evaluate generic LLMs' ability to
analyze child-adult dyadic interactions in a clinically relevant context
involving children with ASD. Specifically, we explore LLMs in performing four
tasks: classifying child-adult utterances, predicting engaged activities,
recognizing language skills and understanding traits that are clinically
relevant. Our evaluation shows that generic LLMs are highly capable of
analyzing long and complex conversations in clinical observation sessions,
often surpassing the performance of non-expert human evaluators. The results
show their potential to segment interactions of interest, assist in language
skills evaluation, identify engaged activities, and offer clinical-relevant
context for assessments.",2024-11-16,"Tiantian Feng, Anfeng Xu, Rimita Lahiri, Helen Tager-Flusberg, So Hyun Kim, Somer Bishop, Catherine Lord, Shrikanth Narayanan",http://arxiv.org/pdf/2411.10761v1,cs.CL
Chain-of-Programming (CoP) : Empowering Large Language Models for Geospatial Code Generation,"With the rapid growth of interdisciplinary demands for geospatial modeling
and the rise of large language models (LLMs), geospatial code generation
technology has seen significant advancements. However, existing LLMs often face
challenges in the geospatial code generation process due to incomplete or
unclear user requirements and insufficient knowledge of specific platform
syntax rules, leading to the generation of non-executable code, a phenomenon
known as ""code hallucination."" To address this issue, this paper proposes a
Chain of Programming (CoP) framework, which decomposes the code generation
process into five steps: requirement analysis, algorithm design, code
implementation, code debugging, and code annotation. The framework incorporates
a shared information pool, knowledge base retrieval, and user feedback
mechanisms, forming an end-to-end code generation flow from requirements to
code without the need for model fine-tuning. Based on a geospatial problem
classification framework and evaluation benchmarks, the CoP strategy
significantly improves the logical clarity, syntactical correctness, and
executability of the generated code, with improvements ranging from 3.0% to
48.8%. Comparative and ablation experiments further validate the superiority of
the CoP strategy over other optimization approaches and confirm the rationality
and necessity of its key components. Through case studies on building data
visualization and fire data analysis, this paper demonstrates the application
and effectiveness of CoP in various geospatial scenarios. The CoP framework
offers a systematic, step-by-step approach to LLM-based geospatial code
generation tasks, significantly enhancing code generation performance in
geospatial tasks and providing valuable insights for code generation in other
vertical domains.",2024-11-16,"Shuyang Hou, Haoyue Jiao, Zhangxiao Shen, Jianyuan Liang, Anqi Zhao, Xiaopu Zhang, Jianxun Wang, Huayi Wu",http://arxiv.org/pdf/2411.10753v1,cs.CL
Comparison of Multilingual and Bilingual Models for Satirical News Detection of Arabic and English,"Satirical news is real news combined with a humorous comment or exaggerated
content, and it often mimics the format and style of real news. However,
satirical news is often misunderstood as misinformation, especially by
individuals from different cultural and social backgrounds. This research
addresses the challenge of distinguishing satire from truthful news by
leveraging multilingual satire detection methods in English and Arabic. We
explore both zero-shot and chain-of-thought (CoT) prompting using two language
models, Jais-chat(13B) and LLaMA-2-chat(7B). Our results show that CoT
prompting offers a significant advantage for the Jais-chat model over the
LLaMA-2-chat model. Specifically, Jais-chat achieved the best performance, with
an F1-score of 80\% in English when using CoT prompting. These results
highlight the importance of structured reasoning in CoT, which enhances
contextual understanding and is vital for complex tasks like satire detection.",2024-11-16,"Omar W. Abdalla, Aditya Joshi, Rahat Masood, Salil S. Kanhere",http://arxiv.org/pdf/2411.10730v1,cs.CL
HJ-Ky-0.1: an Evaluation Dataset for Kyrgyz Word Embeddings,"One of the key tasks in modern applied computational linguistics is
constructing word vector representations (word embeddings), which are widely
used to address natural language processing tasks such as sentiment analysis,
information extraction, and more. To choose an appropriate method for
generating these word embeddings, quality assessment techniques are often
necessary. A standard approach involves calculating distances between vectors
for words with expert-assessed 'similarity'. This work introduces the first
'silver standard' dataset for such tasks in the Kyrgyz language, alongside
training corresponding models and validating the dataset's suitability through
quality evaluation metrics.",2024-11-16,"Anton Alekseev, Gulnara Kabaeva",http://arxiv.org/pdf/2411.10724v2,cs.CL
A Regularized LSTM Method for Detecting Fake News Articles,"Nowadays, the rapid diffusion of fake news poses a significant problem, as it
can spread misinformation and confusion. This paper aims to develop an advanced
machine learning solution for detecting fake news articles. Leveraging a
comprehensive dataset of news articles, including 23,502 fake news articles and
21,417 accurate news articles, we implemented and evaluated three
machine-learning models. Our dataset, curated from diverse sources, provides
rich textual content categorized into title, text, subject, and Date features.
These features are essential for training robust classification models to
distinguish between fake and authentic news articles. The initial model
employed a Long Short-Term Memory (LSTM) network, achieving an accuracy of 94%.
The second model improved upon this by incorporating additional regularization
techniques and fine-tuning hyperparameters, resulting in a 97% accuracy. The
final model combined the strengths of previous architectures with advanced
optimization strategies, achieving a peak accuracy of 98%. These results
demonstrate the effectiveness of our approach in identifying fake news with
high precision. Implementing these models showcases significant advancements in
natural language processing and machine learning techniques, contributing
valuable tools for combating misinformation. Our work highlights the potential
for deploying such models in real-world applications, providing a reliable
method for automated fake news detection and enhancing the credibility of news
dissemination.",2024-11-16,"Tanjina Sultana Camelia, Faizur Rahman Fahim, Md. Musfique Anwar",http://arxiv.org/pdf/2411.10713v1,cs.CL
Structured Dialogue System for Mental Health: An LLM Chatbot Leveraging the PM+ Guidelines,"The Structured Dialogue System, referred to as SuDoSys, is an innovative
Large Language Model (LLM)-based chatbot designed to provide psychological
counseling. SuDoSys leverages the World Health Organization (WHO)'s Problem
Management Plus (PM+) guidelines to deliver stage-aware multi-turn dialogues.
Existing methods for employing an LLM in multi-turn psychological counseling
typically involve direct fine-tuning using generated dialogues, often
neglecting the dynamic stage shifts of counseling sessions. Unlike previous
approaches, SuDoSys considers the different stages of counseling and stores
essential information throughout the counseling process, ensuring coherent and
directed conversations. The system employs an LLM, a stage-aware instruction
generator, a response unpacker, a topic database, and a stage controller to
maintain dialogue flow. In addition, we propose a novel technique that
simulates counseling clients to interact with the evaluated system and evaluate
its performance automatically. When assessed using both objective and
subjective evaluations, SuDoSys demonstrates its effectiveness in generating
logically coherent responses. The system's code and program scripts for
evaluation are open-sourced.",2024-11-16,"Yixiang Chen, Xinyu Zhang, Jinran Wang, Xurong Xie, Nan Yan, Hui Chen, Lan Wang",http://arxiv.org/pdf/2411.10681v1,cs.CL
A Novel Approach to Eliminating Hallucinations in Large Language Model-Assisted Causal Discovery,"The increasing use of large language models (LLMs) in causal discovery as a
substitute for human domain experts highlights the need for optimal model
selection. This paper presents the first hallucination survey of popular LLMs
for causal discovery. We show that hallucinations exist when using LLMs in
causal discovery so the choice of LLM is important. We propose using Retrieval
Augmented Generation (RAG) to reduce hallucinations when quality data is
available. Additionally, we introduce a novel method employing multiple LLMs
with an arbiter in a debate to audit edges in causal graphs, achieving a
comparable reduction in hallucinations to RAG.",2024-11-16,"Grace Sng, Yanming Zhang, Klaus Mueller",http://arxiv.org/pdf/2411.12759v1,cs.CL
IntentGPT: Few-shot Intent Discovery with Large Language Models,"In today's digitally driven world, dialogue systems play a pivotal role in
enhancing user interactions, from customer service to virtual assistants. In
these dialogues, it is important to identify user's goals automatically to
resolve their needs promptly. This has necessitated the integration of models
that perform Intent Detection. However, users' intents are diverse and dynamic,
making it challenging to maintain a fixed set of predefined intents. As a
result, a more practical approach is to develop a model capable of identifying
new intents as they emerge. We address the challenge of Intent Discovery, an
area that has drawn significant attention in recent research efforts. Existing
methods need to train on a substantial amount of data for correctly identifying
new intents, demanding significant human effort. To overcome this, we introduce
IntentGPT, a novel training-free method that effectively prompts Large Language
Models (LLMs) such as GPT-4 to discover new intents with minimal labeled data.
IntentGPT comprises an \textit{In-Context Prompt Generator}, which generates
informative prompts for In-Context Learning, an \textit{Intent Predictor} for
classifying and discovering user intents from utterances, and a
\textit{Semantic Few-Shot Sampler} that selects relevant few-shot examples and
a set of known intents to be injected into the prompt. Our experiments show
that IntentGPT outperforms previous methods that require extensive
domain-specific data and fine-tuning, in popular benchmarks, including CLINC
and BANKING, among others.",2024-11-16,"Juan A. Rodriguez, Nicholas Botzer, David Vazquez, Christopher Pal, Marco Pedersoli, Issam Laradji",http://arxiv.org/pdf/2411.10670v1,cs.CL
SAM Decoding: Speculative Decoding via Suffix Automaton,"Speculative decoding (SD) has been demonstrated as an effective technique for
lossless LLM inference acceleration. Retrieval-based SD methods, one kind of
model-free method, have yielded promising speedup, but they often rely on
incomplete retrieval resources, inefficient retrieval methods, and are
constrained to certain domains. This paper presents a novel retrieval-based
speculative decoding method that adapts suffix automaton (SAM) for efficient
and accurate draft generation by utilizing common text corpus and dynamic text
sequence. Unlike existing $n$-gram matching methods, SAM-Decoding finds the
exact longest suffix match, achieving an average time complexity of O(1) per
generation step of SAM update and suffix retrieval. It can also integrate with
existing methods, adaptively selecting a draft generation strategy based on
match length to generalize to broader domains. Extensive experiments on
Spec-Bench show that our method is $18\%+$ faster than other retrieval-based SD
methods. Additionally, when combined with advanced EAGLE-2, it provides an
additional speedup of $3.28\%$ -- $11.13\%$ across various-sized LLM backbones.
Our code is available at our
\href{https://github.com/hyx1999/SAM-Decoding}{repository}.",2024-11-16,"Yuxuan Hu, Ke Wang, Xiaokang Zhang, Fanjin Zhang, Cuiping Li, Hong Chen, Jing Zhang",http://arxiv.org/pdf/2411.10666v3,cs.CL
BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices,"The emergence and growing popularity of multimodal large language models
(MLLMs) have significant potential to enhance various aspects of daily life,
from improving communication to facilitating learning and problem-solving.
Mobile phones, as essential daily companions, represent the most effective and
accessible deployment platform for MLLMs, enabling seamless integration into
everyday tasks. However, deploying MLLMs on mobile phones presents challenges
due to limitations in memory size and computational capability, making it
difficult to achieve smooth and real-time processing without extensive
optimization. In this paper, we present BlueLM-V-3B, an algorithm and system
co-design approach specifically tailored for the efficient deployment of MLLMs
on mobile platforms. To be specific, we redesign the dynamic resolution scheme
adopted by mainstream MLLMs and implement system optimization for
hardware-aware deployment to optimize model inference on mobile phones.
BlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B
features a language model with 2.7B parameters and a vision encoder with 400M
parameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4
token/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight
quantization. (3) Strong Performance: BlueLM-V-3B has attained the highest
average score of 66.1 on the OpenCompass benchmark among models with $\leq$ 4B
parameters and surpassed a series of models with much larger parameter sizes
(e.g., MiniCPM-V-2.6, InternVL2-8B).",2024-11-16,"Xudong Lu, Yinghao Chen, Cheng Chen, Hui Tan, Boheng Chen, Yina Xie, Rui Hu, Guanxin Tan, Renshou Wu, Yan Hu, Yi Zeng, Lei Wu, Liuyang Bian, Zhaoxiong Wang, Long Liu, Yanzhou Yang, Han Xiao, Aojun Zhou, Yafei Wen, Xiaoxin Chen, Shuai Ren, Hongsheng Li",http://arxiv.org/pdf/2411.10640v1,cs.CL
MTA: Multimodal Task Alignment for BEV Perception and Captioning,"Bird's eye view (BEV)-based 3D perception plays a crucial role in autonomous
driving applications. The rise of large language models has spurred interest in
BEV-based captioning to understand object behavior in the surrounding
environment. However, existing approaches treat perception and captioning as
separate tasks, focusing on the performance of only one task and overlooking
the potential benefits of multimodal alignment. To bridge this gap between
modalities, we introduce MTA, a novel multimodal task alignment framework that
boosts both BEV perception and captioning. MTA consists of two key components:
(1) BEV-Language Alignment (BLA), a contextual learning mechanism that aligns
the BEV scene representations with ground-truth language representations, and
(2) Detection-Captioning Alignment (DCA), a cross-modal prompting mechanism
that aligns detection and captioning outputs. MTA seamlessly integrates into
state-of-the-art baselines during training, adding no extra computational
complexity at runtime. Extensive experiments on the nuScenes and TOD3Cap
datasets show that MTA significantly outperforms state-of-the-art baselines in
both tasks, achieving a 10.7% improvement in challenging rare perception
scenarios and a 9.2% improvement in captioning. These results underscore the
effectiveness of unified alignment in reconciling BEV-based perception and
captioning.",2024-11-16,"Yunsheng Ma, Burhaneddin Yaman, Xin Ye, Jingru Luo, Feng Tao, Abhirup Mallik, Ziran Wang, Liu Ren",http://arxiv.org/pdf/2411.10639v2,cs.CL
Gender Bias Mitigation for Bangla Classification Tasks,"In this study, we investigate gender bias in Bangla pretrained language
models, a largely under explored area in low-resource languages. To assess this
bias, we applied gender-name swapping techniques to existing datasets, creating
four manually annotated, task-specific datasets for sentiment analysis,
toxicity detection, hate speech detection, and sarcasm detection. By altering
names and gender-specific terms, we ensured these datasets were suitable for
detecting and mitigating gender bias. We then proposed a joint loss
optimization technique to mitigate gender bias across task-specific pretrained
models. Our approach was evaluated against existing bias mitigation methods,
with results showing that our technique not only effectively reduces bias but
also maintains competitive accuracy compared to other baseline approaches. To
promote further research, we have made both our implementation and datasets
publicly available
https://github.com/sajib-kumar/Gender-Bias-Mitigation-From-Bangla-PLM",2024-11-16,"Sajib Kumar Saha Joy, Arman Hassan Mahy, Meherin Sultana, Azizah Mamun Abha, MD Piyal Ahmmed, Yue Dong, G M Shahariar",http://arxiv.org/pdf/2411.10636v1,cs.CL
Leveraging large language models for efficient representation learning for entity resolution,"In this paper, the authors propose TriBERTa, a supervised entity resolution
system that utilizes a pre-trained large language model and a triplet loss
function to learn representations for entity matching. The system consists of
two steps: first, name entity records are fed into a Sentence Bidirectional
Encoder Representations from Transformers (SBERT) model to generate vector
representations, which are then fine-tuned using contrastive learning based on
a triplet loss function. Fine-tuned representations are used as input for
entity matching tasks, and the results show that the proposed approach
outperforms state-of-the-art representations, including SBERT without
fine-tuning and conventional Term Frequency-Inverse Document Frequency
(TF-IDF), by a margin of 3 - 19%. Additionally, the representations generated
by TriBERTa demonstrated increased robustness, maintaining consistently higher
performance across a range of datasets. The authors also discussed the
importance of entity resolution in today's data-driven landscape and the
challenges that arise when identifying and reconciling duplicate data across
different sources. They also described the ER process, which involves several
crucial steps, including blocking, entity matching, and clustering.",2024-11-15,"Xiaowei Xu, Bi T. Foua, Xingqiao Wang, Vivek Gunasekaran, John R. Talburt",http://arxiv.org/pdf/2411.10629v1,cs.CL
An exploration of the effect of quantisation on energy consumption and inference time of StarCoder2,"This study examines quantisation and pruning strategies to reduce energy
consumption in code Large Language Models (LLMs) inference. Using StarCoder2,
we observe increased energy demands with quantization due to lower throughput
and some accuracy losses. Conversely, pruning reduces energy usage but impairs
performance. The results highlight challenges and trade-offs in LLM model
compression. We suggest future work on hardware-optimized quantization to
enhance efficiency with minimal loss in accuracy.",2024-11-15,"Pepijn de Reus, Ana Oprescu, Jelle Zuidema",http://arxiv.org/pdf/2411.12758v1,cs.CL
A dataset of questions on decision-theoretic reasoning in Newcomb-like problems,"We introduce a dataset of natural-language questions in the decision theory
of so-called Newcomb-like problems. Newcomb-like problems include, for
instance, decision problems in which an agent interacts with a similar other
agent, and thus has to reason about the fact that the other agent will likely
reason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is
important because interactions between foundation-model-based agents will often
be Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow
for greater cooperation between models.
  Our dataset contains both capabilities questions (i.e., questions with a
unique, uncontroversially correct answer) and attitude questions (i.e.,
questions about which decision theorists would disagree). We use our dataset
for an investigation of decision-theoretical capabilities and expressed
attitudes and their interplay in existing models (different models by OpenAI,
Anthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based
interventions. We find, among other things, that attitudes vary significantly
between existing models; that high capabilities are associated with attitudes
more favorable toward so-called evidential decision theory; and that attitudes
are consistent across different types of questions.",2024-11-15,"Caspar Oesterheld, Emery Cooper, Miles Kodama, Linh Chi Nguyen, Ethan Perez",http://arxiv.org/pdf/2411.10588v3,cs.CL
On the Shortcut Learning in Multilingual Neural Machine Translation,"In this study, we revisit the commonly-cited off-target issue in multilingual
neural machine translation (MNMT). By carefully designing experiments on
different MNMT scenarios and models, we attribute the off-target issue to the
overfitting of the shortcuts of (non-centric, centric) language mappings.
Specifically, the learned shortcuts biases MNMT to mistakenly translate
non-centric languages into the centric language instead of the expected
non-centric language for zero-shot translation. Analyses on learning dynamics
show that the shortcut learning generally occurs in the later stage of model
training, and multilingual pretraining accelerates and aggravates the shortcut
learning. Based on these observations, we propose a simple and effective
training strategy to eliminate the shortcuts in MNMT models by leveraging the
forgetting nature of model training. The only difference from the standard
training is that we remove the training instances that may induce the shortcut
learning in the later stage of model training. Without introducing any
additional data and computational costs, our approach can consistently and
significantly improve the zero-shot translation performance by alleviating the
shortcut learning for different MNMT models and benchmarks.",2024-11-15,"Wenxuan Wang, Wenxiang Jiao, Jen-tse Huang, Zhaopeng Tu, Michael R. Lyu",http://arxiv.org/pdf/2411.10581v1,cs.CL
Hysteresis Activation Function for Efficient Inference,"The widely used ReLU is favored for its hardware efficiency, {as the
implementation at inference is a one bit sign case,} yet suffers from issues
such as the ``dying ReLU'' problem, where during training, neurons fail to
activate and constantly remain at zero, as highlighted by Lu et al. Traditional
approaches to mitigate this issue often introduce more complex and less
hardware-friendly activation functions. In this work, we propose a Hysteresis
Rectified Linear Unit (HeLU), an efficient activation function designed to
address the ``dying ReLU'' problem with minimal complexity. Unlike traditional
activation functions with fixed thresholds for training and inference, HeLU
employs a variable threshold that refines the backpropagation. This refined
mechanism allows simpler activation functions to achieve competitive
performance comparable to their more complex counterparts without introducing
unnecessary complexity or requiring inductive biases. Empirical evaluations
demonstrate that HeLU enhances model generalization across diverse datasets,
offering a promising solution for efficient and effective inference suitable
for a wide range of neural network architectures.",2024-11-15,"Moshe Kimhi, Idan Kashani, Avi Mendelson, Chaim Baskin",http://arxiv.org/pdf/2411.10573v2,cs.CL
Can Artificial Intelligence Generate Quality Research Topics Reflecting Patient Concerns?,"Patient-centered research is increasingly important in narrowing the gap
between research and patient care, yet incorporating patient perspectives into
health research has been inconsistent. We propose an automated framework
leveraging innovative natural language processing (NLP) and artificial
intelligence (AI) with patient portal messages to generate research ideas that
prioritize important patient issues. We further quantified the quality of
AI-generated research topics. To define patient clinical concerns, we analyzed
614,464 patient messages from 25,549 individuals with breast or skin cancer
obtained from a large academic hospital (2013 to 2024), constructing a 2-staged
unsupervised NLP topic model. Then, we generated research topics to resolve the
defined issues using a widely used AI (ChatGPT-4o, OpenAI Inc, April 2024
version) with prompt-engineering strategies. We guided AI to perform
multi-level tasks: 1) knowledge interpretation and summarization (e.g.,
interpreting and summarizing the NLP-defined topics), 2) knowledge generation
(e.g., generating research ideas corresponding to patients issues), 3)
self-reflection and correction (e.g., ensuring and revising the research ideas
after searching for scientific articles), and 4) self-reassurance (e.g.,
confirming and finalizing the research ideas). Six highly experienced breast
oncologists and dermatologists assessed the significance and novelty of
AI-generated research topics using a 5-point Likert scale (1-exceptional,
5-poor). One-third of the AI-suggested research topics were highly significant
and novel when both scores were lower than the average. Two-thirds of the
AI-suggested topics were novel in both cancers. Our findings demonstrate that
AI-generated research topics reflecting patient perspectives via a large volume
of patient messages can meaningfully guide future directions in
patient-centered health research.",2024-11-15,"Jiyeong Kim, Michael L. Chen, Shawheen J. Rezaei, Mariana Ramirez-Posada, Jennifer L. Caswell-Jin, Allison W. Kurian, Fauzia Riaz, Kavita Y. Sarin, Jean Y. Tang, Steven M. Asch, Eleni Linos",http://arxiv.org/pdf/2411.14456v1,cs.CL
MLAN: Language-Based Instruction Tuning Improves Zero-Shot Generalization of Multimodal Large Language Models,"We present a novel instruction tuning recipe to improve the zero-shot task
generalization of multimodal large language models. In contrast to existing
instruction tuning mechanisms that heavily rely on visual instructions, our
approach focuses on language-based instruction tuning, offering a distinct and
more training efficient path for multimodal instruction tuning. We evaluate the
performance of the proposed approach on 9 unseen datasets across both language
and vision modalities. Our results show that our language-only instruction
tuning is able to significantly improve the performance of two pretrained
multimodal models based on Llama 2 and Vicuna on those unseen datasets.
Interestingly, the language instruction following ability also helps unlock the
models to follow vision instructions without explicit training. Compared to the
state of the art multimodal instruction tuning approaches that are mainly based
on visual instructions, our language-based method not only achieves superior
performance but also significantly enhances training efficiency. For instance,
the language-only instruction tuning produces competitive average performance
across the evaluated datasets (with even better performance on language
datasets) with significant training efficiency improvements (on average 4x),
thanks to the striking reduction in the need for vision data. With a small
number of visual instructions, this emerging language instruction following
ability transfers well to the unseen vision datasets, outperforming the state
of the art with greater training efficiency.",2024-11-15,"Jianhong Tu, Zhuohao Ni, Nicholas Crispino, Zihao Yu, Michael Bendersky, Beliz Gunel, Ruoxi Jia, Xin Liu, Lingjuan Lyu, Dawn Song, Chenguang Wang",http://arxiv.org/pdf/2411.10557v2,cs.CL
Efficient Alignment of Large Language Models via Data Sampling,"LLM alignment ensures that large language models behave safely and
effectively by aligning their outputs with human values, goals, and intentions.
Aligning LLMs employ huge amounts of data, computation, and time. Moreover,
curating data with human feedback is expensive and takes time. Recent research
depicts the benefit of data engineering in the fine-tuning and pre-training
paradigms to bring down such costs. However, alignment differs from the
afore-mentioned paradigms and it is unclear if data efficient alignment is
feasible. In this work, we first aim to understand how the performance of LLM
alignment scales with data. We find out that LLM alignment performance follows
an exponential plateau pattern which tapers off post a rapid initial increase.
Based on this, we identify data subsampling as a viable method to reduce
resources required for alignment. Further, we propose an information
theory-based methodology for efficient alignment by identifying a small high
quality subset thereby reducing the computation and time required by alignment.
We evaluate the proposed methodology over multiple datasets and compare the
results. We find that the model aligned using our proposed methodology
outperforms other sampling methods and performs comparable to the model aligned
with the full dataset while using less than 10% data, leading to greater than
90% savings in costs, resources, and faster LLM alignment.",2024-11-15,"Amrit Khera, Rajat Ghosh, Debojyoti Dutta",http://arxiv.org/pdf/2411.10545v2,cs.CL
SoftLMs: Efficient Adaptive Low-Rank Approximation of Language Models using Soft-Thresholding Mechanism,"Extensive efforts have been made to boost the performance in the domain of
language models by introducing various attention-based transformers. However,
the inclusion of linear layers with large dimensions contributes to significant
computational and memory overheads. The escalating computational demands of
these models necessitate the development of various compression techniques to
ensure their deployment on devices, particularly in resource-constrained
environments. In this paper, we propose a novel compression methodology that
dynamically determines the rank of each layer using a soft thresholding
mechanism, which clips the singular values with a small magnitude in a
differentiable form. This approach automates the decision-making process to
identify the optimal degree of compression for each layer. We have successfully
applied the proposed technique to attention-based architectures, including BERT
for discriminative tasks and GPT2 and TinyLlama for generative tasks.
Additionally, we have validated our method on Mamba, a recently proposed
state-space model. Our experiments demonstrate that the proposed technique
achieves a speed-up of 1.33X to 1.72X in the encoder/ decoder with a 50%
reduction in total parameters.",2024-11-15,"Priyansh Bhatnagar, Linfeng Wen, Mingu Kang",http://arxiv.org/pdf/2411.10543v1,cs.CL
Does Prompt Formatting Have Any Impact on LLM Performance?,"In the realm of Large Language Models (LLMs), prompt optimization is crucial
for model performance. Although previous research has explored aspects like
rephrasing prompt contexts, using various prompting techniques (like in-context
learning and chain-of-thought), and ordering few-shot examples, our
understanding of LLM sensitivity to prompt templates remains limited.
Therefore, this paper examines the impact of different prompt templates on LLM
performance. We formatted the same contexts into various human-readable
templates, including plain text, Markdown, JSON, and YAML, and evaluated their
impact across tasks like natural language reasoning, code generation, and
translation using OpenAI's GPT models. Experiments show that GPT-3.5-turbo's
performance varies by up to 40\% in a code translation task depending on the
prompt template, while larger models like GPT-4 are more robust to these
variations. Our analysis highlights the need to reconsider the use of fixed
prompt templates, as different formats can significantly affect model
performance.",2024-11-15,"Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X Wang, Sadid Hasan",http://arxiv.org/pdf/2411.10541v1,cs.CL
On the Compatibility of Generative AI and Generative Linguistics,"In mid-20th century, the linguist Noam Chomsky established generative
linguistics, and made significant contributions to linguistics, computer
science, and cognitive science by developing the computational and
philosophical foundations for a theory that defined language as a formal
system, instantiated in human minds or artificial machines. These developments
in turn ushered a wave of research on symbolic Artificial Intelligence (AI).
More recently, a new wave of non-symbolic AI has emerged with neural Language
Models (LMs) that exhibit impressive linguistic performance, leading many to
question the older approach and wonder about the the compatibility of
generative AI and generative linguistics. In this paper, we argue that
generative AI is compatible with generative linguistics and reinforces its
basic tenets in at least three ways. First, we argue that LMs are formal
generative models as intended originally in Chomsky's work on formal language
theory. Second, LMs can help develop a program for discovery procedures as
defined by Chomsky's ""Syntactic Structures"". Third, LMs can be a major asset
for Chomsky's minimalist approach to Universal Grammar and language
acquisition. In turn, generative linguistics can provide the foundation for
evaluating and improving LMs as well as other generative computational models
of language.",2024-11-15,"Eva Portelance, Masoud Jasbi",http://arxiv.org/pdf/2411.10533v2,cs.CL
Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization,"Existing open-source multimodal large language models (MLLMs) generally
follow a training process involving pre-training and supervised fine-tuning.
However, these models suffer from distribution shifts, which limit their
multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance.
To address this, we introduce a preference optimization (PO) process to enhance
the multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data
side, we design an automated preference data construction pipeline to create
MMPR, a high-quality, large-scale multimodal reasoning preference dataset; and
(2) on the model side, we explore integrating PO with MLLMs, developing a
simple yet effective method, termed Mixed Preference Optimization (MPO), which
boosts multimodal CoT performance. Our approach enhances the multimodal
reasoning abilities of both InternVL2-8B and InternVL2-76B. Notably, our model,
InternVL2-8B-MPO, achieves an accuracy of 67.0 on MathVista, outperforming
InternVL2-8B by 8.7 points and achieving performance comparable to the
10$\times$ larger InternVL2-76B. We hope this study could inspire further
advancements in MLLMs. Code, data, and model are released.",2024-11-15,"Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, Jifeng Dai",http://arxiv.org/pdf/2411.10442v2,cs.CL
Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization,"Multimodal Large Language Models (MLLMs) are known to hallucinate, which
limits their practical applications. Recent works have attempted to apply
Direct Preference Optimization (DPO) to enhance the performance of MLLMs, but
have shown inconsistent improvements in mitigating hallucinations. To address
this issue more effectively, we introduce Hallucination-targeted Direct
Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike
previous approaches, our method tackles hallucinations from their diverse forms
and causes. Specifically, we develop three types of preference pair data
targeting the following causes of MLLM hallucinations: (1) insufficient visual
capabilities, (2) long context generation, and (3) multimodal conflicts.
Experimental results demonstrate that our method achieves superior performance
across multiple hallucination evaluation datasets, surpassing most
state-of-the-art (SOTA) methods and highlighting the potential of our approach.
Ablation studies and in-depth analyses further confirm the effectiveness of our
method and suggest the potential for further improvements through scaling up.",2024-11-15,"Yuhan Fu, Ruobing Xie, Xingwu Sun, Zhanhui Kang, Xirong Li",http://arxiv.org/pdf/2411.10436v1,cs.CL
Towards Automatic Evaluation of Task-Oriented Dialogue Flows,"Task-oriented dialogue systems rely on predefined conversation schemes
(dialogue flows) often represented as directed acyclic graphs. These flows can
be manually designed or automatically generated from previously recorded
conversations. Due to variations in domain expertise or reliance on different
sets of prior conversations, these dialogue flows can manifest in significantly
different graph structures. Despite their importance, there is no standard
method for evaluating the quality of dialogue flows. We introduce FuDGE (Fuzzy
Dialogue-Graph Edit Distance), a novel metric that evaluates dialogue flows by
assessing their structural complexity and representational coverage of the
conversation data. FuDGE measures how well individual conversations align with
a flow and, consequently, how well a set of conversations is represented by the
flow overall. Through extensive experiments on manually configured flows and
flows generated by automated techniques, we demonstrate the effectiveness of
FuDGE and its evaluation framework. By standardizing and optimizing dialogue
flows, FuDGE enables conversational designers and automated techniques to
achieve higher levels of efficiency and automation.",2024-11-15,"Mehrnoosh Mirtaheri, Nikhil Varghese, Chandra Khatri, Amol Kelkar",http://arxiv.org/pdf/2411.10416v1,cs.CL
Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations,"We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for
human-AI conversations that involves image understanding: it can be used to
safeguard content for both multimodal LLM inputs (prompt classification) and
outputs (response classification). Unlike the previous text-only Llama Guard
versions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed
to support image reasoning use cases and is optimized to detect harmful
multimodal (text and image) prompts and text responses to these prompts. Llama
Guard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong
performance on the internal benchmarks using the MLCommons taxonomy. We also
test its robustness against adversarial attacks. We believe that Llama Guard 3
Vision serves as a good starting point to build more capable and robust content
moderation tools for human-AI conversation with multimodal capabilities.",2024-11-15,"Jianfeng Chi, Ujjwal Karn, Hongyuan Zhan, Eric Smith, Javier Rando, Yiming Zhang, Kate Plawiak, Zacharie Delpierre Coudert, Kartikeya Upasani, Mahesh Pasupuleti",http://arxiv.org/pdf/2411.10414v1,cs.CL
Features that Make a Difference: Leveraging Gradients for Improved Dictionary Learning,"Sparse Autoencoders (SAEs) are a promising approach for extracting neural
network representations by learning a sparse and overcomplete decomposition of
the network's internal activations. However, SAEs are traditionally trained
considering only activation values and not the effect those activations have on
downstream computations. This limits the information available to learn
features, and biases the autoencoder towards neglecting features which are
represented with small activation values but strongly influence model outputs.
To address this, we introduce Gradient SAEs (g-SAEs), which modify the
$k$-sparse autoencoder architecture by augmenting the TopK activation function
to rely on the gradients of the input activation when selecting the $k$
elements. For a given sparsity level, g-SAEs produce reconstructions that are
more faithful to original network performance when propagated through the
network. Additionally, we find evidence that g-SAEs learn latents that are on
average more effective at steering models in arbitrary contexts. By considering
the downstream effects of activations, our approach leverages the dual nature
of neural network features as both $\textit{representations}$, retrospectively,
and $\textit{actions}$, prospectively. While previous methods have approached
the problem of feature discovery primarily focused on the former aspect, g-SAEs
represent a step towards accounting for the latter as well.",2024-11-15,"Jeffrey Olmo, Jared Wilson, Max Forsey, Bryce Hepner, Thomas Vin Howe, David Wingate",http://arxiv.org/pdf/2411.10397v2,cs.CL
"A Survey of Event Causality Identification: Principles, Taxonomy, Challenges, and Assessment","Event Causality Identification (ECI) has become a crucial task in Natural
Language Processing (NLP), aimed at automatically extracting causalities from
textual data. In this survey, we systematically address the foundational
principles, technical frameworks, and challenges of ECI, offering a
comprehensive taxonomy to categorize and clarify current research
methodologies, as well as a quantitative assessment of existing models. We
first establish a conceptual framework for ECI, outlining key definitions,
problem formulations, and evaluation standards. Our taxonomy classifies ECI
methods according to the two primary tasks of sentence-level (SECI) and
document-level (DECI) event causality identification. For SECI, we examine
feature pattern-based matching, deep semantic encoding, causal knowledge
pre-training and prompt-based fine-tuning, and external knowledge enhancement
methods. For DECI, we highlight approaches focused on event graph reasoning and
prompt-based techniques to address the complexity of cross-sentence causal
inference. Additionally, we analyze the strengths, limitations, and open
challenges of each approach. We further conduct an extensive quantitative
evaluation of various ECI methods on two benchmark datasets. Finally, we
explore future research directions, highlighting promising pathways to overcome
current limitations and broaden ECI applications.",2024-11-15,"Qing Cheng, Zefan Zeng, Xingchen Hu, Yuehang Si, Zhong Liu",http://arxiv.org/pdf/2411.10371v2,cs.CL
Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding,"In recent years, text-to-image (T2I) generation models have made significant
progress in generating high-quality images that align with text descriptions.
However, these models also face the risk of unsafe generation, potentially
producing harmful content that violates usage policies, such as explicit
material. Existing safe generation methods typically focus on suppressing
inappropriate content by erasing undesired concepts from visual
representations, while neglecting to sanitize the textual representation.
Although these methods help mitigate the risk of misuse to some extent, their
robustness remains insufficient when dealing with adversarial attacks.
  Given that semantic consistency between input text and output image is a core
requirement of T2I models, we identify that textual representations are likely
the primary source of unsafe generation. To this end, we propose Embedding
Sanitizer (ES), which enhances the safety of T2I models by sanitizing
inappropriate concepts in prompt embeddings. To our knowledge, ES is the first
interpretable safe generation framework that assigns a score to each token in
the prompt to indicate its potential harmfulness. In addition, ES adopts a
plug-and-play modular design, offering compatibility for seamless integration
with various T2I models and other safeguards. Evaluations on five prompt
benchmarks show that ES outperforms eleven existing safeguard baselines,
achieving state-of-the-art robustness while maintaining high-quality image
generation.",2024-11-15,"Huming Qiu, Guanxu Chen, Mi Zhang, Xiaohan Zhang, Xiaoyu You, Min Yang",http://arxiv.org/pdf/2411.10329v2,cs.CL
Emotion Detection in Reddit: Comparative Study of Machine Learning and Deep Learning Techniques,"Emotion detection is pivotal in human communication, as it significantly
influences behavior, relationships, and decision-making processes. This study
concentrates on text-based emotion detection by leveraging the GoEmotions
dataset, which annotates Reddit comments with 27 distinct emotions. These
emotions are subsequently mapped to Ekman's six basic categories: joy, anger,
fear, sadness, disgust, and surprise. We employed a range of models for this
task, including six machine learning models, three ensemble models, and a Long
Short-Term Memory (LSTM) model to determine the optimal model for emotion
detection. Results indicate that the Stacking classifier outperforms other
models in accuracy and performance. We also benchmark our models against
EmoBERTa, a pre-trained emotion detection model, with our Stacking classifier
proving more effective. Finally, the Stacking classifier is deployed via a
Streamlit web application, underscoring its potential for real-world
applications in text-based emotion analysis.",2024-11-15,Maliheh Alaeddini,http://arxiv.org/pdf/2411.10328v1,cs.CL
The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use,"The recently released model, Claude 3.5 Computer Use, stands out as the first
frontier AI model to offer computer use in public beta as a graphical user
interface (GUI) agent. As an early beta, its capability in the real-world
complex environment remains unknown. In this case study to explore Claude 3.5
Computer Use, we curate and organize a collection of carefully designed tasks
spanning a variety of domains and software. Observations from these cases
demonstrate Claude 3.5 Computer Use's unprecedented ability in end-to-end
language to desktop actions. Along with this study, we provide an
out-of-the-box agent framework for deploying API-based GUI automation models
with easy implementation. Our case studies aim to showcase a groundwork of
capabilities and limitations of Claude 3.5 Computer Use with detailed analyses
and bring to the fore questions about planning, action, and critic, which must
be considered for future improvement. We hope this preliminary exploration will
inspire future research into the GUI agent community. All the test cases in the
paper can be tried through the project:
https://github.com/showlab/computer_use_ootb.",2024-11-15,"Siyuan Hu, Mingyu Ouyang, Difei Gao, Mike Zheng Shou",http://arxiv.org/pdf/2411.10323v1,cs.CL
Unveiling Topological Structures in Text: A Comprehensive Survey of Topological Data Analysis Applications in NLP,"The surge of data available on the internet has led to the adoption of
various computational methods to analyze and extract valuable insights from
this wealth of information. Among these, the field of Machine Learning (ML) has
thrived by leveraging data to extract meaningful insights. However, ML
techniques face notable challenges when dealing with real-world data, often due
to issues of imbalance, noise, insufficient labeling, and high dimensionality.
To address these limitations, some researchers advocate for the adoption of
Topological Data Analysis (TDA), a statistical approach that discerningly
captures the intrinsic shape of data despite noise. Despite its potential, TDA
has not gained as much traction within the Natural Language Processing (NLP)
domain compared to structurally distinct areas like computer vision.
Nevertheless, a dedicated community of researchers has been exploring the
application of TDA in NLP, yielding 87 papers we comprehensively survey in this
paper. Our findings categorize these efforts into theoretical and
non-theoretical approaches. Theoretical approaches aim to explain linguistic
phenomena from a topological viewpoint, while non-theoretical approaches merge
TDA with ML features, utilizing diverse numerical representation techniques. We
conclude by exploring the challenges and unresolved questions that persist in
this niche field. Resources and a list of papers on this topic can be found at:
https://github.com/AdaUchendu/AwesomeTDA4NLP.",2024-11-15,"Adaku Uchendu, Thai Le",http://arxiv.org/pdf/2411.10298v2,cs.CL
P$^2$ Law: Scaling Law for Post-Training After Model Pruning,"Pruning has become a widely adopted technique for reducing the hardware
requirements of large language models (LLMs). To recover model performance
after pruning, post-training is commonly employed to mitigate the resulting
performance degradation. While post-training benefits from larger datasets,
once the dataset size is already substantial, increasing the training data
provides only limited performance gains. To balance post-training cost and
model performance, it is necessary to explore the optimal amount of
post-training data.Through extensive experiments on the Llama-3 and Qwen-2.5
series models, pruned using various common pruning methods, we uncover the
scaling \textbf{Law} for \textbf{P}ost-training after model \textbf{P}runing,
referred to as the P$^2$ Law.This law identifies four key factors for
predicting the pruned model's post-training loss: the model size before
pruning, the number of post-training tokens, the pruning rate, and the model's
loss before pruning. Moreover, P$^2$ Law can generalize to larger dataset
sizes, larger model sizes, and higher pruning rates, offering valuable insights
for the post-training of pruned LLMs.",2024-11-15,"Xiaodong Chen, Yuxuan Hu, Xiaokang Zhang, Yanling Wang, Cuiping Li, Hong Chen, Jing Zhang",http://arxiv.org/pdf/2411.10272v3,cs.CL
Automated Coding of Communications in Collaborative Problem-solving Tasks Using ChatGPT,"Collaborative problem solving (CPS) is widely recognized as a critical
21st-century skill. Assessing CPS depends heavily on coding the communication
data using a construct-relevant framework, and this process has long been a
major bottleneck to scaling up such assessments. Based on five datasets and two
coding frameworks, we demonstrate that ChatGPT can code communication data to a
satisfactory level, though performance varies across ChatGPT models, and
depends on the coding framework and task characteristics. Interestingly, newer
reasoning-focused models such as GPT-o1-mini and GPT-o3-mini do not necessarily
yield better coding results. Additionally, we show that refining prompts based
on feedback from miscoded cases can improve coding accuracy in some instances,
though the effectiveness of this approach is not consistent across all tasks.
These findings offer practical guidance for researchers and practitioners in
developing scalable, efficient methods to analyze communication data in support
of 21st-century skill assessment.",2024-11-15,"Jiangang Hao, Wenju Cui, Patrick Kyllonen, Emily Kerzabi, Lei Liu, Michael Flor",http://arxiv.org/pdf/2411.10246v3,cs.CL
Measuring Non-Adversarial Reproduction of Training Data in Large Language Models,"Large language models memorize parts of their training data. Memorizing short
snippets and facts is required to answer questions about the world and to be
fluent in any language. But models have also been shown to reproduce long
verbatim sequences of memorized text when prompted by a motivated adversary. In
this work, we investigate an intermediate regime of memorization that we call
non-adversarial reproduction, where we quantify the overlap between model
responses and pretraining data when responding to natural and benign prompts.
For a variety of innocuous prompt categories (e.g., writing a letter or a
tutorial), we show that up to 15% of the text output by popular conversational
language models overlaps with snippets from the Internet. In worst cases, we
find generations where 100% of the content can be found exactly online. For the
same tasks, we find that human-written text has far less overlap with Internet
data. We further study whether prompting strategies can close this reproduction
gap between models and humans. While appropriate prompting can reduce
non-adversarial reproduction on average, we find that mitigating worst-case
reproduction of training data requires stronger defenses -- even for benign
interactions.",2024-11-15,"Michael Aerni, Javier Rando, Edoardo Debenedetti, Nicholas Carlini, Daphne Ippolito, Florian Tramèr",http://arxiv.org/pdf/2411.10242v1,cs.CL
Entropy and type-token ratio in gigaword corpora,"There are different ways of measuring diversity in complex systems. In
particular, in language, lexical diversity is characterized in terms of the
type-token ratio and the word entropy. We here investigate both diversity
metrics in six massive linguistic datasets in English, Spanish, and Turkish,
consisting of books, news articles, and tweets. These gigaword corpora
correspond to languages with distinct morphological features and differ in
registers and genres, thus constituting a varied testbed for a quantitative
approach to lexical diversity. We unveil an empirical functional relation
between entropy and type-token ratio of texts of a given corpus and language,
which is a consequence of the statistical laws observed in natural language.
Further, in the limit of large text lengths we find an analytical expression
for this relation relying on both Zipf and Heaps laws that agrees with our
empirical findings.",2024-11-15,"Pablo Rosillo-Rodes, Maxi San Miguel, David Sanchez",http://arxiv.org/pdf/2411.10227v2,cs.CL
Increasing the Accessibility of Causal Domain Knowledge via Causal Information Extraction Methods: A Case Study in the Semiconductor Manufacturing Industry,"The extraction of causal information from textual data is crucial in the
industry for identifying and mitigating potential failures, enhancing process
efficiency, prompting quality improvements, and addressing various operational
challenges. This paper presents a study on the development of automated methods
for causal information extraction from actual industrial documents in the
semiconductor manufacturing industry. The study proposes two types of causal
information extraction methods, single-stage sequence tagging (SST) and
multi-stage sequence tagging (MST), and evaluates their performance using
existing documents from a semiconductor manufacturing company, including
presentation slides and FMEA (Failure Mode and Effects Analysis) documents. The
study also investigates the effect of representation learning on downstream
tasks. The presented case study showcases that the proposed MST methods for
extracting causal information from industrial documents are suitable for
practical applications, especially for semi structured documents such as FMEAs,
with a 93\% F1 score. Additionally, MST achieves a 73\% F1 score on texts
extracted from presentation slides. Finally, the study highlights the
importance of choosing a language model that is more aligned with the domain
and in-domain fine-tuning.",2024-11-15,"Houssam Razouk, Leonie Benischke, Daniel Garber, Roman Kern",http://arxiv.org/pdf/2411.10172v1,cs.CL
Evaluating the role of `Constitutions' for learning from AI feedback,"The growing capabilities of large language models (LLMs) have led to their
use as substitutes for human feedback for training and assessing other LLMs.
These methods often rely on `constitutions', written guidelines which a critic
model uses to provide feedback and improve generations. We investigate how the
choice of constitution affects feedback quality by using four different
constitutions to improve patient-centered communication in medical interviews.
In pairwise comparisons conducted by 215 human raters, we found that detailed
constitutions led to better results regarding emotive qualities. However, none
of the constitutions outperformed the baseline in learning more
practically-oriented skills related to information gathering and provision. Our
findings indicate that while detailed constitutions should be prioritised,
there are possible limitations to the effectiveness of AI feedback as a reward
signal in certain areas.",2024-11-15,"Saskia Redgate, Andrew M. Bean, Adam Mahdi",http://arxiv.org/pdf/2411.10168v1,cs.CL
Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions,"Large language models (LLMs) demonstrate remarkable performance across
various tasks, prompting researchers to develop diverse evaluation benchmarks.
However, existing benchmarks typically measure the ability of LLMs to respond
to individual questions, neglecting the complex interactions in real-world
applications. In this paper, we introduce Compound Question Synthesis (CQ-Syn)
to create the Compound-QA benchmark, focusing on compound questions with
multiple sub-questions. This benchmark is derived from existing QA datasets,
annotated with proprietary LLMs and verified by humans for accuracy. It
encompasses five categories: Factual-Statement, Cause-and-Effect,
Hypothetical-Analysis, Comparison-and-Selection, and Evaluation-and-Suggestion.
It evaluates the LLM capability in terms of three dimensions including
understanding, reasoning, and knowledge. Our assessment of eight open-source
LLMs using Compound-QA reveals distinct patterns in their responses to compound
questions, which are significantly poorer than those to non-compound questions.
Additionally, we investigate various methods to enhance LLMs performance on
compound questions. The results indicate that these approaches significantly
improve the models' comprehension and reasoning abilities on compound
questions.",2024-11-15,"Yutao Hou, Yajing Luo, Zhiwen Ruan, Hongru Wang, Weifeng Ge, Yun Chen, Guanhua Chen",http://arxiv.org/pdf/2411.10163v1,cs.CL
Everything is a Video: Unifying Modalities through Next-Frame Prediction,"Multimodal learning, which involves integrating information from various
modalities such as text, images, audio, and video, is pivotal for numerous
complex tasks like visual question answering, cross-modal retrieval, and
caption generation. Traditional approaches rely on modality-specific encoders
and late fusion techniques, which can hinder scalability and flexibility when
adapting to new tasks or modalities. To address these limitations, we introduce
a novel framework that extends the concept of task reformulation beyond natural
language processing (NLP) to multimodal learning. We propose to reformulate
diverse multimodal tasks into a unified next-frame prediction problem, allowing
a single model to handle different modalities without modality-specific
components. This method treats all inputs and outputs as sequential frames in a
video, enabling seamless integration of modalities and effective knowledge
transfer across tasks. Our approach is evaluated on a range of tasks, including
text-to-text, image-to-text, video-to-video, video-to-text, and audio-to-text,
demonstrating the model's ability to generalize across modalities with minimal
adaptation. We show that task reformulation can significantly simplify
multimodal model design across various tasks, laying the groundwork for more
generalized multimodal foundation models.",2024-11-15,"G. Thomas Hudson, Dean Slack, Thomas Winterbottom, Jamie Sterling, Chenghao Xiao, Junjie Shentu, Noura Al Moubayed",http://arxiv.org/pdf/2411.10503v1,cs.CL
An Effective Framework to Help Large Language Models Handle Numeric-involved Long-context Tasks,"Large Language Models (LLMs) have demonstrated remarkable capabilities in
handling long texts and have almost perfect performance in traditional
retrieval tasks. However, their performance significantly degrades when it
comes to numerical calculations in the long-context. Numeric-involved
long-context tasks typically cannot be addressed by current LLMs in normal
settings due to their inherent limitations in simultaneously handling complex
and massive information. Some CoT like prompting methods can improve accuracy
but demands massive output tokens, which is costly and slow. To address this
issue, we propose a workflow, which decompose a numeric-involved long-context
task into 4 low-level subtasks: judging, extracting and processing with code
and conclusion. The former 2 subtasks is relatively simple, which allows us to
use smaller models for efficiently processing long context. When numerical
calculations are required, we use code generated by LLMs to avoid the
disadvantage of LLM not being good at calculations. The results in 2
numeric-involved long-context benchmarks demonstrate our workflow can not only
improve accuracy, but also significantly reduce the cost of API calls.",2024-11-15,Yijiong Yu,http://arxiv.org/pdf/2411.10145v2,cs.CL
Legal Evalutions and Challenges of Large Language Models,"In this paper, we review legal testing methods based on Large Language Models
(LLMs), using the OPENAI o1 model as a case study to evaluate the performance
of large models in applying legal provisions. We compare current
state-of-the-art LLMs, including open-source, closed-source, and legal-specific
models trained specifically for the legal domain. Systematic tests are
conducted on English and Chinese legal cases, and the results are analyzed in
depth. Through systematic testing of legal cases from common law systems and
China, this paper explores the strengths and weaknesses of LLMs in
understanding and applying legal texts, reasoning through legal issues, and
predicting judgments. The experimental results highlight both the potential and
limitations of LLMs in legal applications, particularly in terms of challenges
related to the interpretation of legal language and the accuracy of legal
reasoning. Finally, the paper provides a comprehensive analysis of the
advantages and disadvantages of various types of models, offering valuable
insights and references for the future application of AI in the legal field.",2024-11-15,"Jiaqi Wang, Huan Zhao, Zhenyuan Yang, Peng Shu, Junhao Chen, Haobo Sun, Ruixi Liang, Shixin Li, Pengcheng Shi, Longjun Ma, Zongjia Liu, Zhengliang Liu, Tianyang Zhong, Yutong Zhang, Chong Ma, Xin Zhang, Tuo Zhang, Tianli Ding, Yudan Ren, Tianming Liu, Xi Jiang, Shu Zhang",http://arxiv.org/pdf/2411.10137v1,cs.CL
Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation,"Generating accurate code review comments remains a significant challenge due
to the inherently diverse and non-unique nature of the task output. Large
language models pretrained on both programming and natural language data tend
to perform well in code-oriented tasks. However, large-scale pretraining is not
always feasible due to its environmental impact and project-specific
generalizability issues. In this work, first we fine-tune open-source Large
language models (LLM) in parameter-efficient, quantized low-rank (QLoRA)
fashion on consumer-grade hardware to improve review comment generation. Recent
studies demonstrate the efficacy of augmenting semantic metadata information
into prompts to boost performance in other code-related tasks. To explore this
in code review activities, we also prompt proprietary, closed-source LLMs
augmenting the input code patch with function call graphs and code summaries.
Both of our strategies improve the review comment generation performance, with
function call graph augmented few-shot prompting on the GPT-3.5 model
surpassing the pretrained baseline by around 90% BLEU-4 score on the
CodeReviewer dataset. Moreover, few-shot prompted Gemini-1.0 Pro, QLoRA
fine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging
from 25% to 83% performance improvement) on this task. An additional human
evaluation study further validates our experimental findings, reflecting
real-world developers' perceptions of LLM-generated code review comments based
on relevant qualitative metrics.",2024-11-15,"Md. Asif Haider, Ayesha Binte Mostofa, Sk. Sabit Bin Mosaddek, Anindya Iqbal, Toufique Ahmed",http://arxiv.org/pdf/2411.10129v1,cs.CL
Memorization in Attention-only Transformers,"Recent research has explored the memorization capacity of multi-head
attention, but these findings are constrained by unrealistic limitations on the
context size. We present a novel proof for language-based Transformers that
extends the current hypothesis to any context size. Our approach improves upon
the state-of-the-art by achieving more effective exact memorization with an
attention layer, while also introducing the concept of approximate memorization
of distributions. Through experimental validation, we demonstrate that our
proposed bounds more accurately reflect the true memorization capacity of
language models, and provide a precise comparison with prior work.",2024-11-15,"Léo Dana, Muni Sreenivas Pydi, Yann Chevaleyre",http://arxiv.org/pdf/2411.10115v2,cs.CL
Xmodel-1.5: An 1B-scale Multilingual LLM,"We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language
model pretrained on 2 trillion tokens, designed for balanced performance and
scalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5
employs a custom unigram tokenizer with 65,280 tokens, optimizing both
efficiency and accuracy. The model delivers competitive results across multiple
languages, including Thai, Arabic, French, Chinese, and English, outperforming
Alibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in
benchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.
To support low-resource language research, we release Xdata_Thai, a
Thai-specific evaluation dataset featuring unique linguistic challenges such as
gendered particles and idioms. While the model demonstrates strong performance,
there is still room for improvement in handling culturally specific nuances. We
hope this work contributes to advancements in multilingual AI research. Models
and code are publicly available on GitHub at
https://github.com/XiaoduoAILab/XmodelLM-1.5",2024-11-15,"Wang Qun, Liu Yang, Lin Qingquan, Jiang Ling",http://arxiv.org/pdf/2411.10083v3,cs.CL
Understanding The Effect Of Temperature On Alignment With Human Opinions,"With the increasing capabilities of LLMs, recent studies focus on
understanding whose opinions are represented by them and how to effectively
extract aligned opinion distributions. We conducted an empirical analysis of
three straightforward methods for obtaining distributions and evaluated the
results across a variety of metrics. Our findings suggest that sampling and
log-probability approaches with simple parameter adjustments can return better
aligned outputs in subjective tasks compared to direct prompting. Yet, assuming
models reflect human opinions may be limiting, highlighting the need for
further research on how human subjectivity affects model uncertainty.",2024-11-15,"Maja Pavlovic, Massimo Poesio",http://arxiv.org/pdf/2411.10080v1,cs.CL
Layer Importance and Hallucination Analysis in Large Language Models via Enhanced Activation Variance-Sparsity,"Evaluating the importance of different layers in large language models (LLMs)
is crucial for optimizing model performance and interpretability. This paper
first explores layer importance using the Activation Variance-Sparsity Score
(AVSS), which combines normalized activation variance and sparsity to quantify
each layer's contribution to overall model performance. By ranking layers based
on AVSS and pruning the least impactful 25\%, our experiments on tasks such as
question answering, language modeling, and sentiment classification show that
over 90\% of the original performance is retained, highlighting potential
redundancies in LLM architectures. Building on AVSS, we propose an enhanced
version tailored to assess hallucination propensity across layers (EAVSS). This
improved approach introduces Hallucination-Specific Activation Variance (HSAV)
and Hallucination-Specific Sparsity (HSS) metrics, allowing precise
identification of hallucination-prone layers. By incorporating contrastive
learning on these layers, we effectively mitigate hallucination generation,
contributing to more robust and efficient LLMs(The maximum performance
improvement is 12\%). Our results on the NQ, SciQ, TriviaQA, TruthfulQA, and
WikiQA datasets demonstrate the efficacy of this method, offering a
comprehensive framework for both layer importance evaluation and hallucination
mitigation in LLMs.",2024-11-15,"Zichen Song, Sitan Huang, Yuxin Wu, Zhongfeng Kang",http://arxiv.org/pdf/2411.10069v1,cs.CL
CMATH: Cross-Modality Augmented Transformer with Hierarchical Variational Distillation for Multimodal Emotion Recognition in Conversation,"Multimodal emotion recognition in conversation (MER) aims to accurately
identify emotions in conversational utterances by integrating multimodal
information. Previous methods usually treat multimodal information as equal
quality and employ symmetric architectures to conduct multimodal fusion.
However, in reality, the quality of different modalities usually varies
considerably, and utilizing a symmetric architecture is difficult to accurately
recognize conversational emotions when dealing with uneven modal information.
Furthermore, fusing multi-modality information in a single granularity may fail
to adequately integrate modal information, exacerbating the inaccuracy in
emotion recognition. In this paper, we propose a novel Cross-Modality Augmented
Transformer with Hierarchical Variational Distillation, called CMATH, which
consists of two major components, i.e., Multimodal Interaction Fusion and
Hierarchical Variational Distillation. The former is comprised of two
submodules, including Modality Reconstruction and Cross-Modality Augmented
Transformer (CMA-Transformer), where Modality Reconstruction focuses on
obtaining high-quality compressed representation of each modality, and
CMA-Transformer adopts an asymmetric fusion strategy which treats one modality
as the central modality and takes others as auxiliary modalities. The latter
first designs a variational fusion network to fuse the fine-grained
representations learned by CMA- Transformer into a coarse-grained
representations. Then, it introduces a hierarchical distillation framework to
maintain the consistency between modality representations with different
granularities. Experiments on the IEMOCAP and MELD datasets demonstrate that
our proposed model outperforms previous state-of-the-art baselines.
Implementation codes can be available at https://github.com/ cjw-MER/CMATH.",2024-11-15,"Xiaofei Zhu, Jiawei Cheng, Zhou Yang, Zhuo Chen, Qingyang Wang, Jianfeng Yao",http://arxiv.org/pdf/2411.10060v1,cs.CL
Towards unearthing neglected climate innovations from scientific literature using Large Language Models,"Climate change poses an urgent global threat, needing the rapid
identification and deployment of innovative solutions. We hypothesise that many
of these solutions already exist within scientific literature but remain
underutilised. To address this gap, this study employs a curated dataset
sourced from OpenAlex, a comprehensive repository of scientific papers.
Utilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate
title-abstract pairs from scientific papers on seven dimensions, covering
climate change mitigation potential, stage of technological development, and
readiness for deployment. The outputs of the language models are then compared
with human evaluations to assess their effectiveness in identifying promising
yet overlooked climate innovations. Our findings suggest that these LLM-based
models can effectively augment human expertise, uncovering climate solutions
that are potentially impactful but with far greater speed, throughput and
consistency. Here, we focused on UK-based solutions, but the workflow is
region-agnostic. This work contributes to the discovery of neglected
innovations in scientific literature and demonstrates the potential of AI in
enhancing climate action strategies.",2024-11-15,"César Quilodrán-Casas, Christopher Waite, Nicole Alhadeff, Diyona Dsouza, Cathal Hughes, Larissa Kunstel-Tabet, Alyssa Gilbert",http://arxiv.org/pdf/2411.10055v1,cs.CL
Information Extraction from Clinical Notes: Are We Ready to Switch to Large Language Models?,"Backgrounds: Information extraction (IE) is critical in clinical natural
language processing (NLP). While large language models (LLMs) excel on
generative tasks, their performance on extractive tasks remains debated.
Methods: We investigated Named Entity Recognition (NER) and Relation Extraction
(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,
MIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical
entities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3
against BERT in terms of performance, generalizability, computational
resources, and throughput to BERT. Results: LLaMA models outperformed BERT
across datasets. With sufficient training data, LLaMA showed modest
improvements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited
training data. On unseen i2b2 data, LLaMA-3-70B outperformed BERT by 7% (F1) on
NER and 4% on RE. However, LLaMA models required more computing resources and
ran up to 28 times slower. We implemented ""Kiwi,"" a clinical IE package
featuring both models, available at https://kiwi.clinicalnlp.org/. Conclusion:
This study is among the first to develop and evaluate a comprehensive clinical
IE system using open-source LLMs. Results indicate that LLaMA models outperform
BERT for clinical NER and RE but with higher computational costs and lower
throughputs. These findings highlight that choosing between LLMs and
traditional deep learning methods for clinical IE applications should remain
task-specific, taking into account both performance metrics and practical
considerations such as available computing resources and the intended use case
scenarios.",2024-11-15,"Yan Hu, Xu Zuo, Yujia Zhou, Xueqing Peng, Jimin Huang, Vipina K. Keloth, Vincent J. Zhang, Ruey-Ling Weng, Qingyu Chen, Xiaoqian Jiang, Kirk E. Roberts, Hua Xu",http://arxiv.org/pdf/2411.10020v4,cs.CL
"Once More, With Feeling: Measuring Emotion of Acting Performances in Contemporary American Film","Narrative film is a composition of writing, cinematography, editing, and
performance. While much computational work has focused on the writing or visual
style in film, we conduct in this paper a computational exploration of acting
performance. Applying speech emotion recognition models and a variationist
sociolinguistic analytical framework to a corpus of popular, contemporary
American film, we find narrative structure, diachronic shifts, and genre- and
dialogue-based constraints located in spoken performances.",2024-11-15,"Naitian Zhou, David Bamman",http://arxiv.org/pdf/2411.10018v1,cs.CL
Orca: Enhancing Role-Playing Abilities of Large Language Models by Integrating Personality Traits,"Large language models has catalyzed the development of personalized dialogue
systems, numerous role-playing conversational agents have emerged. While
previous research predominantly focused on enhancing the model's capability to
follow instructions by designing character profiles, neglecting the
psychological factors that drive human conversations. In this paper, we propose
Orca, a framework for data processing and training LLMs of custom characters by
integrating personality traits. Orca comprises four stages: (1) Personality
traits inferring, leverage LLMs to infer user's BigFive personality trait
reports and scores. (2) Data Augment, simulate user's profile, background
story, and psychological activities. (3) Dataset construction,
personality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4)
Modeling and Training, personality-conditioned instruction tuning (PTIT and
PSIT), using the generated data to enhance existing open-source LLMs. We
introduce OrcaBench, the first benchmark for evaluating the quality of content
generated by LLMs on social platforms across multiple scales. Our experiments
demonstrate that our proposed model achieves superior performance on this
benchmark, demonstrating its excellence and effectiveness in perceiving
personality traits that significantly improve role-playing abilities. Our Code
is available at https://github.com/Aipura/Orca.",2024-11-15,Yuxuan Huang,http://arxiv.org/pdf/2411.10006v1,cs.CL
HistoLens: An LLM-Powered Framework for Multi-Layered Analysis of Historical Texts -- A Case Application of Yantie Lun,"This paper proposes HistoLens, a multi-layered analysis framework for
historical texts based on Large Language Models (LLMs). Using the important
Western Han dynasty text ""Yantie Lun"" as a case study, we demonstrate the
framework's potential applications in historical research and education.
HistoLens integrates NLP technology (especially LLMs), including named entity
recognition, knowledge graph construction, and geographic information
visualization. The paper showcases how HistoLens explores Western Han culture
in ""Yantie Lun"" through multi-dimensional, visual, and quantitative methods,
focusing particularly on the influence of Confucian and Legalist thoughts on
political, economic, military, and ethnic. We also demonstrate how HistoLens
constructs a machine teaching scenario using LLMs for explainable analysis,
based on a dataset of Confucian and Legalist ideas extracted with LLM
assistance. This approach offers novel and diverse perspectives for studying
historical texts like ""Yantie Lun"" and provides new auxiliary tools for history
education. The framework aims to equip historians and learners with
LLM-assisted tools to facilitate in-depth, multi-layered analysis of historical
texts and foster innovation in historical education.",2024-11-15,Yifan Zeng,http://arxiv.org/pdf/2411.09978v1,cs.CL
Large Language Models as User-Agents for Evaluating Task-Oriented-Dialogue Systems,"Traditionally, offline datasets have been used to evaluate task-oriented
dialogue (TOD) models. These datasets lack context awareness, making them
suboptimal benchmarks for conversational systems. In contrast, user-agents,
which are context-aware, can simulate the variability and unpredictability of
human conversations, making them better alternatives as evaluators. Prior
research has utilized large language models (LLMs) to develop user-agents. Our
work builds upon this by using LLMs to create user-agents for the evaluation of
TOD systems. This involves prompting an LLM, using in-context examples as
guidance, and tracking the user-goal state. Our evaluation of diversity and
task completion metrics for the user-agents shows improved performance with the
use of better prompts. Additionally, we propose methodologies for the automatic
evaluation of TOD models within this dynamic framework.",2024-11-15,"Taaha Kazi, Ruiliang Lyu, Sizhe Zhou, Dilek Hakkani-Tur, Gokhan Tur",http://arxiv.org/pdf/2411.09972v1,cs.CL
LoRA-LiteE: A Computationally Efficient Framework for Chatbot Preference-Tuning,"Effective preference tuning is pivotal in aligning chatbot responses with
human expectations, enhancing user satisfaction and engagement. Traditional
approaches, notably Reinforcement Learning from Human Feedback (RLHF) as
employed in advanced models like GPT-4, have demonstrated considerable success
in this domain. However, RLHF methods are often computationally intensive and
resource-demanding, limiting their scalability and accessibility for broader
applications. To address these challenges, this study introduces LoRA-Lite
Ensemble (LoRA-LiteE), an innovative framework that combines Supervised
Fine-tuning (SFT) with Low-Rank Adaptation (LoRA) and Ensemble Learning
techniques to effectively aggregate predictions of lightweight models, which
aim to achieve a balance between the performance and computational cost.
Utilizing the Chatbot Arena benchmark dataset, we conduct a comprehensive
comparative analysis among our LoRA-LiteE model, corresponding base models at
different scales, and GPT-4 trained with RLHF. Our empirical results
demonstrate that the proposed LoRA-LiteE model achieves comparable performance
to un-finetuned GPT-4 and outperforms the single larger-scale models under
limited resource constraints. These findings highlight that our LoRA-LiteE
provides a feasible and efficient methodology for human preference prediction
in chatbot systems, enhancing scalability and accessibility, and thereby
broadening the applicability of preference-tuned chatbots in
resource-constrained environments.",2024-11-15,"Yahe Yang, Chunliang Tao, Xiaojing Fan",http://arxiv.org/pdf/2411.09947v2,cs.CL
SlimLM: An Efficient Small Language Model for On-Device Document Assistance,"While small language models (SLMs) show promises for mobile deployment, their
real-world performance and applications on smartphones remains underexplored.
We present SlimLM, a series of SLMs optimized for document assistance tasks on
mobile devices. Through extensive experiments on a Samsung Galaxy S24, we
identify the optimal trade-offs between model size (ranging from 125M to 7B
parameters), context length, and inference time for efficient on-device
processing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on
DocAssist, our constructed dataset for summarization, question answering and
suggestion tasks. Our smallest model demonstrates efficient performance on S24,
while larger variants offer enhanced capabilities within mobile constraints. We
evaluate SlimLM against existing SLMs, showing comparable or superior
performance and offering a benchmark for future research in on-device language
models. We also provide an Android application, offering practical insights
into SLM deployment. Our findings provide valuable insights and illuminate the
capabilities of running advanced language models on high-end smartphones,
potentially reducing server costs and enhancing privacy through on-device
processing.",2024-11-15,"Thang M. Pham, Phat T. Nguyen, Seunghyun Yoon, Viet Dac Lai, Franck Dernoncourt, Trung Bui",http://arxiv.org/pdf/2411.09944v3,cs.CL
Refined and Segmented Price Sentiment Indices from Survey Comments,"We aim to enhance a price sentiment index and to more precisely understand
price trends from the perspective of not only consumers but also businesses. We
extract comments related to prices from the Economy Watchers Survey conducted
by the Cabinet Office of Japan and classify price trends using a large language
model (LLM). We classify whether the survey sample reflects the perspective of
consumers or businesses, and whether the comments pertain to goods or services
by utilizing information on the fields of comments and the industries of
respondents included in the Economy Watchers Survey. From these classified
price-related comments, we construct price sentiment indices not only for a
general purpose but also for more specific objectives by combining perspectives
on consumers and prices, as well as goods and services. It becomes possible to
achieve a more accurate classification of price directions by employing a LLM
for classification. Furthermore, integrating the outputs of multiple LLMs
suggests the potential for the better performance of the classification. The
use of more accurately classified comments allows for the construction of an
index with a higher correlation to existing indices than previous studies. We
demonstrate that the correlation of the price index for consumers, which has a
larger sample size, is further enhanced by selecting comments for aggregation
based on the industry of the survey respondents.",2024-11-15,"Masahiro Suzuki, Hiroki Sakaji",http://arxiv.org/pdf/2411.09937v2,cs.CL
JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by Evolutionary Optimization of Model Merging,"With the rapid advancement of large language models (LLMs), foundational
models (FMs) have seen significant advancements. Healthcare is one of the most
crucial application areas for these FMs, given the significant time and effort
required for physicians to analyze large volumes of patient data. Recent
efforts have focused on adapting multimodal FMs to the medical domain through
techniques like instruction-tuning, leading to the development of medical
foundation models (MFMs). However, these approaches typically require large
amounts of training data to effectively adapt models to the medical field.
Moreover, most existing models are trained on English datasets, limiting their
practicality in non-English-speaking regions where healthcare professionals and
patients are not always fluent in English. The need for translation introduces
additional costs and inefficiencies. To address these challenges, we propose a
\textbf{J}apanese \textbf{Radi}ology report generation model enhanced by
\textbf{Evo}lutionary optimization of model merging (JRadiEvo). This is the
first attempt to extend a non-medical vision-language foundation model to the
medical domain through evolutionary optimization of model merging. We
successfully created a model that generates accurate Japanese reports from
X-ray images using only 50 translated samples from publicly available data.
This model, developed with highly efficient use of limited data, outperformed
leading models from recent research trained on much larger datasets.
Additionally, with only 8 billion parameters, this relatively compact
foundation model can be deployed locally within hospitals, making it a
practical solution for environments where APIs and other external services
cannot be used due to strict privacy and security requirements.",2024-11-15,"Kaito Baba, Ryota Yagi, Junichiro Takahashi, Risa Kishikawa, Satoshi Kodera",http://arxiv.org/pdf/2411.09933v1,cs.CL
WavChat: A Survey of Spoken Dialogue Models,"Recent advancements in spoken dialogue models, exemplified by systems like
GPT-4o, have captured significant attention in the speech domain. Compared to
traditional three-tier cascaded spoken dialogue models that comprise speech
recognition (ASR), large language models (LLMs), and text-to-speech (TTS),
modern spoken dialogue models exhibit greater intelligence. These advanced
spoken dialogue models not only comprehend audio, music, and other
speech-related features, but also capture stylistic and timbral characteristics
in speech. Moreover, they generate high-quality, multi-turn speech responses
with low latency, enabling real-time interaction through simultaneous listening
and speaking capability. Despite the progress in spoken dialogue systems, there
is a lack of comprehensive surveys that systematically organize and analyze
these systems and the underlying technologies. To address this, we have first
compiled existing spoken dialogue systems in the chronological order and
categorized them into the cascaded and end-to-end paradigms. We then provide an
in-depth overview of the core technologies in spoken dialogue models, covering
aspects such as speech representation, training paradigm, streaming, duplex,
and interaction capabilities. Each section discusses the limitations of these
technologies and outlines considerations for future research. Additionally, we
present a thorough review of relevant datasets, evaluation metrics, and
benchmarks from the perspectives of training and evaluating spoken dialogue
systems. We hope this survey will contribute to advancing both academic
research and industrial applications in the field of spoken dialogue systems.
The related material is available at https://github.com/jishengpeng/WavChat.",2024-11-15,"Shengpeng Ji, Yifu Chen, Minghui Fang, Jialong Zuo, Jingyu Lu, Hanting Wang, Ziyue Jiang, Long Zhou, Shujie Liu, Xize Cheng, Xiaoda Yang, Zehan Wang, Qian Yang, Jian Li, Yidi Jiang, Jingzhen He, Yunfei Chu, Jin Xu, Zhou Zhao",http://arxiv.org/pdf/2411.13577v2,cs.CL
Research on Domain-Specific Chinese Spelling Correction Method Based on Plugin Extension Modules,"This paper proposes a Chinese spelling correction method based on plugin
extension modules, aimed at addressing the limitations of existing models in
handling domain-specific texts. Traditional Chinese spelling correction models
are typically trained on general-domain datasets, resulting in poor performance
when encountering specialized terminology in domain-specific texts. To address
this issue, we design an extension module that learns the features of
domain-specific terminology, thereby enhancing the model's correction
capabilities within specific domains. This extension module can provide domain
knowledge to the model without compromising its general spelling correction
performance, thus improving its accuracy in specialized fields. Experimental
results demonstrate that after integrating extension modules for medical,
legal, and official document domains, the model's correction performance is
significantly improved compared to the baseline model without any extension
modules.",2024-11-15,"Xiaowu Zhang, Hongfei Zhao, Xuan Chang",http://arxiv.org/pdf/2411.09884v1,cs.CL
KULCQ: An Unsupervised Keyword-based Utterance Level Clustering Quality Metric,"Intent discovery is crucial for both building new conversational agents and
improving existing ones. While several approaches have been proposed for intent
discovery, most rely on clustering to group similar utterances together.
Traditional evaluation of these utterance clusters requires intent labels for
each utterance, limiting scalability. Although some clustering quality metrics
exist that do not require labeled data, they focus solely on cluster geometry
while ignoring the linguistic nuances present in conversational transcripts. In
this paper, we introduce Keyword-based Utterance Level Clustering Quality
(KULCQ), an unsupervised metric that leverages keyword analysis to evaluate
clustering quality. We demonstrate KULCQ's effectiveness by comparing it with
existing unsupervised clustering metrics and validate its performance through
comprehensive ablation studies. Our results show that KULCQ better captures
semantic relationships in conversational data while maintaining consistency
with geometric clustering principles.",2024-11-15,"Pranav Guruprasad, Negar Mokhberian, Nikhil Varghese, Chandra Khatri, Amol Kelkar",http://arxiv.org/pdf/2411.09853v1,cs.CL
A Benchmark for Long-Form Medical Question Answering,"There is a lack of benchmarks for evaluating large language models (LLMs) in
long-form medical question answering (QA). Most existing medical QA evaluation
benchmarks focus on automatic metrics and multiple-choice questions. While
valuable, these benchmarks fail to fully capture or assess the complexities of
real-world clinical applications where LLMs are being deployed. Furthermore,
existing studies on evaluating long-form answer generation in medical QA are
primarily closed-source, lacking access to human medical expert annotations,
which makes it difficult to reproduce results and enhance existing baselines.
In this work, we introduce a new publicly available benchmark featuring
real-world consumer medical questions with long-form answer evaluations
annotated by medical doctors. We performed pairwise comparisons of responses
from various open and closed-source medical and general-purpose LLMs based on
criteria such as correctness, helpfulness, harmfulness, and bias. Additionally,
we performed a comprehensive LLM-as-a-judge analysis to study the alignment
between human judgments and LLMs. Our preliminary results highlight the strong
potential of open LLMs in medical QA compared to leading closed models. Code &
Data: https://github.com/lavita-ai/medical-eval-sphere",2024-11-14,"Pedram Hosseini, Jessica M. Sin, Bing Ren, Bryceton G. Thomas, Elnaz Nouri, Ali Farahanchi, Saeed Hassanpour",http://arxiv.org/pdf/2411.09834v2,cs.CL
Evaluating Gender Bias in Large Language Models,"Gender bias in artificial intelligence has become an important issue,
particularly in the context of language models used in communication-oriented
applications. This study examines the extent to which Large Language Models
(LLMs) exhibit gender bias in pronoun selection in occupational contexts. The
analysis evaluates the models GPT-4, GPT-4o, PaLM 2 Text Bison and Gemini 1.0
Pro using a self-generated dataset. The jobs considered include a range of
occupations, from those with a significant male presence to those with a
notable female concentration, as well as jobs with a relatively equal gender
distribution. Three different sentence processing methods were used to assess
potential gender bias: masked tokens, unmasked sentences, and sentence
completion. In addition, the LLMs suggested names of individuals in specific
occupations, which were then examined for gender distribution. The results show
a positive correlation between the models' pronoun choices and the gender
distribution present in U.S. labor force data. Female pronouns were more often
associated with female-dominated occupations, while male pronouns were more
often associated with male-dominated occupations. Sentence completion showed
the strongest correlation with actual gender distribution, while name
generation resulted in a more balanced 'politically correct' gender
distribution, albeit with notable variations in predominantly male or female
occupations. Overall, the prompting method had a greater impact on gender
distribution than the model selection itself, highlighting the complexity of
addressing gender bias in LLMs. The findings highlight the importance of
prompting in gender mapping.",2024-11-14,"Michael Döll, Markus Döhring, Andreas Müller",http://arxiv.org/pdf/2411.09826v1,cs.CL
Evaluating the Predictive Capacity of ChatGPT for Academic Peer Review Outcomes Across Multiple Platforms,"While previous studies have demonstrated that Large Language Models (LLMs)
can predict peer review outcomes to some extent, this paper builds on that by
introducing two new contexts and employing a more robust method - averaging
multiple ChatGPT scores. The findings that averaging 30 ChatGPT predictions,
based on reviewer guidelines and using only the submitted titles and abstracts,
failed to predict peer review outcomes for F1000Research (Spearman's rho=0.00).
However, it produced mostly weak positive correlations with the quality
dimensions of SciPost Physics (rho=0.25 for validity, rho=0.25 for originality,
rho=0.20 for significance, and rho = 0.08 for clarity) and a moderate positive
correlation for papers from the International Conference on Learning
Representations (ICLR) (rho=0.38). Including the full text of articles
significantly increased the correlation for ICLR (rho=0.46) and slightly
improved it for F1000Research (rho=0.09), while it had variable effects on the
four quality dimension correlations for SciPost LaTeX files. The use of
chain-of-thought system prompts slightly increased the correlation for
F1000Research (rho=0.10), marginally reduced it for ICLR (rho=0.37), and
further decreased it for SciPost Physics (rho=0.16 for validity, rho=0.18 for
originality, rho=0.18 for significance, and rho=0.05 for clarity). Overall, the
results suggest that in some contexts, ChatGPT can produce weak pre-publication
quality assessments. However, the effectiveness of these assessments and the
optimal strategies for employing them vary considerably across different
platforms, journals, and conferences. Additionally, the most suitable inputs
for ChatGPT appear to differ depending on the platform.",2024-11-14,"Mike Thelwall, Abdullah Yaghi",http://arxiv.org/pdf/2411.09763v1,cs.CL
A Bayesian Optimization Approach to Machine Translation Reranking,"Reranking a list of candidates from a machine translation system with an
external scoring model and returning the highest-scoring candidate remains a
simple and effective method for improving the overall output quality.
Translation scoring models continue to grow in size, with the best models being
comparable to generation models. Thus, reranking can add substantial
computational cost to the translation pipeline. In this work, we pose reranking
as a Bayesian optimization (BayesOpt) problem. By strategically selecting
candidates to score based on a balance of exploration and exploitation, we show
that it is possible to find top-scoring candidates when scoring only a fraction
of the candidate list. For instance, our method achieves the same CometKiwi
score using only 70 scoring evaluations compared a baseline system using 180.
We present a multi-fidelity setting for BayesOpt, where the candidates are
first scored with a cheaper but noisier proxy scoring model, which further
improves the cost-performance tradeoff when using smaller but well-trained
distilled proxy scorers.",2024-11-14,"Julius Cheng, Maike Züfle, Vilém Zouhar, Andreas Vlachos",http://arxiv.org/pdf/2411.09694v2,cs.CL
LLM Hallucination Reasoning with Zero-shot Knowledge Test,"LLM hallucination, where LLMs occasionally generate unfaithful text, poses
significant challenges for their practical applications. Most existing
detection methods rely on external knowledge, LLM fine-tuning, or
hallucination-labeled datasets, and they do not distinguish between different
types of hallucinations, which are crucial for improving detection performance.
We introduce a new task, Hallucination Reasoning, which classifies
LLM-generated text into one of three categories: aligned, misaligned, and
fabricated. Our novel zero-shot method assesses whether LLM has enough
knowledge about a given prompt and text. Our experiments conducted on new
datasets demonstrate the effectiveness of our method in hallucination reasoning
and underscore its importance for enhancing detection performance.",2024-11-14,"Seongmin Lee, Hsiang Hsu, Chun-Fu Chen",http://arxiv.org/pdf/2411.09689v1,cs.CL
Squeezed Attention: Accelerating Long Context Length LLM Inference,"Emerging Large Language Model (LLM) applications require long input prompts
to perform complex downstream tasks like document analysis and code generation.
For these long context length applications, the length of the input prompt
poses a significant challenge in terms of inference efficiency since the
inference costs increase linearly with sequence length. However, for many of
these applications, much of the context in the prompt is fixed across different
user inputs, thereby providing the opportunity to perform offline optimizations
to process user inputs quickly, as they are received. In this work, we propose
Squeezed Attention as a mechanism to accelerate LLM applications where a large
portion of the input prompt is fixed. We first leverage K-means clustering
offline to group the keys for the fixed context based on semantic similarity
and represent each cluster with a single centroid value. During inference, we
compare query tokens from the user input with the centroids to predict which of
the keys from the fixed context are semantically relevant and need to be loaded
during inference. We then compute exact attention using only these important
keys from the fixed context, thereby reducing bandwidth and computational
costs. We also extend our method to use a hierarchical centroid lookup to
identify important keys, which can reduce the complexity of attention from
linear to logarithmic with respect to the context length. We implement
optimized Triton kernels for centroid comparison and sparse FlashAttention with
important keys, achieving more than 4x speedups during both the prefill and
generation phases for long-context inference. Furthermore, we have extensively
evaluated our method on various long-context benchmarks including LongBench,
where it achieves a 3x reduction in KV cache budget without accuracy loss and
up to an 8x reduction with <0.5 point accuracy gap for various models.",2024-11-14,"Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, June Paik, Michael W. Mahoney, Kurt Keutzer, Amir Gholami",http://arxiv.org/pdf/2411.09688v2,cs.CL
Adaptive Decoding via Latent Preference Optimization,"During language model decoding, it is known that using higher temperature
sampling gives more creative responses, while lower temperatures are more
factually accurate. However, such models are commonly applied to general
instruction following, which involves both creative and fact seeking tasks,
using a single fixed temperature across all examples and tokens. In this work,
we introduce Adaptive Decoding, a layer added to the model to select the
sampling temperature dynamically at inference time, at either the token or
example level, in order to optimize performance. To learn its parameters we
introduce Latent Preference Optimization (LPO) a general approach to train
discrete latent variables such as choices of temperature. Our method
outperforms all fixed decoding temperatures across a range of tasks that
require different temperatures, including UltraFeedback, Creative Story
Writing, and GSM8K.",2024-11-14,"Shehzaad Dhuliawala, Ilia Kulikov, Ping Yu, Asli Celikyilmaz, Jason Weston, Sainbayar Sukhbaatar, Jack Lanchantin",http://arxiv.org/pdf/2411.09661v1,cs.CL
"Semantic, Orthographic, and Morphological Biases in Humans' Wordle Gameplay","We show that human players' gameplay in the game of Wordle is influenced by
the semantics, orthography, and morphology of the player's previous guesses. We
demonstrate this influence by comparing actual human players' guesses to
near-optimal guesses, showing that human players' guesses are biased to be
similar to previous guesses semantically, orthographically, and
morphologically.",2024-11-14,"Gary Liang, Adam Kabbara, Cindy Liu, Ronaldo Luo, Kina Kim, Michael Guerzhoy",http://arxiv.org/pdf/2411.18634v1,cs.CL
On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse,"Specifying all desirable properties of a language model is challenging, but
certain requirements seem essential. Given samples from an unknown language,
the trained model should produce valid strings not seen in training and be
expressive enough to capture the language's full richness. Otherwise,
outputting invalid strings constitutes ""hallucination,"" and failing to capture
the full range leads to ""mode collapse."" We ask if a language model can meet
both requirements.
  We investigate this within a statistical language generation setting building
on Gold and Angluin. Here, the model receives random samples from a
distribution over an unknown language K, which belongs to a possibly infinite
collection of languages. The goal is to generate unseen strings from K. We say
the model generates from K with consistency and breadth if, as training size
increases, its output converges to all unseen strings in K.
  Kleinberg and Mullainathan [KM24] asked if consistency and breadth in
language generation are possible. We answer this negatively: for a large class
of language models, including next-token prediction models, this is impossible
for most collections of candidate languages. This contrasts with [KM24]'s
result, showing consistent generation without breadth is possible for any
countable collection of languages. Our finding highlights that generation with
breadth fundamentally differs from generation without breadth.
  As a byproduct, we establish near-tight bounds on the number of samples
needed for generation with or without breadth.
  Finally, our results offer hope: consistent generation with breadth is
achievable for any countable collection of languages when negative examples
(strings outside K) are available alongside positive ones. This suggests that
post-training feedback, which encodes negative examples, can be crucial in
reducing hallucinations while limiting mode collapse.",2024-11-14,"Alkis Kalavasis, Anay Mehrotra, Grigoris Velegkas",http://arxiv.org/pdf/2411.09642v1,cs.CL
PTR: Precision-Driven Tool Recommendation for Large Language Models,"By augmenting Large Language Models (LLMs) with external tools, their
capacity to solve complex problems has been significantly enhanced. However,
despite ongoing advancements in the parsing capabilities of LLMs, incorporating
all available tools simultaneously in the prompt remains impractical due to the
vast number of external tools. Consequently, it is essential to provide LLMs
with a precise set of tools tailored to the specific task, considering both
quantity and quality. Current tool retrieval methods primarily focus on
refining the ranking list of tools and directly packaging a fixed number of
top-ranked tools as the tool set. However, these approaches often fail to equip
LLMs with the optimal set of tools prior to execution, since the optimal number
of tools for different tasks could be different, resulting in inefficiencies
such as redundant or unsuitable tools, which impede immediate access to the
most relevant tools. This paper addresses the challenge of recommending precise
toolsets for LLMs. We introduce the problem of tool recommendation, define its
scope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.
PTR captures an initial, concise set of tools by leveraging historical tool
bundle usage and dynamically adjusts the tool set by performing tool matching,
culminating in a multi-view-based tool addition. Additionally, we present a new
dataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness
of tool recommendation for LLMs. We further validate our design choices through
comprehensive experiments, demonstrating promising accuracy across two open
benchmarks and our RecTools dataset.",2024-11-14,"Hang Gao, Yongfeng Zhang",http://arxiv.org/pdf/2411.09613v1,cs.CL
The Moral Foundations Weibo Corpus,"Moral sentiments expressed in natural language significantly influence both
online and offline environments, shaping behavioral styles and interaction
patterns, including social media selfpresentation, cyberbullying, adherence to
social norms, and ethical decision-making. To effectively measure moral
sentiments in natural language processing texts, it is crucial to utilize
large, annotated datasets that provide nuanced understanding for accurate
analysis and modeltraining. However, existing corpora, while valuable, often
face linguistic limitations. To address this gap in the Chinese language
domain,we introduce the Moral Foundation Weibo Corpus. This corpus consists of
25,671 Chinese comments on Weibo, encompassing six diverse topic areas. Each
comment is manually annotated by at least three systematically trained
annotators based on ten moral categories derived from a grounded theory of
morality. To assess annotator reliability, we present the kappa testresults, a
gold standard for measuring consistency. Additionally, we apply several the
latest large language models to supplement the manual annotations, conducting
analytical experiments to compare their performance and report baseline results
for moral sentiment classification.",2024-11-14,"Renjie Cao, Miaoyan Hu, Jiahan Wei, Baha Ihnaini",http://arxiv.org/pdf/2411.09612v1,cs.CL
Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the AutoNuggetizer Framework,"This report provides an initial look at partial results from the TREC 2024
Retrieval-Augmented Generation (RAG) Track. We have identified RAG evaluation
as a barrier to continued progress in information access (and more broadly,
natural language processing and artificial intelligence), and it is our hope
that we can contribute to tackling the many challenges in this space. The
central hypothesis we explore in this work is that the nugget evaluation
methodology, originally developed for the TREC Question Answering Track in
2003, provides a solid foundation for evaluating RAG systems. As such, our
efforts have focused on ""refactoring"" this methodology, specifically applying
large language models to both automatically create nuggets and to automatically
assign nuggets to system answers. We call this the AutoNuggetizer framework.
Within the TREC setup, we are able to calibrate our fully automatic process
against a manual process whereby nuggets are created by human assessors
semi-manually and then assigned manually to system answers. Based on initial
results across 21 topics from 45 runs, we observe a strong correlation between
scores derived from a fully automatic nugget evaluation and a (mostly) manual
nugget evaluation by human assessors. This suggests that our fully automatic
evaluation process can be used to guide future iterations of RAG systems.",2024-11-14,"Ronak Pradeep, Nandan Thakur, Shivani Upadhyay, Daniel Campos, Nick Craswell, Jimmy Lin",http://arxiv.org/pdf/2411.09607v1,cs.CL
LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models,"This work explores expanding the capabilities of large language models (LLMs)
pretrained on text to generate 3D meshes within a unified model. This offers
key advantages of (1) leveraging spatial knowledge already embedded in LLMs,
derived from textual sources like 3D tutorials, and (2) enabling conversational
3D generation and mesh understanding. A primary challenge is effectively
tokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.
To address this, we introduce LLaMA-Mesh, a novel approach that represents the
vertex coordinates and face definitions of 3D meshes as plain text, allowing
direct integration with LLMs without expanding the vocabulary. We construct a
supervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate
3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs
as required, and (3) understand and interpret 3D meshes. Our work is the first
to demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge
for 3D mesh generation in a text-based format, effectively unifying the 3D and
text modalities. LLaMA-Mesh achieves mesh generation quality on par with models
trained from scratch while maintaining strong text generation performance.",2024-11-14,"Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, Xiaohui Zeng",http://arxiv.org/pdf/2411.09595v1,cs.CL
BabyLM Challenge: Exploring the Effect of Variation Sets on Language Model Training Efficiency,"While current large language models have achieved a remarkable success, their
data efficiency remains a challenge to overcome. Recently it has been suggested
that child-directed speech (CDS) can improve training data efficiency of modern
language models based on Transformer neural networks. However, it is not yet
understood which specific properties of CDS are effective for training these
models. In the context of the BabyLM Challenge, we focus on Variation Sets
(VSs), sets of consecutive utterances expressing a similar intent with slightly
different words and structures, which are ubiquitous in CDS. To assess the
impact of VSs on training data efficiency, we augment CDS data with different
proportions of artificial VSs and use these datasets to train an
auto-regressive model, GPT-2. We find that the best proportion of VSs depends
on the evaluation benchmark: BLiMP and GLUE scores benefit from the presence of
VSs, but EWOK scores do not. Additionally, the results vary depending on
multiple factors such as the number of epochs and the order of utterance
presentation. Taken together, these findings suggest that VSs can have a
beneficial influence on language models, while leaving room for further
investigation.",2024-11-14,"Akari Haga, Akiyo Fukatsu, Miyu Oba, Arianna Bisazza, Yohei Oseki",http://arxiv.org/pdf/2411.09587v2,cs.CL
Piecing It All Together: Verifying Multi-Hop Multimodal Claims,"Existing claim verification datasets often do not require systems to perform
complex reasoning or effectively interpret multimodal evidence. To address
this, we introduce a new task: multi-hop multimodal claim verification. This
task challenges models to reason over multiple pieces of evidence from diverse
sources, including text, images, and tables, and determine whether the combined
multimodal evidence supports or refutes a given claim. To study this task, we
construct MMCV, a large-scale dataset comprising 15k multi-hop claims paired
with multimodal evidence, generated and refined using large language models,
with additional input from human feedback. We show that MMCV is challenging
even for the latest state-of-the-art multimodal large language models,
especially as the number of reasoning hops increases. Additionally, we
establish a human performance benchmark on a subset of MMCV. We hope this
dataset and its evaluation task will encourage future research in multimodal
multi-hop claim verification.",2024-11-14,"Haoran Wang, Aman Rangapur, Xiongxiao Xu, Yueqing Liang, Haroon Gharwi, Carl Yang, Kai Shu",http://arxiv.org/pdf/2411.09547v2,cs.CL
A Practical Guide to Fine-tuning Language Models with Limited Data,"Employing pre-trained Large Language Models (LLMs) has become the de facto
standard in Natural Language Processing (NLP) despite their extensive data
requirements. Motivated by the recent surge in research focused on training
LLMs with limited data, particularly in low-resource domains and languages,
this paper surveys recent transfer learning approaches to optimize model
performance in downstream tasks where data is scarce. We first address initial
and continued pre-training strategies to better leverage prior knowledge in
unseen domains and languages. We then examine how to maximize the utility of
limited data during fine-tuning and few-shot learning. The final section takes
a task-specific perspective, reviewing models and methods suited for different
levels of data scarcity. Our goal is to provide practitioners with practical
guidelines for overcoming the challenges posed by constrained data while also
highlighting promising directions for future research.",2024-11-14,"Marton Szep, Daniel Rueckert, Rüdiger von Eisenhart-Rothe, Florian Hinterwimmer",http://arxiv.org/pdf/2411.09539v1,cs.CL
Communication Compression for Tensor Parallel LLM Inference,"Large Language Models (LLMs) have pushed the frontier of artificial
intelligence but are comprised of hundreds of billions of parameters and
operations. For faster inference latency, LLMs are deployed on multiple
hardware accelerators through various Model Parallelism strategies. Our paper
looks into the details on one such strategy - Tensor Parallel - and proposes to
reduce latency by compressing inter-accelerator communication. We leverage fine
grained quantization techniques to compress selected activations by 3.5 - 4.5x.
Our proposed method leads up to 2x reduction of time-to-first-token (TTFT) with
negligible model performance degradation.",2024-11-14,"Jan Hansen-Palmus, Michael Truong Le, Oliver Hausdörfer, Alok Verma",http://arxiv.org/pdf/2411.09510v2,cs.CL
The Use of Readability Metrics in Legal Text: A Systematic Literature Review,"Understanding the text in legal documents can be challenging due to their
complex structure and the inclusion of domain-specific jargon. Laws and
regulations are often crafted in such a manner that engagement with them
requires formal training, potentially leading to vastly different
interpretations of the same texts. Linguistic complexity is an important
contributor to the difficulties experienced by readers. Simplifying texts could
enhance comprehension across a broader audience, not just among trained
professionals. Various metrics have been developed to measure document
readability. Therefore, we adopted a systematic review approach to examine the
linguistic and readability metrics currently employed for legal and regulatory
texts. A total of 3566 initial papers were screened, with 34 relevant studies
found and further assessed. Our primary objective was to identify which current
metrics were applied for evaluating readability within the legal field. Sixteen
different metrics were identified, with the Flesch-Kincaid Grade Level being
the most frequently used method. The majority of studies (73.5%) were found in
the domain of ""informed consent forms"". From the analysis, it is clear that not
all legal domains are well represented in terms of readability metrics and that
there is a further need to develop more consensus on which metrics should be
applied for legal documents.",2024-11-14,"Yu Han, Aaron Ceross, Jeroen H. M. Bergmann",http://arxiv.org/pdf/2411.09497v1,cs.CL
MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in LLMs,"Large language models (LLMs) excel in high-resource languages but face
notable challenges in low-resource languages like Mongolian. This paper
addresses these challenges by categorizing capabilities into language abilities
(syntax and semantics) and cognitive abilities (knowledge and reasoning). To
systematically evaluate these areas, we developed MM-Eval, a specialized
dataset based on Modern Mongolian Language Textbook I and enriched with WebQSP
and MGSM datasets.
  Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat,
Llama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models
performed better on syntactic tasks than semantic tasks, highlighting a gap in
deeper language understanding; and 2) knowledge tasks showed a moderate
decline, suggesting that models can transfer general knowledge from
high-resource to low-resource contexts.
  The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge,
and 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs in
low-resource languages like Mongolian. The dataset is available at
https://github.com/joenahm/MM-Eval.",2024-11-14,"Mengyuan Zhang, Ruihui Wang, Bo Xia, Yuan Sun, Xiaobing Zhao",http://arxiv.org/pdf/2411.09492v1,cs.CL
Robot Tasks with Fuzzy Time Requirements from Natural Language Instructions,"Natural language allows robot programming to be accessible to everyone.
However, the inherent fuzziness in natural language poses challenges for
inflexible, traditional robot systems. We focus on instructions with fuzzy time
requirements (e.g., ""start in a few minutes""). Building on previous robotics
research, we introduce fuzzy skills. These define an execution by the robot
with so-called satisfaction functions representing vague execution time
requirements. Such functions express a user's satisfaction over potential
starting times for skill execution. When the robot handles multiple fuzzy
skills, the satisfaction function provides a temporal tolerance window for
execution, thus, enabling optimal scheduling based on satisfaction. We
generalized such functions based on individual user expectations with a user
study. The participants rated their satisfaction with an instruction's
execution at various times. Our investigations reveal that trapezoidal
functions best approximate the users' satisfaction. Additionally, the results
suggest that users are more lenient if the execution is specified further into
the future.",2024-11-14,"Sascha Sucker, Michael Neubauer, Dominik Henrich",http://arxiv.org/pdf/2411.09436v1,cs.CL
Everyone deserves their voice to be heard: Analyzing Predictive Gender Bias in ASR Models Applied to Dutch Speech Data,"Recent research has shown that state-of-the-art (SotA) Automatic Speech
Recognition (ASR) systems, such as Whisper, often exhibit predictive biases
that disproportionately affect various demographic groups. This study focuses
on identifying the performance disparities of Whisper models on Dutch speech
data from the Common Voice dataset and the Dutch National Public Broadcasting
organisation. We analyzed the word error rate, character error rate and a
BERT-based semantic similarity across gender groups. We used the moral
framework of Weerts et al. (2022) to assess quality of service harms and
fairness, and to provide a nuanced discussion on the implications of these
biases, particularly for automatic subtitling. Our findings reveal substantial
disparities in word error rate (WER) among gender groups across all model
sizes, with bias identified through statistical testing.",2024-11-14,"Rik Raes, Saskia Lensink, Mykola Pechenizkiy",http://arxiv.org/pdf/2411.09431v1,cs.CL
Less is More: Unseen Domain Fake News Detection via Causal Propagation Substructures,"The spread of fake news on social media poses significant threats to
individuals and society. Text-based and graph-based models have been employed
for fake news detection by analysing news content and propagation networks,
showing promising results in specific scenarios. However, these data-driven
models heavily rely on pre-existing in-distribution data for training, limiting
their performance when confronted with fake news from emerging or previously
unseen domains, known as out-of-distribution (OOD) data. Tackling OOD fake news
is a challenging yet critical task. In this paper, we introduce the Causal
Subgraph-oriented Domain Adaptive Fake News Detection (CSDA) model, designed to
enhance zero-shot fake news detection by extracting causal substructures from
propagation graphs using in-distribution data and generalising this approach to
OOD data. The model employs a graph neural network based mask generation
process to identify dominant nodes and edges within the propagation graph,
using these substructures for fake news detection. Additionally, the
performance of CSDA is further improved through contrastive learning in
few-shot scenarios, where a limited amount of OOD data is available for
training. Extensive experiments on public social media datasets demonstrate
that CSDA effectively handles OOD fake news detection, achieving a 7 to 16
percents accuracy improvement over other state-of-the-art models.",2024-11-14,"Shuzhi Gong, Richard O. Sinnott, Jianzhong Qi, Cecile Paris",http://arxiv.org/pdf/2411.09389v1,cs.CL
Re-Parameterization of Lightweight Transformer for On-Device Speech Emotion Recognition,"With the increasing implementation of machine learning models on edge or
Internet-of-Things (IoT) devices, deploying advanced models on
resource-constrained IoT devices remains challenging. Transformer models, a
currently dominant neural architecture, have achieved great success in broad
domains but their complexity hinders its deployment on IoT devices with limited
computation capability and storage size. Although many model compression
approaches have been explored, they often suffer from notorious performance
degradation. To address this issue, we introduce a new method, namely
Transformer Re-parameterization, to boost the performance of lightweight
Transformer models. It consists of two processes: the High-Rank Factorization
(HRF) process in the training stage and the deHigh-Rank Factorization (deHRF)
process in the inference stage. In the former process, we insert an additional
linear layer before the Feed-Forward Network (FFN) of the lightweight
Transformer. It is supposed that the inserted HRF layers can enhance the model
learning capability. In the later process, the auxiliary HRF layer will be
merged together with the following FFN layer into one linear layer and thus
recover the original structure of the lightweight model. To examine the
effectiveness of the proposed method, we evaluate it on three widely used
Transformer variants, i.e., ConvTransformer, Conformer, and SpeechFormer
networks, in the application of speech emotion recognition on the IEMOCAP, M3ED
and DAIC-WOZ datasets. Experimental results show that our proposed method
consistently improves the performance of lightweight Transformers, even making
them comparable to large models. The proposed re-parameterization approach
enables advanced Transformer models to be deployed on resource-constrained IoT
devices.",2024-11-14,"Zixing Zhang, Zhongren Dong, Weixiang Xu, Jing Han",http://arxiv.org/pdf/2411.09339v1,cs.CL
DriveThru: a Document Extraction Platform and Benchmark Datasets for Indonesian Local Language Archives,"Indonesia is one of the most diverse countries linguistically. However,
despite this linguistic diversity, Indonesian languages remain underrepresented
in Natural Language Processing (NLP) research and technologies. In the past two
years, several efforts have been conducted to construct NLP resources for
Indonesian languages. However, most of these efforts have been focused on
creating manual resources thus difficult to scale to more languages. Although
many Indonesian languages do not have a web presence, locally there are
resources that document these languages well in printed forms such as books,
magazines, and newspapers. Digitizing these existing resources will enable
scaling of Indonesian language resource construction to many more languages. In
this paper, we propose an alternative method of creating datasets by digitizing
documents, which have not previously been used to build digital language
resources in Indonesia. DriveThru is a platform for extracting document content
utilizing Optical Character Recognition (OCR) techniques in its system to
provide language resource building with less manual effort and cost. This paper
also studies the utility of current state-of-the-art LLM for post-OCR
correction to show the capability of increasing the character accuracy rate
(CAR) and word accuracy rate (WAR) compared to off-the-shelf OCR.",2024-11-14,"Mohammad Rifqi Farhansyah, Muhammad Zuhdi Fikri Johari, Afinzaki Amiral, Ayu Purwarianti, Kumara Ari Yuana, Derry Tanti Wijaya",http://arxiv.org/pdf/2411.09318v2,cs.CL
DTELS: Towards Dynamic Granularity of Timeline Summarization,"The rapid proliferation of online news has posed significant challenges in
tracking the continuous development of news topics. Traditional timeline
summarization constructs a chronological summary of the events but often lacks
the flexibility to meet the diverse granularity needs. To overcome this
limitation, we introduce a new paradigm, Dynamic-granularity TimELine
Summarization, (DTELS), which aims to construct adaptive timelines based on
user instructions or requirements. This paper establishes a comprehensive
benchmark for DTLES that includes: (1) an evaluation framework grounded in
journalistic standards to assess the timeline quality across four dimensions:
Informativeness, Granular Consistency, Factuality, and Coherence; (2) a
large-scale, multi-source dataset with multiple granularity timeline
annotations based on a consensus process to facilitate authority; (3) extensive
experiments and analysis with two proposed solutions based on Large Language
Models (LLMs) and existing state-of-the-art TLS methods. The experimental
results demonstrate the effectiveness of LLM-based solutions. However, even the
most advanced LLMs struggle to consistently generate timelines that are both
informative and granularly consistent, highlighting the challenges of the DTELS
task.",2024-11-14,"Chenlong Zhang, Tong Zhou, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao",http://arxiv.org/pdf/2411.09297v1,cs.CL
StreamAdapter: Efficient Test Time Adaptation from Contextual Streams,"In-context learning (ICL) allows large language models (LLMs) to adapt to new
tasks directly from the given demonstrations without requiring gradient
updates. While recent advances have expanded context windows to accommodate
more demonstrations, this approach increases inference costs without
necessarily improving performance. To mitigate these issues, We propose
StreamAdapter, a novel approach that directly updates model parameters from
context at test time, eliminating the need for explicit in-context
demonstrations. StreamAdapter employs context mapping and weight absorption
mechanisms to dynamically transform ICL demonstrations into parameter updates
with minimal additional parameters. By reducing reliance on numerous in-context
examples, StreamAdapter significantly reduce inference costs and allows for
efficient inference with constant time complexity, regardless of demonstration
count. Extensive experiments across diverse tasks and model architectures
demonstrate that StreamAdapter achieves comparable or superior adaptation
capability to ICL while requiring significantly fewer demonstrations. The
superior task adaptation and context encoding capabilities of StreamAdapter on
both language understanding and generation tasks provides a new perspective for
adapting LLMs at test time using context, allowing for more efficient
adaptation across scenarios and more cost-effective inference",2024-11-14,"Dilxat Muhtar, Yelong Shen, Yaming Yang, Xiaodong Liu, Yadong Lu, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Xueliang Zhang, Jianfeng Gao, Weizhu Chen, Qi Zhang",http://arxiv.org/pdf/2411.09289v1,cs.CL
Cross-Modal Consistency in Multimodal Large Language Models,"Recent developments in multimodal methodologies have marked the beginning of
an exciting era for models adept at processing diverse data types, encompassing
text, audio, and visual content. Models like GPT-4V, which merge computer
vision with advanced language processing, exhibit extraordinary proficiency in
handling intricate tasks that require a simultaneous understanding of both
textual and visual information. Prior research efforts have meticulously
evaluated the efficacy of these Vision Large Language Models (VLLMs) in various
domains, including object detection, image captioning, and other related
fields. However, existing analyses have often suffered from limitations,
primarily centering on the isolated evaluation of each modality's performance
while neglecting to explore their intricate cross-modal interactions.
Specifically, the question of whether these models achieve the same level of
accuracy when confronted with identical task instances across different
modalities remains unanswered. In this study, we take the initiative to delve
into the interaction and comparison among these modalities of interest by
introducing a novel concept termed cross-modal consistency. Furthermore, we
propose a quantitative evaluation framework founded on this concept. Our
experimental findings, drawn from a curated collection of parallel
vision-language datasets developed by us, unveil a pronounced inconsistency
between the vision and language modalities within GPT-4V, despite its portrayal
as a unified multimodal model. Our research yields insights into the
appropriate utilization of such models and hints at potential avenues for
enhancing their design.",2024-11-14,"Xiang Zhang, Senyu Li, Ning Shi, Bradley Hauer, Zijun Wu, Grzegorz Kondrak, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan",http://arxiv.org/pdf/2411.09273v1,cs.CL
Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey,"The rapid evolution of multimodal foundation models has led to significant
advancements in cross-modal understanding and generation across diverse
modalities, including text, images, audio, and video. However, these models
remain susceptible to jailbreak attacks, which can bypass built-in safety
mechanisms and induce the production of potentially harmful content.
Consequently, understanding the methods of jailbreak attacks and existing
defense mechanisms is essential to ensure the safe deployment of multimodal
generative models in real-world scenarios, particularly in security-sensitive
applications. To provide comprehensive insight into this topic, this survey
reviews jailbreak and defense in multimodal generative models. First, given the
generalized lifecycle of multimodal jailbreak, we systematically explore
attacks and corresponding defense strategies across four levels: input,
encoder, generator, and output. Based on this analysis, we present a detailed
taxonomy of attack methods, defense mechanisms, and evaluation frameworks
specific to multimodal generative models. Additionally, we cover a wide range
of input-output configurations, including modalities such as Any-to-Text,
Any-to-Vision, and Any-to-Any within generative systems. Finally, we highlight
current research challenges and propose potential directions for future
research. The open-source repository corresponding to this work can be found at
https://github.com/liuxuannan/Awesome-Multimodal-Jailbreak.",2024-11-14,"Xuannan Liu, Xing Cui, Peipei Li, Zekun Li, Huaibo Huang, Shuhan Xia, Miaoxuan Zhang, Yueying Zou, Ran He",http://arxiv.org/pdf/2411.09259v2,cs.CL
DAHL: Domain-specific Automated Hallucination Evaluation of Long-Form Text through a Benchmark Dataset in Biomedicine,"We introduce DAHL, a benchmark dataset and automated evaluation system
designed to assess hallucination in long-form text generation, specifically
within the biomedical domain. Our benchmark dataset, meticulously curated from
biomedical research papers, consists of 8,573 questions across 29 categories.
DAHL evaluates fact-conflicting hallucinations in Large Language Models (LLMs)
by deconstructing responses into atomic units, each representing a single piece
of information. The accuracy of these responses is averaged to produce the DAHL
Score, offering a more in-depth evaluation of hallucinations compared to
previous methods that rely on multiple-choice tasks. We conduct experiments
with 8 different models, finding that larger models tend to hallucinate less;
however, beyond a model size of 7 to 8 billion parameters, further scaling does
not significantly improve factual accuracy. The DAHL Score holds potential as
an efficient alternative to human-annotated preference labels, being able to be
expanded to other specialized domains. We release the dataset and code in
public.",2024-11-14,"Jean Seo, Jongwon Lim, Dongjun Jang, Hyopil Shin",http://arxiv.org/pdf/2411.09255v1,cs.CL
Enhancing Financial Domain Adaptation of Language Models via Model Augmentation,"The domain adaptation of language models, including large language models
(LLMs), has become increasingly important as the use of such models continues
to expand. This study demonstrates the effectiveness of Composition to Augment
Language Models (CALM) in adapting to the financial domain. CALM is a model to
extend the capabilities of existing models by introducing cross-attention
between two LLMs with different functions. In our experiments, we developed a
CALM to enhance the financial performance of an LLM with strong response
capabilities by leveraging a financial-specialized LLM. Notably, the CALM was
trained using a financial dataset different from the one used to train the
financial-specialized LLM, confirming CALM's ability to adapt to various
datasets. The models were evaluated through quantitative Japanese financial
benchmarks and qualitative response comparisons, demonstrating that CALM
enables superior responses with higher scores than the original models and
baselines. Additionally, comparative experiments on connection points revealed
that connecting the middle layers of the models is most effective in
facilitating adaptation to the financial domain. These findings confirm that
CALM is a practical approach for adapting LLMs to the financial domain.",2024-11-14,"Kota Tanabe, Masanori Hirano, Kazuki Matoya, Kentaro Imajo, Hiroki Sakaji, Itsuki Noda",http://arxiv.org/pdf/2411.09249v1,cs.CL
HateGPT: Unleashing GPT-3.5 Turbo to Combat Hate Speech on X,"The widespread use of social media platforms like Twitter and Facebook has
enabled people of all ages to share their thoughts and experiences, leading to
an immense accumulation of user-generated content. However, alongside the
benefits, these platforms also face the challenge of managing hate speech and
offensive content, which can undermine rational discourse and threaten
democratic values. As a result, there is a growing need for automated methods
to detect and mitigate such content, especially given the complexity of
conversations that may require contextual analysis across multiple languages,
including code-mixed languages like Hinglish, German-English, and Bangla. We
participated in the English task where we have to classify English tweets into
two categories namely Hate and Offensive and Non Hate-Offensive. In this work,
we experiment with state-of-the-art large language models like GPT-3.5 Turbo
via prompting to classify tweets into Hate and Offensive or Non Hate-Offensive.
In this study, we evaluate the performance of a classification model using
Macro-F1 scores across three distinct runs. The Macro-F1 score, which balances
precision and recall across all classes, is used as the primary metric for
model evaluation. The scores obtained are 0.756 for run 1, 0.751 for run 2, and
0.754 for run 3, indicating a high level of performance with minimal variance
among the runs. The results suggest that the model consistently performs well
in terms of precision and recall, with run 1 showing the highest performance.
These findings highlight the robustness and reliability of the model across
different runs.",2024-11-14,"Aniket Deroy, Subhankar Maity",http://arxiv.org/pdf/2411.09214v3,cs.CL
Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering,"Retrieval-augmented generation (RAG) has emerged as a promising approach to
enhance the performance of large language models (LLMs) in knowledge-intensive
tasks such as those from medical domain. However, the sensitive nature of the
medical domain necessitates a completely accurate and trustworthy system. While
existing RAG benchmarks primarily focus on the standard retrieve-answer
setting, they overlook many practical scenarios that measure crucial aspects of
a reliable medical system. This paper addresses this gap by providing a
comprehensive evaluation framework for medical question-answering (QA) systems
in a RAG setting for these situations, including sufficiency, integration, and
robustness. We introduce Medical Retrieval-Augmented Generation Benchmark
(MedRGB) that provides various supplementary elements to four medical QA
datasets for testing LLMs' ability to handle these specific scenarios.
Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art
commercial LLMs and open-source models across multiple retrieval conditions.
Our experimental results reveals current models' limited ability to handle
noise and misinformation in the retrieved documents. We further analyze the
LLMs' reasoning processes to provides valuable insights and future directions
for developing RAG systems in this critical medical domain.",2024-11-14,"Nghia Trung Ngo, Chien Van Nguyen, Franck Dernoncourt, Thien Huu Nguyen",http://arxiv.org/pdf/2411.09213v1,cs.CL
Unstructured Text Enhanced Open-domain Dialogue System: A Systematic Survey,"Incorporating external knowledge into dialogue generation has been proven to
benefit the performance of an open-domain Dialogue System (DS), such as
generating informative or stylized responses, controlling conversation topics.
In this article, we study the open-domain DS that uses unstructured text as
external knowledge sources (\textbf{U}nstructured \textbf{T}ext
\textbf{E}nhanced \textbf{D}ialogue \textbf{S}ystem, \textbf{UTEDS}). The
existence of unstructured text entails distinctions between UTEDS and
traditional data-driven DS and we aim to analyze these differences. We first
give the definition of the UTEDS related concepts, then summarize the recently
released datasets and models. We categorize UTEDS into Retrieval and Generative
models and introduce them from the perspective of model components. The
retrieval models consist of Fusion, Matching, and Ranking modules, while the
generative models comprise Dialogue and Knowledge Encoding, Knowledge
Selection, and Response Generation modules. We further summarize the evaluation
methods utilized in UTEDS and analyze the current models' performance. At last,
we discuss the future development trends of UTEDS, hoping to inspire new
research in this field.",2024-11-14,"Longxuan Ma, Mingda Li, Weinan Zhang, Jiapeng Li, Ting Liu",http://arxiv.org/pdf/2411.09166v1,cs.CL
DROJ: A Prompt-Driven Attack against Large Language Models,"Large Language Models (LLMs) have demonstrated exceptional capabilities
across various natural language processing tasks. Due to their training on
internet-sourced datasets, LLMs can sometimes generate objectionable content,
necessitating extensive alignment with human feedback to avoid such outputs.
Despite massive alignment efforts, LLMs remain susceptible to adversarial
jailbreak attacks, which usually are manipulated prompts designed to circumvent
safety mechanisms and elicit harmful responses. Here, we introduce a novel
approach, Directed Rrepresentation Optimization Jailbreak (DROJ), which
optimizes jailbreak prompts at the embedding level to shift the hidden
representations of harmful queries towards directions that are more likely to
elicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat
model show that DROJ achieves a 100\% keyword-based Attack Success Rate (ASR),
effectively preventing direct refusals. However, the model occasionally
produces repetitive and non-informative responses. To mitigate this, we
introduce a helpfulness system prompt that enhances the utility of the model's
responses. Our code is available at
https://github.com/Leon-Leyang/LLM-Safeguard.",2024-11-14,"Leyang Hu, Boran Wang",http://arxiv.org/pdf/2411.09125v1,cs.CL
P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs,"Recent advancements in large language models (LLMs) showcase varied
multilingual capabilities across tasks like translation, code generation, and
reasoning. Previous assessments often limited their scope to fundamental
natural language processing (NLP) or isolated capability-specific tasks. To
alleviate this drawback, we aim to present a comprehensive multilingual
multitask benchmark. First, we introduce P-MMEval, a large-scale benchmark
covering effective fundamental and capability-specialized datasets.
Furthermore, P-MMEval delivers consistent language coverage across various
datasets and provides parallel samples. Finally, we conduct extensive
experiments on representative multilingual model series to compare performances
across models and tasks, explore the relationship between multilingual
performances and factors such as tasks, model sizes, languages, and prompts,
and examine the effectiveness of knowledge transfer from English to other
languages. The resulting insights are intended to offer valuable guidance for
future research. The dataset is available at
https://huggingface.co/datasets/Qwen/P-MMEval.",2024-11-14,"Yidan Zhang, Yu Wan, Boyi Deng, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, Fei Huang, Jingren Zhou",http://arxiv.org/pdf/2411.09116v2,cs.CL
Personalized Help for Optimizing Low-Skilled Users' Strategy,"AIs can beat humans in game environments; however, how helpful those agents
are to human remains understudied. We augment CICERO, a natural language agent
that demonstrates superhuman performance in Diplomacy, to generate both move
and message advice based on player intentions. A dozen Diplomacy games with
novice and experienced players, with varying advice settings, show that some of
the generated advice is beneficial. It helps novices compete with experienced
players and in some instances even surpass them. The mere presence of advice
can be advantageous, even if players do not follow it.",2024-11-14,"Feng Gu, Wichayaporn Wongkamjan, Jonathan K. Kummerfeld, Denis Peskoff, Jonathan May, Jordan Boyd-Graber",http://arxiv.org/pdf/2411.09109v3,cs.CL
CHAI for LLMs: Improving Code-Mixed Translation in Large Language Models through Reinforcement Learning with AI Feedback,"Large Language Models (LLMs) have demonstrated remarkable capabilities across
various NLP tasks but struggle with code-mixed (or code-switched) language
understanding. For example, prior work benchmarking the performance of
multilingual LLMs on code-mixed translation tasks has demonstrated that current
state-of-the-art multilingual LLMs are ineffective in dealing with code-mixed
languages. However, the question of how to improve the capability of
multilingual LLMs to handle code-mixed language has not received any attention
to date. In this paper, we tackle this research gap by proposing CHAI, a novel
general-purpose framework for improving the ability of multilingual LLMs to
handle code-mixed languages. CHAI relies on three novel contributions made in
this paper. First, we explore the ability of LLMs to provide accurate
annotations for code-mixed translation tasks. Second, we leverage this ability
of LLMs as annotators to generate preference data for code-mixed translation
tasks at scale, which are then used within a reinforcement learning from AI
feedback (RLAIF) procedure to improve LLMs' capability on code-mixed tasks.
Third, we conduct a rigorous experimental evaluation across various real-world
datasets and settings. Our analysis shows that CHAI-powered LLMs outperform
state-of-the-art open-source LLMs by 25.66% (in terms of win rate adjudicated
by human annotators) in code-mixed translation tasks. This work represents a
first step towards developing more inclusive code-mixed LLMs.",2024-11-13,"Wenbo Zhang, Aditya Majumdar, Amulya Yadav",http://arxiv.org/pdf/2411.09073v2,cs.CL
Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions,"Recent research increasingly focuses on training vision-language models
(VLMs) with long, detailed image captions. However, small-scale VLMs often
struggle to balance the richness of these captions with the risk of
hallucinating content during fine-tuning. In this paper, we explore how well
VLMs adapt to such captions. To quantify caption quality, we propose Decomposed
NLI (DNLI), an evaluation framework that breaks down generated captions into
individual propositions, assessing each in isolation. This fine-grained
analysis reveals a critical balance between capturing descriptive details and
preventing hallucinations. Our findings show that simply reducing caption
complexity or employing standard data curation techniques does not effectively
resolve this issue. To tackle this challenge, we introduce Knowledge Adapted
(KnowAda) fine-tuning, a data-centric approach that automatically adapts
training data with the model's existing knowledge and visual understanding.
KnowAda minimizes hallucinations while preserving high descriptiveness. We
validate this approach across several small-scale VLMs (up to 7B parameters)
and dense caption datasets, demonstrating that KnowAda effectively balances
hallucination reduction and descriptiveness. Our results show that KnowAda
outperforms various baselines in both automatic metrics and human evaluations.
We will release our code and models.",2024-11-13,"Moran Yanuka, Assaf Ben Kish, Yonatan Bitton, Idan Szpektor, Raja Giryes",http://arxiv.org/pdf/2411.09018v4,cs.CL
Cut Your Losses in Large-Vocabulary Language Models,"As language models grow ever larger, so do their vocabularies. This has
shifted the memory footprint of LLMs during training disproportionately to one
single layer: the cross-entropy in the loss computation. Cross-entropy builds
up a logit matrix with entries for each pair of input tokens and vocabulary
items and, for small models, consumes an order of magnitude more memory than
the rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that
computes the cross-entropy loss without materializing the logits for all tokens
into global memory. Rather, CCE only computes the logit for the correct token
and evaluates the log-sum-exp over all logits on the fly. We implement a custom
kernel that performs the matrix multiplications and the log-sum-exp reduction
over the vocabulary in flash memory, making global memory consumption for the
cross-entropy computation negligible. This has a dramatic effect. Taking the
Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss
computation from 24 GB to 1 MB, and the total training-time memory consumption
of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we
leverage the inherent sparsity of softmax and propose to skip elements of the
gradient computation that have a negligible (i.e., below numerical precision)
contribution to the gradient. Experiments demonstrate that the dramatic
reduction in memory consumption is accomplished without sacrificing training
speed or convergence.",2024-11-13,"Erik Wijmans, Brody Huval, Alexander Hertzberg, Vladlen Koltun, Philipp Krähenbühl",http://arxiv.org/pdf/2411.09009v2,cs.CL
Refusal in LLMs is an Affine Function,"We propose affine concept editing (ACE) as an approach for steering language
models' behavior by intervening directly in activations. We begin with an
affine decomposition of model activation vectors and show that prior methods
for steering model behavior correspond to subsets of terms of this
decomposition. We then provide a derivation of ACE and use it to control
refusal behavior on ten different models, including Llama 3 70B. ACE combines
affine subspace projection and activation addition to reliably control the
model's refusal responses across prompt types. We evaluate the results using
LLM-based scoring on a collection of harmful and harmless prompts. Our
experiments demonstrate that ACE consistently achieves more precise control
over model behavior than existing methods and generalizes to models where
directional ablation via affine subspace projection alone produces incoherent
outputs. Code for reproducing our results is available at
https://github.com/EleutherAI/steering-llama3 .",2024-11-13,"Thomas Marshall, Adam Scherlis, Nora Belrose",http://arxiv.org/pdf/2411.09003v3,cs.CL
CoCoP: Enhancing Text Classification with LLM through Code Completion Prompt,"Text classification is a fundamental task in natural language processing
(NLP), and large language models (LLMs) have demonstrated their capability to
perform this task across various domains. However, the performance of LLMs
heavily depends on the quality of their input prompts. Recent studies have also
shown that LLMs exhibit remarkable results in code-related tasks. To leverage
the capabilities of LLMs in text classification, we propose the Code Completion
Prompt (CoCoP) method, which transforms the text classification problem into a
code completion task. CoCoP significantly improves text classification
performance across diverse datasets by utilizing LLMs' code-completion
capability. For instance, CoCoP enhances the accuracy of the SST2 dataset by
more than 20%. Moreover, when CoCoP integrated with LLMs specifically designed
for code-related tasks (code models), such as CodeLLaMA, this method
demonstrates better or comparable performance to few-shot learning techniques
while using only one-tenth of the model size. The source code of our proposed
method will be available to the public upon the acceptance of the paper.",2024-11-13,"Mohammad Mahdi Mohajeri, Mohammad Javad Dousti, Majid Nili Ahmadabadi",http://arxiv.org/pdf/2411.08979v1,cs.CL
Robustness and Confounders in the Demographic Alignment of LLMs with Human Perceptions of Offensiveness,"Large language models (LLMs) are known to exhibit demographic biases, yet few
studies systematically evaluate these biases across multiple datasets or
account for confounding factors. In this work, we examine LLM alignment with
human annotations in five offensive language datasets, comprising approximately
220K annotations. Our findings reveal that while demographic traits,
particularly race, influence alignment, these effects are inconsistent across
datasets and often entangled with other factors. Confounders -- such as
document difficulty, annotator sensitivity, and within-group agreement --
account for more variation in alignment patterns than demographic traits alone.
Specifically, alignment increases with higher annotator sensitivity and group
agreement, while greater document difficulty corresponds to reduced alignment.
Our results underscore the importance of multi-dataset analyses and
confounder-aware methodologies in developing robust measures of demographic
bias in LLMs.",2024-11-13,"Shayan Alipour, Indira Sen, Mattia Samory, Tanushree Mitra",http://arxiv.org/pdf/2411.08977v2,cs.CL
Sparse Upcycling: Inference Inefficient Finetuning,"Small, highly trained, open-source large language models are widely used due
to their inference efficiency, but further improving their quality remains a
challenge. Sparse upcycling is a promising approach that transforms a
pretrained dense model into a Mixture-of-Experts (MoE) architecture, increasing
the model's parameter count and quality. In this work, we compare the
effectiveness of sparse upcycling against continued pretraining (CPT) across
different model sizes, compute budgets, and pretraining durations. Our
experiments show that sparse upcycling can achieve better quality, with
improvements of over 20% relative to CPT in certain scenarios. However, this
comes with a significant inference cost, leading to 40% slowdowns in
high-demand inference settings for larger models. Our findings highlight the
trade-off between model quality and inference efficiency, offering insights for
practitioners seeking to balance model quality and deployment constraints.",2024-11-13,"Sasha Doubov, Nikhil Sardana, Vitaliy Chiley",http://arxiv.org/pdf/2411.08968v1,cs.CL
The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models,"Several recent works seek to adapt general-purpose large language models
(LLMs) and vision-language models (VLMs) for medical applications through
continued pretraining on publicly available biomedical corpora. These works
typically claim that such domain-adaptive pretraining improves performance on
various downstream medical tasks, such as answering medical exam questions. In
this paper, we compare ten ""medical"" LLMs and two VLMs against their
corresponding base models, arriving at a different conclusion: all medical VLMs
and nearly all medical LLMs fail to consistently improve over their base models
in the zero-/few-shot prompting and supervised fine-tuning regimes for medical
question answering (QA). For instance, on clinical-note-based QA tasks in the
3-shot setting, medical LLMs outperform their base models in only 26.7% of
cases, reach a (statistical) tie in 16.7% of cases, and perform significantly
worse in the remaining 56.7% of cases. Our conclusions are based on (i)
comparing each medical model directly against its base model; (ii) optimizing
the prompts for each model separately in zero-/few-shot prompting; and (iii)
accounting for statistical uncertainty in comparisons. Our findings suggest
that state-of-the-art general-domain models may already exhibit strong medical
knowledge and reasoning capabilities, and offer recommendations to strengthen
the conclusions of future studies.",2024-11-13,"Daniel P. Jeong, Pranav Mani, Saurabh Garg, Zachary C. Lipton, Michael Oberst",http://arxiv.org/pdf/2411.08870v2,cs.CL
CamemBERT 2.0: A Smarter French Language Model Aged to Perfection,"French language models, such as CamemBERT, have been widely adopted across
industries for natural language processing (NLP) tasks, with models like
CamemBERT seeing over 4 million downloads per month. However, these models face
challenges due to temporal concept drift, where outdated training data leads to
a decline in performance, especially when encountering new topics and
terminology. This issue emphasizes the need for updated models that reflect
current linguistic trends. In this paper, we introduce two new versions of the
CamemBERT base model-CamemBERTav2 and CamemBERTv2-designed to address these
challenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes use
of the Replaced Token Detection (RTD) objective for better contextual
understanding, while CamemBERTv2 is built on RoBERTa, which uses the Masked
Language Modeling (MLM) objective. Both models are trained on a significantly
larger and more recent dataset with longer context length and an updated
tokenizer that enhances tokenization performance for French. We evaluate the
performance of these models on both general-domain NLP tasks and
domain-specific applications, such as medical field tasks, demonstrating their
versatility and effectiveness across a range of use cases. Our results show
that these updated models vastly outperform their predecessors, making them
valuable tools for modern NLP systems. All our new models, as well as
intermediate checkpoints, are made openly available on Huggingface.",2024-11-13,"Wissam Antoun, Francis Kulumba, Rian Touchent, Éric de la Clergerie, Benoît Sagot, Djamé Seddah",http://arxiv.org/pdf/2411.08868v1,cs.CL
Can sparse autoencoders be used to decompose and interpret steering vectors?,"Steering vectors are a promising approach to control the behaviour of large
language models. However, their underlying mechanisms remain poorly understood.
While sparse autoencoders (SAEs) may offer a potential method to interpret
steering vectors, recent findings show that SAE-reconstructed vectors often
lack the steering properties of the original vectors. This paper investigates
why directly applying SAEs to steering vectors yields misleading
decompositions, identifying two reasons: (1) steering vectors fall outside the
input distribution for which SAEs are designed, and (2) steering vectors can
have meaningful negative projections in feature directions, which SAEs are not
designed to accommodate. These limitations hinder the direct use of SAEs for
interpreting steering vectors.",2024-11-13,"Harry Mayne, Yushi Yang, Adam Mahdi",http://arxiv.org/pdf/2411.08790v1,cs.CL
Zero-shot Cross-lingual Transfer Learning with Multiple Source and Target Languages for Information Extraction: Language Selection and Adversarial Training,"The majority of previous researches addressing multi-lingual IE are limited
to zero-shot cross-lingual single-transfer (one-to-one) setting, with
high-resource languages predominantly as source training data. As a result,
these works provide little understanding and benefit for the realistic goal of
developing a multi-lingual IE system that can generalize to as many languages
as possible. Our study aims to fill this gap by providing a detailed analysis
on Cross-Lingual Multi-Transferability (many-to-many transfer learning), for
the recent IE corpora that cover a diverse set of languages. Specifically, we
first determine the correlation between single-transfer performance and a wide
range of linguistic-based distances. From the obtained insights, a combined
language distance metric can be developed that is not only highly correlated
but also robust across different tasks and model scales. Next, we investigate
the more general zero-shot multi-lingual transfer settings where multiple
languages are involved in the training and evaluation processes. Language
clustering based on the newly defined distance can provide directions for
achieving the optimal cost-performance trade-off in data (languages) selection
problem. Finally, a relational-transfer setting is proposed to further
incorporate multi-lingual unlabeled data based on adversarial training using
the relation induced from the above linguistic distance.",2024-11-13,"Nghia Trung Ngo, Thien Huu Nguyen",http://arxiv.org/pdf/2411.08785v1,cs.CL
Multi-Perspective Stance Detection,"Subjective NLP tasks usually rely on human annotations provided by multiple
annotators, whose judgments may vary due to their diverse backgrounds and life
experiences. Traditional methods often aggregate multiple annotations into a
single ground truth, disregarding the diversity in perspectives that arises
from annotator disagreement. In this preliminary study, we examine the effect
of including multiple annotations on model accuracy in classification. Our
methodology investigates the performance of perspective-aware classification
models in stance detection task and further inspects if annotator disagreement
affects the model confidence. The results show that multi-perspective approach
yields better classification performance outperforming the baseline which uses
the single label. This entails that designing more inclusive perspective-aware
AI models is not only an essential first step in implementing responsible and
ethical AI, but it can also achieve superior results than using the traditional
approaches.",2024-11-13,"Benedetta Muscato, Praveen Bushipaka, Gizem Gezici, Lucia Passaro, Fosca Giannotti",http://arxiv.org/pdf/2411.08752v1,cs.CL
Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers,"A central question in multilingual language modeling is whether large
language models (LLMs) develop a universal concept representation, disentangled
from specific languages. In this paper, we address this question by analyzing
latent representations (latents) during a word translation task in
transformer-based LLMs. We strategically extract latents from a source
translation prompt and insert them into the forward pass on a target
translation prompt. By doing so, we find that the output language is encoded in
the latent at an earlier layer than the concept to be translated. Building on
this insight, we conduct two key experiments. First, we demonstrate that we can
change the concept without changing the language and vice versa through
activation patching alone. Second, we show that patching with the mean over
latents across different languages does not impair and instead improves the
models' performance in translating the concept. Our results provide evidence
for the existence of language-agnostic concept representations within the
investigated models.",2024-11-13,"Clément Dumas, Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West",http://arxiv.org/pdf/2411.08745v3,cs.CL
A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models,"With the rise of Speech Large Language Models (Speech LLMs), there has been
growing interest in discrete speech tokens for their ability to integrate with
text-based tokens seamlessly. Compared to most studies that focus on continuous
speech features, although discrete-token based LLMs have shown promising
results on certain tasks, the performance gap between these two paradigms is
rarely explored. In this paper, we present a fair and thorough comparison
between discrete and continuous features across a variety of semantic-related
tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that
continuous features generally outperform discrete tokens, particularly in tasks
requiring fine-grained semantic understanding. Moreover, this study goes beyond
surface-level comparison by identifying key factors behind the
under-performance of discrete tokens, such as limited token granularity and
inefficient information retention. To enhance the performance of discrete
tokens, we explore potential aspects based on our analysis. We hope our results
can offer new insights into the opportunities for advancing discrete speech
tokens in Speech LLMs.",2024-11-13,"Dingdong Wang, Mingyu Cui, Dongchao Yang, Xueyuan Chen, Helen Meng",http://arxiv.org/pdf/2411.08742v1,cs.CL
Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models,"Aligning Large Language Models (LLMs) traditionally relies on costly training
and human preference annotations. Self-alignment seeks to reduce these expenses
by enabling models to align themselves. To further lower costs and achieve
alignment without any expensive tuning or annotations, we introduce a new
tuning-free approach for self-alignment, Dynamic Rewarding with Prompt
Optimization (DRPO). Our approach leverages a search-based optimization
framework that allows LLMs to iteratively self-improve and craft the optimal
alignment instructions, all without additional training or human intervention.
The core of DRPO is a dynamic rewarding mechanism, which identifies and
rectifies model-specific alignment weaknesses, allowing LLMs to adapt
efficiently to diverse alignment challenges. Empirical evaluations on eight
recent LLMs, both open- and closed-sourced, demonstrate that DRPO significantly
enhances alignment performance, with base models outperforming their
SFT/RLHF-tuned counterparts. Moreover, the prompts automatically optimized by
DRPO surpass those curated by human experts, further validating the
effectiveness of our approach. Our findings highlight the great potential of
current LLMs to achieve adaptive self-alignment through inference-time
optimization, complementing tuning-based alignment methods.",2024-11-13,"Somanshu Singla, Zhen Wang, Tianyang Liu, Abdullah Ashfaq, Zhiting Hu, Eric P. Xing",http://arxiv.org/pdf/2411.08733v2,cs.CL
Analyst Reports and Stock Performance: Evidence from the Chinese Market,"This article applies natural language processing (NLP) to extract and
quantify textual information to predict stock performance. Using an extensive
dataset of Chinese analyst reports and employing a customized BERT deep
learning model for Chinese text, this study categorizes the sentiment of the
reports as positive, neutral, or negative. The findings underscore the
predictive capacity of this sentiment indicator for stock volatility, excess
returns, and trading volume. Specifically, analyst reports with strong positive
sentiment will increase excess return and intraday volatility, and vice versa,
reports with strong negative sentiment also increase volatility and trading
volume, but decrease future excess return. The magnitude of this effect is
greater for positive sentiment reports than for negative sentiment reports.
This article contributes to the empirical literature on sentiment analysis and
the response of the stock market to news in the Chinese stock market.",2024-11-13,"Rui Liu, Jiayou Liang, Haolong Chen, Yujia Hu",http://arxiv.org/pdf/2411.08726v2,cs.CL
Are Triggers Needed for Document-Level Event Extraction?,"Most existing work on event extraction has focused on sentence-level texts
and presumes the identification of a trigger-span -- a word or phrase in the
input that evokes the occurrence of an event of interest. Event arguments are
then extracted with respect to the trigger. Indeed, triggers are treated as
integral to, and trigger detection as an essential component of, event
extraction. In this paper, we provide the first investigation of the role of
triggers for the more difficult and much less studied task of document-level
event extraction. We analyze their usefulness in multiple end-to-end and
pipelined neural event extraction models for three document-level event
extraction datasets, measuring performance using triggers of varying quality
(human-annotated, LLM-generated, keyword-based, and random). Our research shows
that trigger effectiveness varies based on the extraction task's
characteristics and data quality, with basic, automatically-generated triggers
serving as a viable alternative to human-annotated ones. Furthermore, providing
detailed event descriptions to the extraction model helps maintain robust
performance even when trigger quality degrades. Perhaps surprisingly, we also
find that the mere existence of trigger input, even random ones, is important
for prompt-based LLM approaches to the task.",2024-11-13,"Shaden Shaar, Wayne Chen, Maitreyi Chatterjee, Barry Wang, Wenting Zhao, Claire Cardie",http://arxiv.org/pdf/2411.08708v1,cs.CL
Theoretical Analysis of Byte-Pair Encoding,"Byte-Pair Encoding (BPE) is a widely used method for subword tokenization,
with origins in grammar-based text compression. It is employed in a variety of
language processing tasks such as machine translation or large language model
(LLM) pretraining, to create a token dictionary of a prescribed size. Most
evaluations of BPE to date are empirical, and the reasons for its good
practical performance are not well understood.
  In this paper we focus on the optimization problem underlying BPE: finding a
pair encoding that achieves optimal compression utility. We show that this
problem is APX-complete, indicating that it is unlikely to admit a
polynomial-time approximation scheme. This answers, in a stronger form, a
question recently raised by Zouhar et al.
  On the positive side, we show that BPE approximates the compression utility
of the optimal pair encoding to a worst-case factor between $0.333$ and
$0.625$. Our results aim to explain the ongoing success of BPE and are, to our
knowledge, the first rigorous guarantees on its compression utility that hold
for all inputs.",2024-11-13,"László Kozma, Johannes Voderholzer",http://arxiv.org/pdf/2411.08671v1,cs.CL
Towards Efficient Neurally-Guided Program Induction for ARC-AGI,"ARC-AGI is an open-world problem domain in which the ability to generalize
out-of-distribution is a crucial quality. Under the program induction paradigm,
we present a series of experiments that reveal the efficiency and
generalization characteristics of various neurally-guided program induction
approaches. The three paradigms we consider are Learning the grid space,
Learning the program space, and Learning the transform space. We implement and
experiment thoroughly on the first two, and retain the second one for ARC-AGI
submission. After identifying the strengths and weaknesses of both of these
approaches, we suggest the third as a potential solution, and run preliminary
experiments.",2024-11-13,Simon Ouellette,http://arxiv.org/pdf/2411.17708v1,cs.CL
Dynamic Subset Tuning: Expanding the Operational Range of Parameter-Efficient Training for Large Language Models,"We propose a novel parameter-efficient training (PET) method for large
language models that adapts models to downstream tasks by optimizing a small
subset of the existing model parameters. Unlike prior methods, this subset is
not fixed in location but rather which parameters are modified evolves over the
course of training. This dynamic parameter selection can yield good performance
with many fewer parameters than extant methods. Our method enables a seamless
scaling of the subset size across an arbitrary proportion of the total model
size, while popular PET approaches like prompt tuning and LoRA cover only a
small part of this spectrum. We match or outperform prompt tuning and LoRA in
most cases on a variety of NLP tasks (MT, QA, GSM8K, SuperGLUE) for a given
parameter budget across different model families and sizes.",2024-11-13,"Felix Stahlberg, Jared Lichtarge, Shankar Kumar",http://arxiv.org/pdf/2411.08610v1,cs.CL
A Preview of XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL,"To tackle the challenges of large language model performance in natural
language to SQL tasks, we introduce XiYan-SQL, an innovative framework that
employs a multi-generator ensemble strategy to improve candidate generation. We
introduce M-Schema, a semi-structured schema representation method designed to
enhance the understanding of database structures. To enhance the quality and
diversity of generated candidate SQL queries, XiYan-SQL integrates the
significant potential of in-context learning (ICL) with the precise control of
supervised fine-tuning. On one hand, we propose a series of training strategies
to fine-tune models to generate high-quality candidates with diverse
preferences. On the other hand, we implement the ICL approach with an example
selection method based on named entity recognition to prevent overemphasis on
entities. The refiner optimizes each candidate by correcting logical or
syntactical errors. To address the challenge of identifying the best candidate,
we fine-tune a selection model to distinguish nuances of candidate SQL queries.
The experimental results on multiple dialect datasets demonstrate the
robustness of XiYan-SQL in addressing challenges across different scenarios.
Overall, our proposed XiYan-SQL achieves the state-of-the-art execution
accuracy of 75.63% on Bird benchmark, 89.65% on the Spider test set, 69.86% on
SQL-Eval, 41.20% on NL2GQL. The proposed framework not only enhances the
quality and diversity of SQL queries but also outperforms previous methods.",2024-11-13,"Yingqi Gao, Yifu Liu, Xiaoxia Li, Xiaorong Shi, Yin Zhu, Yiming Wang, Shiqi Li, Wei Li, Yuntao Hong, Zhiling Luo, Jinyang Gao, Liyu Mou, Yu Li",http://arxiv.org/pdf/2411.08599v3,cs.CL
Direct Speech-to-Speech Neural Machine Translation: A Survey,"Speech-to-Speech Translation (S2ST) models transform speech from one language
to another target language with the same linguistic information. S2ST is
important for bridging the communication gap among communities and has diverse
applications. In recent years, researchers have introduced direct S2ST models,
which have the potential to translate speech without relying on intermediate
text generation, have better decoding latency, and the ability to preserve
paralinguistic and non-linguistic features. However, direct S2ST has yet to
achieve quality performance for seamless communication and still lags behind
the cascade models in terms of performance, especially in real-world
translation. To the best of our knowledge, no comprehensive survey is available
on the direct S2ST system, which beginners and advanced researchers can look
upon for a quick survey. The present work provides a comprehensive review of
direct S2ST models, data and application issues, and performance metrics. We
critically analyze the models' performance over the benchmark datasets and
provide research challenges and future directions.",2024-11-13,"Mahendra Gupta, Maitreyee Dutta, Chandresh Kumar Maurya",http://arxiv.org/pdf/2411.14453v1,cs.CL
CorrSynth -- A Correlated Sampling Method for Diverse Dataset Generation from LLMs,"Large language models (LLMs) have demonstrated remarkable performance in
diverse tasks using zero-shot and few-shot prompting. Even though their
capabilities of data synthesis have been studied well in recent years, the
generated data suffers from a lack of diversity, less adherence to the prompt,
and potential biases that creep into the data from the generator model. In this
work, we tackle the challenge of generating datasets with high diversity, upon
which a student model is trained for downstream tasks. Taking the route of
decoding-time guidance-based approaches, we propose CorrSynth, which generates
data that is more diverse and faithful to the input prompt using a correlated
sampling strategy. Further, our method overcomes the complexity drawbacks of
some other guidance-based techniques like classifier-based guidance. With
extensive experiments, we show the effectiveness of our approach and
substantiate our claims. In particular, we perform intrinsic evaluation to show
the improvements in diversity. Our experiments show that CorrSynth improves
both student metrics and intrinsic metrics upon competitive baselines across
four datasets, showing the innate advantage of our method.",2024-11-13,"Suhas S Kowshik, Abhishek Divekar, Vijit Malik",http://arxiv.org/pdf/2411.08553v1,cs.CL
Neural Topic Modeling with Large Language Models in the Loop,"Topic modeling is a fundamental task in natural language processing, allowing
the discovery of latent thematic structures in text corpora. While Large
Language Models (LLMs) have demonstrated promising capabilities in topic
discovery, their direct application to topic modeling suffers from issues such
as incomplete topic coverage, misalignment of topics, and inefficiency. To
address these limitations, we propose LLM-ITL, a novel LLM-in-the-loop
framework that integrates LLMs with Neural Topic Models (NTMs). In LLM-ITL,
global topics and document representations are learned through the NTM.
Meanwhile, an LLM refines these topics using an Optimal Transport (OT)-based
alignment objective, where the refinement is dynamically adjusted based on the
LLM's confidence in suggesting topical words for each set of input words. With
the flexibility of being integrated into many existing NTMs, the proposed
approach enhances the interpretability of topics while preserving the
efficiency of NTMs in learning topics and document representations. Extensive
experiments demonstrate that LLM-ITL helps NTMs significantly improve their
topic interpretability while maintaining the quality of document
representation. Our code and datasets will be available at Github.",2024-11-13,"Xiaohao Yang, He Zhao, Weijie Xu, Yuanyuan Qi, Jueqing Lu, Dinh Phung, Lan Du",http://arxiv.org/pdf/2411.08534v2,cs.CL
Tree-of-Table: Unleashing the Power of LLMs for Enhanced Large-Scale Table Understanding,"The ubiquity and value of tables as semi-structured data across various
domains necessitate advanced methods for understanding their complexity and
vast amounts of information. Despite the impressive capabilities of large
language models (LLMs) in advancing the natural language understanding
frontier, their application to large-scale tabular data presents significant
challenges, specifically regarding table size and complex intricate
relationships. Existing works have shown promise with small-scale tables but
often flounder when tasked with the complex reasoning required by larger,
interconnected tables found in real-world scenarios. To address this gap, we
introduce ""Tree-of-Table"", a novel approach designed to enhance LLMs' reasoning
capabilities over large and complex tables. Our method employs Table
Condensation and Decomposition to distill and reorganize relevant data into a
manageable format, followed by the construction of a hierarchical Table-Tree
that facilitates tree-structured reasoning. Through a meticulous Table-Tree
Execution process, we systematically unravel the tree-structured reasoning
chain to derive the solutions. Experiments across diverse datasets, including
WikiTQ, TableFact, FeTaQA, and BIRD, demonstrate that Tree-of-Table sets a new
benchmark with superior performance, showcasing remarkable efficiency and
generalization capabilities in large-scale table reasoning.",2024-11-13,"Deyi Ji, Lanyun Zhu, Siqi Gao, Peng Xu, Hongtao Lu, Jieping Ye, Feng Zhao",http://arxiv.org/pdf/2411.08516v1,cs.CL
Towards Operationalizing Right to Data Protection,"The widespread practice of indiscriminate data scraping to fine-tune language
models (LMs) raises significant legal and ethical concerns, particularly
regarding compliance with data protection laws such as the General Data
Protection Regulation (GDPR). This practice often results in the unauthorized
use of personal information, prompting growing debate within the academic and
regulatory communities. Recent works have introduced the concept of generating
unlearnable datasets (by adding imperceptible noise to the clean data), such
that the underlying model achieves lower loss during training but fails to
generalize to the unseen test setting. Though somewhat effective, these
approaches are predominantly designed for images and are limited by several
practical constraints like requiring knowledge of the target model. To this
end, we introduce RegText, a framework that injects imperceptible spurious
correlations into natural language datasets, effectively rendering them
unlearnable without affecting semantic content. We demonstrate RegText's
utility through rigorous empirical analysis of small and large LMs. Notably,
RegText can restrict newer models like GPT-4o and Llama from learning on our
generated data, resulting in a drop in their test accuracy compared to their
zero-shot performance and paving the way for generating unlearnable text to
protect public data.",2024-11-13,"Abhinav Java, Simra Shahid, Chirag Agarwal",http://arxiv.org/pdf/2411.08506v2,cs.CL
Towards Objective and Unbiased Decision Assessments with LLM-Enhanced Hierarchical Attention Networks,"How objective and unbiased are we while making decisions? This work
investigates cognitive bias identification in high-stake decision making
process by human experts, questioning its effectiveness in real-world settings,
such as candidates assessments for university admission. We begin with a
statistical analysis assessing correlations among different decision points
among in the current process, which discovers discrepancies that imply
cognitive bias and inconsistency in decisions. This motivates our exploration
of bias-aware AI-augmented workflow that surpass human judgment. We propose
BGM-HAN, an enhanced Hierarchical Attention Network with Byte-Pair Encoding,
Gated Residual Connections and Multi-Head Attention. Using it as a backbone
model, we further propose a Shortlist-Analyse-Recommend (SAR) agentic workflow,
which simulate real-world decision-making. In our experiments, both the
proposed model and the agentic workflow significantly improves on both human
judgment and alternative models, validated with real-world data.",2024-11-13,"Junhua Liu, Kwan Hui Lim, Roy Ka-Wei Lee",http://arxiv.org/pdf/2411.08504v2,cs.CL
Towards Evaluating Large Language Models for Graph Query Generation,"Large Language Models (LLMs) are revolutionizing the landscape of Generative
Artificial Intelligence (GenAI), with innovative LLM-backed solutions emerging
rapidly. However, when applied to database technologies, specifically query
generation for graph databases and Knowledge Graphs (KGs), LLMs still face
significant challenges. While research on LLM-driven query generation for
Structured Query Language (SQL) exists, similar systems for graph databases
remain underdeveloped. This paper presents a comparative study addressing the
challenge of generating Cypher queries a powerful language for interacting with
graph databases using open-access LLMs. We rigorously evaluate several LLM
agents (OpenAI ChatGPT 4o, Claude Sonnet 3.5, Google Gemini Pro 1.5, and a
locally deployed Llama 3.1 8B) using a designed few-shot learning prompt and
Retrieval Augmented Generation (RAG) backed by Chain-of-Thoughts (CoT)
reasoning. Our empirical analysis of query generation accuracy reveals that
Claude Sonnet 3.5 outperforms its counterparts in this specific domain.
Further, we highlight promising future research directions to address the
identified limitations and advance LLM-driven query generation for graph
databases.",2024-11-13,"Siraj Munir, Alessandro Aldini",http://arxiv.org/pdf/2411.08449v2,cs.CL
One STEP at a time: Language Agents are Stepwise Planners,"Language agents have shown promising adaptability in dynamic environments to
perform complex tasks. However, despite the versatile knowledge embedded in
large language models, these agents still fall short when it comes to tasks
that require planning. We introduce STEP, a novel framework designed to
efficiently learn from previous experiences to enhance the planning
capabilities of language agents in future steps. Concretely, STEP functions
through four interconnected components. First, the Planner takes on the task,
breaks it down into subtasks and provides relevant insights. Then the Executor
generates action candidates, while the Evaluator ensures the actions align with
learned rules from previous experiences. Lastly, Memory stores experiences to
inform future decisions. In the ScienceWorld benchmark, our results show that
STEP consistently outperforms state-of-the-art models, achieving an overall
score of 67.4 and successfully completing 12 out of 18 tasks. These findings
highlight STEP's potential as a framework for enhancing planning capabilities
in language agents, paving the way for more sophisticated task-solving in
dynamic environments.",2024-11-13,"Minh Nguyen, Ehsan Shareghi",http://arxiv.org/pdf/2411.08432v1,cs.CL
Hateful Meme Detection through Context-Sensitive Prompting and Fine-Grained Labeling,"The prevalence of multi-modal content on social media complicates automated
moderation strategies. This calls for an enhancement in multi-modal
classification and a deeper understanding of understated meanings in images and
memes. Although previous efforts have aimed at improving model performance
through fine-tuning, few have explored an end-to-end optimization pipeline that
accounts for modalities, prompting, labeling, and fine-tuning. In this study,
we propose an end-to-end conceptual framework for model optimization in complex
tasks. Experiments support the efficacy of this traditional yet novel
framework, achieving the highest accuracy and AUROC. Ablation experiments
demonstrate that isolated optimizations are not ineffective on their own.",2024-11-13,"Rongxin Ouyang, Kokil Jaidka, Subhayan Mukerjee, Guangyu Cui",http://arxiv.org/pdf/2411.10480v1,cs.CL
CLaSP: Learning Concepts for Time-Series Signals from Natural Language Supervision,"This paper presents CLaSP, a novel model for retrieving time-series signals
using natural language queries that describe signal characteristics. The
ability to search time-series signals based on descriptive queries is essential
in domains such as industrial diagnostics, where data scientists often need to
find signals with specific characteristics. However, existing methods rely on
sketch-based inputs, predefined synonym dictionaries, or domain-specific manual
designs, limiting their scalability and adaptability. CLaSP addresses these
challenges by employing contrastive learning to map time-series signals to
natural language descriptions. Unlike prior approaches, it eliminates the need
for predefined synonym dictionaries and leverages the rich contextual knowledge
of large language models (LLMs). Using the TRUCE and SUSHI datasets, which pair
time-series signals with natural language descriptions, we demonstrate that
CLaSP achieves high accuracy in retrieving a variety of time series patterns
based on natural language queries.",2024-11-13,"Aoi Ito, Kota Dohi, Yohei Kawaguchi",http://arxiv.org/pdf/2411.08397v2,cs.CL
Interpretable Syntactic Representations Enable Hierarchical Word Vectors,"The distributed representations currently used are dense and uninterpretable,
leading to interpretations that themselves are relative, overcomplete, and hard
to interpret. We propose a method that transforms these word vectors into
reduced syntactic representations. The resulting representations are compact
and interpretable allowing better visualization and comparison of the word
vectors and we successively demonstrate that the drawn interpretations are in
line with human judgment. The syntactic representations are then used to create
hierarchical word vectors using an incremental learning approach similar to the
hierarchical aspect of human learning. As these representations are drawn from
pre-trained vectors, the generation process and learning approach are
computationally efficient. Most importantly, we find out that syntactic
representations provide a plausible interpretation of the vectors and
subsequent hierarchical vectors outperform the original vectors in benchmark
tests.",2024-11-13,Biraj Silwal,http://arxiv.org/pdf/2411.08384v1,cs.CL
Refining Translations with LLMs: A Constraint-Aware Iterative Prompting Approach,"Large language models (LLMs) have demonstrated remarkable proficiency in
machine translation (MT), even without specific training on the languages in
question. However, translating rare words in low-resource or domain-specific
contexts remains challenging for LLMs. To address this issue, we propose a
multi-step prompt chain that enhances translation faithfulness by prioritizing
key terms crucial for semantic accuracy. Our method first identifies these
keywords and retrieves their translations from a bilingual dictionary,
integrating them into the LLM's context using Retrieval-Augmented Generation
(RAG). We further mitigate potential output hallucinations caused by long
prompts through an iterative self-checking mechanism, where the LLM refines its
translations based on lexical and semantic constraints. Experiments using Llama
and Qwen as base models on the FLORES-200 and WMT datasets demonstrate
significant improvements over baselines, highlighting the effectiveness of our
approach in enhancing translation faithfulness and robustness, particularly in
low-resource scenarios.",2024-11-13,"Shangfeng Chen, Xiayang Shi, Pu Li, Yinlin Li, Jingjing Liu",http://arxiv.org/pdf/2411.08348v1,cs.CL
A Chinese Multi-label Affective Computing Dataset Based on Social Media Network Users,"Emotion and personality are central elements in understanding human
psychological states. Emotions reflect an individual subjective experiences,
while personality reveals relatively stable behavioral and cognitive patterns.
Existing affective computing datasets often annotate emotion and personality
traits separately, lacking fine-grained labeling of micro-emotions and emotion
intensity in both single-label and multi-label classifications. Chinese emotion
datasets are extremely scarce, and datasets capturing Chinese user personality
traits are even more limited. To address these gaps, this study collected data
from the major social media platform Weibo, screening 11,338 valid users from
over 50,000 individuals with diverse MBTI personality labels and acquiring
566,900 posts along with the user MBTI personality tags. Using the EQN method,
we compiled a multi-label Chinese affective computing dataset that integrates
the same user's personality traits with six emotions and micro-emotions, each
annotated with intensity levels. Validation results across multiple NLP
classification models demonstrate the dataset strong utility. This dataset is
designed to advance machine recognition of complex human emotions and provide
data support for research in psychology, education, marketing, finance, and
politics.",2024-11-13,"Jingyi Zhou, Senlin Luo, Haofan Chen",http://arxiv.org/pdf/2411.08347v1,cs.CL
Bangla Grammatical Error Detection Leveraging Transformer-based Token Classification,"Bangla is the seventh most spoken language by a total number of speakers in
the world, and yet the development of an automated grammar checker in this
language is an understudied problem. Bangla grammatical error detection is a
task of detecting sub-strings of a Bangla text that contain grammatical,
punctuation, or spelling errors, which is crucial for developing an automated
Bangla typing assistant. Our approach involves breaking down the task as a
token classification problem and utilizing state-of-the-art transformer-based
models. Finally, we combine the output of these models and apply rule-based
post-processing to generate a more reliable and comprehensive result. Our
system is evaluated on a dataset consisting of over 25,000 texts from various
sources. Our best model achieves a Levenshtein distance score of 1.04. Finally,
we provide a detailed analysis of different components of our system.",2024-11-13,"Shayekh Bin Islam, Ridwanul Hasan Tanvir, Sihat Afnan",http://arxiv.org/pdf/2411.08344v1,cs.CL
Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle,"Many existing evaluation benchmarks for Large Language Models (LLMs) quickly
become outdated due to the emergence of new models and training data. These
benchmarks also fall short in assessing how LLM performance changes over time,
as they consist of static questions without a temporal dimension. To address
these limitations, we propose using future event prediction as a continuous
evaluation method to assess LLMs' temporal generalization and forecasting
abilities. Our benchmark, Daily Oracle, automatically generates question-answer
(QA) pairs from daily news, challenging LLMs to predict ""future"" event
outcomes. Our findings reveal that as pre-training data becomes outdated, LLM
performance degrades over time. While Retrieval Augmented Generation (RAG) has
the potential to enhance prediction accuracy, the performance degradation
pattern persists, highlighting the need for continuous model updates.",2024-11-13,"Hui Dai, Ryan Teehan, Mengye Ren",http://arxiv.org/pdf/2411.08324v1,cs.CL
R3HF: Reward Redistribution for Enhancing Reinforcement Learning from Human Feedback,"Reinforcement learning from human feedback (RLHF) provides a paradigm for
aligning large language models (LLMs) with human preferences. This involves the
initial training of a reward model based on pairwise human feedback. The reward
model is subsequently utilized in reinforcement learning to assess the scores
of each generated sentence as a whole, further guiding the optimization of
LLMs. However, current approaches have a significant shortcoming: \emph{They
allocate a single, sparse, and delayed reward to an entire sequence of output}.
This may overlook some significant individual contributions of each token
towards the desired outcome. To overcome this limitation, our paper proposes a
novel reward redistribution method called R3HF, which facilitates a more
fine-grained, token-level reward allocation. Specifically, our method treats
the reward prediction task of the reward model as a regression problem. As a
result, the redistributed rewards are computed by evaluating the specific
contribution of each token to the reward model's output. This detailed approach
improves the model's understanding of language nuances, leading to more precise
enhancements in its performance. Our method is crafted to integrate seamlessly
with most current techniques while incurring minimal computational costs.
Through comprehensive experiments across diverse datasets and tasks, we have
verified the effectiveness and superiority of our approach.",2024-11-13,"Jiahui Li, Tai-wei Chang, Fengda Zhang, Kun Kuang, Long Chen",http://arxiv.org/pdf/2411.08302v1,cs.CL
Knowledge Bases in Support of Large Language Models for Processing Web News,"Large Language Models (LLMs) have received considerable interest in wide
applications lately. During pre-training via massive datasets, such a model
implicitly memorizes the factual knowledge of trained datasets in its hidden
parameters. However, knowledge held implicitly in parameters often makes its
use by downstream applications ineffective due to the lack of common-sense
reasoning. In this article, we introduce a general framework that permits to
build knowledge bases with an aid of LLMs, tailored for processing Web news.
The framework applies a rule-based News Information Extractor (NewsIE) to news
items for extracting their relational tuples, referred to as knowledge bases,
which are then graph-convoluted with the implicit knowledge facts of news items
obtained by LLMs, for their classification. It involves two lightweight
components: 1) NewsIE: for extracting the structural information of every news
item, in the form of relational tuples; 2) BERTGraph: for graph convoluting the
implicit knowledge facts with relational tuples extracted by NewsIE. We have
evaluated our framework under different news-related datasets for news category
classification, with promising experimental results.",2024-11-13,"Yihe Zhang, Nabin Pakka, Nian-Feng Tzeng",http://arxiv.org/pdf/2411.08278v2,cs.CL
A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look,"The application of large language models to provide relevance assessments
presents exciting opportunities to advance information retrieval, natural
language processing, and beyond, but to date many unknowns remain. This paper
reports on the results of a large-scale evaluation (the TREC 2024 RAG Track)
where four different relevance assessment approaches were deployed in situ: the
""standard"" fully manual process that NIST has implemented for decades and three
different alternatives that take advantage of LLMs to different extents using
the open-source UMBRELA tool. This setup allows us to correlate system rankings
induced by the different approaches to characterize tradeoffs between cost and
quality. We find that in terms of nDCG@20, nDCG@100, and Recall@100, system
rankings induced by automatically generated relevance assessments from UMBRELA
correlate highly with those induced by fully manual assessments across a
diverse set of 77 runs from 19 teams. Our results suggest that automatically
generated UMBRELA judgments can replace fully manual judgments to accurately
capture run-level effectiveness. Surprisingly, we find that LLM assistance does
not appear to increase correlation with fully manual assessments, suggesting
that costs associated with human-in-the-loop processes do not bring obvious
tangible benefits. Overall, human assessors appear to be stricter than UMBRELA
in applying relevance criteria. Our work validates the use of LLMs in academic
TREC-style evaluations and provides the foundation for future studies.",2024-11-13,"Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Daniel Campos, Nick Craswell, Ian Soboroff, Hoa Trang Dang, Jimmy Lin",http://arxiv.org/pdf/2411.08275v1,cs.CL
Deceiving Question-Answering Models: A Hybrid Word-Level Adversarial Approach,"Deep learning underpins most of the currently advanced natural language
processing (NLP) tasks such as textual classification, neural machine
translation (NMT), abstractive summarization and question-answering (QA).
However, the robustness of the models, particularly QA models, against
adversarial attacks is a critical concern that remains insufficiently explored.
This paper introduces QA-Attack (Question Answering Attack), a novel word-level
adversarial strategy that fools QA models. Our attention-based attack exploits
the customized attention mechanism and deletion ranking strategy to identify
and target specific words within contextual passages. It creates deceptive
inputs by carefully choosing and substituting synonyms, preserving grammatical
integrity while misleading the model to produce incorrect responses. Our
approach demonstrates versatility across various question types, particularly
when dealing with extensive long textual inputs. Extensive experiments on
multiple benchmark datasets demonstrate that QA-Attack successfully deceives
baseline QA models and surpasses existing adversarial techniques regarding
success rate, semantics changes, BLEU score, fluency and grammar error rate.",2024-11-12,"Jiyao Li, Mingze Ni, Yongshun Gong, Wei Liu",http://arxiv.org/pdf/2411.08248v1,cs.CL
Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset,"In an effort to mitigate the harms of large language models (LLMs), learning
from human feedback (LHF) has been used to steer LLMs towards outputs that are
intended to be both less harmful and more helpful. Despite the widespread
adoption of LHF in practice, the quality of this feedback and its effectiveness
as a safety mitigation technique remain unclear. This study addresses these
issues by auditing the widely-used Helpful and Harmless (HH) dataset by
Anthropic. Our work includes: (1) a thorough investigation of the dataset's
content through both manual and automated evaluation; (2) experiments
demonstrating the dataset's impact on models' safety; and (3) an analysis of
the 100 most influential papers citing this dataset. Through our audit, we
showcase how conceptualization failures and quality issues identified in the HH
dataset can create additional harms by leading to disparate safety behaviors
across demographic groups. Our findings highlight the need for more nuanced,
context-sensitive approaches to safety mitigation in LLMs.",2024-11-12,"Khaoula Chehbouni, Jonathan Colaço Carr, Yash More, Jackie CK Cheung, Golnoosh Farnadi",http://arxiv.org/pdf/2411.08243v2,cs.CL
"Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion","The Knowledge Graph Completion~(KGC) task aims to infer the missing entity
from an incomplete triple. Existing embedding-based methods rely solely on
triples in the KG, which is vulnerable to specious relation patterns and
long-tail entities. On the other hand, text-based methods struggle with the
semantic gap between KG triples and natural language. Apart from triples,
entity contexts (e.g., labels, descriptions, aliases) also play a significant
role in augmenting KGs. To address these limitations, we propose KGR3, a
context-enriched framework for KGC. KGR3 is composed of three modules. Firstly,
the Retrieval module gathers supporting triples from the KG, collects plausible
candidate answers from a base embedding model, and retrieves context for each
related entity. Then, the Reasoning module employs a large language model to
generate potential answers for each query triple. Finally, the Re-ranking
module combines candidate answers from the two modules mentioned above, and
fine-tunes an LLM to provide the best answer. Extensive experiments on widely
used datasets demonstrate that KGR3 consistently improves various KGC methods.
Specifically, the best variant of KGR3 achieves absolute Hits@1 improvements of
12.3% and 5.6% on the FB15k237 and WN18RR datasets.",2024-11-12,"Muzhi Li, Cehao Yang, Chengjin Xu, Xuhui Jiang, Yiyan Qi, Jian Guo, Ho-fung Leung, Irwin King",http://arxiv.org/pdf/2411.08165v2,cs.CL
Large Language Models Can Self-Improve in Long-context Reasoning,"Large language models (LLMs) have achieved substantial progress in processing
long contexts but still struggle with long-context reasoning. Existing
approaches typically involve fine-tuning LLMs with synthetic data, which
depends on annotations from human experts or advanced models like GPT-4, thus
restricting further advancements. To address this issue, we investigate the
potential for LLMs to self-improve in long-context reasoning and propose \ours,
an approach specifically designed for this purpose. This approach is
straightforward: we sample multiple outputs for each question, score them with
Minimum Bayes Risk, and then apply supervised fine-tuning or preference
optimization based on these outputs. Extensive experiments on several leading
LLMs demonstrate the effectiveness of \ours, with an absolute improvement of
$4.2$ points for Llama-3.1-8B-Instruct. Furthermore, \ours achieves superior
performance compared to prior approaches that depend on data produced by human
experts or advanced models. We anticipate that this work will open new avenues
for self-improvement techniques in long-context scenarios, which are essential
for the continual advancement of LLMs.",2024-11-12,"Siheng Li, Cheng Yang, Zesen Cheng, Lemao Liu, Mo Yu, Yujiu Yang, Wai Lam",http://arxiv.org/pdf/2411.08147v1,cs.CL
On the Role of Speech Data in Reducing Toxicity Detection Bias,"Text toxicity detection systems exhibit significant biases, producing
disproportionate rates of false positives on samples mentioning demographic
groups. But what about toxicity detection in speech? To investigate the extent
to which text-based biases are mitigated by speech-based systems, we produce a
set of high-quality group annotations for the multilingual MuTox dataset, and
then leverage these annotations to systematically compare speech- and
text-based toxicity classifiers. Our findings indicate that access to speech
data during inference supports reduced bias against group mentions,
particularly for ambiguous and disagreement-inducing samples. Our results also
suggest that improving classifiers, rather than transcription pipelines, is
more helpful for reducing group bias. We publicly release our annotations and
provide recommendations for future toxicity dataset construction.",2024-11-12,"Samuel J. Bell, Mariano Coria Meglioli, Megan Richards, Eduardo Sánchez, Christophe Ropers, Skyler Wang, Adina Williams, Levent Sagun, Marta R. Costa-jussà",http://arxiv.org/pdf/2411.08135v2,cs.CL
Language Models as Causal Effect Generators,"We present a framework for large language model (LLM) based data generation
with controllable causal structure. In particular, we define a procedure for
turning any language model and any directed acyclic graph (DAG) into a
sequence-driven structural causal model (SD-SCM). Broadly speaking, an SD-SCM
is a causal model with user-defined structure and LLM-defined structural
equations. We characterize how an SD-SCM allows sampling from observational,
interventional, and counterfactual distributions according to the desired
causal structure. We then leverage this procedure to propose a new type of
benchmark for causal inference methods, generating individual-level
counterfactual data without needing to manually specify functional
relationships between variables. We create an example benchmark consisting of
thousands of datasets, and test a suite of popular estimation methods on these
datasets for average, conditional average, and individual treatment effect
estimation, both with and without hidden confounding. Apart from generating
data, the same procedure also allows us to test for the presence of a causal
effect that might be encoded in an LLM. This procedure can underpin auditing
LLMs for misinformation, discrimination, or otherwise undesirable behavior. We
believe SD-SCMs can serve as a useful tool in any application that would
benefit from sequential data with controllable causal structure.",2024-11-12,"Lucius E. J. Bynum, Kyunghyun Cho",http://arxiv.org/pdf/2411.08019v1,cs.CL
ExpressivityArena: Can LLMs Express Information Implicitly?,"While Large Language Models (LLMs) have demonstrated remarkable performance
in certain dimensions, their ability to express implicit language cues that
human use for effective communication remains unclear. This paper presents
ExpressivityArena, a Python library for measuring the implicit communication
abilities of LLMs. We provide a comprehensive framework to evaluate
expressivity of arbitrary LLMs and explore its practical implications. To this
end, we refine the definition and measurements of ``expressivity,'' and use our
framework in a set of small experiments. These experiments test LLMs in
creative and logical tasks such as poetry, coding, and emotion-based responses.
They are then evaluated by an automated grader, through ExpressivityArena,
which we verify to be the most pragmatic for testing expressivity. Building on
these experiments, we deepen our understanding of the expressivity of LLMs by
assessing their ability to remain expressive in conversations. Our findings
indicate that LLMs are capable of generating and understanding expressive
content, however, with some limitations. These insights will inform the future
development and deployment of expressive LLMs. We provide the code for
ExpressivityArena alongside our paper.",2024-11-12,"Joshua Tint, Som Sagar, Aditya Taparia, Kelly Raines, Bimsara Pathiraja, Caleb Liu, Ransalu Senanayake",http://arxiv.org/pdf/2411.08010v1,cs.CL
Can adversarial attacks by large language models be attributed?,"Attributing outputs from Large Language Models (LLMs) in adversarial
settings-such as cyberattacks and disinformation-presents significant
challenges that are likely to grow in importance. We investigate this
attribution problem using formal language theory, specifically language
identification in the limit as introduced by Gold and extended by Angluin. By
modeling LLM outputs as formal languages, we analyze whether finite text
samples can uniquely pinpoint the originating model. Our results show that due
to the non-identifiability of certain language classes, under some mild
assumptions about overlapping outputs from fine-tuned models it is
theoretically impossible to attribute outputs to specific LLMs with certainty.
This holds also when accounting for expressivity limitations of Transformer
architectures. Even with direct model access or comprehensive monitoring,
significant computational hurdles impede attribution efforts. These findings
highlight an urgent need for proactive measures to mitigate risks posed by
adversarial LLM use as their influence continues to expand.",2024-11-12,"Manuel Cebrian, Jan Arne Telle",http://arxiv.org/pdf/2411.08003v1,cs.CL
Derivational Morphology Reveals Analogical Generalization in Large Language Models,"What mechanisms underlie linguistic generalization in large language models
(LLMs)? This question has attracted considerable attention, with most studies
analyzing the extent to which the language skills of LLMs resemble rules. As of
yet, it is not known whether linguistic generalization in LLMs could equally
well be explained as the result of analogical processes, which can be
formalized as similarity operations on stored exemplars. A key shortcoming of
prior research is its focus on linguistic phenomena with a high degree of
regularity, for which rule-based and analogical approaches make the same
predictions. Here, we instead examine derivational morphology, specifically
English adjective nominalization, which displays notable variability. We
introduce a new method for investigating linguistic generalization in LLMs:
focusing on GPT-J, we fit cognitive models that instantiate rule-based and
analogical learning to the LLM training data and compare their predictions on a
set of nonce adjectives with those of the LLM, allowing us to draw direct
conclusions regarding underlying mechanisms. As expected, rule-based and
analogical models explain the predictions of GPT-J equally well for adjectives
with regular nominalization patterns. However, for adjectives with variable
nominalization patterns, the analogical model provides a much better match.
Furthermore, GPT-J's behavior is sensitive to the individual word frequencies,
even for regular forms, a behavior that is consistent with an analogical
account of regular forms but not a rule-based one. These findings refute the
hypothesis that GPT-J's linguistic generalization on adjective nominalization
involves rules, suggesting similarity operations on stored exemplars as the
underlying mechanism. Overall, our study suggests that analogical processes
play a bigger role in the linguistic generalization of LLMs than previously
thought.",2024-11-12,"Valentin Hofmann, Leonie Weissweiler, David Mortensen, Hinrich Schütze, Janet Pierrehumbert",http://arxiv.org/pdf/2411.07990v1,cs.CL
JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation,"We present JanusFlow, a powerful framework that unifies image understanding
and generation in a single model. JanusFlow introduces a minimalist
architecture that integrates autoregressive language models with rectified
flow, a state-of-the-art method in generative modeling. Our key finding
demonstrates that rectified flow can be straightforwardly trained within the
large language model framework, eliminating the need for complex architectural
modifications. To further improve the performance of our unified model, we
adopt two key strategies: (i) decoupling the understanding and generation
encoders, and (ii) aligning their representations during unified training.
Extensive experiments show that JanusFlow achieves comparable or superior
performance to specialized models in their respective domains, while
significantly outperforming existing unified approaches across standard
benchmarks. This work represents a step toward more efficient and versatile
vision-language models.",2024-11-12,"Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, Chong Ruan",http://arxiv.org/pdf/2411.07975v2,cs.CL
SHARP: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing LLMs,"The advanced role-playing capabilities of Large Language Models (LLMs) have
enabled rich interactive scenarios, yet existing research in social
interactions neglects hallucination while struggling with poor generalizability
and implicit character fidelity judgments. To bridge this gap, motivated by
human behaviour, we introduce a generalizable and explicit paradigm for
uncovering interactive patterns of LLMs across diverse worldviews.
Specifically, we first define interactive hallucination through stance
transfer, then construct SHARP, a benchmark built by extracting relations from
commonsense knowledge graphs and utilizing LLMs' inherent hallucination
properties to simulate multi-role interactions. Extensive experiments confirm
our paradigm's effectiveness and stability, examine the factors that influence
these metrics, and challenge conventional hallucination mitigation solutions.
More broadly, our work reveals a fundamental limitation in popular
post-training methods for role-playing LLMs: the tendency to obscure knowledge
beneath style, resulting in monotonous yet human-like behaviors - interactive
hallucination.",2024-11-12,"Chuyi Kong, Ziyang Luo, Hongzhan Lin, Zhiyuan Fan, Yaxin Fan, Yuxi Sun, Jing Ma",http://arxiv.org/pdf/2411.07965v4,cs.CL
CryptoLLM: Unleashing the Power of Prompted LLMs for SmartQnA and Classification of Crypto Posts,"The rapid growth of social media has resulted in an large volume of
user-generated content, particularly in niche domains such as cryptocurrency.
This task focuses on developing robust classification models to accurately
categorize cryptocurrency-related social media posts into predefined classes,
including but not limited to objective, positive, negative, etc. Additionally,
the task requires participants to identify the most relevant answers from a set
of posts in response to specific questions. By leveraging advanced LLMs, this
research aims to enhance the understanding and filtering of cryptocurrency
discourse, thereby facilitating more informed decision-making in this volatile
sector. We have used a prompt-based technique to solve the classification task
for reddit posts and twitter posts. Also, we have used 64-shot technique along
with prompts on GPT-4-Turbo model to determine whether a answer is relevant to
a question or not.",2024-11-12,"Aniket Deroy, Subhankar Maity",http://arxiv.org/pdf/2411.07917v2,cs.CL
Mapping the Podcast Ecosystem with the Structured Podcast Research Corpus,"Podcasts provide highly diverse content to a massive listener base through a
unique on-demand modality. However, limited data has prevented large-scale
computational analysis of the podcast ecosystem. To fill this gap, we introduce
a massive dataset of over 1.1M podcast transcripts that is largely
comprehensive of all English language podcasts available through public RSS
feeds from May and June of 2020. This data is not limited to text, but rather
includes audio features and speaker turns for a subset of 370K episodes, and
speaker role inferences and other metadata for all 1.1M episodes. Using this
data, we also conduct a foundational investigation into the content, structure,
and responsiveness of this ecosystem. Together, our data and analyses open the
door to continued computational research of this popular and impactful medium.",2024-11-12,"Benjamin Litterer, David Jurgens, Dallas Card",http://arxiv.org/pdf/2411.07892v1,cs.CL
Trustful LLMs: Customizing and Grounding Text Generation with Knowledge Bases and Dual Decoders,"Although people are impressed by the content generation skills of large
language models, the use of LLMs, such as ChatGPT, is limited by the domain
grounding of the content. The correctness and groundedness of the generated
content need to be based on a verified context, such as results from
Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to
a customized domain is that the generated responses are often incomplete, or
the additions are not verified and may even be hallucinated. Prior studies on
hallucination detection have focused on evaluation metrics, which are not
easily adaptable to dynamic domains and can be vulnerable to attacks like
jail-breaking. In this work, we propose 1) a post-processing algorithm that
leverages knowledge triplets in RAG context to correct hallucinations and 2) a
dual-decoder model that fuses RAG context to guide the generation process.",2024-11-12,"Xiaofeng Zhu, Jaya Krishna Mandivarapu",http://arxiv.org/pdf/2411.07870v6,cs.CL
Verbosity $\neq$ Veracity: Demystify Verbosity Compensation Behavior of Large Language Models,"Although Large Language Models (LLMs) have demonstrated their strong
capabilities in various tasks, recent work has revealed LLMs also exhibit
undesirable behaviors, such as hallucination and toxicity, limiting their
reliability and broader adoption. In this paper, we discover an understudied
type of undesirable behavior of LLMs, which we term Verbosity Compensation
(VC), similar to the hesitation behavior of humans under uncertainty, where
they respond with excessive words such as repeating questions, introducing
ambiguity, or providing excessive enumeration. We present the first work that
defines and analyzes Verbosity Compensation, explores its causes, and proposes
a simple mitigating approach. Our experiments, conducted on five datasets of
knowledge and reasoning-based QA tasks with 14 newly developed LLMs, reveal
three conclusions. 1) We reveal a pervasive presence of VC across all models
and all datasets. Notably, GPT-4 exhibits a VC frequency of 50.40%. 2) We
reveal the large performance gap between verbose and concise responses, with a
notable difference of 27.61% on the Qasper dataset. We also demonstrate that
this difference does not naturally diminish as LLM capability increases. Both
1) and 2) highlight the urgent need to mitigate the frequency of VC behavior
and disentangle verbosity with veracity. We propose a simple yet effective
cascade algorithm that replaces the verbose responses with the other
model-generated responses. The results show that our approach effectively
alleviates the VC of the Mistral model from 63.81% to 16.16% on the Qasper
dataset. 3) We also find that verbose responses exhibit higher uncertainty
across all five datasets, suggesting a strong connection between verbosity and
model uncertainty. Our dataset and code are available at
https://github.com/psunlpgroup/VerbosityLLM.",2024-11-12,"Yusen Zhang, Sarkar Snigdha Sarathi Das, Rui Zhang",http://arxiv.org/pdf/2411.07858v2,cs.CL
Tucano: Advancing Neural Text Generation for Portuguese,"Significant advances have been made in natural language processing in recent
years. However, our current deep learning approach to language modeling
requires substantial resources in terms of data and computation. One of the
side effects of this data-hungry paradigm is the current schism between
languages, separating those considered high-resource, where most of the
development happens and resources are available, and the low-resource ones,
which struggle to attain the same level of performance and autonomy. This study
aims to introduce a new set of resources to stimulate the future development of
neural text generation in Portuguese. In this work, we document the development
of GigaVerbo, a concatenation of deduplicated Portuguese text corpora amounting
to 200 billion tokens. Via this corpus, we trained a series of
decoder-transformers named Tucano. Our models perform equal or superior to
other Portuguese and multilingual language models of similar size in several
Portuguese benchmarks. The evaluation of our models also reveals that model
performance on many currently available benchmarks used by the Portuguese NLP
community has little to no correlation with the scaling of token ingestion
during training, highlighting the limitations of such evaluations when it comes
to the assessment of Portuguese generative language models. All derivatives of
our study are openly released on GitHub and Hugging Face. See
https://nkluge-correa.github.io/Tucano/",2024-11-12,"Nicholas Kluge Corrêa, Aniket Sen, Sophia Falk, Shiza Fatimah",http://arxiv.org/pdf/2411.07854v1,cs.CL
IAE: Irony-based Adversarial Examples for Sentiment Analysis Systems,"Adversarial examples, which are inputs deliberately perturbed with
imperceptible changes to induce model errors, have raised serious concerns for
the reliability and security of deep neural networks (DNNs). While adversarial
attacks have been extensively studied in continuous data domains such as
images, the discrete nature of text presents unique challenges. In this paper,
we propose Irony-based Adversarial Examples (IAE), a method that transforms
straightforward sentences into ironic ones to create adversarial text. This
approach exploits the rhetorical device of irony, where the intended meaning is
opposite to the literal interpretation, requiring a deeper understanding of
context to detect. The IAE method is particularly challenging due to the need
to accurately locate evaluation words, substitute them with appropriate
collocations, and expand the text with suitable ironic elements while
maintaining semantic coherence. Our research makes the following key
contributions: (1) We introduce IAE, a strategy for generating textual
adversarial examples using irony. This method does not rely on pre-existing
irony corpora, making it a versatile tool for creating adversarial text in
various NLP tasks. (2) We demonstrate that the performance of several
state-of-the-art deep learning models on sentiment analysis tasks significantly
deteriorates when subjected to IAE attacks. This finding underscores the
susceptibility of current NLP systems to adversarial manipulation through
irony. (3) We compare the impact of IAE on human judgment versus NLP systems,
revealing that humans are less susceptible to the effects of irony in text.",2024-11-12,"Xiaoyin Yi, Jiacheng Huang",http://arxiv.org/pdf/2411.07850v1,cs.CL
Ethical Concern Identification in NLP: A Corpus of ACL Anthology Ethics Statements,"What ethical concerns, if any, do LLM researchers have? We introduce EthiCon,
a corpus of 1,580 ethical concern statements extracted from scientific papers
published in the ACL Anthology. We extract ethical concern keywords from the
statements and show promising results in automating the concern identification
process. Through a survey, we compare the ethical concerns of the corpus to the
concerns listed by the general public and professionals in the field. Finally,
we compare our retrieved ethical concerns with existing taxonomies pointing to
gaps and future research directions.",2024-11-12,"Antonia Karamolegkou, Sandrine Schiller Hansen, Ariadni Christopoulou, Filippos Stamatiou, Anne Lauscher, Anders Søgaard",http://arxiv.org/pdf/2411.07845v1,cs.CL
Chain Association-based Attacking and Shielding Natural Language Processing Systems,"Association as a gift enables people do not have to mention something in
completely straightforward words and allows others to understand what they
intend to refer to. In this paper, we propose a chain association-based
adversarial attack against natural language processing systems, utilizing the
comprehension gap between humans and machines. We first generate a chain
association graph for Chinese characters based on the association paradigm for
building search space of potential adversarial examples. Then, we introduce an
discrete particle swarm optimization algorithm to search for the optimal
adversarial examples. We conduct comprehensive experiments and show that
advanced natural language processing models and applications, including large
language models, are vulnerable to our attack, while humans appear good at
understanding the perturbed text. We also explore two methods, including
adversarial training and associative graph-based recovery, to shield systems
from chain association-based attack. Since a few examples that use some
derogatory terms, this paper contains materials that may be offensive or
upsetting to some people.",2024-11-12,"Jiacheng Huang, Long Chen",http://arxiv.org/pdf/2411.07843v1,cs.CL
Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models,"We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel
approach designed to bridge the pre-retrieval information gap in
Retrieval-Augmented Generation (RAG) systems through query optimization
tailored to meet the specific knowledge requirements of Large Language Models
(LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR
framework begins by extracting parametric knowledge from LLMs, followed by
using a specialized query optimizer for refining these queries. This process
ensures the retrieval of only the most pertinent information essential for
generating accurate responses. Moreover, to enhance flexibility and reduce
computational costs, we propose a trainable scheme for our pipeline that
utilizes a smaller, tunable model as the query optimizer, which is refined
through knowledge distillation from a larger teacher model. Our evaluations on
various question-answering (QA) datasets and with different retrieval systems
show that ERRR consistently outperforms existing baselines, proving to be a
versatile and cost-effective module for improving the utility and accuracy of
RAG systems.",2024-11-12,"Youan Cong, Cheng Wang, Pritom Saha Akash, Kevin Chen-Chuan Chang",http://arxiv.org/pdf/2411.07820v2,cs.CL
Pointwise Mutual Information as a Performance Gauge for Retrieval-Augmented Generation,"Recent work suggests that large language models enhanced with
retrieval-augmented generation are easily influenced by the order, in which the
retrieved documents are presented to the model when solving tasks such as
question answering (QA). However, there is no method to date that exploits this
phenomenon to improve generation. We fill this gap. In this study, we show that
the pointwise mutual information between a context and a question is an
effective gauge for language model performance. Importantly, this gauge does
not depend on knowing the answer to the question a priori. Through experiments
on two question-answering datasets and a variety of large language models, we
find evidence for an empirical correlation between answer accuracy and
pointwise mutual information. Additionally, we propose two methods that use the
pointwise mutual information between a document and a question as a gauge for
selecting and constructing prompts that lead to better performance, whose
effectiveness we demonstrate through experimentation.",2024-11-12,"Tianyu Liu, Jirui Qi, Paul He, Arianna Bisazza, Mrinmaya Sachan, Ryan Cotterell",http://arxiv.org/pdf/2411.07773v2,cs.CL
Automatic Album Sequencing,"Album sequencing is a critical part of the album production process.
Recently, a data-driven approach was proposed that sequences general
collections of independent media by extracting the narrative essence of the
items in the collections. While this approach implies an album sequencing
technique, it is not widely accessible to a less technical audience, requiring
advanced knowledge of machine learning techniques to use. To address this, we
introduce a new user-friendly web-based tool that allows a less technical
audience to upload music tracks, execute this technique in one click, and
subsequently presents the result in a clean visualization to the user. To both
increase the number of templates available to the user and address shortcomings
of previous work, we also introduce a new direct transformer-based album
sequencing method. We find that our more direct method outperforms a random
baseline but does not reach the same performance as the narrative essence
approach. Both methods are included in our web-based user interface, and this
-- alongside a full copy of our implementation -- is publicly available at
https://github.com/dylanashley/automatic-album-sequencing",2024-11-12,"Vincent Herrmann, Dylan R. Ashley, Jürgen Schmidhuber",http://arxiv.org/pdf/2411.07772v2,cs.CL
Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows,"Real-world enterprise text-to-SQL workflows often involve complex cloud or
local data across various database systems, multiple SQL queries in various
dialects, and diverse operations from data transformation to analytics. We
introduce Spider 2.0, an evaluation framework comprising 632 real-world
text-to-SQL workflow problems derived from enterprise-level database use cases.
The databases in Spider 2.0 are sourced from real data applications, often
containing over 1,000 columns and stored in local or cloud database systems
such as BigQuery and Snowflake. We show that solving problems in Spider 2.0
frequently requires understanding and searching through database metadata,
dialect documentation, and even project-level codebases. This challenge calls
for models to interact with complex SQL workflow environments, process
extremely long contexts, perform intricate reasoning, and generate multiple SQL
queries with diverse operations, often exceeding 100 lines, which goes far
beyond traditional text-to-SQL challenges. Our evaluations indicate that based
on o1-preview, our code agent framework successfully solves only 21.3% of the
tasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on
Spider 2.0 show that while language models have demonstrated remarkable
performance in code generation -- especially in prior text-to-SQL benchmarks --
they require significant improvement in order to achieve adequate performance
for real-world enterprise usage. Progress on Spider 2.0 represents crucial
steps towards developing intelligent, autonomous, code agents for real-world
enterprise settings. Our code, baseline models, and data are available at
https://spider2-sql.github.io",2024-11-12,"Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su, Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, Victor Zhong, Caiming Xiong, Ruoxi Sun, Qian Liu, Sida Wang, Tao Yu",http://arxiv.org/pdf/2411.07763v2,cs.CL
Mitigating Bias in Queer Representation within Large Language Models: A Collaborative Agent Approach,"Large Language Models (LLMs) often perpetuate biases in pronoun usage,
leading to misrepresentation or exclusion of queer individuals. This paper
addresses the specific problem of biased pronoun usage in LLM outputs,
particularly the inappropriate use of traditionally gendered pronouns (""he,""
""she"") when inclusive language is needed to accurately represent all
identities. We introduce a collaborative agent pipeline designed to mitigate
these biases by analyzing and optimizing pronoun usage for inclusivity. Our
multi-agent framework includes specialized agents for both bias detection and
correction. Experimental evaluations using the Tango dataset-a benchmark
focused on gender pronoun usage-demonstrate that our approach significantly
improves inclusive pronoun classification, achieving a 32.6 percentage point
increase over GPT-4o in correctly disagreeing with inappropriate traditionally
gendered pronouns $(\chi^2 = 38.57, p < 0.0001)$. These results accentuate the
potential of agent-driven frameworks in enhancing fairness and inclusivity in
AI-generated content, demonstrating their efficacy in reducing biases and
promoting socially responsible AI.",2024-11-12,"Tianyi Huang, Arya Somasundaram",http://arxiv.org/pdf/2411.07656v2,cs.CL
Annotating Constructions with UD: the experience of the Italian Constructicon,"The paper descirbes a first attempt of linking the Italian constructicon to
UD resources",2024-11-12,"Ludovica Pannitto, Beatrice Bernasconi, Lucia Busso, Flavio Pisciotta, Giulia Rambelli, Francesca Masini",http://arxiv.org/pdf/2411.07623v1,cs.CL
Direct Preference Optimization Using Sparse Feature-Level Constraints,"The alignment of large language models (LLMs) with human preferences remains
a key challenge. While post-training techniques like Reinforcement Learning
from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have
achieved notable success, they often introduce computational inefficiencies and
training instability. In this paper, we propose Feature-level constrained
Preference Optimization (FPO), a novel method designed to simplify the
alignment process while ensuring stability. FPO leverages pre-trained Sparse
Autoencoders (SAEs) and introduces feature-level constraints, allowing for
efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using
sparse features activated in a well-trained sparse autoencoder and the quality
of sequential KL divergence by using the feature-level offline reference.
Experimental results on benchmark datasets demonstrate that FPO achieves a
5.08% absolute improvement in win rate with much lower computational cost
compared to state-of-the-art baselines, making it a promising solution for
efficient and controllable LLM alignments.",2024-11-12,"Qingyu Yin, Chak Tou Leong, Hongbo Zhang, Minjun Zhu, Hanqi Yan, Qiang Zhang, Yulan He, Wenjie Li, Jun Wang, Yue Zhang, Linyi Yang",http://arxiv.org/pdf/2411.07618v1,cs.CL
Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models,"Interpretation is critical for disease diagnosis, but existing models
struggle to balance predictive accuracy with human-understandable rationales.
While large language models (LLMs) offer strong reasoning abilities, their
clinical use is limited by high computational costs and restricted multimodal
reasoning ability. Small language models (SLMs) are efficient but lack advanced
reasoning for integrating multimodal medical data. In addition, both LLMs and
SLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose
ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via
rationale distillation and domain knowledge injection for trustworthy
multimodal rationale generation. Key innovations include a sequential rationale
distillation framework that equips SLMs with LLM-comparable mutlimodal
reasoning abilities, and a knowledge-augmented attention mechanism that jointly
unifies multimodal representation from time series and textual data in a same
encoding space, enabling it naturally interpreted by SLMs while incorporating
domain knowledge for reliable rationale generation. Experiments on real-world
medical datasets show that ClinRaGen achieves state-of-the-art performance in
disease diagnosis and rationale generation, demonstrating the effectiveness of
combining LLM-driven reasoning with knowledge augmentation for improved
interpretability.",2024-11-12,"Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Yida Xu, Yunya Song, Xian Yang",http://arxiv.org/pdf/2411.07611v3,cs.CL
Circuit Complexity Bounds for RoPE-based Transformer Architecture,"Characterizing the express power of the Transformer architecture is critical
to understanding its capacity limits and scaling law. Recent works provide the
circuit complexity bounds to Transformer-like architecture. On the other hand,
Rotary Position Embedding ($\mathsf{RoPE}$) has emerged as a crucial technique
in modern large language models, offering superior performance in capturing
positional information compared to traditional position embeddings, which shows
great potential in application prospects, particularly for the long context
scenario. Empirical evidence also suggests that $\mathsf{RoPE}$-based
Transformer architectures demonstrate greater generalization capabilities
compared to conventional Transformer models. In this work, we establish a
circuit complexity bound for Transformers with $\mathsf{RoPE}$ attention. Our
key contribution is that we show that unless $\mathsf{TC}^0 = \mathsf{NC}^1$, a
$\mathsf{RoPE}$-based Transformer with $\mathrm{poly}(n)$-precision, $O(1)$
layers, hidden dimension $d \leq O(n)$ cannot solve the Arithmetic formula
evaluation problem or the Boolean formula value problem. This result
significantly demonstrates the fundamental limitation of the expressivity of
the $\mathsf{RoPE}$-based Transformer architecture, although it achieves giant
empirical success. Our theoretical result not only establishes the complexity
bound but also may instruct further work on the $\mathsf{RoPE}$-based
Transformer.",2024-11-12,"Bo Chen, Xiaoyu Li, Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song",http://arxiv.org/pdf/2411.07602v2,cs.CL
Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring Conversations,"Many open-ended conversations (e.g., tutoring lessons or business meetings)
revolve around pre-defined reference materials, like worksheets or meeting
bullets. To provide a framework for studying such conversation structure, we
introduce Problem-Oriented Segmentation & Retrieval (POSR), the task of jointly
breaking down conversations into segments and linking each segment to the
relevant reference item. As a case study, we apply POSR to education where
effectively structuring lessons around problems is critical yet difficult. We
present LessonLink, the first dataset of real-world tutoring lessons, featuring
3,500 segments, spanning 24,300 minutes of instruction and linked to 116 SAT
math problems. We define and evaluate several joint and independent approaches
for POSR, including segmentation (e.g., TextTiling), retrieval (e.g., ColBERT),
and large language models (LLMs) methods. Our results highlight that modeling
POSR as one joint task is essential: POSR methods outperform independent
segmentation and retrieval pipelines by up to +76% on joint metrics and surpass
traditional segmentation methods by up to +78% on segmentation metrics. We
demonstrate POSR's practical impact on downstream education applications,
deriving new insights on the language and time use in real-world lesson
structures.",2024-11-12,"Rose E. Wang, Pawan Wirawarn, Kenny Lam, Omar Khattab, Dorottya Demszky",http://arxiv.org/pdf/2411.07598v1,cs.CL
Entropy Controllable Direct Preference Optimization,"In the post-training of large language models (LLMs), Reinforcement Learning
from Human Feedback (RLHF) is an effective approach to achieve generation
aligned with human preferences. Direct Preference Optimization (DPO) allows for
policy training with a simple binary cross-entropy loss without a reward model.
The objective of DPO is regularized by reverse KL divergence that encourages
mode-seeking fitting to the reference policy. Nonetheless, we indicate that
minimizing reverse KL divergence could fail to capture a mode of the reference
distribution, which may hurt the policy's performance. Based on this
observation, we propose a simple modification to DPO, H-DPO, which allows for
control over the entropy of the resulting policy, enhancing the distribution's
sharpness and thereby enabling mode-seeking fitting more effectively. In our
experiments, we show that H-DPO outperformed DPO across various tasks,
demonstrating superior results in pass@$k$ evaluations for mathematical tasks.
Moreover, H-DPO is simple to implement, requiring only minor modifications to
the loss calculation of DPO, which makes it highly practical and promising for
wide-ranging applications in the training of LLMs.",2024-11-12,"Motoki Omura, Yasuhiro Fujita, Toshiki Kataoka",http://arxiv.org/pdf/2411.07595v1,cs.CL
Contrastive Language Prompting to Ease False Positives in Medical Anomaly Detection,"A pre-trained visual-language model, contrastive language-image pre-training
(CLIP), successfully accomplishes various downstream tasks with text prompts,
such as finding images or localizing regions within the image. Despite CLIP's
strong multi-modal data capabilities, it remains limited in specialized
environments, such as medical applications. For this purpose, many CLIP
variants-i.e., BioMedCLIP, and MedCLIP-SAMv2-have emerged, but false positives
related to normal regions persist. Thus, we aim to present a simple yet
important goal of reducing false positives in medical anomaly detection. We
introduce a Contrastive LAnguage Prompting (CLAP) method that leverages both
positive and negative text prompts. This straightforward approach identifies
potential lesion regions by visual attention to the positive prompts in the
given image. To reduce false positives, we attenuate attention on normal
regions using negative prompts. Extensive experiments with the BMAD dataset,
including six biomedical benchmarks, demonstrate that CLAP method enhances
anomaly detection performance. Our future plans include developing an automated
fine prompting method for more practical usage.",2024-11-12,"YeongHyeon Park, Myung Jin Kim, Hyeong Seok Kim",http://arxiv.org/pdf/2411.07546v2,cs.CL
Large Language Models as Neurolinguistic Subjects: Discrepancy in Performance and Competence for Form and Meaning,"This study investigates the linguistic understanding of Large Language Models
(LLMs) regarding signifier (form) and signified (meaning) by distinguishing two
LLM assessment paradigms: psycholinguistic and neurolinguistic. Traditional
psycholinguistic evaluations often reflect statistical rules that may not
accurately represent LLMs' true linguistic competence. We introduce a
neurolinguistic approach, utilizing a novel method that combines minimal pair
and diagnostic probing to analyze activation patterns across model layers. This
method allows for a detailed examination of how LLMs represent form and
meaning, and whether these representations are consistent across languages. We
found: (1) Psycholinguistic and neurolinguistic methods reveal that language
performance and competence are distinct; (2) Direct probability measurement may
not accurately assess linguistic competence; (3) Instruction tuning won't
change much competence but improve performance; (4) LLMs exhibit higher
competence and performance in form compared to meaning. Additionally, we
introduce new conceptual minimal pair datasets for Chinese (COMPS-ZH) and
German (COMPS-DE), complementing existing English datasets.",2024-11-12,"Linyang He, Ercong Nie, Helmut Schmid, Hinrich Schütze, Nima Mesgarani, Jonathan Brennan",http://arxiv.org/pdf/2411.07533v2,cs.CL
SecEncoder: Logs are All You Need in Security,"Large and Small Language Models (LMs) are typically pretrained using
extensive volumes of text, which are sourced from publicly accessible platforms
such as Wikipedia, Book Corpus, or through web scraping. These models, due to
their exposure to a wide range of language data, exhibit impressive
generalization capabilities and can perform a multitude of tasks
simultaneously. However, they often fall short when it comes to domain-specific
tasks due to their broad training data. This paper introduces SecEncoder, a
specialized small language model that is pretrained using security logs.
SecEncoder is designed to address the domain-specific limitations of general
LMs by focusing on the unique language and patterns found in security logs.
Experimental results indicate that SecEncoder outperforms other LMs, such as
BERTlarge, DeBERTa-v3-large and OpenAI's Embedding (textembedding-ada-002)
models, which are pretrained mainly on natural language, across various tasks.
Furthermore, although SecEncoder is primarily pretrained on log data, it
outperforms models pretrained on natural language for a range of tasks beyond
log analysis, such as incident prioritization and threat intelligence document
retrieval. This suggests that domain specific pretraining with logs can
significantly enhance the performance of LMs in security. These findings pave
the way for future research into security-specific LMs and their potential
applications.",2024-11-12,"Muhammed Fatih Bulut, Yingqi Liu, Naveed Ahmad, Maximilian Turner, Sami Ait Ouahmane, Cameron Andrews, Lloyd Greenwald",http://arxiv.org/pdf/2411.07528v1,cs.CL
Prompt-enhanced Network for Hateful Meme Classification,"The dynamic expansion of social media has led to an inundation of hateful
memes on media platforms, accentuating the growing need for efficient
identification and removal. Acknowledging the constraints of conventional
multimodal hateful meme classification, which heavily depends on external
knowledge and poses the risk of including irrelevant or redundant content, we
developed Pen -- a prompt-enhanced network framework based on the prompt
learning approach. Specifically, after constructing the sequence through the
prompt method and encoding it with a language model, we performed region
information global extraction on the encoded sequence for multi-view
perception. By capturing global information about inference instances and
demonstrations, Pen facilitates category selection by fully leveraging sequence
information. This approach significantly improves model classification
accuracy. Additionally, to bolster the model's reasoning capabilities in the
feature space, we introduced prompt-aware contrastive learning into the
framework to improve the quality of sample feature distributions. Through
extensive ablation experiments on two public datasets, we evaluate the
effectiveness of the Pen framework, concurrently comparing it with
state-of-the-art model baselines. Our research findings highlight that Pen
surpasses manual prompt methods, showcasing superior generalization and
classification accuracy in hateful meme classification tasks. Our code is
available at https://github.com/juszzi/Pen.",2024-11-12,"Junxi Liu, Yanyan Feng, Jiehai Chen, Yun Xue, Fenghuan Li",http://arxiv.org/pdf/2411.07527v2,cs.CL
Fair Summarization: Bridging Quality and Diversity in Extractive Summaries,"Fairness in multi-document summarization of user-generated content remains a
critical challenge in natural language processing (NLP). Existing summarization
methods often fail to ensure equitable representation across different social
groups, leading to biased outputs. In this paper, we introduce two novel
methods for fair extractive summarization: FairExtract, a clustering-based
approach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints.
We evaluate these methods using Divsumm summarization dataset of White-aligned,
Hispanic, and African-American dialect tweets and compare them against relevant
baselines. The results obtained using a comprehensive set of summarization
quality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well
as a fairness metric F, demonstrate that FairExtract and FairGPT achieve
superior fairness while maintaining competitive summarization quality.
Additionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that
integrate quality and fairness into a single evaluation framework, offering a
more nuanced understanding of the trade-offs between these objectives. Our code
is available online.",2024-11-12,"Sina Bagheri Nezhad, Sayan Bandyapadhyay, Ameeta Agrawal",http://arxiv.org/pdf/2411.07521v5,cs.CL
SparrowVQE: Visual Question Explanation for Course Content Understanding,"Visual Question Answering (VQA) research seeks to create AI systems to answer
natural language questions in images, yet VQA methods often yield overly
simplistic and short answers. This paper aims to advance the field by
introducing Visual Question Explanation (VQE), which enhances the ability of
VQA to provide detailed explanations rather than brief responses and address
the need for more complex interaction with visual content. We first created an
MLVQE dataset from a 14-week streamed video machine learning course, including
885 slide images, 110,407 words of transcripts, and 9,416 designed
question-answer (QA) pairs. Next, we proposed a novel SparrowVQE, a small 3
billion parameters multimodal model. We trained our model with a three-stage
training mechanism consisting of multimodal pre-training (slide images and
transcripts feature alignment), instruction tuning (tuning the pre-trained
model with transcripts and QA pairs), and domain fine-tuning (fine-tuning slide
image and QA pairs). Eventually, our SparrowVQE can understand and connect
visual information using the SigLIP model with transcripts using the Phi-2
language model with an MLP adapter. Experimental results demonstrate that our
SparrowVQE achieves better performance in our developed MLVQE dataset and
outperforms state-of-the-art methods in the other five benchmark VQA datasets.
The source code is available at
\url{https://github.com/YoushanZhang/SparrowVQE}.",2024-11-12,"Jialu Li, Manish Kumar Thota, Ruslan Gokhman, Radek Holik, Youshan Zhang",http://arxiv.org/pdf/2411.07516v1,cs.CL
Rapid Response: Mitigating LLM Jailbreaks with a Few Examples,"As large language models (LLMs) grow more powerful, ensuring their safety
against misuse becomes crucial. While researchers have focused on developing
robust defenses, no method has yet achieved complete invulnerability to
attacks. We propose an alternative approach: instead of seeking perfect
adversarial robustness, we develop rapid response techniques to look to block
whole classes of jailbreaks after observing only a handful of attacks. To study
this setting, we develop RapidResponseBench, a benchmark that measures a
defense's robustness against various jailbreak strategies after adapting to a
few observed examples. We evaluate five rapid response methods, all of which
use jailbreak proliferation, where we automatically generate additional
jailbreaks similar to the examples observed. Our strongest method, which
fine-tunes an input classifier to block proliferated jailbreaks, reduces attack
success rate by a factor greater than 240 on an in-distribution set of
jailbreaks and a factor greater than 15 on an out-of-distribution set, having
observed just one example of each jailbreaking strategy. Moreover, further
studies suggest that the quality of proliferation model and number of
proliferated examples play an key role in the effectiveness of this defense.
Overall, our results highlight the potential of responding rapidly to novel
jailbreaks to limit LLM misuse.",2024-11-12,"Alwin Peng, Julian Michael, Henry Sleight, Ethan Perez, Mrinank Sharma",http://arxiv.org/pdf/2411.07494v1,cs.CL
Controlled Evaluation of Syntactic Knowledge in Multilingual Language Models,"Language models (LMs) are capable of acquiring elements of human-like
syntactic knowledge. Targeted syntactic evaluation tests have been employed to
measure how well they form generalizations about syntactic phenomena in
high-resource languages such as English. However, we still lack a thorough
understanding of LMs' capacity for syntactic generalizations in low-resource
languages, which are responsible for much of the diversity of syntactic
patterns worldwide. In this study, we develop targeted syntactic evaluation
tests for three low-resource languages (Basque, Hindi, and Swahili) and use
them to evaluate five families of open-access multilingual Transformer LMs. We
find that some syntactic tasks prove relatively easy for LMs while others
(agreement in sentences containing indirect objects in Basque, agreement across
a prepositional phrase in Swahili) are challenging. We additionally uncover
issues with publicly available Transformers, including a bias toward the
habitual aspect in Hindi in multilingual BERT and underperformance compared to
similar-sized models in XGLM-4.5B.",2024-11-12,"Daria Kryvosheieva, Roger Levy",http://arxiv.org/pdf/2411.07474v2,cs.CL
IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark for LLMs,"Recent evaluations of LLMs on coreference resolution have revealed that
traditional output formats and evaluation metrics do not fully capture the
models' referential understanding. To address this, we introduce IdentifyMe, a
new benchmark for mention resolution presented in a multiple-choice question
(MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long
narratives and employs heuristics to exclude easily identifiable mentions,
creating a more challenging task. The benchmark also consists of a curated
mixture of different mention types and corresponding entities, allowing for a
fine-grained analysis of model performance. We evaluate both closed- and open
source LLMs on IdentifyMe and observe a significant performance gap (20-30%)
between the state-of-the-art sub-10B open models vs. closed ones. We observe
that pronominal mentions, which have limited surface information, are typically
much harder for models to resolve than nominal mentions. Additionally, we find
that LLMs often confuse entities when their mentions overlap in nested
structures. The highest-scoring model, GPT-4o, achieves 81.9% accuracy,
highlighting the strong referential capabilities of state-of-the-art LLMs while
also indicating room for further improvement.",2024-11-12,"Kawshik Manikantan, Makarand Tapaswi, Vineet Gandhi, Shubham Toshniwal",http://arxiv.org/pdf/2411.07466v2,cs.CL
BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating Machine Learning Tasks,"Large Language Models (LLMs) excel in diverse applications including
generation of code snippets, but often struggle with generating code for
complex Machine Learning (ML) tasks. Although existing LLM single-agent based
systems give varying performance depending on the task complexity, they purely
rely on larger and expensive models such as GPT-4. Our investigation reveals
that no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama
perform far worse than GPT-4 in a single-agent setting. With the motivation of
developing a cost-efficient LLM based solution for solving ML tasks, we propose
an LLM Multi-Agent based system which leverages combination of experts using
profiling, efficient retrieval of past observations, LLM cascades, and
ask-the-expert calls. Through empirical analysis on ML engineering tasks in the
MLAgentBench benchmark, we demonstrate the effectiveness of our system, using
no-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and
expert to serve occasional ask-the-expert calls for planning. With 94.2\%
reduction in the cost (from \$0.931 per run cost averaged over all tasks for
GPT-4 single agent system to \$0.054), our system is able to yield better
average success rate of 32.95\% as compared to GPT-4 single-agent system
yielding 22.72\% success rate averaged over all the tasks of MLAgentBench.",2024-11-12,"Shubham Gandhi, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff",http://arxiv.org/pdf/2411.07464v2,cs.CL
DecoPrompt : Decoding Prompts Reduces Hallucinations when Large Language Models Meet False Premises,"While large language models (LLMs) have demonstrated increasing power, they
have also called upon studies on their hallucinated outputs that deviate from
factually correct statements. In this paper, we focus on one important scenario
of false premises, where LLMs are distracted by misaligned claims although the
model possesses the required factual knowledge to answer original questions
accurately. Inspired by the observation that entropy of the false-premise
prompt is closely related to its likelihood to elicit hallucination generation,
we propose a new prompting algorithm, named DecoPrompt, to mitigate
hallucination. DecoPrompt leverages LLMs to ""decode"" the false-premise prompts
without really eliciting hallucination output from LLMs. We perform experiments
on two datasets, demonstrating that DecoPrompt can reduce hallucinations
effectively on outputs from different LLMs. Moreover, DecoPrompt exhibits
cross-model transferability, which facilitates its applications to scenarios
such as LLMs of large sizes or unavailable model logits.",2024-11-12,"Nan Xu, Xuezhe Ma",http://arxiv.org/pdf/2411.07457v2,cs.CL
Efficient and Accurate Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection,"Automatic prompt engineering aims to enhance the generation quality of large
language models (LLMs). Recent works utilize feedbacks generated from erroneous
cases to guide the prompt optimization. During inference, they may further
retrieve several semantically-related exemplars and concatenate them to the
optimized prompts to improve the performance. However, those works only utilize
the feedback at the current step, ignoring historical and unseleccted feedbacks
which are potentially beneficial. Moreover, the selection of exemplars only
considers the general semantic relationship and may not be optimal in terms of
task performance and matching with the optimized prompt. In this work, we
propose an Exemplar-Guided Reflection with Memory mechanism (ERM) to realize
more efficient and accurate prompt optimization. Specifically, we design an
exemplar-guided reflection mechanism where the feedback generation is
additionally guided by the generated exemplars. We further build two kinds of
memory to fully utilize the historical feedback information and support more
effective exemplar retrieval. Empirical evaluations show our method surpasses
previous state-of-the-arts with less optimization steps, i.e., improving F1
score by 10.1 on LIAR dataset, and reducing half of the optimization steps on
ProTeGi.",2024-11-12,"Cilin Yan, Jingyun Wang, Lin Zhang, Ruihui Zhao, Xiaopu Wu, Kai Xiong, Qingsong Liu, Guoliang Kang, Yangyang Kang",http://arxiv.org/pdf/2411.07446v1,cs.CL
Untangling Hate Speech Definitions: A Semantic Componential Analysis Across Cultures and Domains,"Hate speech relies heavily on cultural influences, leading to varying
individual interpretations. For that reason, we propose a Semantic Componential
Analysis (SCA) framework for a cross-cultural and cross-domain analysis of hate
speech definitions. We create the first dataset of hate speech definitions
encompassing 493 definitions from more than 100 cultures, drawn from five key
domains: online dictionaries, academic research, Wikipedia, legal texts, and
online platforms. By decomposing these definitions into semantic components,
our analysis reveals significant variation across definitions, yet many domains
borrow definitions from one another without taking into account the target
culture. We conduct zero-shot model experiments using our proposed dataset,
employing three popular open-sourced LLMs to understand the impact of different
definitions on hate speech detection. Our findings indicate that LLMs are
sensitive to definitions: responses for hate speech detection change according
to the complexity of definitions used in the prompt.",2024-11-11,"Katerina Korre, Arianna Muti, Federico Ruggeri, Alberto Barrón-Cedeño",http://arxiv.org/pdf/2411.07417v2,cs.CL
Using Generative AI and Multi-Agents to Provide Automatic Feedback,"This study investigates the use of generative AI and multi-agent systems to
provide automatic feedback in educational contexts, particularly for student
constructed responses in science assessments. The research addresses a key gap
in the field by exploring how multi-agent systems, called AutoFeedback, can
improve the quality of GenAI-generated feedback, overcoming known issues such
as over-praise and over-inference that are common in single-agent large
language models (LLMs). The study developed a multi-agent system consisting of
two AI agents: one for generating feedback and another for validating and
refining it. The system was tested on a dataset of 240 student responses, and
its performance was compared to that of a single-agent LLM. Results showed that
AutoFeedback significantly reduced the occurrence of over-praise and
over-inference errors, providing more accurate and pedagogically sound
feedback. The findings suggest that multi-agent systems can offer a more
reliable solution for generating automated feedback in educational settings,
highlighting their potential for scalable and personalized learning support.
These results have important implications for educators and researchers seeking
to leverage AI in formative assessments, offering a pathway to more effective
feedback mechanisms that enhance student learning outcomes.",2024-11-11,"Shuchen Guo, Ehsan Latif, Yifan Zhou, Xuan Huang, Xiaoming Zhai",http://arxiv.org/pdf/2411.07407v1,cs.CL
Controllable Context Sensitivity and the Knob Behind It,"When making predictions, a language model must trade off how much it relies
on its context vs. its prior knowledge. Choosing how sensitive the model is to
its context is a fundamental functionality, as it enables the model to excel at
tasks like retrieval-augmented generation and question-answering. In this
paper, we search for a knob which controls this sensitivity, determining
whether language models answer from the context or their prior knowledge. To
guide this search, we design a task for controllable context sensitivity. In
this task, we first feed the model a context (Paris is in England) and a
question (Where is Paris?); we then instruct the model to either use its prior
or contextual knowledge and evaluate whether it generates the correct answer
for both intents (either France or England). When fine-tuned on this task,
instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it
with high accuracy (85-95%). Analyzing these high-performing models, we narrow
down which layers may be important to context sensitivity using a novel linear
time algorithm. Then, in each model, we identify a 1-D subspace in a single
layer that encodes whether the model follows context or prior knowledge.
Interestingly, while we identify this subspace in a fine-tuned model, we find
that the exact same subspace serves as an effective knob in not only that model
but also non-fine-tuned instruct and base models of that model family. Finally,
we show a strong correlation between a model's performance and how distinctly
it separates context-agreeing from context-ignoring answers in this subspace.
These results suggest a single subspace facilitates how the model chooses
between context and prior knowledge, hinting at a simple fundamental mechanism
that controls this behavior.",2024-11-11,"Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell",http://arxiv.org/pdf/2411.07404v2,cs.CL
Beyond Keywords: A Context-based Hybrid Approach to Mining Ethical Concern-related App Reviews,"With the increasing proliferation of mobile applications in our everyday
experiences, the concerns surrounding ethics have surged significantly. Users
generally communicate their feedback, report issues, and suggest new
functionalities in application (app) reviews, frequently emphasizing safety,
privacy, and accountability concerns. Incorporating these reviews is essential
to developing successful products. However, app reviews related to ethical
concerns generally use domain-specific language and are expressed using a more
varied vocabulary. Thus making automated ethical concern-related app review
extraction a challenging and time-consuming effort.
  This study proposes a novel Natural Language Processing (NLP) based approach
that combines Natural Language Inference (NLI), which provides a deep
comprehension of language nuances, and a decoder-only (LLaMA-like) Large
Language Model (LLM) to extract ethical concern-related app reviews at scale.
Utilizing 43,647 app reviews from the mental health domain, the proposed
methodology 1) Evaluates four NLI models to extract potential privacy reviews
and compares the results of domain-specific privacy hypotheses with generic
privacy hypotheses; 2) Evaluates four LLMs for classifying app reviews to
privacy concerns; and 3) Uses the best NLI and LLM models further to extract
new privacy reviews from the dataset. Results show that the
DeBERTa-v3-base-mnli-fever-anli NLI model with domain-specific hypotheses
yields the best performance, and Llama3.1-8B-Instruct LLM performs best in the
classification of app reviews. Then, using NLI+LLM, an additional 1,008 new
privacy-related reviews were extracted that were not identified through the
keyword-based approach in previous research, thus demonstrating the
effectiveness of the proposed approach.",2024-11-11,"Aakash Sorathiya, Gouri Ginde",http://arxiv.org/pdf/2411.07398v1,cs.CL
Toward Optimal Search and Retrieval for RAG,"Retrieval-augmented generation (RAG) is a promising method for addressing
some of the memory-related challenges associated with Large Language Models
(LLMs). Two separate systems form the RAG pipeline, the retriever and the
reader, and the impact of each on downstream task performance is not
well-understood. Here, we work towards the goal of understanding how retrievers
can be optimized for RAG pipelines for common tasks such as Question Answering
(QA). We conduct experiments focused on the relationship between retrieval and
RAG performance on QA and attributed QA and unveil a number of insights useful
to practitioners developing high-performance RAG pipelines. For example,
lowering search accuracy has minor implications for RAG performance while
potentially increasing retrieval speed and memory efficiency.",2024-11-11,"Alexandria Leto, Cecilia Aguerrebere, Ishwar Bhati, Ted Willke, Mariano Tepper, Vy Ai Vo",http://arxiv.org/pdf/2411.07396v1,cs.CL
Isochrony-Controlled Speech-to-Text Translation: A study on translating from Sino-Tibetan to Indo-European Languages,"End-to-end speech translation (ST), which translates source language speech
directly into target language text, has garnered significant attention in
recent years. Many ST applications require strict length control to ensure that
the translation duration matches the length of the source audio, including both
speech and pause segments. Previous methods often controlled the number of
words or characters generated by the Machine Translation model to approximate
the source sentence's length without considering the isochrony of pauses and
speech segments, as duration can vary between languages. To address this, we
present improvements to the duration alignment component of our
sequence-to-sequence ST model. Our method controls translation length by
predicting the duration of speech and pauses in conjunction with the
translation process. This is achieved by providing timing information to the
decoder, ensuring it tracks the remaining duration for speech and pauses while
generating the translation. The evaluation on the Zh-En test set of CoVoST 2,
demonstrates that the proposed Isochrony-Controlled ST achieves 0.92 speech
overlap and 8.9 BLEU, which has only a 1.4 BLEU drop compared to the ST
baseline.",2024-11-11,"Midia Yousefi, Yao Qian, Junkun Chen, Gang Wang, Yanqing Liu, Dongmei Wang, Xiaofei Wang, Jian Xue",http://arxiv.org/pdf/2411.07387v1,cs.CL
MaLei at the PLABA Track of TREC 2024: RoBERTa for Term Replacement -- LLaMA3.1 and GPT-4o for Complete Abstract Adaptation,"This report is the system description of the MaLei team (Manchester and
Leiden) for the shared task Plain Language Adaptation of Biomedical Abstracts
(PLABA) 2024 (we had an earlier name BeeManc following last year), affiliated
with TREC2024 (33rd Text REtrieval Conference
https://ir.nist.gov/evalbase/conf/trec-2024). This report contains two sections
corresponding to the two sub-tasks in PLABA-2024. In task one (term
replacement), we applied fine-tuned ReBERTa-Base models to identify and
classify the difficult terms, jargon, and acronyms in the biomedical abstracts
and reported the F1 score (Task 1A and 1B). In task two (complete abstract
adaptation), we leveraged Llamma3.1-70B-Instruct and GPT-4o with the one-shot
prompts to complete the abstract adaptation and reported the scores in BLEU,
SARI, BERTScore, LENS, and SALSA. From the official Evaluation from PLABA-2024
on Task 1A and 1B, our much smaller fine-tuned RoBERTa-Base model ranked 3rd
and 2nd respectively on the two sub-tasks, and the 1st on averaged F1 scores
across the two tasks from 9 evaluated systems. Our LLaMA-3.1-70B-instructed
model achieved the highest Completeness score for Task 2. We share our source
codes, fine-tuned models, and related resources at
https://github.com/HECTA-UoM/PLABA2024",2024-11-11,"Zhidong Ling, Zihao Li, Pablo Romero, Lifeng Han, Goran Nenadic",http://arxiv.org/pdf/2411.07381v4,cs.CL
Multi-head Span-based Detector for AI-generated Fragments in Scientific Papers,"This paper describes a system designed to distinguish between AI-generated
and human-written scientific excerpts in the DAGPap24 competition hosted within
the Fourth Workshop on Scientific Document Processing. In this competition the
task is to find artificially generated token-level text fragments in documents
of a scientific domain. Our work focuses on the use of a multi-task learning
architecture with two heads. The application of this approach is justified by
the specificity of the task, where class spans are continuous over several
hundred characters. We considered different encoder variations to obtain a
state vector for each token in the sequence, as well as a variation in
splitting fragments into tokens to further feed into the input of a
transform-based encoder. This approach allows us to achieve a 9% quality
improvement relative to the baseline solution score on the development set
(from 0.86 to 0.95) using the average macro F1-score, as well as a score of
0.96 on a closed test part of the dataset from the competition.",2024-11-11,"German Gritsai, Ildar Khabutdinov, Andrey Grabovoy",http://arxiv.org/pdf/2411.07343v1,cs.CL
SetLexSem Challenge: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models,"Set theory is foundational to mathematics and, when sets are finite, to
reasoning about the world. An intelligent system should perform set operations
consistently, regardless of superficial variations in the operands. Initially
designed for semantically-oriented NLP tasks, large language models (LLMs) are
now being evaluated on algorithmic tasks. Because sets are comprised of
arbitrary symbols (e.g. numbers, words), they provide an opportunity to test,
systematically, the invariance of LLMs' algorithmic abilities under simple
lexical or semantic variations. To this end, we present the SetLexSem
Challenge, a synthetic benchmark that evaluates the performance of LLMs on set
operations. SetLexSem assesses the robustness of LLMs' instruction-following
abilities under various conditions, focusing on the set operations and the
nature and construction of the set members. Evaluating seven LLMs with
SetLexSem, we find that they exhibit poor robustness to variation in both
operation and operands. We show -- via the framework's systematic sampling of
set members along lexical and semantic dimensions -- that LLMs are not only not
robust to variation along these dimensions but demonstrate unique failure modes
in particular, easy-to-create semantic groupings of ""deceptive"" sets. We find
that rigorously measuring language model robustness to variation in frequency
and length is challenging and present an analysis that measures them
independently. The code for reproducing the results of this paper, and for
generating the SetLexSem Challenge dataset, is available at
\href{https://github.com/amazon-science/SetLexSem-Challenge}{https://github.com/amazon-science/SetLexSem-Challenge}.",2024-11-11,"Bardiya Akhbari, Manish Gawali, Nicholas A. Dronen",http://arxiv.org/pdf/2411.07336v1,cs.CL
Richer Output for Richer Countries: Uncovering Geographical Disparities in Generated Stories and Travel Recommendations,"While a large body of work inspects language models for biases concerning
gender, race, occupation and religion, biases of geographical nature are
relatively less explored. Some recent studies benchmark the degree to which
large language models encode geospatial knowledge. However, the impact of the
encoded geographical knowledge (or lack thereof) on real-world applications has
not been documented. In this work, we examine large language models for two
common scenarios that require geographical knowledge: (a) travel
recommendations and (b) geo-anchored story generation. Specifically, we study
five popular language models, and across about $100$K travel requests, and
$200$K story generations, we observe that travel recommendations corresponding
to poorer countries are less unique with fewer location references, and stories
from these regions more often convey emotions of hardship and sadness compared
to those from wealthier nations.",2024-11-11,"Kirti Bhagat, Kinshuk Vasisht, Danish Pruthi",http://arxiv.org/pdf/2411.07320v2,cs.CL
The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,"Language models (LMs) have shown impressive performance on tasks within their
training distribution, but often struggle with structurally novel tasks even
when given a small number of in-context task examples. We investigate the
effectiveness of test-time training (TTT) -- temporarily updating model
parameters during inference using a loss derived from input data -- as a
mechanism for improving LMs' reasoning and few-shot learning capabilities. On
the Abstraction and Reasoning Corpus (ARC), performing TTT with in-context
examples yields up to $6\times$ higher accuracy compared to fine-tuned
baselines -- reaching $53.0\%$ on the public validation set with an
8B-parameter LM and $61.9\%$ when ensembled with program-synthesis methods,
matching average human performance. On BIG-Bench Hard (BBH), TTT on in-context
examples surpasses standard few-shot prompting in the $10$-shot setting by
$7.3$ percentage points ($50.5\%$ to $57.8\%$). Our findings highlight the
limitations of in-context learning for novel tasks and demonstrate the
potential of test-time training to enhance language model adaptability.",2024-11-11,"Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, Jacob Andreas",http://arxiv.org/pdf/2411.07279v2,cs.CL
UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts,"The evaluation of mathematical reasoning capabilities is essential for
advancing Artificial General Intelligence (AGI). While Large Language Models
(LLMs) have shown impressive performance in solving mathematical problems,
existing benchmarks such as GSM8K and MATH present limitations, including
narrow problem definitions with specific numbers and reliance on predetermined
rules that hinder accurate assessments of reasoning and generality. This paper
introduces the UTMath Benchmark, a robust evaluation framework designed to
assess LLMs through extensive unit tests, with a focus on both the accuracy and
generality of model responses. It comprises 1,053 cutting-edge problems
spanning nine mathematical domains, with an average of 68 test cases per
problem. UTMath is highly challenging, with the best-performing model, o1-mini,
solving only 32.57\% of the problems, followed by o1-preview at 27.16\%, and
GPT-4o at 26.93\%. Furthermore, we present the Reasoning-to-Coding of Thoughts
(RCoT) approach, which encourages LLMs to engage in explicit reasoning prior to
code generation, thereby facilitating the production of more sophisticated
solutions and enhancing overall performance and efficiency. Additionally, we
also release the UTMath-Train training dataset (more than 70k samples), to
support the community in further exploring mathematical reasoning. Our
benchmark can be accessed via the following link:
https://github.com/UTMathGroup/UTMath",2024-11-11,"Bo Yang, Qingping Yang, Yingwei Ma, Runtao Liu",http://arxiv.org/pdf/2411.07240v2,cs.CL
OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model,"OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,
finetuned on over 2,000,000 Thai instruction pairs. This report provides an
engineering perspective on the model's development, capabilities, and
performance. We discuss the model's architecture, training process, and key
features, including multi-turn conversation support, Retrieval Augmented
Generation (RAG) compatibility, and tool-calling functionality. Benchmark
results demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various
Thai language tasks, outperforming other open-source Thai language models. We
also address practical considerations such as GPU memory requirements and
deployment strategies.",2024-11-11,"Sumeth Yuenyong, Kobkrit Viriyayudhakorn, Apivadee Piyatumrong, Jillaphat Jaroenkantasima",http://arxiv.org/pdf/2411.07238v2,cs.CL
Contextualized Evaluations: Judging Language Model Responses to Underspecified Queries,"Language model users often issue queries that lack specification, where the
context under which a query was issued -- such as the user's identity, the
query's intent, and the criteria for a response to be useful -- is not
explicit. For instance, a good response to a subjective query like ""What book
should I read next?"" would depend on the user's preferences, and a good
response to an open-ended query like ""How do antibiotics work against
bacteria?"" would depend on the user's expertise. This makes evaluation of
responses to such queries an ill-posed task, as evaluators may make arbitrary
judgments about the response quality. To remedy this, we present contextualized
evaluations, a protocol that synthetically constructs context surrounding an
underspecified query and provides it during evaluation. We find that the
presence of context can 1) alter conclusions drawn from evaluation, even
flipping benchmark rankings between model pairs, 2) nudge evaluators to make
fewer judgments based on surface-level criteria, like style, and 3) provide new
insights about model behavior across diverse contexts. Specifically, our
procedure suggests a potential bias towards WEIRD (Western, Educated,
Industrialized, Rich and Democratic) contexts in models' ""default"" responses
and we find that models are not equally sensitive to following different
contexts, even when they are provided in prompts.",2024-11-11,"Chaitanya Malaviya, Joseph Chee Chang, Dan Roth, Mohit Iyyer, Mark Yatskar, Kyle Lo",http://arxiv.org/pdf/2411.07237v2,cs.CL
TempCharBERT: Keystroke Dynamics for Continuous Access Control Based on Pre-trained Language Models,"With the widespread of digital environments, reliable authentication and
continuous access control has become crucial. It can minimize cyber attacks and
prevent frauds, specially those associated with identity theft. A particular
interest lies on keystroke dynamics (KD), which refers to the task of
recognizing individuals' identity based on their unique typing style. In this
work, we propose the use of pre-trained language models (PLMs) to recognize
such patterns. Although PLMs have shown high performance on multiple NLP
benchmarks, the use of these models on specific tasks requires customization.
BERT and RoBERTa, for instance, rely on subword tokenization, and they cannot
be directly applied to KD, which requires temporal-character information to
recognize users. Recent character-aware PLMs are able to process both subwords
and character-level information and can be an alternative solution.
Notwithstanding, they are still not suitable to be directly fine-tuned for KD
as they are not optimized to account for user's temporal typing information
(e.g., hold time and flight time). To overcome this limitation, we propose
TempCharBERT, an architecture that incorporates temporal-character information
in the embedding layer of CharBERT. This allows modeling keystroke dynamics for
the purpose of user identification and authentication. Our results show a
significant improvement with this customization. We also showed the feasibility
of training TempCharBERT on a federated learning settings in order to foster
data privacy.",2024-11-11,"Matheus Simão, Fabiano Prado, Omar Abdul Wahab, Anderson Avila",http://arxiv.org/pdf/2411.07224v1,cs.CL
TreeCoders: Trees of Transformers,"In this paper, we introduce TreeCoders, a novel family of transformer trees.
We moved away from traditional linear transformers to complete k-ary trees.
Transformer blocks serve as nodes, and generic classifiers learn to select the
best child and route the sequence of tokens to a specific leaf. The selectors,
moved outside the transformer blocks, allow for the use of a variety of
architecture without further modifications. Furthermore, our proposed
architecture supports sparse node activation due to the logarithmic complexity
of a tree search. We validate our idea by testing a series of decoder-only tree
transformers, achieving competitive results across a diverse range of language
datasets. Our study demonstrates that the proposed tree transformer model
outperforms a size-equivalent linear transformer model 76\% of the time over a
wide range of tree architectures. Furthermore, our proposed model naturally
lends itself to distributed implementation.",2024-11-11,"Pierre Colonna D'Istria, Abdulrahman Altahhan",http://arxiv.org/pdf/2411.07218v1,cs.CL
The Super Weight in Large Language Models,"Recent works have shown a surprising result: a small fraction of Large
Language Model (LLM) parameter outliers are disproportionately important to the
quality of the model. LLMs contain billions of parameters, so these small
fractions, such as 0.01%, translate to hundreds of thousands of parameters. In
this work, we present an even more surprising finding: Pruning as few as a
single parameter can destroy an LLM's ability to generate text -- increasing
perplexity by 3 orders of magnitude and reducing zero-shot accuracy to
guessing. We propose a data-free method for identifying such parameters, termed
super weights, using a single forward pass through the model. We additionally
find that these super weights induce correspondingly rare and large activation
outliers, termed super activations. When preserved with high precision, super
activations can improve simple round-to-nearest quantization to become
competitive with state-of-the-art methods. For weight quantization, we
similarly find that by preserving the super weight and clipping other weight
outliers, round-to-nearest quantization can scale to much larger block sizes
than previously considered. To facilitate further research into super weights,
we provide an index of super weight coordinates for common, openly available
LLMs.",2024-11-11,"Mengxia Yu, De Wang, Qi Shan, Colorado Reed, Alvin Wan",http://arxiv.org/pdf/2411.07191v1,cs.CL
Gumbel Counterfactual Generation From Language Models,"Understanding and manipulating the causal generation mechanisms in language
models is essential for controlling their behavior. Previous work has primarily
relied on techniques such as representation surgery -- e.g., model ablations or
manipulation of linear subspaces tied to specific concepts -- to
\emph{intervene} on these models. To understand the impact of interventions
precisely, it is useful to examine \emph{counterfactuals} -- e.g., how a given
sentence would have appeared had it been generated by the model following a
specific intervention. We highlight that counterfactual reasoning is
conceptually distinct from interventions, as articulated in Pearl's causal
hierarchy. Based on this observation, we propose a framework for generating
true string counterfactuals by reformulating language models as a structural
equation model using the Gumbel-max trick, which we called Gumbel
counterfactual generation. This reformulation allows us to model the joint
distribution over original strings and their counterfactuals resulting from the
same instantiation of the sampling noise. We develop an algorithm based on
hindsight Gumbel sampling that allows us to infer the latent noise variables
and generate counterfactuals of observed strings. Our experiments demonstrate
that the approach produces meaningful counterfactuals while at the same time
showing that commonly used intervention techniques have considerable undesired
side effects.",2024-11-11,"Shauli Ravfogel, Anej Svete, Vésteinn Snæbjarnarson, Ryan Cotterell",http://arxiv.org/pdf/2411.07180v5,cs.CL
More Expressive Attention with Negative Weights,"We propose a novel attention mechanism, named Cog Attention, that enables
attention weights to be negative for enhanced expressiveness, which stems from
two key factors: (1) Cog Attention enhances parameter flexibility. For example,
unlike traditional softmax attention heads that use a static output-value (OV)
matrix to delete or copy inputs that the heads attend to, Cog Attention
naturally learns to use the sign of dynamic query-key (QK) inner products to
represent these operations. This enables Cog Attention to perform multiple
operations simultaneously within a single head. Meanwhile, Cog Attention's OV
matrix can focus more on refinement or modification. (2) Cog Attention enhances
the model's robustness against representational collapse by preventing the
``over-squashing'' of earlier tokens into later positions. We develop
Transformer-like models which use Cog Attention as attention modules, including
decoder-only models at various scales for language modeling and U-ViT diffusion
models for image generation. Experiments show that models using Cog Attention
exhibit superior performance compared to those employing traditional softmax
attention modules. Our approach suggests a promising research direction for
rethinking and breaking the entrenched constraints of traditional softmax
attention, such as the requirement for non-negative weights.",2024-11-11,"Ang Lv, Ruobing Xie, Shuaipeng Li, Jiayi Liao, Xingwu Sun, Zhanhui Kang, Di Wang, Rui Yan",http://arxiv.org/pdf/2411.07176v3,cs.CL
Continual Memorization of Factoids in Language Models,"As new knowledge rapidly accumulates, language models (LMs) with pretrained
knowledge quickly become obsolete. A common approach to updating LMs is
fine-tuning them directly on new knowledge. However, recent studies have shown
that fine-tuning for memorization may be ineffective in storing knowledge or
may exacerbate hallucinations. In this work, we introduce a setting we call
continual memorization, where a model must memorize and retain a set of
factoids through multiple stages of fine-tuning on subsequent datasets. We
characterized the forgetting patterns through extensive experiments and show
that LMs widely suffer from forgetting, especially when needing to memorize
factoids in the second stage. We posit that forgetting can be alleviated by
modifying training dynamics: (1) protecting the memorization process when
learning factoids or (2) reducing interference from subsequent training stages.
Intriguingly, we find that mixing randomly generated word sequences or generic
data sampled from pretraining corpora at different training stages effectively
mitigates forgetting REMIX: Random and Generic Data Mixing). REMIX can recover
performance from severe forgetting, outperforming replay methods and other
continual learning baselines. We analyze how REMIX influences the learning
process and find that robust memorization follows a distinct pattern: the model
stores factoids in earlier layers than usual and diversifies the layers that
retain them, which results in easier recall and manipulate of the learned
factoids.",2024-11-11,"Howard Chen, Jiayi Geng, Adithya Bhaskar, Dan Friedman, Danqi Chen",http://arxiv.org/pdf/2411.07175v2,cs.CL
A Primer on Word Embeddings: AI Techniques for Text Analysis in Social Work,"Word embeddings represent a transformative technology for analyzing text data
in social work research, offering sophisticated tools for understanding case
notes, policy documents, research literature, and other text-based materials.
This methodological paper introduces word embeddings to social work
researchers, explaining how these mathematical representations capture meaning
and relationships in text data more effectively than traditional keyword-based
approaches. We discuss fundamental concepts, technical foundations, and
practical applications, including semantic search, clustering, and retrieval
augmented generation. The paper demonstrates how embeddings can enhance
research workflows through concrete examples from social work practice, such as
analyzing case notes for housing instability patterns and comparing social work
licensing examinations across languages. While highlighting the potential of
embeddings for advancing social work research, we acknowledge limitations
including information loss, training data constraints, and potential biases. We
conclude that successfully implementing embedding technologies in social work
requires developing domain-specific models, creating accessible tools, and
establishing best practices aligned with social work's ethical principles. This
integration can enhance our ability to analyze complex patterns in text data
while supporting more effective services and interventions.",2024-11-11,"Brian E. Perron, Kelley A. Rivenburgh, Bryan G. Victor, Zia Qi, Hui Luan",http://arxiv.org/pdf/2411.07156v1,cs.CL
HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals,"Task-Oriented Dialogue (TOD) systems assist users in completing tasks through
natural language interactions, often relying on a single-layered workflow
structure for slot-filling in public tasks, such as hotel bookings. However, in
enterprise environments, which involve rich domain-specific knowledge, TOD
systems face challenges due to task complexity and the lack of standardized
documentation. In this work, we introduce HierTOD, an enterprise TOD system
driven by hierarchical goals and can support composite workflows. By focusing
on goal-driven interactions, our system serves a more proactive role,
facilitating mixed-initiative dialogue and improving task completion. Equipped
with components for natural language understanding, composite goal retriever,
dialogue management, and response generation, backed by a well-organized data
service with domain knowledge base and retrieval engine, HierTOD delivers
efficient task assistance. Furthermore, our system implementation unifies two
TOD paradigms: slot-filling for information collection and step-by-step
guidance for task execution. Our human study demonstrates the effectiveness and
helpfulness of HierTOD in performing both paradigms.",2024-11-11,"Lingbo Mo, Shun Jiang, Akash Maharaj, Bernard Hishamunda, Yunyao Li",http://arxiv.org/pdf/2411.07152v1,cs.CL
Greenback Bears and Fiscal Hawks: Finance is a Jungle and Text Embeddings Must Adapt,"Financial documents are filled with specialized terminology, arcane jargon,
and curious acronyms that pose challenges for general-purpose text embeddings.
Yet, few text embeddings specialized for finance have been reported in the
literature, perhaps in part due to a lack of public datasets and benchmarks. We
present BAM embeddings, a set of text embeddings finetuned on a carefully
constructed dataset of 14.3M query-passage pairs. Demonstrating the benefits of
domain-specific training, BAM embeddings achieve Recall@1 of 62.8% on a
held-out test set, vs. only 39.2% for the best general-purpose text embedding
from OpenAI. Further, BAM embeddings increase question answering accuracy by 8%
on FinanceBench and show increased sensitivity to the finance-specific elements
that are found in detailed, forward-looking and company and date-specific
queries. To support further research we describe our approach in detail,
quantify the importance of hard negative mining and dataset scale.",2024-11-11,"Peter Anderson, Mano Vikash Janardhanan, Jason He, Wei Cheng, Charlie Flanagan",http://arxiv.org/pdf/2411.07142v1,cs.CL
Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models,"New LLM evaluation benchmarks are important to align with the rapid
development of Large Language Models (LLMs). In this work, we present Chinese
SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality
ability of language models to answer short questions, and Chinese SimpleQA
mainly has five properties (i.e., Chinese, Diverse, High-quality, Static,
Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6
major topics with 99 diverse subtopics. Second, we conduct a comprehensive
quality control process to achieve high-quality questions and answers, where
the reference answers are static and cannot be changed over time. Third,
following SimpleQA, the questions and answers are very short, and the grading
process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we
perform a comprehensive evaluation on the factuality abilities of existing
LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to
better understand the Chinese factuality abilities of their models and
facilitate the growth of foundation models.",2024-11-11,"Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Zhuoran Lin, Xuepeng Liu, Dekai Sun, Shirong Lin, Zhicheng Zheng, Xiaoyong Zhu, Wenbo Su, Bo Zheng",http://arxiv.org/pdf/2411.07140v2,cs.CL
Stronger Models are NOT Stronger Teachers for Instruction Tuning,"Instruction tuning has been widely adopted to ensure large language models
(LLMs) follow user instructions effectively. The resulting
instruction-following capabilities of LLMs heavily rely on the instruction
datasets used for tuning. Recently, synthetic instruction datasets have emerged
as an economically viable solution to provide LLMs diverse and high-quality
instructions. However, existing approaches typically assume that larger or
stronger models are stronger teachers for instruction tuning, and hence simply
adopt these models as response generators to the synthetic instructions. In
this paper, we challenge this commonly-adopted assumption. Our extensive
experiments across five base models and twenty response generators reveal that
larger and stronger models are not necessarily stronger teachers of smaller
models. We refer to this phenomenon as the Larger Models' Paradox. We observe
that existing metrics cannot precisely predict the effectiveness of response
generators since they ignore the compatibility between teachers and base models
being fine-tuned. We thus develop a novel metric, named as
Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response
generators. Our experiments across five base models demonstrate that CAR
outperforms almost all baselines.",2024-11-11,"Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Radha Poovendran",http://arxiv.org/pdf/2411.07133v3,cs.CL
On Many-Shot In-Context Learning for Long-Context Evaluation,"Many-shot in-context learning (ICL) has emerged as a unique setup to both
utilize and test the ability of large language models to handle long context.
This paper delves into long-context language model (LCLM) evaluation through
many-shot ICL. We first ask: what types of ICL tasks benefit from additional
demonstrations, and how effective are they in evaluating LCLMs? We find that
classification and summarization tasks show performance improvements with
additional demonstrations, while translation and reasoning tasks do not exhibit
clear trends. Next, we investigate the extent to which different tasks
necessitate retrieval versus global context understanding. We develop metrics
to categorize ICL tasks into two groups: (i) similar-sample learning (SSL):
tasks where retrieval of the most similar examples is sufficient for good
performance, and (ii) all-sample learning (ASL): tasks that necessitate a
deeper comprehension of all examples in the prompt. Lastly, we introduce a new
many-shot ICL benchmark, MANYICLBENCH, to characterize model's ability on both
fronts and benchmark 12 LCLMs using MANYICLBENCH. We find that while
state-of-the-art models demonstrate good performance up to 64k tokens in SSL
tasks, many models experience significant performance drops at only 16k tokens
in ASL tasks.",2024-11-11,"Kaijian Zou, Muhammad Khalifa, Lu Wang",http://arxiv.org/pdf/2411.07130v2,cs.CL
Benchmarking LLMs' Judgments with No Gold Standard,"We introduce the GEM (Generative Estimator for Mutual Information), an
evaluation metric for assessing language generation by Large Language Models
(LLMs), particularly in generating informative judgments, without the need for
a gold standard reference. GEM broadens the scenarios where we can benchmark
LLM generation performance-from traditional ones, like machine translation and
summarization, where gold standard references are readily available, to
subjective tasks without clear gold standards, such as academic peer review.
  GEM uses a generative model to estimate mutual information between candidate
and reference responses, without requiring the reference to be a gold standard.
In experiments on a human-annotated dataset, GEM demonstrates competitive
correlations with human scores compared to the state-of-the-art GPT-4o
Examiner, and outperforms all other baselines. Additionally, GEM is more robust
against strategic manipulations, such as rephrasing or elongation, which can
artificially inflate scores under a GPT-4o Examiner.
  We also present GRE-bench (Generating Review Evaluation Benchmark) which
evaluates LLMs based on how well they can generate high-quality peer reviews
for academic research papers. Because GRE-bench is based upon GEM, it inherits
its robustness properties. Additionally, GRE-bench circumvents data
contamination problems (or data leakage) by using the continuous influx of new
open-access research papers and peer reviews each year. We show GRE-bench
results of various popular LLMs on their peer review capabilities using the
ICLR2023 dataset.",2024-11-11,"Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong",http://arxiv.org/pdf/2411.07127v2,cs.CL
SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering in LLMs,"Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating human-like text, but their output may not be aligned with the user
or even produce harmful content. This paper presents a novel approach to detect
and steer concepts such as toxicity before generation. We introduce the Sparse
Conditioned Autoencoder (SCAR), a single trained module that extends the
otherwise untouched LLM. SCAR ensures full steerability, towards and away from
concepts (e.g., toxic content), without compromising the quality of the model's
text generation on standard evaluation benchmarks. We demonstrate the effective
application of our approach through a variety of concepts, including toxicity,
safety, and writing style alignment. As such, this work establishes a robust
framework for controlling LLM generations, ensuring their ethical and safe
deployment in real-world applications.",2024-11-11,"Ruben Härle, Felix Friedrich, Manuel Brack, Björn Deiseroth, Patrick Schramowski, Kristian Kersting",http://arxiv.org/pdf/2411.07122v2,cs.CL
Building a Taiwanese Mandarin Spoken Language Model: A First Attempt,"This technical report presents our initial attempt to build a spoken large
language model (LLM) for Taiwanese Mandarin, specifically tailored to enable
real-time, speech-to-speech interaction in multi-turn conversations. Our
end-to-end model incorporates a decoder-only transformer architecture and aims
to achieve seamless interaction while preserving the conversational flow,
including full-duplex capabilities allowing simultaneous speaking and
listening. The paper also details the training process, including data
preparation with synthesized dialogues and adjustments for real-time
interaction. We also developed a platform to evaluate conversational fluency
and response coherence in multi-turn dialogues. We hope the release of the
report can contribute to the future development of spoken LLMs in Taiwanese
Mandarin.",2024-11-11,"Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, Tzu-Quan Lin, Hsiu-Hsuan Wang, En-Pei Hu, Chan-Jan Hsu, Liang-Hsuan Tseng, I-Hsiang Chiu, Ulin Sanga, Xuanjun Chen, Po-chun Hsu, Shu-wen Yang, Hung-yi Lee",http://arxiv.org/pdf/2411.07111v2,cs.CL
Training Neural Networks as Recognizers of Formal Languages,"Characterizing the computational power of neural network architectures in
terms of formal language theory remains a crucial line of research, as it
describes lower and upper bounds on the reasoning capabilities of modern AI.
However, when empirically testing these bounds, existing work often leaves a
discrepancy between experiments and the formal claims they are meant to
support. The problem is that formal language theory pertains specifically to
recognizers: machines that receive a string as input and classify whether it
belongs to a language. On the other hand, it is common instead to evaluate
language models on proxy tasks, e.g., language modeling or sequence-to-sequence
transduction, that are similar in only an informal sense to the underlying
theory. We correct this mismatch by training and evaluating neural networks
directly as binary classifiers of strings, using a general method that can be
applied to a wide variety of languages. As part of this, we extend an algorithm
recently proposed by Sn{\ae}bjarnarson et al. (2025) for efficient
length-controlled sampling of strings from regular languages. We provide
results on a variety of languages across the Chomsky hierarchy for three neural
architectures: a simple RNN, an LSTM, and a causally-masked transformer. We
find that the RNN and LSTM often outperform the transformer, and that auxiliary
training objectives such as language modeling can help, although no single
objective uniformly improves performance across languages and architectures.
Our contributions will facilitate theoretically sound empirical testing of
language recognition claims in future work. We have released our datasets as a
benchmark called FLaRe (Formal Language Recognition), along with our code.",2024-11-11,"Alexandra Butoi, Ghazal Khalighinejad, Anej Svete, Josef Valvoda, Ryan Cotterell, Brian DuSell",http://arxiv.org/pdf/2411.07107v3,cs.CL
Transformer verbatim in-context retrieval across time and scale,"To predict upcoming text, language models must in some cases retrieve
in-context information verbatim. In this report, we investigated how the
ability of language models to retrieve arbitrary in-context nouns developed
during training (across time) and as language models trained on the same
dataset increase in size (across scale). We then asked whether learning of
in-context retrieval correlates with learning of more challenging zero-shot
benchmarks. Furthermore, inspired by semantic effects in human short-term
memory, we evaluated the retrieval with respect to a major semantic component
of target nouns, namely whether they denote a concrete or abstract entity, as
rated by humans. We show that verbatim in-context retrieval developed in a
sudden transition early in the training process, after about 1% of the training
tokens. This was observed across model sizes (from 14M and up to 12B
parameters), and the transition occurred slightly later for the two smallest
models. We further found that the development of verbatim in-context retrieval
is positively correlated with the learning of zero-shot benchmarks. Around the
transition point, all models showed the advantage of retrieving concrete nouns
as opposed to abstract nouns. In all but two smallest models, the advantage
dissipated away toward the end of training.",2024-11-11,"Kristijan Armeni, Marko Pranjić, Senja Pollak",http://arxiv.org/pdf/2411.07075v1,cs.CL
Universal Response and Emergence of Induction in LLMs,"While induction is considered a key mechanism for in-context learning in
LLMs, understanding its precise circuit decomposition beyond toy models remains
elusive. Here, we study the emergence of induction behavior within LLMs by
probing their response to weak single-token perturbations of the residual
stream. We find that LLMs exhibit a robust, universal regime in which their
response remains scale-invariant under changes in perturbation strength,
thereby allowing us to quantify the build-up of token correlations throughout
the model. By applying our method, we observe signatures of induction behavior
within the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across
all models, we find that these induction signatures gradually emerge within
intermediate layers and identify the relevant model sections composing this
behavior. Our results provide insights into the collective interplay of
components within LLMs and serve as a benchmark for large-scale circuit
analysis.",2024-11-11,Niclas Luick,http://arxiv.org/pdf/2411.07071v1,cs.CL
On Active Privacy Auditing in Supervised Fine-tuning for White-Box Language Models,"The pretraining and fine-tuning approach has become the leading technique for
various NLP applications. However, recent studies reveal that fine-tuning data,
due to their sensitive nature, domain-specific characteristics, and
identifiability, pose significant privacy concerns. To help develop more
privacy-resilient fine-tuning models, we introduce a novel active privacy
auditing framework, dubbed Parsing, designed to identify and quantify privacy
leakage risks during the supervised fine-tuning (SFT) of language models (LMs).
The framework leverages improved white-box membership inference attacks (MIAs)
as the core technology, utilizing novel learning objectives and a two-stage
pipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the
exposure of privacy risks. Additionally, we have improved the effectiveness of
MIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our
research aims to provide the SFT community of LMs with a reliable, ready-to-use
privacy auditing tool, and to offer valuable insights into safeguarding privacy
during the fine-tuning process. Experimental results confirm the framework's
efficiency across various models and tasks, emphasizing notable privacy
concerns in the fine-tuning process. Project code available for
https://anonymous.4open.science/r/PARSING-4817/.",2024-11-11,"Qian Sun, Hanpeng Wu, Xi Sheryl Zhang",http://arxiv.org/pdf/2411.07070v2,cs.CL
Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training,"Network pruning focuses on computational techniques that aim to reduce a
given model's computational cost by removing a subset of its parameters while
having minimal impact on performance. Throughout the last decade, the most
widely used pruning paradigm has been pruning and re-training, which nowadays
is inconvenient due to the vast amount of pre-trained models, which are in any
case too expensive to re-train. In this paper, we exploit functional
information from dense pre-trained models, i.e., their activations, to obtain
sparse models that maximize the activations' alignment w.r.t. their
corresponding dense models. Hence, we propose \textsc{NeuroAL}, a \emph{top-up}
algorithm that can be used on top of any given pruning algorithm for LLMs,
which modifies the block-wise and row-wise sparsity exploiting information from
both the dense model and its sparse version to maximize the \emph{neuron
alignment} among activations. Differently from existing methods, our approach
adaptively selects the best hyperparameters for the block-wise and row-wise
sparsity ratios w.r.t. the model and the desired sparsity, and requires
\emph{no re-training}. We test our method over 276 cases combining four LLM
families, three sparsity ratios, and ten language tasks (three language
modeling and seven zero-shot datasets), showing how it consistently outperforms
the latest state-of-the-art methods in terms of performance-runtime trade-off.
The code is available at
\href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.",2024-11-11,"Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca",http://arxiv.org/pdf/2411.07066v3,cs.CL
Minion: A Technology Probe for Resolving Value Conflicts through Expert-Driven and User-Driven Strategies in AI Companion Applications,"AI companions based on large language models can role-play and converse very
naturally. When value conflicts arise between the AI companion and the user, it
may offend or upset the user. Yet, little research has examined such conflicts.
We first conducted a formative study that analyzed 151 user complaints about
conflicts with AI companions, providing design implications for our study.
Based on these, we created Minion, a technology probe to help users resolve
human-AI value conflicts. Minion applies a user-empowerment intervention method
that provides suggestions by combining expert-driven and user-driven conflict
resolution strategies. We conducted a technology probe study, creating 40 value
conflict scenarios on Character.AI and Talkie. 22 participants completed 274
tasks and successfully resolved conflicts 94.16% of the time. We summarize user
responses, preferences, and needs in resolving value conflicts, and propose
design implications to reduce conflicts and empower users to resolve them more
effectively.",2024-11-11,"Xianzhe Fan, Qing Xiao, Xuhui Zhou, Yuran Su, Zhicong Lu, Maarten Sap, Hong Shen",http://arxiv.org/pdf/2411.07042v1,cs.CL
LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios,"As Large Language Models (LLMs) evolve in natural language processing (NLP),
their ability to stably follow instructions in long-context inputs has become
critical for real-world applications. However, existing benchmarks seldom focus
on instruction-following in long-context scenarios or stability on different
inputs. To bridge this gap, we introduce LIFBench, a scalable dataset designed
to evaluate LLMs' instruction-following capabilities and stability across long
contexts. LIFBench comprises three long-context scenarios and eleven diverse
tasks, featuring 2,766 instructions generated through an automated expansion
method across three dimensions: length, expression, and variables. For
evaluation, we propose LIFEval, a rubric-based assessment method that enables
precise, automated scoring of complex LLM responses without reliance on
LLM-assisted assessments or human judgment. This method allows for a
comprehensive analysis of model performance and stability from multiple
perspectives. We conduct detailed experiments on 20 prominent LLMs across six
length intervals. Our work contributes LIFBench and LIFEval as robust tools for
assessing LLM performance in complex and long-context settings, offering
valuable insights to guide future advancements in LLM development.",2024-11-11,"Xiaodong Wu, Minhao Wang, Yichen Liu, Xiaoming Shi, He Yan, Xiangju Lu, Junmin Zhu, Wei Zhang",http://arxiv.org/pdf/2411.07037v2,cs.CL
UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction,"Beyond-triple fact representations including hyper-relational facts with
auxiliary key-value pairs, temporal facts with additional timestamps, and
nested facts implying relationships between facts, are gaining significant
attention. However, constrained by complex fact representation forms, existing
link prediction models for beyond-triple facts have difficulty achieving
hierarchical fact modeling and generalizing the modules for one specific facts
to other fact types. To overcome this limitation, we propose a Unified
Hierarchical Representation learning framework (UniHR) for unified knowledge
graph link prediction. It consists of a unified Hierarchical Data
Representation (HiDR) module and a unified Hierarchical Structure Learning
(HiSL) module as graph encoder. The HiDR module unifies hyper-relational KGs,
temporal KGs, and nested factual KGs into triple-based representations. Then
HiSL incorporates intra-fact and inter-fact message passing, focusing on
enhancing the semantic information within individual facts and enriching the
structural information between facts. Empirical results demonstrate the
effectiveness of UniHR and highlight the strong potential of unified
representations. Code and data are available at
https://github.com/Lza12a/UniHR.",2024-11-11,"Zhiqiang Liu, Yin Hua, Mingyang Chen, Zhuo Chen, Ziqi Liu, Lei Liang, Huajun Chen, Wen Zhang",http://arxiv.org/pdf/2411.07019v3,cs.CL
The Backpropagation of the Wave Network,"This paper provides an in-depth analysis of Wave Network, a novel token
representation method derived from the Wave Network, designed to capture both
global and local semantics of input text through wave-inspired complex vectors.
In complex vector token representation, each token is represented with a
magnitude component, capturing the global semantics of the entire input text,
and a phase component, encoding the relationships between individual tokens and
the global semantics. Building on prior research that demonstrated the
effectiveness of wave-like operations, such as interference and modulation,
during forward propagation, this study investigates the convergence behavior,
backpropagation characteristics, and embedding independence within the
Token2Wave framework. A detailed computational complexity analysis shows that
Token2Wave can significantly reduce video memory usage and training time
compared to BERT. Gradient comparisons for the [CLS] token, total input text,
and classifier parameters further highlight Token2Wave's unique
characteristics. This research offers new insights into wave-based token
representations, demonstrating their potential to enable efficient and
computationally friendly language model architectures.",2024-11-11,"Xin Zhang, Victor S. Sheng",http://arxiv.org/pdf/2411.06989v2,cs.CL
Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring LLM's Perceptual Alignment with Human Smell Experiences,"Aligning AI with human intent is important, yet perceptual alignment-how AI
interprets what we see, hear, or smell-remains underexplored. This work focuses
on olfaction, human smell experiences. We conducted a user study with 40
participants to investigate how well AI can interpret human descriptions of
scents. Participants performed ""sniff and describe"" interactive tasks, with our
designed AI system attempting to guess what scent the participants were
experiencing based on their descriptions. These tasks evaluated the Large
Language Model's (LLMs) contextual understanding and representation of scent
relationships within its internal states - high-dimensional embedding space.
Both quantitative and qualitative methods were used to evaluate the AI system's
performance. Results indicated limited perceptual alignment, with biases
towards certain scents, like lemon and peppermint, and continued failing to
identify others, like rosemary. We discuss these findings in light of human-AI
alignment advancements, highlighting the limitations and opportunities for
enhancing HCI systems with multisensory experience integration.",2024-11-11,"Shu Zhong, Zetao Zhou, Christopher Dawes, Giada Brianz, Marianna Obrist",http://arxiv.org/pdf/2411.06950v1,cs.CL
Cancer-Answer: Empowering Cancer Care with Advanced Large Language Models,"Gastrointestinal (GI) tract cancers account for a substantial portion of the
global cancer burden, where early diagnosis is critical for improved management
and patient outcomes. The complex aetiologies and overlapping symptoms across
GI cancers often delay diagnosis, leading to suboptimal treatment strategies.
Cancer-related queries are crucial for timely diagnosis, treatment, and patient
education, as access to accurate, comprehensive information can significantly
influence outcomes. However, the complexity of cancer as a disease, combined
with the vast amount of available data, makes it difficult for clinicians and
patients to quickly find precise answers. To address these challenges, we
leverage large language models (LLMs) such as GPT-3.5 Turbo to generate
accurate, contextually relevant responses to cancer-related queries.
Pre-trained with medical data, these models provide timely, actionable insights
that support informed decision-making in cancer diagnosis and care, ultimately
improving patient outcomes. We calculate two metrics: A1 (which represents the
fraction of entities present in the model-generated answer compared to the gold
standard) and A2 (which represents the linguistic correctness and
meaningfulness of the model-generated answer with respect to the gold
standard), achieving maximum values of 0.546 and 0.881, respectively.",2024-11-11,"Aniket Deroy, Subhankar Maity",http://arxiv.org/pdf/2411.06946v2,cs.CL
Multi-class Decoding of Attended Speaker Direction Using Electroencephalogram and Audio Spatial Spectrum,"Decoding the directional focus of an attended speaker from listeners'
electroencephalogram (EEG) signals is essential for developing brain-computer
interfaces to improve the quality of life for individuals with hearing
impairment. Previous works have concentrated on binary directional focus
decoding, i.e., determining whether the attended speaker is on the left or
right side of the listener. However, a more precise decoding of the exact
direction of the attended speaker is necessary for effective speech processing.
Additionally, audio spatial information has not been effectively leveraged,
resulting in suboptimal decoding results. In this paper, it is found that on
the recently presented dataset with 14-class directional focus, models relying
exclusively on EEG inputs exhibit significantly lower accuracy when decoding
the directional focus in both leave-one-subject-out and leave-one-trial-out
scenarios. By integrating audio spatial spectra with EEG features, the decoding
accuracy can be effectively improved. The CNN, LSM-CNN, and Deformer models are
employed to decode the directional focus from listeners' EEG signals and audio
spatial spectra. The proposed Sp-EEG-Deformer model achieves notable 14-class
decoding accuracies of 55.35% and 57.19% in leave-one-subject-out and
leave-one-trial-out scenarios with a decision window of 1 second, respectively.
Experiment results indicate increased decoding accuracy as the number of
alternative directions reduces. These findings suggest the efficacy of our
proposed dual modal directional focus decoding strategy.",2024-11-11,"Yuanming Zhang, Jing Lu, Fei Chen, Haoliang Du, Xia Gao, Zhibin Lin",http://arxiv.org/pdf/2411.06928v2,cs.CL
EVQAScore: A Fine-grained Metric for Video Question Answering Data Quality Evaluation,"Video question-answering (QA) is a core task in video understanding.
Evaluating the quality of video QA and video caption data quality for training
video large language models (VideoLLMs) is an essential challenge. Although
various methods have been proposed for assessing video caption quality, there
remains a lack of dedicated evaluation methods for Video QA. To address this
gap, we introduce EVQAScore, a reference-free method that leverages keyword
extraction to assess both video caption and video QA data quality.
Additionally, we incorporate frame sampling and rescaling techniques to enhance
the efficiency and robustness of our evaluation, this enables our score to
evaluate the quality of extremely long videos. Our approach achieves
state-of-the-art (SOTA) performance (32.8 for Kendall correlation and 42.3 for
Spearman correlation, 4.7 and 5.9 higher than the previous method PAC-S++) on
the VATEX-EVAL benchmark for video caption evaluation. Furthermore, by using
EVQAScore for data selection, we achieved SOTA results with only 12.5\% of the
original data volume, outperforming the previous SOTA method PAC-S and 100\% of
data.",2024-11-11,"Hao Liang, Zirong Chen, Hejun Dong, Wentao Zhang",http://arxiv.org/pdf/2411.06908v3,cs.CL
LongSafety: Enhance Safety for Long-Context LLMs,"Recent advancements in model architectures and length extrapolation
techniques have significantly extended the context length of large language
models (LLMs), paving the way for their application in increasingly complex
tasks. However, despite the growing capabilities of long-context LLMs, the
safety issues in long-context scenarios remain underexplored. While safety
alignment in short context has been widely studied, the safety concerns of
long-context LLMs have not been adequately addressed. In this work, we
introduce \textbf{LongSafety}, a comprehensive safety alignment dataset for
long-context LLMs, containing 10 tasks and 17k samples, with an average length
of 40.9k tokens. Our experiments demonstrate that training with LongSafety can
enhance long-context safety performance while enhancing short-context safety
and preserving general capabilities. Furthermore, we demonstrate that
long-context safety does not equal long-context alignment with short-context
safety data and LongSafety has generalizing capabilities in context length and
long-context safety scenarios.",2024-11-11,"Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Qipeng Guo, Linyang Li, Chenkun Tan, Yang Gao, Pengyu Wang, Linlin Li, Qun Liu, Yaqian Zhou, Xipeng Qiu, Xuanjing Huang",http://arxiv.org/pdf/2411.06899v2,cs.CL
Subgraph Retrieval Enhanced by Graph-Text Alignment for Commonsense Question Answering,"Commonsense question answering is a crucial task that requires machines to
employ reasoning according to commonsense. Previous studies predominantly
employ an extracting-and-modeling paradigm to harness the information in KG,
which first extracts relevant subgraphs based on pre-defined rules and then
proceeds to design various strategies aiming to improve the representations and
fusion of the extracted structural knowledge. Despite their effectiveness,
there are still two challenges. On one hand, subgraphs extracted by rule-based
methods may have the potential to overlook critical nodes and result in
uncontrollable subgraph size. On the other hand, the misalignment between graph
and text modalities undermines the effectiveness of knowledge fusion,
ultimately impacting the task performance. To deal with the problems above, we
propose a novel framework: \textbf{S}ubgraph R\textbf{E}trieval Enhanced by
Gra\textbf{P}h-\textbf{T}ext \textbf{A}lignment, named \textbf{SEPTA}. Firstly,
we transform the knowledge graph into a database of subgraph vectors and
propose a BFS-style subgraph sampling strategy to avoid information loss,
leveraging the analogy between BFS and the message-passing mechanism. In
addition, we propose a bidirectional contrastive learning approach for
graph-text alignment, which effectively enhances both subgraph retrieval and
knowledge fusion. Finally, all the retrieved information is combined for
reasoning in the prediction module. Extensive experiments on five datasets
demonstrate the effectiveness and robustness of our framework.",2024-11-11,"Boci Peng, Yongchao Liu, Xiaohe Bo, Sheng Tian, Baokun Wang, Chuntao Hong, Yan Zhang",http://arxiv.org/pdf/2411.06866v1,cs.CL
A Unified Multi-Task Learning Architecture for Hate Detection Leveraging User-Based Information,"Hate speech, offensive language, aggression, racism, sexism, and other
abusive language are common phenomena in social media. There is a need for
Artificial Intelligence(AI)based intervention which can filter hate content at
scale. Most existing hate speech detection solutions have utilized the features
by treating each post as an isolated input instance for the classification.
This paper addresses this issue by introducing a unique model that improves
hate speech identification for the English language by utilising intra-user and
inter-user-based information. The experiment is conducted over single-task
learning (STL) and multi-task learning (MTL) paradigms that use deep neural
networks, such as convolutional neural networks (CNN), gated recurrent unit
(GRU), bidirectional encoder representations from the transformer (BERT), and A
Lite BERT (ALBERT). We use three benchmark datasets and conclude that combining
certain user features with textual features gives significant improvements in
macro-F1 and weighted-F1.",2024-11-11,"Prashant Kapil, Asif Ekbal",http://arxiv.org/pdf/2411.06855v2,cs.CL
Evaluating Large Language Models on Financial Report Summarization: An Empirical Study,"In recent years, Large Language Models (LLMs) have demonstrated remarkable
versatility across various applications, including natural language
understanding, domain-specific knowledge tasks, etc. However, applying LLMs to
complex, high-stakes domains like finance requires rigorous evaluation to
ensure reliability, accuracy, and compliance with industry standards. To
address this need, we conduct a comprehensive and comparative study on three
state-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their
effectiveness in generating automated financial reports. Our primary motivation
is to explore how these models can be harnessed within finance, a field
demanding precision, contextual relevance, and robustness against erroneous or
misleading information. By examining each model's capabilities, we aim to
provide an insightful assessment of their strengths and limitations. Our paper
offers benchmarks for financial report analysis, encompassing proposed metrics
such as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative
evaluation framework that integrates both quantitative metrics (e.g.,
precision, recall) and qualitative analyses (e.g., contextual fit, consistency)
to provide a holistic view of each model's output quality. Additionally, we
make our financial dataset publicly available, inviting researchers and
practitioners to leverage, scrutinize, and enhance our findings through broader
community engagement and collaborative improvement. Our dataset is available on
huggingface.",2024-11-11,"Xinqi Yang, Scott Zang, Yong Ren, Dingjie Peng, Zheng Wen",http://arxiv.org/pdf/2411.06852v1,cs.CL
"1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of Language, Hate Speech, and Targets using LLMs","This paper presents a detailed system description of our entry for the
CHiPSAL 2025 shared task, focusing on language detection, hate speech
identification, and target detection in Devanagari script languages. We
experimented with a combination of large language models and their ensembles,
including MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like
focal loss to address challenges in the natural understanding of Devanagari
languages, such as multilingual processing and class imbalance. Our approach
achieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804
for Sub-tasks A, B, and C respectively. This work provides insights into the
effectiveness of transformer models in tasks with domain-specific and
linguistic challenges, as well as areas for potential improvement in future
iterations.",2024-11-11,"Jebish Purbey, Siddartha Pullakhandam, Kanwal Mehreen, Muhammad Arham, Drishti Sharma, Ashay Srivastava, Ram Mohan Rao Kadiyala",http://arxiv.org/pdf/2411.06850v1,cs.CL
LLM-NEO: Parameter Efficient Knowledge Distillation for Large Language Models,"Knowledge distillation (KD) has been a predominant method for compressing
Large Language Models (LLMs). In this paper, we first revisit KD and Low-Rank
Adaption (LoRA) and demonstrate that they follow the same paradigm. Inspired by
this observation, we propose a parameter-efficient knowledge distillation
method, LLM-NEO, which integrates LoRA into KD to improve the efficiency of
knowledge transfer. After that, we summarize some valuable guidelines for the
hyperparameters in LLM-NEO. Experimental results on compressing Llama 2 and
Llama 3.2 show that LLM-NEO outperforms various baselines. Further analysis
demonstrates the robustness of the proposed LLM-NEO on variants of LoRA. The
code and trained models are available at
[Github](https://github.com/yang3121099/LLM-Neo).",2024-11-11,"Runming Yang, Taiqiang Wu, Jiahao Wang, Pengfei Hu, Yik-Chung Wu, Ngai Wong, Yujiu Yang",http://arxiv.org/pdf/2411.06839v2,cs.CL
A Survey on Importance of Homophones Spelling Correction Model for Khmer Authors,"Homophones present a significant challenge to authors in any languages due to
their similarities of pronunciations but different meanings and spellings. This
issue is particularly pronounced in the Khmer language, rich in homophones due
to its complex structure and extensive character set. This research aims to
address the difficulties faced by Khmer authors when using homophones in their
writing and proposes potential solutions based on an extensive literature
review and survey analysis. A survey of 108 Khmer native speakers, including
students, employees, and professionals, revealed that many frequently encounter
challenges with homophones in their writing, often struggling to choose the
correct word based on context. The survey also highlighted the absence of
effective tools to address homophone errors in Khmer, which complicates the
writing process. Additionally, a review of existing studies on spelling
correction in other languages, such as English, Azerbaijani, and Bangla,
identified a lack of research focused specifically on homophones, particularly
in the Khmer language. In summary, this research highlights the necessity for a
specialized tool to address Khmer homophone errors. By bridging current gaps in
research and available resources, such a tool would enhance the confidence and
accuracy of Khmer authors in their writing, thereby contributing to the
enrichment and preservation of the language. Continued efforts in this domain
are essential for ensuring that Khmer can leverage advancements in technology
and linguistics effectively.",2024-11-11,"Seanghort Born, Madeth May, Claudine Piau-Toffolon, Sébastien Iksal",http://arxiv.org/pdf/2411.10477v1,cs.CL
Persuasion with Large Language Models: a Survey,"The rapid rise of Large Language Models (LLMs) has created new disruptive
possibilities for persuasive communication, by enabling fully-automated
personalized and interactive content generation at an unprecedented scale. In
this paper, we survey the research field of LLM-based persuasion that has
emerged as a result. We begin by exploring the different modes in which LLM
Systems are used to influence human attitudes and behaviors. In areas such as
politics, marketing, public health, e-commerce, and charitable giving, such LLM
Systems have already achieved human-level or even super-human persuasiveness.
We identify key factors influencing their effectiveness, such as the manner of
personalization and whether the content is labelled as AI-generated. We also
summarize the experimental designs that have been used to evaluate progress.
Our survey suggests that the current and future potential of LLM-based
persuasion poses profound ethical and societal risks, including the spread of
misinformation, the magnification of biases, and the invasion of privacy. These
risks underscore the urgent need for ethical guidelines and updated regulatory
frameworks to avoid the widespread deployment of irresponsible and harmful LLM
Systems.",2024-11-11,"Alexander Rogiers, Sander Noels, Maarten Buyl, Tijl De Bie",http://arxiv.org/pdf/2411.06837v1,cs.CL
HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of Quantization on Model Alignment,"With the introduction of the transformers architecture, LLMs have
revolutionized the NLP field with ever more powerful models. Nevertheless,
their development came up with several challenges. The exponential growth in
computational power and reasoning capabilities of language models has
heightened concerns about their security. As models become more powerful,
ensuring their safety has become a crucial focus in research. This paper aims
to address gaps in the current literature on jailbreaking techniques and the
evaluation of LLM vulnerabilities. Our contributions include the creation of a
novel dataset designed to assess the harmfulness of model outputs across
multiple harm levels, as well as a focus on fine-grained harm-level analysis.
Using this framework, we provide a comprehensive benchmark of state-of-the-art
jailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model.
Additionally, we examine how quantization techniques, such as AWQ and GPTQ,
influence the alignment and robustness of models, revealing trade-offs between
enhanced robustness with regards to transfer attacks and potential increases in
vulnerability on direct ones. This study aims to demonstrate the influence of
harmful input queries on the complexity of jailbreaking techniques, as well as
to deepen our understanding of LLM vulnerabilities and improve methods for
assessing model robustness when confronted with harmful content, particularly
in the context of compression strategies.",2024-11-11,"Yannis Belkhiter, Giulio Zizzo, Sergio Maffeis",http://arxiv.org/pdf/2411.06835v1,cs.CL
AssistRAG: Boosting the Potential of Large Language Models with an Intelligent Information Assistant,"The emergence of Large Language Models (LLMs) has significantly advanced
natural language processing, but these models often generate factually
incorrect information, known as ""hallucination"". Initial retrieval-augmented
generation (RAG) methods like the ""Retrieve-Read"" framework was inadequate for
complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised
Fine-Tuning (SFT) methods improved performance but required frequent retraining
and risked altering foundational LLM capabilities. To cope with these
challenges, we propose Assistant-based Retrieval-Augmented Generation
(AssistRAG), integrating an intelligent information assistant within LLMs. This
assistant manages memory and knowledge through tool usage, action execution,
memory building, and plan specification. Using a two-phase training approach,
Curriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG
enhances information retrieval and decision-making. Experiments show AssistRAG
significantly outperforms benchmarks, especially benefiting less advanced LLMs,
by providing superior reasoning capabilities and accurate responses.",2024-11-11,"Yujia Zhou, Zheng Liu, Zhicheng Dou",http://arxiv.org/pdf/2411.06805v1,cs.CL
LA4SR: illuminating the dark proteome with generative AI,"AI language models (LMs) show promise for biological sequence analysis. We
re-engineered open-source LMs (GPT-2, BLOOM, DistilRoBERTa, ELECTRA, and Mamba,
ranging from 70M to 12B parameters) for microbial sequence classification. The
models achieved F1 scores up to 95 and operated 16,580x faster and at 2.9x the
recall of BLASTP. They effectively classified the algal dark proteome -
uncharacterized proteins comprising about 65% of total proteins - validated on
new data including a new, complete Hi-C/Pacbio Chlamydomonas genome. Larger
(>1B) LA4SR models reached high accuracy (F1 > 86) when trained on less than 2%
of available data, rapidly achieving strong generalization capacity. High
accuracy was achieved when training data had intact or scrambled terminal
information, demonstrating robust generalization to incomplete sequences.
Finally, we provide custom AI explainability software tools for attributing
amino acid patterns to AI generative processes and interpret their outputs in
evolutionary and biophysical contexts.",2024-11-11,"David R. Nelson, Ashish Kumar Jaiswal, Noha Ismail, Alexandra Mystikou, Kourosh Salehi-Ashtiani",http://arxiv.org/pdf/2411.06798v2,cs.CL
Large-scale moral machine experiment on large language models,"The rapid advancement of Large Language Models (LLMs) and their potential
integration into autonomous driving systems necessitates understanding their
moral decision-making capabilities. While our previous study examined four
prominent LLMs using the Moral Machine experimental framework, the dynamic
landscape of LLM development demands a more comprehensive analysis. Here, we
evaluate moral judgments across 52 different LLMs, including multiple versions
of proprietary models (GPT, Claude, Gemini) and open-source alternatives
(Llama, Gemma), to assess their alignment with human moral preferences in
autonomous driving scenarios. Using a conjoint analysis framework, we evaluated
how closely LLM responses aligned with human preferences in ethical dilemmas
and examined the effects of model size, updates, and architecture. Results
showed that proprietary models and open-source models exceeding 10 billion
parameters demonstrated relatively close alignment with human judgments, with a
significant negative correlation between model size and distance from human
judgments in open-source models. However, model updates did not consistently
improve alignment with human preferences, and many LLMs showed excessive
emphasis on specific ethical principles. These findings suggest that while
increasing model size may naturally lead to more human-like moral judgments,
practical implementation in autonomous driving systems requires careful
consideration of the trade-off between judgment quality and computational
efficiency. Our comprehensive analysis provides crucial insights for the
ethical design of autonomous systems and highlights the importance of
considering cultural contexts in AI moral decision-making.",2024-11-11,"Muhammad Shahrul Zaim bin Ahmad, Kazuhiro Takemoto",http://arxiv.org/pdf/2411.06790v2,cs.CL
PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing,"Code Large Language Models (Code LLMs), such as Code llama and
DeepSeek-Coder, have demonstrated exceptional performance in the code
generation tasks. However, most existing models focus on the abilities of
generating correct code, but often struggle with bug repair. We introduce a
suit of methods to enhance LLM's SQL bug-fixing abilities. The methods are
mainly consisted of two parts: A Progressive Dataset Construction (PDC) from
scratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data
expansion methods from the perspectives of breadth first and depth first
respectively. DM-SFT introduces an efficient bug-fixing supervised learning
approach, which effectively reduce the total training steps and mitigate the
""disorientation"" in SQL code bug-fixing training. In our evaluation, the code
LLM models trained with two methods have exceeds all current best performing
model which size is much larger.",2024-11-11,"Yiwen Duan, Yonghong Yu, Xiaoming Zhao, Yichang Wu, Wenbo Liu",http://arxiv.org/pdf/2411.06767v1,cs.CL
Reverse Prompt Engineering,"We explore a new language model inversion problem under strict black-box,
zero-shot, and limited data conditions. We propose a novel training-free
framework that reconstructs prompts using only a limited number of text outputs
from a language model. Existing methods rely on the availability of a large
number of outputs for both training and inference, an assumption that is
unrealistic in the real world, and they can sometimes produce garbled text. In
contrast, our approach, which relies on limited resources, consistently yields
coherent and semantically meaningful prompts. Our framework leverages a large
language model together with an optimization process inspired by the genetic
algorithm to effectively recover prompts. Experimental results on several
datasets derived from public sources indicate that our approach achieves
high-quality prompt recovery and generates prompts more semantically and
functionally aligned with the originals than current state-of-the-art methods.
Additionally, use-case studies introduced demonstrate the method's strong
potential for generating high-quality text data on perturbed prompts.",2024-11-11,"Hanqing Li, Diego Klabjan",http://arxiv.org/pdf/2411.06729v3,cs.CL
Model Fusion through Bayesian Optimization in Language Model Fine-Tuning,"Fine-tuning pre-trained models for downstream tasks is a widely adopted
technique known for its adaptability and reliability across various domains.
Despite its conceptual simplicity, fine-tuning entails several troublesome
engineering choices, such as selecting hyperparameters and determining
checkpoints from an optimization trajectory. To tackle the difficulty of
choosing the best model, one effective solution is model fusion, which combines
multiple models in a parameter space. However, we observe a large discrepancy
between loss and metric landscapes during the fine-tuning of pre-trained
language models. Building on this observation, we introduce a novel model
fusion technique that optimizes both the desired metric and loss through
multi-objective Bayesian optimization. In addition, to effectively select
hyperparameters, we establish a two-stage procedure by integrating Bayesian
optimization processes into our framework. Experiments across various
downstream tasks show considerable performance improvements using our Bayesian
optimization-guided method.",2024-11-11,"Chaeyun Jang, Hyungi Lee, Jungtaek Kim, Juho Lee",http://arxiv.org/pdf/2411.06710v2,cs.CL
What Should Baby Models Read? Exploring Sample-Efficient Data Composition on Model Performance,"We explore the impact of pre-training data composition on the performance of
small language models in a sample-efficient setting. Using datasets limited to
10 million words, we evaluate several dataset sources, including child-directed
speech (CHILDES), classic books (Gutenberg), synthetic data (TinyStories), and
a mix of these (Mix) across different model sizes ranging from 18 million to
705 million parameters. Our experiments show that smaller models (e.g.,
GPT2-97M, GPT2-705M, Llama-360M) perform better when trained on more complex
and rich datasets like Gutenberg. Models trained on the CHILDES and TinyStories
datasets underperformed across all model sizes. These findings suggest that the
optimal dataset for sample efficient training depends on the model size, and
that neither child-directed speech nor simplified stories are optimal for
language models of all sizes. We highlight the importance of considering both
dataset composition and model capacity for effective sample efficient language
model training.",2024-11-11,"Hong Meng Yam, Nathan J Paek",http://arxiv.org/pdf/2411.06672v1,cs.CL
Bridge: A Unified Framework to Knowledge Graph Completion via Language Models and Knowledge Representation,"Knowledge graph completion (KGC) is a task of inferring missing triples based
on existing Knowledge Graphs (KGs). Both structural and semantic information
are vital for successful KGC. However, existing methods only use either the
structural knowledge from the KG embeddings or the semantic information from
pre-trained language models (PLMs), leading to suboptimal model performance.
Moreover, since PLMs are not trained on KGs, directly using PLMs to encode
triples may be inappropriate. To overcome these limitations, we propose a novel
framework called Bridge, which jointly encodes structural and semantic
information of KGs. Specifically, we strategically encode entities and
relations separately by PLMs to better utilize the semantic knowledge of PLMs
and enable structured representation learning via a structural learning
principle. Furthermore, to bridge the gap between KGs and PLMs, we employ a
self-supervised representation learning method called BYOL to fine-tune PLMs
with two different views of a triple. Unlike BYOL, which uses augmentation
methods to create two semantically similar views of the same image, potentially
altering the semantic information. We strategically separate the triple into
two parts to create different views, thus avoiding semantic alteration.
Experiments demonstrate that Bridge outperforms the SOTA models on three
benchmark datasets.",2024-11-11,"Qiao Qiao, Yuepei Li, Qing Wang, Kang Zhou, Qi Li",http://arxiv.org/pdf/2411.06660v2,cs.CL
Renaissance: Investigating the Pretraining of Vision-Language Encoders,"In the past several years there has been an explosion of available models for
vision-language tasks. Unfortunately, the literature still leaves open a number
of questions related to best practices in designing and training such models.
In this paper we seek to answer several questions related to the pretraining of
vision-language encoders through meta-analysis. In our first set of
experiments, we show that we can save significant compute at no cost to
downstream performance, by freezing large parts of vision-language models
during pretraining. In our second set of experiments we examine the effect of
basing a VL transformer on a vision model versus a text model. Additionally, we
introduce a VL modeling platform called Renaissance that we use to conduct all
of the experiments. This program offers a great deal of flexibility in
creating, training and evaluating transformer encoders for VL modeling. The
source code for Renaissance can be found at
https://github.com/bsu-slim/renaissance.",2024-11-11,"Clayton Fields, Casey Kennington",http://arxiv.org/pdf/2411.06657v1,cs.CL
Explore the Reasoning Capability of LLMs in the Chess Testbed,"Reasoning is a central capability of human intelligence. In recent years,
with the advent of large-scale datasets, pretrained large language models have
emerged with new capabilities, including reasoning. However, these models still
struggle with long-term, complex reasoning tasks, such as playing chess. Based
on the observation that expert chess players employ a dual approach combining
long-term strategic play with short-term tactical play along with language
explanation, we propose improving the reasoning capability of large language
models in chess by integrating annotated strategy and tactic. Specifically, we
collect a dataset named MATE, which consists of 1 million chess positions with
candidate moves annotated by chess experts for strategy and tactics. We
finetune the LLaMA-3-8B model and compare it against state-of-the-art
commercial language models in the task of selecting better chess moves. Our
experiments show that our models perform better than GPT, Claude, and Gemini
models. We find that language explanations can enhance the reasoning capability
of large language models.",2024-11-11,"Shu Wang, Lei Ji, Renxi Wang, Wenxiao Zhao, Haokun Liu, Yifan Hou, Ying Nian Wu",http://arxiv.org/pdf/2411.06655v2,cs.CL
Understanding Scaling Laws with Statistical and Approximation Theory for Transformer Neural Networks on Intrinsically Low-dimensional Data,"When training deep neural networks, a model's generalization error is often
observed to follow a power scaling law dependent both on the model size and the
data size. Perhaps the best known example of such scaling laws are for
transformer-based large language models, where networks with billions of
parameters are trained on trillions of tokens of text. Yet, despite sustained
widespread interest, a rigorous understanding of why transformer scaling laws
exist is still missing. To answer this question, we establish novel statistical
estimation and mathematical approximation theories for transformers when the
input data are concentrated on a low-dimensional manifold. Our theory predicts
a power law between the generalization error and both the training data size
and the network size for transformers, where the power depends on the intrinsic
dimension $d$ of the training data. Notably, the constructed model architecture
is shallow, requiring only logarithmic depth in $d$. By leveraging
low-dimensional data structures under a manifold hypothesis, we are able to
explain transformer scaling laws in a way which respects the data geometry.
Moreover, we test our theory with empirical observation by training LLMs on
natural language datasets. We find the observed empirical data scaling laws
closely agree with our theoretical predictions. Taken together, these results
rigorously show the intrinsic dimension of data to be a crucial quantity
affecting transformer scaling laws in both theory and practice.",2024-11-11,"Alex Havrilla, Wenjing Liao",http://arxiv.org/pdf/2411.06646v1,cs.CL
Model Editing for LLMs4Code: How Far are We?,"Large Language Models for Code (LLMs4Code) have been found to exhibit
outstanding performance in the software engineering domain, especially the
remarkable performance in coding tasks. However, even the most advanced
LLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to
the high cost of training LLMs4Code, it is impractical to re-train the models
for fixing these problematic code knowledge. Model editing is a new technical
field for effectively and efficiently correcting erroneous knowledge in LLMs,
where various model editing techniques and benchmarks have been proposed
recently. Despite that, a comprehensive study that thoroughly compares and
analyzes the performance of the state-of-the-art model editing techniques for
adapting the knowledge within LLMs4Code across various code-related tasks is
notably absent. To bridge this gap, we perform the first systematic study on
applying state-of-the-art model editing approaches to repair the inaccuracy of
LLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists
of two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and
CodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help
of CLMEEval, we evaluate six advanced model editing techniques on three
LLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings
include that the external memorization-based GRACE approach achieves the best
knowledge editing effectiveness and specificity (the editing does not influence
untargeted knowledge), while generalization (whether the editing can generalize
to other semantically-identical inputs) is a universal challenge for existing
techniques. Furthermore, building on in-depth case analysis, we introduce an
enhanced version of GRACE called A-GRACE, which incorporates contrastive
learning to better capture the semantics of the inputs.",2024-11-11,"Xiaopeng Li, Shangwen Wang, Shasha Li, Jun Ma, Jie Yu, Xiaodong Liu, Jing Wang, Bin Ji, Weimin Zhang",http://arxiv.org/pdf/2411.06638v2,cs.CL
CriticAL: Critic Automation with Language Models,"Understanding the world through models is a fundamental goal of scientific
research. While large language model (LLM) based approaches show promise in
automating scientific discovery, they often overlook the importance of
criticizing scientific models. Criticizing models deepens scientific
understanding and drives the development of more accurate models. Automating
model criticism is difficult because it traditionally requires a human expert
to define how to compare a model with data and evaluate if the discrepancies
are significant--both rely heavily on understanding the modeling assumptions
and domain. Although LLM-based critic approaches are appealing, they introduce
new challenges: LLMs might hallucinate the critiques themselves. Motivated by
this, we introduce CriticAL (Critic Automation with Language Models). CriticAL
uses LLMs to generate summary statistics that capture discrepancies between
model predictions and data, and applies hypothesis tests to evaluate their
significance. We can view CriticAL as a verifier that validates models and
their critiques by embedding them in a hypothesis testing framework. In
experiments, we evaluate CriticAL across key quantitative and qualitative
dimensions. In settings where we synthesize discrepancies between models and
datasets, CriticAL reliably generates correct critiques without hallucinating
incorrect ones. We show that both human and LLM judges consistently prefer
CriticAL's critiques over alternative approaches in terms of transparency and
actionability. Finally, we show that CriticAL's critiques enable an LLM
scientist to improve upon human-designed models on real-world datasets.",2024-11-10,"Michael Y. Li, Vivek Vajipey, Noah D. Goodman, Emily B. Fox",http://arxiv.org/pdf/2411.06590v1,cs.CL
The KIPARLA Forest treebank of spoken Italian: an overview of initial design choices,"The paper presents an overview of initial design choices discussed towards
the creation of a treebank for the Italian KIParla corpus",2024-11-10,"Ludovica Pannitto, Caterina Mauri",http://arxiv.org/pdf/2411.06554v1,cs.CL
In-Context Learning for Preserving Patient Privacy: A Framework for Synthesizing Realistic Patient Portal Messages,"Since the COVID-19 pandemic, clinicians have seen a large and sustained
influx in patient portal messages, significantly contributing to clinician
burnout. To the best of our knowledge, there are no large-scale public patient
portal messages corpora researchers can use to build tools to optimize
clinician portal workflows. Informed by our ongoing work with a regional
hospital, this study introduces an LLM-powered framework for configurable and
realistic patient portal message generation. Our approach leverages few-shot
grounded text generation, requiring only a small number of de-identified
patient portal messages to help LLMs better match the true style and tone of
real data. Clinical experts in our team deem this framework as HIPAA-friendly,
unlike existing privacy-preserving approaches to synthetic text generation
which cannot guarantee all sensitive attributes will be protected. Through
extensive quantitative and human evaluation, we show that our framework
produces data of higher quality than comparable generation methods as well as
all related datasets. We believe this work provides a path forward for (i) the
release of large-scale synthetic patient message datasets that are
stylistically similar to ground-truth samples and (ii) HIPAA-friendly data
generation which requires minimal human de-identification efforts.",2024-11-10,"Joseph Gatto, Parker Seegmiller, Timothy E. Burdick, Sarah Masud Preum",http://arxiv.org/pdf/2411.06549v1,cs.CL
CineXDrama: Relevance Detection and Sentiment Analysis of Bangla YouTube Comments on Movie-Drama using Transformers: Insights from Interpretability Tool,"In recent years, YouTube has become the leading platform for Bangla movies
and dramas, where viewers express their opinions in comments that convey their
sentiments about the content. However, not all comments are relevant for
sentiment analysis, necessitating a filtering mechanism. We propose a system
that first assesses the relevance of comments and then analyzes the sentiment
of those deemed relevant. We introduce a dataset of 14,000 manually collected
and preprocessed comments, annotated for relevance (relevant or irrelevant) and
sentiment (positive or negative). Eight transformer models, including
BanglaBERT, were used for classification tasks, with BanglaBERT achieving the
highest accuracy (83.99% for relevance detection and 93.3% for sentiment
analysis). The study also integrates LIME to interpret model decisions,
enhancing transparency.",2024-11-10,"Usafa Akther Rifa, Pronay Debnath, Busra Kamal Rafa, Shamaun Safa Hridi, Md. Aminur Rahman",http://arxiv.org/pdf/2411.06548v2,cs.CL
Probabilistic Consensus through Ensemble Validation: A Framework for LLM Reliability,"Large Language Models (LLMs) have shown significant advances in text
generation but often lack the reliability needed for autonomous deployment in
high-stakes domains like healthcare, law, and finance. Existing approaches rely
on external knowledge or human oversight, limiting scalability. We introduce a
novel framework that repurposes ensemble methods for content validation through
model consensus. In tests across 78 complex cases requiring factual accuracy
and causal consistency, our framework improved precision from 73.1% to 93.9%
with two models (95% CI: 83.5%-97.9%) and to 95.6% with three models (95% CI:
85.2%-98.8%). Statistical analysis indicates strong inter-model agreement
($\kappa$ > 0.76) while preserving sufficient independence to catch errors
through disagreement. We outline a clear pathway to further enhance precision
with additional validators and refinements. Although the current approach is
constrained by multiple-choice format requirements and processing latency, it
offers immediate value for enabling reliable autonomous AI systems in critical
applications.",2024-11-10,Ninad Naik,http://arxiv.org/pdf/2411.06535v1,cs.CL
Epistemic Integrity in Large Language Models,"Large language models are increasingly relied upon as sources of information,
but their propensity for generating false or misleading statements with high
confidence poses risks for users and society. In this paper, we confront the
critical problem of epistemic miscalibration $\unicode{x2013}$ where a model's
linguistic assertiveness fails to reflect its true internal certainty. We
introduce a new human-labeled dataset and a novel method for measuring the
linguistic assertiveness of Large Language Models (LLMs) which cuts error rates
by over 50% relative to previous benchmarks. Validated across multiple
datasets, our method reveals a stark misalignment between how confidently
models linguistically present information and their actual accuracy. Further
human evaluations confirm the severity of this miscalibration. This evidence
underscores the urgent risk of the overstated certainty LLMs hold which may
mislead users on a massive scale. Our framework provides a crucial step forward
in diagnosing this miscalibration, offering a path towards correcting it and
more trustworthy AI across domains.",2024-11-10,"Bijean Ghafouri, Shahrad Mohammadzadeh, James Zhou, Pratheeksha Nair, Jacob-Junqi Tian, Mayank Goel, Reihaneh Rabbany, Jean-François Godbout, Kellin Pelrine",http://arxiv.org/pdf/2411.06528v1,cs.CL
CULL-MT: Compression Using Language and Layer pruning for Machine Translation,"Multilingual machine translation models often outperform traditional
bilingual models by leveraging translation knowledge transfer. Recent
advancements have led to these models supporting hundreds of languages and
achieving state-of-the-art results across various translation directions.
However, as these models grow larger, their inference operations become
increasingly costly. In many use cases, there is no need to support such a wide
range of language pairs, as translation is typically needed in only a few
selected directions. In this paper, we present CULL-MT, a compression method
for machine translation models based on structural layer pruning and selected
language directions. Our approach identifies and prunes unimportant layers
using a greedy strategy, then mitigates the impact by applying knowledge
distillation from the original model along with parameter-efficient
fine-tuning. We apply CULL-MT to the NLLB-3.3B and LLaMA3.1-8B-Instruct models.
In a multi-way translation scenario (Persian, French, and German to English),
we find the NLLB-3.3B model to be robust, allowing 25% of layers to be pruned
with only a 0.9 spBLEU drop. However, LLaMA3.1-8B-Instruct is more sensitive,
with a 2.0 spBLEU drop after pruning 5 layers.",2024-11-10,"Pedram Rostami, Mohammad Javad Dousti",http://arxiv.org/pdf/2411.06506v1,cs.CL
VocalTweets: Investigating Social Media Offensive Language Among Nigerian Musicians,"Musicians frequently use social media to express their opinions, but they
often convey different messages in their music compared to their posts online.
Some utilize these platforms to abuse their colleagues, while others use it to
show support for political candidates or engage in activism, as seen during the
#EndSars protest. There are extensive research done on offensive language
detection on social media, the usage of offensive language by musicians has
received limited attention. In this study, we introduce VocalTweets, a
code-switched and multilingual dataset comprising tweets from 12 prominent
Nigerian musicians, labeled with a binary classification method as Normal or
Offensive. We trained a model using HuggingFace's base-Twitter-RoBERTa,
achieving an F1 score of 74.5. Additionally, we conducted cross-corpus
experiments with the OLID dataset to evaluate the generalizability of our
dataset.",2024-11-10,"Sunday Oluyele, Juwon Akingbade, Victor Akinode",http://arxiv.org/pdf/2411.06477v1,cs.CL
ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?,"Large Language Models (LLMs) hold great promise to revolutionize current
clinical systems for their superior capacities on medical text processing tasks
and medical licensing exams. Meanwhile, traditional ML models such as SVM and
XGBoost have still been mainly adopted in clinical prediction tasks. An
emerging question is Can LLMs beat traditional ML models in clinical
prediction? Thus, we build a new benchmark ClinicalBench to comprehensively
study the clinical predictive modeling capacities of both general-purpose and
medical LLMs, and compare them with traditional ML models. ClinicalBench
embraces three common clinical prediction tasks, two databases, 14
general-purpose LLMs, 8 medical LLMs, and 11 traditional ML models. Through
extensive empirical investigation, we discover that both general-purpose and
medical LLMs, even with different model scales, diverse prompting or
fine-tuning strategies, still cannot beat traditional ML models in clinical
prediction yet, shedding light on their potential deficiency in clinical
reasoning and decision-making. We call for caution when practitioners adopt
LLMs in clinical applications. ClinicalBench can be utilized to bridge the gap
between LLMs' development for healthcare and real-world clinical practice.",2024-11-10,"Canyu Chen, Jian Yu, Shan Chen, Che Liu, Zhongwei Wan, Danielle Bitterman, Fei Wang, Kai Shu",http://arxiv.org/pdf/2411.06469v1,cs.CL
Prompt-Efficient Fine-Tuning for GPT-like Deep Models to Reduce Hallucination and to Improve Reproducibility in Scientific Text Generation Using Stochastic Optimisation Techniques,"Large Language Models (LLMs) are increasingly adopted for complex scientific
text generation tasks, yet they often suffer from limitations in accuracy,
consistency, and hallucination control. This thesis introduces a
Parameter-Efficient Fine-Tuning (PEFT) approach tailored for GPT-like models,
aiming to mitigate hallucinations and enhance reproducibility, particularly in
the computational domain of mass spectrometry. We implemented Low-Rank
Adaptation (LoRA) adapters to refine GPT-2, termed MS-GPT, using a specialized
corpus of mass spectrometry literature. Through novel evaluation methods
applied to LLMs, including BLEU, ROUGE, and Perplexity scores, the fine-tuned
MS-GPT model demonstrated superior text coherence and reproducibility compared
to the baseline GPT-2, confirmed through statistical analysis with the Wilcoxon
rank-sum test. Further, we propose a reproducibility metric based on cosine
similarity of model outputs under controlled prompts, showcasing MS-GPT's
enhanced stability. This research highlights PEFT's potential to optimize LLMs
for scientific contexts, reducing computational costs while improving model
reliability.",2024-11-10,Daniil Sulimov,http://arxiv.org/pdf/2411.06445v1,cs.CL
Conditional [MASK] Discrete Diffusion Language Model,"Although auto-regressive models excel in natural language processing, they
often struggle to generate diverse text and provide limited controllability.
Non-auto-regressive methods could be an alternative but often produce
degenerate outputs and exhibit shortcomings in conditional generation. To
address these challenges, we propose Diffusion-EAGS, a novel framework that
integrates conditional masked language models into diffusion language models
through the theoretical lens of a conditional Markov Random Field. In doing so,
we propose entropy-adaptive Gibbs sampling and entropy-based noise scheduling
to counterbalance each model's shortcomings. Experimental results show that
Diffusion-EAGS outperforms baselines and achieves the best quality-diversity
tradeoff, demonstrating its effectiveness in non-autoregressive text
generation.",2024-11-10,"Hyukhun Koh, Minha Jhang, Dohyung Kim, Sangmook Lee, Kyomin Jung",http://arxiv.org/pdf/2411.06438v5,cs.CL
CTC-Assisted LLM-Based Contextual ASR,"Contextual ASR or hotword customization holds substantial practical value.
Despite the impressive performance of current end-to-end (E2E) automatic speech
recognition (ASR) systems, they often face challenges in accurately recognizing
rare words. Typical E2E contextual ASR models commonly feature complex
architectures and decoding mechanisms, limited in performance and susceptible
to interference from distractor words. With large language model (LLM)-based
ASR models emerging as the new mainstream, we propose a CTC-Assisted LLM-Based
Contextual ASR model with an efficient filtering algorithm. By using coarse CTC
decoding results to filter potential relevant hotwords and incorporating them
into LLM prompt input, our model attains WER/B-WER of 1.27%/3.67% and
2.72%/8.02% on the Librispeech test-clean and test-other sets targeting on
recognizing rare long-tail words, demonstrating significant improvements
compared to the baseline LLM-based ASR model, and substantially surpassing
other related work. More remarkably, with the help of the large language model
and proposed filtering algorithm, our contextual ASR model still performs well
with 2000 biasing words.",2024-11-10,"Guanrou Yang, Ziyang Ma, Zhifu Gao, Shiliang Zhang, Xie Chen",http://arxiv.org/pdf/2411.06437v1,cs.CL
SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains,"As the integration of the Large Language Models (LLMs) into various
applications increases, so does their susceptibility to misuse, raising
significant security concerns. Numerous jailbreak attacks have been proposed to
assess the security defense of LLMs. Current jailbreak attacks mainly rely on
scenario camouflage, prompt obfuscation, prompt optimization, and prompt
iterative optimization to conceal malicious prompts. In particular, sequential
prompt chains in a single query can lead LLMs to focus on certain prompts while
ignoring others, facilitating context manipulation. This paper introduces
SequentialBreak, a novel jailbreak attack that exploits this vulnerability. We
discuss several scenarios, not limited to examples like Question Bank, Dialog
Completion, and Game Environment, where the harmful prompt is embedded within
benign ones that can fool LLMs into generating harmful responses. The distinct
narrative structures of these scenarios show that SequentialBreak is flexible
enough to adapt to various prompt formats beyond those discussed. Extensive
experiments demonstrate that SequentialBreak uses only a single query to
achieve a substantial gain of attack success rate over existing baselines
against both open-source and closed-source models. Through our research, we
highlight the urgent need for more robust and resilient safeguards to enhance
LLM security and prevent potential misuse. All the result files and website
associated with this research are available in this GitHub repository:
https://anonymous.4open.science/r/JailBreakAttack-4F3B/.",2024-11-10,"Bijoy Ahmed Saiem, MD Sadik Hossain Shanto, Rakib Ahsan, Md Rafi ur Rashid",http://arxiv.org/pdf/2411.06426v2,cs.CL
Beyond Toxic Neurons: A Mechanistic Analysis of DPO for Toxicity Reduction,"Safety fine-tuning algorithms are widely used to reduce harmful outputs in
language models, but how they achieve this remain unclear. Studying the Direct
Preference Optimization (DPO) algorithm for toxicity reduction, current
explanations claim that DPO achieves this by dampening the activations of toxic
MLP neurons. However, through activation patching, we show that this
explanation is incomplete. Projections onto a toxicity probe's direction show
that only 4.9% of toxicity reduction comes from dampened toxic neurons.
Instead, DPO reduces toxicity through distributed activation shifts across a
majority of neurons, progressively shifting MLP layer outputs away from
toxicity. These shifts accumulate across four neuron groups: two reducing
toxicity and two promoting anti-toxicity. Activation patching validates the
cumulative roles of these groups, where patching all identified groups
effectively replicates DPO's effects. These findings illustrate DPO's
mechanism: it reduces toxicity by accumulating small activation shifts across
many neurons throughout the layers. Our findings provide new mechanistic
insights into how safety fine-tuning reduces harmful outputs in language
models.",2024-11-10,"Yushi Yang, Filip Sondej, Harry Mayne, Adam Mahdi",http://arxiv.org/pdf/2411.06424v2,cs.CL
Fineweb-Edu-Ar: Machine-translated Corpus to Support Arabic Small Language Models,"As large language models (LLMs) grow and develop, so do their data demands.
This is especially true for multilingual LLMs, where the scarcity of
high-quality and readily available data online has led to a multitude of
synthetic dataset generation approaches. A key technique in this space is
machine translation (MT), where high-quality English text is adapted to a
target, comparatively low-resource language. This report introduces
FineWeb-Edu-Ar, a machine-translated version of the exceedingly popular
(deduplicated) FineWeb-Edu dataset from HuggingFace. To the best of our
knowledge, FineWeb-Edu-Ar is the largest publicly available machine-translated
Arabic dataset out there, with its size of 202B tokens of an Arabic-trained
tokenizer.",2024-11-10,"Sultan Alrashed, Dmitrii Khizbullin, David R. Pugh",http://arxiv.org/pdf/2411.06402v1,cs.CL
CausalStock: Deep End-to-end Causal Discovery for News-driven Stock Movement Prediction,"There are two issues in news-driven multi-stock movement prediction tasks
that are not well solved in the existing works. On the one hand, ""relation
discovery"" is a pivotal part when leveraging the price information of other
stocks to achieve accurate stock movement prediction. Given that stock
relations are often unidirectional, such as the ""supplier-consumer""
relationship, causal relations are more appropriate to capture the impact
between stocks. On the other hand, there is substantial noise existing in the
news data leading to extracting effective information with difficulty. With
these two issues in mind, we propose a novel framework called CausalStock for
news-driven multi-stock movement prediction, which discovers the temporal
causal relations between stocks. We design a lag-dependent temporal causal
discovery mechanism to model the temporal causal graph distribution. Then a
Functional Causal Model is employed to encapsulate the discovered causal
relations and predict the stock movements. Additionally, we propose a Denoised
News Encoder by taking advantage of the excellent text evaluation ability of
large language models (LLMs) to extract useful information from massive news
data. The experiment results show that CausalStock outperforms the strong
baselines for both news-driven multi-stock movement prediction and multi-stock
movement prediction tasks on six real-world datasets collected from the US,
China, Japan, and UK markets. Moreover, getting benefit from the causal
relations, CausalStock could offer a clear prediction mechanism with good
explainability.",2024-11-10,"Shuqi Li, Yuebo Sun, Yuxin Lin, Xin Gao, Shuo Shang, Rui Yan",http://arxiv.org/pdf/2411.06391v1,cs.CL
Self-Training Meets Consistency: Improving LLMs' Reasoning with Consistency-Driven Rationale Evaluation,"Self-training approach for large language models (LLMs) improves reasoning
abilities by training the models on their self-generated rationales. Previous
approaches have labeled rationales that produce correct answers for a given
question as appropriate for training. However, a single measure risks
misjudging rationale quality, leading the models to learn flawed reasoning
patterns. To address this issue, we propose CREST (Consistency-driven Rationale
Evaluation for Self-Training), a self-training framework that further evaluates
each rationale through follow-up questions and leverages this evaluation to
guide its training. Specifically, we introduce two methods: (1) filtering out
rationales that frequently result in incorrect answers on follow-up questions
and (2) preference learning based on mixed preferences from rationale
evaluation results of both original and follow-up questions. Experiments on
three question-answering datasets using open LLMs show that CREST not only
improves the logical robustness and correctness of rationales but also improves
reasoning abilities compared to previous self-training approaches.",2024-11-10,"Jaehyeok Lee, Keisuke Sakaguchi, JinYeong Bak",http://arxiv.org/pdf/2411.06387v4,cs.CL
LLM Vocabulary Compression for Low-Compute Environments,"We present a method to compress the final linear layer of language models,
reducing memory usage by up to 3.4x without significant performance loss. By
grouping tokens based on Byte Pair Encoding (BPE) merges, we prevent
materialization of the memory-intensive logits tensor. Evaluations on the
TinyStories dataset show that our method performs on par with GPT-Neo and GPT2
while significantly improving throughput by up to 3x, making it suitable for
low-compute environments.",2024-11-10,"Sreeram Vennam, Anish Joishy, Ponnurangam Kumaraguru",http://arxiv.org/pdf/2411.06371v1,cs.CL
Prompts Matter: Comparing ML/GAI Approaches for Generating Inductive Qualitative Coding Results,"Inductive qualitative methods have been a mainstay of education research for
decades, yet it takes much time and effort to conduct rigorously. Recent
advances in artificial intelligence, particularly with generative AI (GAI),
have led to initial success in generating inductive coding results. Like human
coders, GAI tools rely on instructions to work, and how to instruct it may
matter. To understand how ML/GAI approaches could contribute to qualitative
coding processes, this study applied two known and two theory-informed novel
approaches to an online community dataset and evaluated the resulting coding
results. Our findings show significant discrepancies between ML/GAI approaches
and demonstrate the advantage of our approaches, which introduce human coding
processes into GAI prompts.",2024-11-10,"John Chen, Alexandros Lotsos, Lexie Zhao, Grace Wang, Uri Wilensky, Bruce Sherin, Michael Horn",http://arxiv.org/pdf/2411.06316v1,cs.CL
Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models,"As large language models become increasingly prevalent in the financial
sector, there is a pressing need for a standardized method to comprehensively
assess their performance. However, existing finance benchmarks often suffer
from limited language and task coverage, as well as challenges such as
low-quality datasets and inadequate adaptability for LLM evaluation. To address
these limitations, we propose ""Golden Touchstone"", the first comprehensive
bilingual benchmark for financial LLMs, which incorporates representative
datasets from both Chinese and English across eight core financial NLP tasks.
Developed from extensive open source data collection and industry-specific
demands, this benchmark includes a variety of financial tasks aimed at
thoroughly assessing models' language understanding and generation
capabilities. Through comparative analysis of major models on the benchmark,
such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and
limitations in processing complex financial information. Additionally, we
open-sourced Touchstone-GPT, a financial LLM trained through continual
pre-training and financial instruction tuning, which demonstrates strong
performance on the bilingual benchmark but still has limitations in specific
tasks.This research not only provides the financial large language models with
a practical evaluation tool but also guides the development and optimization of
future research. The source code for Golden Touchstone and model weight of
Touchstone-GPT have been made publicly available at
\url{https://github.com/IDEA-FinAI/Golden-Touchstone}, contributing to the
ongoing evolution of FinLLMs and fostering further research in this critical
area.",2024-11-09,"Xiaojun Wu, Junxi Liu, Huanyi Su, Zhouchi Lin, Yiyan Qi, Chengjin Xu, Jiajun Su, Jiajie Zhong, Fuwei Wang, Saizhuo Wang, Fengrui Hua, Jia Li, Jian Guo",http://arxiv.org/pdf/2411.06272v1,cs.CL
Robust Detection of LLM-Generated Text: A Comparative Analysis,"The ability of large language models to generate complex texts allows them to
be widely integrated into many aspects of life, and their output can quickly
fill all network resources. As the impact of LLMs grows, it becomes
increasingly important to develop powerful detectors for the generated text.
This detector is essential to prevent the potential misuse of these
technologies and to protect areas such as social media from the negative
effects of false content generated by LLMS. The main goal of LLM-generated text
detection is to determine whether text is generated by an LLM, which is a basic
binary classification task. In our work, we mainly use three different
classification methods based on open source datasets: traditional machine
learning techniques such as logistic regression, k-means clustering, Gaussian
Naive Bayes, support vector machines, and methods based on converters such as
BERT, and finally algorithms that use LLMs to detect LLM-generated text. We
focus on model generalization, potential adversarial attacks, and accuracy of
model evaluation. Finally, the possible research direction in the future is
proposed, and the current experimental results are summarized.",2024-11-09,"Yongye Su, Yuqing Wu",http://arxiv.org/pdf/2411.06248v1,cs.CL
An $\mathbf{L^*}$ Algorithm for Deterministic Weighted Regular Languages,"Extracting finite state automata (FSAs) from black-box models offers a
powerful approach to gaining interpretable insights into complex model
behaviors. To support this pursuit, we present a weighted variant of Angluin's
(1987) $\mathbf{L^*}$ algorithm for learning FSAs. We stay faithful to the
original algorithm, devising a way to exactly learn deterministic weighted FSAs
whose weights support division. Furthermore, we formulate the learning process
in a manner that highlights the connection with FSA minimization, showing how
$\mathbf{L^*}$ directly learns a minimal automaton for the target language.",2024-11-09,"Clemente Pasti, Talu Karagöz, Anej Svete, Franz Nowak, Reda Boumasmoud, Ryan Cotterell",http://arxiv.org/pdf/2411.06228v2,cs.CL
Target-driven Attack for Large Language Models,"Current large language models (LLM) provide a strong foundation for
large-scale user-oriented natural language tasks. Many users can easily inject
adversarial text or instructions through the user interface, thus causing LLM
model security challenges like the language model not giving the correct
answer. Although there is currently a large amount of research on black-box
attacks, most of these black-box attacks use random and heuristic strategies.
It is unclear how these strategies relate to the success rate of attacks and
thus effectively improve model robustness. To solve this problem, we propose
our target-driven black-box attack method to maximize the KL divergence between
the conditional probabilities of the clean text and the attack text to redefine
the attack's goal. We transform the distance maximization problem into two
convex optimization problems based on the attack goal to solve the attack text
and estimate the covariance. Furthermore, the projected gradient descent
algorithm solves the vector corresponding to the attack text. Our target-driven
black-box attack approach includes two attack strategies: token manipulation
and misinformation attack. Experimental results on multiple Large Language
Models and datasets demonstrate the effectiveness of our attack method.",2024-11-09,"Chong Zhang, Mingyu Jin, Dong Shu, Taowen Wang, Dongfang Liu, Xiaobo Jin",http://arxiv.org/pdf/2411.07268v2,cs.CL
Incorporating Human Explanations for Robust Hate Speech Detection,"Given the black-box nature and complexity of large transformer language
models (LM), concerns about generalizability and robustness present ethical
implications for domains such as hate speech (HS) detection. Using the content
rich Social Bias Frames dataset, containing human-annotated stereotypes,
intent, and targeted groups, we develop a three stage analysis to evaluate if
LMs faithfully assess hate speech. First, we observe the need for modeling
contextually grounded stereotype intents to capture implicit semantic meaning.
Next, we design a new task, Stereotype Intent Entailment (SIE), which
encourages a model to contextually understand stereotype presence. Finally,
through ablation tests and user studies, we find a SIE objective improves
content understanding, but challenges remain in modeling implicit intent.",2024-11-09,"Jennifer L. Chen, Faisal Ladhak, Daniel Li, Noémie Elhadad",http://arxiv.org/pdf/2411.06213v1,cs.CL
IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization,"In the realm of large language models (LLMs), the ability of models to
accurately follow instructions is paramount as more agents and applications
leverage LLMs for construction, where the complexity of instructions are
rapidly increasing. However, on the one hand, there is only a certain amount of
complex instruction evaluation data; on the other hand, there are no dedicated
algorithms to improve the ability to follow complex instructions. To this end,
this paper introduces TRACE, a benchmark for improving and evaluating the
complex instructionfollowing ability, which consists of 120K training data and
1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference
Optimization) alignment method which takes both input and output preference
pairs into consideration, where LLMs not only rapidly align with response
preferences but also meticulously explore the instruction preferences.
Extensive experiments on both in-domain and outof-domain datasets confirm the
effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and
6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.",2024-11-09,"Xinghua Zhang, Haiyang Yu, Cheng Fu, Fei Huang, Yongbin Li",http://arxiv.org/pdf/2411.06208v2,cs.CL
Exploring Knowledge Boundaries in Large Language Models for Retrieval Judgment,"Large Language Models (LLMs) are increasingly recognized for their practical
applications. However, these models often encounter challenges in dynamically
changing knowledge, as well as in managing unknown static knowledge.
Retrieval-Augmented Generation (RAG) tackles this challenge and has shown a
significant impact on LLMs. Actually, we find that the impact of RAG on the
question answering capabilities of LLMs can be categorized into three groups:
beneficial, neutral, and harmful. By minimizing retrieval requests that yield
neutral or harmful results, we can effectively reduce both time and
computational costs, while also improving the overall performance of LLMs. This
insight motivates us to differentiate between types of questions using certain
metrics as indicators, to decrease the retrieval ratio without compromising
performance. In our work, we propose a method that is able to identify
different types of questions from this view by training a Knowledge Boundary
Model (KBM). Experiments conducted on 11 English and Chinese datasets
illustrate that the KBM effectively delineates the knowledge boundary,
significantly decreasing the proportion of retrievals required for optimal
end-to-end performance. Specifically, we evaluate the effectiveness of KBM in
three complex scenarios: dynamic knowledge, long-tail static knowledge, and
multi-hop problems, as well as its functionality as an external LLM plug-in.",2024-11-09,"Zhen Zhang, Xinyu Wang, Yong Jiang, Zhuo Chen, Feiteng Mu, Mengting Hu, Pengjun Xie, Fei Huang",http://arxiv.org/pdf/2411.06207v1,cs.CL
WMT24 Test Suite: Gender Resolution in Speaker-Listener Dialogue Roles,"We assess the difficulty of gender resolution in literary-style dialogue
settings and the influence of gender stereotypes. Instances of the test suite
contain spoken dialogue interleaved with external meta-context about the
characters and the manner of speaking. We find that character and manner
stereotypes outside of the dialogue significantly impact the gender agreement
of referents within the dialogue.",2024-11-09,"Hillary Dawkins, Isar Nejadgholi, Chi-kiu Lo",http://arxiv.org/pdf/2411.06194v1,cs.CL
M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework,"The ability to understand and answer questions over documents can be useful
in many business and practical applications. However, documents often contain
lengthy and diverse multimodal contents such as texts, figures, and tables,
which are very time-consuming for humans to read thoroughly. Hence, there is an
urgent need to develop effective and automated methods to aid humans in this
task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an
automated framework to evaluate the performance of large multimodal models. We
further propose a retrieval-aware tuning approach for efficient and effective
multimodal document reading. Compared to existing works, our benchmark consists
of more recent and lengthy documents with hundreds of pages, while also
requiring open-ended solutions and not just extractive answers. To our
knowledge, our training framework is the first to directly address the
retrieval setting for multimodal long documents. To enable tuning open-source
models, we construct a training corpus in a fully automatic manner for the
question-answering task over such documents. Experiments show that our tuning
approach achieves a relative improvement of 4.6% for the correctness of model
responses, compared to the baseline open-source models. Our data, code, and
models are available at https://multimodal-documents.github.io.",2024-11-09,"Yew Ken Chia, Liying Cheng, Hou Pong Chan, Chaoqun Liu, Maojia Song, Sharifah Mahani Aljunied, Soujanya Poria, Lidong Bing",http://arxiv.org/pdf/2411.06176v1,cs.CL
Clustering Algorithms and RAG Enhancing Semi-Supervised Text Classification with Large LLMs,"This paper proposes a Clustering, Labeling, then Augmenting framework that
significantly enhances performance in Semi-Supervised Text Classification
(SSTC) tasks, effectively addressing the challenge of vast datasets with
limited labeled examples. Unlike traditional SSTC approaches that rely on a
predefined small set of labeled data to generate pseudo-labels for the
unlabeled data, this framework innovatively employs clustering to select
representative ""landmarks"" for labeling. These landmarks subsequently act as
intermediaries in an ensemble of augmentation techniques, including
Retrieval-Augmented Generation (RAG), Large Language Model (LLMs)-based
rewriting, and synonym substitution, to generate synthetic labeled data without
making pseudo-labels for the unlabeled data. Empirical results show that even
in complex text document classification scenarios involving over 100
categories, our method achieves state-of-the-art accuracies of 95.41% on the
Reuters dataset and 82.43% on the Web of Science dataset. Our approach
significantly reduces the reliance on human labeling efforts and the associated
expenses, while simultaneously ensuring high data quality and minimizing
privacy risks. The finetuning results further show the efficiency of
fine-tuning LLMs for text classification tasks, highlighting a robust solution
for leveraging limited labeled data.",2024-11-09,"Shan Zhong, Jiahao Zeng, Yongxin Yu, Bohong Lin",http://arxiv.org/pdf/2411.06175v3,cs.CL
SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models,"Continual learning (CL) is crucial for language models to dynamically adapt
to the evolving real-world demands. To mitigate the catastrophic forgetting
problem in CL, data replay has been proven a simple and effective strategy, and
the subsequent data-replay-based distillation can further enhance the
performance. However, existing methods fail to fully exploit the knowledge
embedded in models from previous tasks, resulting in the need for a relatively
large number of replay samples to achieve good results. In this work, we first
explore and emphasize the importance of attention weights in knowledge
retention, and then propose a SElective attEntion-guided Knowledge Retention
method (SEEKR) for data-efficient replay-based continual learning of large
language models (LLMs). Specifically, SEEKR performs attention distillation on
the selected attention heads for finer-grained knowledge retention, where the
proposed forgettability-based and task-sensitivity-based measures are used to
identify the most valuable attention heads. Experimental results on two
continual learning benchmarks for LLMs demonstrate the superiority of SEEKR
over the existing methods on both performance and efficiency. Explicitly, SEEKR
achieves comparable or even better performance with only 1/10 of the replayed
data used by other methods, and reduces the proportion of replayed data to 1%.",2024-11-09,"Jinghan He, Haiyun Guo, Kuan Zhu, Zihan Zhao, Ming Tang, Jinqiao Wang",http://arxiv.org/pdf/2411.06171v1,cs.CL
Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework,"Text emotion detection constitutes a crucial foundation for advancing
artificial intelligence from basic comprehension to the exploration of
emotional reasoning. Most existing emotion detection datasets rely on manual
annotations, which are associated with high costs, substantial subjectivity,
and severe label imbalances. This is particularly evident in the inadequate
annotation of micro-emotions and the absence of emotional intensity
representation, which fail to capture the rich emotions embedded in sentences
and adversely affect the quality of downstream task completion. By proposing an
all-labels and training-set label regression method, we map label values to
energy intensity levels, thereby fully leveraging the learning capabilities of
machine models and the interdependencies among labels to uncover multiple
emotions within samples. This led to the establishment of the Emotion
Quantization Network (EQN) framework for micro-emotion detection and
annotation. Using five commonly employed sentiment datasets, we conducted
comparative experiments with various models, validating the broad applicability
of our framework within NLP machine learning models. Based on the EQN
framework, emotion detection and annotation are conducted on the GoEmotions
dataset. A comprehensive comparison with the results from Google literature
demonstrates that the EQN framework possesses a high capability for automatic
detection and annotation of micro-emotions. The EQN framework is the first to
achieve automatic micro-emotion annotation with energy-level scores, providing
strong support for further emotion detection analysis and the quantitative
research of emotion computing.",2024-11-09,"Jingyi Zhou, Senlin Luo, Haofan Chen",http://arxiv.org/pdf/2411.06160v2,cs.CL
Mixture of Knowledge Minigraph Agents for Literature Review Generation,"Literature reviews play a crucial role in scientific research for
understanding the current state of research, identifying gaps, and guiding
future studies on specific topics. However, the process of conducting a
comprehensive literature review is yet time-consuming. This paper proposes a
novel framework, collaborative knowledge minigraph agents (CKMAs), to automate
scholarly literature reviews. A novel prompt-based algorithm, the knowledge
minigraph construction agent (KMCA), is designed to identify relations between
concepts from academic literature and automatically constructs knowledge
minigraphs. By leveraging the capabilities of large language models on
constructed knowledge minigraphs, the multiple path summarization agent (MPSA)
efficiently organizes concepts and relations from different viewpoints to
generate literature review paragraphs. We evaluate CKMAs on three benchmark
datasets. Experimental results show the effectiveness of the proposed method,
further revealing promising applications of LLMs in scientific research.",2024-11-09,"Zhi Zhang, Yan Liu, Sheng-hua Zhong, Gong Chen, Yu Yang, Jiannong Cao",http://arxiv.org/pdf/2411.06159v3,cs.CL
Building an Efficient Multilingual Non-Profit IR System for the Islamic Domain Leveraging Multiprocessing Design in Rust,"The widespread use of large language models (LLMs) has dramatically improved
many applications of Natural Language Processing (NLP), including Information
Retrieval (IR). However, domains that are not driven by commercial interest
often lag behind in benefiting from AI-powered solutions. One such area is
religious and heritage corpora. Alongside similar domains, Islamic literature
holds significant cultural value and is regularly utilized by scholars and the
general public. Navigating this extensive amount of text is challenging, and
there is currently no unified resource that allows for easy searching of this
data using advanced AI tools. This work focuses on the development of a
multilingual non-profit IR system for the Islamic domain. This process brings a
few major challenges, such as preparing multilingual domain-specific corpora
when data is limited in certain languages, deploying a model on
resource-constrained devices, and enabling fast search on a limited budget. By
employing methods like continued pre-training for domain adaptation and
language reduction to decrease model size, a lightweight multilingual retrieval
model was prepared, demonstrating superior performance compared to larger
models pre-trained on general domain data. Furthermore, evaluating the proposed
architecture that utilizes Rust Language capabilities shows the possibility of
implementing efficient semantic search in a low-resource setting.",2024-11-09,"Vera Pavlova, Mohammed Makhlouf",http://arxiv.org/pdf/2411.06151v1,cs.CL
StopHC: A Harmful Content Detection and Mitigation Architecture for Social Media Platforms,"The mental health of social media users has started more and more to be put
at risk by harmful, hateful, and offensive content. In this paper, we propose
\textsc{StopHC}, a harmful content detection and mitigation architecture for
social media platforms. Our aim with \textsc{StopHC} is to create more secure
online environments. Our solution contains two modules, one that employs deep
neural network architecture for harmful content detection, and one that uses a
network immunization algorithm to block toxic nodes and stop the spread of
harmful content. The efficacy of our solution is demonstrated by experiments
conducted on two real-world datasets.",2024-11-09,"Ciprian-Octavian Truică, Ana-Teodora Constantinescu, Elena-Simona Apostol",http://arxiv.org/pdf/2411.06138v1,cs.CL
Detecting Reference Errors in Scientific Literature with Large Language Models,"Reference errors, such as citation and quotation errors, are common in
scientific papers. Such errors can result in the propagation of inaccurate
information, but are difficult and time-consuming to detect, posing a
significant challenge to scientific publishing. To support automatic detection
of reference errors, this work evaluated the ability of large language models
in OpenAI's GPT family to detect quotation errors. Specifically, we prepared an
expert-annotated, general-domain dataset of statement-reference pairs from
journal articles. Large language models were evaluated in different settings
with varying amounts of reference information provided by retrieval
augmentation. Our results showed that large language models are able to detect
erroneous citations with limited context and without fine-tuning. This study
contributes to the growing literature that seeks to utilize artificial
intelligence to assist in the writing, reviewing, and publishing of scientific
papers. Potential avenues for further improvements in this task are also
discussed.",2024-11-09,"Tianmai M. Zhang, Neil F. Abernethy",http://arxiv.org/pdf/2411.06101v1,cs.CL
ZhoBLiMP: a Systematic Assessment of Language Models with Linguistic Minimal Pairs in Chinese,"Whether and how language models (LMs) acquire the syntax of natural languages
has been widely evaluated under the minimal pair paradigm. However, a lack of
wide-coverage benchmarks in languages other than English has constrained
systematic investigations into the issue. Addressing it, we first introduce
ZhoBLiMP, the most comprehensive benchmark of linguistic minimal pairs for
Chinese to date, with 118 paradigms, covering 15 linguistic phenomena. We then
train 20 LMs of different sizes (14M to 1.4B) on Chinese corpora of various
volumes (100M to 3B tokens) and evaluate them along with 14 off-the-shelf LLMs
on ZhoBLiMP. The overall results indicate that Chinese grammar can be mostly
learned by models with around 500M parameters, trained on 1B tokens with one
epoch, showing limited benefits for further scaling. Most (N=95) linguistic
paradigms are of easy or medium difficulty for LMs, while there are still 13
paradigms that remain challenging even for models with up to 32B parameters. In
regard to how LMs acquire Chinese grammar, we observe a U-shaped learning
pattern in several phenomena, similar to those observed in child language
acquisition.",2024-11-09,"Yikang Liu, Yeting Shen, Hongao Zhu, Lilong Xu, Zhiheng Qian, Siyuan Song, Kejia Zhang, Jialong Tang, Pei Zhang, Baosong Yang, Rui Wang, Hai Hu",http://arxiv.org/pdf/2411.06096v1,cs.CL
Optimizing Large Language Models through Quantization: A Comparative Analysis of PTQ and QAT Techniques,"This paper presents a comprehensive analysis of quantization techniques for
optimizing Large Language Models (LLMs), specifically focusing on Post-Training
Quantization (PTQ) and Quantization-Aware Training (QAT). Through empirical
evaluation across models ranging from 10M to 1B parameters, we demonstrate that
quantization can achieve up to 68% reduction in model size while maintaining
performance within 6% of full-precision baselines when utilizing our proposed
scaling factor {\gamma}. Our experiments show that INT8 quantization delivers a
40% reduction in computational cost and power consumption, while INT4
quantization further improves these metrics by 60%. We introduce a novel
theoretical framework for mixed-precision quantization, deriving optimal bit
allocation strategies based on layer sensitivity and weight variance. Hardware
efficiency evaluations on edge devices reveal that our quantization approach
enables up to 2.4x throughput improvement for INT8 and 3x for INT4, with 60%
power reduction compared to full-precision models.",2024-11-09,Jahid Hasan,http://arxiv.org/pdf/2411.06084v1,cs.CL
Zyda-2: a 5 Trillion Token High-Quality Dataset,"In this technical report, we present Zyda-2: a five trillion token dataset
for language model pretraining. Zyda-2 was used to train our Zamba2 series of
models which are state-of-the-art for their weight class. We build Zyda-2 by
collating high-quality open-source tokens such as FineWeb and DCLM, then
distilling them to the highest-quality subset via cross-deduplication and
model-based quality filtering. Zyda-2 is released under a permissive open
license, and is available at https://huggingface.co/datasets/Zyphra/Zyda-2",2024-11-09,"Yury Tokpanov, Paolo Glorioso, Quentin Anthony, Beren Millidge",http://arxiv.org/pdf/2411.06068v1,cs.CL
Sufficient Context: A New Lens on Retrieval Augmented Generation Systems,"Augmenting LLMs with context leads to improved performance across many
applications. Despite much research on Retrieval Augmented Generation (RAG)
systems, an open question is whether errors arise because LLMs fail to utilize
the context from retrieval or the context itself is insufficient to answer the
query. To shed light on this, we develop a new notion of sufficient context,
along with a method to classify instances that have enough information to
answer the query. We then use sufficient context to analyze several models and
datasets. By stratifying errors based on context sufficiency, we find that
larger models with higher baseline performance (Gemini 1.5 Pro, GPT 4o, Claude
3.5) excel at answering queries when the context is sufficient, but often
output incorrect answers instead of abstaining when the context is not. On the
other hand, smaller models with lower baseline performance (Mistral 3, Gemma 2)
hallucinate or abstain often, even with sufficient context. We further
categorize cases when the context is useful, and improves accuracy, even though
it does not fully answer the query and the model errs without the context.
Building on our findings, we explore ways to reduce hallucinations in RAG
systems, including a new selective generation method that leverages sufficient
context information for guided abstention. Our method improves the fraction of
correct answers among times where the model responds by 2--10\% for Gemini,
GPT, and Gemma. Key findings and the prompts used in our autorater analysis are
available on our github.",2024-11-09,"Hailey Joren, Jianyi Zhang, Chun-Sung Ferng, Da-Cheng Juan, Ankur Taly, Cyrus Rashtchian",http://arxiv.org/pdf/2411.06037v3,cs.CL
LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output,"Immense effort has been dedicated to minimizing the presence of harmful or
biased generative content and better aligning AI output to human intention;
however, research investigating the cultural values of LLMs is still in very
early stages. Cultural values underpin how societies operate, providing
profound insights into the norms, priorities, and decision making of their
members. In recognition of this need for further research, we draw upon
cultural psychology theory and the empirically-validated GLOBE framework to
propose the LLM-GLOBE benchmark for evaluating the cultural value systems of
LLMs, and we then leverage the benchmark to compare the values of Chinese and
US LLMs. Our methodology includes a novel ""LLMs-as-a-Jury"" pipeline which
automates the evaluation of open-ended content to enable large-scale analysis
at a conceptual level. Results clarify similarities and differences that exist
between Eastern and Western cultural value systems and suggest that
open-generation tasks represent a more promising direction for evaluation of
cultural values. We interpret the implications of this research for subsequent
model development, evaluation, and deployment efforts as they relate to LLMs,
AI cultural alignment more broadly, and the influence of AI cultural value
systems on human-AI collaboration outcomes.",2024-11-09,"Elise Karinshak, Amanda Hu, Kewen Kong, Vishwanatha Rao, Jingren Wang, Jindong Wang, Yi Zeng",http://arxiv.org/pdf/2411.06032v1,cs.CL
Improved intent classification based on context information using a windows-based approach,"Conversational systems have a Natural Language Understanding (NLU) module. In
this module, there is a task known as an intent classification that aims at
identifying what a user is attempting to achieve from an utterance. Previous
works use only the current utterance to predict the intent of a given query and
they do not consider the role of the context (one or a few previous utterances)
in the dialog flow for this task. In this work, we propose several approaches
to investigate the role of contextual information for the intent classification
task. Each approach is used to carry out a concatenation between the dialogue
history and the current utterance. Our intent classification method is based on
a convolutional neural network that obtains effective vector representations
from BERT to perform accurate intent classification using an approach
window-based. Our experiments were carried out on a real-world Brazilian
Portuguese corpus with dialog flows provided by Wavy global company. Our
results achieved substantial improvements over the baseline, isolated
utterances (without context), in three approaches using the user's utterance
and system's response from previous messages as dialogue context.",2024-11-09,"Jeanfranco D. Farfan-Escobedo, Julio C. Dos Reis",http://arxiv.org/pdf/2411.06022v1,cs.CL
The Dark Patterns of Personalized Persuasion in Large Language Models: Exposing Persuasive Linguistic Features for Big Five Personality Traits in LLMs Responses,"This study explores how the Large Language Models (LLMs) adjust linguistic
features to create personalized persuasive outputs. While research showed that
LLMs personalize outputs, a gap remains in understanding the linguistic
features of their persuasive capabilities. We identified 13 linguistic features
crucial for influencing personalities across different levels of the Big Five
model of personality. We analyzed how prompts with personality trait
information influenced the output of 19 LLMs across five model families. The
findings show that models use more anxiety-related words for neuroticism,
increase achievement-related words for conscientiousness, and employ fewer
cognitive processes words for openness to experience. Some model families excel
at adapting language for openness to experience, others for conscientiousness,
while only one model adapts language for neuroticism. Our findings show how
LLMs tailor responses based on personality cues in prompts, indicating their
potential to create persuasive content affecting the mind and well-being of the
recipients.",2024-11-08,"Wiktoria Mieleszczenko-Kowszewicz, Dawid Płudowski, Filip Kołodziejczyk, Jakub Świstak, Julian Sienkiewicz, Przemysław Biecek",http://arxiv.org/pdf/2411.06008v2,cs.CL
GUIDEQ: Framework for Guided Questioning for progressive informational collection and classification,"Question Answering (QA) is an important part of tasks like text
classification through information gathering. These are finding increasing use
in sectors like healthcare, customer support, legal services, etc., to collect
and classify responses into actionable categories. LLMs, although can support
QA systems, they face a significant challenge of insufficient or missing
information for classification. Although LLMs excel in reasoning, the models
rely on their parametric knowledge to answer. However, questioning the user
requires domain-specific information aiding to collect accurate information.
Our work, GUIDEQ, presents a novel framework for asking guided questions to
further progress a partial information. We leverage the explainability derived
from the classifier model for along with LLMs for asking guided questions to
further enhance the information. This further information helps in more
accurate classification of a text. GUIDEQ derives the most significant
key-words representative of a label using occlusions. We develop GUIDEQ's
prompting strategy for guided questions based on the top-3 classifier label
outputs and the significant words, to seek specific and relevant information,
and classify in a targeted manner. Through our experimental results, we
demonstrate that GUIDEQ outperforms other LLM-based baselines, yielding
improved F1-Score through the accurate collection of relevant further
information. We perform various analytical studies and also report better
question quality compared to our method.",2024-11-08,"Priya Mishra, Suraj Racha, Kaustubh Ponkshe, Adit Akarsh, Ganesh Ramakrishnan",http://arxiv.org/pdf/2411.05991v1,cs.CL
Game-theoretic LLM: Agent Workflow for Negotiation Games,"This paper investigates the rationality of large language models (LLMs) in
strategic decision-making contexts, specifically within the framework of game
theory. We evaluate several state-of-the-art LLMs across a spectrum of
complete-information and incomplete-information games. Our findings reveal that
LLMs frequently deviate from rational strategies, particularly as the
complexity of the game increases with larger payoff matrices or deeper
sequential trees.
  To address these limitations, we design multiple game-theoretic workflows
that guide the reasoning and decision-making processes of LLMs. These workflows
aim to enhance the models' ability to compute Nash Equilibria and make rational
choices, even under conditions of uncertainty and incomplete information.
Experimental results demonstrate that the adoption of these workflows
significantly improves the rationality and robustness of LLMs in game-theoretic
tasks. Specifically, with the workflow, LLMs exhibit marked improvements in
identifying optimal strategies, achieving near-optimal allocations in
negotiation scenarios, and reducing susceptibility to exploitation during
negotiations. Furthermore, we explore the meta-strategic considerations of
whether it is rational for agents to adopt such workflows, recognizing that the
decision to use or forgo the workflow constitutes a game-theoretic issue in
itself.
  Our research contributes to a deeper understanding of LLMs' decision-making
capabilities in strategic contexts and provides insights into enhancing their
rationality through structured workflows. The findings have implications for
the development of more robust and strategically sound AI agents capable of
navigating complex interactive environments. Code and data supporting this
study are available at \url{https://github.com/Wenyueh/game_theory}.",2024-11-08,"Wenyue Hua, Ollie Liu, Lingyao Li, Alfonso Amayuelas, Julie Chen, Lucas Jiang, Mingyu Jin, Lizhou Fan, Fei Sun, William Wang, Xintong Wang, Yongfeng Zhang",http://arxiv.org/pdf/2411.05990v2,cs.CL
Fine-Grained Reward Optimization for Machine Translation using Error Severity Mappings,"Reinforcement learning (RL) has been proven to be an effective and robust
method for training neural machine translation systems, especially when paired
with powerful reward models that accurately assess translation quality.
However, most research has focused on RL methods that use sentence-level
feedback, leading to inefficient learning signals due to the reward sparsity
problem -- the model receives a single score for the entire sentence. To
address this, we propose a novel approach that leverages fine-grained,
token-level quality assessments along with error severity levels using RL
methods. Specifically, we use xCOMET, a state-of-the-art quality estimation
system, as our token-level reward model. We conduct experiments on small and
large translation datasets with standard encoder-decoder and large language
models-based machine translation systems, comparing the impact of
sentence-level versus fine-grained reward signals on translation quality. Our
results show that training with token-level rewards improves translation
quality across language pairs over baselines according to both automatic and
human evaluation. Furthermore, token-level reward optimization improves
training stability, evidenced by a steady increase in mean rewards over
training epochs.",2024-11-08,"Miguel Moura Ramos, Tomás Almeida, Daniel Vareta, Filipe Azevedo, Sweta Agrawal, Patrick Fernandes, André F. T. Martins",http://arxiv.org/pdf/2411.05986v2,cs.CL
FactLens: Benchmarking Fine-Grained Fact Verification,"Large Language Models (LLMs) have shown impressive capability in language
generation and understanding, but their tendency to hallucinate and produce
factually incorrect information remains a key limitation. To verify
LLM-generated contents and claims from other sources, traditional verification
approaches often rely on holistic models that assign a single factuality label
to complex claims, potentially obscuring nuanced errors. In this paper, we
advocate for a shift toward fine-grained verification, where complex claims are
broken down into smaller sub-claims for individual verification, allowing for
more precise identification of inaccuracies, improved transparency, and reduced
ambiguity in evidence retrieval. However, generating sub-claims poses
challenges, such as maintaining context and ensuring semantic equivalence with
respect to the original claim. We introduce FactLens, a benchmark for
evaluating fine-grained fact verification, with metrics and automated
evaluators of sub-claim quality. The benchmark data is manually curated to
ensure high-quality ground truth. Our results show alignment between automated
FactLens evaluators and human judgments, and we discuss the impact of sub-claim
characteristics on the overall verification performance.",2024-11-08,"Kushan Mitra, Dan Zhang, Sajjadur Rahman, Estevam Hruschka",http://arxiv.org/pdf/2411.05980v2,cs.CL
The Empirical Impact of Data Sanitization on Language Models,"Data sanitization in the context of language modeling involves identifying
sensitive content, such as personally identifiable information (PII), and
redacting them from a dataset corpus. It is a common practice used in natural
language processing (NLP) to maintain privacy. Nevertheless, the impact of data
sanitization on the language understanding capability of a language model
remains less studied. This paper empirically analyzes the effects of data
sanitization across several benchmark language-modeling tasks including
comprehension question answering (Q&A), entailment, sentiment analysis, and
text classification. Our experiments cover a wide spectrum comprising
finetuning small-scale language models, to prompting large language models
(LLMs), on both original and sanitized datasets, and comparing their
performance across the tasks. Interestingly, our results suggest that for some
tasks such as sentiment analysis or entailment, the impact of redaction is
quite low, typically around 1-5%, while for tasks such as comprehension Q&A
there is a big drop of >25% in performance observed in redacted queries as
compared to the original. For tasks that have a higher impact, we perform a
deeper dive to inspect the presence of task-critical entities. Finally, we
investigate correlation between performance and number of redacted entities,
and also suggest a strategy to repair an already redacted dataset by means of
content-based subsampling. Additional details are available at
https://sites.google.com/view/datasan.",2024-11-08,"Anwesan Pal, Radhika Bhargava, Kyle Hinsz, Jacques Esterhuizen, Sudipta Bhattacharya",http://arxiv.org/pdf/2411.05978v1,cs.CL
Multi-Document Financial Question Answering using LLMs,"We propose two new methods for multi-document financial question answering.
First, a method that uses semantic tagging, and then, queries the index to get
the context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that
uses semantic tagging, and, retrieves knowledge graph triples from a graph
database, as context. KG_RAG uses knowledge graphs constructed using a small
model that is fine-tuned using knowledge distillation using a large teacher
model. The data consists of 18 10K reports of Apple, Microsoft, Alphabet,
NVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of
questions in the data consists of 111 complex questions including many esoteric
questions that are difficult to answer and the answers are not completely
obvious. As evaluation metrics, we use overall scores as well as segmented
scores for measurement including the faithfulness, relevance, correctness,
similarity, an LLM based overall score and the rouge scores as well as a
similarity of embeddings. We find that both methods outperform plain RAG
significantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.",2024-11-08,"Shalin Shah, Srikanth Ryali, Ramasubbu Venkatesh",http://arxiv.org/pdf/2411.07264v1,cs.CL
Toward Transdisciplinary Approaches to Audio Deepfake Discernment,"This perspective calls for scholars across disciplines to address the
challenge of audio deepfake detection and discernment through an
interdisciplinary lens across Artificial Intelligence methods and linguistics.
With an avalanche of tools for the generation of realistic-sounding fake speech
on one side, the detection of deepfakes is lagging on the other. Particularly
hindering audio deepfake detection is the fact that current AI models lack a
full understanding of the inherent variability of language and the complexities
and uniqueness of human speech. We see the promising potential in recent
transdisciplinary work that incorporates linguistic knowledge into AI
approaches to provide pathways for expert-in-the-loop and to move beyond expert
agnostic AI-based methods for more robust and comprehensive deepfake detection.",2024-11-08,"Vandana P. Janeja, Christine Mallinson",http://arxiv.org/pdf/2411.05969v1,cs.CL
Sentiment Analysis of Cyberbullying Data in Social Media,"Social media has become an integral part of modern life, but it has also
brought with it the pervasive issue of cyberbullying a serious menace in
today's digital age. Cyberbullying, a form of harassment that occurs on social
networks, has escalated alongside the growth of these platforms. Sentiment
analysis holds significant potential not only for detecting bullying phrases
but also for identifying victims who are at high risk of harm, whether to
themselves or others. Our work focuses on leveraging deep learning and natural
language understanding techniques to detect traces of bullying in social media
posts. We developed a Recurrent Neural Network with Long Short-Term Memory
(LSTM) cells, using different embeddings. One approach utilizes BERT
embeddings, while the other replaces the embeddings layer with the recently
released embeddings API from OpenAI. We conducted a performance comparison
between these two approaches to evaluate their effectiveness in sentiment
analysis of Formspring Cyberbullying data. Our Code is Available at
https://github.com/ppujari/xcs224u",2024-11-08,"Arvapalli Sai Susmitha, Pradeep Pujari",http://arxiv.org/pdf/2411.05958v1,cs.CL
NeKo: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts,"Construction of a general-purpose post-recognition error corrector poses a
crucial question: how can we most effectively train a model on a large mixture
of domain datasets? The answer would lie in learning dataset-specific features
and digesting their knowledge in a single model. Previous methods achieve this
by having separate correction language models, resulting in a significant
increase in parameters. In this work, we present Mixture-of-Experts as a
solution, highlighting that MoEs are much more than a scalability tool. We
propose a Multi-Task Correction MoE, where we train the experts to become an
``expert'' of speech-to-text, language-to-text and vision-to-text datasets by
learning to route each dataset's tokens to its mapped expert. Experiments on
the Open ASR Leaderboard show that we explore a new state-of-the-art
performance by achieving an average relative $5.0$% WER reduction and
substantial improvements in BLEU scores for speech and translation tasks. On
zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with $15.5$% to
$27.6$% relative WER reduction in the Hyporadise benchmark. NeKo performs
competitively on grammar and post-OCR correction as a multi-task model.",2024-11-08,"Yen-Ting Lin, Chao-Han Huck Yang, Zhehuai Chen, Piotr Zelasko, Xuesong Yang, Zih-Ching Chen, Krishna C Puvvada, Szu-Wei Fu, Ke Hu, Jun Wei Chiu, Jagadeesh Balam, Boris Ginsburg, Yu-Chiang Frank Wang",http://arxiv.org/pdf/2411.05945v1,cs.CL
Quantifying artificial intelligence through algebraic generalization,"The rapid development of modern artificial intelligence (AI) systems has
created an urgent need for their scientific quantification. While their fluency
across a variety of domains is impressive, modern AI systems fall short on
tests requiring symbolic processing and abstraction - a glaring limitation
given the necessity for interpretable and reliable technology. Despite a surge
of reasoning benchmarks emerging from the academic community, no comprehensive
and theoretically-motivated framework exists to quantify reasoning (and more
generally, symbolic ability) in AI systems. Here, we adopt a framework from
computational complexity theory to explicitly quantify symbolic generalization:
algebraic circuit complexity. Many symbolic reasoning problems can be recast as
algebraic expressions. Thus, algebraic circuit complexity theory - the study of
algebraic expressions as circuit models (i.e., directed acyclic graphs) - is a
natural framework to study the complexity of symbolic computation. The tools of
algebraic circuit complexity enable the study of generalization by defining
benchmarks in terms of their complexity-theoretic properties (i.e., the
difficulty of a problem). Moreover, algebraic circuits are generic mathematical
objects; for a given algebraic circuit, an arbitrarily large number of samples
can be generated for a specific circuit, making it an optimal testbed for the
data-hungry machine learning algorithms that are used today. Here, we adopt
tools from algebraic circuit complexity theory, apply it to formalize a science
of symbolic generalization, and address key theoretical and empirical
challenges for its successful application to AI science and its impact on the
broader community.",2024-11-08,"Takuya Ito, Murray Campbell, Lior Horesh, Tim Klinger, Parikshit Ram",http://arxiv.org/pdf/2411.05943v1,cs.CL
Mitigating Hallucination with ZeroG: An Advanced Knowledge Management Engine,"The growth of digital documents presents significant challenges in efficient
management and knowledge extraction. Traditional methods often struggle with
complex documents, leading to issues such as hallucinations and high latency in
responses from Large Language Models (LLMs). ZeroG, an innovative approach,
significantly mitigates these challenges by leveraging knowledge distillation
and prompt tuning to enhance model performance.
  ZeroG utilizes a smaller model that replicates the behavior of a larger
teacher model, ensuring contextually relevant and grounded responses, by
employing a black-box distillation approach, it creates a distilled dataset
without relying on intermediate features, optimizing computational efficiency.
This method significantly enhances accuracy and reduces response times,
providing a balanced solution for modern document management.
  Incorporating advanced techniques for document ingestion and metadata
utilization, ZeroG improves the accuracy of question-and-answer systems. The
integration of graph databases and robust metadata management further
streamlines information retrieval, allowing for precise and context-aware
responses. By transforming how organizations interact with complex data, ZeroG
enhances productivity and user experience, offering a scalable solution for the
growing demands of digital document management.",2024-11-08,"Anantha Sharma, Sheeba Elizabeth John, Fatemeh Rezapoor Nikroo, Krupali Bhatt, Mrunal Zambre, Aditi Wikhe",http://arxiv.org/pdf/2411.05936v1,cs.CL
BERTrend: Neural Topic Modeling for Emerging Trends Detection,"Detecting and tracking emerging trends and weak signals in large, evolving
text corpora is vital for applications such as monitoring scientific
literature, managing brand reputation, surveilling critical infrastructure and
more generally to any kind of text-based event detection. Existing solutions
often fail to capture the nuanced context or dynamically track evolving
patterns over time. BERTrend, a novel method, addresses these limitations using
neural topic modeling in an online setting. It introduces a new metric to
quantify topic popularity over time by considering both the number of documents
and update frequency. This metric classifies topics as noise, weak, or strong
signals, flagging emerging, rapidly growing topics for further investigation.
Experimentation on two large real-world datasets demonstrates BERTrend's
ability to accurately detect and track meaningful weak signals while filtering
out noise, offering a comprehensive solution for monitoring emerging trends in
large-scale, evolving text corpora. The method can also be used for
retrospective analysis of past events. In addition, the use of Large Language
Models together with BERTrend offers efficient means for the interpretability
of trends of events.",2024-11-08,"Allaa Boutaleb, Jerome Picault, Guillaume Grosjean",http://arxiv.org/pdf/2411.05930v2,cs.CL
Reducing Distraction in Long-Context Language Models by Focused Learning,"Recent advancements in Large Language Models (LLMs) have significantly
enhanced their capacity to process long contexts. However, effectively
utilizing this long context remains a challenge due to the issue of
distraction, where irrelevant information dominates lengthy contexts, causing
LLMs to lose focus on the most relevant segments. To address this, we propose a
novel training method that enhances LLMs' ability to discern relevant
information through a unique combination of retrieval-based data augmentation
and contrastive learning. Specifically, during fine-tuning with long contexts,
we employ a retriever to extract the most relevant segments, serving as
augmented inputs. We then introduce an auxiliary contrastive learning objective
to explicitly ensure that outputs from the original context and the retrieved
sub-context are closely aligned. Extensive experiments on long single-document
and multi-document QA benchmarks demonstrate the effectiveness of our proposed
method.",2024-11-08,"Zijun Wu, Bingyuan Liu, Ran Yan, Lei Chen, Thomas Delteil",http://arxiv.org/pdf/2411.05928v1,cs.CL
RefreshKV: Updating Small KV Cache During Long-form Generation,"Generating long sequences of tokens given a long-context input is a very
compute-intensive inference scenario for large language models (LLMs). One
prominent inference speed-up approach is to construct a smaller key-value (KV)
cache, relieving LLMs from computing attention over a long sequence of tokens.
While such methods work well to generate short sequences, their performance
degrades rapidly for long-form generation. Most KV compression happens once,
prematurely removing tokens that can be useful later in the generation. We
propose a new inference method, RefreshKV, that flexibly alternates between
full context attention and attention over a subset of input tokens during
generation. After each full attention step, we update the smaller KV cache
based on the attention pattern over the entire input. Applying our method to
off-the-shelf LLMs achieves comparable speedup to eviction-based methods while
improving performance for various long-form generation tasks. Lastly, we show
that continued pretraining with our inference setting brings further gains in
performance.",2024-11-08,"Fangyuan Xu, Tanya Goyal, Eunsol Choi",http://arxiv.org/pdf/2411.05787v2,cs.CL
ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles,"Deaf and hard-of-hearing (DHH) students face significant barriers in
accessing science, technology, engineering, and mathematics (STEM) education,
notably due to the scarcity of STEM resources in signed languages. To help
address this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia
articles on STEM topics in English, interpreted into over 300 hours of American
Sign Language (ASL). ASL STEM Wiki is the first continuous signing dataset
focused on STEM, facilitating the development of AI resources for STEM
education in ASL. We identify several use cases of ASL STEM Wiki with
human-centered applications. For example, because this dataset highlights the
frequent use of fingerspelling for technical concepts, which inhibits DHH
students' ability to learn, we develop models to identify fingerspelled words
-- which can later be used to query for appropriate ASL signs to suggest to
interpreters.",2024-11-08,"Kayo Yin, Chinmay Singh, Fyodor O. Minakov, Vanessa Milan, Hal Daumé III, Cyril Zhang, Alex X. Lu, Danielle Bragg",http://arxiv.org/pdf/2411.05783v1,cs.CL
Using Language Models to Disambiguate Lexical Choices in Translation,"In translation, a concept represented by a single word in a source language
can have multiple variations in a target language. The task of lexical
selection requires using context to identify which variation is most
appropriate for a source text. We work with native speakers of nine languages
to create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual
concept variation when translating from English. We evaluate recent LLMs and
neural machine translation systems on DTAiLS, with the best-performing model,
GPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use
language models to generate English rules describing target-language concept
variations. Providing weaker models with high-quality lexical rules improves
accuracy substantially, in some cases reaching or outperforming GPT-4.",2024-11-08,"Josh Barua, Sanjay Subramanian, Kayo Yin, Alane Suhr",http://arxiv.org/pdf/2411.05781v1,cs.CL
LLMs as Method Actors: A Model for Prompt Engineering and Architecture,"We introduce ""Method Actors"" as a mental model for guiding LLM prompt
engineering and prompt architecture. Under this mental model, LLMs should be
thought of as actors; prompts as scripts and cues; and LLM responses as
performances. We apply this mental model to the task of improving LLM
performance at playing Connections, a New York Times word puzzle game that
prior research identified as a challenging benchmark for evaluating LLM
reasoning. Our experiments with GPT-4o show that a ""Method Actors"" approach can
significantly improve LLM performance over both a vanilla and ""Chain of
Thoughts"" approach. A vanilla approach solves 27% of Connections puzzles in our
dataset and a ""Chain of Thoughts"" approach solves 41% of puzzles, whereas our
strongest ""Method Actor"" approach solves 86% of puzzles. We also test OpenAI's
newest model designed specifically for complex reasoning tasks, o1-preview.
When asked to solve a puzzle all at once, o1-preview solves 79% of Connections
puzzles in our dataset, and when allowed to build puzzle solutions one guess at
a time over multiple API calls, o1-preview solves 100% of the puzzles.
Incorporating a ""Method Actor"" prompt architecture increases the percentage of
puzzles that o1-preview solves perfectly from 76% to 87%.",2024-11-08,Colin Doyle,http://arxiv.org/pdf/2411.05778v2,cs.CL
Quantitative Assessment of Intersectional Empathetic Bias and Understanding,"A growing amount of literature critiques the current operationalizations of
empathy based on loose definitions of the construct. Such definitions
negatively affect dataset quality, model robustness, and evaluation
reliability. We propose an empathy evaluation framework that operationalizes
empathy close to its psychological origins. The framework measures the variance
in responses of LLMs to prompts using existing metrics for empathy and
emotional valence. The variance is introduced through the controlled generation
of the prompts by varying social biases affecting context understanding, thus
impacting empathetic understanding. The control over generation ensures high
theoretical validity of the constructs in the prompt dataset. Also, it makes
high-quality translation, especially into languages that currently have
little-to-no way of evaluating empathy or bias, such as the Slavonic family,
more manageable. Using chosen LLMs and various prompt types, we demonstrate the
empathy evaluation with the framework, including multiple-choice answers and
free generation. The variance in our initial evaluation sample is small and we
were unable to measure convincing differences between the empathetic
understanding in contexts given by different social groups. However, the
results are promising because the models showed significant alterations their
reasoning chains needed to capture the relatively subtle changes in the
prompts. This provides the basis for future research into the construction of
the evaluation sample and statistical methods for measuring the results.",2024-11-08,"Vojtech Formanek, Ondrej Sotolar",http://arxiv.org/pdf/2411.05777v2,cs.CL
Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?,"Political misinformation poses significant challenges to democratic
processes, shaping public opinion and trust in media. Manual fact-checking
methods face issues of scalability and annotator bias, while machine learning
models require large, costly labelled datasets. This study investigates the use
of state-of-the-art large language models (LLMs) as reliable annotators for
detecting political factuality in news articles. Using open-source LLMs, we
create a politically diverse dataset, labelled for bias through LLM-generated
annotations. These annotations are validated by human experts and further
evaluated by LLM-based judges to assess the accuracy and reliability of the
annotations. Our approach offers a scalable and robust alternative to
traditional fact-checking, enhancing transparency and public trust in media.",2024-11-08,"Veronica Chatrath, Marcelo Lotif, Shaina Raza",http://arxiv.org/pdf/2411.05775v1,cs.CL
FinDVer: Explainable Claim Verification over Long and Hybrid-Content Financial Documents,"We introduce FinDVer, a comprehensive benchmark specifically designed to
evaluate the explainable claim verification capabilities of LLMs in the context
of understanding and analyzing long, hybrid-content financial documents.
FinDVer contains 2,400 expert-annotated examples, divided into three subsets:
information extraction, numerical reasoning, and knowledge-intensive reasoning,
each addressing common scenarios encountered in real-world financial contexts.
We assess a broad spectrum of LLMs under long-context and RAG settings. Our
results show that even the current best-performing system, GPT-4o, still lags
behind human experts. We further provide in-depth analysis on long-context and
RAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering
insights to drive future advancements. We believe that FinDVer can serve as a
valuable benchmark for evaluating LLMs in claim verification over complex,
expert-domain documents.",2024-11-08,"Yilun Zhao, Yitao Long, Yuru Jiang, Chengye Wang, Weiyuan Chen, Hongjun Liu, Yiming Zhang, Xiangru Tang, Chen Zhao, Arman Cohan",http://arxiv.org/pdf/2411.05764v1,cs.CL
Multi-hop Evidence Pursuit Meets the Web: Team Papelo at FEVER 2024,"Separating disinformation from fact on the web has long challenged both the
search and the reasoning powers of humans. We show that the reasoning power of
large language models (LLMs) and the retrieval power of modern search engines
can be combined to automate this process and explainably verify claims. We
integrate LLMs and search under a multi-hop evidence pursuit strategy. This
strategy generates an initial question based on an input claim using a sequence
to sequence model, searches and formulates an answer to the question, and
iteratively generates follow-up questions to pursue the evidence that is
missing using an LLM. We demonstrate our system on the FEVER 2024 (AVeriTeC)
shared task. Compared to a strategy of generating all the questions at once,
our method obtains .045 higher label accuracy and .155 higher AVeriTeC score
(evaluating the adequacy of the evidence). Through ablations, we show the
importance of various design choices, such as the question generation method,
medium-sized context, reasoning with one document at a time, adding metadata,
paraphrasing, reducing the problem to two classes, and reconsidering the final
verdict. Our submitted system achieves .510 AVeriTeC score on the dev set and
.477 AVeriTeC score on the test set.",2024-11-08,Christopher Malon,http://arxiv.org/pdf/2411.05762v1,cs.CL
End-to-End Navigation with Vision Language Models: Transforming Spatial Reasoning into Question-Answering,"We present VLMnav, an embodied framework to transform a Vision-Language Model
(VLM) into an end-to-end navigation policy. In contrast to prior work, we do
not rely on a separation between perception, planning, and control; instead, we
use a VLM to directly select actions in one step. Surprisingly, we find that a
VLM can be used as an end-to-end policy zero-shot, i.e., without any
fine-tuning or exposure to navigation data. This makes our approach open-ended
and generalizable to any downstream navigation task. We run an extensive study
to evaluate the performance of our approach in comparison to baseline prompting
methods. In addition, we perform a design analysis to understand the most
impactful design decisions. Visual examples and code for our project can be
found at https://jirl-upenn.github.io/VLMnav/",2024-11-08,"Dylan Goetting, Himanshu Gaurav Singh, Antonio Loquercio",http://arxiv.org/pdf/2411.05755v1,cs.CL
FisherMask: Enhancing Neural Network Labeling Efficiency in Image Classification Using Fisher Information,"Deep learning (DL) models are popular across various domains due to their
remarkable performance and efficiency. However, their effectiveness relies
heavily on large amounts of labeled data, which are often time-consuming and
labor-intensive to generate manually. To overcome this challenge, it is
essential to develop strategies that reduce reliance on extensive labeled data
while preserving model performance. In this paper, we propose FisherMask, a
Fisher information-based active learning (AL) approach that identifies key
network parameters by masking them based on their Fisher information values.
FisherMask enhances batch AL by using Fisher information to select the most
critical parameters, allowing the identification of the most impactful samples
during AL training. Moreover, Fisher information possesses favorable
statistical properties, offering valuable insights into model behavior and
providing a better understanding of the performance characteristics within the
AL pipeline. Our extensive experiments demonstrate that FisherMask
significantly outperforms state-of-the-art methods on diverse datasets,
including CIFAR-10 and FashionMNIST, especially under imbalanced settings.
These improvements lead to substantial gains in labeling efficiency. Hence
serving as an effective tool to measure the sensitivity of model parameters to
data samples. Our code is available on
\url{https://github.com/sgchr273/FisherMask}.",2024-11-08,"Shreen Gul, Mohamed Elmahallawy, Sanjay Madria, Ardhendu Tripathy",http://arxiv.org/pdf/2411.05752v1,cs.CL
Aioli: A Unified Optimization Framework for Language Model Data Mixing,"Language model performance depends on identifying the optimal mixture of data
groups to train on (e.g., law, code, math). Prior work has proposed a diverse
set of methods to efficiently learn mixture proportions, ranging from fitting
regression models over training runs to dynamically updating proportions
throughout training. Surprisingly, we find that no existing method consistently
outperforms a simple stratified sampling baseline in terms of average test
perplexity. To understand this inconsistency, we unify existing methods into a
standard framework, showing they are equivalent to solving a common
optimization problem: minimize average loss subject to a method-specific mixing
law -- an implicit assumption on the relationship between loss and mixture
proportions. This framework suggests that measuring the fidelity of a method's
mixing law can offer insights into its performance. Empirically, we find that
existing methods set their mixing law parameters inaccurately, resulting in the
inconsistent mixing performance we observe. Using this insight, we derive a new
online method named Aioli, which directly estimates the mixing law parameters
throughout training and uses them to dynamically adjust proportions. Aioli
outperforms stratified sampling on 6 out of 6 datasets by an average of 0.27
test perplexity points, whereas existing methods fail to consistently beat
stratified sampling, doing up to 6.9 points worse. Moreover, in a practical
setting where proportions are learned on shorter runs due to computational
constraints, Aioli can dynamically adjust these proportions over the full
training run, consistently improving performance over existing methods by up to
12.012 test perplexity points.",2024-11-08,"Mayee F. Chen, Michael Y. Hu, Nicholas Lourie, Kyunghyun Cho, Christopher Ré",http://arxiv.org/pdf/2411.05735v2,cs.CL
Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small Language Model,"We present a novel 4.5B parameter small language model that can handle
multiple input and output modalities, including text, images, videos, and
audio. Despite its small size, the model achieves near state-of-the-art
performance on a variety of tasks, demonstrating the potential of multi-modal
models to tackle complex real-world problems. Our approach leverages recent
advancements in language modeling and multi-task learning to create a versatile
and high-performing model that can even be deployed for edge inference.
Experimental results show the model's strong performance across multiple
benchmarks, paving the way for further progress in multi-modal artificial
intelligence.",2024-11-08,"Ben Koska, Mojmír Horváth",http://arxiv.org/pdf/2411.05903v1,cs.CL
Autoregressive Models in Vision: A Survey,"Autoregressive modeling has been a huge success in the field of natural
language processing (NLP). Recently, autoregressive models have emerged as a
significant area of focus in computer vision, where they excel in producing
high-quality visual content. Autoregressive models in NLP typically operate on
subword tokens. However, the representation strategy in computer vision can
vary in different levels, \textit{i.e.}, pixel-level, token-level, or
scale-level, reflecting the diverse and hierarchical nature of visual data
compared to the sequential structure of language. This survey comprehensively
examines the literature on autoregressive models applied to vision. To improve
readability for researchers from diverse research backgrounds, we start with
preliminary sequence representation and modeling in vision. Next, we divide the
fundamental frameworks of visual autoregressive models into three general
sub-categories, including pixel-based, token-based, and scale-based models
based on the strategy of representation. We then explore the interconnections
between autoregressive models and other generative models. Furthermore, we
present a multi-faceted categorization of autoregressive models in computer
vision, including image generation, video generation, 3D generation, and
multi-modal generation. We also elaborate on their applications in diverse
domains, including emerging domains such as embodied AI and 3D medical AI, with
about 250 related references. Finally, we highlight the current challenges to
autoregressive models in vision with suggestions about potential research
directions. We have also set up a Github repository to organize the papers
included in this survey at:
\url{https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey}.",2024-11-08,"Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, Zhongwei Wan, Jinfa Huang, Chaofan Tao, Shen Yan, Huaxiu Yao, Lingpeng Kong, Hongxia Yang, Mi Zhang, Guillermo Sapiro, Jiebo Luo, Ping Luo, Ngai Wong",http://arxiv.org/pdf/2411.05902v1,cs.CL
Image2Text2Image: A Novel Framework for Label-Free Evaluation of Image-to-Text Generation with Text-to-Image Diffusion Models,"Evaluating the quality of automatically generated image descriptions is a
complex task that requires metrics capturing various dimensions, such as
grammaticality, coverage, accuracy, and truthfulness. Although human evaluation
provides valuable insights, its cost and time-consuming nature pose
limitations. Existing automated metrics like BLEU, ROUGE, METEOR, and CIDEr
attempt to fill this gap, but they often exhibit weak correlations with human
judgment. To address this challenge, we propose a novel evaluation framework
called Image2Text2Image, which leverages diffusion models, such as Stable
Diffusion or DALL-E, for text-to-image generation. In the Image2Text2Image
framework, an input image is first processed by a selected image captioning
model, chosen for evaluation, to generate a textual description. Using this
generated description, a diffusion model then creates a new image. By comparing
features extracted from the original and generated images, we measure their
similarity using a designated similarity metric. A high similarity score
suggests that the model has produced a faithful textual description, while a
low score highlights discrepancies, revealing potential weaknesses in the
model's performance. Notably, our framework does not rely on human-annotated
reference captions, making it a valuable tool for assessing image captioning
models. Extensive experiments and human evaluations validate the efficacy of
our proposed Image2Text2Image evaluation framework. The code and dataset will
be published to support further research in the community.",2024-11-08,"Jia-Hong Huang, Hongyi Zhu, Yixian Shen, Stevan Rudinac, Evangelos Kanoulas",http://arxiv.org/pdf/2411.05706v1,cs.CL
Asterisk*: Keep it Simple,"This paper describes Asterisk, a compact GPT-based model for generating text
embeddings. The model uses a minimalist architecture with two layers, two
attention heads, and 256 embedding dimensions. By applying knowledge
distillation from larger pretrained models, we explore the trade-offs between
model size and performance while minimizing computational and memory
requirements. The model is primarily evaluated and optimized for classification
tasks, with experimental results showing its moderate performance in zero-shot
classification across various downstream applications. With additional
configuration, the model performance can approach or even surpass that of
larger architectures on specific classification tasks.",2024-11-08,Andrew Semenov,http://arxiv.org/pdf/2411.05691v1,cs.CL
Unmasking the Limits of Large Language Models: A Systematic Evaluation of Masked Text Processing Ability through MskQA and MskCal,"This paper sheds light on the limitations of Large Language Models (LLMs) by
rigorously evaluating their ability to process masked text. We introduce two
novel tasks: MskQA, measuring reasoning on masked question-answering datasets
like RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmetic
problems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit some
resilience to masked text, their performance is highly contingent on masking
rates and semantic cues. Specifically, ""solid masking,"" where semantic clues
are entirely absent, leads to a significant performance drop compared to
""partial lifting,"" where some semantic information is retained, indicating
LLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistently
outperforms 4o-mini, particularly in MskCal, demonstrating a greater ability to
handle numerical reasoning with masked text. This underscores the crucial role
of semantic cues in the reasoning process of LLMs. Our study illuminates the
interplay between background knowledge and reasoning ability in masked text
processing, paving the way for a deeper understanding of LLM capabilities and
limitations, and highlighting the need for more robust evaluation methods to
accurately assess their true comprehension abilities.",2024-11-08,"Fuka Matsuzaki, Haru-Tada Sato",http://arxiv.org/pdf/2411.05665v1,cs.CL
Humans and Large Language Models in Clinical Decision Support: A Study with Medical Calculators,"Although large language models (LLMs) have been assessed for general medical
knowledge using licensing exams, their ability to support clinical
decision-making, such as selecting medical calculators, remains uncertain. We
assessed nine LLMs, including open-source, proprietary, and domain-specific
models, with 1,009 multiple-choice question-answer pairs across 35 clinical
calculators and compared LLMs to humans on a subset of questions. While the
highest-performing LLM, OpenAI o1, provided an answer accuracy of 66.0% (CI:
56.7-75.3%) on the subset of 100 questions, two human annotators nominally
outperformed LLMs with an average answer accuracy of 79.5% (CI: 73.5-85.0%).
Ultimately, we evaluated medical trainees and LLMs in recommending medical
calculators across clinical scenarios like risk stratification and diagnosis.
With error analysis showing that the highest-performing LLMs continue to make
mistakes in comprehension (49.3% of errors) and calculator knowledge (7.1% of
errors), our findings highlight that LLMs are not superior to humans in
calculator recommendation.",2024-11-08,"Nicholas Wan, Qiao Jin, Joey Chan, Guangzhi Xiong, Serina Applebaum, Aidan Gilson, Reid McMurry, R. Andrew Taylor, Aidong Zhang, Qingyu Chen, Zhiyong Lu",http://arxiv.org/pdf/2411.05897v2,cs.CL
Evaluating Large Language Model Capability in Vietnamese Fact-Checking Data Generation,"Large Language Models (LLMs), with gradually improving reading comprehension
and reasoning capabilities, are being applied to a range of complex language
tasks, including the automatic generation of language data for various
purposes. However, research on applying LLMs for automatic data generation in
low-resource languages like Vietnamese is still underdeveloped and lacks
comprehensive evaluation. In this paper, we explore the use of LLMs for
automatic data generation for the Vietnamese fact-checking task, which faces
significant data limitations. Specifically, we focus on fact-checking data
where claims are synthesized from multiple evidence sentences to assess the
information synthesis capabilities of LLMs. We develop an automatic data
construction process using simple prompt techniques on LLMs and explore several
methods to improve the quality of the generated data. To evaluate the quality
of the data generated by LLMs, we conduct both manual quality assessments and
performance evaluations using language models. Experimental results and manual
evaluations illustrate that while the quality of the generated data has
significantly improved through fine-tuning techniques, LLMs still cannot match
the data quality produced by humans.",2024-11-08,"Long Truong To, Hung Tuan Le, Dat Van-Thanh Nguyen, Manh Trong Nguyen, Tri Thien Nguyen, Tin Van Huynh, Kiet Van Nguyen",http://arxiv.org/pdf/2411.05641v1,cs.CL
Assessing Open-Source Large Language Models on Argumentation Mining Subtasks,"We explore the capability of four open-sourcelarge language models (LLMs) in
argumentation mining (AM). We conduct experiments on three different corpora;
persuasive essays(PE), argumentative microtexts (AMT) Part 1 and Part 2, based
on two argumentation mining sub-tasks: (i) argumentative discourse units
classifications (ADUC), and (ii) argumentative relation classification (ARC).
This work aims to assess the argumentation capability of open-source LLMs,
including Mistral 7B, Mixtral8x7B, LlamA2 7B and LlamA3 8B in both, zero-shot
and few-shot scenarios. Our analysis contributes to further assessing
computational argumentation with open-source LLMs in future research efforts.",2024-11-08,"Mohammad Yeghaneh Abkenar, Weixing Wang, Hendrik Graupner, Manfred Stede",http://arxiv.org/pdf/2411.05639v1,cs.CL
Impact of Fake News on Social Media Towards Public Users of Different Age Groups,"This study examines how fake news affects social media users across a range
of age groups and how machine learning (ML) and artificial intelligence (AI)
can help reduce the spread of false information. The paper evaluates various
machine learning models for their efficacy in identifying and categorizing fake
news and examines current trends in the spread of fake news, including deepfake
technology. The study assesses four models using a Kaggle dataset: Random
Forest, Support Vector Machine (SVM), Neural Networks, and Logistic Regression.
The results show that SVM and neural networks perform better than other models,
with accuracies of 93.29% and 93.69%, respectively. The study also emphasises
how people in the elder age group diminished capacity for critical analysis of
news content makes them more susceptible to disinformation. Natural language
processing (NLP) and deep learning approaches have the potential to improve the
accuracy of false news detection. Biases in AI and ML models and difficulties
in identifying information generated by AI continue to be major problems in
spite of the developments. The study recommends that datasets be expanded to
encompass a wider range of languages and that detection algorithms be
continuously improved to keep up with the latest advancements in disinformation
tactics. In order to combat fake news and promote an informed and resilient
society, this study emphasizes the value of cooperative efforts between AI
researchers, social media platforms, and governments.",2024-11-08,"Kahlil bin Abdul Hakim, Sathishkumar Veerappampalayam Easwaramoorthy",http://arxiv.org/pdf/2411.05638v1,cs.CL
One Small and One Large for Document-level Event Argument Extraction,"Document-level Event Argument Extraction (EAE) faces two challenges due to
increased input length: 1) difficulty in distinguishing semantic boundaries
between events, and 2) interference from redundant information. To address
these issues, we propose two methods. The first method introduces the Co and
Structure Event Argument Extraction model (CsEAE) based on Small Language
Models (SLMs). CsEAE includes a co-occurrences-aware module, which integrates
information about all events present in the current input through context
labeling and co-occurrences event prompts extraction. Additionally, CsEAE
includes a structure-aware module that reduces interference from redundant
information by establishing structural relationships between the sentence
containing the trigger and other sentences in the document. The second method
introduces new prompts to transform the extraction task into a generative task
suitable for Large Language Models (LLMs), addressing gaps in EAE performance
using LLMs under Supervised Fine-Tuning (SFT) conditions. We also fine-tuned
multiple datasets to develop an LLM that performs better across most datasets.
Finally, we applied insights from CsEAE to LLMs, achieving further performance
improvements. This suggests that reliable insights validated on SLMs are also
applicable to LLMs. We tested our models on the Rams, WikiEvents, and MLEE
datasets. The CsEAE model achieved improvements of 2.1\%, 2.3\%, and 3.2\% in
the Arg-C F1 metric compared to the baseline, PAIE~\cite{PAIE}. For LLMs, we
demonstrated that their performance on document-level datasets is comparable to
that of SLMs~\footnote{All code is available at
https://github.com/simon-p-j-r/CsEAE}.",2024-11-08,"Jiaren Peng, Hongda Sun, Wenzhong Yang, Fuyuan Wei, Liang He, Liejun Wang",http://arxiv.org/pdf/2411.05895v1,cs.CL
Evaluating and Adapting Large Language Models to Represent Folktales in Low-Resource Languages,"Folktales are a rich resource of knowledge about the society and culture of a
civilisation. Digital folklore research aims to use automated techniques to
better understand these folktales, and it relies on abstract representations of
the textual data. Although a number of large language models (LLMs) claim to be
able to represent low-resource langauges such as Irish and Gaelic, we present
two classification tasks to explore how useful these representations are, and
three adaptations to improve the performance of these models. We find that
adapting the models to work with longer sequences, and continuing pre-training
on the domain of folktales improves classification performance, although these
findings are tempered by the impressive performance of a baseline SVM with
non-contextual features.",2024-11-08,"JA Meaney, Beatrice Alex, William Lamb",http://arxiv.org/pdf/2411.05593v1,cs.CL
SSSD: Simply-Scalable Speculative Decoding,"Over the past year, Speculative Decoding has gained popularity as a technique
for accelerating Large Language Model inference. While several methods have
been introduced, most struggle to deliver satisfactory performance at batch
sizes typical for data centers ($\geq 8$) and often involve significant
deployment complexities. In this work, we offer a theoretical explanation of
how Speculative Decoding can be effectively utilized with larger batch sizes.
We also introduce a method that integrates seamlessly into existing systems
without additional training or the complexity of deploying a small LLM. In a
continuous batching setting, we achieve a 4x increase in throughput without any
latency impact for short context generation, and a 1.7-2x improvement in both
latency and throughput for longer contexts.",2024-11-08,"Michele Marzollo, Jiawei Zhuang, Niklas Roemer, Lorenz K. Müller, Lukas Cavigelli",http://arxiv.org/pdf/2411.05894v1,cs.CL
Assessing the Answerability of Queries in Retrieval-Augmented Code Generation,"Thanks to unprecedented language understanding and generation capabilities of
large language model (LLM), Retrieval-augmented Code Generation (RaCG) has
recently been widely utilized among software developers. While this has
increased productivity, there are still frequent instances of incorrect codes
being provided. In particular, there are cases where plausible yet incorrect
codes are generated for queries from users that cannot be answered with the
given queries and API descriptions. This study proposes a task for evaluating
answerability, which assesses whether valid answers can be generated based on
users' queries and retrieved APIs in RaCG. Additionally, we build a benchmark
dataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to
evaluate the performance of models performing this task. Experimental results
show that this task remains at a very challenging level, with baseline models
exhibiting a low performance of 46.7%. Furthermore, this study discusses
methods that could significantly improve performance.",2024-11-08,"Geonmin Kim, Jaeyeon Kim, Hancheol Park, Wooksu Shin, Tae-Ho Kim",http://arxiv.org/pdf/2411.05547v2,cs.CL
Identifying and Decomposing Compound Ingredients in Meal Plans Using Large Language Models,"This study explores the effectiveness of Large Language Models in meal
planning, focusing on their ability to identify and decompose compound
ingredients. We evaluated three models-GPT-4o, Llama-3 (70b), and Mixtral
(8x7b)-to assess their proficiency in recognizing and breaking down complex
ingredient combinations. Preliminary results indicate that while Llama-3 (70b)
and GPT-4o excels in accurate decomposition, all models encounter difficulties
with identifying essential elements like seasonings and oils. Despite strong
overall performance, variations in accuracy and completeness were observed
across models. These findings underscore LLMs' potential to enhance
personalized nutrition but highlight the need for further refinement in
ingredient decomposition. Future research should address these limitations to
improve nutritional recommendations and health outcomes.",2024-11-08,"Leon Kopitar, Leon Bedrac, Larissa J Strath, Jiang Bian, Gregor Stiglic",http://arxiv.org/pdf/2411.05892v1,cs.CL
How Good is Your Wikipedia? Auditing Data Quality for Low-resource and Multilingual NLP,"Wikipedia's perceived high quality and broad language coverage have
established it as a fundamental resource in multilingual NLP. In the context of
low-resource languages, however, these quality assumptions are increasingly
being scrutinised. This paper critically examines the data quality of Wikipedia
in a non-English setting by subjecting it to various quality filtering
techniques, revealing widespread issues such as a high percentage of one-line
articles and duplicate articles. We evaluate the downstream impact of quality
filtering on Wikipedia and find that data quality pruning is an effective means
for resource-efficient training without hurting performance, especially for
low-resource languages. Moreover, we advocate for a shift in perspective from
seeking a general definition of data quality towards a more language- and
task-specific one. Ultimately, we aim for this study to serve as a guide to
using Wikipedia for pretraining in a multilingual setting.",2024-11-08,"Kushal Tatariya, Artur Kulmizev, Wessel Poelman, Esther Ploeger, Marcel Bollmann, Johannes Bjerva, Jiaming Luo, Heather Lent, Miryam de Lhoneux",http://arxiv.org/pdf/2411.05527v2,cs.CL
An Early FIRST Reproduction and Improvements to Single-Token Decoding for Fast Listwise Reranking,"Recent advances have demonstrated that large language models (LLMs) excel as
listwise rerankers, but their high computational demands remain a barrier to
widespread adoption. Further, the traditional language modeling (LM) objective
is not ideally suited for reranking tasks. FIRST is a novel approach that
addresses these challenges by integrating a learning-to-rank objective and
leveraging the logits of only the first generated token, thereby significantly
reducing inference latency compared to traditional LLM rerankers. In this
study, we extend the evaluation of FIRST to the TREC Deep Learning datasets
(DL19-22), validating its robustness across diverse domains. We investigate the
influence of different first-stage retrievers on FIRST rerankers, observing
diminishing returns and patterns consistent with traditional LLM rerankers.
Through applying the FIRST objective to a broader range of backbone models, we
achieve effectiveness surpassing the original implementation. Our experiments
confirm that fast reranking with single-token logits does not compromise
out-of-domain reranking quality. To better quantify the computational savings
in the original study, we measure and compare latency to find a 21%-42% gain
across various models and benchmarks. Moreover, while LM training implicitly
improves zero-shot single-token reranking, our experiments also raise questions
about whether LM pre-training may hinder subsequent fine-tuning with the FIRST
objective. These findings pave the way for more efficient and effective
listwise reranking in future applications.",2024-11-08,"Zijian Chen, Ronak Pradeep, Jimmy Lin",http://arxiv.org/pdf/2411.05508v2,cs.CL
LBPE: Long-token-first Tokenization to Improve Large Language Models,"The prevalent use of Byte Pair Encoding (BPE) in Large Language Models (LLMs)
facilitates robust handling of subword units and avoids issues of
out-of-vocabulary words. Despite its success, a critical challenge persists:
long tokens, rich in semantic information, have fewer occurrences in tokenized
datasets compared to short tokens, which can result in imbalanced learning
issue across different tokens. To address that, we propose LBPE, which
prioritizes long tokens during the encoding process. LBPE generates tokens
according to their reverse ranks of token length rather than their ranks in the
vocabulary, granting longer tokens higher priority during the encoding process.
Consequently, LBPE smooths the frequency differences between short and long
tokens, and thus mitigates the learning imbalance. Extensive experiments across
diverse language modeling tasks demonstrate that LBPE consistently outperforms
the original BPE, well demonstrating its effectiveness.",2024-11-08,"Haoran Lian, Yizhe Xiong, Zijia Lin, Jianwei Niu, Shasha Mo, Hui Chen, Peng Liu, Guiguang Ding",http://arxiv.org/pdf/2411.05504v1,cs.CL
"KyrgyzNLP: Challenges, Progress, and Future","Large language models (LLMs) have excelled in numerous benchmarks, advancing
AI applications in both linguistic and non-linguistic tasks. However, this has
primarily benefited well-resourced languages, leaving less-resourced ones
(LRLs) at a disadvantage. In this paper, we highlight the current state of the
NLP field in the specific LRL: kyrgyz tili.
  Human evaluation, including annotated datasets created by native speakers,
remains an irreplaceable component of reliable NLP performance, especially for
LRLs where automatic evaluations can fall short. In recent assessments of the
resources for Turkic languages, Kyrgyz is labeled with the status 'Scraping
By', a severely under-resourced language spoken by millions. This is concerning
given the growing importance of the language, not only in Kyrgyzstan but also
among diaspora communities where it holds no official status.
  We review prior efforts in the field, noting that many of the publicly
available resources have only recently been developed, with few exceptions
beyond dictionaries (the processed data used for the analysis is presented at
https://kyrgyznlp.github.io/). While recent papers have made some headway, much
more remains to be done. Despite interest and support from both business and
government sectors in the Kyrgyz Republic, the situation for Kyrgyz language
resources remains challenging. We stress the importance of community-driven
efforts to build these resources, ensuring the future advancement
sustainability. We then share our view of the most pressing challenges in
Kyrgyz NLP. Finally, we propose a roadmap for future development in terms of
research topics and language resources.",2024-11-08,"Anton Alekseev, Timur Turatali",http://arxiv.org/pdf/2411.05503v2,cs.CL
EUREKHA: Enhancing User Representation for Key Hackers Identification in Underground Forums,"Underground forums serve as hubs for cybercriminal activities, offering a
space for anonymity and evasion of conventional online oversight. In these
hidden communities, malicious actors collaborate to exchange illicit knowledge,
tools, and tactics, driving a range of cyber threats from hacking techniques to
the sale of stolen data, malware, and zero-day exploits. Identifying the key
instigators (i.e., key hackers), behind these operations is essential but
remains a complex challenge. This paper presents a novel method called EUREKHA
(Enhancing User Representation for Key Hacker Identification in Underground
Forums), designed to identify these key hackers by modeling each user as a
textual sequence. This sequence is processed through a large language model
(LLM) for domain-specific adaptation, with LLMs acting as feature extractors.
These extracted features are then fed into a Graph Neural Network (GNN) to
model user structural relationships, significantly improving identification
accuracy. Furthermore, we employ BERTopic (Bidirectional Encoder
Representations from Transformers Topic Modeling) to extract personalized
topics from user-generated content, enabling multiple textual representations
per user and optimizing the selection of the most representative sequence. Our
study demonstrates that fine-tuned LLMs outperform state-of-the-art methods in
identifying key hackers. Additionally, when combined with GNNs, our model
achieves significant improvements, resulting in approximately 6% and 10%
increases in accuracy and F1-score, respectively, over existing methods.
EUREKHA was tested on the Hack-Forums dataset, and we provide open-source
access to our code.",2024-11-08,"Abdoul Nasser Hassane Amadou, Anas Motii, Saida Elouardi, EL Houcine Bergou",http://arxiv.org/pdf/2411.05479v1,cs.CL
Supporting Automated Fact-checking across Topics: Similarity-driven Gradual Topic Learning for Claim Detection,"Selecting check-worthy claims for fact-checking is considered a crucial part
of expediting the fact-checking process by filtering out and ranking the
check-worthy claims for being validated among the impressive amount of claims
could be found online. The check-worthy claim detection task, however, becomes
more challenging when the model needs to deal with new topics that differ from
those seen earlier. In this study, we propose a domain-adaptation framework for
check-worthy claims detection across topics for the Arabic language to adopt a
new topic, mimicking a real-life scenario of the daily emergence of events
worldwide. We propose the Gradual Topic Learning (GTL) model, which builds an
ability to learning gradually and emphasizes the check-worthy claims for the
target topic during several stages of the learning process. In addition, we
introduce the Similarity-driven Gradual Topic Learning (SGTL) model that
synthesizes gradual learning with a similarity-based strategy for the target
topic. Our experiments demonstrate the effectiveness of our proposed model,
showing an overall tendency for improving performance over the state-of-the-art
baseline across 11 out of the 14 topics under study.",2024-11-08,"Amani S. Abumansour, Arkaitz Zubiaga",http://arxiv.org/pdf/2411.05460v1,cs.CL
WorkflowLLM: Enhancing Workflow Orchestration Capability of Large Language Models,"Recent advancements in large language models (LLMs) have driven a
revolutionary paradigm shift in process automation from Robotic Process
Automation to Agentic Process Automation by automating the workflow
orchestration procedure based on LLMs. However, existing LLMs (even the
advanced OpenAI GPT-4o) are confined to achieving satisfactory capability in
workflow orchestration. To address this limitation, we present WorkflowLLM, a
data-centric framework elaborately designed to enhance the capability of LLMs
in workflow orchestration. It first constructs a large-scale fine-tuning
dataset WorkflowBench with 106,763 samples, covering 1,503 APIs from 83
applications across 28 categories. Specifically, the construction process can
be divided into three phases: (1) Data Collection: we collect real-world
workflow data from Apple Shortcuts and RoutineHub, transcribing them into
Python-style code. We further equip them with generated hierarchical thought
via ChatGPT. (2) Query Expansion: we prompt ChatGPT to generate more task
queries to enrich the diversity and complexity of workflows. (3) Workflow
Generation: we leverage an annotator model trained on collected data to
generate workflows for synthesized queries. Finally, we merge the synthetic
samples that pass quality confirmation with the collected samples to obtain the
WorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain
WorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong
capacity to orchestrate complex workflows, while also achieving notable
generalization performance on previously unseen APIs. Additionally,
WorkflowBench exhibits robust zero-shot generalization capabilities on an
out-of-distribution task planning dataset, T-Eval. Our data and code are
available at https://github.com/OpenBMB/WorkflowLLM.",2024-11-08,"Shengda Fan, Xin Cong, Yuepeng Fu, Zhong Zhang, Shuyan Zhang, Yuanwei Liu, Yesai Wu, Yankai Lin, Zhiyuan Liu, Maosong Sun",http://arxiv.org/pdf/2411.05451v1,cs.CL
VISTA: Visual Integrated System for Tailored Automation in Math Problem Generation Using LLM,"Generating accurate and consistent visual aids is a critical challenge in
mathematics education, where visual representations like geometric shapes and
functions play a pivotal role in enhancing student comprehension. This paper
introduces a novel multi-agent framework that leverages Large Language Models
(LLMs) to automate the creation of complex mathematical visualizations
alongside coherent problem text. Our approach not only simplifies the
generation of precise visual aids but also aligns these aids with the problem's
core mathematical concepts, improving both problem creation and assessment. By
integrating multiple agents, each responsible for distinct tasks such as
numeric calculation, geometry validation, and visualization, our system
delivers mathematically accurate and contextually relevant problems with visual
aids. Evaluation across Geometry and Function problem types shows that our
method significantly outperforms basic LLMs in terms of text coherence,
consistency, relevance and similarity, while maintaining the essential
geometrical and functional integrity of the original problems. Although some
challenges remain in ensuring consistent visual outputs, our framework
demonstrates the immense potential of LLMs in transforming the way educators
generate and utilize visual aids in math education.",2024-11-08,"Jeongwoo Lee, Kwangsuk Park, Jihyeon Park",http://arxiv.org/pdf/2411.05423v1,cs.CL
Learning the rules of peptide self-assembly through data mining with large language models,"Peptides are ubiquitous and important biologically derived molecules, that
have been found to self-assemble to form a wide array of structures. Extensive
research has explored the impacts of both internal chemical composition and
external environmental stimuli on the self-assembly behaviour of these systems.
However, there is yet to be a systematic study that gathers this rich
literature data and collectively examines these experimental factors to provide
a global picture of the fundamental rules that govern protein self-assembly
behavior. In this work, we curate a peptide assembly database through a
combination of manual processing by human experts and literature mining
facilitated by a large language model. As a result, we collect more than 1,000
experimental data entries with information about peptide sequence, experimental
conditions and corresponding self-assembly phases. Utilizing the collected
data, ML models are trained and evaluated, demonstrating excellent accuracy
(>80\%) and efficiency in peptide assembly phase classification. Moreover, we
fine-tune our GPT model for peptide literature mining with the developed
dataset, which exhibits markedly superior performance in extracting information
from academic publications relative to the pre-trained model. We find that this
workflow can substantially improve efficiency when exploring potential
self-assembling peptide candidates, through guiding experimental work, while
also deepening our understanding of the mechanisms governing peptide
self-assembly. In doing so, novel structures can be accessed for a range of
applications including sensing, catalysis and biomaterials.",2024-11-08,"Zhenze Yang, Sarah K. Yorke, Tuomas P. J. Knowles, Markus J. Buehler",http://arxiv.org/pdf/2411.05421v1,cs.CL
Gap-Filling Prompting Enhances Code-Assisted Mathematical Reasoning,"Despite the strong performance of large language models (LLMs) in tasks like
mathematical reasoning, their practical use is limited by high computational
demands and proprietary restrictions. Chain-of-thought (CoT) and
program-of-thought (PoT) fine-tuning are common methods to transfer LLM
knowledge to small language models (SLMs). However, CoT often leads to
calculation errors in SLMs, while PoT has shown more promise. While most
PoT-based approaches focus on direct problem-to-code conversion or extracting
only the key information from questions and then providing code solution for
it, this work emphasizes filling the gaps in the question to clearly illustrate
the solution path, which can be challenging for an SLM to understand when such
information is not explicitly provided. Therefore, this paper introduces
Gap-Filling Prompting (GFP), a novel two-step prompting strategy designed to
enhance the problem-solving process for SLMs. The first step identifies these
gaps and provides hints for filling them, while the second step adds the hints
to the question to generate a final code solution. Experimental results on two
benchmark datasets demonstrate that GFP significantly improves the mathematical
reasoning abilities of SLMs.",2024-11-08,Mohammad Ghiasvand Mohammadkhani,http://arxiv.org/pdf/2411.05407v1,cs.CL
Benchmarking Distributional Alignment of Large Language Models,"Language models (LMs) are increasingly used as simulacra for people, yet
their ability to match the distribution of views of a specific demographic
group and be \textit{distributionally aligned} remains uncertain. This notion
of distributional alignment is complex, as there is significant variation in
the types of attributes that are simulated. Prior works have underexplored the
role of three critical variables -- the question domain, steering method, and
distribution expression method -- which motivates our contribution of a
benchmark explicitly addressing these dimensions. We construct a dataset
expanding beyond political values, create human baselines for this task, and
evaluate the extent to which an LM can align with a particular group's opinion
distribution to inform design choices of such simulation systems. Our analysis
reveals open problems regarding if, and how, LMs can be used to simulate
humans, and that LLMs can more accurately describe the opinion distribution
than simulate such distributions.",2024-11-08,"Nicole Meister, Carlos Guestrin, Tatsunori Hashimoto",http://arxiv.org/pdf/2411.05403v1,cs.CL
Towards Low-Resource Harmful Meme Detection with LMM Agents,"The proliferation of Internet memes in the age of social media necessitates
effective identification of harmful ones. Due to the dynamic nature of memes,
existing data-driven models may struggle in low-resource scenarios where only a
few labeled examples are available. In this paper, we propose an agency-driven
framework for low-resource harmful meme detection, employing both outward and
inward analysis with few-shot annotated samples. Inspired by the powerful
capacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first
retrieve relative memes with annotations to leverage label information as
auxiliary signals for the LMM agent. Then, we elicit knowledge-revising
behavior within the LMM agent to derive well-generalized insights into meme
harmfulness. By combining these strategies, our approach enables dialectical
reasoning over intricate and implicit harm-indicative patterns. Extensive
experiments conducted on three meme datasets demonstrate that our proposed
approach achieves superior performance than state-of-the-art methods on the
low-resource harmful meme detection task.",2024-11-08,"Jianzhao Huang, Hongzhan Lin, Ziyan Liu, Ziyang Luo, Guang Chen, Jing Ma",http://arxiv.org/pdf/2411.05383v1,cs.CL
When are 1.58 bits enough? A Bottom-up Exploration of BitNet Quantization,"Contemporary machine learning models, such as language models, are powerful,
but come with immense resource requirements both at training and inference
time. It has been shown that decoder-only language models can be trained to a
competitive state with ternary weights (1.58 bits per weight), facilitating
efficient inference. Here, we start our exploration with non-transformer model
architectures, investigating 1.58-bit training for multi-layer perceptrons and
graph neural networks. Then, we explore 1.58-bit training in other
transformer-based language models, namely encoder-only and encoder-decoder
models. Our results show that in all of these settings, 1.58-bit training is on
par with or sometimes even better than the standard 32/16-bit models.",2024-11-08,"Jacob Nielsen, Lukas Galke, Peter Schneider-Kamp",http://arxiv.org/pdf/2411.05882v1,cs.CL
Word reuse and combination support efficient communication of emerging concepts,"A key function of the lexicon is to express novel concepts as they emerge
over time through a process known as lexicalization. The most common
lexicalization strategies are the reuse and combination of existing words, but
they have typically been studied separately in the areas of word meaning
extension and word formation. Here we offer an information-theoretic account of
how both strategies are constrained by a fundamental tradeoff between competing
communicative pressures: word reuse tends to preserve the average length of
word forms at the cost of less precision, while word combination tends to
produce more informative words at the expense of greater word length. We test
our proposal against a large dataset of reuse items and compounds that appeared
in English, French and Finnish over the past century. We find that these
historically emerging items achieve higher levels of communicative efficiency
than hypothetical ways of constructing the lexicon, and both literal reuse
items and compounds tend to be more efficient than their non-literal
counterparts. These results suggest that reuse and combination are both
consistent with a unified account of lexicalization grounded in the theory of
efficient communication.",2024-11-08,"Aotao Xu, Charles Kemp, Lea Frermann, Yang Xu",http://arxiv.org/pdf/2411.05379v1,cs.CL
Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking,"Current automated fact-checking (AFC) approaches commonly evaluate evidence
either implicitly via the predicted verdicts or by comparing retrieved evidence
with a predefined closed knowledge source, such as Wikipedia. However, these
methods suffer from limitations, resulting from their reliance on evaluation
metrics developed for different purposes and constraints imposed by closed
knowledge sources. Recent advances in natural language generation (NLG)
evaluation offer new possibilities for evidence assessment. In this work, we
introduce Ev2R, an evaluation framework for AFC that comprises three types of
approaches for evidence evaluation: reference-based, proxy-reference, and
reference-less. We evaluate their effectiveness through agreement with human
ratings and adversarial tests, and demonstrate that prompt-based scorers,
particularly those leveraging LLMs and reference evidence, outperform
traditional evaluation approaches.",2024-11-08,"Mubashara Akhtar, Michael Schlichtkrull, Andreas Vlachos",http://arxiv.org/pdf/2411.05375v1,cs.CL
Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks,"Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized
human-machine interactions by seamlessly integrating various forms of data.
Developing a universal spoken language model that comprehends a wide range of
natural language instructions is critical for bridging communication gaps and
facilitating more intuitive interactions. However, the absence of a
comprehensive evaluation benchmark poses a significant challenge. We present
Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive
evaluation of instruction-based universal speech models. Building upon the
first generation, this second version incorporates 125 new tasks contributed
collaboratively by the global research community, expanding the benchmark to a
total of 180 tasks, making it the largest benchmark for speech and audio
evaluation. While the first generation of Dynamic-SUPERB was limited to
classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation
capabilities by introducing a wide array of novel and diverse tasks, including
regression and sequence generation, across speech, music, and environmental
audio. Evaluation results indicate that none of the models performed well
universally. SALMONN-13B excelled in English ASR, while WavLLM demonstrated
high accuracy in emotion recognition, but current models still require further
innovations to handle a broader range of tasks. We will soon open-source all
task data and the evaluation pipeline.",2024-11-08,"Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, Xuanjun Chen, Chi-Yuan Hsiao, Puyuan Peng, Shih-Heng Wang, Chun-Yi Kuan, Ke-Han Lu, Kai-Wei Chang, Chih-Kai Yang, Fabian Ritter-Gutierrez, Ming To Chuang, Kuan-Po Huang, Siddhant Arora, You-Kuan Lin, Eunjung Yeo, Kalvin Chang, Chung-Ming Chien, Kwanghee Choi, Cheng-Hsiu Hsieh, Yi-Cheng Lin, Chee-En Yu, I-Hsiang Chiu, Heitor R. Guimarães, Jionghao Han, Tzu-Quan Lin, Tzu-Yuan Lin, Homu Chang, Ting-Wu Chang, Chun Wei Chen, Shou-Jen Chen, Yu-Hua Chen, Hsi-Chun Cheng, Kunal Dhawan, Jia-Lin Fang, Shi-Xin Fang, Kuan-Yu Fang Chiang, Chi An Fu, Hsien-Fu Hsiao, Ching Yu Hsu, Shao-Syuan Huang, Lee Chen Wei, Hsi-Che Lin, Hsuan-Hao Lin, Hsuan-Ting Lin, Jian-Ren Lin, Ting-Chun Liu, Li-Chun Lu, Tsung-Min Pai, Ankita Pasad, Shih-Yun Shan Kuan, Suwon Shon, Yuxun Tang, Yun-Shao Tsai, Jui-Chiang Wei, Tzu-Chieh Wei, Chengxi Wu, Dien-Ruei Wu, Chao-Han Huck Yang, Chieh-Chi Yang, Jia Qi Yip, Shao-Xiang Yuan, Vahid Noroozi, Zhehuai Chen, Haibin Wu, Karen Livescu, David Harwath, Shinji Watanabe, Hung-yi Lee",http://arxiv.org/pdf/2411.05361v1,cs.CL
Reasoning Robustness of LLMs to Adversarial Typographical Errors,"Large Language Models (LLMs) have demonstrated impressive capabilities in
reasoning using Chain-of-Thought (CoT) prompting. However, CoT can be biased by
users' instruction. In this work, we study the reasoning robustness of LLMs to
typographical errors, which can naturally occur in users' queries. We design an
Adversarial Typo Attack ($\texttt{ATA}$) algorithm that iteratively samples
typos for words that are important to the query and selects the edit that is
most likely to succeed in attacking. It shows that LLMs are sensitive to
minimal adversarial typographical changes. Notably, with 1 character edit,
Mistral-7B-Instruct's accuracy drops from 43.7% to 38.6% on GSM8K, while with 8
character edits the performance further drops to 19.2%. To extend our
evaluation to larger and closed-source LLMs, we develop the $\texttt{R$^2$ATA}$
benchmark, which assesses models' $\underline{R}$easoning
$\underline{R}$obustness to $\underline{\texttt{ATA}}$. It includes adversarial
typographical questions derived from three widely used reasoning
datasets-GSM8K, BBH, and MMLU-by applying $\texttt{ATA}$ to open-source LLMs.
$\texttt{R$^2$ATA}$ demonstrates remarkable transferability and causes notable
performance drops across multiple super large and closed-source LLMs.",2024-11-08,"Esther Gan, Yiran Zhao, Liying Cheng, Yancan Mao, Anirudh Goyal, Kenji Kawaguchi, Min-Yen Kan, Michael Shieh",http://arxiv.org/pdf/2411.05345v1,cs.CL
Improving Multi-Domain Task-Oriented Dialogue System with Offline Reinforcement Learning,"Task-oriented dialogue (TOD) system is designed to accomplish user-defined
tasks through dialogues. The TOD system has progressed towards end-to-end
modeling by leveraging pre-trained large language models. Fine-tuning the
pre-trained language models using only supervised learning leads to the
exposure bias and token loss problem and it deviates the models from completing
the user's task. To address these issues, we propose a TOD system that
leverages a unified pre-trained language model, GPT2, as a base model. It is
optimized using supervised learning and reinforcement learning (RL). The issues
in the TOD system are mitigated using a non-differentiable reward function. The
reward is calculated using the weighted sum of the success rate and BLEU
evaluation metrics. The success rate and BLEU metrics in reward calculation
guide the language model for user task completion while ensuring a coherent and
fluent response. Our model is acquired by fine-tuning a pre-trained model on
the dialogue-session level which comprises user utterance, belief state, system
act, and system response. Experimental results on MultiWOZ2.1 demonstrate that
our model increases the inform rate by 1.60% and the success rate by 3.17%
compared to the baseline.",2024-11-08,"Dharmendra Prajapat, Durga Toshniwal",http://arxiv.org/pdf/2411.05340v1,cs.CL
SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers,"Scientific literature is typically dense, requiring significant background
knowledge and deep comprehension for effective engagement. We introduce SciDQA,
a new dataset for reading comprehension that challenges LLMs for a deep
understanding of scientific articles, consisting of 2,937 QA pairs. Unlike
other scientific QA datasets, SciDQA sources questions from peer reviews by
domain experts and answers by paper authors, ensuring a thorough examination of
the literature. We enhance the dataset's quality through a process that
carefully filters out lower quality questions, decontextualizes the content,
tracks the source document across different versions, and incorporates a
bibliography for multi-document question-answering. Questions in SciDQA
necessitate reasoning across figures, tables, equations, appendices, and
supplementary materials, and require multi-document reasoning. We evaluate
several open-source and proprietary LLMs across various configurations to
explore their capabilities in generating relevant and factual responses. Our
comprehensive evaluation, based on metrics for surface-level similarity and LLM
judgements, highlights notable performance discrepancies. SciDQA represents a
rigorously curated, naturally derived scientific QA dataset, designed to
facilitate research on complex scientific text understanding.",2024-11-08,"Shruti Singh, Nandan Sarkar, Arman Cohan",http://arxiv.org/pdf/2411.05338v1,cs.CL
SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding,"Large Language Models (LLMs) have become essential in advancing natural
language processing (NLP) tasks, but their sequential token generation limits
inference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising
solution by using a smaller draft model to generate multiple token sequences,
which the target LLM verifies in parallel. However, current heuristic
approaches, such as Recursive Rejection Sampling (RRS), suffer from low
acceptance rates in subsequent drafts, limiting the advantages of using
multiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can
theoretically improve acceptance rates, but its computational cost is too high
for real-time use. We present SpecHub, a novel, efficient sampling-verification
method for MDSD that improves acceptance rates with only linear computational
overhead. By simplifying the OTM problem into a compact Linear Programming
model, SpecHub significantly reduces computational complexity. It further
accelerates sampling by leveraging a sparse joint distribution, focusing
computation on high-probability token sequences. In extensive experiments,
Spechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step
than RRS and RRS without replacement. We attach our code at
\url{https://github.com/MasterGodzilla/Speculative_decoding_OT}.",2024-11-08,"Ryan Sun, Tianyi Zhou, Xun Chen, Lichao Sun",http://arxiv.org/pdf/2411.05289v1,cs.CL
Fox-1: Open Small Language Model for Cloud and Edge,"We present Fox-1, a series of small language models (SLMs) consisting of
Fox-1-1.6B and Fox-1-1.6B-Instruct-v0.1. These models are pre-trained on 3
trillion tokens of web-scraped document data and fine-tuned with 5 billion
tokens of instruction-following and multi-turn conversation data. Aiming to
improve the pre-training efficiency, Fox-1-1.6B model introduces a novel
3-stage data curriculum across all the training data with 2K-8K sequence
length. In architecture design, Fox-1 features a deeper layer structure, an
expanded vocabulary, and utilizes Grouped Query Attention (GQA), offering a
performant and efficient architecture compared to other SLMs. Fox-1 achieves
better or on-par performance in various benchmarks compared to StableLM-2-1.6B,
Gemma-2B, Qwen1.5-1.8B, and OpenELM1.1B, with competitive inference speed and
throughput. The model weights have been released under the Apache 2.0 license,
where we aim to promote the democratization of LLMs and make them fully
accessible to the whole open-source community.",2024-11-08,"Zijian Hu, Jipeng Zhang, Rui Pan, Zhaozhuo Xu, Shanshan Han, Han Jin, Alay Dilipbhai Shah, Dimitris Stripelis, Yuhang Yao, Salman Avestimehr, Tong Zhang, Chaoyang He",http://arxiv.org/pdf/2411.05281v3,cs.CL
Revisiting the Robustness of Watermarking to Paraphrasing Attacks,"Amidst rising concerns about the internet being proliferated with content
generated from language models (LMs), watermarking is seen as a principled way
to certify whether text was generated from a model. Many recent watermarking
techniques slightly modify the output probabilities of LMs to embed a signal in
the generated output that can later be detected. Since early proposals for text
watermarking, questions about their robustness to paraphrasing have been
prominently discussed. Lately, some techniques are deliberately designed and
claimed to be robust to paraphrasing. However, such watermarking schemes do not
adequately account for the ease with which they can be reverse-engineered. We
show that with access to only a limited number of generations from a black-box
watermarked model, we can drastically increase the effectiveness of
paraphrasing attacks to evade watermark detection, thereby rendering the
watermark ineffective.",2024-11-08,"Saksham Rastogi, Danish Pruthi",http://arxiv.org/pdf/2411.05277v1,cs.CL
Seeing Through the Fog: A Cost-Effectiveness Analysis of Hallucination Detection Systems,"This paper presents a comparative analysis of hallucination detection systems
for AI, focusing on automatic summarization and question answering tasks for
Large Language Models (LLMs). We evaluate different hallucination detection
systems using the diagnostic odds ratio (DOR) and cost-effectiveness metrics.
Our results indicate that although advanced models can perform better they come
at a much higher cost. We also demonstrate how an ideal hallucination detection
system needs to maintain performance across different model sizes. Our findings
highlight the importance of choosing a detection system aligned with specific
application needs and resource constraints. Future research will explore hybrid
systems and automated identification of underperforming components to enhance
AI reliability and efficiency in detecting and mitigating hallucinations.",2024-11-08,"Alexander Thomas, Seth Rosen, Vishnu Vettrivel",http://arxiv.org/pdf/2411.05270v1,cs.CL
Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation,"Despite significant advancements in automated report generation, the
opaqueness of text interpretability continues to cast doubt on the reliability
of the content produced. This paper introduces a novel approach to identify
specific image features in X-ray images that influence the outputs of report
generation models. Specifically, we propose Cyclic Vision-Language Manipulator
CVLM, a module to generate a manipulated X-ray from an original X-ray and its
report from a designated report generator. The essence of CVLM is that cycling
manipulated X-rays to the report generator produces altered reports aligned
with the alterations pre-injected into the reports for X-ray generation,
achieving the term ""cyclic manipulation"". This process allows direct comparison
between original and manipulated X-rays, clarifying the critical image features
driving changes in reports and enabling model users to assess the reliability
of the generated texts. Empirical evaluations demonstrate that CVLM can
identify more precise and reliable features compared to existing explanation
methods, significantly enhancing the transparency and applicability of
AI-generated reports.",2024-11-08,"Yingying Fang, Zihao Jin, Shaojie Guo, Jinda Liu, Zhiling Yue, Yijian Gao, Junzhi Ning, Zhi Li, Simon Walsh, Guang Yang",http://arxiv.org/pdf/2411.05261v2,cs.CL
What talking you?: Translating Code-Mixed Messaging Texts to English,"Translation of code-mixed texts to formal English allow a wider audience to
understand these code-mixed languages, and facilitate downstream analysis
applications such as sentiment analysis. In this work, we look at translating
Singlish, which is colloquial Singaporean English, to formal standard English.
Singlish is formed through the code-mixing of multiple Asian languages and
dialects. We analysed the presence of other Asian languages and variants which
can facilitate translation. Our dataset is short message texts, written as
informal communication between Singlish speakers. We use a multi-step prompting
scheme on five Large Language Models (LLMs) for language detection and
translation. Our analysis show that LLMs do not perform well in this task, and
we describe the challenges involved in translation of code-mixed languages. We
also release our dataset in this link https://github.com/luoqichan/singlish.",2024-11-08,"Lynnette Hui Xian Ng, Luo Qi Chan",http://arxiv.org/pdf/2411.05253v1,cs.CL
Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass,"Large language models (LMs) are typically adapted to improve performance on
new contexts (\eg text prompts that define new tasks or domains) through
fine-tuning or prompting. However, there is an accuracy compute tradeoff --
fine-tuning incurs significant training cost and prompting increases inference
overhead. We introduce $GenerativeAdapter$, an effective and efficient
adaptation method that directly maps new contexts to low-rank LM adapters,
thereby significantly reducing inference overhead with no need for finetuning.
The adapter generator is trained via self-supervised learning, and can be used
to adapt a single frozen LM for any new task simply by mapping the associated
task or domain context to a new adapter. We apply $GenerativeAdapter$ to two
pretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the
adapted models in three adaption scenarios: knowledge acquisition from
documents, learning from demonstrations, and personalization for users. In
StreamingQA, our approach is effective in injecting knowledge into the LM's
parameters, achieving a 63.5% improvement in F1 score over the model with
supervised fine-tuning (from $19.5$ to $31.5$) for contexts as long as 32K
tokens. In the MetaICL in-context learning evaluation, our method achieves an
average accuracy of $44.9$ across 26 tasks, outperforming the base model. On
MSC, our method proves to be highly competitive in memorizing user information
from conversations with a 4x reduction in computation and memory costs compared
to prompting with full conversation history. Together, these results suggest
that $GenerativeAdapter$ should allow for general adaption to a wide range of
different contexts.",2024-11-08,"Tong Chen, Hao Fang, Patrick Xia, Xiaodong Liu, Benjamin Van Durme, Luke Zettlemoyer, Jianfeng Gao, Hao Cheng",http://arxiv.org/pdf/2411.05877v1,cs.CL
Towards Improved Preference Optimization Pipeline: from Data Generation to Budget-Controlled Regularization,"Direct Preference Optimization (DPO) and its variants have become the de
facto standards for aligning large language models (LLMs) with human
preferences or specific goals. However, DPO requires high-quality preference
data and suffers from unstable preference optimization. In this work, we aim to
improve the preference optimization pipeline by taking a closer look at
preference data generation and training regularization techniques. For
preference data generation, we demonstrate that existing scoring-based reward
models produce unsatisfactory preference data and perform poorly on
out-of-distribution tasks. This significantly impacts the LLM alignment
performance when using these data for preference tuning. To ensure high-quality
preference data generation, we propose an iterative pairwise ranking mechanism
that derives preference ranking of completions using pairwise comparison
signals. For training regularization, we observe that preference optimization
tends to achieve better convergence when the LLM predicted likelihood of
preferred samples gets slightly reduced. However, the widely used supervised
next-word prediction regularization strictly prevents any likelihood reduction
of preferred samples. This observation motivates our design of a
budget-controlled regularization formulation. Empirically we show that
combining the two designs leads to aligned models that surpass existing SOTA
across two popular benchmarks.",2024-11-07,"Zhuotong Chen, Fang Liu, Jennifer Zhu, Wanyu Du, Yanjun Qi",http://arxiv.org/pdf/2411.05875v1,cs.CL
Abstract2Appendix: Academic Reviews Enhance LLM Long-Context Capabilities,"Large language models (LLMs) have shown remarkable performance across various
tasks, yet their ability to handle long-context reading remains challenging.
This study explores the effectiveness of leveraging high-quality academic peer
review data for fine-tuning LLMs to enhance their long-context capabilities. We
compare the Direct Preference Optimization (DPO) method with the Supervised
Fine-Tuning (SFT) method, demonstrating DPO's superiority and data efficiency.
Our experiments show that the fine-tuned model achieves a 4.04-point
improvement over phi-3 and a 2.6\% increase on the Qasper benchmark using only
2000 samples. Despite facing limitations in data scale and processing costs,
this study underscores the potential of DPO and high-quality data in advancing
LLM performance.
  Additionally, the zero-shot benchmark results indicate that aggregated
high-quality human reviews are overwhelmingly preferred over LLM-generated
responses, even for the most capable models like GPT-4o. This suggests that
high-quality human reviews are extremely rich in information, reasoning, and
long-context retrieval, capabilities that even the most advanced models have
not fully captured. These findings highlight the high utility of leveraging
human reviews to further advance the field.",2024-11-07,"Shengzhi Li, Kittipat Kampa, Rongyu Lin, Bohang Li, Shichao Pei",http://arxiv.org/pdf/2411.05232v1,cs.CL
Evaluating GPT-4 at Grading Handwritten Solutions in Math Exams,"Recent advances in generative artificial intelligence (AI) have shown promise
in accurately grading open-ended student responses. However, few prior works
have explored grading handwritten responses due to a lack of data and the
challenge of combining visual and textual information. In this work, we
leverage state-of-the-art multi-modal AI models, in particular GPT-4o, to
automatically grade handwritten responses to college-level math exams. Using
real student responses to questions in a probability theory exam, we evaluate
GPT-4o's alignment with ground-truth scores from human graders using various
prompting techniques. We find that while providing rubrics improves alignment,
the model's overall accuracy is still too low for real-world settings, showing
there is significant room for growth in this task.",2024-11-07,"Adriana Caraeni, Alexander Scarlatos, Andrew Lan",http://arxiv.org/pdf/2411.05231v2,cs.CL
CHATTER: A Character Attribution Dataset for Narrative Understanding,"Computational narrative understanding studies the identification,
description, and interaction of the elements of a narrative: characters,
attributes, events, and relations. Narrative research has given considerable
attention to defining and classifying character types. However, these
character-type taxonomies do not generalize well because they are small, too
simple, or specific to a domain. We require robust and reliable benchmarks to
test whether narrative models truly understand the nuances of the character's
development in the story. Our work addresses this by curating the CHATTER
dataset that labels whether a character portrays some attribute for 88124
character-attribute pairs, encompassing 2998 characters, 12967 attributes and
660 movies. We validate a subset of CHATTER, called CHATTEREVAL, using human
annotations to serve as a benchmark to evaluate the character attribution task
in movie scripts. \evaldataset{} also assesses narrative understanding and the
long-context modeling capacity of language models.",2024-11-07,"Sabyasachee Baruah, Shrikanth Narayanan",http://arxiv.org/pdf/2411.05227v2,cs.CL
Beyond the Numbers: Transparency in Relation Extraction Benchmark Creation and Leaderboards,"This paper investigates the transparency in the creation of benchmarks and
the use of leaderboards for measuring progress in NLP, with a focus on the
relation extraction (RE) task. Existing RE benchmarks often suffer from
insufficient documentation, lacking crucial details such as data sources,
inter-annotator agreement, the algorithms used for the selection of instances
for datasets, and information on potential biases like dataset imbalance.
Progress in RE is frequently measured by leaderboards that rank systems based
on evaluation methods, typically limited to aggregate metrics like F1-score.
However, the absence of detailed performance analysis beyond these metrics can
obscure the true generalisation capabilities of models. Our analysis reveals
that widely used RE benchmarks, such as TACRED and NYT, tend to be highly
imbalanced and contain noisy labels. Moreover, the lack of class-based
performance metrics fails to accurately reflect model performance across
datasets with a large number of relation types. These limitations should be
carefully considered when reporting progress in RE. While our discussion
centers on the transparency of RE benchmarks and leaderboards, the observations
we discuss are broadly applicable to other NLP tasks as well. Rather than
undermining the significance and value of existing RE benchmarks and the
development of new models, this paper advocates for improved documentation and
more rigorous evaluation to advance the field.",2024-11-07,"Varvara Arzt, Allan Hanbury",http://arxiv.org/pdf/2411.05224v1,cs.CL
Dialectal Coverage And Generalization in Arabic Speech Recognition,"Developing robust automatic speech recognition (ASR) systems for Arabic, a
language characterized by its rich dialectal diversity and often considered a
low-resource language in speech technology, demands effective strategies to
manage its complexity. This study explores three critical factors influencing
ASR performance: the role of dialectal coverage in pre-training, the
effectiveness of dialect-specific fine-tuning compared to a multi-dialectal
approach, and the ability to generalize to unseen dialects. Through extensive
experiments across different dialect combinations, our findings offer key
insights towards advancing the development of ASR systems for pluricentric
languages like Arabic.",2024-11-07,"Amirbek Djanibekov, Hawau Olamide Toyin, Raghad Alshalan, Abdullah Alitr, Hanan Aldarmaki",http://arxiv.org/pdf/2411.05872v2,cs.CL
STAND-Guard: A Small Task-Adaptive Content Moderation Model,"Content moderation, the process of reviewing and monitoring the safety of
generated content, is important for development of welcoming online platforms
and responsible large language models. Content moderation contains various
tasks, each with its unique requirements tailored to specific scenarios.
Therefore, it is crucial to develop a model that can be easily adapted to novel
or customized content moderation tasks accurately without extensive model
tuning. This paper presents STAND-GUARD, a Small Task-Adaptive coNtent
moDeration model. The basic motivation is: by performing instruct tuning on
various content moderation tasks, we can unleash the power of small language
models (SLMs) on unseen (out-of-distribution) content moderation tasks. We also
carefully study the effects of training tasks and model size on the efficacy of
cross-task fine-tuning mechanism. Experiments demonstrate STAND-Guard is
comparable to GPT-3.5-Turbo across over 40 public datasets, as well as
proprietary datasets derived from real-world business scenarios. Remarkably,
STAND-Guard achieved nearly equivalent results to GPT-4-Turbo on unseen English
binary classification tasks",2024-11-07,"Minjia Wang, Pingping Lin, Siqi Cai, Shengnan An, Shengjie Ma, Zeqi Lin, Congrui Huang, Bixiong Xu",http://arxiv.org/pdf/2411.05214v1,cs.CL
Alopex: A Computational Framework for Enabling On-Device Function Calls with LLMs,"The rapid advancement of Large Language Models (LLMs) has led to their
increased integration into mobile devices for personalized assistance, which
enables LLMs to call external API functions to enhance their performance.
However, challenges such as data scarcity, ineffective question formatting, and
catastrophic forgetting hinder the development of on-device LLM agents. To
tackle these issues, we propose Alopex, a framework that enables precise
on-device function calls using the Fox LLM. Alopex introduces a logic-based
method for generating high-quality training data and a novel
``description-question-output'' format for fine-tuning, reducing risks of
function information leakage. Additionally, a data mixing strategy is used to
mitigate catastrophic forgetting, combining function call data with textbook
datasets to enhance performance in various tasks. Experimental results show
that Alopex improves function call accuracy and significantly reduces
catastrophic forgetting, providing a robust solution for integrating function
call capabilities into LLMs without manual intervention.",2024-11-07,"Yide Ran, Zhaozhuo Xu, Yuhang Yao, Zijian Hu, Shanshan Han, Han Jin, Alay Dilipbhai Shah, Jipeng Zhang, Dimitris Stripelis, Tong Zhang, Salman Avestimehr, Chaoyang He",http://arxiv.org/pdf/2411.05209v1,cs.CL
Deploying Large Language Models With Retrieval Augmented Generation,"Knowing that the generative capabilities of large language models (LLM) are
sometimes hampered by tendencies to hallucinate or create non-factual
responses, researchers have increasingly focused on methods to ground generated
outputs in factual data. Retrieval Augmented Generation (RAG) has emerged as a
key approach for integrating knowledge from data sources outside of the LLM's
training set, including proprietary and up-to-date information. While many
research papers explore various RAG strategies, their true efficacy is tested
in real-world applications with actual data. The journey from conceiving an
idea to actualizing it in the real world is a lengthy process. We present
insights from the development and field-testing of a pilot project that
integrates LLMs with RAG for information retrieval. Additionally, we examine
the impacts on the information value chain, encompassing people, processes, and
technology. Our aim is to identify the opportunities and challenges of
implementing this emerging technology, particularly within the context of
behavioral research in the information systems (IS) field. The contributions of
this work include the development of best practices and recommendations for
adopting this promising technology while ensuring compliance with industry
regulations through a proposed AI governance model.",2024-11-07,"Sonal Prabhune, Donald J. Berndt",http://arxiv.org/pdf/2411.11895v1,cs.CL
Toward Cultural Interpretability: A Linguistic Anthropological Framework for Describing and Evaluating Large Language Models (LLMs),"This article proposes a new integration of linguistic anthropology and
machine learning (ML) around convergent interests in both the underpinnings of
language and making language technologies more socially responsible. While
linguistic anthropology focuses on interpreting the cultural basis for human
language use, the ML field of interpretability is concerned with uncovering the
patterns that Large Language Models (LLMs) learn from human verbal behavior.
Through the analysis of a conversation between a human user and an LLM-powered
chatbot, we demonstrate the theoretical feasibility of a new, conjoint field of
inquiry, cultural interpretability (CI). By focusing attention on the
communicative competence involved in the way human users and AI chatbots
co-produce meaning in the articulatory interface of human-computer interaction,
CI emphasizes how the dynamic relationship between language and culture makes
contextually sensitive, open-ended conversation possible. We suggest that, by
examining how LLMs internally ""represent"" relationships between language and
culture, CI can: (1) provide insight into long-standing linguistic
anthropological questions about the patterning of those relationships; and (2)
aid model developers and interface designers in improving value alignment
between language models and stylistically diverse speakers and culturally
diverse speech communities. Our discussion proposes three critical research
axes: relativity, variation, and indexicality.",2024-11-07,"Graham M. Jones, Shai Satran, Arvind Satyanarayan",http://arxiv.org/pdf/2411.05200v1,cs.CL
CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement,"Large Language Models (LLMs) have revolutionized code generation but require
significant resources and often over-generalize, limiting their task-specific
efficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective
alternative. However, standard supervised approaches rely only on correct
examples, missing valuable insights from failures. We introduce CodeLutra, a
framework that leverages both correct and incorrect code attempts. Instead of
using only correct solutions, CodeLutra applies iterative preference-based
refinement, comparing successful and failed outputs to better approximate
desired results. This approach narrows the performance gap with
state-of-the-art larger models without requiring massive datasets or auxiliary
models. For instance, on a challenging data science coding task, using only 500
samples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's
level. By learning from both successes and mistakes, CodeLutra provides a
scalable and efficient path to high-quality code generation, making smaller
open-source models more competitive with leading closed-source alternatives.",2024-11-07,"Leitian Tao, Xiang Chen, Tong Yu, Tung Mai, Ryan Rossi, Yixuan Li, Saayan Mitra",http://arxiv.org/pdf/2411.05199v2,cs.CL
Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder,"Recent research has shown that CLIP models struggle with visual reasoning
tasks that require grounding compositionality, understanding spatial
relationships, or capturing fine-grained details. One natural hypothesis is
that the CLIP vision encoder does not embed essential information for these
tasks. However, we find that this is not always the case: The encoder gathers
query-relevant visual information, while CLIP fails to extract it. In
particular, we show that another branch of Vision-Language Models (VLMs),
Generative Multimodal Large Language Models (MLLMs), achieve significantly
higher accuracy than CLIP in many of these tasks using the same vision encoder
and weights, indicating that these Generative MLLMs perceive more -- as they
extract and utilize visual information more effectively. We conduct a series of
controlled experiments and reveal that their success is attributed to multiple
key design choices, including patch tokens, position embeddings, and
prompt-based weighting. On the other hand, enhancing the training data alone or
applying a stronger text encoder does not suffice to solve the task, and
additional text tokens offer little benefit. Interestingly, we find that
fine-grained visual reasoning is not exclusive to generative models trained by
an autoregressive loss: When converted into CLIP-like encoders by contrastive
finetuning, these MLLMs still outperform CLIP under the same cosine
similarity-based evaluation protocol. Our study highlights the importance of
VLM architectural choices and suggests directions for improving the performance
of CLIP-like contrastive VLMs.",2024-11-07,"Siting Li, Pang Wei Koh, Simon Shaolei Du",http://arxiv.org/pdf/2411.05195v2,cs.CL
Interactive Dialogue Agents via Reinforcement Learning on Hindsight Regenerations,"Recent progress on large language models (LLMs) has enabled dialogue agents
to generate highly naturalistic and plausible text. However, current LLM
language generation focuses on responding accurately to questions and requests
with a single effective response. In reality, many real dialogues are
interactive, meaning an agent's utterances will influence their conversational
partner, elicit information, or change their opinion. Accounting for how an
agent can effectively steer a conversation is a crucial ability in many
dialogue tasks, from healthcare to preference elicitation. Existing methods for
fine-tuning dialogue agents to accomplish such tasks would rely on curating
some amount of expert data. However, doing so often requires understanding the
underlying cognitive processes of the conversational partner, which is a skill
neither humans nor LLMs trained on human data can reliably do. Our key insight
is that while LLMs may not be adept at identifying effective strategies for
steering conversations a priori, or in the middle of an ongoing conversation,
they can do so post-hoc, or in hindsight, after seeing how their conversational
partner responds. We use this fact to rewrite and augment existing suboptimal
data, and train via offline reinforcement learning (RL) an agent that
outperforms both prompting and learning from unaltered human demonstrations. We
apply our approach to two domains that require understanding human mental
state, intelligent interaction, and persuasion: mental health support, and
soliciting charitable donations. Our results in a user study with real humans
show that our approach greatly outperforms existing state-of-the-art dialogue
agents.",2024-11-07,"Joey Hong, Jessica Lin, Anca Dragan, Sergey Levine",http://arxiv.org/pdf/2411.05194v1,cs.CL
Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning,"Value-based reinforcement learning (RL) can in principle learn effective
policies for a wide range of multi-turn problems, from games to dialogue to
robotic control, including via offline RL from static previously collected
datasets. However, despite the widespread use of policy gradient methods to
train large language models for single turn tasks (e.g., question answering),
value-based methods for multi-turn RL in an off-policy or offline setting have
proven particularly challenging to scale to the setting of large language
models. This setting requires effectively leveraging pretraining, scaling to
large architectures with billions of parameters, and training on large
datasets, all of which represent major challenges for current value-based RL
methods. In this work, we propose a novel offline RL algorithm that addresses
these drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT)
problem where the probabilities of tokens directly translate to Q-values. In
this way we obtain an algorithm that smoothly transitions from maximizing the
likelihood of the data during pretraining to learning a near-optimal Q-function
during finetuning. Our algorithm has strong theoretical foundations, enjoying
performance bounds similar to state-of-the-art Q-learning methods, while in
practice utilizing an objective that closely resembles SFT. Because of this,
our approach can enjoy the full benefits of the pretraining of language models,
without the need to reinitialize any weights before RL finetuning, and without
the need to initialize new heads for predicting values or advantages.
Empirically, we evaluate our method on both pretrained LLMs and VLMs, on a
variety of tasks including both natural language dialogue and robotic
manipulation and navigation from images.",2024-11-07,"Joey Hong, Anca Dragan, Sergey Levine",http://arxiv.org/pdf/2411.05193v2,cs.CL
Explaining Mixtures of Sources in News Articles,"Human writers plan, then write. For large language models (LLMs) to play a
role in longer-form article generation, we must understand the planning steps
humans make before writing. We explore one kind of planning, source-selection
in news, as a case-study for evaluating plans in long-form generation. We ask:
why do specific stories call for specific kinds of sources? We imagine a
generative process for story writing where a source-selection schema is first
selected by a journalist, and then sources are chosen based on categories in
that schema. Learning the article's plan means predicting the schema initially
chosen by the journalist. Working with professional journalists, we adapt five
existing schemata and introduce three new ones to describe journalistic plans
for the inclusion of sources in documents. Then, inspired by Bayesian
latent-variable modeling, we develop metrics to select the most likely plan, or
schema, underlying a story, which we use to compare schemata. We find that two
schemata: stance and social affiliation best explain source plans in most
documents. However, other schemata like textual entailment explain source plans
in factually rich topics like ""Science"". Finally, we find we can predict the
most suitable schema given just the article's headline with reasonable
accuracy. We see this as an important case-study for human planning, and
provides a framework and approach for evaluating other kinds of plans. We
release a corpora, NewsSources, with annotations for 4M articles.",2024-11-07,"Alexander Spangher, James Youn, Matt DeButts, Nanyun Peng, Emilio Ferrara, Jonathan May",http://arxiv.org/pdf/2411.05192v1,cs.CL
ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Sentence,"Handling implicit language is essential for natural language processing
systems to achieve precise text understanding and facilitate natural
interactions with users. Despite its importance, the absence of a metric for
accurately measuring the implicitness of language significantly constrains the
depth of analysis possible in evaluating models' comprehension capabilities.
This paper addresses this gap by developing a scalar metric that quantifies the
implicitness level of language without relying on external references. Drawing
on principles from traditional linguistics, we define ""implicitness"" as the
divergence between semantic meaning and pragmatic interpretation. To
operationalize this definition, we introduce ImpScore, a reference-free metric
formulated through an interpretable regression model. This model is trained
using pairwise contrastive learning on a specially curated dataset consisting
of (implicit sentence, explicit sentence) pairs. We validate ImpScore through a
user study that compares its assessments with human evaluations on
out-of-distribution data, demonstrating its accuracy and strong correlation
with human judgments. Additionally, we apply ImpScore to hate speech detection
datasets, illustrating its utility and highlighting significant limitations in
current large language models' ability to understand highly implicit content.
Our metric is publicly available at https://github.com/audreycs/ImpScore.",2024-11-07,"Yuxin Wang, Xiaomeng Zhu, Weimin Lyu, Saeed Hassanpour, Soroush Vosoughi",http://arxiv.org/pdf/2411.05172v3,cs.CL
Watermarking Language Models through Language Models,"This paper presents a novel framework for watermarking language models
through prompts generated by language models. The proposed approach utilizes a
multi-model setup, incorporating a Prompting language model to generate
watermarking instructions, a Marking language model to embed watermarks within
generated content, and a Detecting language model to verify the presence of
these watermarks. Experiments are conducted using ChatGPT and Mistral as the
Prompting and Marking language models, with detection accuracy evaluated using
a pretrained classifier model. Results demonstrate that the proposed framework
achieves high classification accuracy across various configurations, with 95%
accuracy for ChatGPT, 88.79% for Mistral. These findings validate the and
adaptability of the proposed watermarking strategy across different language
model architectures. Hence the proposed framework holds promise for
applications in content attribution, copyright protection, and model
authentication.",2024-11-07,"Xin Zhong, Agnibh Dasgupta, Abdullah Tanvir",http://arxiv.org/pdf/2411.05091v1,cs.CL
Findings of the IWSLT 2024 Evaluation Campaign,"This paper reports on the shared tasks organized by the 21st IWSLT
Conference. The shared tasks address 7 scientific challenges in spoken language
translation: simultaneous and offline translation, automatic subtitling and
dubbing, speech-to-speech translation, dialect and low-resource speech
translation, and Indic languages. The shared tasks attracted 18 teams whose
submissions are documented in 26 system papers. The growing interest towards
spoken language translation is also witnessed by the constantly increasing
number of shared task organizers and contributors to the overview paper, almost
evenly distributed across industry and academia.",2024-11-07,"Ibrahim Said Ahmad, Antonios Anastasopoulos, Ondřej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, William Chen, Qianqian Dong, Marcello Federico, Barry Haddow, Dávid Javorský, Mateusz Krubiński, Tsz Kin Lam, Xutai Ma, Prashant Mathur, Evgeny Matusov, Chandresh Maurya, John McCrae, Kenton Murray, Satoshi Nakamura, Matteo Negri, Jan Niehues, Xing Niu, Atul Kr. Ojha, John Ortega, Sara Papi, Peter Polák, Adam Pospíšil, Pavel Pecina, Elizabeth Salesky, Nivedita Sethiya, Balaram Sarkar, Jiatong Shi, Claytone Sikasote, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Brian Thompson, Marco Turchi, Alex Waibel, Shinji Watanabe, Patrick Wilken, Petr Zemánek, Rodolfo Zevallos",http://arxiv.org/pdf/2411.05088v1,cs.CL
PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation,"Radiology report generation (RRG) aims to create free-text radiology reports
from clinical imaging. Grounded radiology report generation (GRRG) extends RRG
by including the localisation of individual findings on the image. Currently,
there are no manually annotated chest X-ray (CXR) datasets to train GRRG
models. In this work, we present a dataset called PadChest-GR
(Grounded-Reporting) derived from PadChest aimed at training GRRG models for
CXR images. We curate a public bi-lingual dataset of 4,555 CXR studies with
grounded reports (3,099 abnormal and 1,456 normal), each containing complete
lists of sentences describing individual present (positive) and absent
(negative) findings in English and Spanish. In total, PadChest-GR contains
7,037 positive and 3,422 negative finding sentences. Every positive finding
sentence is associated with up to two independent sets of bounding boxes
labelled by different readers and has categorical labels for finding type,
locations, and progression. To the best of our knowledge, PadChest-GR is the
first manually curated dataset designed to train GRRG models for understanding
and interpreting radiological images and generated text. By including detailed
localization and comprehensive annotations of all clinically relevant findings,
it provides a valuable resource for developing and evaluating GRRG models from
CXR images. PadChest-GR can be downloaded under request from
https://bimcv.cipf.es/bimcv-projects/padchest-gr/",2024-11-07,"Daniel C. Castro, Aurelia Bustos, Shruthi Bannur, Stephanie L. Hyland, Kenza Bouzid, Maria Teodora Wetscherek, Maria Dolores Sánchez-Valverde, Lara Jaques-Pérez, Lourdes Pérez-Rodríguez, Kenji Takeda, José María Salinas, Javier Alvarez-Valle, Joaquín Galant Herrero, Antonio Pertusa",http://arxiv.org/pdf/2411.05085v1,cs.CL
Precision or Recall? An Analysis of Image Captions for Training Text-to-Image Generation Model,"Despite advancements in text-to-image models, generating images that
precisely align with textual descriptions remains challenging due to
misalignment in training data. In this paper, we analyze the critical role of
caption precision and recall in text-to-image model training. Our analysis of
human-annotated captions shows that both precision and recall are important for
text-image alignment, but precision has a more significant impact. Leveraging
these insights, we utilize Large Vision Language Models to generate synthetic
captions for training. Models trained with these synthetic captions show
similar behavior to those trained on human-annotated captions, underscores the
potential for synthetic data in text-to-image training.",2024-11-07,"Sheng Cheng, Maitreya Patel, Yezhou Yang",http://arxiv.org/pdf/2411.05079v1,cs.CL
Analyzing The Language of Visual Tokens,"With the introduction of transformer-based models for vision and language
tasks, such as LLaVA and Chameleon, there has been renewed interest in the
discrete tokenized representation of images. These models often treat image
patches as discrete tokens, analogous to words in natural language, learning
joint alignments between visual and human languages. However, little is known
about the statistical behavior of these visual languages - whether they follow
similar frequency distributions, grammatical structures, or topologies as
natural languages. In this paper, we take a natural-language-centric approach
to analyzing discrete visual languages and uncover striking similarities and
fundamental differences. We demonstrate that, although visual languages adhere
to Zipfian distributions, higher token innovation drives greater entropy and
lower compression, with tokens predominantly representing object parts,
indicating intermediate granularity. We also show that visual languages lack
cohesive grammatical structures, leading to higher perplexity and weaker
hierarchical organization compared to natural languages. Finally, we
demonstrate that, while vision models align more closely with natural languages
than other models, this alignment remains significantly weaker than the
cohesion found within natural languages. Through these experiments, we
demonstrate how understanding the statistical properties of discrete visual
languages can inform the design of more effective computer vision models.",2024-11-07,"David M. Chan, Rodolfo Corona, Joonyong Park, Cheol Jun Cho, Yutong Bai, Trevor Darrell",http://arxiv.org/pdf/2411.05001v1,cs.CL
Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?,"As the context limits of Large Language Models (LLMs) increase, the range of
possible applications and downstream functions broadens. In many real-world
tasks, decisions depend on details scattered across collections of often
disparate documents containing mostly irrelevant information. Long-context LLMs
appear well-suited to this form of complex information retrieval and reasoning,
which has traditionally proven costly and time-consuming. However, although the
development of longer context models has seen rapid gains in recent years, our
understanding of how effectively LLMs use their context has not kept pace. To
address this, we conduct a set of retrieval experiments designed to evaluate
the capabilities of 17 leading LLMs, such as their ability to follow threads of
information through the context window. Strikingly, we find that many models
are remarkably threadsafe: capable of simultaneously following multiple threads
without significant loss in performance. Still, for many models, we find the
effective context limit is significantly shorter than the supported context
length, with accuracy decreasing as the context window grows. Our study also
highlights the important point that token counts from different tokenizers
should not be directly compared -- they often correspond to substantially
different numbers of written characters. We release our code and long-context
experimental data.",2024-11-07,"Jonathan Roberts, Kai Han, Samuel Albanie",http://arxiv.org/pdf/2411.05000v2,cs.CL
LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation,"CLIP is a foundational multimodal model that aligns image and text features
into a shared representation space via contrastive learning on large-scale
image-text pairs. Its effectiveness primarily stems from the use of natural
language as rich supervision. Motivated by the remarkable advancements in large
language models (LLMs), this work explores how LLMs' superior text
understanding and extensive open-world knowledge can enhance CLIP's capability,
especially for processing longer and more complex image captions. We propose an
efficient post-training strategy that integrates LLMs into pretrained CLIP. To
address the challenge posed by the autoregressive nature of LLMs, we introduce
a caption-to-caption contrastive fine-tuning framework, significantly enhancing
the discriminative quality of LLM outputs. Extensive experiments demonstrate
that our approach outperforms LoRA-based methods, achieving nearly fourfold
faster training with superior performance. Furthermore, we validate substantial
improvements over state-of-the-art models such as CLIP, EVA02, and SigLip2
across various zero-shot multimodal retrieval tasks, cross-lingual retrieval
tasks, and multimodal language model pretraining.",2024-11-07,"Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Chunyu Wang, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu",http://arxiv.org/pdf/2411.04997v4,cs.CL
Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models,"The development of large language models (LLMs) has expanded to multi-modal
systems capable of processing text, images, and speech within a unified
framework. Training these models demands significantly larger datasets and
computational resources compared to text-only LLMs. To address the scaling
challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal
transformer architecture that significantly reduces pretraining computational
costs. MoT decouples non-embedding parameters of the model by modality --
including feed-forward networks, attention matrices, and layer normalization --
enabling modality-specific processing with global self-attention over the full
input sequence. We evaluate MoT across multiple settings and model scales. In
the Chameleon 7B setting (autoregressive text-and-image generation), MoT
matches the dense baseline's performance using only 55.8\% of the FLOPs. When
extended to include speech, MoT reaches speech performance comparable to the
dense baseline with only 37.2\% of the FLOPs. In the Transfusion setting, where
text and image are trained with different objectives, a 7B MoT model matches
the image modality performance of the dense baseline with one third of the
FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image
generation metrics. System profiling further highlights MoT's practical
benefits, achieving dense baseline image quality in 47.2\% of the wall-clock
time and text quality in 75.6\% of the wall-clock time (measured on AWS
p4de.24xlarge instances with NVIDIA A100 GPUs).",2024-11-07,"Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, Xi Victoria Lin",http://arxiv.org/pdf/2411.04996v2,cs.CL
The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities,"Modern language models can process inputs across diverse languages and
modalities. We hypothesize that models acquire this capability through learning
a shared representation space across heterogeneous data types (e.g., different
languages and modalities), which places semantically similar inputs near one
another, even if they are from different modalities/languages. We term this the
semantic hub hypothesis, following the hub-and-spoke model from neuroscience
(Patterson et al., 2007) which posits that semantic knowledge in the human
brain is organized through a transmodal semantic ""hub"" which integrates
information from various modality-specific ""spokes"" regions. We first show that
model representations for semantically equivalent inputs in different languages
are similar in the intermediate layers, and that this space can be interpreted
using the model's dominant pretraining language via the logit lens. This
tendency extends to other data types, including arithmetic expressions, code,
and visual/audio inputs. Interventions in the shared representation space in
one data type also predictably affect model outputs in other data types,
suggesting that this shared representations space is not simply a vestigial
byproduct of large-scale training on broad data, but something that is actively
utilized by the model during input processing.",2024-11-07,"Zhaofeng Wu, Xinyan Velocity Yu, Dani Yogatama, Jiasen Lu, Yoon Kim",http://arxiv.org/pdf/2411.04986v3,cs.CL
SuffixDecoding: A Model-Free Approach to Speeding Up Large Language Model Inference,"We present SuffixDecoding, a novel model-free approach to accelerating large
language model (LLM) inference through speculative decoding. Unlike existing
methods that rely on draft models or specialized decoding heads, SuffixDecoding
leverages suffix trees built from previously generated outputs to efficiently
predict candidate token sequences. Our approach enables flexible
tree-structured speculation without the overhead of maintaining and
orchestrating additional models. SuffixDecoding builds and dynamically updates
suffix trees to capture patterns in the generated text, using them to construct
speculation trees through a principled scoring mechanism based on empirical
token frequencies. SuffixDecoding requires only CPU memory which is plentiful
and underutilized on typical LLM serving nodes. We demonstrate that
SuffixDecoding achieves competitive speedups compared to model-based approaches
across diverse workloads including open-domain chat, code generation, and
text-to-SQL tasks. For open-ended chat and code generation tasks,
SuffixDecoding achieves up to $1.4\times$ higher output throughput than
SpecInfer and up to $1.1\times$ lower time-per-token (TPOT) latency. For a
proprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to
$2.9\times$ higher output throughput and $3\times$ lower latency than
speculative decoding. Our evaluation shows that SuffixDecoding maintains high
acceptance rates even with small reference corpora of 256 examples, while
continuing to improve performance as more historical outputs are incorporated.",2024-11-07,"Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao",http://arxiv.org/pdf/2411.04975v1,cs.CL
A Guide to Misinformation Detection Data and Evaluation,"Misinformation is a complex societal issue, and mitigating solutions are
difficult to create due to data deficiencies. To address this, we have curated
the largest collection of (mis)information datasets in the literature, totaling
75. From these, we evaluated the quality of 36 datasets that consist of
statements or claims, as well as the 9 datasets that consist of data in purely
paragraph form. We assess these datasets to identify those with solid
foundations for empirical work and those with flaws that could result in
misleading and non-generalizable results, such as spurious correlations, or
examples that are ambiguous or otherwise impossible to assess for veracity. We
find the latter issue is particularly severe and affects most datasets in the
literature. We further provide state-of-the-art baselines on all these
datasets, but show that regardless of label quality, categorical labels may no
longer give an accurate evaluation of detection model performance. Finally, we
propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the
field toward systemic solutions rather than inadvertently propagating issues in
evaluation. Overall, this guide aims to provide a roadmap for higher quality
data and better grounded evaluations, ultimately improving research in
misinformation detection. All datasets and other artifacts are available at
misinfo-datasets.complexdatalab.com.",2024-11-07,"Camille Thibault, Jacob-Junqi Tian, Gabrielle Peloquin-Skulski, Taylor Lynn Curtis, James Zhou, Florence Laflamme, Yuxiang Guan, Reihaneh Rabbany, Jean-François Godbout, Kellin Pelrine",http://arxiv.org/pdf/2411.05060v3,cs.CL
BitNet a4.8: 4-bit Activations for 1-bit LLMs,"Recent research on the 1-bit Large Language Models (LLMs), such as BitNet
b1.58, presents a promising direction for reducing the inference cost of LLMs
while maintaining their performance. In this work, we introduce BitNet a4.8,
enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid
quantization and sparsification strategy to mitigate the quantization errors
introduced by the outlier channels. Specifically, we utilize 4-bit activations
for inputs to the attention and feed-forward network layers, while sparsifying
intermediate states followed with 8-bit quantization. Extensive experiments
demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58
with equivalent training costs, while being faster in inference with enabling
4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of
parameters and supports 3-bit KV cache, further enhancing the efficiency of
large-scale LLM deployment and inference.",2024-11-07,"Hongyu Wang, Shuming Ma, Furu Wei",http://arxiv.org/pdf/2411.04965v1,cs.CL
Position Paper On Diagnostic Uncertainty Estimation from Large Language Models: Next-Word Probability Is Not Pre-test Probability,"Large language models (LLMs) are being explored for diagnostic decision
support, yet their ability to estimate pre-test probabilities, vital for
clinical decision-making, remains limited. This study evaluates two LLMs,
Mistral-7B and Llama3-70B, using structured electronic health record data on
three diagnosis tasks. We examined three current methods of extracting LLM
probability estimations and revealed their limitations. We aim to highlight the
need for improved techniques in LLM confidence estimation.",2024-11-07,"Yanjun Gao, Skatje Myers, Shan Chen, Dmitriy Dligach, Timothy A Miller, Danielle Bitterman, Guanhua Chen, Anoop Mayampurath, Matthew Churpek, Majid Afshar",http://arxiv.org/pdf/2411.04962v1,cs.CL
M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding,"Document visual question answering (DocVQA) pipelines that answer questions
from documents have broad applications. Existing methods focus on handling
single-page documents with multi-modal language models (MLMs), or rely on
text-based retrieval-augmented generation (RAG) that uses text extraction tools
such as optical character recognition (OCR). However, there are difficulties in
applying these methods in real-world scenarios: (a) questions often require
information across different pages or documents, where MLMs cannot handle many
long documents; (b) documents often have important information in visual
elements such as figures, but text extraction tools ignore them. We introduce
M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various
document contexts (closed-domain and open-domain), question hops (single-hop
and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG
finds relevant documents and answers questions using a multi-modal retriever
and an MLM, so that it can efficiently handle single or many documents while
preserving visual information. Since previous DocVQA datasets ask questions in
the context of a specific document, we also present M3DocVQA, a new benchmark
for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.
In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results
show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance
than many strong baselines, including state-of-the-art performance in
MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and
retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully
handle various scenarios, such as when relevant information exists across
multiple pages and when answer evidence only exists in images.",2024-11-07,"Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, Mohit Bansal",http://arxiv.org/pdf/2411.04952v1,cs.CL
Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach,"We introduce a data-centric hypothesis-testing framework to quantify the
influence of sequentially correlated literary properties--such as thematic
continuity--on textual classification tasks. Our method models label sequences
as stochastic processes and uses an empirical autocovariance matrix to generate
surrogate labelings that preserve sequential dependencies. This enables
statistical testing to determine whether classification outcomes are primarily
driven by thematic structure or by non-sequential features like authorial
style. Applying this framework across a diverse corpus of English prose, we
compare traditional (word n-grams and character k-mers) and neural
(contrastively trained) embeddings in both supervised and unsupervised
classification settings. Crucially, our method identifies when classifications
are confounded by sequentially correlated similarity, revealing that supervised
and neural models are more prone to false positives--mistaking shared themes
and cross-genre differences for stylistic signals. In contrast, unsupervised
models using traditional features often yield high true positive rates with
minimal false positives, especially in genre-consistent settings. By
disentangling sequential from non-sequential influences, our approach provides
a principled way to assess and interpret classification reliability. This is
particularly impactful for authorship attribution, forensic linguistics, and
the analysis of redacted or composite texts, where conventional methods may
conflate theme with style. Our results demonstrate that controlling for
sequential correlation is essential for reducing false positives and ensuring
that classification outcomes reflect genuine stylistic distinctions.",2024-11-07,"Gideon Yoffe, Nachum Dershowitz, Ariel Vishne, Barak Sober",http://arxiv.org/pdf/2411.04950v4,cs.CL
FineTuneBench: How well do commercial fine-tuning APIs infuse knowledge into LLMs?,"There is great interest in fine-tuning frontier large language models (LLMs)
to inject new information and update existing knowledge. While commercial LLM
fine-tuning APIs from providers such as OpenAI and Google promise flexible
adaptation for various applications, the efficacy of fine-tuning remains
unclear. In this study, we introduce FineTuneBench, an evaluation framework and
dataset for understanding how well commercial fine-tuning APIs can successfully
learn new and updated knowledge. We analyze five frontier LLMs with
commercially available fine-tuning APIs, including GPT-4o and Gemini 1.5 Pro,
on their effectiveness in two settings: (1) ingesting novel information, such
as recent news events and new people profiles, and (2) updating existing
knowledge, such as updated medical guidelines and code frameworks. Our results
reveal substantial shortcomings in all the models' abilities to effectively
learn new information through fine-tuning, with an average generalization
accuracy of 37% across all models. When updating existing knowledge, such as
incorporating medical guideline updates, commercial fine-tuning APIs show even
more limited capability (average generalization accuracy of 19%). Overall,
fine-tuning GPT-4o mini is the most effective for infusing new knowledge and
updating knowledge, followed by GPT-3.5 Turbo and GPT-4o. The fine-tuning APIs
for Gemini 1.5 Flesh and Gemini 1.5 Pro are unable to learn new knowledge or
update existing knowledge. These findings underscore a major shortcoming in
using current commercial fine-tuning services to achieve reliable knowledge
infusion in common scenarios. We open source the FineTuneBench dataset at
https://github.com/kevinwu23/StanfordFineTuneBench.",2024-11-07,"Eric Wu, Kevin Wu, James Zou",http://arxiv.org/pdf/2411.05059v2,cs.CL
GPTKB: Comprehensively Materializing Factual LLM Knowledge,"LLMs have majorly advanced NLP and AI, and next to their ability to perform a
wide range of procedural tasks, a major success factor is their internalized
factual knowledge. Since (Petroni et al., 2019), analyzing this knowledge has
gained attention. However, most approaches investigate one question at a time
via modest-sized pre-defined samples, introducing an availability bias (Tversky
and Kahnemann, 1973) that prevents the discovery of knowledge (or beliefs) of
LLMs beyond the experimenter's predisposition.
  To address this challenge, we propose a novel methodology to comprehensively
materializing an LLM's factual knowledge through recursive querying and result
consolidation.
  As a prototype, we employ GPT-4o-mini to construct GPTKB, a large-scale
knowledge base (KB) comprising 105 million triples for over 2.9 million
entities - achieved at 1% of the cost of previous KB projects. This work marks
a milestone in two areas: For LLM research, for the first time, it provides
constructive insights into the scope and structure of LLMs' knowledge (or
beliefs). For KB construction, it pioneers new pathways for the long-standing
challenge of general-domain KB construction. GPTKB is accessible at
https://gptkb.org.",2024-11-07,"Yujia Hu, Tuan-Phong Nguyen, Shrestha Ghosh, Simon Razniewski",http://arxiv.org/pdf/2411.04920v3,cs.CL
GASE: Generatively Augmented Sentence Encoding,"We propose an approach to enhance sentence embeddings by applying generative
text models for data augmentation at inference time. Unlike conventional data
augmentation that utilises synthetic training data, our approach does not
require access to model parameters or the computational resources typically
required for fine-tuning state-of-the-art models. Generatively Augmented
Sentence Encoding uses diverse linguistic synthetic variants of input texts
generated by paraphrasing, summarising, or extracting keywords, followed by
pooling the original and synthetic embeddings. Experimental results on the
Massive Text Embedding Benchmark for Semantic Textual Similarity (STS)
demonstrate performance improvements across a range of embedding models using
different generative models for augmentation. We find that generative
augmentation leads to larger performance improvements for embedding models with
lower baseline performance. These findings suggest that integrating generative
augmentation at inference time adds semantic diversity and can enhance the
robustness and generalizability of sentence embeddings for embedding models.
Our results show that the degree to which generative augmentation can improve
STS performance depends not only on the embedding model but also on the
dataset. From a broader perspective, the approach allows trading training for
inference compute.",2024-11-07,"Manuel Frank, Haithem Afli",http://arxiv.org/pdf/2411.04914v1,cs.CL
A Brief History of Named Entity Recognition,"A large amount of information in today's world is now stored in knowledge
bases. Named Entity Recognition (NER) is a process of extracting,
disambiguation, and linking an entity from raw text to insightful and
structured knowledge bases. More concretely, it is identifying and classifying
entities in the text that are crucial for Information Extraction, Semantic
Annotation, Question Answering, Ontology Population, and so on. The process of
NER has evolved in the last three decades since it first appeared in 1996. In
this survey, we study the evolution of techniques employed for NER and compare
the results, starting from supervised to the developing unsupervised learning
methods.",2024-11-07,Monica Munnangi,http://arxiv.org/pdf/2411.05057v1,cs.CL
OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models,"Large language models (LLMs) for code have become indispensable in various
domains, including code generation, reasoning tasks and agent systems. While
open-access code LLMs are increasingly approaching the performance levels of
proprietary models, high-quality code LLMs suitable for rigorous scientific
investigation, particularly those with reproducible data processing pipelines
and transparent training protocols, remain limited. The scarcity is due to
various challenges, including resource constraints, ethical considerations, and
the competitive advantages of keeping models advanced. To address the gap, we
introduce OpenCoder, a top-tier code LLM that not only achieves performance
comparable to leading models but also serves as an ""open cookbook"" for the
research community. Unlike most prior efforts, we release not only model
weights and inference code, but also the reproducible training data, complete
data processing pipeline, rigorous experimental ablation results, and detailed
training protocols for open scientific research. Through this comprehensive
release, we identify the key ingredients for building a top-tier code LLM: (1)
code optimized heuristic rules for data cleaning and methods for data
deduplication, (2) recall of text corpus related to code and (3) high-quality
synthetic data in both annealing and supervised fine-tuning stages. By offering
this level of openness, we aim to broaden access to all aspects of a top-tier
code LLM, with OpenCoder serving as both a powerful model and an open
foundation to accelerate research, and enable reproducible advancements in code
AI.",2024-11-07,"Siming Huang, Tianhao Cheng, J. K. Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, Jiaheng Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, Wei Chu",http://arxiv.org/pdf/2411.04905v3,cs.CL
Sentiment Analysis of Spanish Political Party Tweets Using Pre-trained Language Models,"Title: Sentiment Analysis of Spanish Political Party Communications on
Twitter Using Pre-trained Language Models
  Authors: Chuqiao Song, Shunzhang Chen, Xinyi Cai, Hao Chen
  Comments: 21 pages, 6 figures
  Abstract: This study investigates sentiment patterns within Spanish political
party communications on Twitter by leveraging BETO and RoBERTuito, two
pre-trained language models optimized for Spanish text. Using a dataset of
tweets from major Spanish political parties: PSOE, PP, Vox, Podemos, and
Ciudadanos, spanning 2019 to 2024, this research analyzes sentiment
distributions and explores the relationship between sentiment expression and
party ideology. The findings indicate that both models consistently identify a
predominant Neutral sentiment across all parties, with significant variations
in Negative and Positive sentiments that align with ideological distinctions.
Specifically, Vox exhibits higher levels of Negative sentiment, while PSOE
demonstrates relatively high Positive sentiment, supporting the hypothesis that
emotional appeals in political messaging reflect ideological stances. This
study underscores the potential of pre-trained language models for non-English
sentiment analysis on social media, providing insights into sentiment dynamics
that shape public discourse within Spain's multi-party political system.
  Keywords: Spanish politics, sentiment analysis, pre-trained language models,
Twitter, BETO, RoBERTuito, political ideology, multi-party system",2024-11-07,"Chuqiao Song, Shunzhang Chen, Xinyi Cai, Hao Chen",http://arxiv.org/pdf/2411.04862v1,cs.CL
Prompt-Guided Internal States for Hallucination Detection of Large Language Models,"Large Language Models (LLMs) have demonstrated remarkable capabilities across
a variety of tasks in different domains. However, they sometimes generate
responses that are logically coherent but factually incorrect or misleading,
which is known as LLM hallucinations. Data-driven supervised methods train
hallucination detectors by leveraging the internal states of LLMs, but
detectors trained on specific domains often struggle to generalize well to
other domains. In this paper, we aim to enhance the cross-domain performance of
supervised detectors with only in-domain data. We propose a novel framework,
prompt-guided internal states for hallucination detection of LLMs, namely
PRISM. By utilizing appropriate prompts to guide changes to the structure
related to text truthfulness in LLMs' internal states, we make this structure
more salient and consistent across texts from different domains. We integrated
our framework with existing hallucination detection methods and conducted
experiments on datasets from different domains. The experimental results
indicate that our framework significantly enhances the cross-domain
generalization of existing hallucination detection methods.",2024-11-07,"Fujie Zhang, Peiqi Yu, Biao Yi, Baolei Zhang, Tong Li, Zheli Liu",http://arxiv.org/pdf/2411.04847v3,cs.CL
VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and Benchmark Models,"Existing text simplification or paraphrase datasets mainly focus on
sentence-level text generation in a general domain. These datasets are
typically developed without using domain knowledge. In this paper, we release a
novel dataset, VTechAGP, which is the first academic-to-general-audience text
paraphrase dataset consisting of document-level these and dissertation academic
and general-audience abstract pairs from 8 colleges authored over 25 years. We
also propose a novel dynamic soft prompt generative language model, DSPT5. For
training, we leverage a contrastive-generative loss function to learn the
keyword vectors in the dynamic prompt. For inference, we adopt a crowd-sampling
decoding strategy at both semantic and structural levels to further select the
best output candidate. We evaluate DSPT5 and various state-of-the-art large
language models (LLMs) from multiple perspectives. Results demonstrate that the
SOTA LLMs do not provide satisfactory outcomes, while the lightweight DSPT5 can
achieve competitive results. To the best of our knowledge, we are the first to
build a benchmark dataset and solutions for academic-to-general-audience text
paraphrase dataset. Models will be public after acceptance.",2024-11-07,"Ming Cheng, Jiaying Gong, Chenhan Yuan, William A. Ingram, Edward Fox, Hoda Eldardiry",http://arxiv.org/pdf/2411.04825v2,cs.CL
When Does Classical Chinese Help? Quantifying Cross-Lingual Transfer in Hanja and Kanbun,"Historical and linguistic connections within the Sinosphere have led
researchers to use Classical Chinese resources for cross-lingual transfer when
processing historical documents from Korea and Japan. In this paper, we
question the assumption of cross-lingual transferability from Classical Chinese
to Hanja and Kanbun, the ancient written languages of Korea and Japan,
respectively. Our experiments across machine translation, named entity
recognition, and punctuation restoration tasks show minimal impact of Classical
Chinese datasets on language model performance for ancient Korean documents
written in Hanja, with performance differences within $\pm{}0.0068$ F1-score
for sequence labeling tasks and up to $+0.84$ BLEU score for translation. These
limitations persist consistently across various model sizes, architectures, and
domain-specific datasets. Our analysis reveals that the benefits of Classical
Chinese resources diminish rapidly as local language data increases for Hanja,
while showing substantial improvements only in extremely low-resource scenarios
for both Korean and Japanese historical documents. These mixed results
emphasize the need for careful empirical validation rather than assuming
benefits from indiscriminate cross-lingual transfer.",2024-11-07,"Seyoung Song, Haneul Yoo, Jiho Jin, Kyunghyun Cho, Alice Oh",http://arxiv.org/pdf/2411.04822v1,cs.CL
LuxBank: The First Universal Dependency Treebank for Luxembourgish,"The Universal Dependencies (UD) project has significantly expanded linguistic
coverage across 161 languages, yet Luxembourgish, a West Germanic language
spoken by approximately 400,000 people, has remained absent until now. In this
paper, we introduce LuxBank, the first UD Treebank for Luxembourgish,
addressing the gap in syntactic annotation and analysis for this `low-research'
language. We establish formal guidelines for Luxembourgish language annotation,
providing the foundation for the first large-scale quantitative analysis of its
syntax. LuxBank serves not only as a resource for linguists and language
learners but also as a tool for developing spell checkers and grammar checkers,
organising existing text archives and even training large language models. By
incorporating Luxembourgish into the UD framework, we aim to enhance the
understanding of syntactic variation within West Germanic languages and offer a
model for documenting smaller, semi-standardised languages. This work positions
Luxembourgish as a valuable resource in the broader linguistic and NLP
communities, contributing to the study of languages with limited research and
resources.",2024-11-07,"Alistair Plum, Caroline Döhmer, Emilia Milano, Anne-Marie Lutgen, Christoph Purschke",http://arxiv.org/pdf/2411.04813v1,cs.CL
Kwai-STaR: Transform LLMs into State-Transition Reasoners,"Mathematical reasoning presents a significant challenge to the cognitive
capabilities of LLMs. Various methods have been proposed to enhance the
mathematical ability of LLMs. However, few recognize the value of state
transition for LLM reasoning. In this work, we define mathematical
problem-solving as a process of transiting from an initial unsolved state to
the final resolved state, and propose Kwai-STaR framework, which transforms
LLMs into State-Transition Reasoners to improve their intuitive reasoning
capabilities. Our approach comprises three main steps: (1) Define the state
space tailored to the mathematical reasoning. (2) Generate state-transition
data based on the state space. (3) Convert original LLMs into State-Transition
Reasoners via a curricular training strategy. Our experiments validate the
effectiveness of Kwai-STaR in enhancing mathematical reasoning: After training
on the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and
LLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard
dataset. Additionally, the state transition-based design endows Kwai-STaR with
remarkable training and inference efficiency. Further experiments are underway
to establish the generality of Kwai-STaR.",2024-11-07,"Xingyu Lu, Yuhang Hu, Changyi Liu, Tianke Zhang, Zhenyu Yang, Zhixiang Ding, Shengsheng Qian, Meng Du, Ruiwen Kang, Kaiyu Tang, Fan Yang, Tingting Gao, Di Zhang, Hai-Tao Zheng, Bin Wen",http://arxiv.org/pdf/2411.04799v2,cs.CL
KnowCoder-X: Boosting Multilingual Information Extraction via Code,"Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual
alignment. However, although LLMs show promising cross-lingual alignment in IE,
a significant imbalance across languages persists, highlighting an underlying
deficiency. To address this, we propose KnowCoder-X, a powerful code LLM with
advanced cross-lingual and multilingual capabilities for universal information
extraction. Firstly, it standardizes the representation of multilingual schemas
using Python classes, ensuring a consistent ontology across different
languages. Then, IE across languages is formulated as a unified code generation
task. Secondly, we enhance the model's cross-lingual transferability through IE
cross-lingual alignment instruction tuning on a translated instance prediction
task we proposed. During this phase, we also construct a high-quality and
diverse bilingual IE parallel dataset with 257k samples, called ParallelNER,
synthesized by our proposed robust three-stage pipeline, with manual annotation
to ensure quality. Although without training in 29 unseen languages,
KnowCoder-X surpasses ChatGPT by $30.17\%$ and SoTA by $20.03\%$, thereby
demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations
on 64 IE benchmarks in Chinese and English under various settings demonstrate
that KnowCoder-X significantly enhances cross-lingual IE transfer through
boosting the IE alignment. Our code and dataset are available at:
https://github.com/ICT-GoKnow/KnowCoder",2024-11-07,"Yuxin Zuo, Wenxuan Jiang, Wenxuan Liu, Zixuan Li, Long Bai, Hanbin Wang, Yutao Zeng, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng",http://arxiv.org/pdf/2411.04794v2,cs.CL
Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research,"In recent years, the application of generative artificial intelligence
(GenAI) in financial analysis and investment decision-making has gained
significant attention. However, most existing approaches rely on single-agent
systems, which fail to fully utilize the collaborative potential of multiple AI
agents. In this paper, we propose a novel multi-agent collaboration system
designed to enhance decision-making in financial investment research. The
system incorporates agent groups with both configurable group sizes and
collaboration structures to leverage the strengths of each agent group type. By
utilizing a sub-optimal combination strategy, the system dynamically adapts to
varying market conditions and investment scenarios, optimizing performance
across different tasks. We focus on three sub-tasks: fundamentals, market
sentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30
companies listed on the Dow Jones Index. Our findings reveal significant
performance variations based on the configurations of AI agents for different
tasks. The results demonstrate that our multi-agent collaboration system
outperforms traditional single-agent models, offering improved accuracy,
efficiency, and adaptability in complex financial environments. This study
highlights the potential of multi-agent systems in transforming financial
analysis and investment decision-making by integrating diverse analytical
perspectives.",2024-11-07,"Xuewen Han, Neng Wang, Shangkun Che, Hongyang Yang, Kunpeng Zhang, Sean Xin Xu",http://arxiv.org/pdf/2411.04788v1,cs.CL
A study of Vietnamese readability assessing through semantic and statistical features,"Determining the difficulty of a text involves assessing various textual
features that may impact the reader's text comprehension, yet current research
in Vietnamese has only focused on statistical features. This paper introduces a
new approach that integrates statistical and semantic approaches to assessing
text readability. Our research utilized three distinct datasets: the Vietnamese
Text Readability Dataset (ViRead), OneStopEnglish, and RACE, with the latter
two translated into Vietnamese. Advanced semantic analysis methods were
employed for the semantic aspect using state-of-the-art language models such as
PhoBERT, ViDeBERTa, and ViBERT. In addition, statistical methods were
incorporated to extract syntactic and lexical features of the text. We
conducted experiments using various machine learning models, including Support
Vector Machine (SVM), Random Forest, and Extra Trees and evaluated their
performance using accuracy and F1 score metrics. Our results indicate that a
joint approach that combines semantic and statistical features significantly
enhances the accuracy of readability classification compared to using each
method in isolation. The current study emphasizes the importance of considering
both statistical and semantic aspects for a more accurate assessment of text
difficulty in Vietnamese. This contribution to the field provides insights into
the adaptability of advanced language models in the context of Vietnamese text
readability. It lays the groundwork for future research in this area.",2024-11-07,"Hung Tuan Le, Long Truong To, Manh Trong Nguyen, Quyen Nguyen, Trong-Hop Do",http://arxiv.org/pdf/2411.04756v1,cs.CL
RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval,"Code-mixing, the integration of lexical and grammatical elements from
multiple languages within a single sentence, is a widespread linguistic
phenomenon, particularly prevalent in multilingual societies. In India, social
media users frequently engage in code-mixed conversations using the Roman
script, especially among migrant communities who form online groups to share
relevant local information. This paper focuses on the challenges of extracting
relevant information from code-mixed conversations, specifically within Roman
transliterated Bengali mixed with English. This study presents a novel approach
to address these challenges by developing a mechanism to automatically identify
the most relevant answers from code-mixed conversations. We have experimented
with a dataset comprising of queries and documents from Facebook, and Query
Relevance files (QRels) to aid in this task. Our results demonstrate the
effectiveness of our approach in extracting pertinent information from complex,
code-mixed digital conversations, contributing to the broader field of natural
language processing in multilingual and informal text environments. We use
GPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant
documents to frame a mathematical model which helps to detect relevant
documents corresponding to a query.",2024-11-07,"Aniket Deroy, Subhankar Maity",http://arxiv.org/pdf/2411.04752v3,cs.CL
BhasaAnuvaad: A Speech Translation Dataset for 13 Indian Languages,"Automatic Speech Translation (AST) datasets for Indian languages remain
critically scarce, with public resources covering fewer than 10 of the 22
official languages. This scarcity has resulted in AST systems for Indian
languages lagging far behind those available for high-resource languages like
English. In this paper, we first evaluate the performance of widely-used AST
systems on Indian languages, identifying notable performance gaps and
challenges. Our findings show that while these systems perform adequately on
read speech, they struggle significantly with spontaneous speech, including
disfluencies like pauses and hesitations. Additionally, there is a striking
absence of systems capable of accurately translating colloquial and informal
language, a key aspect of everyday communication. To this end, we introduce
BhasaAnuvaad, the largest publicly available dataset for AST involving 13 out
of 22 scheduled Indian languages and English spanning over 44,400 hours and 17M
text segments. BhasaAnuvaad contains data for English speech to Indic text, as
well as Indic speech to English text. This dataset comprises three key
categories: (1) Curated datasets from existing resources, (2) Large-scale web
mining, and (3) Synthetic data generation. By offering this diverse and
expansive dataset, we aim to bridge the resource gap and promote advancements
in AST for Indian languages.",2024-11-07,"Sparsh Jain, Ashwin Sankar, Devilal Choudhary, Dhairya Suman, Nikhil Narasimhan, Mohammed Safi Ur Rahman Khan, Anoop Kunchukuttan, Mitesh M Khapra, Raj Dabre",http://arxiv.org/pdf/2411.04699v2,cs.CL
FMEA Builder: Expert Guided Text Generation for Equipment Maintenance,"Foundation models show great promise for generative tasks in many domains.
Here we discuss the use of foundation models to generate structured documents
related to critical assets. A Failure Mode and Effects Analysis (FMEA) captures
the composition of an asset or piece of equipment, the ways it may fail and the
consequences thereof. Our system uses large language models to enable fast and
expert supervised generation of new FMEA documents. Empirical analysis shows
that foundation models can correctly generate over half of an FMEA's key
content. Results from polling audiences of reliability professionals show a
positive outlook on using generative AI to create these documents for critical
assets.",2024-11-07,"Karol Lynch, Fabio Lorenzi, John Sheehan, Duygu Kabakci-Zorlu, Bradley Eck",http://arxiv.org/pdf/2411.05054v1,cs.CL
DISCO: DISCovering Overfittings as Causal Rules for Text Classification Models,"With the rapid advancement of neural language models, the deployment of
over-parameterized models has surged, increasing the need for interpretable
explanations comprehensible to human inspectors. Existing post-hoc
interpretability methods, which often focus on unigram features of single input
textual instances, fail to capture the models' decision-making process fully.
Additionally, many methods do not differentiate between decisions based on
spurious correlations and those based on a holistic understanding of the input.
Our paper introduces DISCO, a novel method for discovering global, rule-based
explanations by identifying causal n-gram associations with model predictions.
This method employs a scalable sequence mining technique to extract relevant
text spans from training data, associate them with model predictions, and
conduct causality checks to distill robust rules that elucidate model behavior.
These rules expose potential overfitting and provide insights into misleading
feature combinations. We validate DISCO through extensive testing,
demonstrating its superiority over existing methods in offering comprehensive
insights into complex model behaviors. Our approach successfully identifies all
shortcuts manually introduced into the training data (100% detection rate on
the MultiRC dataset), resulting in an 18.8% regression in model performance --
a capability unmatched by any other method. Furthermore, DISCO supports
interactive explanations, enabling human inspectors to distinguish spurious
causes in the rule-based output. This alleviates the burden of abundant
instance-wise explanations and helps assess the model's risk when encountering
out-of-distribution (OOD) data.",2024-11-07,"Zijian Zhang, Vinay Setty, Yumeng Wang, Avishek Anand",http://arxiv.org/pdf/2411.04649v1,cs.CL
Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop,"Training and deploying machine learning models relies on a large amount of
human-annotated data. As human labeling becomes increasingly expensive and
time-consuming, recent research has developed multiple strategies to speed up
annotation and reduce costs and human workload: generating synthetic training
data, active learning, and hybrid labeling. This tutorial is oriented toward
practical applications: we will present the basics of each strategy, highlight
their benefits and limitations, and discuss in detail real-life case studies.
Additionally, we will walk through best practices for managing human annotators
and controlling the quality of the final dataset. The tutorial includes a
hands-on workshop, where attendees will be guided in implementing a hybrid
annotation setup. This tutorial is designed for NLP practitioners from both
research and industry backgrounds who are involved in or interested in
optimizing data labeling projects.",2024-11-07,"Ekaterina Artemova, Akim Tsvigun, Dominik Schlechtweg, Natalia Fedorova, Konstantin Chernyshev, Sergei Tilga, Boris Obmoroshev",http://arxiv.org/pdf/2411.04637v3,cs.CL
FASSILA: A Corpus for Algerian Dialect Fake News Detection and Sentiment Analysis,"In the context of low-resource languages, the Algerian dialect (AD) faces
challenges due to the absence of annotated corpora, hindering its effective
processing, notably in Machine Learning (ML) applications reliant on corpora
for training and assessment. This study outlines the development process of a
specialized corpus for Fake News (FN) detection and sentiment analysis (SA) in
AD called FASSILA. This corpus comprises 10,087 sentences, encompassing over
19,497 unique words in AD, and addresses the significant lack of linguistic
resources in the language and covers seven distinct domains. We propose an
annotation scheme for FN detection and SA, detailing the data collection,
cleaning, and labelling process. Remarkable Inter-Annotator Agreement indicates
that the annotation scheme produces consistent annotations of high quality.
Subsequent classification experiments using BERT-based models and ML models are
presented, demonstrate promising results and highlight avenues for further
research. The dataset is made freely available on GitHub
(https://github.com/amincoding/FASSILA) to facilitate future advancements in
the field.",2024-11-07,"Amin Abdedaiem, Abdelhalim Hafedh Dahou, Mohamed Amine Cheragui, Brigitte Mathiak",http://arxiv.org/pdf/2411.04604v1,cs.CL
Self-Calibrated Listwise Reranking with Large Language Models,"Large language models (LLMs), with advanced linguistic capabilities, have
been employed in reranking tasks through a sequence-to-sequence approach. In
this paradigm, multiple passages are reranked in a listwise manner and a
textual reranked permutation is generated. However, due to the limited context
window of LLMs, this reranking paradigm requires a sliding window strategy to
iteratively handle larger candidate sets. This not only increases computational
costs but also restricts the LLM from fully capturing all the comparison
information for all candidates. To address these challenges, we propose a novel
self-calibrated listwise reranking method, which aims to leverage LLMs to
produce global relevance scores for ranking. To achieve it, we first propose
the relevance-aware listwise reranking framework, which incorporates explicit
list-view relevance scores to improve reranking efficiency and enable global
comparison across the entire candidate set. Second, to ensure the comparability
of the computed scores, we propose self-calibrated training that uses
point-view relevance assessments generated internally by the LLM itself to
calibrate the list-view relevance assessments. Extensive experiments and
comprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks
demonstrate the effectiveness and efficiency of our proposed method.",2024-11-07,"Ruiyang Ren, Yuhao Wang, Kun Zhou, Wayne Xin Zhao, Wenjie Wang, Jing Liu, Ji-Rong Wen, Tat-Seng Chua",http://arxiv.org/pdf/2411.04602v1,cs.CL
Tibyan Corpus: Balanced and Comprehensive Error Coverage Corpus Using ChatGPT for Arabic Grammatical Error Correction,"Natural language processing (NLP) utilizes text data augmentation to overcome
sample size constraints. Increasing the sample size is a natural and widely
used strategy for alleviating these challenges. In this study, we chose Arabic
to increase the sample size and correct grammatical errors. Arabic is
considered one of the languages with limited resources for grammatical error
correction (GEC). Furthermore, QALB-14 and QALB-15 are the only datasets used
in most Arabic grammatical error correction research, with approximately 20,500
parallel examples, which is considered low compared with other languages.
Therefore, this study aims to develop an Arabic corpus called ""Tibyan"" for
grammatical error correction using ChatGPT. ChatGPT is used as a data augmenter
tool based on a pair of Arabic sentences containing grammatical errors matched
with a sentence free of errors extracted from Arabic books, called guide
sentences. Multiple steps were involved in establishing our corpus, including
the collection and pre-processing of a pair of Arabic texts from various
sources, such as books and open-access corpora. We then used ChatGPT to
generate a parallel corpus based on the text collected previously, as a guide
for generating sentences with multiple types of errors. By engaging linguistic
experts to review and validate the automatically generated sentences, we
ensured that they were correct and error-free. The corpus was validated and
refined iteratively based on feedback provided by linguistic experts to improve
its accuracy. Finally, we used the Arabic Error Type Annotation tool (ARETA) to
analyze the types of errors in the Tibyan corpus. Our corpus contained 49 of
errors, including seven types: orthography, morphology, syntax, semantics,
punctuation, merge, and split. The Tibyan corpus contains approximately 600 K
tokens.",2024-11-07,"Ahlam Alrehili, Areej Alhothali",http://arxiv.org/pdf/2411.04588v1,cs.CL
The State and Fate of Summarization Datasets: A Survey,"Automatic summarization has consistently attracted attention due to its
versatility and wide application in various downstream tasks. Despite its
popularity, we find that annotation efforts have largely been disjointed, and
have lacked common terminology. Consequently, it is challenging to discover
existing resources or identify coherent research directions. To address this,
we survey a large body of work spanning 133 datasets in over 100 languages,
creating a novel ontology covering sample properties, collection methods and
distribution. With this ontology we make key observations, including the lack
in accessible high-quality datasets for low-resource languages, and the field's
over-reliance on the news domain and on automatically collected distant
supervision. Finally, we make available a web interface that allows users to
interact and explore our ontology and dataset collection, as well as a template
for a summarization data card, which can be used to streamline future research
into a more coherent body of work.",2024-11-07,"Noam Dahan, Gabriel Stanovsky",http://arxiv.org/pdf/2411.04585v2,cs.CL
Multistage Fine-tuning Strategies for Automatic Speech Recognition in Low-resource Languages,"This paper presents a novel multistage fine-tuning strategy designed to
enhance automatic speech recognition (ASR) performance in low-resource
languages using OpenAI's Whisper model. In this approach we aim to build ASR
model for languages with limited digital resources by sequentially adapting the
model across linguistically similar languages. We experimented this on the
Malasar language, a Dravidian language spoken by approximately ten thousand
people in the Western Ghats of South India. Malasar language faces critical
challenges for technological intervention due to its lack of a native script
and absence of digital or spoken data resources. Working in collaboration with
Wycliffe India and Malasar community members, we created a spoken Malasar
corpus paired with transcription in Tamil script, a closely related major
language. In our approach to build ASR model for Malasar, we first build an
intermediate Tamil ASR, leveraging higher data availability for Tamil annotated
speech. This intermediate model is subsequently fine-tuned on Malasar data,
allowing for more effective ASR adaptation despite limited resources. The
multistage fine-tuning strategy demonstrated significant improvements over
direct fine-tuning on Malasar data alone, achieving a word error rate (WER) of
51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning
method. Further a WER reduction to 47.3% was achieved through punctuation
removal in post-processing, which addresses formatting inconsistencies that
impact evaluation. Our results underscore the effectiveness of sequential
multistage fine-tuning combined with targeted post-processing as a scalable
strategy for ASR system development in low-resource languages, especially where
linguistic similarities can be leveraged to bridge gaps in training data.",2024-11-07,"Leena G Pillai, Kavya Manohar, Basil K Raju, Elizabeth Sherly",http://arxiv.org/pdf/2411.04573v1,cs.CL
Pruning Literals for Highly Efficient Explainability at Word Level,"Designing an explainable model becomes crucial now for Natural Language
Processing(NLP) since most of the state-of-the-art machine learning models
provide a limited explanation for the prediction. In the spectrum of an
explainable model, Tsetlin Machine(TM) is promising because of its capability
of providing word-level explanation using proposition logic. However, concern
rises over the elaborated combination of literals (propositional logic) in the
clause that makes the model difficult for humans to comprehend, despite having
a transparent learning process. In this paper, we design a post-hoc pruning of
clauses that eliminate the randomly placed literals in the clause thereby
making the model more efficiently interpretable than the vanilla TM.
Experiments on the publicly available YELP-HAT Dataset demonstrate that the
proposed pruned TM's attention map aligns more with the human attention map
than the vanilla TM's attention map. In addition, the pairwise similarity
measure also surpasses the attention map-based neural network models. In terms
of accuracy, the proposed pruning method does not degrade the accuracy
significantly but rather enhances the performance up to 4% to 9% in some test
data.",2024-11-07,"Rohan Kumar Yadav, Bimal Bhattarai, Abhik Jana, Lei Jiao, Seid Muhie Yimam",http://arxiv.org/pdf/2411.04557v1,cs.CL
Best Practices for Distilling Large Language Models into BERT for Web Search Ranking,"Recent studies have highlighted the significant potential of Large Language
Models (LLMs) as zero-shot relevance rankers. These methods predominantly
utilize prompt learning to assess the relevance between queries and documents
by generating a ranked list of potential documents. Despite their promise, the
substantial costs associated with LLMs pose a significant challenge for their
direct implementation in commercial search systems. To overcome this barrier
and fully exploit the capabilities of LLMs for text ranking, we explore
techniques to transfer the ranking expertise of LLMs to a more compact model
similar to BERT, using a ranking loss to enable the deployment of less
resource-intensive models. Specifically, we enhance the training of LLMs
through Continued Pre-Training, taking the query as input and the clicked title
and summary as output. We then proceed with supervised fine-tuning of the LLM
using a rank loss, assigning the final token as a representative of the entire
sentence. Given the inherent characteristics of autoregressive language models,
only the final token </s> can encapsulate all preceding tokens. Additionally,
we introduce a hybrid point-wise and margin MSE loss to transfer the ranking
knowledge from LLMs to smaller models like BERT. This method creates a viable
solution for environments with strict resource constraints. Both offline and
online evaluations have confirmed the efficacy of our approach, and our model
has been successfully integrated into a commercial web search engine as of
February 2024.",2024-11-07,"Dezhi Ye, Junwei Hu, Jiabin Fan, Bowen Tian, Jie Liu, Haijin Liang, Jin Ma",http://arxiv.org/pdf/2411.04539v1,cs.CL
Meta-Reasoning Improves Tool Use in Large Language Models,"External tools help large language models succeed at tasks where they would
otherwise typically fail. In existing frameworks, choosing tools at test time
relies on naive greedy decoding, regardless of whether the model has been
fine-tuned on tool-annotated data or prompted with in-context examples. In
contrast, we find that gathering and choosing among a suitable set of candidate
tools has greater potential to lead to an optimal selection. We present Tool
selECTion via meta-reasONing (TECTON), a two-phase system that first reasons
over a task and outputs candidate tools using a custom fine-tuned language
modelling head. Then, with the custom head disabled, it meta-reasons (i.e., it
reasons over the previous reasoning process) to make a final choice. We show
that TECTON results in substantial gains--both in-distribution and
out-of-distribution--on a range of math reasoning datasets.",2024-11-07,"Lisa Alazraki, Marek Rei",http://arxiv.org/pdf/2411.04535v2,cs.CL
"Tomato, Tomahto, Tomate: Measuring the Role of Shared Semantics among Subwords in Multilingual Language Models","Human understanding of language is robust to different word choices as far as
they represent similar semantic concepts. To what extent does our human
intuition transfer to language models, which represent all subwords as distinct
embeddings? In this work, we take an initial step on measuring the role of
shared semantics among subwords in the encoder-only multilingual language
models (mLMs). To this end, we form ""semantic tokens"" by merging the
semantically similar subwords and their embeddings, and evaluate the updated
mLMs on 5 heterogeneous multilingual downstream tasks. Results show that the
general shared semantics could get the models a long way in making the
predictions on mLMs with different tokenizers and model sizes. Inspections on
the grouped subwords show that they exhibit a wide range of semantic
similarities, including synonyms and translations across many languages and
scripts. Lastly, we found the zero-shot results with semantic tokens are on par
or even better than the original models on certain classification tasks,
suggesting that the shared subword-level semantics may serve as the anchors for
cross-lingual transferring.",2024-11-07,"Xinyu Zhang, Jing Lu, Vinh Q. Tran, Tal Schuster, Donald Metzler, Jimmy Lin",http://arxiv.org/pdf/2411.04530v1,cs.CL
Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model,"To increase social bonding with interlocutors, humans naturally acquire the
ability to respond appropriately in a given situation by considering which
conversational skill is most suitable for the response - a process we call
skill-of-mind. For large language model (LLM)-based conversational agents,
planning appropriate conversational skills, as humans do, is challenging due to
the complexity of social dialogue, especially in interactive scenarios. To
address this, we propose a skill-of-mind-annotated conversation dataset, named
Multifaceted Skill-of-Mind, which includes multi-turn and multifaceted
conversational skills across various interactive scenarios (e.g., long-term,
counseling, task-oriented), grounded in diverse social contexts (e.g.,
demographics, persona, rules of thumb). This dataset consists of roughly 100K
conversations. Using this dataset, we introduce a new family of
skill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B
parameters. With extensive experiments, these models successfully demonstrate
the skill-of-mind process and exhibit strong generalizability in inferring
multifaceted skills across a variety of domains. Moreover, we show that Thanos
significantly enhances the quality of responses generated by LLM-based
conversational agents and promotes prosocial behavior in human evaluations.",2024-11-07,"Young-Jun Lee, Dokyong Lee, Junyoung Youn, Kyeongjin Oh, Ho-Jin Choi",http://arxiv.org/pdf/2411.04496v1,cs.CL
Selecting Between BERT and GPT for Text Classification in Political Science Research,"Political scientists often grapple with data scarcity in text classification.
Recently, fine-tuned BERT models and their variants have gained traction as
effective solutions to address this issue. In this study, we investigate the
potential of GPT-based models combined with prompt engineering as a viable
alternative. We conduct a series of experiments across various classification
tasks, differing in the number of classes and complexity, to evaluate the
effectiveness of BERT-based versus GPT-based models in low-data scenarios. Our
findings indicate that while zero-shot and few-shot learning with GPT models
provide reasonable performance and are well-suited for early-stage research
exploration, they generally fall short - or, at best, match - the performance
of BERT fine-tuning, particularly as the training set reaches a substantial
size (e.g., 1,000 samples). We conclude by comparing these approaches in terms
of performance, ease of use, and cost, providing practical guidance for
researchers facing data limitations. Our results are particularly relevant for
those engaged in quantitative text analysis in low-resource settings or with
limited labeled data.",2024-11-07,"Yu Wang, Wen Qu, Xin Ye",http://arxiv.org/pdf/2411.05050v1,cs.CL
ML-Promise: A Multilingual Dataset for Corporate Promise Verification,"Promises made by politicians, corporate leaders, and public figures have a
significant impact on public perception, trust, and institutional reputation.
However, the complexity and volume of such commitments, coupled with
difficulties in verifying their fulfillment, necessitate innovative methods for
assessing their credibility. This paper introduces the concept of Promise
Verification, a systematic approach involving steps such as promise
identification, evidence assessment, and the evaluation of timing for
verification. We propose the first multilingual dataset, ML-Promise, which
includes English, French, Chinese, Japanese, and Korean, aimed at facilitating
in-depth verification of promises, particularly in the context of
Environmental, Social, and Governance (ESG) reports. Given the growing emphasis
on corporate environmental contributions, this dataset addresses the challenge
of evaluating corporate promises, especially in light of practices like
greenwashing. Our findings also explore textual and image-based baselines, with
promising results from retrieval-augmented generation (RAG) approaches. This
work aims to foster further discourse on the accountability of public
commitments across multiple languages and domains.",2024-11-07,"Yohei Seki, Hakusen Shu, Anaïs Lhuissier, Hanwool Lee, Juyeon Kang, Min-Yuh Day, Chung-Chi Chen",http://arxiv.org/pdf/2411.04473v1,cs.CL
ProverbEval: Exploring LLM Evaluation Challenges for Low-resource Language Understanding,"With the rapid development of evaluation datasets to assess LLMs
understanding across a wide range of subjects and domains, identifying a
suitable language understanding benchmark has become increasingly challenging.
In this work, we explore LLM evaluation challenges for low-resource language
understanding and introduce \proverbeval, LLM evaluation benchmark for
low-resource languages, focusing on low-resource language understanding in
culture-specific scenarios. We benchmark various LLMs and explore factors that
create variability in the benchmarking process. We observed performance
variances of up to 50\%, depending on the order in which answer choices were
presented in multiple-choice tasks. Native language proverb descriptions
significantly improve tasks such as proverb generation, contributing to
improved outcomes. Additionally, monolingual evaluations consistently
outperformed their cross-lingual counterparts in generation tasks. We argue
that special attention must be given to the order of choices, the choice of
prompt language, task variability, and generation tasks when creating LLM
evaluation benchmarks. Evaluation data available at
https://huggingface.co/datasets/israel/ProverbEval, evaluation code
https://github.com/EthioNLP/EthioProverbEval.",2024-11-07,"Israel Abebe Azime, Atnafu Lambebo Tonja, Tadesse Destaw Belay, Yonas Chanie, Bontu Fufa Balcha, Negasi Haile Abadi, Henok Biadglign Ademtew, Mulubrhan Abebe Nerea, Debela Desalegn Yadeta, Derartu Dagne Geremew, Assefa Atsbiha tesfau, Philipp Slusallek, Thamar Solorio, Dietrich Klakow",http://arxiv.org/pdf/2411.05049v3,cs.CL
Gradient Localization Improves Lifelong Pretraining of Language Models,"Large Language Models (LLMs) trained on web-scale text corpora have been
shown to capture world knowledge in their parameters. However, the mechanism by
which language models store different types of knowledge is poorly understood.
In this work, we examine two types of knowledge relating to temporally
sensitive entities and demonstrate that each type is localized to different
sets of parameters within the LLMs. We hypothesize that the lack of
consideration of the locality of knowledge in existing continual learning
methods contributes to both: the failed uptake of new information, and
catastrophic forgetting of previously learned information. We observe that
sequences containing references to updated and newly mentioned entities exhibit
larger gradient norms in a subset of layers. We demonstrate that targeting
parameter updates to these relevant layers can improve the performance of
continually pretraining on language containing temporal drift.",2024-11-07,"Jared Fernandez, Yonatan Bisk, Emma Strubell",http://arxiv.org/pdf/2411.04448v1,cs.CL
ACCIO: Table Understanding Enhanced via Contrastive Learning with Aggregations,"The attention to table understanding using recent natural language models has
been growing. However, most related works tend to focus on learning the
structure of the table directly. Just as humans improve their understanding of
sentences by comparing them, they can also enhance their understanding by
comparing tables. With this idea, in this paper, we introduce ACCIO, tAble
understanding enhanCed via Contrastive learnIng with aggregatiOns, a novel
approach to enhancing table understanding by contrasting original tables with
their pivot summaries through contrastive learning. ACCIO trains an encoder to
bring these table pairs closer together. Through validation via column type
annotation, ACCIO achieves competitive performance with a macro F1 score of
91.1 compared to state-of-the-art methods. This work represents the first
attempt to utilize pairs of tables for table embedding, promising significant
advancements in table comprehension. Our code is available at
https://github.com/whnhch/ACCIO/.",2024-11-07,Whanhee Cho,http://arxiv.org/pdf/2411.04443v1,cs.CL
"One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity","Researchers in social science and psychology have recently proposed using
large language models (LLMs) as replacements for humans in behavioral research.
In addition to arguments about whether LLMs accurately capture population-level
patterns, this has raised questions about whether LLMs capture human-like
conceptual diversity. Separately, it is debated whether post-training alignment
(RLHF or RLAIF) affects models' internal diversity. Inspired by human studies,
we use a new way of measuring the conceptual diversity of
synthetically-generated LLM ""populations"" by relating the internal variability
of simulated individuals to the population-level variability. We use this
approach to evaluate non-aligned and aligned LLMs on two domains with rich
human behavioral data. While no model reaches human-like diversity, aligned
models generally display less diversity than their instruction fine-tuned
counterparts. Our findings highlight potential trade-offs between increasing
models' value alignment and decreasing the diversity of their conceptual
representations.",2024-11-07,"Sonia K. Murthy, Tomer Ullman, Jennifer Hu",http://arxiv.org/pdf/2411.04427v2,cs.CL
DELIFT: Data Efficient Language model Instruction Fine Tuning,"Fine-tuning large language models (LLMs) is essential for enhancing their
performance on specific tasks but is often resource-intensive due to redundant
or uninformative data. To address this inefficiency, we introduce DELIFT (Data
Efficient Language model Instruction Fine-Tuning), a novel algorithm that
systematically optimizes data selection across the three key stages of
fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,
reasoning, question-answering), and (3) continual fine-tuning (e.g.,
incorporating new data versions). Unlike existing methods that focus on
single-stage optimization or rely on computationally intensive gradient
calculations, DELIFT operates efficiently across all stages. Central to our
approach is a pairwise utility metric that quantifies how beneficial a data
sample is for improving the model's responses to other samples, effectively
measuring the informational value relative to the model's current capabilities.
By leveraging different submodular functions applied to this metric, DELIFT
selects diverse and optimal subsets that are useful across all stages of
fine-tuning. Experiments across various tasks and model scales demonstrate that
DELIFT can reduce the fine-tuning data size by up to 70% without compromising
performance, offering significant computational savings and outperforming
existing methods in both efficiency and efficacy.",2024-11-07,"Ishika Agarwal, Krishnateja Killamsetty, Lucian Popa, Marina Danilevksy",http://arxiv.org/pdf/2411.04425v3,cs.CL
Bayesian Calibration of Win Rate Estimation with LLM Evaluators,"Recent advances in large language models (LLMs) show the potential of using
LLMs as evaluators for assessing the quality of text generations from LLMs.
However, applying LLM evaluators naively to compare or judge between different
systems can lead to unreliable results due to the intrinsic win rate estimation
bias of LLM evaluators. In order to mitigate this problem, we propose two
calibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian
Dawid-Skene, both of which leverage Bayesian inference to more accurately infer
the true win rate of generative language models. We empirically validate our
methods on six datasets covering story generation, summarization, and
instruction following tasks. We show that both our methods are effective in
improving the accuracy of win rate estimation using LLMs as evaluators,
offering a promising direction for reliable automatic text quality evaluation.",2024-11-07,"Yicheng Gao, Gonghan Xu, Zhe Wang, Arman Cohan",http://arxiv.org/pdf/2411.04424v1,cs.CL
Variational Low-Rank Adaptation Using IVON,"We show that variational learning can significantly improve the accuracy and
calibration of Low-Rank Adaptation (LoRA) without a substantial increase in the
cost. We replace AdamW by the Improved Variational Online Newton (IVON)
algorithm to finetune large language models. For Llama-2 with 7 billion
parameters, IVON improves the accuracy over AdamW by 2.8% and expected
calibration error by 4.6%. The accuracy is also better than the other Bayesian
alternatives, yet the cost is lower and the implementation is easier. Our work
provides additional evidence for the effectiveness of IVON for large language
models. The code is available at
https://github.com/team-approx-bayes/ivon-lora.",2024-11-07,"Bai Cong, Nico Daheim, Yuesong Shen, Daniel Cremers, Rio Yokota, Mohammad Emtiyaz Khan, Thomas Möllenhoff",http://arxiv.org/pdf/2411.04421v2,cs.CL
Leveraging LLMs to Enable Natural Language Search on Go-to-market Platforms,"Enterprise searches require users to have complex knowledge of queries,
configurations, and metadata, rendering it difficult for them to access
information as needed. Most go-to-market (GTM) platforms utilize advanced
search, an interface that enables users to filter queries by various fields
using categories or keywords, which, historically, however, has proven to be
exceedingly cumbersome, as users are faced with seemingly hundreds of options,
fields, and buttons. Consequently, querying with natural language has long been
ideal, a notion further empowered by Large Language Models (LLMs).
  In this paper, we implement and evaluate a solution for the Zoominfo product
for sellers, which prompts the LLM with natural language, producing search
fields through entity extraction that are then converted into a search query.
The intermediary search fields offer numerous advantages for each query,
including the elimination of syntax errors, simpler ground truths, and an
intuitive format for the LLM to interpret.
  We paired this pipeline with many advanced prompt engineering strategies,
featuring an intricate system message, few-shot prompting, chain-of-thought
(CoT) reasoning, and execution refinement. Furthermore, we manually created the
ground truth for 500+ natural language queries, enabling the supervised
fine-tuning of Llama-3-8B-Instruct and the introduction of sophisticated
numerical metrics.
  Comprehensive experiments with closed, open source, and fine-tuned LLM models
were conducted through exact, Jaccard, cosine, and semantic similarity on
individual search entities to demonstrate the efficacy of our approach.
Overall, the most accurate closed model had an average accuracy of 97% per
query, with only one field performing under 90%, with comparable results
observed from the fine-tuned models.",2024-11-07,"Jesse Yao, Saurav Acharya, Priyaranjan Parida, Srinivas Attipalli, Ali Dasdan",http://arxiv.org/pdf/2411.05048v1,cs.CL
PhoneLM:an Efficient and Capable Small Language Model Family through Principled Pre-training,"The interest in developing small language models (SLM) for on-device
deployment is fast growing. However, the existing SLM design hardly considers
the device hardware characteristics. Instead, this work presents a simple yet
effective principle for SLM design: architecture searching for (near-)optimal
runtime efficiency before pre-training. Guided by this principle, we develop
PhoneLM SLM family (currently with 0.5B and 1.5B versions), that acheive the
state-of-the-art capability-efficiency tradeoff among those with similar
parameter size. We fully open-source the code, weights, and training datasets
of PhoneLM for reproducibility and transparency, including both base and
instructed versions. We also release a finetuned version of PhoneLM capable of
accurate Android Intent invocation, and an end-to-end Android demo. All
materials are available at https://github.com/UbiquitousLearning/PhoneLM.",2024-11-07,"Rongjie Yi, Xiang Li, Weikai Xie, Zhenyan Lu, Chenghua Wang, Ao Zhou, Shangguang Wang, Xiwen Zhang, Mengwei Xu",http://arxiv.org/pdf/2411.05046v1,cs.CL
Measuring short-form factuality in large language models,"We present SimpleQA, a benchmark that evaluates the ability of language
models to answer short, fact-seeking questions. We prioritized two properties
in designing this eval. First, SimpleQA is challenging, as it is adversarially
collected against GPT-4 responses. Second, responses are easy to grade, because
questions are created such that there exists only a single, indisputable
answer. Each answer in SimpleQA is graded as either correct, incorrect, or not
attempted. A model with ideal behavior would get as many questions correct as
possible while not attempting the questions for which it is not confident it
knows the correct answer. SimpleQA is a simple, targeted evaluation for whether
models ""know what they know,"" and our hope is that this benchmark will remain
relevant for the next few generations of frontier models. SimpleQA can be found
at https://github.com/openai/simple-evals.",2024-11-07,"Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, William Fedus",http://arxiv.org/pdf/2411.04368v1,cs.CL
Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale,"Large Language Models (LLMs) face significant challenges at inference time
due to their high computational demands. To address this, we present
Performance-Guided Knowledge Distillation (PGKD), a cost-effective and
high-throughput solution for production text classification applications. PGKD
utilizes teacher-student Knowledge Distillation to distill the knowledge of
LLMs into smaller, task-specific models. PGKD establishes an active learning
routine between the student model and the LLM; the LLM continuously generates
new training data leveraging hard-negative mining, student model validation
performance, and early-stopping protocols to inform the data generation. By
employing a cyclical, performance-aware approach tailored for highly
multi-class, sparsely annotated datasets prevalent in industrial text
classification, PGKD effectively addresses training challenges and outperforms
traditional BERT-base models and other knowledge distillation methods on
several multi-class classification datasets. Additionally, cost and latency
benchmarking reveals that models fine-tuned with PGKD are up to 130X faster and
25X less expensive than LLMs for inference on the same classification task.
While PGKD is showcased for text classification tasks, its versatile framework
can be extended to any LLM distillation task, including language generation,
making it a powerful tool for optimizing performance across a wide range of AI
applications.",2024-11-07,"Flavio Di Palo, Prateek Singhi, Bilal Fadlallah",http://arxiv.org/pdf/2411.05045v1,cs.CL
Robust and Efficient Fine-tuning of LLMs with Bayesian Reparameterization of Low-Rank Adaptation,"Large Language Models (LLMs) are highly resource-intensive to fine-tune due
to their enormous size. While low-rank adaptation is a prominent
parameter-efficient fine-tuning approach, it suffers from sensitivity to
hyperparameter choices, leading to instability in model performance on
fine-tuning downstream tasks. This paper highlights the importance of effective
parameterization in low-rank fine-tuning to reduce estimator variance and
enhance the stability of final model outputs. We propose MonteCLoRA, an
efficient fine-tuning technique, employing Monte Carlo estimation to learn an
unbiased posterior estimation of low-rank parameters with low expected
variance, which stabilizes fine-tuned LLMs with only O(1) additional
parameters. MonteCLoRA shows significant improvements in accuracy and
robustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness
than existing efficient fine-tuning methods on natural language understanding
tasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with
pre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance
with 50% lower variance than the contemporary efficient fine-tuning methods.
The theoretical and empirical results presented in the paper underscore how
parameterization and hyperpriors balance exploration-exploitation in the
low-rank parametric space, therefore leading to more optimal and robust
parameter estimation during efficient fine-tuning.",2024-11-07,"Ayan Sengupta, Vaibhav Seth, Arinjay Pathak, Natraj Raman, Sriram Gopalakrishnan, Tanmoy Chakraborty",http://arxiv.org/pdf/2411.04358v2,cs.CL
Scaling Laws for Precision,"Low precision training and inference affect both the quality and cost of
language models, but current scaling laws do not account for this. In this
work, we devise ""precision-aware"" scaling laws for both training and inference.
We propose that training in lower precision reduces the model's ""effective
parameter count,"" allowing us to predict the additional loss incurred from
training in low precision and post-train quantization. For inference, we find
that the degradation introduced by post-training quantization increases as
models are trained on more data, eventually making additional pretraining data
actively harmful. For training, our scaling laws allow us to predict the loss
of a model with different parts in different precisions, and suggest that
training larger models in lower precision may be compute optimal. We unify the
scaling laws for post and pretraining quantization to arrive at a single
functional form that predicts degradation from training and inference in varied
precisions. We fit on over 465 pretraining runs and validate our predictions on
model sizes up to 1.7B parameters trained on up to 26B tokens.",2024-11-07,"Tanishq Kumar, Zachary Ankner, Benjamin F. Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, Aditi Raghunathan",http://arxiv.org/pdf/2411.04330v2,cs.CL
CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models,"Pre-trained on massive amounts of code and text data, large language models
(LLMs) have demonstrated remarkable achievements in performing code generation
tasks. With additional execution-based feedback, these models can act as agents
with capabilities to self-refine and improve generated code autonomously.
However, on challenging coding tasks with extremely large search space, current
agentic approaches still struggle with multi-stage planning, generating, and
debugging. To address this problem, we propose CodeTree, a framework for LLM
agents to efficiently explore the search space in different stages of the code
generation process. Specifically, we adopted a unified tree structure to
explicitly explore different coding strategies, generate corresponding coding
solutions, and subsequently refine the solutions. In each stage, critical
decision-making (ranking, termination, expanding) of the exploration process is
guided by both the environmental execution-based feedback and
LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code
generation benchmarks and demonstrated the significant performance gains of
CodeTree against strong baselines. Using GPT-4o as the base model, we
consistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0
on CodeContests. On the challenging SWEBench benchmark, our approach led to
significant performance gains.",2024-11-07,"Jierui Li, Hung Le, Yingbo Zhou, Caiming Xiong, Silvio Savarese, Doyen Sahoo",http://arxiv.org/pdf/2411.04329v2,cs.CL
Balancing Transparency and Accuracy: A Comparative Analysis of Rule-Based and Deep Learning Models in Political Bias Classification,"The unchecked spread of digital information, combined with increasing
political polarization and the tendency of individuals to isolate themselves
from opposing political viewpoints, has driven researchers to develop systems
for automatically detecting political bias in media. This trend has been
further fueled by discussions on social media. We explore methods for
categorizing bias in US news articles, comparing rule-based and deep learning
approaches. The study highlights the sensitivity of modern self-learning
systems to unconstrained data ingestion, while reconsidering the strengths of
traditional rule-based systems. Applying both models to left-leaning (CNN) and
right-leaning (FOX) news articles, we assess their effectiveness on data beyond
the original training and test sets.This analysis highlights each model's
accuracy, offers a framework for exploring deep-learning explainability, and
sheds light on political bias in US news media. We contrast the opaque
architecture of a deep learning model with the transparency of a linguistically
informed rule-based model, showing that the rule-based model performs
consistently across different data conditions and offers greater transparency,
whereas the deep learning model is dependent on the training set and struggles
with unseen data.",2024-11-07,"Manuel Nunez Martinez, Sonja Schmer-Galunder, Zoey Liu, Sangpil Youm, Chathuri Jayaweera, Bonnie J. Dorr",http://arxiv.org/pdf/2411.04328v1,cs.CL
A Multilingual Sentiment Lexicon for Low-Resource Language Translation using Large Languages Models and Explainable AI,"South Africa and the Democratic Republic of Congo (DRC) present a complex
linguistic landscape with languages such as Zulu, Sepedi, Afrikaans, French,
English, and Tshiluba (Ciluba), which creates unique challenges for AI-driven
translation and sentiment analysis systems due to a lack of accurately labeled
data. This study seeks to address these challenges by developing a multilingual
lexicon designed for French and Tshiluba, now expanded to include translations
in English, Afrikaans, Sepedi, and Zulu. The lexicon enhances cultural
relevance in sentiment classification by integrating language-specific
sentiment scores. A comprehensive testing corpus is created to support
translation and sentiment analysis tasks, with machine learning models such as
Random Forest, Support Vector Machine (SVM), Decision Trees, and Gaussian Naive
Bayes (GNB) trained to predict sentiment across low resource languages (LRLs).
Among them, the Random Forest model performed particularly well, capturing
sentiment polarity and handling language-specific nuances effectively.
Furthermore, Bidirectional Encoder Representations from Transformers (BERT), a
Large Language Model (LLM), is applied to predict context-based sentiment with
high accuracy, achieving 99% accuracy and 98% precision, outperforming other
models. The BERT predictions were clarified using Explainable AI (XAI),
improving transparency and fostering confidence in sentiment classification.
Overall, findings demonstrate that the proposed lexicon and machine learning
models significantly enhance translation and sentiment analysis for LRLs in
South Africa and the DRC, laying a foundation for future AI models that support
underrepresented languages, with applications across education, governance, and
business in multilingual contexts.",2024-11-06,"Melusi Malinga, Isaac Lupanda, Mike Wa Nkongolo, Phil van Deventer",http://arxiv.org/pdf/2411.04316v1,cs.CL
Improving Bilingual Capabilities of Language Models to Support Diverse Linguistic Practices in Education,"Large language models (LLMs) offer promise in generating educational content,
providing instructor feedback, and reducing teacher workload on assessments.
While prior studies have focused on studying LLM-powered learning analytics,
limited research has examined how effective LLMs are in a bilingual context. In
this paper, we study the effectiveness of multilingual large language models
(MLLMs) across monolingual (English-only, Spanish-only) and bilingual
(Spanglish) student writing. We present a learning analytics use case that
details LLM performance in assessing acceptable and unacceptable explanations
of Science and Social Science concepts. Our findings reveal a significant bias
in the grading performance of pre-trained models for bilingual writing compared
to English-only and Spanish-only writing. Following this, we fine-tune
open-source MLLMs including Llama 3.1 and Mistral NeMo using synthetic datasets
generated in English, Spanish, and Spanglish. Our experiments indicate that the
models perform significantly better for all three languages after fine-tuning
with bilingual data. This study highlights the potential of enhancing MLLM
effectiveness to support authentic language practices amongst bilingual
learners. It also aims to illustrate the value of incorporating non-English
languages into the design and implementation of language models in education.",2024-11-06,"Anand Syamkumar, Nora Tseng, Kaycie Barron, Shanglin Yang, Shamya Karumbaiah, Rheeya Uppal, Junjie Hu",http://arxiv.org/pdf/2411.04308v1,cs.CL
A Capabilities Approach to Studying Bias and Harm in Language Technologies,"Mainstream Natural Language Processing (NLP) research has ignored the
majority of the world's languages. In moving from excluding the majority of the
world's languages to blindly adopting what we make for English, we first risk
importing the same harms we have at best mitigated and at least measured for
English. However, in evaluating and mitigating harms arising from adopting new
technologies into such contexts, we often disregard (1) the actual community
needs of Language Technologies, and (2) biases and fairness issues within the
context of the communities. In this extended abstract, we consider fairness,
bias, and inclusion in Language Technologies through the lens of the
Capabilities Approach. The Capabilities Approach centers on what people are
capable of achieving, given their intersectional social, political, and
economic contexts instead of what resources are (theoretically) available to
them. We detail the Capabilities Approach, its relationship to multilingual and
multicultural evaluation, and how the framework affords meaningful
collaboration with community members in defining and measuring the harms of
Language Technologies.",2024-11-06,"Hellina Hailu Nigatu, Zeerak Talat",http://arxiv.org/pdf/2411.04298v1,cs.CL
Unfair Alignment: Examining Safety Alignment Across Vision Encoder Layers in Vision-Language Models,"Vision-language models (VLMs) have improved significantly in multi-modal
tasks, but their more complex architecture makes their safety alignment more
challenging than the alignment of large language models (LLMs). In this paper,
we reveal an unfair distribution of safety across the layers of VLM's vision
encoder, with earlier and middle layers being disproportionately vulnerable to
malicious inputs compared to the more robust final layers. This 'cross-layer'
vulnerability stems from the model's inability to generalize its safety
training from the default architectural settings used during training to unseen
or out-of-distribution scenarios, leaving certain layers exposed. We conduct a
comprehensive analysis by projecting activations from various intermediate
layers and demonstrate that these layers are more likely to generate harmful
outputs when exposed to malicious inputs. Our experiments with LLaVA-1.5 and
Llama 3.2 show discrepancies in attack success rates and toxicity scores across
layers, indicating that current safety alignment strategies focused on a single
default layer are insufficient.",2024-11-06,"Saketh Bachu, Erfan Shayegani, Trishna Chakraborty, Rohit Lal, Arindam Dutta, Chengyu Song, Yue Dong, Nael Abu-Ghazaleh, Amit K. Roy-Chowdhury",http://arxiv.org/pdf/2411.04291v1,cs.CL
Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding,"Large language models (LLMs) have shown impressive capabilities, but still
struggle with complex reasoning tasks requiring multiple steps. While
prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at
inference time, optimizing reasoning capabilities during training remains
challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled
framework that formulates reasoning as sampling from a latent distribution and
optimizes it via variational approaches. LaTRO enables LLMs to concurrently
improve both their reasoning process and ability to evaluate reasoning quality,
without requiring external feedback or reward models. We validate LaTRO through
experiments on GSM8K and ARC-Challenge datasets using multiple model
architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of
12.5% over base models and 9.6% over supervised fine-tuning across
Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that
pre-trained LLMs possess latent reasoning capabilities that can be unlocked and
enhanced through our proposed optimization approach in a self-improvement
manner. The code of LaTRO is available at
\url{https://github.com/SalesforceAIResearch/LaTRO}.",2024-11-06,"Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, Huan Wang",http://arxiv.org/pdf/2411.04282v2,cs.CL
PhDGPT: Introducing a psychometric and linguistic dataset about how large language models perceive graduate students and professors in psychology,"Machine psychology aims to reconstruct the mindset of Large Language Models
(LLMs), i.e. how these artificial intelligences perceive and associate ideas.
This work introduces PhDGPT, a prompting framework and synthetic dataset that
encapsulates the machine psychology of PhD researchers and professors as
perceived by OpenAI's GPT-3.5. The dataset consists of 756,000 datapoints,
counting 300 iterations repeated across 15 academic events, 2 biological
genders, 2 career levels and 42 unique item responses of the Depression,
Anxiety, and Stress Scale (DASS-42). PhDGPT integrates these psychometric
scores with their explanations in plain language. This synergy of scores and
texts offers a dual, comprehensive perspective on the emotional well-being of
simulated academics, e.g. male/female PhD students or professors. By combining
network psychometrics and psycholinguistic dimensions, this study identifies
several similarities and distinctions between human and LLM data. The
psychometric networks of simulated male professors do not differ between
physical and emotional anxiety subscales, unlike humans. Other LLMs'
personification can reconstruct human DASS factors with a purity up to 80%.
Furthemore, LLM-generated personifications across different scenarios are found
to elicit explanations lower in concreteness and imageability in items coding
for anxiety, in agreement with past studies about human psychology. Our
findings indicate an advanced yet incomplete ability for LLMs to reproduce the
complexity of human psychometric data, unveiling convenient advantages and
limitations in using LLMs to replace human participants. PhDGPT also
intriguingly capture the ability for LLMs to adapt and change language patterns
according to prompted mental distress contextual features, opening new
quantitative opportunities for assessing the machine psychology of these
artificial intelligences.",2024-11-06,"Edoardo Sebastiano De Duro, Enrique Taietta, Riccardo Improta, Massimo Stella",http://arxiv.org/pdf/2411.10473v1,cs.CL
Diversity Helps Jailbreak Large Language Models,"We have uncovered a powerful jailbreak technique that leverages large
language models' ability to diverge from prior context, enabling them to bypass
safety constraints and generate harmful outputs. By simply instructing the LLM
to deviate and obfuscate previous attacks, our method dramatically outperforms
existing approaches, achieving up to a 62.83% higher success rate in
compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while
using only 12.9% of the queries. This revelation exposes a critical flaw in
current LLM safety training, suggesting that existing methods may merely mask
vulnerabilities rather than eliminate them. Our findings sound an urgent alarm
for the need to revolutionize testing methodologies to ensure robust and
reliable LLM security.",2024-11-06,"Weiliang Zhao, Daniel Ben-Levi, Wei Hao, Junfeng Yang, Chengzhi Mao",http://arxiv.org/pdf/2411.04223v3,cs.CL
Improving Radiology Report Conciseness and Structure via Local Large Language Models,"In this study, we aim to enhance radiology reporting by improving both the
conciseness and structured organization of findings (also referred to as
templating), specifically by organizing information according to anatomical
regions. This structured approach allows physicians to locate relevant
information quickly, increasing the report's utility. We utilize Large Language
Models (LLMs) such as Mixtral, Mistral, and Llama to generate concise,
well-structured reports. Among these, we primarily focus on the Mixtral model
due to its superior adherence to specific formatting requirements compared to
other models. To maintain data security and privacy, we run these LLMs locally
behind our institution's firewall. We leverage the LangChain framework and
apply five distinct prompting strategies to enforce a consistent structure in
radiology reports, aiming to eliminate extraneous language and achieve a high
level of conciseness. We also introduce a novel metric, the Conciseness
Percentage (CP) score, to evaluate report brevity. Our dataset comprises 814
radiology reports authored by seven board-certified body radiologists at our
cancer center. In evaluating the different prompting methods, we discovered
that the most effective approach for generating concise, well-structured
reports involves first instructing the LLM to condense the report, followed by
a prompt to structure the content according to specific guidelines. We assessed
all prompting strategies based on their ability to handle formatting issues,
reduce report length, and adhere to formatting instructions. Our findings
demonstrate that open-source, locally deployed LLMs can significantly improve
radiology report conciseness and structure while conforming to specified
formatting standards.",2024-11-06,"Iryna Hartsock, Cyrillo Araujo, Les Folio, Ghulam Rasool",http://arxiv.org/pdf/2411.05042v1,cs.CL
"Bottom-Up and Top-Down Analysis of Values, Agendas, and Observations in Corpora and LLMs","Large language models (LLMs) generate diverse, situated, persuasive texts
from a plurality of potential perspectives, influenced heavily by their prompts
and training data. As part of LLM adoption, we seek to characterize - and
ideally, manage - the socio-cultural values that they express, for reasons of
safety, accuracy, inclusion, and cultural fidelity. We present a validated
approach to automatically (1) extracting heterogeneous latent value
propositions from texts, (2) assessing resonance and conflict of values with
texts, and (3) combining these operations to characterize the pluralistic value
alignment of human-sourced and LLM-sourced textual data.",2024-11-06,"Scott E. Friedman, Noam Benkler, Drisana Mosaphir, Jeffrey Rye, Sonja M. Schmer-Galunder, Micah Goldwater, Matthew McLure, Ruta Wheelock, Jeremy Gottlieb, Robert P. Goldman, Christopher Miller",http://arxiv.org/pdf/2411.05040v1,cs.CL
Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?,"Several recent works seek to develop foundation models specifically for
medical applications, adapting general-purpose large language models (LLMs) and
vision-language models (VLMs) via continued pretraining on publicly available
biomedical corpora. These works typically claim that such domain-adaptive
pretraining (DAPT) improves performance on downstream medical tasks, such as
answering medical licensing exam questions. In this paper, we compare seven
public ""medical"" LLMs and two VLMs against their corresponding base models,
arriving at a different conclusion: all medical VLMs and nearly all medical
LLMs fail to consistently improve over their base models in the zero-/few-shot
prompting regime for medical question-answering (QA) tasks. For instance,
across the tasks and model pairs we consider in the 3-shot setting, medical
LLMs only outperform their base models in 12.1% of cases, reach a (statistical)
tie in 49.8% of cases, and are significantly worse than their base models in
the remaining 38.2% of cases. Our conclusions are based on (i) comparing each
medical model head-to-head, directly against the corresponding base model; (ii)
optimizing the prompts for each model separately; and (iii) accounting for
statistical uncertainty in comparisons. While these basic practices are not
consistently adopted in the literature, our ablations show that they
substantially impact conclusions. Our findings suggest that state-of-the-art
general-domain models may already exhibit strong medical knowledge and
reasoning capabilities, and offer recommendations to strengthen the conclusions
of future studies.",2024-11-06,"Daniel P. Jeong, Saurabh Garg, Zachary C. Lipton, Michael Oberst",http://arxiv.org/pdf/2411.04118v2,cs.CL
Self-Consistency Preference Optimization,"Self-alignment, whereby models learn to improve themselves without human
annotation, is a rapidly growing research area. However, existing techniques
often fail to improve complex reasoning tasks due to the difficulty of
assigning correct rewards. An orthogonal approach that is known to improve
correctness is self-consistency, a method applied at inference time based on
multiple sampling in order to find the most consistent answer. In this work, we
extend the self-consistency concept to help train models. We thus introduce
self-consistency preference optimization (ScPO), which iteratively trains
consistent answers to be preferred over inconsistent ones on unsupervised new
problems. We show ScPO leads to large improvements over conventional reward
model training on reasoning tasks such as GSM8K and MATH, closing the gap with
supervised training with gold answers or preferences, and that combining ScPO
with standard supervised learning improves results even further. On ZebraLogic,
ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and
Claude-3 Haiku.",2024-11-06,"Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, Jane Yu",http://arxiv.org/pdf/2411.04109v2,cs.CL
How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis,"Large language models (LLMs) have shown amazing performance on tasks that
require planning and reasoning. Motivated by this, we investigate the internal
mechanisms that underpin a network's ability to perform complex logical
reasoning. We first construct a synthetic propositional logic problem that
serves as a concrete test-bed for network training and evaluation. Crucially,
this problem demands nontrivial planning to solve. We perform our study on two
fronts. First, we pursue an understanding of precisely how a three-layer
transformer, trained from scratch and attains perfect test accuracy, solves
this problem. We are able to identify certain ""planning"" and ""reasoning""
mechanisms in the network that necessitate cooperation between the attention
blocks to implement the desired logic. Second, we study how pretrained LLMs,
namely Mistral-7B and Gemma-2-9B, solve this problem. We characterize their
reasoning circuits through causal intervention experiments, providing necessity
and sufficiency evidence for the circuits. We find evidence suggesting that the
two models' latent reasoning strategies are surprisingly similar, and
human-like. Overall, our work systemically uncovers novel aspects of small and
large transformers, and continues the study of how they plan and reason.",2024-11-06,"Guan Zhe Hong, Nishanth Dikkala, Enming Luo, Cyrus Rashtchian, Xin Wang, Rina Panigrahy",http://arxiv.org/pdf/2411.04105v3,cs.CL
Summarization of Opinionated Political Documents with Varied Perspectives,"Global partisan hostility and polarization has increased, and this
polarization is heightened around presidential elections. Models capable of
generating accurate summaries of diverse perspectives can help reduce such
polarization by exposing users to alternative perspectives. In this work, we
introduce a novel dataset and task for independently summarizing each political
perspective in a set of passages from opinionated news articles. For this task,
we propose a framework for evaluating different dimensions of perspective
summary performance. We benchmark 10 models of varying sizes and architectures
through both automatic and human evaluation. While recent models like GPT-4o
perform well on this task, we find that all models struggle to generate
summaries faithful to the intended perspective. Our analysis of summaries
focuses on how extraction behavior depends on the features of the input
documents.",2024-11-06,"Nicholas Deas, Kathleen McKeown",http://arxiv.org/pdf/2411.04093v1,cs.CL
A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement,"Content moderation typically combines the efforts of human moderators and
machine learning models. However, these systems often rely on data where
significant disagreement occurs during moderation, reflecting the subjective
nature of toxicity perception. Rather than dismissing this disagreement as
noise, we interpret it as a valuable signal that highlights the inherent
ambiguity of the content,an insight missed when only the majority label is
considered. In this work, we introduce a novel content moderation framework
that emphasizes the importance of capturing annotation disagreement. Our
approach uses multitask learning, where toxicity classification serves as the
primary task and annotation disagreement is addressed as an auxiliary task.
Additionally, we leverage uncertainty estimation techniques, specifically
Conformal Prediction, to account for both the ambiguity in comment annotations
and the model's inherent uncertainty in predicting toxicity and
disagreement.The framework also allows moderators to adjust thresholds for
annotation disagreement, offering flexibility in determining when ambiguity
should trigger a review. We demonstrate that our joint approach enhances model
performance, calibration, and uncertainty estimation, while offering greater
parameter efficiency and improving the review process in comparison to
single-task methods.",2024-11-06,"Guillermo Villate-Castillo, Javier Del Ser, Borja Sanz",http://arxiv.org/pdf/2411.04090v2,cs.CL
YouTube Comments Decoded: Leveraging LLMs for Low Resource Language Classification,"Sarcasm detection is a significant challenge in sentiment analysis,
particularly due to its nature of conveying opinions where the intended meaning
deviates from the literal expression. This challenge is heightened in social
media contexts where code-mixing, especially in Dravidian languages, is
prevalent. Code-mixing involves the blending of multiple languages within a
single utterance, often with non-native scripts, complicating the task for
systems trained on monolingual data. This shared task introduces a novel gold
standard corpus designed for sarcasm and sentiment detection within code-mixed
texts, specifically in Tamil-English and Malayalam-English languages. The
primary objective of this task is to identify sarcasm and sentiment polarity
within a code-mixed dataset of Tamil-English and Malayalam-English comments and
posts collected from social media platforms. Each comment or post is annotated
at the message level for sentiment polarity, with particular attention to the
challenges posed by class imbalance, reflecting real-world scenarios.In this
work, we experiment with state-of-the-art large language models like GPT-3.5
Turbo via prompting to classify comments into sarcastic or non-sarcastic
categories. We obtained a macro-F1 score of 0.61 for Tamil language. We
obtained a macro-F1 score of 0.50 for Malayalam language.",2024-11-06,"Aniket Deroy, Subhankar Maity",http://arxiv.org/pdf/2411.05039v2,cs.CL
M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models,"Existing benchmarks for evaluating foundation models mainly focus on
single-document, text-only tasks. However, they often fail to fully capture the
complexity of research workflows, which typically involve interpreting
non-textual data and gathering information across multiple documents. To
address this gap, we introduce M3SciQA, a multi-modal, multi-document
scientific question answering benchmark designed for a more comprehensive
evaluation of foundation models. M3SciQA consists of 1,452 expert-annotated
questions spanning 70 natural language processing paper clusters, where each
cluster represents a primary paper along with all its cited documents,
mirroring the workflow of comprehending a single paper by requiring multi-modal
and multi-document data. With M3SciQA, we conduct a comprehensive evaluation of
18 foundation models. Our results indicate that current foundation models still
significantly underperform compared to human experts in multi-modal information
retrieval and in reasoning across multiple scientific documents. Additionally,
we explore the implications of these findings for the future advancement of
applying foundation models in multi-modal scientific literature analysis.",2024-11-06,"Chuhan Li, Ziyao Shangguan, Yilun Zhao, Deyuan Li, Yixin Liu, Arman Cohan",http://arxiv.org/pdf/2411.04075v1,cs.CL
Beemo: Benchmark of Expert-edited Machine-generated Outputs,"The rapid proliferation of large language models (LLMs) has increased the
volume of machine-generated texts (MGTs) and blurred text authorship in various
domains. However, most existing MGT benchmarks include single-author texts
(human-written and machine-generated). This conventional design fails to
capture more practical multi-author scenarios, where the user refines the LLM
response for natural flow, coherence, and factual correctness. Our paper
introduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),
which includes 6.5k texts written by humans, generated by ten
instruction-finetuned LLMs, and edited by experts for various use cases,
ranging from creative writing to summarization. Beemo additionally comprises
13.1k machine-generated and LLM-edited texts, allowing for diverse MGT
detection evaluation across various edit types. We document Beemo's creation
protocol and present the results of benchmarking 33 configurations of MGT
detectors in different experimental setups. We find that expert-based editing
evades MGT detection, while LLM-edited texts are unlikely to be recognized as
human-written. Beemo and all materials are publicly available.",2024-11-06,"Ekaterina Artemova, Jason Lucas, Saranya Venkatraman, Jooyoung Lee, Sergei Tilga, Adaku Uchendu, Vladislav Mikhailov",http://arxiv.org/pdf/2411.04032v3,cs.CL
Towards Interpreting Language Models: A Case Study in Multi-Hop Reasoning,"Answering multi-hop reasoning questions requires retrieving and synthesizing
information from diverse sources. Language models (LMs) struggle to perform
such reasoning consistently. We propose an approach to pinpoint and rectify
multi-hop reasoning failures through targeted memory injections on LM attention
heads. First, we analyze the per-layer activations of GPT-2 models in response
to single- and multi-hop prompts. We then propose a mechanism that allows users
to inject relevant prompt-specific information, which we refer to as
""memories,"" at critical LM locations during inference. By thus enabling the LM
to incorporate additional relevant information during inference, we enhance the
quality of multi-hop prompt completions. We empirically show that a simple,
efficient, and targeted memory injection into a key attention layer often
increases the probability of the desired next token in multi-hop tasks, by up
to 424%. We observe that small subsets of attention heads can significantly
impact the model prediction during multi-hop reasoning. To more faithfully
interpret these heads, we develop Attention Lens: an open source tool that
translates the outputs of attention heads into vocabulary tokens via learned
transformations called lenses. We demonstrate the use of lenses to reveal how a
model arrives at its answer and use them to localize sources of model failures
such as in the case of biased and malicious language generation.",2024-11-06,Mansi Sakarvadia,http://arxiv.org/pdf/2411.05037v1,cs.CL
Prompt Engineering Using GPT for Word-Level Code-Mixed Language Identification in Low-Resource Dravidian Languages,"Language Identification (LI) is crucial for various natural language
processing tasks, serving as a foundational step in applications such as
sentiment analysis, machine translation, and information retrieval. In
multilingual societies like India, particularly among the youth engaging on
social media, text often exhibits code-mixing, blending local languages with
English at different linguistic levels. This phenomenon presents formidable
challenges for LI systems, especially when languages intermingle within single
words. Dravidian languages, prevalent in southern India, possess rich
morphological structures yet suffer from under-representation in digital
platforms, leading to the adoption of Roman or hybrid scripts for
communication. This paper introduces a prompt based method for a shared task
aimed at addressing word-level LI challenges in Dravidian languages. In this
work, we leveraged GPT-3.5 Turbo to understand whether the large language
models is able to correctly classify words into correct categories. Our
findings show that the Kannada model consistently outperformed the Tamil model
across most metrics, indicating a higher accuracy and reliability in
identifying and categorizing Kannada language instances. In contrast, the Tamil
model showed moderate performance, particularly needing improvement in
precision and recall.",2024-11-06,"Aniket Deroy, Subhankar Maity",http://arxiv.org/pdf/2411.04025v2,cs.CL
"From Word Vectors to Multimodal Embeddings: Techniques, Applications, and Future Directions For Large Language Models","Word embeddings and language models have transformed natural language
processing (NLP) by facilitating the representation of linguistic elements in
continuous vector spaces. This review visits foundational concepts such as the
distributional hypothesis and contextual similarity, tracing the evolution from
sparse representations like one-hot encoding to dense embeddings including
Word2Vec, GloVe, and fastText. We examine both static and contextualized
embeddings, underscoring advancements in models such as ELMo, BERT, and GPT and
their adaptations for cross-lingual and personalized applications. The
discussion extends to sentence and document embeddings, covering aggregation
methods and generative topic models, along with the application of embeddings
in multimodal domains, including vision, robotics, and cognitive science.
Advanced topics such as model compression, interpretability, numerical
encoding, and bias mitigation are analyzed, addressing both technical
challenges and ethical implications. Additionally, we identify future research
directions, emphasizing the need for scalable training techniques, enhanced
interpretability, and robust grounding in non-textual modalities. By
synthesizing current methodologies and emerging trends, this survey offers
researchers and practitioners an in-depth resource to push the boundaries of
embedding-based language models.",2024-11-06,"Charles Zhang, Benji Peng, Xintian Sun, Qian Niu, Junyu Liu, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Ming Liu, Yichao Zhang, Cheng Fei, Caitlyn Heqi Yin, Lawrence KQ Yan, Tianyang Wang",http://arxiv.org/pdf/2411.05036v1,cs.CL
LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation for Design Space Exploration,"GraphRAG integrates (knowledge) graphs with large language models (LLMs) to
improve reasoning accuracy and contextual relevance. Despite its promising
applications and strong relevance to multiple research communities, such as
databases and natural language processing, GraphRAG currently lacks modular
workflow analysis, systematic solution frameworks, and insightful empirical
studies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework
that enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)
systematic classification of existing techniques and implemented GraphRAG
instances, and 3) creation of new GraphRAG instances. Our framework facilitates
comprehensive empirical studies of GraphRAG on large-scale real-world graphs
and diverse query sets, revealing insights into balancing reasoning quality,
runtime efficiency, and token or GPU cost, that are essential for building
advanced GraphRAG systems.",2024-11-06,"Yukun Cao, Zengyi Gao, Zhiyang Li, Xike Xie, Kevin Zhou, Jianliang Xu",http://arxiv.org/pdf/2411.05844v2,cs.CL
WorryWords: Norms of Anxiety Association for over 44k English Words,"Anxiety, the anticipatory unease about a potential negative outcome, is a
common and beneficial human emotion. However, there is still much that is not
known, such as how anxiety relates to our body and how it manifests in
language. This is especially pertinent given the increasing impact of
anxiety-related disorders. In this work, we introduce WorryWords, the first
large-scale repository of manually derived word--anxiety associations for over
44,450 English words. We show that the anxiety associations are highly
reliable. We use WorryWords to study the relationship between anxiety and other
emotion constructs, as well as the rate at which children acquire anxiety words
with age. Finally, we show that using WorryWords alone, one can accurately
track the change of anxiety in streams of text. The lexicon enables a wide
variety of anxiety-related research in psychology, NLP, public health, and
social sciences. WorryWords (and its translations to over 100 languages) is
freely available. http://saifmohammad.com/worrywords.html",2024-11-06,Saif M. Mohammad,http://arxiv.org/pdf/2411.03966v1,cs.CL
What Really is Commonsense Knowledge?,"Commonsense datasets have been well developed in Natural Language Processing,
mainly through crowdsource human annotation. However, there are debates on the
genuineness of commonsense reasoning benchmarks. In specific, a significant
portion of instances in some commonsense benchmarks do not concern commonsense
knowledge. That problem would undermine the measurement of the true commonsense
reasoning ability of evaluated models. It is also suggested that the problem
originated from a blurry concept of commonsense knowledge, as distinguished
from other types of knowledge. To demystify all of the above claims, in this
study, we survey existing definitions of commonsense knowledge, ground into the
three frameworks for defining concepts, and consolidate them into a
multi-framework unified definition of commonsense knowledge (so-called
consolidated definition). We then use the consolidated definition for
annotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets
to examine the above claims. Our study shows that there exists a large portion
of non-commonsense-knowledge instances in the two datasets, and a large
performance gap on these two subsets where Large Language Models (LLMs) perform
worse on commonsense-knowledge instances.",2024-11-06,"Quyet V. Do, Junze Li, Tung-Duong Vuong, Zhaowei Wang, Yangqiu Song, Xiaojuan Ma",http://arxiv.org/pdf/2411.03964v1,cs.CL
How Does A Text Preprocessing Pipeline Affect Ontology Syntactic Matching?,"The classic text preprocessing pipeline, comprising Tokenisation,
Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been
implemented in many systems for syntactic ontology matching (OM). However, the
lack of standardisation in text preprocessing creates diversity in mapping
results. In this paper, we investigate the effect of the text preprocessing
pipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)
tracks with 49 distinct alignments. We find that Phase 1 text preprocessing
(Tokenisation and Normalisation) is more effective than Phase 2 text
preprocessing (Stop Words Removal and Stemming/Lemmatisation). We propose two
novel approaches to repair unwanted false mappings caused by Phase 2 text
preprocessing. One is an ad hoc logic-based repair approach that employs an
ontology-specific check to find common words that cause false mappings. These
words are stored in a reserved word set and applied before the text
preprocessing. By leveraging the power of large language models (LLMs), we also
propose a post hoc LLM-based repair approach. This approach utilises the strong
background knowledge provided by LLMs to repair non-existent and
counter-intuitive false mappings after the text preprocessing. It also
overcomes the tendency towards unstable true mappings by injecting the
classical text preprocessing pipeline via function calling. The experimental
results show that these two approaches can improve the matching correctness and
the overall matching performance.",2024-11-06,"Zhangcheng Qiang, Kerry Taylor, Weiqing Wang",http://arxiv.org/pdf/2411.03962v6,cs.CL
Mitigating Privacy Risks in LLM Embeddings from Embedding Inversion,"Embeddings have become a cornerstone in the functionality of large language
models (LLMs) due to their ability to transform text data into rich, dense
numerical representations that capture semantic and syntactic properties. These
embedding vector databases serve as the long-term memory of LLMs, enabling
efficient handling of a wide range of natural language processing tasks.
However, the surge in popularity of embedding vector databases in LLMs has been
accompanied by significant concerns about privacy leakage. Embedding vector
databases are particularly vulnerable to embedding inversion attacks, where
adversaries can exploit the embeddings to reverse-engineer and extract
sensitive information from the original text data. Existing defense mechanisms
have shown limitations, often struggling to balance security with the
performance of downstream tasks. To address these challenges, we introduce
Eguard, a novel defense mechanism designed to mitigate embedding inversion
attacks. Eguard employs a transformer-based projection network and text mutual
information optimization to safeguard embeddings while preserving the utility
of LLMs. Our approach significantly reduces privacy risks, protecting over 95%
of tokens from inversion while maintaining high performance across downstream
tasks consistent with original embeddings.",2024-11-06,"Tiantian Liu, Hongwei Yao, Tong Wu, Zhan Qin, Feng Lin, Kui Ren, Chun Chen",http://arxiv.org/pdf/2411.05034v1,cs.CL
Interactions Across Blocks in Post-Training Quantization of Large Language Models,"Post-training quantization is widely employed to reduce the computational
demands of neural networks. Typically, individual substructures, such as layers
or blocks of layers, are quantized with the objective of minimizing
quantization errors in their pre-activations by fine-tuning the corresponding
weights. Deriving this local objective from the global objective of minimizing
task loss involves two key simplifications: assuming substructures are mutually
independent and ignoring the knowledge of subsequent substructures as well as
the task loss. In this work, we assess the effects of these simplifications on
weight-only quantization of large language models. We introduce two multi-block
fine-tuning strategies and compare them against the baseline of fine-tuning
single transformer blocks. The first captures correlations of weights across
blocks by jointly optimizing multiple quantized blocks. The second incorporates
knowledge of subsequent blocks by minimizing the error in downstream
pre-activations rather than focusing solely on the quantized block. Our
findings indicate that the effectiveness of these methods depends on the
specific network model, with no impact on some models but demonstrating
significant benefits for others.",2024-11-06,"Khasmamad Shabanovi, Lukas Wiest, Vladimir Golkov, Daniel Cremers, Thomas Pfeil",http://arxiv.org/pdf/2411.03934v1,cs.CL
Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?,"Hampering the interpretation of benchmark scores, evaluation data
contamination has become a growing concern in the evaluation of LLMs, and an
active area of research studies its effects. While evaluation data
contamination is easily understood intuitively, it is surprisingly difficult to
define precisely which samples should be considered contaminated and,
consequently, how it impacts benchmark scores. We propose that these questions
should be addressed together and that contamination metrics can be assessed
based on whether models benefit from the examples they mark contaminated. We
propose a novel analysis method called ConTAM, and show with a large scale
survey of existing and novel n-gram based contamination metrics across 13
benchmarks and 7 models from 2 different families that ConTAM can be used to
better understand evaluation data contamination and its effects. We find that
contamination may have a much larger effect than reported in recent LLM
releases and benefits models differently at different scales. We also find that
considering only the longest contaminated substring provides a better signal
than considering a union of all contaminated substrings, and that doing model
and benchmark specific threshold analysis greatly increases the specificity of
the results. Lastly, we investigate the impact of hyperparameter choices,
finding that, among other things, both using larger values of n and
disregarding matches that are infrequent in the pre-training data lead to many
false negatives. With ConTAM, we provide a method to empirically ground
evaluation data contamination metrics in downstream effects. With our
exploration, we shed light on how evaluation data contamination can impact LLMs
and provide insight into the considerations important when doing contamination
analysis. We end our paper by discussing these in more detail and providing
concrete suggestions for future work.",2024-11-06,"Aaditya K. Singh, Muhammed Yusuf Kocyigit, Andrew Poulton, David Esiobu, Maria Lomeli, Gergely Szilvasy, Dieuwke Hupkes",http://arxiv.org/pdf/2411.03923v1,cs.CL
RAGulator: Lightweight Out-of-Context Detectors for Grounded Text Generation,"Real-time detection of out-of-context LLM outputs is crucial for enterprises
looking to safely adopt RAG applications. In this work, we train lightweight
models to discriminate LLM-generated text that is semantically out-of-context
from retrieved text documents. We preprocess a combination of summarisation and
semantic textual similarity datasets to construct training data using minimal
resources. We find that DeBERTa is not only the best-performing model under
this pipeline, but it is also fast and does not require additional text
preprocessing or feature engineering. While emerging work demonstrates that
generative LLMs can also be fine-tuned and used in complex data pipelines to
achieve state-of-the-art performance, we note that speed and resource limits
are important considerations for on-premise deployment.",2024-11-06,"Ian Poey, Jiajun Liu, Qishuai Zhong, Adrien Chenailler",http://arxiv.org/pdf/2411.03920v1,cs.CL
Analyzing Multimodal Features of Spontaneous Voice Assistant Commands for Mild Cognitive Impairment Detection,"Mild cognitive impairment (MCI) is a major public health concern due to its
high risk of progressing to dementia. This study investigates the potential of
detecting MCI with spontaneous voice assistant (VA) commands from 35 older
adults in a controlled setting. Specifically, a command-generation task is
designed with pre-defined intents for participants to freely generate commands
that are more associated with cognitive ability than read commands. We develop
MCI classification and regression models with audio, textual, intent, and
multimodal fusion features. We find the command-generation task outperforms the
command-reading task with an average classification accuracy of 82%, achieved
by leveraging multimodal fusion features. In addition, generated commands
correlate more strongly with memory and attention subdomains than read
commands. Our results confirm the effectiveness of the command-generation task
and imply the promise of using longitudinal in-home commands for MCI detection.",2024-11-06,"Nana Lin, Youxiang Zhu, Xiaohui Liang, John A. Batsis, Caroline Summerour",http://arxiv.org/pdf/2411.04158v1,cs.CL
Lexicalization Is All You Need: Examining the Impact of Lexical Knowledge in a Compositional QALD System,"In this paper, we examine the impact of lexicalization on Question Answering
over Linked Data (QALD). It is well known that one of the key challenges in
interpreting natural language questions with respect to SPARQL lies in bridging
the lexical gap, that is mapping the words in the query to the correct
vocabulary elements. We argue in this paper that lexicalization, that is
explicit knowledge about the potential interpretations of a word with respect
to the given vocabulary, significantly eases the task and increases the
performance of QA systems. Towards this goal, we present a compositional QA
system that can leverage explicit lexical knowledge in a compositional manner
to infer the meaning of a question in terms of a SPARQL query. We show that
such a system, given lexical knowledge, has a performance well beyond current
QA systems, achieving up to a $35.8\%$ increase in the micro $F_1$ score
compared to the best QA system on QALD-9. This shows the importance and
potential of including explicit lexical knowledge. In contrast, we show that
LLMs have limited abilities to exploit lexical knowledge, with only marginal
improvements compared to a version without lexical knowledge. This shows that
LLMs have no ability to compositionally interpret a question on the basis of
the meaning of its parts, a key feature of compositional approaches. Taken
together, our work shows new avenues for QALD research, emphasizing the
importance of lexicalization and compositionality.",2024-11-06,"David Maria Schmidt, Mohammad Fazleh Elahi, Philipp Cimiano",http://arxiv.org/pdf/2411.03906v2,cs.CL
Computational Analysis of Gender Depiction in the Comedias of Calderón de la Barca,"In theatre, playwrights use the portrayal of characters to explore culturally
based gender norms. In this paper, we develop quantitative methods to study
gender depiction in the non-religious works (comedias) of Pedro Calder\'on de
la Barca, a prolific Spanish 17th century author. We gather insights from a
corpus of more than 100 plays by using a gender classifier and applying model
explainability (attribution) methods to determine which text features are most
influential in the model's decision to classify speech as 'male' or 'female',
indicating the most gendered elements of dialogue in Calder\'on's comedias in a
human accessible manner. We find that female and male characters are portrayed
differently and can be identified by the gender prediction model at practically
useful accuracies (up to f=0.83). Analysis reveals semantic aspects of gender
portrayal, and demonstrates that the model is even useful in providing a
relatively accurate scene-by-scene prediction of cross-dressing characters.",2024-11-06,"Allison Keith, Antonio Rojas Castro, Sebastian Padó",http://arxiv.org/pdf/2411.03895v1,cs.CL
"Multi3Hate: Multimodal, Multilingual, and Multicultural Hate Speech Detection with Vision-Language Models","Warning: this paper contains content that may be offensive or upsetting
  Hate speech moderation on global platforms poses unique challenges due to the
multimodal and multilingual nature of content, along with the varying cultural
perceptions. How well do current vision-language models (VLMs) navigate these
nuances? To investigate this, we create the first multimodal and multilingual
parallel hate speech dataset, annotated by a multicultural set of annotators,
called Multi3Hate. It contains 300 parallel meme samples across 5 languages:
English, German, Spanish, Hindi, and Mandarin. We demonstrate that cultural
background significantly affects multimodal hate speech annotation in our
dataset. The average pairwise agreement among countries is just 74%,
significantly lower than that of randomly selected annotator groups. Our
qualitative analysis indicates that the lowest pairwise label agreement-only
67% between the USA and India-can be attributed to cultural factors. We then
conduct experiments with 5 large VLMs in a zero-shot setting, finding that
these models align more closely with annotations from the US than with those
from other cultures, even when the memes and prompts are presented in the
dominant language of the other culture. Code and dataset are available at
https://github.com/MinhDucBui/Multi3Hate.",2024-11-06,"Minh Duc Bui, Katharina von der Wense, Anne Lauscher",http://arxiv.org/pdf/2411.03888v2,cs.CL
Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models,"Transformers have found extensive applications across various domains due to
the powerful fitting capabilities. This success can be partially attributed to
their inherent nonlinearity. Thus, in addition to the ReLU function employed in
the original transformer architecture, researchers have explored alternative
modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment
representational capacity. In this paper, we propose a novel category of
polynomial composition activations (PolyCom), designed to optimize the dynamics
of transformers. Theoretically, we provide a comprehensive mathematical
analysis of PolyCom, highlighting its enhanced expressivity and efficacy
relative to other activation functions. Notably, we demonstrate that networks
incorporating PolyCom achieve the $\textbf{optimal approximation rate}$,
indicating that PolyCom networks require minimal parameters to approximate
general smooth functions in Sobolev spaces. We conduct empirical experiments on
the pre-training configurations of large language models (LLMs), including both
dense and sparse architectures. By substituting conventional activation
functions with PolyCom, we enable LLMs to capture higher-order interactions
within the data, thus improving performance metrics in terms of accuracy and
convergence rates. Extensive experimental results demonstrate the effectiveness
of our method, showing substantial improvements over other activation
functions. Code is available at https://github.com/BryceZhuo/PolyCom.",2024-11-06,"Zhijian Zhuo, Ya Wang, Yutao Zeng, Xiaoqing Li, Xun Zhou, Jinwen Ma",http://arxiv.org/pdf/2411.03884v3,cs.CL
MEG: Medical Knowledge-Augmented Large Language Models for Question Answering,"Question answering is a natural language understanding task that involves
reasoning over both explicit context, and unstated relevant domain knowledge.
Despite the high cost of training, large language models (LLMs) -- the backbone
of most modern question-answering systems -- still struggle to reliably capture
the nuanced relationships between concepts that are crucial for reasoning in
specialized fields like medicine. In this work, we present MEG, a
parameter-efficient approach for medical knowledge-augmented LLMs. MEG uses a
lightweight mapping network to incorporate knowledge graph embeddings into the
LLM, enabling it to leverage external knowledge in a cost-effective way. We
evaluate our method on four popular medical multiple-choice datasets and show
that LLMs i) can effectively interpret knowledge graph embeddings and ii) gain
significant advantages from the factual grounding these embeddings provide. MEG
attains an average of +6.7% and +9.9% accuracy over specialized models like
BioMistral-7B and MediTron-7B, respectively. Finally, we show that MEG's
performance remains robust to the choice of graph encoder.",2024-11-06,"Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders Søgaard, Carlos Bobed",http://arxiv.org/pdf/2411.03883v3,cs.CL
"Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the Way Forward","Recent research has demonstrated that training a linear connector between
speech foundation encoders and large language models (LLMs) enables this
architecture to achieve strong ASR capabilities. Despite the impressive
results, it remains unclear whether these simple approaches are robust enough
across different scenarios and speech conditions, such as domain shifts and
speech perturbations. In this paper, we address these questions by conducting
various ablation experiments using a recent and widely adopted approach called
SLAM-ASR. We present novel empirical findings that offer insights on how to
effectively utilize the SLAM-ASR architecture across a wide range of settings.
Our main findings indicate that SLAM-ASR exhibits poor performance in
cross-domain evaluation settings. Additionally, speech perturbations on
in-domain data, such as changes in speech rate or additive noise, can
significantly degrade performance. Our findings offer critical insights for
fine-tuning and configuring robust LLM-based ASR models, tailored to different
data characteristics and computational resources.",2024-11-06,"Shashi Kumar, Iuliia Thorbecke, Sergio Burdisso, Esaú Villatoro-Tello, Manjunath K E, Kadri Hacioğlu, Pradeep Rangappa, Petr Motlicek, Aravind Ganapathiraju, Andreas Stolcke",http://arxiv.org/pdf/2411.03866v2,cs.CL
MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba,"An ecosystem of Transformer-based models has been established by building
large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a
crucial technology for deploying these models to downstream tasks with minimal
cost while achieving effective performance. Recently, Mamba, a State Space
Model (SSM)-based model, has attracted attention as a potential alternative to
Transformers. While many large-scale Mamba-based models have been proposed,
efficiently adapting pre-trained Mamba-based models to downstream tasks remains
unexplored. In this paper, we conduct an exploratory analysis of PEFT methods
for Mamba. We investigate the effectiveness of existing PEFT methods for
Transformers when applied to Mamba. We also modify these methods to better
align with the Mamba architecture. Additionally, we propose new Mamba-specific
PEFT methods that leverage the distinctive structure of Mamba. Our experiments
indicate that PEFT performs more effectively for Mamba than Transformers.
Lastly, we demonstrate how to effectively combine multiple PEFT methods and
provide a framework that outperforms previous works. To ensure reproducibility,
we will release the code after publication.",2024-11-06,"Masakazu Yoshimura, Teruaki Hayashi, Yota Maeda",http://arxiv.org/pdf/2411.03855v3,cs.CL
Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination,"The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting models' contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is quite effective and sensitive in identifying varying degrees
of contamination, and can highlight significant performance improvements due to
the leakage of multimodal benchmark training sets. Furthermore, we explore
whether the contamination originates from the base LLMs used by MLLMs or the
multimodal training phase, providing new insights into the stages at which
contamination may be introduced.",2024-11-06,"Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang",http://arxiv.org/pdf/2411.03823v2,cs.CL
From Novice to Expert: LLM Agent Policy Optimization via Step-wise Reinforcement Learning,"The outstanding capabilities of large language models (LLMs) render them a
crucial component in various autonomous agent systems. While traditional
methods depend on the inherent knowledge of LLMs without fine-tuning, more
recent approaches have shifted toward the reinforcement learning strategy to
further enhance agents' ability to solve complex interactive tasks with
environments and tools. However, previous approaches are constrained by the
sparse reward issue, where existing datasets solely provide a final scalar
reward for each multi-step reasoning chain, potentially leading to
ineffectiveness and inefficiency in policy learning. In this paper, we
introduce StepAgent, which utilizes step-wise reward to optimize the agent's
reinforcement learning process. Inheriting the spirit of novice-to-expert
theory, we first compare the actions of the expert and the agent to
automatically generate intermediate rewards for fine-grained optimization.
Additionally, we propose implicit-reward and inverse reinforcement learning
techniques to facilitate agent reflection and policy adjustment. Further
theoretical analysis demonstrates that the action distribution of the agent can
converge toward the expert action distribution over multiple training cycles.
Experimental results across various datasets indicate that StepAgent
outperforms existing baseline methods.",2024-11-06,"Zhirui Deng, Zhicheng Dou, Yutao Zhu, Ji-Rong Wen, Ruibin Xiong, Mang Wang, Weipeng Chen",http://arxiv.org/pdf/2411.03817v3,cs.CL
MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue,"Large Language Models (LLMs) demonstrate outstanding performance in their
reservoir of knowledge and understanding capabilities, but they have also been
shown to be prone to illegal or unethical reactions when subjected to jailbreak
attacks. To ensure their responsible deployment in critical applications, it is
crucial to understand the safety capabilities and vulnerabilities of LLMs.
Previous works mainly focus on jailbreak in single-round dialogue, overlooking
the potential jailbreak risks in multi-round dialogues, which are a vital way
humans interact with and extract information from LLMs. Some studies have
increasingly concentrated on the risks associated with jailbreak in multi-round
dialogues. These efforts typically involve the use of manually crafted
templates or prompt engineering techniques. However, due to the inherent
complexity of multi-round dialogues, their jailbreak performance is limited. To
solve this problem, we propose a novel multi-round dialogue jailbreaking agent,
emphasizing the importance of stealthiness in identifying and mitigating
potential threats to human values posed by LLMs. We propose a risk
decomposition strategy that distributes risks across multiple rounds of queries
and utilizes psychological strategies to enhance attack strength. Extensive
experiments show that our proposed method surpasses other attack methods and
achieves state-of-the-art attack success rate. We will make the corresponding
code and dataset available for future research. The code will be released soon.",2024-11-06,"Fengxiang Wang, Ranjie Duan, Peng Xiao, Xiaojun Jia, Shiji Zhao, Cheng Wei, YueFeng Chen, Chongwen Wang, Jialing Tao, Hang Su, Jun Zhu, Hui Xue",http://arxiv.org/pdf/2411.03814v2,cs.CL
Crystal: Illuminating LLM Abilities on Language and Code,"Large Language Models (LLMs) specializing in code generation (which are also
often referred to as code LLMs), e.g., StarCoder and Code Llama, play
increasingly critical roles in various software development scenarios. It is
also crucial for code LLMs to possess both code generation and natural language
abilities for many specific applications, such as code snippet retrieval using
natural language or code explanations. The intricate interaction between
acquiring language and coding skills complicates the development of strong code
LLMs. Furthermore, there is a lack of thorough prior studies on the LLM
pretraining strategy that mixes code and natural language. In this work, we
propose a pretraining strategy to enhance the integration of natural language
and coding capabilities within a single LLM. Specifically, it includes two
phases of training with appropriately adjusted code/language ratios. The
resulting model, Crystal, demonstrates remarkable capabilities in both domains.
Specifically, it has natural language and coding performance comparable to that
of Llama 2 and Code Llama, respectively. Crystal exhibits better data
efficiency, using 1.4 trillion tokens compared to the more than 2 trillion
tokens used by Llama 2 and Code Llama. We verify our pretraining strategy by
analyzing the training process and observe consistent improvements in most
benchmarks. We also adopted a typical application adaptation phase with a
code-centric data mixture, only to find that it did not lead to enhanced
performance or training efficiency, underlining the importance of a carefully
designed data recipe. To foster research within the community, we commit to
open-sourcing every detail of the pretraining, including our training datasets,
code, loggings and 136 checkpoints throughout the training.",2024-11-06,"Tianhua Tao, Junbo Li, Bowen Tan, Hongyi Wang, William Marshall, Bhargav M Kanakiya, Joel Hestness, Natalia Vassilieva, Zhiqiang Shen, Eric P. Xing, Zhengzhong Liu",http://arxiv.org/pdf/2411.04156v1,cs.CL
The natural stability of autonomous morphology,"Autonomous morphology, such as inflection class systems and paradigmatic
distribution patterns, is widespread and diachronically resilient in natural
language. Why this should be so has remained unclear given that autonomous
morphology imposes learning costs, offers no clear benefit relative to its
absence and could easily be removed by the analogical forces which are
constantly reshaping it. Here we propose an explanation for the resilience of
autonomous morphology, in terms of a diachronic dynamic of attraction and
repulsion between morphomic categories, which emerges spontaneously from a
simple paradigm cell filling process. Employing computational evolutionary
models, our key innovation is to bring to light the role of `dissociative
evidence', i.e., evidence for inflectional distinctiveness which a rational
reasoner will have access to during analogical inference. Dissociative evidence
creates a repulsion dynamic which prevents morphomic classes from collapsing
together entirely, i.e., undergoing complete levelling. As we probe alternative
models, we reveal the limits of conditional entropy as a measure for
predictability in systems that are undergoing change. Finally, we demonstrate
that autonomous morphology, far from being `unnatural' (e.g.
\citealt{Aronoff1994}), is rather the natural (emergent) consequence of a
natural (rational) process of inference applied to inflectional systems.",2024-11-06,"Erich Round, Louise Esher, Sacha Beniamine",http://arxiv.org/pdf/2411.03811v1,cs.CL
Understanding the Effects of Human-written Paraphrases in LLM-generated Text Detection,"Natural Language Generation has been rapidly developing with the advent of
large language models (LLMs). While their usage has sparked significant
attention from the general public, it is important for readers to be aware when
a piece of text is LLM-generated. This has brought about the need for building
models that enable automated LLM-generated text detection, with the aim of
mitigating potential negative outcomes of such content. Existing LLM-generated
detectors show competitive performances in telling apart LLM-generated and
human-written text, but this performance is likely to deteriorate when
paraphrased texts are considered. In this study, we devise a new data
collection strategy to collect Human & LLM Paraphrase Collection (HLPC), a
first-of-its-kind dataset that incorporates human-written texts and
paraphrases, as well as LLM-generated texts and paraphrases. With the aim of
understanding the effects of human-written paraphrases on the performance of
state-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark
detectors, we perform classification experiments that incorporate human-written
paraphrases, watermarked and non-watermarked LLM-generated documents from GPT
and OPT, and LLM-generated paraphrases from DIPPER and BART. The results show
that the inclusion of human-written paraphrases has a significant impact of
LLM-generated detector performance, promoting TPR@1%FPR with a possible
trade-off of AUROC and accuracy.",2024-11-06,"Hiu Ting Lau, Arkaitz Zubiaga",http://arxiv.org/pdf/2411.03806v1,cs.CL
A Comparative Study of Recent Large Language Models on Generating Hospital Discharge Summaries for Lung Cancer Patients,"Generating discharge summaries is a crucial yet time-consuming task in
clinical practice, essential for conveying pertinent patient information and
facilitating continuity of care. Recent advancements in large language models
(LLMs) have significantly enhanced their capability in understanding and
summarizing complex medical texts. This research aims to explore how LLMs can
alleviate the burden of manual summarization, streamline workflow efficiencies,
and support informed decision-making in healthcare settings. Clinical notes
from a cohort of 1,099 lung cancer patients were utilized, with a subset of 50
patients for testing purposes, and 102 patients used for model fine-tuning.
This study evaluates the performance of multiple LLMs, including GPT-3.5,
GPT-4, GPT-4o, and LLaMA 3 8b, in generating discharge summaries. Evaluation
metrics included token-level analysis (BLEU, ROUGE-1, ROUGE-2, ROUGE-L) and
semantic similarity scores between model-generated summaries and
physician-written gold standards. LLaMA 3 8b was further tested on clinical
notes of varying lengths to examine the stability of its performance. The study
found notable variations in summarization capabilities among LLMs. GPT-4o and
fine-tuned LLaMA 3 demonstrated superior token-level evaluation metrics, while
LLaMA 3 consistently produced concise summaries across different input lengths.
Semantic similarity scores indicated GPT-4o and LLaMA 3 as leading models in
capturing clinical relevance. This study contributes insights into the efficacy
of LLMs for generating discharge summaries, highlighting LLaMA 3's robust
performance in maintaining clarity and relevance across varying clinical
contexts. These findings underscore the potential of automated summarization
tools to enhance documentation precision and efficiency, ultimately improving
patient care and operational capability in healthcare settings.",2024-11-06,"Yiming Li, Fang Li, Kirk Roberts, Licong Cui, Cui Tao, Hua Xu",http://arxiv.org/pdf/2411.03805v1,cs.CL
On-Device Emoji Classifier Trained with GPT-based Data Augmentation for a Mobile Keyboard,"Emojis improve communication quality among smart-phone users that use mobile
keyboards to exchange text. To predict emojis for users based on input text, we
should consider the on-device low memory and time constraints, ensure that the
on-device emoji classifier covers a wide range of emoji classes even though the
emoji dataset is typically imbalanced, and adapt the emoji classifier output to
user favorites. This paper proposes an on-device emoji classifier based on
MobileBert with reasonable memory and latency requirements for SwiftKey. To
account for the data imbalance, we utilize the widely used GPT to generate one
or more tags for each emoji class. For each emoji and corresponding tags, we
merge the original set with GPT-generated sentences and label them with this
emoji without human intervention to alleviate the data imbalance. At inference
time, we interpolate the emoji output with the user history for emojis for
better emoji classifications. Results show that the proposed on-device emoji
classifier deployed for SwiftKey increases the accuracy performance of emoji
prediction particularly on rare emojis and emoji engagement.",2024-11-06,"Hossam Amer, Joe Osborne, Michael Zaki, Mohamed Afify",http://arxiv.org/pdf/2411.05031v2,cs.CL
"No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages","Research in vision and language has made considerable progress thanks to
benchmarks such as COCO. COCO captions focused on unambiguous facts in English;
ArtEmis introduced subjective emotions and ArtELingo introduced some
multilinguality (Chinese and Arabic). However we believe there should be more
multilinguality. Hence, we present ArtELingo-28, a vision-language benchmark
that spans $\textbf{28}$ languages and encompasses approximately
$\textbf{200,000}$ annotations ($\textbf{140}$ annotations per image).
Traditionally, vision research focused on unambiguous class labels, whereas
ArtELingo-28 emphasizes diversity of opinions over languages and cultures. The
challenge is to build machine learning systems that assign emotional captions
to images. Baseline results will be presented for three novel conditions:
Zero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual
transfer is more successful for culturally-related languages. Data and code are
provided at www.artelingo.org.",2024-11-06,"Youssef Mohamed, Runjia Li, Ibrahim Said Ahmad, Kilichbek Haydarov, Philip Torr, Kenneth Ward Church, Mohamed Elhoseiny",http://arxiv.org/pdf/2411.03769v1,cs.CL
Number Cookbook: Number Understanding of Language Models and How to Improve It,"Large language models (LLMs) can solve an increasing number of complex
reasoning tasks while making surprising mistakes in basic numerical
understanding and processing (such as 9.11 > 9.9). The latter ability is
essential for tackling complex arithmetic and mathematical problems and serves
as a foundation for most reasoning tasks, but previous work paid little
attention to it or only discussed several restricted tasks (like integer
addition). In this paper, we comprehensively investigate the numerical
understanding and processing ability (NUPA) of LLMs. Firstly, we introduce a
benchmark covering four common numerical representations and 17 distinct
numerical tasks in four major categories, resulting in 41 meaningful
combinations in total. These tasks are derived from primary and secondary
education curricula, encompassing nearly all everyday numerical understanding
and processing scenarios, and the rules of these tasks are very simple and
clear. Through the benchmark, we find that current LLMs fail frequently in many
of the tasks. To study the problem, we train small models with existing and
potential techniques for enhancing NUPA (such as tokenizers, PEs, and number
formats), comprehensively evaluating their effectiveness using our testbed. We
also finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)
naive finetuning can improve NUPA a lot on many but not all tasks, and 2)
surprisingly, techniques designed to enhance NUPA prove ineffective for
finetuning pretrained models. We further explore the impact of chain-of-thought
techniques on NUPA. Our work provides a more detailed and comprehensive
understanding of NUPA in LLMs. Our benchmark and code are released at
https://github.com/GraphPKU/number_cookbook.",2024-11-06,"Haotong Yang, Yi Hu, Shijia Kang, Zhouchen Lin, Muhan Zhang",http://arxiv.org/pdf/2411.03766v3,cs.CL
A Library Perspective on Supervised Text Processing in Digital Libraries: An Investigation in the Biomedical Domain,"Digital libraries that maintain extensive textual collections may want to
further enrich their content for certain downstream applications, e.g.,
building knowledge graphs, semantic enrichment of documents, or implementing
novel access paths. All of these applications require some text processing,
either to identify relevant entities, extract semantic relationships between
them, or to classify documents into some categories. However, implementing
reliable, supervised workflows can become quite challenging for a digital
library because suitable training data must be crafted, and reliable models
must be trained. While many works focus on achieving the highest accuracy on
some benchmarks, we tackle the problem from a digital library practitioner. In
other words, we also consider trade-offs between accuracy and application
costs, dive into training data generation through distant supervision and large
language models such as ChatGPT, LLama, and Olmo, and discuss how to design
final pipelines. Therefore, we focus on relation extraction and text
classification, using the showcase of eight biomedical benchmarks.",2024-11-06,"Hermann Kroll, Pascal Sackhoff, Bill Matthias Thang, Maha Ksouri, Wolf-Tilo Balke",http://arxiv.org/pdf/2411.12752v1,cs.CL
The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models,"Natural-language assistants are designed to provide users with helpful
responses while avoiding harmful outputs, largely achieved through alignment to
human preferences. Yet there is limited understanding of whether alignment
techniques may inadvertently perpetuate or even amplify harmful biases
inherited from their pre-aligned base models. This issue is compounded by the
choice of bias evaluation benchmarks in popular preference-finetuned models,
which predominantly focus on dominant social categories, such as binary gender,
thereby limiting insights into biases affecting underrepresented groups.
Towards addressing this gap, we center transgender, nonbinary, and other
gender-diverse identities to investigate how alignment procedures interact with
pre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a
comprehensive survey of bias evaluation modalities across leading
preference-finetuned LLMs, highlighting critical gaps in gender-diverse
representation, 2) systematic evaluation of gender-diverse biases across 16
models spanning Direct Preference Optimization (DPO) stages, uncovering harms
popular bias benchmarks fail to detect, and 3) a flexible framework for
measuring harmful biases in implicit reward signals applicable to other social
contexts. Our findings reveal that DPO-aligned models are particularly
sensitive to supervised finetuning (SFT), and can amplify two forms of
real-world gender-diverse harms from their base models: stigmatization and
gender non-affirmative language. We conclude with recommendations tailored to
DPO and broader alignment practices, advocating for the adoption of
community-informed bias evaluation frameworks to more effectively identify and
address underrepresented harms in LLMs.",2024-11-06,"Anaelia Ovalle, Krunoslav Lehman Pavasovic, Louis Martin, Luke Zettlemoyer, Eric Michael Smith, Kai-Wei Chang, Adina Williams, Levent Sagun",http://arxiv.org/pdf/2411.03700v2,cs.CL
QUILL: Quotation Generation Enhancement of Large Language Models,"While Large language models (LLMs) have become excellent writing assistants,
they still struggle with quotation generation. This is because they either
hallucinate when providing factual quotations or fail to provide quotes that
exceed human expectations. To bridge the gap, we systematically study how to
evaluate and improve LLMs' performance in quotation generation tasks. We first
establish a holistic and automatic evaluation system for quotation generation
task, which consists of five criteria each with corresponding automatic metric.
To improve the LLMs' quotation generation abilities, we construct a bilingual
knowledge base that is broad in scope and rich in dimensions, containing up to
32,022 quotes. Moreover, guided by our critiria, we further design a
quotation-specific metric to rerank the retrieved quotations from the knowledge
base. Extensive experiments show that our metrics strongly correlate with human
preferences. Existing LLMs struggle to generate desired quotes, but our
quotation knowledge base and reranking metric help narrow this gap. Our dataset
and code are publicly available at https://github.com/GraceXiaoo/QUILL.",2024-11-06,"Jin Xiao, Bowei Zhang, Qianyu He, Jiaqing Liang, Feng Wei, Jinglei Chen, Zujie Liang, Deqing Yang, Yanghua Xiao",http://arxiv.org/pdf/2411.03675v2,cs.CL
Evaluating Moral Beliefs across LLMs through a Pluralistic Framework,"Proper moral beliefs are fundamental for language models, yet assessing these
beliefs poses a significant challenge. This study introduces a novel
three-module framework to evaluate the moral beliefs of four prominent large
language models. Initially, we constructed a dataset containing 472 moral
choice scenarios in Chinese, derived from moral words. The decision-making
process of the models in these scenarios reveals their moral principle
preferences. By ranking these moral choices, we discern the varying moral
beliefs held by different language models. Additionally, through moral debates,
we investigate the firmness of these models to their moral choices. Our
findings indicate that English language models, namely ChatGPT and Gemini,
closely mirror moral decisions of the sample of Chinese university students,
demonstrating strong adherence to their choices and a preference for
individualistic moral beliefs. In contrast, Chinese models such as Ernie and
ChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their
moral choices and debates. This study also uncovers gender bias embedded within
the moral beliefs of all examined language models. Our methodology offers an
innovative means to assess moral beliefs in both artificial and human
intelligence, facilitating a comparison of moral values across different
cultures.",2024-11-06,"Xuelin Liu, Yanfei Zhu, Shucheng Zhu, Pengyuan Liu, Ying Liu, Dong Yu",http://arxiv.org/pdf/2411.03665v1,cs.CL
Deploying Multi-task Online Server with Large Language Model,"In the industry, numerous tasks are deployed online. Traditional approaches
often tackle each task separately by its own network, which leads to excessive
costs for developing and scaling models, especially in the context of large
language models. Although multi-task methods can save costs through parameter
sharing, they often struggle to outperform single-task methods in real-world
applications. To tackle these challenges, we present a three-stage multi-task
learning framework for large language models. It involves task filtering,
followed by fine-tuning on high-resource tasks, and finally fine-tuning on all
tasks. We conducted comprehensive experiments in single-task and multi-task
settings. Our approach, exemplified on different benchmarks, demonstrates that
it is able to achieve performance comparable to the single-task method while
reducing up to 90.9\% of its overhead.",2024-11-06,"Yincen Qu, Chao Ma, Xiangying Dai, Hui Zhou, Yiting Wu, Hengyue Liu",http://arxiv.org/pdf/2411.03644v2,cs.CL
From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond,"Run-time steering strategies like Medprompt are valuable for guiding large
language models (LLMs) to top performance on challenging tasks. Medprompt
demonstrates that a general LLM can be focused to deliver state-of-the-art
performance on specialized domains like medicine by using a prompt to elicit a
run-time strategy involving chain of thought reasoning and ensembling. OpenAI's
o1-preview model represents a new paradigm, where a model is designed to do
run-time reasoning before generating final responses. We seek to understand the
behavior of o1-preview on a diverse set of medical challenge problem
benchmarks. Following on the Medprompt study with GPT-4, we systematically
evaluate the o1-preview model across various medical benchmarks. Notably, even
without prompting techniques, o1-preview largely outperforms the GPT-4 series
with Medprompt. We further systematically study the efficacy of classic prompt
engineering strategies, as represented by Medprompt, within the new paradigm of
reasoning models. We found that few-shot prompting hinders o1's performance,
suggesting that in-context learning may no longer be an effective steering
approach for reasoning-native models. While ensembling remains viable, it is
resource-intensive and requires careful cost-performance optimization. Our cost
and accuracy analysis across run-time strategies reveals a Pareto frontier,
with GPT-4o representing a more affordable option and o1-preview achieving
state-of-the-art performance at higher cost. Although o1-preview offers top
performance, GPT-4o with steering strategies like Medprompt retains value in
specific contexts. Moreover, we note that the o1-preview model has reached
near-saturation on many existing medical benchmarks, underscoring the need for
new, challenging benchmarks. We close with reflections on general directions
for inference-time computation with LLMs.",2024-11-06,"Harsha Nori, Naoto Usuyama, Nicholas King, Scott Mayer McKinney, Xavier Fernandes, Sheng Zhang, Eric Horvitz",http://arxiv.org/pdf/2411.03590v1,cs.CL
The American Sign Language Knowledge Graph: Infusing ASL Models with Linguistic Knowledge,"Language models for American Sign Language (ASL) could make language
technologies substantially more accessible to those who sign. To train models
on tasks such as isolated sign recognition (ISR) and ASL-to-English
translation, datasets provide annotated video examples of ASL signs. To
facilitate the generalizability and explainability of these models, we
introduce the American Sign Language Knowledge Graph (ASLKG), compiled from
twelve sources of expert linguistic knowledge. We use the ASLKG to train
neuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of
91% on ISR, 14% for predicting the semantic features of unseen signs, and 36%
for classifying the topic of Youtube-ASL videos.",2024-11-06,"Lee Kezar, Nidhi Munikote, Zian Zeng, Zed Sehyr, Naomi Caselli, Jesse Thomason",http://arxiv.org/pdf/2411.03568v1,cs.CL
Learning to Write Rationally: How Information Is Distributed in Non-Native Speakers' Essays,"People tend to distribute information evenly in language production for
better and clearer communication. In this study, we compared essays written by
second language learners with various native language (L1) backgrounds to
investigate how they distribute information in their non-native language (L2)
production. Analyses of surprisal and constancy of entropy rate indicated that
writers with higher L2 proficiency can reduce the expected uncertainty of
language production while still conveying informative content. However, the
uniformity of information distribution showed less variability among different
groups of L2 speakers, suggesting that this feature may be universal in L2
essay writing and less affected by L2 writers' variability in L1 background and
L2 proficiency.",2024-11-05,"Zixin Tang, Janet G. van Hell",http://arxiv.org/pdf/2411.03550v1,cs.CL
Exploring the Benefits of Domain-Pretraining of Generative Large Language Models for Chemistry,"A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and
more) are driving forward novel development of multipurpose AI for a variety of
tasks, particularly natural language processing (NLP) tasks. These models
demonstrate strong performance on a range of tasks; however, there has been
evidence of brittleness when applied to more niche or narrow domains where
hallucinations or fluent but incorrect responses reduce performance. Given the
complex nature of scientific domains, it is prudent to investigate the
trade-offs of leveraging off-the-shelf versus more targeted foundation models
for scientific domains. In this work, we examine the benefits of in-domain
pre-training for a given scientific domain, chemistry, and compare these to
open-source, off-the-shelf models with zero-shot and few-shot prompting. Our
results show that not only do in-domain base models perform reasonably well on
in-domain tasks in a zero-shot setting but that further adaptation using
instruction fine-tuning yields impressive performance on chemistry-specific
tasks such as named entity recognition and molecular formula generation.",2024-11-05,"Anurag Acharya, Shivam Sharma, Robin Cosbey, Megha Subramanian, Scott Howland, Maria Glenski",http://arxiv.org/pdf/2411.03542v1,cs.CL
Long Context RAG Performance of Large Language Models,"Retrieval Augmented Generation (RAG) has emerged as a crucial technique for
enhancing the accuracy of Large Language Models (LLMs) by incorporating
external information. With the advent of LLMs that support increasingly longer
context lengths, there is a growing interest in understanding how these models
perform in RAG scenarios. Can these new long context models improve RAG
performance? This paper presents a comprehensive study of the impact of
increased context length on RAG performance across 20 popular open source and
commercial LLMs. We ran RAG workflows while varying the total context length
from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three
domain-specific datasets, and report key insights on the benefits and
limitations of long context in RAG applications. Our findings reveal that while
retrieving more documents can improve performance, only a handful of the most
recent state of the art LLMs can maintain consistent accuracy at long context
above 64k tokens. We also identify distinct failure modes in long context
scenarios, suggesting areas for future research.",2024-11-05,"Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, Michael Carbin",http://arxiv.org/pdf/2411.03538v1,cs.CL
Mitigating Metric Bias in Minimum Bayes Risk Decoding,"While Minimum Bayes Risk (MBR) decoding using metrics such as COMET or
MetricX has outperformed traditional decoding methods such as greedy or beam
search, it introduces a challenge we refer to as metric bias. As MBR decoding
aims to produce translations that score highly according to a specific utility
metric, this very process makes it impossible to use the same metric for both
decoding and evaluation, as improvements might simply be due to reward hacking
rather than reflecting real quality improvements. In this work we find that
compared to human ratings, neural metrics not only overestimate the quality of
MBR decoding when the same metric is used as the utility metric, but they also
overestimate the quality of MBR/QE decoding with other neural utility metrics
as well. We also show that the metric bias issue can be mitigated by using an
ensemble of utility metrics during MBR decoding: human evaluations show that
MBR decoding using an ensemble of utility metrics outperforms a single utility
metric.",2024-11-05,"Geza Kovacs, Daniel Deutsch, Markus Freitag",http://arxiv.org/pdf/2411.03524v1,cs.CL
Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy,"This paper introduces a novel model compression approach through dynamic
layer-specific pruning in Large Language Models (LLMs), enhancing the
traditional methodology established by SliceGPT. By transitioning from constant
to dynamic slicing, our method leverages the newly proposed Layer Redundancy
(LR) score, which assesses how much change each layer changes its input by
measuring the cosine similarity of the input to the output of the layer. We use
this score to prune parts of individual layers based on redundancy in such a
way that the average pruned percentage for all layers is a fixed value. We
conducted extensive experiments using models like Llama3-8B and Mistral-7B on
multiple datasets, evaluating different slicing bases and percentages to
determine optimal configurations that balance efficiency and performance. Our
findings show that our dynamic slicing approach not only maintains but, in many
cases, enhances model performance compared to the baseline established by
constant slicing methods. For instance, in several settings, we see performance
improvements of up to 5% over the SliceGPT baseline. Additionally, a perplexity
decrease by as much as 7% was observed across multiple benchmarks, validating
the effectiveness of our method. The code, model weights, and datasets are
open-sourced at https://github.com/RazvanDu/DynamicSlicing.",2024-11-05,"Razvan-Gabriel Dumitru, Paul-Ioan Clotan, Vikas Yadav, Darius Peteleaza, Mihai Surdeanu",http://arxiv.org/pdf/2411.03513v1,cs.CL
Uncertainty Quantification for Clinical Outcome Predictions with (Large) Language Models,"To facilitate healthcare delivery, language models (LMs) have significant
potential for clinical prediction tasks using electronic health records (EHRs).
However, in these high-stakes applications, unreliable decisions can result in
high costs due to compromised patient safety and ethical concerns, thus
increasing the need for good uncertainty modeling of automated clinical
predictions. To address this, we consider the uncertainty quantification of LMs
for EHR tasks in white- and black-box settings. We first quantify uncertainty
in white-box models, where we can access model parameters and output logits. We
show that an effective reduction of model uncertainty can be achieved by using
the proposed multi-tasking and ensemble methods in EHRs. Continuing with this
idea, we extend our approach to black-box settings, including popular
proprietary LMs such as GPT-4. We validate our framework using longitudinal
clinical data from more than 6,000 patients in ten clinical prediction tasks.
Results show that ensembling methods and multi-task prediction prompts reduce
uncertainty across different scenarios. These findings increase the
transparency of the model in white-box and black-box settings, thus advancing
reliable AI healthcare.",2024-11-05,"Zizhang Chen, Peizhao Li, Xiaomeng Dong, Pengyu Hong",http://arxiv.org/pdf/2411.03497v1,cs.CL
Automatic Generation of Question Hints for Mathematics Problems using Large Language Models in Educational Technology,"The automatic generation of hints by Large Language Models (LLMs) within
Intelligent Tutoring Systems (ITSs) has shown potential to enhance student
learning. However, generating pedagogically sound hints that address student
misconceptions and adhere to specific educational objectives remains
challenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as
teachers to generate effective hints for students simulated through LLMs
(GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math
exercises designed for human high-school students, and designed using cognitive
science principles. We present here the study of several dimensions: 1)
identifying error patterns made by simulated students on secondary-level math
exercises; 2) developing various prompts for GPT-4o as a teacher and evaluating
their effectiveness in generating hints that enable simulated students to
self-correct; and 3) testing the best-performing prompts, based on their
ability to produce relevant hints and facilitate error correction, with
Llama-3-8B-Instruct as the teacher, allowing for a performance comparison with
GPT-4o. The results show that model errors increase with higher temperature
settings. Notably, when hints are generated by GPT-4o, the most effective
prompts include prompts tailored to specific errors as well as prompts
providing general hints based on common mathematical errors. Interestingly,
Llama-3-8B-Instruct as a teacher showed better overall performance than GPT-4o.
Also the problem-solving and response revision capabilities of the LLMs as
students, particularly GPT-3.5-turbo, improved significantly after receiving
hints, especially at lower temperature settings. However, models like
Mistral-7B-Instruct demonstrated a decline in performance as the temperature
increased.",2024-11-05,"Junior Cedric Tonga, Benjamin Clement, Pierre-Yves Oudeyer",http://arxiv.org/pdf/2411.03495v1,cs.CL
LASER: Attention with Exponential Transformation,"Transformers have had tremendous impact for several sequence related tasks,
largely due to their ability to retrieve from any part of the sequence via
softmax based dot-product attention. This mechanism plays a crucial role in
Transformer's performance. We analyze the gradients backpropagated through the
softmax operation in the attention mechanism and observe that these gradients
can often be small. This poor gradient signal backpropagation can lead to
inefficient learning of parameters preceeding the attention operations. To this
end, we introduce a new attention mechanism called LASER, which we analytically
show to admit a larger gradient signal. We show that LASER Attention can be
implemented by making small modifications to existing attention
implementations. We conduct experiments on autoregressive large language models
(LLMs) with upto 2.2 billion parameters where we show upto 3.38% and an average
of ~1% improvement over standard attention on downstream evaluations. Using
LASER gives the following relative improvements in generalization performance
across a variety of tasks (vision, text and speech): 4.67% accuracy in Vision
Transformer (ViT) on Imagenet, 2.25% error rate in Conformer on the Librispeech
speech-to-text and 0.93% fraction of incorrect predictions in BERT with 2.2
billion parameters.",2024-11-05,"Sai Surya Duvvuri, Inderjit S. Dhillon",http://arxiv.org/pdf/2411.03493v1,cs.CL
"LLM Generated Distribution-Based Prediction of US Electoral Results, Part I","This paper introduces distribution-based prediction, a novel approach to
using Large Language Models (LLMs) as predictive tools by interpreting output
token probabilities as distributions representing the models' learned
representation of the world. This distribution-based nature offers an
alternative perspective for analyzing algorithmic fidelity, complementing the
approach used in silicon sampling. We demonstrate the use of distribution-based
prediction in the context of recent United States presidential election,
showing that this method can be used to determine task specific bias, prompt
noise, and algorithmic fidelity. This approach has significant implications for
assessing the reliability and increasing transparency of LLM-based predictions
across various domains.",2024-11-05,"Caleb Bradshaw, Caelen Miller, Sean Warnick",http://arxiv.org/pdf/2411.03486v1,cs.CL
MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs,"Large Language Models (LLMs) have been applied to various hardware design
tasks, including Verilog code generation, EDA tool scripting, and RTL bug
fixing. Despite this extensive exploration, LLMs are yet to be used for the
task of post-synthesis metric reasoning and estimation of HDL designs. In this
paper, we assess the ability of LLMs to reason about post-synthesis metrics of
Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868
Verilog HDL designs and their corresponding post-synthesis metrics, namely
area, delay, and static power. MetRex incorporates a Chain of Thought (CoT)
template to enhance LLMs' reasoning about these metrics. Extensive experiments
show that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities
on average by 37.0\%, 25.3\%, and 25.7\% on the area, delay, and static power,
respectively. While SFT improves performance on our benchmark, it remains far
from achieving optimal results, especially on complex problems. Comparing to
state-of-the-art regression models, our approach delivers accurate
post-synthesis predictions for 17.4\% more designs (within a 5\% error margin),
in addition to offering a 1.7x speedup by eliminating the need for
pre-processing. This work lays the groundwork for advancing LLM-based Verilog
code metric reasoning.",2024-11-05,"Manar Abdelatty, Jingxiao Ma, Sherief Reda",http://arxiv.org/pdf/2411.03471v2,cs.CL
Solving Trojan Detection Competitions with Linear Weight Classification,"Neural networks can conceal malicious Trojan backdoors that allow a trigger
to covertly change the model behavior. Detecting signs of these backdoors,
particularly without access to any triggered data, is the subject of ongoing
research and open challenges. In one common formulation of the problem, we are
given a set of clean and poisoned models and need to predict whether a given
test model is clean or poisoned. In this paper, we introduce a detector that
works remarkably well across many of the existing datasets and domains. It is
obtained by training a binary classifier on a large number of models' weights
after performing a few different pre-processing steps including feature
selection and standardization, reference model weights subtraction, and model
alignment prior to detection. We evaluate this algorithm on a diverse set of
Trojan detection benchmarks and domains and examine the cases where the
approach is most and least effective.",2024-11-05,"Todd Huster, Peter Lin, Razvan Stefanescu, Emmanuel Ekwedike, Ritu Chadha",http://arxiv.org/pdf/2411.03445v1,cs.CL
MME-Finance: A Multimodal Finance Benchmark for Expert-level Understanding and Reasoning,"In recent years, multimodal benchmarks for general domains have guided the
rapid development of multimodal models on general tasks. However, the financial
field has its peculiarities. It features unique graphical images (e.g.,
candlestick charts, technical indicator charts) and possesses a wealth of
specialized financial knowledge (e.g., futures, turnover rate). Therefore,
benchmarks from general fields often fail to measure the performance of
multimodal models in the financial domain, and thus cannot effectively guide
the rapid development of large financial models. To promote the development of
large financial multimodal models, we propose MME-Finance, an bilingual
open-ended and practical usage-oriented Visual Question Answering (VQA)
benchmark. The characteristics of our benchmark are finance and expertise,
which include constructing charts that reflect the actual usage needs of users
(e.g., computer screenshots and mobile photography), creating questions
according to the preferences in financial domain inquiries, and annotating
questions by experts with 10+ years of experience in the financial industry.
Additionally, we have developed a custom-designed financial evaluation system
in which visual information is first introduced in the multi-modal evaluation
process. Extensive experimental evaluations of 19 mainstream MLLMs are
conducted to test their perception, reasoning, and cognition capabilities. The
results indicate that models performing well on general benchmarks cannot do
well on MME-Finance; for instance, the top-performing open-source and
closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o),
respectively. Their performance is particularly poor in categories most
relevant to finance, such as candlestick charts and technical indicator charts.
In addition, we propose a Chinese version, which helps compare performance of
MLLMs under a Chinese context.",2024-11-05,"Ziliang Gan, Yu Lu, Dong Zhang, Haohan Li, Che Liu, Jian Liu, Ji Liu, Haipang Wu, Chaoyou Fu, Zenglin Xu, Rongjunchen Zhang, Yong Dai",http://arxiv.org/pdf/2411.03314v1,cs.CL
Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS'24 Experiment,"Large language models (LLMs) represent a promising, but controversial, tool
in aiding scientific peer review. This study evaluates the usefulness of LLMs
in a conference setting as a tool for vetting paper submissions against
submission standards. We conduct an experiment at the 2024 Neural Information
Processing Systems (NeurIPS) conference, where 234 papers were voluntarily
submitted to an ""LLM-based Checklist Assistant."" This assistant validates
whether papers adhere to the author checklist used by NeurIPS, which includes
questions to ensure compliance with research and manuscript preparation
standards. Evaluation of the assistant by NeurIPS paper authors suggests that
the LLM-based assistant was generally helpful in verifying checklist
completion. In post-usage surveys, over 70% of authors found the assistant
useful, and 70% indicate that they would revise their papers or checklist
responses based on its feedback. While causal attribution to the assistant is
not definitive, qualitative evidence suggests that the LLM contributed to
improving some submissions. Survey responses and analysis of re-submissions
indicate that authors made substantive revisions to their submissions in
response to specific feedback from the LLM. The experiment also highlights
common issues with LLMs: inaccuracy (20/52) and excessive strictness (14/52)
were the most frequent issues flagged by authors. We also conduct experiments
to understand potential gaming of the system, which reveal that the assistant
could be manipulated to enhance scores through fabricated justifications,
highlighting potential vulnerabilities of automated review tools.",2024-11-05,"Alexander Goldberg, Ihsan Ullah, Thanh Gia Hieu Khuong, Benedictus Kent Rachmat, Zhen Xu, Isabelle Guyon, Nihar B. Shah",http://arxiv.org/pdf/2411.03417v2,cs.CL
AI Ethics by Design: Implementing Customizable Guardrails for Responsible AI Development,"This paper explores the development of an ethical guardrail framework for AI
systems, emphasizing the importance of customizable guardrails that align with
diverse user values and underlying ethics. We address the challenges of AI
ethics by proposing a structure that integrates rules, policies, and AI
assistants to ensure responsible AI behavior, while comparing the proposed
framework to the existing state-of-the-art guardrails. By focusing on practical
mechanisms for implementing ethical standards, we aim to enhance transparency,
user autonomy, and continuous improvement in AI systems. Our approach
accommodates ethical pluralism, offering a flexible and adaptable solution for
the evolving landscape of AI governance. The paper concludes with strategies
for resolving conflicts between ethical directives, underscoring the present
and future need for robust, nuanced and context-aware AI systems.",2024-11-05,"Kristina Šekrst, Jeremy McHugh, Jonathan Rodriguez Cefalu",http://arxiv.org/pdf/2411.14442v1,cs.CL
SAUCE: Synchronous and Asynchronous User-Customizable Environment for Multi-Agent LLM Interaction,"Many human interactions, such as political debates, are carried out in group
settings, where there are arbitrarily many participants, each with different
views and agendas. To explore such complex social settings, we present SAUCE: a
customizable Python platform, allowing researchers to plug-and-play various
LLMs participating in discussions on any topic chosen by the user. Our platform
takes care of instantiating the models, scheduling their responses, managing
the discussion history, and producing a comprehensive output log, all
customizable through configuration files, requiring little to no coding skills.
A novel feature of SAUCE is our asynchronous communication feature, where
models decide when to speak in addition to what to say, thus modeling an
important facet of human communication. We show SAUCE's attractiveness in two
initial experiments, and invite the community to use it in simulating various
group simulations.",2024-11-05,"Shlomo Neuberger, Niv Eckhaus, Uri Berger, Amir Taubenfeld, Gabriel Stanovsky, Ariel Goldstein",http://arxiv.org/pdf/2411.03397v1,cs.CL
Exploring Large Language Models for Specialist-level Oncology Care,"Large language models (LLMs) have shown remarkable progress in encoding
clinical knowledge and responding to complex medical queries with appropriate
clinical reasoning. However, their applicability in subspecialist or complex
medical settings remains underexplored. In this work, we probe the performance
of AMIE, a research conversational diagnostic AI system, in the subspecialist
domain of breast oncology care without specific fine-tuning to this challenging
domain. To perform this evaluation, we curated a set of 50 synthetic breast
cancer vignettes representing a range of treatment-naive and
treatment-refractory cases and mirroring the key information available to a
multidisciplinary tumor board for decision-making (openly released with this
work). We developed a detailed clinical rubric for evaluating management plans,
including axes such as the quality of case summarization, safety of the
proposed care plan, and recommendations for chemotherapy, radiotherapy, surgery
and hormonal therapy. To improve performance, we enhanced AMIE with the
inference-time ability to perform web search retrieval to gather relevant and
up-to-date clinical knowledge and refine its responses with a multi-stage
self-critique pipeline. We compare response quality of AMIE with internal
medicine trainees, oncology fellows, and general oncology attendings under both
automated and specialist clinician evaluations. In our evaluations, AMIE
outperformed trainees and fellows demonstrating the potential of the system in
this challenging and important domain. We further demonstrate through
qualitative examples, how systems such as AMIE might facilitate conversational
interactions to assist clinicians in their decision making. However, AMIE's
performance was overall inferior to attending oncologists suggesting that
further research is needed prior to consideration of prospective uses.",2024-11-05,"Anil Palepu, Vikram Dhillon, Polly Niravath, Wei-Hung Weng, Preethi Prasad, Khaled Saab, Ryutaro Tanno, Yong Cheng, Hanh Mai, Ethan Burns, Zainub Ajmal, Kavita Kulkarni, Philip Mansfield, Dale Webster, Joelle Barral, Juraj Gottweis, Mike Schaekermann, S. Sara Mahdavi, Vivek Natarajan, Alan Karthikesalingam, Tao Tu",http://arxiv.org/pdf/2411.03395v1,cs.CL
LLMs for Domain Generation Algorithm Detection,"This work analyzes the use of large language models (LLMs) for detecting
domain generation algorithms (DGAs). We perform a detailed evaluation of two
important techniques: In-Context Learning (ICL) and Supervised Fine-Tuning
(SFT), showing how they can improve detection. SFT increases performance by
using domain-specific data, whereas ICL helps the detection model to quickly
adapt to new threats without requiring much retraining. We use Meta's Llama3 8B
model, on a custom dataset with 68 malware families and normal domains,
covering several hard-to-detect schemes, including recent word-based DGAs.
Results proved that LLM-based methods can achieve competitive results in DGA
detection. In particular, the SFT-based LLM DGA detector outperforms
state-of-the-art models using attention layers, achieving 94% accuracy with a
4% false positive rate (FPR) and excelling at detecting word-based DGA domains.",2024-11-05,"Reynier Leyva La O, Carlos A. Catania, Tatiana Parlanti",http://arxiv.org/pdf/2411.03307v1,cs.CL
VERITAS: A Unified Approach to Reliability Evaluation,"Large language models (LLMs) often fail to synthesize information from their
context to generate an accurate response. This renders them unreliable in
knowledge intensive settings where reliability of the output is key. A critical
component for reliable LLMs is the integration of a robust fact-checking system
that can detect hallucinations across various formats. While several
open-access fact-checking models are available, their functionality is often
limited to specific tasks, such as grounded question-answering or entailment
verification, and they perform less effectively in conversational settings. On
the other hand, closed-access models like GPT-4 and Claude offer greater
flexibility across different contexts, including grounded dialogue
verification, but are hindered by high costs and latency. In this work, we
introduce VERITAS, a family of hallucination detection models designed to
operate flexibly across diverse contexts while minimizing latency and costs.
VERITAS achieves state-of-the-art results considering average performance on
all major hallucination detection benchmarks, with $10\%$ increase in average
performance when compared to similar-sized models and get close to the
performance of GPT4 turbo with LLM-as-a-judge setting.",2024-11-05,"Rajkumar Ramamurthy, Meghana Arakkal Rajeev, Oliver Molenschot, James Zou, Nazneen Rajani",http://arxiv.org/pdf/2411.03300v1,cs.CL
SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents,"While multi-agent systems have been shown to significantly enhance the
performance of Large Language Models (LLMs) across various tasks and
applications, the dense interaction between scaling agents potentially hampers
their efficiency and diversity. To address these challenges, we draw
inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse
mixture-of-agents (SMoA) framework to improve the efficiency and diversity of
multi-agent LLMs. Unlike completely connected structures, SMoA introduces novel
Response Selection and Early Stopping mechanisms to sparsify information flows
among individual LLM agents, striking a balance between performance and
efficiency. Additionally, inspired by the expert diversity principle in SMoE
frameworks for workload balance between experts, we assign distinct role
descriptions to each LLM agent, fostering diverse and divergent thinking.
Extensive experiments on reasoning, alignment, and fairness benchmarks
demonstrate that SMoA achieves performance comparable to traditional
mixture-of-agents approaches but with significantly lower computational costs.
Further analysis reveals that SMoA is more stable, has a greater capacity to
scale, and offers considerable potential through hyper-parameter optimization.
Code and data will be available at: https://github.com/David-Li0406/SMoA.",2024-11-05,"Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Kumar Satvik Chaudhary, Lijie Hu, Jiayi Shen",http://arxiv.org/pdf/2411.03284v1,cs.CL
DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models,"Recent advancements in large language models (LLMs) have significantly
enhanced their knowledge and generative capabilities, leading to a surge of
interest in leveraging LLMs for high-quality data synthesis. However, synthetic
data generation via prompting LLMs remains challenging due to LLMs' limited
understanding of target data distributions and the complexity of prompt
engineering, especially for structured formatted data. To address these issues,
we introduce DiffLM, a controllable data synthesis framework based on
variational autoencoder (VAE), which further (1) leverages diffusion models to
reserve more information of original distribution and format structure in the
learned latent distribution and (2) decouples the learning of target
distribution knowledge from the LLM's generative objectives via a plug-and-play
latent feature injection module. As we observed significant discrepancies
between the VAE's latent representations and the real data distribution, the
latent diffusion module is introduced into our framework to learn a fully
expressive latent distribution. Evaluations on seven real-world datasets with
structured formatted data (i.e., Tabular, Code and Tool data) demonstrate that
DiffLM generates high-quality data, with performance on downstream tasks
surpassing that of real data by 2-7 percent in certain cases. The data and code
will be publicly available upon completion of internal review.",2024-11-05,"Ying Zhou, Xinyao Wang, Yulei Niu, Yaojie Shen, Lexin Tang, Fan Chen, Ben He, Le Sun, Longyin Wen",http://arxiv.org/pdf/2411.03250v1,cs.CL
Predictor-Corrector Enhanced Transformers with Exponential Moving Average Coefficient Learning,"Residual networks, as discrete approximations of Ordinary Differential
Equations (ODEs), have inspired significant advancements in neural network
design, including multistep methods, high-order methods, and multi-particle
dynamical systems. The precision of the solution to ODEs significantly affects
parameter optimization, thereby impacting model performance. In this work, we
present a series of advanced explorations of Transformer architecture design to
minimize the error compared to the true ``solution.'' First, we introduce a
predictor-corrector learning framework to minimize truncation errors, which
consists of a high-order predictor and a multistep corrector. Second, we
propose an exponential moving average-based coefficient learning method to
strengthen our higher-order predictor. Extensive experiments on large-scale
machine translation, abstractive summarization, language modeling, and natural
language understanding benchmarks demonstrate the superiority of our approach.
On the WMT'14 English-German and English-French tasks, our model achieved BLEU
scores of 30.95 and 44.27, respectively. Furthermore, on the OPUS multilingual
machine translation task, our model surpasses a robust 3.8B DeepNet by an
average of 2.9 SacreBLEU, using only 1/3 parameters. Notably, it also beats
LLama models by 5.7 accuracy points on the LM Harness Evaluation.",2024-11-05,"Bei Li, Tong Zheng, Rui Wang, Jiahao Liu, Qingyan Guo, Junliang Guo, Xu Tan, Tong Xiao, Jingbo Zhu, Jingang Wang, Xunliang Cai",http://arxiv.org/pdf/2411.03042v1,cs.CL
Self-Compositional Data Augmentation for Scientific Keyphrase Generation,"State-of-the-art models for keyphrase generation require large amounts of
training data to achieve good performance. However, obtaining keyphrase-labeled
documents can be challenging and costly. To address this issue, we present a
self-compositional data augmentation method. More specifically, we measure the
relatedness of training documents based on their shared keyphrases, and combine
similar documents to generate synthetic samples. The advantage of our method
lies in its ability to create additional training samples that keep domain
coherence, without relying on external data or resources. Our results on
multiple datasets spanning three different domains, demonstrate that our method
consistently improves keyphrase generation. A qualitative analysis of the
generated keyphrases for the Computer Science domain confirms this improvement
towards their representativity property.",2024-11-05,"Mael Houbre, Florian Boudin, Beatrice Daille, Akiko Aizawa",http://arxiv.org/pdf/2411.03039v2,cs.CL
Leveraging Large Language Models in Code Question Answering: Baselines and Issues,"Question answering over source code provides software engineers and project
managers with helpful information about the implemented features of a software
product. This paper presents a work devoted to using large language models for
question answering over source code in Python. The proposed method for
implementing a source code question answering system involves fine-tuning a
large language model on a unified dataset of questions and answers for Python
code. To achieve the highest quality answers, we tested various models trained
on datasets preprocessed in different ways: a dataset without grammar
correction, a dataset with grammar correction, and a dataset augmented with the
generated summaries. The model answers were also analyzed for errors manually.
We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along
with the conclusions from the manual error analysis. The obtained experimental
results highlight the current problems of the research area, such as poor
quality of the public genuine question-answering datasets. In addition, the
findings include the positive effect of the grammar correction of the training
data on the testing metric values. The addressed findings and issues could be
important for other researchers who attempt to improve the quality of source
code question answering solutions. The training and evaluation code is publicly
available at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.",2024-11-05,"Georgy Andryushchenko, Vladimir Ivanov, Vladimir Makharev, Elizaveta Tukhtina, Aidar Valeev",http://arxiv.org/pdf/2411.03012v1,cs.CL
Growing a Tail: Increasing Output Diversity in Large Language Models,"How diverse are the outputs of large language models when diversity is
desired? We examine the diversity of responses of various models to questions
with multiple possible answers, comparing them with human responses. Our
findings suggest that models' outputs are highly concentrated, reflecting a
narrow, mainstream 'worldview', in comparison to humans, whose responses
exhibit a much longer-tail. We examine three ways to increase models' output
diversity: 1) increasing generation randomness via temperature sampling; 2)
prompting models to answer from diverse perspectives; 3) aggregating outputs
from several models. A combination of these measures significantly increases
models' output diversity, reaching that of humans. We discuss implications of
these findings for AI policy that wishes to preserve cultural diversity, an
essential building block of a democratic social fabric.",2024-11-05,"Michal Shur-Ofry, Bar Horowitz-Amsalem, Adir Rahamim, Yonatan Belinkov",http://arxiv.org/pdf/2411.02989v1,cs.CL
[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for Diabetic Retinopathy using Chatbots and Generative AI,"We present an outline of the first large language model (LLM) based chatbot
application in the context of patient-reported outcome measures (PROMs) for
diabetic retinopathy. By utilizing the capabilities of current LLMs, we enable
patients to provide feedback about their quality of life and treatment progress
via an interactive application. The proposed framework offers significant
advantages over the current approach, which encompasses only qualitative
collection of survey data or a static survey with limited answer options. Using
the PROBot LLM-PROM application, patients will be asked tailored questions
about their individual challenges, and can give more detailed feedback on the
progress of their treatment. Based on this input, we will use machine learning
to infer conventional PROM scores, which can be used by clinicians to evaluate
the treatment status. The goal of the application is to improve adherence to
the healthcare system and treatments, and thus ultimately reduce cases of
subsequent vision impairment. The approach needs to be further validated using
a survey and a clinical study.",2024-11-05,"Maren Pielka, Tobias Schneider, Jan Terheyden, Rafet Sifa",http://arxiv.org/pdf/2411.02973v1,cs.CL
Grounding Natural Language to SQL Translation with Data-Based Self-Explanations,"Natural Language Interfaces for Databases empower non-technical users to
interact with data using natural language (NL). Advanced approaches, utilizing
either neural sequence-to-sequence or more recent sophisticated large-scale
language models, typically implement NL to SQL (NL2SQL) translation in an
end-to-end fashion. However, like humans, these end-to-end translation models
may not always generate the best SQL output on their first try. In this paper,
we propose CycleSQL, an iterative framework designed for end-to-end translation
models to autonomously generate the best output through self-evaluation. The
main idea of CycleSQL is to introduce data-grounded NL explanations of query
results as self-provided feedback, and use the feedback to validate the
correctness of the translation iteratively, hence improving the overall
translation accuracy. Extensive experiments, including quantitative and
qualitative evaluations, are conducted to study CycleSQL by applying it to
seven existing translation models on five widely used benchmarks. The results
show that 1) the feedback loop introduced in CycleSQL can consistently improve
the performance of existing models, and in particular, by applying CycleSQL to
RESDSQL, obtains a translation accuracy of 82.0% (+2.6%) on the validation set,
and 81.6% (+3.2%) on the test set of Spider benchmark; 2) the generated NL
explanations can also provide insightful information for users, aiding in the
comprehension of translation results and consequently enhancing the
interpretability of NL2SQL translation.",2024-11-05,"Yuankai Fan, Tonghui Ren, Can Huang, Zhenying He, X. Sean Wang",http://arxiv.org/pdf/2411.02948v2,cs.CL
Capturing research literature attitude towards Sustainable Development Goals: an LLM-based topic modeling approach,"The world is facing a multitude of challenges that hinder the development of
human civilization and the well-being of humanity on the planet. The
Sustainable Development Goals (SDGs) were formulated by the United Nations in
2015 to address these global challenges by 2030. Natural language processing
techniques can help uncover discussions on SDGs within research literature. We
propose a completely automated pipeline to 1) fetch content from the Scopus
database and prepare datasets dedicated to five groups of SDGs; 2) perform
topic modeling, a statistical technique used to identify topics in large
collections of textual data; and 3) enable topic exploration through
keywords-based search and topic frequency time series extraction. For topic
modeling, we leverage the stack of BERTopic scaled up to be applied on large
corpora of textual documents (we find hundreds of topics on hundreds of
thousands of documents), introducing i) a novel LLM-based embeddings
computation for representing scientific abstracts in the continuous space and
ii) a hyperparameter optimizer to efficiently find the best configuration for
any new big datasets. We additionally produce the visualization of results on
interactive dashboards reporting topics' temporal evolution. Results are made
inspectable and explorable, contributing to the interpretability of the topic
modeling process. Our proposed LLM-based topic modeling pipeline for big-text
datasets allows users to capture insights on the evolution of the attitude
toward SDGs within scientific abstracts in the 2006-2023 time span. All the
results are reproducible by using our system; the workflow can be generalized
to be applied at any point in time to any big corpus of textual documents.",2024-11-05,"Francesco Invernici, Francesca Curati, Jelena Jakimov, Amirhossein Samavi, Anna Bernasconi",http://arxiv.org/pdf/2411.02943v2,cs.CL
A Post-Training Enhanced Optimization Approach for Small Language Models,"This paper delves into the continuous post-training optimization methods for
small language models, and proposes a continuous post-training alignment data
construction method for small language models. The core of this method is based
on the data guidance of large models, optimizing the diversity and accuracy of
alignment data. In addition, to verify the effectiveness of the methods in this
paper, we used Qwen2-0.5B-Instruct model as the baseline model for small
language models, using the alignment dataset constructed by our proposed
method, we trained and compared several groups of experiments, including SFT
(Supervised Fine Tuning) post-training experiment and KTO (Kahneman Tversky
optimization) post-training experiment, as well as SFT-KTO two-stage
post-training experiment and model weight fusion experiment. Finally, we
evaluated and analyzed the performance of post-training models, and confirmed
that the continuous post-training optimization method proposed by us can
significantly improve the performance of small language models.",2024-11-05,Keke Zhai,http://arxiv.org/pdf/2411.02939v1,cs.CL
Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent,"Multimodal Retrieval Augmented Generation (mRAG) plays an important role in
mitigating the ""hallucination"" issue inherent in multimodal large language
models (MLLMs). Although promising, existing heuristic mRAGs typically
predefined fixed retrieval processes, which causes two issues: (1) Non-adaptive
Retrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws
cannot be adequately reflected by current knowledge-seeking visual question
answering (VQA) datasets, since the most required knowledge can be readily
obtained with a standard two-step retrieval. To bridge the dataset gap, we
first construct Dyn-VQA dataset, consisting of three types of ""dynamic""
questions, which require complex knowledge retrieval strategies variable in
query, tool, and time: (1) Questions with rapidly changing answers. (2)
Questions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments
on Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient
and precisely relevant knowledge for dynamic questions due to their rigid
retrieval processes. Hence, we further propose the first self-adaptive planning
agent for multimodal retrieval, OmniSearch. The underlying idea is to emulate
the human behavior in question solution which dynamically decomposes complex
multimodal questions into sub-question chains with retrieval action. Extensive
experiments prove the effectiveness of our OmniSearch, also provide direction
for advancing mRAG. The code and dataset will be open-sourced at
https://github.com/Alibaba-NLP/OmniSearch.",2024-11-05,"Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Philip S. Yu, Fei Huang, Jingren Zhou",http://arxiv.org/pdf/2411.02937v5,cs.CL
Textual Aesthetics in Large Language Models,"Image aesthetics is a crucial metric in the field of image generation.
However, textual aesthetics has not been sufficiently explored. With the
widespread application of large language models (LLMs), previous work has
primarily focused on the correctness of content and the helpfulness of
responses. Nonetheless, providing responses with textual aesthetics is also an
important factor for LLMs, which can offer a cleaner layout and ensure greater
consistency and coherence in content. In this work, we introduce a pipeline for
aesthetics polishing and help construct a textual aesthetics dataset named
TexAes. We propose a textual aesthetics-powered fine-tuning method based on
direct preference optimization, termed TAPO, which leverages textual aesthetics
without compromising content correctness. Additionally, we develop two
evaluation methods for textual aesthetics based on text and image analysis,
respectively. Our experiments demonstrate that using textual aesthetics data
and employing the TAPO fine-tuning method not only improves aesthetic scores
but also enhances performance on general evaluation datasets such as
AlpacalEval and Anera-hard.",2024-11-05,"Lingjie Jiang, Shaohan Huang, Xun Wu, Furu Wei",http://arxiv.org/pdf/2411.02930v1,cs.CL
Membership Inference Attacks against Large Vision-Language Models,"Large vision-language models (VLLMs) exhibit promising capabilities for
processing multi-modal tasks across various application scenarios. However,
their emergence also raises significant data security concerns, given the
potential inclusion of sensitive information, such as private photos and
medical records, in their training datasets. Detecting inappropriately used
data in VLLMs remains a critical and unresolved issue, mainly due to the lack
of standardized datasets and suitable methodologies. In this study, we
introduce the first membership inference attack (MIA) benchmark tailored for
various VLLMs to facilitate training data detection. Then, we propose a novel
MIA pipeline specifically designed for token-level image detection. Lastly, we
present a new metric called MaxR\'enyi-K%, which is based on the confidence of
the model output and applies to both text and image data. We believe that our
work can deepen the understanding and methodology of MIAs in the context of
VLLMs. Our code and datasets are available at
https://github.com/LIONS-EPFL/VL-MIA.",2024-11-05,"Zhan Li, Yongtao Wu, Yihang Chen, Francesco Tonin, Elias Abad Rocamora, Volkan Cevher",http://arxiv.org/pdf/2411.02902v1,cs.CL
The Translation of Circumlocution in Arabic Short Stories into English,"This study investigates the translation of circumlocution from Arabic to
English in a corpus of short stories by renowned Arabic authors. By analyzing
the source and target texts, the study aims to identify and categorize
circumlocution instances in Arabic and their corresponding renditions in
English. The study employs Nida's (1964) translation theory as a framework to
assess the appropriateness of the translation strategies employed. It examines
the extent to which translators successfully rendered Arabic circumlocution
into English, identifying potential challenges and limitations in the
translation process. The findings reveal significant similarities between
Arabic circumlocution categories and English metadiscourse categories,
particularly in terms of textual and interpersonal functions. However, the
study also highlights instances where translators encountered difficulties in
accurately conveying the nuances of circumlocution, often resorting to
strategies like addition, subtraction, and alteration.https://ntu.edu.iq/",2024-11-05,Dalal Waadallah Shehab,http://arxiv.org/pdf/2411.02887v2,cs.CL
TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection,"The rapid advancement of Large Language Models (LLMs) has driven growing
demand for processing extended context sequences in contemporary applications.
However, this progress faces two major challenges: performance degradation due
to sequence lengths out-of-distribution, and excessively long inference times
caused by the quadratic computational complexity of attention. These issues
hinder the application of LLMs in long-context scenarios. In this paper, we
propose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free
method for efficient and accurate long-context inference. TokenSelect builds
upon the observation of non-contiguous attention sparsity, using Query-Key dot
products to measure per-head KV Cache criticality at token-level. By per-head
soft voting mechanism, TokenSelect selectively involves a few critical KV cache
tokens in attention calculation without sacrificing accuracy. To further
accelerate TokenSelect, we design the Selection Cache based on observations of
consecutive Query similarity and implemented efficient dot product kernel,
significantly reducing the overhead. A comprehensive evaluation of TokenSelect
demonstrates up to 23.84x speedup in attention computation and up to 2.28x
acceleration in end-to-end latency, while providing superior performance
compared to state-of-the-art long-context inference methods.",2024-11-05,"Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Tianfu Wang, Kun Fu, Zheng Wang, Hui Xiong",http://arxiv.org/pdf/2411.02886v2,cs.CL
Software Design Pattern Model and Data Structure Algorithm Abilities on Microservices Architecture Design in High-tech Enterprises,"This study investigates the impact of software design model capabilities and
data structure algorithm abilities on microservices architecture design within
enterprises. Utilizing a qualitative methodology, the research involved
in-depth interviews with software architects and developers who possess
extensive experience in microservices implementation. The findings reveal that
organizations emphasizing robust design models and efficient algorithms achieve
superior scalability, performance, and flexibility in their microservices
architecture. Notably, participants highlighted that a strong foundation in
these areas facilitates better service decomposition, optimizes data
processing, and enhances system responsiveness. Despite these insights, gaps
remain regarding the integration of emerging technologies and the evolving
nature of software design practices. This paper contributes to the existing
literature by underscoring the critical role of these competencies in fostering
effective microservices architectures and suggests avenues for future research
to address identified gaps",2024-11-05,Jun Cui,http://arxiv.org/pdf/2411.04143v1,cs.CL
Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document Relation Extraction with Graph-of-Thoughts Reasoning,"Large language models (LLMs) pre-trained on massive corpora have demonstrated
impressive few-shot learning capability on many NLP tasks. Recasting an NLP
task into a text-to-text generation task is a common practice so that
generative LLMs can be prompted to resolve it. However, performing
document-level relation extraction (DocRE) tasks with generative LLM models is
still challenging due to the structured output format of DocRE, which
complicates the conversion to plain text. Limited information available in
few-shot samples and prompt instructions induce further difficulties and
challenges in relation extraction for mentioned entities in a document. In this
paper, we represent the structured output as a graph-style triplet rather than
natural language expressions and leverage generative LLMs for the DocRE task.
Our approach, the Graph-DPEP framework is grounded in the reasoning behind
triplet explanation thoughts presented in natural language. In this framework,
we first introduce a ``decomposed-plug"" method for performing the generation
from LLMs over prompts with type-space decomposition to alleviate the burden of
distinguishing all relation types. Second, we employ a verifier for calibrating
the generation and identifying overlooked query entity pairs. Third, we develop
""ensemble-play"", reapplying generation on the entire type list by leveraging
the reasoning thoughts embedded in a sub-graph associated with the missing
query pair to address the missingness issue. Through extensive comparisons with
existing prompt techniques and alternative Language Models (LLMs), our
framework demonstrates superior performance on publicly available benchmarks in
experiments.",2024-11-05,"Tao Zhang, Ning Yan, Masood Mortazavi, Hoang H. Nguyen, Zhongfen Deng, Philip S. Yu",http://arxiv.org/pdf/2411.02864v1,cs.CL
"Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual Visual Answer Localization","The goal of Multilingual Visual Answer Localization (MVAL) is to locate a
video segment that answers a given multilingual question. Existing methods
either focus solely on visual modality or integrate visual and subtitle
modalities. However, these methods neglect the audio modality in videos,
consequently leading to incomplete input information and poor performance in
the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span
Localization (AVTSL) method that incorporates audio modality to augment both
visual and textual representations for the MVAL task. Specifically, we
integrate features from three modalities and develop three predictors, each
tailored to the unique contributions of the fused modalities: an audio-visual
predictor, a visual predictor, and a textual predictor. Each predictor
generates predictions based on its respective modality. To maintain consistency
across the predicted results, we introduce an Audio-Visual-Textual Consistency
module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing
each modality's predictor to dynamically learn from the others. This
collaborative learning ensures that the model generates consistent and
comprehensive answers. Extensive experiments show that our proposed method
outperforms several state-of-the-art (SOTA) methods, which demonstrates the
effectiveness of the audio modality.",2024-11-05,"Zhibin Wen, Bin Li",http://arxiv.org/pdf/2411.02851v1,cs.CL
Unified Pathological Speech Analysis with Prompt Tuning,"Pathological speech analysis has been of interest in the detection of certain
diseases like depression and Alzheimer's disease and attracts much interest
from researchers. However, previous pathological speech analysis models are
commonly designed for a specific disease while overlooking the connection
between diseases, which may constrain performance and lower training
efficiency. Instead of fine-tuning deep models for different tasks, prompt
tuning is a much more efficient training paradigm. We thus propose a unified
pathological speech analysis system for as many as three diseases with the
prompt tuning technique. This system uses prompt tuning to adjust only a small
part of the parameters to detect different diseases from speeches of possible
patients. Our system leverages a pre-trained spoken language model and
demonstrates strong performance across multiple disorders while only
fine-tuning a fraction of the parameters. This efficient training approach
leads to faster convergence and improved F1 scores by allowing knowledge to be
shared across tasks. Our experiments on Alzheimer's disease, Depression, and
Parkinson's disease show competitive results, highlighting the effectiveness of
our method in pathological speech analysis.",2024-11-05,"Fei Yang, Xuenan Xu, Mengyue Wu, Kai Yu",http://arxiv.org/pdf/2411.04142v1,cs.CL
A Comparative Study on the Impact of Test-Driven Development (TDD) and Behavior-Driven Development (BDD) on Enterprise Software Delivery Effectiveness,"This paper compares the impact of Test-Driven Development (TDD) and
Behavior-Driven Development (BDD) on software delivery effectiveness within
enterprise environments. Using a qualitative research design, data were
collected through in-depth interviews with developers and project managers from
enterprises adopting TDD or BDD. Moreover, the findings reveal distinct effects
of each model on delivery speed, software quality, and team collaboration.
Specifically, TDD emphasizes early testing and iterative development, leading
to enhanced code quality and fewer defects, while BDD improves cross-functional
communication by focusing on behavior specifications that involve stakeholders
directly. However, TDD may create a higher initial time investment, and BDD
might encounter challenges in requirement clarity. These differences highlight
gaps in understanding how each model aligns with varying project types and
stakeholder needs, which can guide enterprises in selecting the most suitable
model for their unique requirements. The study contributes to the literature by
providing insights into the practical application and challenges of TDD and
BDD, suggesting future research on their long-term impacts in diverse settings.",2024-11-05,Jun Cui,http://arxiv.org/pdf/2411.04141v1,cs.CL
PersianRAG: A Retrieval-Augmented Generation System for Persian Language,"Retrieval augmented generation (RAG) models, which integrate large-scale
pre-trained generative models with external retrieval mechanisms, have shown
significant success in various natural language processing (NLP) tasks.
However, applying RAG models in Persian language as a low-resource language,
poses distinct challenges. These challenges primarily involve the
preprocessing, embedding, retrieval, prompt construction, language modeling,
and response evaluation of the system. In this paper, we address the challenges
towards implementing a real-world RAG system for Persian language called
PersianRAG. We propose novel solutions to overcome these obstacles and evaluate
our approach using several Persian benchmark datasets. Our experimental results
demonstrate the capability of the PersianRAG framework to enhance question
answering task in Persian.",2024-11-05,"Hossein Hosseini, Mohammad Sobhan Zare, Amir Hossein Mohammadi, Arefeh Kazemi, Zahra Zojaji, Mohammad Ali Nematbakhsh",http://arxiv.org/pdf/2411.02832v2,cs.CL
Mixtures of In-Context Learners,"In-context learning (ICL) adapts LLMs by providing demonstrations without
fine-tuning the model parameters; however, it does not differentiate between
demonstrations and quadratically increases the complexity of Transformer LLMs,
exhausting the memory. As a solution, we propose Mixtures of In-Context
Learners (MoICL), a novel approach to treat subsets of demonstrations as
experts and learn a weighting function to merge their output distributions
based on a training set. In our experiments, we show performance improvements
on 5 out of 7 classification datasets compared to a set of strong baselines (up
to +13\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of
ICL by reducing the inference time needed to achieve the same performance with
fewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to
+11\%), imbalanced (up to +49\%), or noisy demonstrations (up to +38\%) or can
filter these out from datasets. Overall, MoICL is a more expressive approach to
learning from demonstrations without exhausting the context window or memory.",2024-11-05,"Giwon Hong, Emile van Krieken, Edoardo Ponti, Nikolay Malkin, Pasquale Minervini",http://arxiv.org/pdf/2411.02830v1,cs.CL
DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM Serving,"Large Language Models (LLMs) are increasingly employed in complex workflows,
where different LLMs and fine-tuned variants collaboratively address complex
tasks. However, these systems face significant inefficiencies due to redundant
context processing of the shared context. We propose DroidSpeak, a framework
that optimizes context sharing between fine-tuned LLMs derived from the same
foundational model. DroidSpeak identifies critical layers in the KV cache and
selectively recomputes them, enabling effective reuse of intermediate data
while maintaining high accuracy.
  Our approach balances computational efficiency and task fidelity,
significantly reducing inference latency and throughput bottlenecks.
Experiments on diverse datasets and model pairs demonstrate that DroidSpeak
achieves up to 3x higher throughputs and 2.6x faster prefill times with
negligible accuracy loss compared to full recomputation.",2024-11-05,"Yuhan Liu, Yuyang Huang, Jiayi Yao, Zhuohan Gu, Kuntai Du, Hanchen Li, Yihua Cheng, Junchen Jiang, Shan Lu, Madan Musuvathi, Esha Choukse",http://arxiv.org/pdf/2411.02820v3,cs.CL
The Evolution of RWKV: Advancements in Efficient Language Modeling,"This paper reviews the development of the Receptance Weighted Key Value
(RWKV) architecture, emphasizing its advancements in efficient language
modeling. RWKV combines the training efficiency of Transformers with the
inference efficiency of RNNs through a novel linear attention mechanism. We
examine its core innovations, adaptations across various domains, and
performance advantages over traditional models. The paper also discusses
challenges and future directions for RWKV as a versatile architecture in deep
learning.",2024-11-05,Akul Datta,http://arxiv.org/pdf/2411.02795v1,cs.CL
Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning,"Multimodal Sentiment Analysis (MSA) is an important research area that aims
to understand and recognize human sentiment through multiple modalities. The
complementary information provided by multimodal fusion promotes better
sentiment analysis compared to utilizing only a single modality. Nevertheless,
in real-world applications, many unavoidable factors may lead to situations of
uncertain modality missing, thus hindering the effectiveness of multimodal
modeling and degrading the model's performance. To this end, we propose a
Hierarchical Representation Learning Framework (HRLF) for the MSA task under
uncertain missing modalities. Specifically, we propose a fine-grained
representation factorization module that sufficiently extracts valuable
sentiment information by factorizing modality into sentiment-relevant and
modality-specific representations through crossmodal translation and sentiment
semantic reconstruction. Moreover, a hierarchical mutual information
maximization mechanism is introduced to incrementally maximize the mutual
information between multi-scale representations to align and reconstruct the
high-level semantics in the representations. Ultimately, we propose a
hierarchical adversarial learning mechanism that further aligns and adapts the
latent distribution of sentiment-relevant representations to produce robust
joint multimodal representations. Comprehensive experiments on three datasets
demonstrate that HRLF significantly improves MSA performance under uncertain
modality missing cases.",2024-11-05,"Mingcheng Li, Dingkang Yang, Yang Liu, Shunli Wang, Jiawei Chen, Shuaibing Wang, Jinjie Wei, Yue Jiang, Qingyao Xu, Xiaolu Hou, Mingyang Sun, Ziyun Qian, Dongliang Kou, Lihua Zhang",http://arxiv.org/pdf/2411.02793v1,cs.CL
Language Models and Cycle Consistency for Self-Reflective Machine Translation,"This paper introduces a novel framework that leverages large language models
(LLMs) for machine translation (MT). We start with one conjecture: an ideal
translation should contain complete and accurate information for a strong
enough LLM to recover the original sentence. We generate multiple translation
candidates from a source language A to a target language B, and subsequently
translate these candidates back to the original language A. By evaluating the
cycle consistency between the original and back-translated sentences using
metrics such as token-level precision and accuracy, we implicitly estimate the
translation quality in language B, without knowing its ground-truth. This also
helps to evaluate the LLM translation capability, only with monolingual
corpora. For each source sentence, we identify the translation candidate with
optimal cycle consistency with the original sentence as the final answer. Our
experiments demonstrate that larger LLMs, or the same LLM with more forward
passes during inference, exhibit increased cycle consistency, aligning with the
LLM model size scaling law and test-time computation scaling law. This work
provide methods for, 1) to implicitly evaluate translation quality of a
sentence in the target language, 2), to evaluate capability of LLM for
any-to-any-language translation, and 3), how to generate a better translation
for a specific LLM.",2024-11-05,Jianqiao Wangni,http://arxiv.org/pdf/2411.02791v1,cs.CL
Bridging Personalization and Control in Scientific Personalized Search,"Personalized search is a problem where models benefit from learning user
preferences from per-user historical interaction data. The inferred preferences
enable personalized ranking models to improve the relevance of documents for
users. However, personalization is also seen as opaque in its use of historical
interactions and is not amenable to users' control. Further, personalization
limits the diversity of information users are exposed to. While search results
may be automatically diversified this does little to address the lack of
control over personalization. In response, we introduce a model for
personalized search that enables users to control personalized rankings
proactively. Our model, CtrlCE, is a novel cross-encoder model augmented with
an editable memory built from users' historical interactions. The editable
memory allows cross-encoders to be personalized efficiently and enables users
to control personalized ranking. Next, because all queries do not require
personalization, we introduce a calibrated mixing model which determines when
personalization is necessary. This enables users to control personalization via
their editable memory only when necessary. To thoroughly evaluate CtrlCE, we
demonstrate its empirical performance in four domains of science, its ability
to selectively request user control in a calibration evaluation of the mixing
model, and the control provided by its editable memory in a user study.",2024-11-05,"Sheshera Mysore, Garima Dhanania, Kishor Patil, Surya Kallumadi, Andrew McCallum, Hamed Zamani",http://arxiv.org/pdf/2411.02790v2,cs.CL
Novelty-focused R&D landscaping using transformer and local outlier factor,"While numerous studies have explored the field of research and development
(R&D) landscaping, the preponderance of these investigations has emphasized
predictive analysis based on R&D outcomes, specifically patents, and academic
literature. However, the value of research proposals and novelty analysis has
seldom been addressed. This study proposes a systematic approach to
constructing and navigating the R&D landscape that can be utilized to guide
organizations to respond in a reproducible and timely manner to the challenges
presented by increasing number of research proposals. At the heart of the
proposed approach is the composite use of the transformer-based language model
and the local outlier factor (LOF). The semantic meaning of the research
proposals is captured with our further-trained transformers, thereby
constructing a comprehensive R&D landscape. Subsequently, the novelty of the
newly selected research proposals within the annual landscape is quantified on
a numerical scale utilizing the LOF by assessing the dissimilarity of each
proposal to others preceding and within the same year. A case study examining
research proposals in the energy and resource sector in South Korea is
presented. The systematic process and quantitative outcomes are expected to be
useful decision-support tools, providing future insights regarding R&D planning
and roadmapping.",2024-11-05,Jaewoong Choi,http://arxiv.org/pdf/2411.02738v1,cs.CL
A Natural Language Processing Approach to Support Biomedical Data Harmonization: Leveraging Large Language Models,"Biomedical research requires large, diverse samples to produce unbiased
results. Automated methods for matching variables across datasets can
accelerate this process. Research in this area has been limited, primarily
focusing on lexical matching and ontology based semantic matching. We aimed to
develop new methods, leveraging large language models (LLM) and ensemble
learning, to automate variable matching. Methods: We utilized data from two
GERAS cohort (European and Japan) studies to develop variable matching methods.
We first manually created a dataset by matching 352 EU variables with 1322
candidate JP variables, where matched variable pairs were positive and
unmatched pairs were negative instances. Using this dataset, we developed and
evaluated two types of natural language processing (NLP) methods, which matched
variables based on variable labels and definitions from data dictionaries: (1)
LLM-based and (2) fuzzy matching. We then developed an ensemble-learning
method, using the Random Forest model, to integrate individual NLP methods. RF
was trained and evaluated on 50 trials. Each trial had a random split (4:1) of
training and test sets, with the model's hyperparameters optimized through
cross-validation on the training set. For each EU variable, 1322 candidate JP
variables were ranked based on NLP-derived similarity scores or RF's
probability scores, denoting their likelihood to match the EU variable. Ranking
performance was measured by top-n hit ratio (HRn) and mean reciprocal rank
(MRR). Results:E5 performed best among individual methods, achieving 0.90 HR-30
and 0.70 MRR. RF performed better than E5 on all metrics over 50 trials (P less
than 0.001) and achieved an average HR 30 of 0.98 and MRR of 0.73. LLM-derived
features contributed most to RF's performance. One major cause of errors in
automatic variable matching was ambiguous variable definitions within data
dictionaries.",2024-11-05,"Zexu Li, Suraj P. Prabhu, Zachary T. Popp, Shubhi S. Jain, Vijetha Balakundi, Ting Fang Alvin Ang, Rhoda Au, Jinying Chen",http://arxiv.org/pdf/2411.02730v1,cs.CL
Multimodal Commonsense Knowledge Distillation for Visual Question Answering,"Existing Multimodal Large Language Models (MLLMs) and Visual Language
Pretrained Models (VLPMs) have shown remarkable performances in the general
Visual Question Answering (VQA). However, these models struggle with VQA
questions that require external commonsense knowledge due to the challenges in
generating high-quality prompts and the high computational costs of
fine-tuning. In this work, we propose a novel graph-based multimodal
commonsense knowledge distillation framework that constructs a unified
relational graph over commonsense knowledge, visual objects and questions
through a Graph Convolutional Network (GCN) following a teacher-student
environment. This proposed framework is flexible with any type of teacher and
student models without further fine-tuning, and has achieved competitive
performances on the ScienceQA dataset.",2024-11-05,"Shuo Yang, Siwen Luo, Soyeon Caren Han",http://arxiv.org/pdf/2411.02722v1,cs.CL
Game Plot Design with an LLM-powered Assistant: An Empirical Study with Game Designers,"We introduce GamePlot, an LLM-powered assistant that supports game designers
in crafting immersive narratives for turn-based games, and allows them to test
these games through a collaborative game play and refine the plot throughout
the process. Our user study with 14 game designers shows high levels of both
satisfaction with the generated game plots and sense of ownership over the
narratives, but also reconfirms that LLM are limited in their ability to
generate complex and truly innovative content. We also show that diverse user
populations have different expectations from AI assistants, and encourage
researchers to study how tailoring assistants to diverse user groups could
potentially lead to increased job satisfaction and greater creativity and
innovation over time.",2024-11-05,"Seyed Hossein Alavi, Weijia Xu, Nebojsa Jojic, Daniel Kennett, Raymond T. Ng, Sudha Rao, Haiyan Zhang, Bill Dolan, Vered Shwartz",http://arxiv.org/pdf/2411.02714v1,cs.CL
Exploring Response Uncertainty in MLLMs: An Empirical Evaluation under Misleading Scenarios,"Ensuring that Multimodal Large Language Models (MLLMs) maintain consistency
in their responses is essential for developing trustworthy multimodal
intelligence. However, existing benchmarks include many samples where all MLLMs
\textit{exhibit high response uncertainty when encountering misleading
information}, requiring even 5-15 response attempts per sample to effectively
assess uncertainty. Therefore, we propose a two-stage pipeline: first, we
collect MLLMs' responses without misleading information, and then gather
misleading ones via specific misleading instructions. By calculating the
misleading rate, and capturing both correct-to-incorrect and
incorrect-to-correct shifts between the two sets of responses, we can
effectively metric the model's response uncertainty. Eventually, we establish a
\textbf{\underline{M}}ultimodal \textbf{\underline{U}}ncertainty
\textbf{\underline{B}}enchmark (\textbf{MUB}) that employs both explicit and
implicit misleading instructions to comprehensively assess the vulnerability of
MLLMs across diverse domains. Our experiments reveal that all open-source and
close-source MLLMs are highly susceptible to misleading instructions, with an
average misleading rate exceeding 86\%. To enhance the robustness of MLLMs, we
further fine-tune all open-source MLLMs by incorporating explicit and implicit
misleading data, which demonstrates a significant reduction in misleading
rates. Our code is available at:
\href{https://github.com/Yunkai696/MUB}{https://github.com/Yunkai696/MUB}",2024-11-05,"Yunkai Dang, Mengxi Gao, Yibo Yan, Xin Zou, Yanggan Gu, Aiwei Liu, Xuming Hu",http://arxiv.org/pdf/2411.02708v1,cs.CL
RT-Affordance: Affordances are Versatile Intermediate Representations for Robot Manipulation,"We explore how intermediate policy representations can facilitate
generalization by providing guidance on how to perform manipulation tasks.
Existing representations such as language, goal images, and trajectory sketches
have been shown to be helpful, but these representations either do not provide
enough context or provide over-specified context that yields less robust
policies. We propose conditioning policies on affordances, which capture the
pose of the robot at key stages of the task. Affordances offer expressive yet
lightweight abstractions, are easy for users to specify, and facilitate
efficient learning by transferring knowledge from large internet datasets. Our
method, RT-Affordance, is a hierarchical model that first proposes an
affordance plan given the task language, and then conditions the policy on this
affordance plan to perform manipulation. Our model can flexibly bridge
heterogeneous sources of supervision including large web datasets and robot
trajectories. We additionally train our model on cheap-to-collect in-domain
affordance images, allowing us to learn new tasks without collecting any
additional costly robot trajectories. We show on a diverse set of novel tasks
how RT-Affordance exceeds the performance of existing methods by over 50%, and
we empirically demonstrate that affordances are robust to novel settings.
Videos available at https://snasiriany.me/rt-affordance",2024-11-05,"Soroush Nasiriany, Sean Kirmani, Tianli Ding, Laura Smith, Yuke Zhu, Danny Driess, Dorsa Sadigh, Ted Xiao",http://arxiv.org/pdf/2411.02704v1,cs.CL
JEL: Applying End-to-End Neural Entity Linking in JPMorgan Chase,"Knowledge Graphs have emerged as a compelling abstraction for capturing key
relationship among the entities of interest to enterprises and for integrating
data from heterogeneous sources. JPMorgan Chase (JPMC) is leading this trend by
leveraging knowledge graphs across the organization for multiple mission
critical applications such as risk assessment, fraud detection, investment
advice, etc. A core problem in leveraging a knowledge graph is to link mentions
(e.g., company names) that are encountered in textual sources to entities in
the knowledge graph. Although several techniques exist for entity linking, they
are tuned for entities that exist in Wikipedia, and fail to generalize for the
entities that are of interest to an enterprise. In this paper, we propose a
novel end-to-end neural entity linking model (JEL) that uses minimal context
information and a margin loss to generate entity embeddings, and a Wide & Deep
Learning model to match character and semantic information respectively. We
show that JEL achieves the state-of-the-art performance to link mentions of
company names in financial news with entities in our knowledge graph. We report
on our efforts to deploy this model in the company-wide system to generate
alerts in response to financial news. The methodology used for JEL is directly
applicable and usable by other enterprises who need entity linking solutions
for data that are unique to their respective situations.",2024-11-05,"Wanying Ding, Vinay K. Chaudhri, Naren Chittar, Krishna Konakanchi",http://arxiv.org/pdf/2411.02695v1,cs.CL
On the Loss of Context-awareness in General Instruction Fine-tuning,"Pre-trained Large Language Models (LLMs) require post-training methods such
as supervised fine-tuning (SFT) on instruction-response pairs to enable
instruction following. However, this process can potentially harm existing
capabilities learned during pre-training. In this paper, we investigate the
loss of context awareness after SFT, where context awareness is defined as the
ability to extract and understand information from user-provided context and
respond accordingly. We identify and demonstrate that the loss of context
awareness, particularly in open-source models, occurs in instruction fine-tuned
LLMs when the chat template is applied to input prompts. We identify that the
performance decline is associated with a bias toward different roles learned
during conversational instruction fine-tuning. We demonstrate this correlation
by visualizing changes in attention allocation after the chat template is
applied and manually steering the attention heads. The bias can be learned from
training examples that align with the model's internal knowledge and rely less
on the user-provided context to generate correct responses. Based on these
observations, we propose a metric to identify context-dependent examples from
general instruction fine-tuning datasets. We then apply conditional instruction
fine-tuning with a context-dependency indicator, enabling the model to preserve
context awareness after SFT. Empirical experiments on four context-dependent
downstream tasks and three pre-trained LLMs of different sizes show that our
method effectively mitigates the loss of context awareness without compromising
general instruction-following capabilities.",2024-11-05,"Yihan Wang, Andrew Bai, Nanyun Peng, Cho-Jui Hsieh",http://arxiv.org/pdf/2411.02688v3,cs.CL
Wave Network: An Ultra-Small Language Model,"We propose an innovative token representation and update method in a new
ultra-small language model: the Wave network. Specifically, we use a complex
vector to represent each token, encoding both global and local semantics of the
input text. A complex vector consists of two components: a magnitude vector
representing the global semantics of the input text, and a phase vector
capturing the relationships between individual tokens and global semantics.
Experiments on the AG News text classification task demonstrate that, when
generating complex vectors from randomly initialized token embeddings, our
single-layer Wave Network achieves 90.91% accuracy with wave interference and
91.66% with wave modulation - outperforming a single Transformer layer using
BERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching
the accuracy of the pre-trained and fine-tuned BERT base model (94.64%).
Additionally, compared to BERT base, the Wave Network reduces video memory
usage and training time by 77.34% and 85.62% during wave modulation. In
summary, we used a 2.4-million-parameter small language model to achieve
accuracy comparable to a 100-million-parameter BERT model in text
classification.",2024-11-04,"Xin Zhang, Victor S. Sheng",http://arxiv.org/pdf/2411.02674v4,cs.CL
Fair In-Context Learning via Latent Concept Variables,"The emerging in-context learning (ICL) ability of large language models
(LLMs) has prompted their use for predictive tasks in various domains with
different types of data facilitated by serialization methods. However, with
increasing applications in high-stakes domains, it has been shown that LLMs can
inherit social bias and discrimination from their pre-training data. In this
work, we investigate this inherent bias in LLMs during in-context learning with
tabular data. We focus on an optimal demonstration selection approach that
utilizes latent concept variables for resource-efficient task adaptation. We
design data augmentation strategies that reduce correlation between predictive
outcomes and sensitive variables helping to promote fairness during latent
concept learning. We utilize the learned concept and select demonstrations from
a training dataset to obtain fair predictions during inference while
maintaining model utility. The latent concept variable is learned using a
smaller internal LLM and the selected demonstrations can be used for inference
with larger external LLMs. We empirically verify that the fair latent variable
approach improves fairness results on tabular datasets compared to multiple
heuristic demonstration selection methods.",2024-11-04,"Karuna Bhaila, Minh-Hao Van, Kennedy Edemacu, Chen Zhao, Feng Chen, Xintao Wu",http://arxiv.org/pdf/2411.02671v1,cs.CL
Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge,"Rare diseases present unique challenges in healthcare, often suffering from
delayed diagnosis and fragmented information landscapes. The scarcity of
reliable knowledge in these conditions poses a distinct challenge for Large
Language Models (LLMs) in supporting clinical management and delivering precise
patient information underscoring the need for focused training on these 'zebra'
cases. We present Zebra-Llama, a specialized context-aware language model with
high precision Retrieval Augmented Generation (RAG) capability, focusing on
Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000
individuals, exemplifies the complexities of rare diseases with its diverse
symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing
a novel context-aware fine-tuning methodology trained on questions derived from
medical literature, patient experiences, and clinical resources, along with
expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities
in handling EDS-related queries. On a test set of real-world questions
collected from EDS patients and clinicians, medical experts evaluated the
responses generated by both models, revealing Zebra-Llama's substantial
improvements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs.
70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation
reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama
not only provides more accessible and reliable EDS information but also
establishes a framework for developing specialized AI solutions for other rare
conditions. This work represents a crucial step towards democratizing
expert-level knowledge in rare disease management, potentially transforming how
healthcare providers and patients navigate the complex landscape of rare
diseases.",2024-11-04,"Karthik Soman, Andrew Langdon, Catalina Villouta, Chinmay Agrawal, Lashaw Salta, Braian Peetoom, Gianmarco Bellucci, Orion J Buske",http://arxiv.org/pdf/2411.02657v1,cs.CL
A Comparative Analysis of Counterfactual Explanation Methods for Text Classifiers,"Counterfactual explanations can be used to interpret and debug text
classifiers by producing minimally altered text inputs that change a
classifier's output. In this work, we evaluate five methods for generating
counterfactual explanations for a BERT text classifier on two datasets using
three evaluation metrics. The results of our experiments suggest that
established white-box substitution-based methods are effective at generating
valid counterfactuals that change the classifier's output. In contrast, newer
methods based on large language models (LLMs) excel at producing natural and
linguistically plausible text counterfactuals but often fail to generate valid
counterfactuals that alter the classifier's output. Based on these results, we
recommend developing new counterfactual explanation methods that combine the
strengths of established gradient-based approaches and newer LLM-based
techniques to generate high-quality, valid, and plausible text counterfactual
explanations.",2024-11-04,"Stephen McAleese, Mark Keane",http://arxiv.org/pdf/2411.02643v1,cs.CL
Extracting Unlearned Information from LLMs with Activation Steering,"An unintended consequence of the vast pretraining of Large Language Models
(LLMs) is the verbatim memorization of fragments of their training data, which
may contain sensitive or copyrighted information. In recent years, unlearning
has emerged as a solution to effectively remove sensitive knowledge from models
after training. Yet, recent work has shown that supposedly deleted information
can still be extracted by malicious actors through various attacks. Still,
current attacks retrieve sets of possible candidate generations and are unable
to pinpoint the output that contains the actual target information. We propose
activation steering as a method for exact information retrieval from unlearned
LLMs. We introduce a novel approach to generating steering vectors, named
Anonymized Activation Steering. Additionally, we develop a simple word
frequency method to pinpoint the correct answer among a set of candidates when
retrieving unlearned information. Our evaluation across multiple unlearning
techniques and datasets demonstrates that activation steering successfully
recovers general knowledge (e.g., widely known fictional characters) while
revealing limitations in retrieving specific information (e.g., details about
non-public individuals). Overall, our results demonstrate that exact
information retrieval from unlearned models is possible, highlighting a severe
vulnerability of current unlearning techniques.",2024-11-04,"Atakan Seyitoğlu, Aleksei Kuvshinov, Leo Schwinn, Stephan Günnemann",http://arxiv.org/pdf/2411.02631v1,cs.CL
TeleOracle: Fine-Tuned Retrieval-Augmented Generation with Long-Context Support for Network,"The telecommunications industry's rapid evolution demands intelligent systems
capable of managing complex networks and adapting to emerging technologies.
While large language models (LLMs) show promise in addressing these challenges,
their deployment in telecom environments faces significant constraints due to
edge device limitations and inconsistent documentation. To bridge this gap, we
present TeleOracle, a telecom-specialized retrieval-augmented generation (RAG)
system built on the Phi-2 small language model (SLM). To improve context
retrieval, TeleOracle employs a two-stage retriever that incorporates semantic
chunking and hybrid keyword and semantic search. Additionally, we expand the
context window during inference to enhance the model's performance on
open-ended queries. We also employ low-rank adaption for efficient fine-tuning.
A thorough analysis of the model's performance indicates that our RAG framework
is effective in aligning Phi-2 to the telecom domain in a downstream question
and answer (QnA) task, achieving a 30% improvement in accuracy over the base
Phi-2 model, reaching an overall accuracy of 81.20%. Notably, we show that our
model not only performs on par with the much larger LLMs but also achieves a
higher faithfulness score, indicating higher adherence to the retrieved
context.",2024-11-04,"Nouf Alabbasi, Omar Erak, Omar Alhussein, Ismail Lotfi, Sami Muhaidat, Merouane Debbah",http://arxiv.org/pdf/2411.02617v1,cs.CL
Investigating Idiomaticity in Word Representations,"Idiomatic expressions are an integral part of human languages, often used to
express complex ideas in compressed or conventional ways (e.g. eager beaver as
a keen and enthusiastic person). However, their interpretations may not be
straightforwardly linked to the meanings of their individual components in
isolation and this may have an impact for compositional approaches. In this
paper, we investigate to what extent word representation models are able to go
beyond compositional word combinations and capture multiword expression
idiomaticity and some of the expected properties related to idiomatic meanings.
We focus on noun compounds of varying levels of idiomaticity in two languages
(English and Portuguese), presenting a dataset of minimal pairs containing
human idiomaticity judgments for each noun compound at both type and token
levels, their paraphrases and their occurrences in naturalistic and
sense-neutral contexts, totalling 32,200 sentences. We propose this set of
minimal pairs for evaluating how well a model captures idiomatic meanings, and
define a set of fine-grained metrics of Affinity and Scaled Similarity, to
determine how sensitive the models are to perturbations that may lead to
changes in idiomaticity. The results obtained with a variety of representative
and widely used models indicate that, despite superficial indications to the
contrary in the form of high similarities, idiomaticity is not yet accurately
represented in current models. Moreover, the performance of models with
different levels of contextualisation suggests that their ability to capture
context is not yet able to go beyond more superficial lexical clues provided by
the words and to actually incorporate the relevant semantic clues needed for
idiomaticity.",2024-11-04,"Wei He, Tiago Kramer Vieira, Marcos Garcia, Carolina Scarton, Marco Idiart, Aline Villavicencio",http://arxiv.org/pdf/2411.02610v1,cs.CL
FactTest: Factuality Testing in Large Language Models with Finite-Sample and Distribution-Free Guarantees,"The propensity of Large Language Models (LLMs) to generate hallucinations and
non-factual content undermines their reliability in high-stakes domains, where
rigorous control over Type I errors (the conditional probability of incorrectly
classifying hallucinations as truthful content) is essential. Despite its
importance, formal verification of LLM factuality with such guarantees remains
largely unexplored. In this paper, we introduce FactTest, a novel framework
that statistically assesses whether a LLM can confidently provide correct
answers to given questions with high-probability correctness guarantees. We
formulate factuality testing as hypothesis testing problem to enforce an upper
bound of Type I errors at user-specified significance levels. Notably, we prove
that our framework also ensures strong Type II error control under mild
conditions and can be extended to maintain its effectiveness when covariate
shifts exist. Our approach is distribution-free and works for any number of
human-annotated samples. It is model-agnostic and applies to any black-box or
white-box LM. Extensive experiments on question-answering (QA) and
multiple-choice benchmarks demonstrate that FactTest effectively detects
hallucinations and improves the model's ability to abstain from answering
unknown questions, leading to an over 40% accuracy improvement.",2024-11-04,"Fan Nie, Xiaotian Hou, Shuhang Lin, James Zou, Huaxiu Yao, Linjun Zhang",http://arxiv.org/pdf/2411.02603v3,cs.CL
Vocal Sandbox: Continual Learning and Adaptation for Situated Human-Robot Collaboration,"We introduce Vocal Sandbox, a framework for enabling seamless human-robot
collaboration in situated environments. Systems in our framework are
characterized by their ability to adapt and continually learn at multiple
levels of abstraction from diverse teaching modalities such as spoken dialogue,
object keypoints, and kinesthetic demonstrations. To enable such adaptation, we
design lightweight and interpretable learning algorithms that allow users to
build an understanding and co-adapt to a robot's capabilities in real-time, as
they teach new behaviors. For example, after demonstrating a new low-level
skill for ""tracking around"" an object, users are provided with trajectory
visualizations of the robot's intended motion when asked to track a new object.
Similarly, users teach high-level planning behaviors through spoken dialogue,
using pretrained language models to synthesize behaviors such as ""packing an
object away"" as compositions of low-level skills $-$ concepts that can be
reused and built upon. We evaluate Vocal Sandbox in two settings: collaborative
gift bag assembly and LEGO stop-motion animation. In the first setting, we run
systematic ablations and user studies with 8 non-expert participants,
highlighting the impact of multi-level teaching. Across 23 hours of total robot
interaction time, users teach 17 new high-level behaviors with an average of 16
novel low-level skills, requiring 22.1% less active supervision compared to
baselines and yielding more complex autonomous performance (+19.7%) with fewer
failures (-67.1%). Qualitatively, users strongly prefer Vocal Sandbox systems
due to their ease of use (+20.6%) and overall performance (+13.9%). Finally, we
pair an experienced system-user with a robot to film a stop-motion animation;
over two hours of continuous collaboration, the user teaches progressively more
complex motion skills to shoot a 52 second (232 frame) movie.",2024-11-04,"Jennifer Grannen, Siddharth Karamcheti, Suvir Mirchandani, Percy Liang, Dorsa Sadigh",http://arxiv.org/pdf/2411.02599v1,cs.CL
"""It's a conversation, not a quiz"": A Risk Taxonomy and Reflection Tool for LLM Adoption in Public Health","Recent breakthroughs in large language models (LLMs) have generated both
interest and concern about their potential adoption as accessible information
sources or communication tools across different domains. In public health --
where stakes are high and impacts extend across populations -- adopting LLMs
poses unique challenges that require thorough evaluation. However, structured
approaches for assessing potential risks in public health remain
under-explored. To address this gap, we conducted focus groups with health
professionals and health issue experiencers to unpack their concerns, situated
across three distinct and critical public health issues that demand
high-quality information: vaccines, opioid use disorder, and intimate partner
violence. We synthesize participants' perspectives into a risk taxonomy,
distinguishing and contextualizing the potential harms LLMs may introduce when
positioned alongside traditional health communication. This taxonomy highlights
four dimensions of risk in individual behaviors, human-centered care,
information ecosystem, and technology accountability. For each dimension, we
discuss specific risks and example reflection questions to help practitioners
adopt a risk-reflexive approach. This work offers a shared vocabulary and
reflection tool for experts in both computing and public health to
collaboratively anticipate, evaluate, and mitigate risks in deciding when to
employ LLM capabilities (or not) and how to mitigate harm when they are used.",2024-11-04,"Jiawei Zhou, Amy Z. Chen, Darshi Shah, Laura Schwab Reese, Munmun De Choudhury",http://arxiv.org/pdf/2411.02594v1,cs.CL
Geometry of orofacial neuromuscular signals: speech articulation decoding using surface electromyography,"Each year, millions of individuals lose the ability to speak intelligibly due
to causes such as neuromuscular disease, stroke, trauma, and head/neck cancer
surgery (e.g. laryngectomy) or treatment (e.g. radiotherapy toxicity to the
speech articulators). Effective communication is crucial for daily activities,
and losing the ability to speak leads to isolation, depression, anxiety, and a
host of detrimental sequelae. Noninvasive surface electromyography (sEMG) has
shown promise to restore speech output in these individuals. The goal is to
collect sEMG signals from multiple articulatory sites as people silently
produce speech and then decode the signals to enable fluent and natural
communication. Currently, many fundamental properties of orofacial
neuromuscular signals relating to speech articulation remain unanswered. They
include questions relating to 1) the data structure of the orofacial sEMG
signals, 2)the signal distribution shift of sEMG across individuals, 3) ability
of sEMG signals to span the entire English language phonetic space during
silent speech articulations, and 4) the generalization capability of
non-invasive sEMG based silent speech interfaces. We address these questions
through a series of experiments involving healthy human subjects. We show that
sEMG signals evince graph data structure and that the signal distribution shift
is given by a change of basis. Furthermore, we show that silently voiced
articulations spanning the entire English language phonetic space can be
decoded using small neural networks which can be trained with little data and
that such architectures work well across individuals. To ensure transparency
and reproducibility, we open-source all the data and codes used in this study.",2024-11-04,"Harshavardhana T. Gowda, Zachary D. McNaughton, Lee M. Miller",http://arxiv.org/pdf/2411.02591v2,cs.CL
Context-Informed Machine Translation of Manga using Multimodal Large Language Models,"Due to the significant time and effort required for handcrafting
translations, most manga never leave the domestic Japanese market. Automatic
manga translation is a promising potential solution. However, it is a budding
and underdeveloped field and presents complexities even greater than those
found in standard translation due to the need to effectively incorporate visual
elements into the translation process to resolve ambiguities. In this work, we
investigate to what extent multimodal large language models (LLMs) can provide
effective manga translation, thereby assisting manga authors and publishers in
reaching wider audiences. Specifically, we propose a methodology that leverages
the vision component of multimodal LLMs to improve translation quality and
evaluate the impact of translation unit size, context length, and propose a
token efficient approach for manga translation. Moreover, we introduce a new
evaluation dataset -- the first parallel Japanese-Polish manga translation
dataset -- as part of a benchmark to be used in future research. Finally, we
contribute an open-source software suite, enabling others to benchmark LLMs for
manga translation. Our findings demonstrate that our proposed methods achieve
state-of-the-art results for Japanese-English translation and set a new
standard for Japanese-Polish.",2024-11-04,"Philip Lippmann, Konrad Skublicki, Joshua Tanner, Shonosuke Ishiwatari, Jie Yang",http://arxiv.org/pdf/2411.02589v2,cs.CL
Social Support Detection from Social Media Texts,"Social support, conveyed through a multitude of interactions and platforms
such as social media, plays a pivotal role in fostering a sense of belonging,
aiding resilience in the face of challenges, and enhancing overall well-being.
This paper introduces Social Support Detection (SSD) as a Natural language
processing (NLP) task aimed at identifying supportive interactions within
online communities. The study presents the task of Social Support Detection
(SSD) in three subtasks: two binary classification tasks and one multiclass
task, with labels detailed in the dataset section. We conducted experiments on
a dataset comprising 10,000 YouTube comments. Traditional machine learning
models were employed, utilizing various feature combinations that encompass
linguistic, psycholinguistic, emotional, and sentiment information.
Additionally, we experimented with neural network-based models using various
word embeddings to enhance the performance of our models across these
subtasks.The results reveal a prevalence of group-oriented support in online
dialogues, reflecting broader societal patterns. The findings demonstrate the
effectiveness of integrating psycholinguistic, emotional, and sentiment
features with n-grams in detecting social support and distinguishing whether it
is directed toward an individual or a group. The best results for different
subtasks across all experiments range from 0.72 to 0.82.",2024-11-04,"Zahra Ahani, Moein Shahiki Tash, Fazlourrahman Balouchzahi, Luis Ramos, Grigori Sidorov, Alexander Gelbukh",http://arxiv.org/pdf/2411.02580v1,cs.CL
MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs,"State-of-the-art retrieval models typically address a straightforward search
scenario, in which retrieval tasks are fixed (e.g., finding a passage to answer
a specific question) and only a single modality is supported for both queries
and retrieved results. This paper introduces techniques for advancing
information retrieval with multimodal large language models (MLLMs), enabling a
broader search scenario, termed universal multimodal retrieval, where multiple
modalities and diverse retrieval tasks are accommodated. To this end, we first
study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16
retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever
is capable of understanding challenging queries, composed of both text and
image, but it underperforms compared to a smaller CLIP retriever in cross-modal
retrieval tasks due to the modality bias exhibited by MLLMs. To address the
issue, we propose modality-aware hard negative mining to mitigate the modality
bias exhibited by MLLM retrievers. Second, we propose continuously fine-tuning
the universal multimodal retriever to enhance its text retrieval capability
while preserving multimodal retrieval capability. As a result, our model,
MM-Embed, achieves state-of-the-art performance on the multimodal retrieval
benchmark M-BEIR, which spans multiple domains and tasks, while also surpassing
the state-of-the-art text retrieval model, NV-Embed-v1, on the MTEB retrieval
benchmark. We also explore prompting the off-the-shelf MLLMs as zero-shot
rerankers to refine the ranking of the candidates from the multimodal
retriever. We find that, through prompt-and-reranking, MLLMs can further
improve multimodal retrieval when the user queries (e.g., text-image composed
queries) are more complex and challenging to understand. These findings also
pave the way for advancing universal multimodal retrieval in the future.",2024-11-04,"Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, Wei Ping",http://arxiv.org/pdf/2411.02571v2,cs.CL
Enhancing Risk Assessment in Transformers with Loss-at-Risk Functions,"In the financial field, precise risk assessment tools are essential for
decision-making. Recent studies have challenged the notion that traditional
network loss functions like Mean Square Error (MSE) are adequate, especially
under extreme risk conditions that can lead to significant losses during market
upheavals. Transformers and Transformer-based models are now widely used in
financial forecasting according to their outstanding performance in
time-series-related predictions. However, these models typically lack
sensitivity to extreme risks and often underestimate great financial losses. To
address this problem, we introduce a novel loss function, the Loss-at-Risk,
which incorporates Value at Risk (VaR) and Conditional Value at Risk (CVaR)
into Transformer models. This integration allows Transformer models to
recognize potential extreme losses and further improves their capability to
handle high-stakes financial decisions. Moreover, we conduct a series of
experiments with highly volatile financial datasets to demonstrate that our
Loss-at-Risk function improves the Transformers' risk prediction and management
capabilities without compromising their decision-making accuracy or efficiency.
The results demonstrate that integrating risk-aware metrics during training
enhances the Transformers' risk assessment capabilities while preserving their
core strengths in decision-making and reasoning across diverse scenarios.",2024-11-04,"Jinghan Zhang, Henry Xie, Xinhao Zhang, Kunpeng Liu",http://arxiv.org/pdf/2411.02558v1,cs.CL
Leveraging Transformer-Based Models for Predicting Inflection Classes of Words in an Endangered Sami Language,"This paper presents a methodology for training a transformer-based model to
classify lexical and morphosyntactic features of Skolt Sami, an endangered
Uralic language characterized by complex morphology. The goal of our approach
is to create an effective system for understanding and analyzing Skolt Sami,
given the limited data availability and linguistic intricacies inherent to the
language. Our end-to-end pipeline includes data extraction, augmentation, and
training a transformer-based model capable of predicting inflection classes.
The motivation behind this work is to support language preservation and
revitalization efforts for minority languages like Skolt Sami. Accurate
classification not only helps improve the state of Finite-State Transducers
(FSTs) by providing greater lexical coverage but also contributes to systematic
linguistic documentation for researchers working with newly discovered words
from literature and native speakers. Our model achieves an average weighted F1
score of 1.00 for POS classification and 0.81 for inflection class
classification. The trained model and code will be released publicly to
facilitate future research in endangered NLP.",2024-11-04,"Khalid Alnajjar, Mika Hämäläinen, Jack Rueter",http://arxiv.org/pdf/2411.02556v1,cs.CL
TripletCLIP: Improving Compositional Reasoning of CLIP via Synthetic Vision-Language Negatives,"Contrastive Language-Image Pretraining (CLIP) models maximize the mutual
information between text and visual modalities to learn representations. This
makes the nature of the training data a significant factor in the efficacy of
CLIP for downstream tasks. However, the lack of compositional diversity in
contemporary image-text datasets limits the compositional reasoning ability of
CLIP. We show that generating ``hard'' negative captions via in-context
learning and synthesizing corresponding negative images with text-to-image
generators offers a solution. We introduce a novel contrastive pre-training
strategy that leverages these hard negative captions and images in an
alternating fashion to train CLIP. We demonstrate that our method, named
TripletCLIP, when applied to existing datasets such as CC3M and CC12M, enhances
the compositional capabilities of CLIP, resulting in an absolute improvement of
over 9% on the SugarCrepe benchmark on an equal computational budget, as well
as improvements in zero-shot image classification and image retrieval. Our
code, models, and data are available at: https://tripletclip.github.io",2024-11-04,"Maitreya Patel, Abhiram Kusumba, Sheng Cheng, Changhoon Kim, Tejas Gokhale, Chitta Baral, Yezhou Yang",http://arxiv.org/pdf/2411.02545v1,cs.CL
MILU: A Multi-task Indic Language Understanding Benchmark,"Evaluating Large Language Models (LLMs) in low-resource and linguistically
diverse languages remains a significant challenge in NLP, particularly for
languages using non-Latin scripts like those spoken in India. Existing
benchmarks predominantly focus on English, leaving substantial gaps in
assessing LLM capabilities in these languages. We introduce MILU, a Multi task
Indic Language Understanding Benchmark, a comprehensive evaluation benchmark
designed to address this gap. MILU spans 8 domains and 41 subjects across 11
Indic languages, reflecting both general and culturally specific knowledge.
With an India-centric design, incorporates material from regional and
state-level examinations, covering topics such as local history, arts,
festivals, and laws, alongside standard subjects like science and mathematics.
We evaluate over 42 LLMs, and find that current LLMs struggle with MILU, with
GPT-4o achieving the highest average accuracy at 74 percent. Open multilingual
models outperform language-specific fine-tuned models, which perform only
slightly better than random baselines. Models also perform better in high
resource languages as compared to low resource ones. Domain-wise analysis
indicates that models perform poorly in culturally relevant areas like Arts and
Humanities, Law and Governance compared to general fields like STEM. To the
best of our knowledge, MILU is the first of its kind benchmark focused on Indic
languages, serving as a crucial step towards comprehensive cultural evaluation.
All code, benchmarks, and artifacts are publicly available to foster open
research.",2024-11-04,"Sshubam Verma, Mohammed Safi Ur Rahman Khan, Vishwajeet Kumar, Rudra Murthy, Jaydeep Sen",http://arxiv.org/pdf/2411.02538v3,cs.CL
INQUIRE: A Natural World Text-to-Image Retrieval Benchmark,"We introduce INQUIRE, a text-to-image retrieval benchmark designed to
challenge multimodal vision-language models on expert-level queries. INQUIRE
includes iNaturalist 2024 (iNat24), a new dataset of five million natural world
images, along with 250 expert-level retrieval queries. These queries are paired
with all relevant images comprehensively labeled within iNat24, comprising
33,000 total matches. Queries span categories such as species identification,
context, behavior, and appearance, emphasizing tasks that require nuanced image
understanding and domain expertise. Our benchmark evaluates two core retrieval
tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)
INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed
evaluation of a range of recent multimodal models demonstrates that INQUIRE
poses a significant challenge, with the best models failing to achieve an
mAP@50 above 50%. In addition, we show that reranking with more powerful
multimodal models can enhance retrieval performance, yet there remains a
significant margin for improvement. By focusing on scientifically-motivated
ecological challenges, INQUIRE aims to bridge the gap between AI capabilities
and the needs of real-world scientific inquiry, encouraging the development of
retrieval systems that can assist with accelerating ecological and biodiversity
research. Our dataset and code are available at
https://inquire-benchmark.github.io",2024-11-04,"Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, Grant Van Horn",http://arxiv.org/pdf/2411.02537v3,cs.CL
Towards Leveraging News Media to Support Impact Assessment of AI Technologies,"Expert-driven frameworks for impact assessments (IAs) may inadvertently
overlook the effects of AI technologies on the public's social behavior,
policy, and the cultural and geographical contexts shaping the perception of AI
and the impacts around its use. This research explores the potentials of
fine-tuning LLMs on negative impacts of AI reported in a diverse sample of
articles from 266 news domains spanning 30 countries around the world to
incorporate more diversity into IAs. Our findings highlight (1) the potential
of fine-tuned open-source LLMs in supporting IA of AI technologies by
generating high-quality negative impacts across four qualitative dimensions:
coherence, structure, relevance, and plausibility, and (2) the efficacy of
small open-source LLM (Mistral-7B) fine-tuned on impacts from news media in
capturing a wider range of categories of impacts that GPT-4 had gaps in
covering.",2024-11-04,"Mowafak Allaham, Kimon Kieslich, Nicholas Diakopoulos",http://arxiv.org/pdf/2411.02536v1,cs.CL
What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length,"When comparing the linguistic capabilities of language models (LMs) with
humans using LM probabilities, factors such as the length of the sequence and
the unigram frequency of lexical items have a significant effect on LM
probabilities in ways that humans are largely robust to. Prior works in
comparing LM and human acceptability judgments treat these effects uniformly
across models, making a strong assumption that models require the same degree
of adjustment to control for length and unigram frequency effects. We propose
MORCELA, a new linking theory between LM scores and acceptability judgments
where the optimal level of adjustment for these effects is estimated from data
via learned parameters for length and unigram frequency. We first show that
MORCELA outperforms a commonly used linking theory for acceptability - SLOR
(Pauls and Klein, 2012; Lau et al. 2017) - across two families of transformer
LMs (Pythia and OPT). Furthermore, we demonstrate that the assumed degrees of
adjustment in SLOR for length and unigram frequency overcorrect for these
confounds, and that larger models require a lower relative degree of adjustment
for unigram frequency, though a significant amount of adjustment is still
necessary for all models. Finally, our subsequent analysis shows that larger
LMs' lower susceptibility to frequency effects can be explained by an ability
to better predict rarer words in context.",2024-11-04,"Lindia Tjuatja, Graham Neubig, Tal Linzen, Sophie Hao",http://arxiv.org/pdf/2411.02528v2,cs.CL
Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin Script Languages,"Although multilingual LLMs have achieved remarkable performance across
benchmarks, we find they continue to underperform on non-Latin script languages
across contemporary LLM families. This discrepancy arises from the fact that
LLMs are pretrained with orthographic scripts, which are dominated by Latin
characters that obscure their shared phonology with non-Latin scripts. We
propose leveraging phonemic transcriptions as complementary signals to induce
script-invariant representations. Our study demonstrates that integrating
phonemic signals improves performance across both non-Latin and Latin script
languages, with a particularly significant impact on closing the performance
gap between the two. Through detailed experiments, we show that phonemic and
orthographic scripts retrieve distinct examples for in-context learning (ICL).
This motivates our proposed Mixed-ICL retrieval strategy, where further
aggregation from both leads to our significant performance improvements for
both Latin script languages (up to 12.6%) and non-Latin script languages (up to
15.1%) compared to randomized ICL retrieval.",2024-11-04,"Hoang H Nguyen, Khyati Mahajan, Vikas Yadav, Julian Salazar, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary",http://arxiv.org/pdf/2411.02398v2,cs.CL
Attacking Vision-Language Computer Agents via Pop-ups,"Autonomous agents powered by large vision and language models (VLM) have
demonstrated significant potential in completing daily computer tasks, such as
browsing the web to book travel and operating desktop software, which requires
agents to understand these interfaces. Despite such visual inputs becoming more
integrated into agentic applications, what types of risks and attacks exist
around them still remain unclear. In this work, we demonstrate that VLM agents
can be easily attacked by a set of carefully designed adversarial pop-ups,
which human users would typically recognize and ignore. This distraction leads
agents to click these pop-ups instead of performing their tasks as usual.
Integrating these pop-ups into existing agent testing environments like OSWorld
and VisualWebArena leads to an attack success rate (the frequency of the agent
clicking the pop-ups) of 86% on average and decreases the task success rate by
47%. Basic defense techniques, such as asking the agent to ignore pop-ups or
including an advertisement notice, are ineffective against the attack.",2024-11-04,"Yanzhe Zhang, Tao Yu, Diyi Yang",http://arxiv.org/pdf/2411.02391v2,cs.CL
Dr. SoW: Density Ratio of Strong-over-weak LLMs for Reducing the Cost of Human Annotation in Preference Tuning,"Preference tuning relies on high-quality human preference data, which is
often expensive and time-consuming to gather. In this paper, we introduce
Dr.SoW (Density Ratio of Strong over Weak) a cost-effective method that
eliminates the reliance for human annotation by leveraging off-the-shelf LLMs
for preference data annotation. Dr.SoW uses the log-density ratio between a
better-aligned and a less-aligned LLM as a reward signal. We evaluate Dr.SoW
across 221 different LLM pairs and empirically find a strong correlation
between the performance gap of the paired models and the quality of the reward
signal. This insight provides a practical guideline for selecting LLMs for data
annotation.
  Additionally, we introduce an end-to-end pipeline that customizes reward
functions based on user query domains. Without fine-tuning, it improves
accuracy on domain-specific evaluations. With a pair of Mistral-7B models,
Dr.SoW achieves a RewardBench score of 82.6, outperforming the best trained
reward functions from same model class and demonstrating competitive
performance against SoTA models in Safety (91.0) and Reasoning (88.0) domains.
Further, we preference-tune Llama-3-8B-Instruct using data annotated by Dr.SoW.
Our approach pushes Llama-3-8B to achieve a 37.4 % (+15.1 %) win rate on
ArenaHard and a 40.7 % (+17.8 %) win rate on length-controlled AlpacaEval 2.0.",2024-11-04,"Guangxuan Xu, Kai Xu, Shivchander Sudalairaj, Hao Wang, Akash Srivastava",http://arxiv.org/pdf/2411.02481v3,cs.CL
Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models,"Large language models (LLMs) have demonstrated remarkable capabilities in
various scientific domains, from natural language processing to complex
problem-solving tasks. Their ability to understand and generate human-like text
has opened up new possibilities for advancing scientific research, enabling
tasks such as data analysis, literature review, and even experimental design.
One of the most promising applications of LLMs in this context is hypothesis
generation, where they can identify novel research directions by analyzing
existing knowledge. However, despite their potential, LLMs are prone to
generating ``hallucinations'', outputs that are plausible-sounding but
factually incorrect. Such a problem presents significant challenges in
scientific fields that demand rigorous accuracy and verifiability, potentially
leading to erroneous or misleading conclusions. To overcome these challenges,
we propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that
enhances LLM hypothesis generation by integrating external, structured
knowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured
reasoning process, organizing their output as a chain of ideas (CoI), and
includes a KG-supported module for the detection of hallucinations. With
experiments on our newly constructed hypothesis generation dataset, we
demonstrate that KG-CoI not only improves the accuracy of LLM-generated
hypotheses but also reduces the hallucination in their reasoning chains,
highlighting its effectiveness in advancing real-world scientific research.",2024-11-04,"Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang",http://arxiv.org/pdf/2411.02382v1,cs.CL
Can Large Language Models generalize analogy solving like people can?,"When we solve an analogy we transfer information from a known context to a
new one through abstract rules and relational similarity. In people, the
ability to solve analogies such as ""body : feet :: table : ?"" emerges in
childhood, and appears to transfer easily to other domains, such as the visual
domain ""( : ) :: < : ?"". Recent research shows that large language models
(LLMs) can solve various forms of analogies. However, can LLMs generalize
analogy solving to new domains like people can? To investigate this, we had
children, adults, and LLMs solve a series of letter-string analogies (e.g., a b
: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek
alphabet), and a far transfer domain (list of symbols). As expected, children
and adults easily generalized their knowledge to unfamiliar domains, whereas
LLMs did not. This key difference between human and AI performance is evidence
that these LLMs still struggle with robust human-like analogical transfer.",2024-11-04,"Claire E. Stevenson, Alexandra Pafford, Han L. J. van der Maas, Melanie Mitchell",http://arxiv.org/pdf/2411.02348v2,cs.CL
Seq-VCR: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning,"Decoder-only Transformers often struggle with complex reasoning tasks,
particularly arithmetic reasoning requiring multiple sequential operations. In
this work, we identify representation collapse in the model's intermediate
layers as a key factor limiting their reasoning capabilities. To address this,
we propose Sequential Variance-Covariance Regularization (Seq-VCR), which
enhances the entropy of intermediate representations and prevents collapse.
Combined with dummy pause tokens as substitutes for chain-of-thought (CoT)
tokens, our method significantly improves performance in arithmetic reasoning
problems. In the challenging $5 \times 5$ integer multiplication task, our
approach achieves $99.5\%$ exact match accuracy, outperforming models of the
same size (which yield $0\%$ accuracy) and GPT-4 with five-shot CoT prompting
($44\%$). We also demonstrate superior results on arithmetic expression and
longest increasing subsequence (LIS) datasets. Our findings highlight the
importance of preventing intermediate layer representation collapse to enhance
the reasoning capabilities of Transformers and show that Seq-VCR offers an
effective solution without requiring explicit CoT supervision.",2024-11-04,"Md Rifat Arefin, Gopeshh Subbaraj, Nicolas Gontier, Yann LeCun, Irina Rish, Ravid Shwartz-Ziv, Christopher Pal",http://arxiv.org/pdf/2411.02344v2,cs.CL
A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification,"Large Language Models (LLMs) have demonstrated impressive capabilities across
diverse Natural Language Processing (NLP) tasks, including language
understanding, reasoning, and generation. However, general-domain LLMs often
struggle with financial tasks due to the technical and specialized nature of
financial texts. This study investigates the efficacy of instruction
fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini,
to enhance their performance in financial text classification tasks. We
fine-tuned both instruction-tuned and base models across four financial
classification tasks, achieving significant improvements in task-specific
performance. Furthermore, we evaluated the zero-shot capabilities of these
fine-tuned models on three unseen complex financial tasks, including argument
classification, deal completeness classification, and causal classification.
Our results indicate while base model fine-tuning led to greater degradation,
instruction-tuned models maintained more robust performance. To address this
degradation, we employed model merging techniques, integrating single-task
domain-specific fine-tuned models with the base model. Using this merging
method resulted in significant enhancements in zero-shot performance, even
exceeding the original model's accuracy on certain datasets. Our findings
underscore the effectiveness of instruction fine-tuning and model merging for
adapting LLMs to specialized financial text classification tasks.",2024-11-04,"Sorouralsadat Fatemi, Yuheng Hu, Maryam Mousavi",http://arxiv.org/pdf/2411.02476v1,cs.CL
WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning,"Large language models (LLMs) have shown remarkable potential as autonomous
agents, particularly in web-based tasks. However, existing LLM web agents
heavily rely on expensive proprietary LLM APIs, while open LLMs lack the
necessary decision-making capabilities. This paper introduces WebRL, a
self-evolving online curriculum reinforcement learning framework designed to
train high-performance web agents using open LLMs. WebRL addresses three key
challenges in building LLM web agents, including the scarcity of training
tasks, sparse feedback signals, and policy distribution drift in online
learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that
generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised
reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure
consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4
models into proficient web agents. On WebArena-Lite, WebRL improves the success
rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B.
These open models significantly surpass the performance of GPT-4-Turbo (17.6%)
and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained
on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's
effectiveness in bridging the gap between open and proprietary LLM-based web
agents, paving the way for more accessible and powerful autonomous web
interaction systems.",2024-11-04,"Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, Yuxiao Dong",http://arxiv.org/pdf/2411.02337v3,cs.CL
Sparsing Law: Towards Large Language Models with Greater Activation Sparsity,"Activation sparsity denotes the existence of substantial weakly-contributed
elements within activation outputs that can be eliminated, benefiting many
important applications concerned with large language models (LLMs). Although
promoting greater activation sparsity within LLMs deserves deep studies,
existing works lack comprehensive and quantitative research on the correlation
between activation sparsity and potentially influential factors. In this paper,
we present a comprehensive study on the quantitative scaling properties and
influential factors of the activation sparsity within decoder-only
Transformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a precise
and performance-aware activation sparsity metric that is applicable to any
activation function. Through extensive experiments, we find several important
phenomena. Firstly, different activation functions exhibit comparable
performance but opposite training-time sparsity trends. The activation ratio
(i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasing
power-law and decreasing logspace power-law with the amount of training data
for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate
that ReLU is more efficient as the activation function than SiLU and can
leverage more training data to improve activation sparsity. Secondly, the
activation ratio linearly increases with the width-depth ratio below a certain
bottleneck point, indicating the potential advantage of a deeper architecture
at a fixed parameter scale. Finally, at similar width-depth ratios, we
surprisingly find that the limit value of activation sparsity varies weakly
with the parameter scale, i.e., the activation patterns within LLMs are
insensitive to the parameter scale. These empirical laws towards LLMs with
greater activation sparsity have important implications for making LLMs more
efficient and interpretable.",2024-11-04,"Yuqi Luo, Chenyang Song, Xu Han, Yingfa Chen, Chaojun Xiao, Zhiyuan Liu, Maosong Sun",http://arxiv.org/pdf/2411.02335v3,cs.CL
Evaluating Creative Short Story Generation in Humans and Large Language Models,"Story-writing is a fundamental aspect of human imagination, relying heavily
on creativity to produce narratives that are novel, effective, and surprising.
While large language models (LLMs) have demonstrated the ability to generate
high-quality stories, their creative story-writing capabilities remain
under-explored. In this work, we conduct a systematic analysis of creativity in
short story generation across 60 LLMs and 60 people using a five-sentence
cue-word-based creative story-writing task. We use measures to automatically
evaluate model- and human-generated stories across several dimensions of
creativity, including novelty, surprise, diversity, and linguistic complexity.
We also collect creativity ratings and Turing Test classifications from
non-expert and expert human raters and LLMs. Automated metrics show that LLMs
generate stylistically complex stories, but tend to fall short in terms of
novelty, surprise and diversity when compared to average human writers. Expert
ratings generally coincide with automated metrics. However, LLMs and
non-experts rate LLM stories to be more creative than human-generated stories.
We discuss why and how these differences in ratings occur, and their
implications for both human and artificial creativity.",2024-11-04,"Mete Ismayilzada, Claire Stevenson, Lonneke van der Plas",http://arxiv.org/pdf/2411.02316v5,cs.CL
MdEval: Massively Multilingual Code Debugging,"Code large language models (LLMs) have made significant progress in code
debugging by directly generating the correct code based on the buggy code
snippet. Programming benchmarks, typically consisting of buggy code snippet and
their associated test cases, are used to assess the debugging capabilities of
LLMs. However, many existing benchmarks primarily focus on Python and are often
limited in terms of language diversity (e.g., DebugBench and DebugEval). To
advance the field of multilingual debugging with LLMs, we propose the first
massively multilingual debugging benchmark, which includes 3.6K test samples of
18 programming languages and covers the automated program repair (APR) task,
the code review (CR) task, and the bug identification (BI) task. Further, we
introduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs
into the correct multilingual queries and solutions (xDebugGen). Further, a
multilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong
baseline specifically to handle the bugs of a wide range of programming
languages (e.g. ""Missing Mut"" in language Rust and ""Misused Macro Definition""
in language C). Our extensive experiments on MDEVAL reveal a notable
performance gap between open-source models and closed-source LLMs (e.g., GPT
and Claude series), highlighting huge room for improvement in multilingual code
debugging scenarios.",2024-11-04,"Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He Zhu, Liran Wang, Ke Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, Tao Sun, Jiaheng Liu, Yunlong Duan, Yu Hao, Liqun Yang, Guanglin Niu, Ge Zhang, Zhoujun Li",http://arxiv.org/pdf/2411.02310v2,cs.CL
CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments,"Customer Relationship Management (CRM) systems are vital for modern
enterprises, providing a foundation for managing customer interactions and
data. Integrating AI agents into CRM systems can automate routine processes and
enhance personalized service. However, deploying and evaluating these agents is
challenging due to the lack of realistic benchmarks that reflect the complexity
of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel
benchmark designed to evaluate AI agents on realistic tasks grounded in
professional work environments. Following guidance from CRM experts and
industry best practices, we designed CRMArena with nine customer service tasks
distributed across three personas: service agent, analyst, and manager. The
benchmark includes 16 commonly used industrial objects (e.g., account, order,
knowledge article, case) with high interconnectivity, along with latent
variables (e.g., complaint habits, policy violations) to simulate realistic
data distributions. Experimental results reveal that state-of-the-art LLM
agents succeed in less than 40% of the tasks with ReAct prompting, and less
than 55% even with function-calling abilities. Our findings highlight the need
for enhanced agent capabilities in function-calling and rule-following to be
deployed in real-world work environments. CRMArena is an open challenge to the
community: systems that can reliably complete tasks showcase direct business
value in a popular work environment.",2024-11-04,"Kung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese, Caiming Xiong, Philippe Laban, Chien-Sheng Wu",http://arxiv.org/pdf/2411.02305v2,cs.CL
The LLM Language Network: A Neuroscientific Approach for Identifying Causally Task-Relevant Units,"Large language models (LLMs) exhibit remarkable capabilities on not just
language tasks, but also various tasks that are not linguistic in nature, such
as logical reasoning and social inference. In the human brain, neuroscience has
identified a core language system that selectively and causally supports
language processing. We here ask whether similar specialization for language
emerges in LLMs. We identify language-selective units within 18 popular LLMs,
using the same localization approach that is used in neuroscience. We then
establish the causal role of these units by demonstrating that ablating LLM
language-selective units -- but not random units -- leads to drastic deficits
in language tasks. Correspondingly, language-selective LLM units are more
aligned to brain recordings from the human language system than random units.
Finally, we investigate whether our localization method extends to other
cognitive domains: while we find specialized networks in some LLMs for
reasoning and social capabilities, there are substantial differences among
models. These findings provide functional and causal evidence for
specialization in large language models, and highlight parallels with the
functional organization in the brain.",2024-11-04,"Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf",http://arxiv.org/pdf/2411.02280v2,cs.CL
Combining Induction and Transduction for Abstract Reasoning,"When learning an input-output mapping from very few examples, is it better to
first infer a latent function that explains the examples, or is it better to
directly predict new test outputs, e.g. using a neural network? We study this
question on ARC by training neural models for induction (inferring latent
functions) and transduction (directly predicting the test output for a given
test input). We train on synthetically generated variations of Python programs
that solve ARC training tasks. We find inductive and transductive models solve
different kinds of test problems, despite having the same training problems and
sharing the same neural architecture: Inductive program synthesis excels at
precise computations, and at composing multiple concepts, while transduction
succeeds on fuzzier perceptual concepts. Ensembling them approaches human-level
performance on ARC.",2024-11-04,"Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu, Kevin Ellis",http://arxiv.org/pdf/2411.02272v4,cs.CL
Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent,"In this paper, we introduce Hunyuan-Large, which is currently the largest
open-source Transformer-based mixture of experts model, with a total of 389
billion parameters and 52 billion activation parameters, capable of handling up
to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior
performance across various benchmarks including language understanding and
generation, logical reasoning, mathematical problem-solving, coding,
long-context, and aggregated tasks, where it outperforms LLama3.1-70B and
exhibits comparable performance when compared to the significantly larger
LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale
synthetic data that is orders larger than in previous literature, a mixed
expert routing strategy, a key-value cache compression technique, and an
expert-specific learning rate strategy. Additionally, we also investigate the
scaling laws and learning rate schedule of mixture of experts models, providing
valuable insights and guidances for future model development and optimization.
The code and checkpoints of Hunyuan-Large are released to facilitate future
innovations and applications.
  Codes: https://github.com/Tencent/Hunyuan-Large
  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",2024-11-04,"Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian, Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Jun Xia, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie Yu, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jianqiang Ma, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao Yu, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, Jie Jiang",http://arxiv.org/pdf/2411.02265v3,cs.CL
Positive Experience Reflection for Agents in Interactive Text Environments,"Intelligent agents designed for interactive environments face significant
challenges in text-based games, a domain that demands complex reasoning and
adaptability. While agents based on large language models (LLMs) using
self-reflection have shown promise, they struggle when initially successful and
exhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour,
a novel approach that addresses these limitations in existing reflection
methods by incorporating positive experiences and managed memory to enrich the
context available to the agent at decision time. Our comprehensive analysis
spans both closed- and open-source LLMs and demonstrates the effectiveness of
Sweet&Sour in improving agent performance, particularly in scenarios where
previous approaches fall short.",2024-11-04,"Philip Lippmann, Matthijs T. J. Spaan, Jie Yang",http://arxiv.org/pdf/2411.02223v1,cs.CL
The Role of DevOps in Enhancing Enterprise Software Delivery Success through R&D Efficiency and Source Code Management,"This study examines the impact of DevOps practices on enterprise software
delivery success, focusing on enhancing R&D efficiency and source code
management (SCM). Using a qualitative methodology, data were collected from
case studies of large-scale enterprises implementing DevOps to explore how
these practices streamline software development processes. Findings reveal that
DevOps significantly improves R&D productivity by fostering cross-functional
collaboration, reducing development cycle times, and enhancing software quality
through effective SCM practices, such as version control and continuous
integration. Additionally, SCM tools within DevOps enable precise change
tracking and reliable code maintenance, further supporting faster, more robust
software delivery. However, the study identifies challenges, including cultural
resistance and tool integration issues, that can hinder DevOps implementation.
Additionally, This research contributes to the growing body of DevOps
literature by highlighting the role of R&D efficiency and SCM as crucial
factors for software delivery success. Future studies should investigate these
factors across diverse industries to validate findings.",2024-11-04,Jun Cui,http://arxiv.org/pdf/2411.02209v1,cs.CL
Improving Steering Vectors by Targeting Sparse Autoencoder Features,"To control the behavior of language models, steering methods attempt to
ensure that outputs of the model satisfy specific pre-defined properties.
Adding steering vectors to the model is a promising method of model control
that is easier than finetuning, and may be more robust than prompting. However,
it can be difficult to anticipate the effects of steering vectors produced by
methods such as CAA [Panickssery et al., 2024] or the direct use of SAE latents
[Templeton et al., 2024]. In our work, we address this issue by using SAEs to
measure the effects of steering vectors, giving us a method that can be used to
understand the causal effect of any steering vector intervention. We use this
method for measuring causal effects to develop an improved steering method,
SAE-Targeted Steering (SAE-TS), which finds steering vectors to target specific
SAE features while minimizing unintended side effects. We show that overall,
SAE-TS balances steering effects with coherence better than CAA and SAE feature
steering, when evaluated on a range of tasks.",2024-11-04,"Sviatoslav Chalnev, Matthew Siu, Arthur Conmy",http://arxiv.org/pdf/2411.02193v2,cs.CL
Grounding Emotional Descriptions to Electrovibration Haptic Signals,"Designing and displaying haptic signals with sensory and emotional attributes
can improve the user experience in various applications. Free-form user
language provides rich sensory and emotional information for haptic design
(e.g., ``This signal feels smooth and exciting''), but little work exists on
linking user descriptions to haptic signals (i.e., language grounding). To
address this gap, we conducted a study where 12 users described the feel of 32
signals perceived on a surface haptics (i.e., electrovibration) display. We
developed a computational pipeline using natural language processing (NLP)
techniques, such as GPT-3.5 Turbo and word embedding methods, to extract
sensory and emotional keywords and group them into semantic clusters (i.e.,
concepts). We linked the keyword clusters to haptic signal features (e.g.,
pulse count) using correlation analysis. The proposed pipeline demonstrates the
viability of a computational approach to analyzing haptic experiences. We
discuss our future plans for creating a predictive model of haptic experience.",2024-11-04,"Guimin Hu, Zirui Zhao, Lukas Heilmann, Yasemin Vardar, Hasti Seifi",http://arxiv.org/pdf/2411.02118v1,cs.CL
AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis,"The evaluation of layer importance in deep learning has been an active area
of research, with significant implications for model optimization and
interpretability. Recently, large language models (LLMs) have gained prominence
across various domains, yet limited studies have explored the functional
importance and performance contributions of individual layers within LLMs,
especially from the perspective of activation distribution. In this work, we
propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining
normalized activation variance and sparsity to assess each layer's contribution
to model performance. By identifying and removing approximately the lowest 25%
of layers based on AVSS, we achieve over 90% of original model performance
across tasks such as question answering, language modeling, and sentiment
classification, indicating that these layers may be non-essential. Our approach
provides a systematic method for identifying less critical layers, contributing
to efficient large language model architectures.",2024-11-04,"Zichen Song, Yuxin Wu, Sitan Huang, Zhongfeng Kang",http://arxiv.org/pdf/2411.02117v1,cs.CL
Advancements and limitations of LLMs in replicating human color-word associations,"Color-word associations play a fundamental role in human cognition and design
applications. Large Language Models (LLMs) have become widely available and
have demonstrated intelligent behaviors in various benchmarks with natural
conversation skills. However, their ability to replicate human color-word
associations remains understudied. We compared multiple generations of LLMs
(from GPT-3 to GPT-4o) against human color-word associations using data
collected from over 10,000 Japanese participants, involving 17 colors and 80
words (10 word from eight categories) in Japanese. Our findings reveal a clear
progression in LLM performance across generations, with GPT-4o achieving the
highest accuracy in predicting the best voted word for each color and category.
However, the highest median performance was approximately 50% even for GPT-4o
with visual inputs (chance level of 10%). Moreover, we found performance
variations across word categories and colors: while LLMs tended to excel in
categories such as Rhythm and Landscape, they struggled with categories such as
Emotions. Interestingly, color discrimination ability estimated from our
color-word association data showed high correlation with human color
discrimination patterns, consistent with previous studies. Thus, despite
reasonable alignment in basic color discrimination, humans and LLMs still
diverge systematically in the words they assign to those colors. Our study
highlights both the advancements in LLM capabilities and their persistent
limitations, raising the possibility of systematic differences in semantic
memory structures between humans and LLMs in representing color-word
associations.",2024-11-04,"Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara",http://arxiv.org/pdf/2411.02116v3,cs.CL
"Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models","While language models have exceptional capabilities at text generation, they
lack a natural inductive bias for emitting numbers and thus struggle in tasks
involving quantitative reasoning, especially arithmetic. One fundamental
limitation is the nature of the Cross Entropy loss, which assumes a nominal
scale and thus cannot convey proximity between generated number tokens. In
response, we here present a regression-like loss that operates purely on token
level. Our proposed Number Token Loss (NTL) comes in two flavors and minimizes
either the Lp norm or the Wasserstein distance between the numerical values of
the real and predicted number tokens. NTL can easily be added to any language
model and extend the Cross Entropy objective during training without runtime
overhead. We evaluate the proposed scheme on various mathematical datasets and
find that it consistently improves performance in math-related tasks. In a
direct comparison on a regression task, we find that NTL can match the
performance of a regression head, despite operating on token level. Finally, we
scale NTL up to 3B parameter models and observe improved performance,
demonstrating its potential for seamless integration into LLMs. We hope that
this work can inspire LLM developers to improve their pretraining objectives.
The code is available via: https://tum-ai.github.io/number-token-loss/",2024-11-04,"Jonas Zausinger, Lars Pennig, Anamarija Kozina, Sean Sdahl, Julian Sikora, Adrian Dendorfer, Timofey Kuznetsov, Mohamad Hagog, Nina Wiedemann, Kacper Chlodny, Vincent Limbach, Anna Ketteler, Thorben Prein, Vishwa Mohan Singh, Michael Morris Danziger, Jannis Born",http://arxiv.org/pdf/2411.02083v2,cs.CL
Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention,"Improving the effectiveness and efficiency of large language models (LLMs)
simultaneously is a critical yet challenging research goal. In this paper, we
find that low-rank pre-training, normally considered as efficient methods that
will compromise performance, can be scalably effective when reduced parameters
are precisely targeted. Specifically, applying the low-dimensional module only
to the attention layer -- resolves this issue and enhances both effectiveness
and efficiency. We refer to this structure as Low-dimensional Projected
Attention (LPA) and provide an explanatory analysis. Through extensive
experimentation at parameter scales of 130M, 370M, and scaling up to 3B, we
have validated the effectiveness and scalability of LPA. Our results show that
LPA model can save up to 12.4% in time while achieving an approximate 5%
improvement in test perplexity (ppl) and on downstream tasks compared with the
vanilla Transformer.",2024-11-04,"Xingtai Lv, Ning Ding, Kaiyan Zhang, Ermo Hua, Ganqu Cui, Bowen Zhou",http://arxiv.org/pdf/2411.02063v1,cs.CL
Explainable cognitive decline detection in free dialogues with a Machine Learning approach based on pre-trained Large Language Models,"Cognitive and neurological impairments are very common, but only a small
proportion of affected individuals are diagnosed and treated, partly because of
the high costs associated with frequent screening. Detecting pre-illness stages
and analyzing the progression of neurological disorders through effective and
efficient intelligent systems can be beneficial for timely diagnosis and early
intervention. We propose using Large Language Models to extract features from
free dialogues to detect cognitive decline. These features comprise high-level
reasoning content-independent features (such as comprehension, decreased
awareness, increased distraction, and memory problems). Our solution comprises
(i) preprocessing, (ii) feature engineering via Natural Language Processing
techniques and prompt engineering, (iii) feature analysis and selection to
optimize performance, and (iv) classification, supported by automatic
explainability. We also explore how to improve Chatgpt's direct cognitive
impairment prediction capabilities using the best features in our models.
Evaluation metrics obtained endorse the effectiveness of a mixed approach
combining feature extraction with Chatgpt and a specialized Machine Learning
model to detect cognitive decline within free-form conversational dialogues
with older adults. Ultimately, our work may facilitate the development of an
inexpensive, non-invasive, and rapid means of detecting and explaining
cognitive decline.",2024-11-04,"Francisco de Arriba-Pérez, Silvia García-Méndez, Javier Otero-Mosquera, Francisco J. González-Castaño",http://arxiv.org/pdf/2411.02036v1,cs.CL
Shortcut Learning in In-Context Learning: A Survey,"Shortcut learning refers to the phenomenon where models employ simple,
non-robust decision rules in practical tasks, which hinders their
generalization and robustness. With the rapid development of large language
models (LLMs) in recent years, an increasing number of studies have shown the
impact of shortcut learning on LLMs. This paper provides a novel perspective to
review relevant research on shortcut learning in In-Context Learning (ICL). It
conducts a detailed exploration of the types of shortcuts in ICL tasks, their
causes, available benchmarks, and strategies for mitigating shortcuts. Based on
corresponding observations, it summarizes the unresolved issues in existing
research and attempts to outline the future research landscape of shortcut
learning.",2024-11-04,"Rui Song, Yingji Li, Lida Shi, Fausto Giunchiglia, Hao Xu",http://arxiv.org/pdf/2411.02018v2,cs.CL
Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task,"The advent of Large Language Models (LLMs) have shown promise in various
creative domains, including culinary arts. However, many LLMs still struggle to
deliver the desired level of culinary creativity, especially when tasked with
adapting recipes to meet specific cultural requirements. This study focuses on
cuisine transfer-applying elements of one cuisine to another-to assess LLMs'
culinary creativity. We employ a diverse set of LLMs to generate and evaluate
culturally adapted recipes, comparing their evaluations against LLM and human
judgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark
to evaluate LLMs' recipe generation abilities in the cuisine transfer task,
assessing their cultural accuracy and creativity in the culinary domain. Our
findings reveal crucial insights into both generative and evaluative
capabilities of LLMs in the culinary domain, highlighting strengths and
limitations in understanding and applying cultural nuances in recipe creation.
The code and dataset used in this project will be openly available in
\url{http://github.com/dmis-lab/CulinaryASH}.",2024-11-04,"Hoonick Lee, Mogan Gim, Donghyeon Park, Donghee Choi, Jaewoo Kang",http://arxiv.org/pdf/2411.01996v1,cs.CL
Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control,"As the development and application of Large Language Models (LLMs) continue
to advance rapidly, enhancing their trustworthiness and aligning them with
human preferences has become a critical area of research. Traditional methods
rely heavily on extensive data for Reinforcement Learning from Human Feedback
(RLHF), but representation engineering offers a new, training-free approach.
This technique leverages semantic features to control the representation of
LLM's intermediate hidden states, enabling the model to meet specific
requirements such as increased honesty or heightened safety awareness. However,
a significant challenge arises when attempting to fulfill multiple requirements
simultaneously. It proves difficult to encode various semantic contents, like
honesty and safety, into a singular semantic feature, restricting its
practicality. In this work, we address this issue through ``Sparse Activation
Control''. By delving into the intrinsic mechanisms of LLMs, we manage to
identify and pinpoint components that are closely related to specific tasks
within the model, i.e., attention heads. These heads display sparse
characteristics that allow for near-independent control over different tasks.
Our experiments, conducted on the open-source Llama series models, have yielded
encouraging results. The models were able to align with human preferences on
issues of safety, factuality, and bias concurrently.",2024-11-04,"Yuxin Xiao, Chaoqun Wan, Yonggang Zhang, Wenxiao Wang, Binbin Lin, Xiaofei He, Xu Shen, Jieping Ye",http://arxiv.org/pdf/2411.02461v1,cs.CL
QCG-Rerank: Chunks Graph Rerank with Query Expansion in Retrieval-Augmented LLMs for Tourism Domain,"Retrieval-Augmented Generation (RAG) mitigates the issue of hallucination in
Large Language Models (LLMs) by integrating information retrieval techniques.
However, in the tourism domain, since the query is usually brief and the
content in the database is diverse, existing RAG may contain a significant
amount of irrelevant or contradictory information contents after retrieval. To
address this challenge, we propose the QCG-Rerank model. This model first
performs an initial retrieval to obtain candidate chunks and then enhances
semantics by extracting critical information to expand the original query.
Next, we utilize the expanded query and candidate chunks to calculate
similarity scores as the initial transition probability and construct the
chunks graph. Subsequently, We iteratively compute the transition probabilities
based on an initial estimate until convergence. The chunks with the highest
score are selected and input into the LLMs to generate responses. We evaluate
the model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets.
The experimental results demonstrate the effectiveness and superiority of the
QCG-Rerank method.",2024-11-04,"Qikai Wei, Mingzhi Yang, Chunlong Han, Jingfu Wei, Minghao Zhang, Feifei Shi, Huansheng Ning",http://arxiv.org/pdf/2411.08724v1,cs.CL
Can Language Models Learn to Skip Steps?,"Trained on vast corpora of human language, language models demonstrate
emergent human-like reasoning abilities. Yet they are still far from true
intelligence, which opens up intriguing opportunities to explore the parallels
of humans and model behaviors. In this work, we study the ability to skip steps
in reasoning - a hallmark of human expertise developed through practice. Unlike
humans, who may skip steps to enhance efficiency or to reduce cognitive load,
models do not inherently possess such motivations to minimize reasoning steps.
To address this, we introduce a controlled framework that stimulates
step-skipping behavior by iteratively refining models to generate shorter and
accurate reasoning paths. Empirical results indicate that models can develop
the step skipping ability under our guidance. Moreover, after fine-tuning on
expanded datasets that include both complete and skipped reasoning sequences,
the models can not only resolve tasks with increased efficiency without
sacrificing accuracy, but also exhibit comparable and even enhanced
generalization capabilities in out-of-domain scenarios. Our work presents the
first exploration into human-like step-skipping ability and provides fresh
perspectives on how such cognitive abilities can benefit AI models.",2024-11-04,"Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, Zheng Zhang",http://arxiv.org/pdf/2411.01855v1,cs.CL
Code-Switching Curriculum Learning for Multilingual Transfer in LLMs,"Large language models (LLMs) now exhibit near human-level performance in
various tasks, but their performance drops drastically after a handful of
high-resource languages due to the imbalance in pre-training data. Inspired by
the human process of second language acquisition, particularly code-switching
(the practice of language alternation in a conversation), we propose
code-switching curriculum learning (CSCL) to enhance cross-lingual transfer for
LLMs. CSCL mimics the stages of human language learning by progressively
training models with a curriculum consisting of 1) token-level code-switching,
2) sentence-level code-switching, and 3) monolingual corpora. Using Qwen 2 as
our underlying model, we demonstrate the efficacy of the CSCL in improving
language transfer to Korean, achieving significant performance gains compared
to monolingual continual pre-training methods. Ablation studies reveal that
both token- and sentence-level code-switching significantly enhance
cross-lingual transfer and that curriculum learning amplifies these effects. We
also extend our findings into various languages, including Japanese
(high-resource) and Indonesian (low-resource), and using two additional models
(Gemma 2 and Phi 3.5). We further show that CSCL mitigates spurious
correlations between language resources and safety alignment, presenting a
robust, efficient framework for more equitable language transfer in LLMs. We
observe that CSCL is effective for low-resource settings where high-quality,
monolingual corpora for language transfer are hardly available.",2024-11-04,"Haneul Yoo, Cheonbok Park, Sangdoo Yun, Alice Oh, Hwaran Lee",http://arxiv.org/pdf/2411.02460v1,cs.CL
Exploring Optimal Transport-Based Multi-Grained Alignments for Text-Molecule Retrieval,"The field of bioinformatics has seen significant progress, making the
cross-modal text-molecule retrieval task increasingly vital. This task focuses
on accurately retrieving molecule structures based on textual descriptions, by
effectively aligning textual descriptions and molecules to assist researchers
in identifying suitable molecular candidates. However, many existing approaches
overlook the details inherent in molecule sub-structures. In this work, we
introduce the Optimal TRansport-based Multi-grained Alignments model (ORMA), a
novel approach that facilitates multi-grained alignments between textual
descriptions and molecules. Our model features a text encoder and a molecule
encoder. The text encoder processes textual descriptions to generate both
token-level and sentence-level representations, while molecules are modeled as
hierarchical heterogeneous graphs, encompassing atom, motif, and molecule nodes
to extract representations at these three levels. A key innovation in ORMA is
the application of Optimal Transport (OT) to align tokens with motifs, creating
multi-token representations that integrate multiple token alignments with their
corresponding motifs. Additionally, we employ contrastive learning to refine
cross-modal alignments at three distinct scales: token-atom, multitoken-motif,
and sentence-molecule, ensuring that the similarities between correctly matched
text-molecule pairs are maximized while those of unmatched pairs are minimized.
To our knowledge, this is the first attempt to explore alignments at both the
motif and multi-token levels. Experimental results on the ChEBI-20 and PCdes
datasets demonstrate that ORMA significantly outperforms existing
state-of-the-art (SOTA) models.",2024-11-04,"Zijun Min, Bingshuai Liu, Liang Zhang, Jia Song, Jinsong Su, Song He, Xiaochen Bo",http://arxiv.org/pdf/2411.11875v1,cs.CL
Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification,"Accurate annotation of educational resources is crucial for effective
personalized learning and resource recommendation in online education. However,
fine-grained knowledge labels often overlap or share similarities, making it
difficult for existing multi-label classification methods to differentiate
them. The label distribution imbalance due to sparsity of human annotations
further intensifies these challenges. To address these issues, this paper
introduces RR2QC, a novel Retrieval Reranking method to multi-label Question
Classification by leveraging label semantics and meta-label refinement. First,
RR2QC improves the pre-training strategy by utilizing semantic relationships
within and across label groups. Second, it introduces a class center learning
task to align questions with label semantics during downstream training.
Finally, this method decomposes labels into meta-labels and uses a meta-label
classifier to rerank the retrieved label sequences. In doing so, RR2QC enhances
the understanding and prediction capability of long-tail labels by learning
from meta-labels that frequently appear in other labels. Additionally, a
mathematical LLM is used to generate solutions for questions, extracting latent
information to further refine the model's insights. Experimental results show
that RR2QC outperforms existing methods in Precision@K and F1 scores across
multiple educational datasets, demonstrating its effectiveness for online
education applications. The code and datasets are available at
https://github.com/78Erii/RR2QC.",2024-11-04,"Shi Dong, Xiaobei Niu, Rui Zhong, Zhifeng Wang, Mingzhang Zuo",http://arxiv.org/pdf/2411.01841v3,cs.CL
TriG-NER: Triplet-Grid Framework for Discontinuous Named Entity Recognition,"Discontinuous Named Entity Recognition (DNER) presents a challenging problem
where entities may be scattered across multiple non-adjacent tokens, making
traditional sequence labelling approaches inadequate. Existing methods
predominantly rely on custom tagging schemes to handle these discontinuous
entities, resulting in models tightly coupled to specific tagging strategies
and lacking generalisability across diverse datasets. To address these
challenges, we propose TriG-NER, a novel Triplet-Grid Framework that introduces
a generalisable approach to learning robust token-level representations for
discontinuous entity extraction. Our framework applies triplet loss at the
token level, where similarity is defined by word pairs existing within the same
entity, effectively pulling together similar and pushing apart dissimilar ones.
This approach enhances entity boundary detection and reduces the dependency on
specific tagging schemes by focusing on word-pair relationships within a
flexible grid structure. We evaluate TriG-NER on three benchmark DNER datasets
and demonstrate significant improvements over existing grid-based
architectures. These results underscore our framework's effectiveness in
capturing complex entity structures and its adaptability to various tagging
schemes, setting a new benchmark for discontinuous entity extraction.",2024-11-04,"Rina Carines Cabral, Soyeon Caren Han, Areej Alhassan, Riza Batista-Navarro, Goran Nenadic, Josiah Poon",http://arxiv.org/pdf/2411.01839v3,cs.CL
Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback,"While textless Spoken Language Models (SLMs) have shown potential in
end-to-end speech-to-speech modeling, they still lag behind text-based Large
Language Models (LLMs) in terms of semantic coherence and relevance. This work
introduces the Align-SLM framework, which leverages preference optimization
inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the
semantic understanding of SLMs. Our approach generates multiple speech
continuations from a given prompt and uses semantic metrics to create
preference data for Direct Preference Optimization (DPO). We evaluate the
framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling,
the spoken version of the StoryCloze dataset for semantic coherence, and other
speech generation metrics, including the GPT4-o score and human evaluation.
Experimental results show that our method achieves state-of-the-art performance
for SLMs on most benchmarks, highlighting the importance of preference
optimization to improve the semantics of SLMs.",2024-11-04,"Guan-Ting Lin, Prashanth Gurunath Shivakumar, Aditya Gourav, Yile Gu, Ankur Gandhe, Hung-yi Lee, Ivan Bulyko",http://arxiv.org/pdf/2411.01834v1,cs.CL
"A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness","Large language models (LLMs) have demonstrated emergent abilities in text
generation, question answering, and reasoning, facilitating various tasks and
domains. Despite their proficiency in various tasks, LLMs like PaLM 540B and
Llama-3.1 405B face limitations due to large parameter sizes and computational
demands, often requiring cloud API use which raises privacy concerns, limits
real-time applications on edge devices, and increases fine-tuning costs.
Additionally, LLMs often underperform in specialized domains such as healthcare
and law due to insufficient domain-specific knowledge, necessitating
specialized models. Therefore, Small Language Models (SLMs) are increasingly
favored for their low inference latency, cost-effectiveness, efficient
development, and easy customization and adaptability. These models are
particularly well-suited for resource-limited environments and domain knowledge
acquisition, addressing LLMs' challenges and proving ideal for applications
that require localized data handling for privacy, minimal inference latency for
efficiency, and domain knowledge acquisition through lightweight fine-tuning.
The rising demand for SLMs has spurred extensive research and development.
However, a comprehensive survey investigating issues related to the definition,
acquisition, application, enhancement, and reliability of SLM remains lacking,
prompting us to conduct a detailed survey on these topics. The definition of
SLMs varies widely, thus to standardize, we propose defining SLMs by their
capability to perform specialized tasks and suitability for
resource-constrained settings, setting boundaries based on the minimal size for
emergent abilities and the maximum size sustainable under resource constraints.
For other aspects, we provide a taxonomy of relevant models/methods and develop
general frameworks for each category to enhance and utilize SLMs effectively.",2024-11-04,"Fali Wang, Zhiwei Zhang, Xianren Zhang, Zongyu Wu, Tzuhao Mo, Qiuhao Lu, Wanjing Wang, Rui Li, Junjie Xu, Xianfeng Tang, Qi He, Yao Ma, Ming Huang, Suhang Wang",http://arxiv.org/pdf/2411.03350v2,cs.CL
Towards Pedagogical LLMs with Supervised Fine Tuning for Computing Education,"This paper investigates supervised fine-tuning of large language models
(LLMs) to improve their pedagogical alignment in computing education,
addressing concerns that LLMs may hinder learning outcomes. The project
utilised a proprietary dataset of 2,500 high quality question/answer pairs from
programming course forums, and explores two research questions: the suitability
of university course forums in contributing to fine-tuning datasets, and how
supervised fine-tuning can improve LLMs' alignment with educational principles
such as constructivism. Initial findings suggest benefits in pedagogical
alignment of LLMs, with deeper evaluations required.",2024-11-04,"Alexandra Vassar, Jake Renzella, Emily Ross, Andrew Taylor",http://arxiv.org/pdf/2411.01765v1,cs.CL
RAGViz: Diagnose and Visualize Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) combines knowledge from domain-specific
sources into large language models to ground answer generation. Current RAG
systems lack customizable visibility on the context documents and the model's
attentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool
that visualizes the attentiveness of the generated tokens in retrieved
documents. With a built-in user interface, retrieval index, and Large Language
Model (LLM) backbone, RAGViz provides two main functionalities: (1) token and
document-level attention visualization, and (2) generation comparison upon
context document addition and removal. As an open-source toolkit, RAGViz can be
easily hosted with a custom embedding model and HuggingFace-supported LLM
backbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,
memory-efficient LLM inference tool, and custom context snippet method, RAGViz
operates efficiently with a median query time of about 5 seconds on a moderate
GPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo
video of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.",2024-11-04,"Tevin Wang, Jingyuan He, Chenyan Xiong",http://arxiv.org/pdf/2411.01751v1,cs.CL
A Multi-Task Role-Playing Agent Capable of Imitating Character Linguistic Styles,"The advent of large language models (LLMs) has significantly propelled the
advancement of Role-Playing Agents (RPAs). However, current Role-Playing Agents
predominantly focus on mimicking a character's fundamental attributes while
neglecting the replication of linguistic style, and they are incapable of
effectively replicating characters when performing tasks beyond multi-turn
dialogues, which results in generated responses that lack authenticity. The
reason current RPAs lack this capability is due to the nature of existing
character datasets, which lack collections of character quotations and are
limited to multi-turn dialogue tasks, constraining the RPA's performance across
other task domains and failing to mimic a character's linguistic style. To
address this gap, we developed a multi-task role-playing dataset named MRstyle,
which encompasses a substantial number of real individuals along with their
quotations and covers seven different tasks. On this basis, we develop
StyleRPA, a Multi-Task Role-Playing Agent (MRPA) that significantly outperforms
recent open-source LLMs and RPAs baselines on 7 tasks including Dialogue,
Dictionary, Composition, Story Generation, Product Description, Music
Commentary, and Open Question Answering. The code and data will be released.",2024-11-04,"Siyuan Chen, Qingyi Si, Chenxu Yang, Yunzhi Liang, Zheng Lin, Huan Liu, Weiping Wang",http://arxiv.org/pdf/2411.02457v1,cs.CL
DynaSaur: Large Language Agents Beyond Predefined Actions,"Existing LLM agent systems typically select actions from a fixed and
predefined set at every step. While this approach is effective in closed,
narrowly-scoped environments, we argue that it presents two major challenges
when deploying LLM agents in real-world scenarios: (1) selecting from a fixed
set of actions significantly restricts the planning and acting capabilities of
LLM agents, and (2) this approach requires substantial human effort to
enumerate and implement all possible actions, which becomes impractical in
complex environments with a vast number of potential actions. In this work, we
propose an LLM agent framework that enables the dynamic creation and
composition of actions in an online manner. In this framework, the agent
interacts with the environment by generating and executing programs written in
a general-purpose programming language at each step. Furthermore, generated
actions are accumulated over time for future reuse. Our extensive experiments
on the GAIA benchmark demonstrate that this framework offers significantly
greater flexibility and outperforms previous methods. Notably, it allows an LLM
agent to recover in scenarios where no relevant action exists in the predefined
set or when existing actions fail due to unforeseen edge cases. At the time of
writing, we hold the top position on the GAIA public leaderboard. Our code can
be found in
\href{https://github.com/adobe-research/dynasaur}{https://github.com/adobe-research/dynasaur}.",2024-11-04,"Dang Nguyen, Viet Dac Lai, Seunghyun Yoon, Ryan A. Rossi, Handong Zhao, Ruiyi Zhang, Puneet Mathur, Nedim Lipka, Yu Wang, Trung Bui, Franck Dernoncourt, Tianyi Zhou",http://arxiv.org/pdf/2411.01747v1,cs.CL
RuAG: Learned-rule-augmented Generation for Large Language Models,"In-context learning (ICL) and Retrieval-Augmented Generation (RAG) have
gained attention for their ability to enhance LLMs' reasoning by incorporating
external knowledge but suffer from limited contextual window size, leading to
insufficient information injection. To this end, we propose a novel framework,
RuAG, to automatically distill large volumes of offline data into interpretable
first-order logic rules, which are injected into LLMs to boost their reasoning
capabilities. Our method begins by formulating the search process relying on
LLMs' commonsense, where LLMs automatically define head and body predicates.
Then, RuAG applies Monte Carlo Tree Search (MCTS) to address the combinational
searching space and efficiently discover logic rules from data. The resulting
logic rules are translated into natural language, allowing targeted knowledge
injection and seamless integration into LLM prompts for LLM's downstream task
reasoning. We evaluate our framework on public and private industrial tasks,
including natural language processing, time-series, decision-making, and
industrial tasks, demonstrating its effectiveness in enhancing LLM's capability
over diverse tasks.",2024-11-04,"Yudi Zhang, Pei Xiao, Lu Wang, Chaoyun Zhang, Meng Fang, Yali Du, Yevgeniy Puzyrev, Randolph Yao, Si Qin, Qingwei Lin, Mykola Pechenizkiy, Dongmei Zhang, Saravan Rajmohan, Qi Zhang",http://arxiv.org/pdf/2411.03349v1,cs.CL
Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models,"Modern optimizers such as AdamW, equipped with momentum and adaptive learning
rate, are designed to escape local minima and explore the vast parameter space.
This exploration is beneficial for finding good loss basins when training from
scratch. It is not necessarily ideal when resuming from a powerful foundation
model because it can lead to large deviations from the pre-trained
initialization and, consequently, worse robustness and generalization. At the
same time, strong regularization on all parameters can lead to under-fitting.
We hypothesize that selectively regularizing the parameter space is the key to
fitting and retraining the pre-trained knowledge. This paper proposes a new
weight decay technique, Selective Projection Decay (SPD), that selectively
imposes a strong penalty on certain layers while allowing others to change
freely. Intuitively, SPD expands and contracts the parameter search space for
layers with consistent and inconsistent loss reduction, respectively.
Experimentally, when equipped with SPD, Adam consistently provides better
in-distribution generalization and out-of-distribution robustness performance
on multiple popular vision and language benchmarks. Code available
at~\url{https://github.com/GT-RIPL/Selective-Projection-Decay.git}",2024-11-03,"Junjiao Tian, Chengyue Huang, Zsolt Kira",http://arxiv.org/pdf/2411.01713v1,cs.CL
SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation,"Spurred by the demand for interpretable models, research on eXplainable AI
for language technologies has experienced significant growth, with feature
attribution methods emerging as a cornerstone of this progress. While prior
work in NLP explored such methods for classification tasks and textual
applications, explainability intersecting generation and speech is lagging,
with existing techniques failing to account for the autoregressive nature of
state-of-the-art models and to provide fine-grained, phonetically meaningful
explanations. We address this gap by introducing Spectrogram Perturbation for
Explainable Speech-to-text Generation (SPES), a feature attribution technique
applicable to sequence generation tasks with autoregressive models. SPES
provides explanations for each predicted token based on both the input
spectrogram and the previously generated tokens. Extensive evaluation on speech
recognition and translation demonstrates that SPES generates explanations that
are faithful and plausible to humans.",2024-11-03,"Dennis Fucci, Marco Gaido, Beatrice Savoldi, Matteo Negri, Mauro Cettolo, Luisa Bentivogli",http://arxiv.org/pdf/2411.01710v2,cs.CL
Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups,"Complex Word Identification (CWI) is an essential step in the lexical
simplification task and has recently become a task on its own. Some variations
of this binary classification task have emerged, such as lexical complexity
prediction (LCP) and complexity evaluation of multi-word expressions (MWE).
Large language models (LLMs) recently became popular in the Natural Language
Processing community because of their versatility and capability to solve
unseen tasks in zero/few-shot settings. Our work investigates LLM usage,
specifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and
closed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE
settings. We evaluate zero-shot, few-shot, and fine-tuning settings and show
that LLMs struggle in certain conditions or achieve comparable results against
existing methods. In addition, we provide some views on meta-learning combined
with prompt learning. In the end, we conclude that the current state of LLMs
cannot or barely outperform existing methods, which are usually much smaller.",2024-11-03,"Răzvan-Alexandru Smădu, David-Gabriel Ion, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel",http://arxiv.org/pdf/2411.01706v1,cs.CL
Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors,"Despite significant advancements, large language models (LLMs) still struggle
with providing accurate answers when lacking domain-specific or up-to-date
knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by
incorporating external knowledge bases, but it also introduces new attack
surfaces. In this paper, we investigate data extraction attacks targeting RAG's
knowledge databases. We show that previous prompt injection-based extraction
attacks largely rely on the instruction-following capabilities of LLMs. As a
result, they fail on models that are less responsive to such malicious prompts
-- for example, our experiments show that state-of-the-art attacks achieve
near-zero success on Gemma-2B-IT. Moreover, even for models that can follow
these instructions, we found fine-tuning may significantly reduce attack
performance. To further reveal the vulnerability, we propose to backdoor RAG,
where a small portion of poisoned data is injected during the fine-tuning phase
to create a backdoor within the LLM. When this compromised LLM is integrated
into a RAG system, attackers can exploit specific triggers in prompts to
manipulate the LLM to leak documents from the retrieval database. By carefully
designing the poisoned data, we achieve both verbatim and paraphrased document
extraction. For example, on Gemma-2B-IT, we show that with only 5\% poisoned
data, our method achieves an average success rate of 94.1\% for verbatim
extraction (ROUGE-L score: 82.1) and 63.6\% for paraphrased extraction (average
ROUGE score: 66.4) across four datasets. These results underscore the privacy
risks associated with the supply chain when deploying RAG systems.",2024-11-03,"Yuefeng Peng, Junda Wang, Hong Yu, Amir Houmansadr",http://arxiv.org/pdf/2411.01705v2,cs.CL
UniGuard: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models,"Multimodal large language models (MLLMs) have revolutionized vision-language
understanding but remain vulnerable to multimodal jailbreak attacks, where
adversarial inputs are meticulously crafted to elicit harmful or inappropriate
responses. We propose UniGuard, a novel multimodal safety guardrail that
jointly considers the unimodal and cross-modal harmful signals. UniGuard trains
a multimodal guardrail to minimize the likelihood of generating harmful
responses in a toxic corpus. The guardrail can be seamlessly applied to any
input prompt during inference with minimal computational costs. Extensive
experiments demonstrate the generalizability of UniGuard across multiple
modalities, attack strategies, and multiple state-of-the-art MLLMs, including
LLaVA, Gemini Pro, GPT-4o, MiniGPT-4, and InstructBLIP. Notably, this robust
defense mechanism maintains the models' overall vision-language understanding
capabilities.",2024-11-03,"Sejoon Oh, Yiqiao Jin, Megha Sharma, Donghyun Kim, Eric Ma, Gaurav Verma, Srijan Kumar",http://arxiv.org/pdf/2411.01703v2,cs.CL
An Exploration of Higher Education Course Evaluation by Large Language Models,"Course evaluation is a critical component in higher education pedagogy. It
not only serves to identify limitations in existing course designs and provide
a basis for curricular innovation, but also to offer quantitative insights for
university administrative decision-making. Traditional evaluation methods,
primarily comprising student surveys, instructor self-assessments, and expert
reviews, often encounter challenges, including inherent subjectivity, feedback
delays, inefficiencies, and limitations in addressing innovative teaching
approaches. Recent advancements in large language models (LLMs) within
artificial intelligence (AI) present promising new avenues for enhancing course
evaluation processes. This study explores the application of LLMs in automated
course evaluation from multiple perspectives and conducts rigorous experiments
across 100 courses at a major university in China. The findings indicate that:
(1) LLMs can be an effective tool for course evaluation; (2) their
effectiveness is contingent upon appropriate fine-tuning and prompt
engineering; and (3) LLM-generated evaluation results demonstrate a notable
level of rationality and interpretability.",2024-11-03,"Bo Yuan, Jiazi Hu",http://arxiv.org/pdf/2411.02455v1,cs.CL
Graph-based Confidence Calibration for Large Language Models,"Reliable confidence estimation is essential for enhancing the trustworthiness
of large language models (LLMs), especially in high-stakes scenarios. Despite
its importance, accurately estimating confidence in LLM responses remains a
significant challenge. In this work, we propose using an auxiliary learning
model to assess response correctness based on the self-consistency of multiple
outputs generated by the LLM. Our method builds a consistency graph to
represent the agreement among multiple responses and uses a graph neural
network (GNN) to estimate the likelihood that each response is correct.
Experiments demonstrate that this method has strong calibration performance on
various benchmark datasets and generalizes well to out-of-domain cases.",2024-11-03,"Yukun Li, Sijia Wang, Lifu Huang, Li-Ping Liu",http://arxiv.org/pdf/2411.02454v2,cs.CL
Unlocking the Theory Behind Scaling 1-Bit Neural Networks,"Recently, 1-bit Large Language Models (LLMs) have emerged, showcasing an
impressive combination of efficiency and performance that rivals traditional
LLMs. Research by Wang et al. (2023); Ma et al. (2024) indicates that the
performance of these 1-bit LLMs progressively improves as the number of
parameters increases, hinting at the potential existence of a Scaling Law for
1-bit Neural Networks. In this paper, we present the first theoretical result
that rigorously establishes this scaling law for 1-bit models. We prove that,
despite the constraint of weights restricted to $\{-1, +1\}$, the dynamics of
model training inevitably align with kernel behavior as the network width
grows. This theoretical breakthrough guarantees convergence of the 1-bit model
to an arbitrarily small loss as width increases. Furthermore, we introduce the
concept of the generalization difference, defined as the gap between the
outputs of 1-bit networks and their full-precision counterparts, and
demonstrate that this difference maintains a negligible level as network width
scales. Building on the work of Kaplan et al. (2020), we conclude by examining
how the training loss scales as a power-law function of the model size, dataset
size, and computational resources utilized for training. Our findings
underscore the promising potential of scaling 1-bit neural networks, suggesting
that int1 could become the standard in future neural network precision.",2024-11-03,"Majid Daliri, Zhao Song, Chiwun Yang",http://arxiv.org/pdf/2411.01663v1,cs.CL
Diagnosing Medical Datasets with Training Dynamics,"This study explores the potential of using training dynamics as an automated
alternative to human annotation for evaluating the quality of training data.
The framework used is Data Maps, which classifies data points into categories
such as easy-to-learn, hard-to-learn, and ambiguous (Swayamdipta et al., 2020).
Swayamdipta et al. (2020) highlight that difficult-to-learn examples often
contain errors, and ambiguous cases significantly impact model training. To
confirm the reliability of these findings, we replicated the experiments using
a challenging dataset, with a focus on medical question answering. In addition
to text comprehension, this field requires the acquisition of detailed medical
knowledge, which further complicates the task. A comprehensive evaluation was
conducted to assess the feasibility and transferability of the Data Maps
framework to the medical domain. The evaluation indicates that the framework is
unsuitable for addressing datasets' unique challenges in answering medical
questions.",2024-11-03,Laura Wenderoth,http://arxiv.org/pdf/2411.01653v1,cs.CL
EcoAct: Economic Agent Determines When to Register What Action,"Recent advancements have enabled Large Language Models (LLMs) to function as
agents that can perform actions using external tools. This requires
registering, i.e., integrating tool information into the LLM context prior to
taking actions. Current methods indiscriminately incorporate all candidate
tools into the agent's context and retain them across multiple reasoning steps.
This process remains opaque to LLM agents and is not integrated into their
reasoning procedures, leading to inefficiencies due to increased context length
from irrelevant tools. To address this, we introduce EcoAct, a tool using
algorithm that allows LLMs to selectively register tools as needed, optimizing
context use. By integrating the tool registration process into the reasoning
procedure, EcoAct reduces computational costs by over 50% in multiple steps
reasoning tasks while maintaining performance, as demonstrated through
extensive experiments. Moreover, it can be plugged into any reasoning pipeline
with only minor modifications to the prompt, making it applicable to LLM agents
now and future.",2024-11-03,"Shaokun Zhang, Jieyu Zhang, Dujian Ding, Mirian Hipolito Garcia, Ankur Mallick, Daniel Madrigal, Menglin Xia, Victor Rühle, Qingyun Wu, Chi Wang",http://arxiv.org/pdf/2411.01643v1,cs.CL
"Leveraging Microservices Architecture for Dynamic Pricing in the Travel Industry: Algorithms, Scalability, and Impact on Revenue and Customer Satisfaction","This research investigates the implementation of a real-time,
microservices-oriented dynamic pricing system for the travel sector. The system
is designed to address factors such as demand, competitor pricing, and other
external circumstances in real-time. Both controlled simulation and real-life
application showed a respectable gain of 22% in revenue generation and a 17%
improvement in pricing response time which concern the issues of scaling and
flexibility of classical pricing mechanisms. Demand forecasting, competitor
pricing strategies, and event-based pricing were implemented as separate
microservices to enhance their scalability and reduce resource consumption by
30% during peak loads. Customers were also more content as depicted by a 15%
increase in satisfaction score post-implementation given the appreciation of
more appropriate pricing. This research enhances the existing literature with
practical illustrations of the possible application of microservices technology
in developing dynamic pricing solutions in a complex and data-driven context.
There exist however areas for improvement for instance inter-service latency
and the need for extensive real-time data pipelines. The present research goes
on to suggest combining these with direct data capture from customer behavior
at the same time as machine learning capacity developments in pricing
algorithms to assist in more accurate real time pricing. It is determined that
the use of microservices is a reasonable and efficient model for dynamic
pricing, allowing the tourism sector to employ evidence-based and customer
centric pricing techniques, which ensures that their profits are not
jeopardized because of the need for customers.",2024-11-03,"Biman Barua, M. Shamim Kaiser",http://arxiv.org/pdf/2411.01636v1,cs.CL
Ontology Population using LLMs,"Knowledge graphs (KGs) are increasingly utilized for data integration,
representation, and visualization. While KG population is critical, it is often
costly, especially when data must be extracted from unstructured text in
natural language, which presents challenges, such as ambiguity and complex
interpretations. Large Language Models (LLMs) offer promising capabilities for
such tasks, excelling in natural language understanding and content generation.
However, their tendency to ``hallucinate'' can produce inaccurate outputs.
Despite these limitations, LLMs offer rapid and scalable processing of natural
language data, and with prompt engineering and fine-tuning, they can
approximate human-level performance in extracting and structuring data for KGs.
This study investigates LLM effectiveness for the KG population, focusing on
the Enslaved.org Hub Ontology. In this paper, we report that compared to the
ground truth, LLM's can extract ~90% of triples, when provided a modular
ontology as guidance in the prompts.",2024-11-03,"Sanaz Saki Norouzi, Adrita Barua, Antrea Christou, Nikita Gautam, Andrew Eells, Pascal Hitzler, Cogan Shimizu",http://arxiv.org/pdf/2411.01612v1,cs.CL
Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM,"Contrastive decoding (CD) (Li et al., 2023) improves the next-token
distribution of a large expert language model (LM) using a small amateur LM.
Although CD is applied to various LMs and domains to enhance open-ended text
generation, it is still unclear why CD often works well, when it could fail,
and how we can make it better. To deepen our understanding of CD, we first
theoretically prove that CD could be viewed as linearly extrapolating the
next-token logits from a huge and hypothetical LM. We also highlight that the
linear extrapolation could make CD unable to output the most obvious answers
that have already been assigned high probabilities by the amateur LM.
  To overcome CD's limitation, we propose a new unsupervised decoding method
called $\mathbf{A}$symptotic $\mathbf{P}$robability $\mathbf{D}$ecoding (APD).
APD explicitly extrapolates the probability curves from the LMs of different
sizes to infer the asymptotic probabilities from an infinitely large LM without
inducing more inference costs than CD. In FactualityPrompts, an open-ended text
generation benchmark, sampling using APD significantly boosts factuality in
comparison to the CD sampling and its variants, and achieves state-of-the-art
results for Pythia 6.9B and OPT 6.7B. Furthermore, in five commonsense QA
datasets, APD is often significantly better than CD and achieves a similar
effect of using a larger LLM. For example, the perplexity of APD on top of
Pythia 6.9B is even lower than the perplexity of Pythia 12B in CommonsenseQA
and LAMBADA.",2024-11-03,"Haw-Shiuan Chang, Nanyun Peng, Mohit Bansal, Anil Ramakrishna, Tagyoung Chung",http://arxiv.org/pdf/2411.01610v1,cs.CL
Are LLMs good pragmatic speakers?,"Large language models (LLMs) are trained on data assumed to include natural
language pragmatics, but do they actually behave like pragmatic speakers? We
attempt to answer this question using the Rational Speech Act (RSA) framework,
which models pragmatic reasoning in human communication. Using the paradigm of
a reference game constructed from the TUNA corpus, we score candidate
referential utterances in both a state-of-the-art LLM (Llama3-8B-Instruct) and
in the RSA model, comparing and contrasting these scores. Given that RSA
requires defining alternative utterances and a truth-conditional meaning
function, we explore such comparison for different choices of each of these
requirements. We find that while scores from the LLM have some positive
correlation with those from RSA, there isn't sufficient evidence to claim that
it behaves like a pragmatic speaker. This initial study paves way for further
targeted efforts exploring different models and settings, including
human-subject evaluation, to see if LLMs truly can, or be made to, behave like
pragmatic speakers.",2024-11-03,"Mingyue Jian, N. Siddharth",http://arxiv.org/pdf/2411.01562v1,cs.CL
LLMs and the Madness of Crowds,"We investigate the patterns of incorrect answers produced by large language
models (LLMs) during evaluation. These errors exhibit highly non-intuitive
behaviors unique to each model. By analyzing these patterns, we measure the
similarities between LLMs and construct a taxonomy that categorizes them based
on their error correlations. Our findings reveal that the incorrect responses
are not randomly distributed but systematically correlated across models,
providing new insights into the underlying structures and relationships among
LLMs.",2024-11-03,William F. Bradley,http://arxiv.org/pdf/2411.01539v2,cs.CL
Enhancing LLM Evaluations: The Garbling Trick,"As large language models (LLMs) become increasingly powerful, traditional
evaluation metrics tend to saturate, making it challenging to distinguish
between models. We propose a general method to transform existing LLM
evaluations into a series of progressively more difficult tasks. These enhanced
evaluations emphasize reasoning capabilities and can reveal relative
performance differences that are not apparent in the original assessments.
  To demonstrate the effectiveness of our approach, we create a new
multiple-choice test corpus, extend it into a family of evaluations, and assess
a collection of LLMs. Our results offer insights into the comparative abilities
of these models, particularly highlighting the differences between base LLMs
and more recent ""reasoning"" models.",2024-11-03,William F. Bradley,http://arxiv.org/pdf/2411.01533v3,cs.CL
DAG: Dictionary-Augmented Generation for Disambiguation of Sentences in Endangered Uralic Languages using ChatGPT,"We showcase that ChatGPT can be used to disambiguate lemmas in two endangered
languages ChatGPT is not proficient in, namely Erzya and Skolt Sami. We augment
our prompt by providing dictionary translations of the candidate lemmas to a
majority language - Finnish in our case. This dictionary augmented generation
approach results in 50\% accuracy for Skolt Sami and 41\% accuracy for Erzya.
On a closer inspection, many of the error types were of the kind even an
untrained human annotator would make.",2024-11-03,Mika Hämäläinen,http://arxiv.org/pdf/2411.01531v1,cs.CL
SinaTools: Open Source Toolkit for Arabic Natural Language Processing,"We introduce SinaTools, an open-source Python package for Arabic natural
language processing and understanding. SinaTools is a unified package allowing
people to integrate it into their system workflow, offering solutions for
various tasks such as flat and nested Named Entity Recognition (NER),
fully-flagged Word Sense Disambiguation (WSD), Semantic Relatedness, Synonymy
Extractions and Evaluation, Lemmatization, Part-of-speech Tagging, Root
Tagging, and additional helper utilities such as corpus processing, text
stripping methods, and diacritic-aware word matching. This paper presents
SinaTools and its benchmarking results, demonstrating that SinaTools
outperforms all similar tools on the aforementioned tasks, such as Flat NER
(87.33%), Nested NER (89.42%), WSD (82.63%), Semantic Relatedness (0.49
Spearman rank), Lemmatization (90.5%), POS tagging (97.5%), among others.
SinaTools can be downloaded from (https://sina.birzeit.edu/sinatools).",2024-11-03,"Tymaa Hammouda, Mustafa Jarrar, Mohammed Khalilia",http://arxiv.org/pdf/2411.01523v1,cs.CL
Integration of Large Vision Language Models for Efficient Post-disaster Damage Assessment and Reporting,"Traditional natural disaster response involves significant coordinated
teamwork where speed and efficiency are key. Nonetheless, human limitations can
delay critical actions and inadvertently increase human and economic losses.
Agentic Large Vision Language Models (LVLMs) offer a new avenue to address this
challenge, with the potential for substantial socio-economic impact,
particularly by improving resilience and resource access in underdeveloped
regions. We introduce DisasTeller, the first multi-LVLM-powered framework
designed to automate tasks in post-disaster management, including on-site
assessment, emergency alerts, resource allocation, and recovery planning. By
coordinating four specialised LVLM agents with GPT-4 as the core model,
DisasTeller autonomously implements disaster response activities, reducing
human execution time and optimising resource distribution. Our evaluations
through both LVLMs and humans demonstrate DisasTeller's effectiveness in
streamlining disaster response. This framework not only supports expert teams
but also simplifies access to disaster management processes for non-experts,
bridging the gap between traditional response methods and LVLM-driven
efficiency.",2024-11-03,"Zhaohui Chen, Elyas Asadi Shamsabadi, Sheng Jiang, Luming Shen, Daniel Dias-da-Costa",http://arxiv.org/pdf/2411.01511v1,cs.CL
High-performance automated abstract screening with large language model ensembles,"Large language models (LLMs) excel in tasks requiring processing and
interpretation of input text. Abstract screening is a labour-intensive
component of systematic review involving repetitive application of inclusion
and exclusion criteria on a large volume of studies identified by a literature
search. Here, LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama 3 70B, Gemini 1.5
Pro, and Claude Sonnet 3.5) were trialled on systematic reviews in a full issue
of the Cochrane Library to evaluate their accuracy in zero-shot binary
classification for abstract screening. Trials over a subset of 800 records
identified optimal prompting strategies and demonstrated superior performance
of LLMs to human researchers in terms of sensitivity (LLM-max = 1.000,
human-max = 0.775), precision (LLM-max = 0.927, human-max = 0.911), and
balanced accuracy (LLM-max = 0.904, human-max = 0.865). The best performing
LLM-prompt combinations were trialled across every replicated search result (n
= 119,691), and exhibited consistent sensitivity (range 0.756-1.000) but
diminished precision (range 0.004-0.096). 66 LLM-human and LLM-LLM ensembles
exhibited perfect sensitivity with a maximal precision of 0.458, with less
observed performance drop in larger trials. Significant variation in
performance was observed between reviews, highlighting the importance of
domain-specific validation before deployment. LLMs may reduce the human labour
cost of systematic review with maintained or improved accuracy and sensitivity.
Systematic review is the foundation of evidence synthesis across academic
disciplines, including evidence-based medicine, and LLMs may increase the
efficiency and quality of this mode of research.",2024-11-03,"Rohan Sanghera, Arun James Thirunavukarasu, Marc El Khoury, Jessica O'Logbon, Yuqing Chen, Archie Watt, Mustafa Mahmood, Hamid Butt, George Nishimura, Andrew Soltan",http://arxiv.org/pdf/2411.02451v2,cs.CL
Sample-Efficient Alignment for LLMs,"We study methods for efficiently aligning large language models (LLMs) with
human preferences given budgeted online feedback. We first formulate the LLM
alignment problem in the frame of contextual dueling bandits. This formulation,
subsuming recent paradigms such as online RLHF and online DPO, inherently
quests for sample-efficient algorithms that incorporate online active
exploration. Leveraging insights from bandit theory, we introduce a unified
algorithm based on Thompson sampling and highlight its applications in two
distinct LLM alignment scenarios. The practical agent that efficiently
implements this algorithm, named SEA (Sample-Efficient Alignment), is
empirically validated through extensive experiments across three model scales
(1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The
results demonstrate that SEA achieves highly sample-efficient alignment with
oracle's preferences, outperforming recent active exploration methods for LLMs.
Additionally, we release the implementation of SEA together with an efficient
codebase designed for online alignment of LLMs, aiming to accelerate future
research in this field.",2024-11-03,"Zichen Liu, Changyu Chen, Chao Du, Wee Sun Lee, Min Lin",http://arxiv.org/pdf/2411.01493v2,cs.CL
Domain-specific Guided Summarization for Mental Health Posts,"In domain-specific contexts, particularly mental health, abstractive
summarization requires advanced techniques adept at handling specialized
content to generate domain-relevant and faithful summaries. In response to
this, we introduce a guided summarizer equipped with a dual-encoder and an
adapted decoder that utilizes novel domain-specific guidance signals, i.e.,
mental health terminologies and contextually rich sentences from the source
document, to enhance its capacity to align closely with the content and context
of guidance, thereby generating a domain-relevant summary. Additionally, we
present a post-editing correction model to rectify errors in the generated
summary, thus enhancing its consistency with the original content in detail.
Evaluation on the MentSum dataset reveals that our model outperforms existing
baseline models in terms of both ROUGE and FactCC scores. Although the
experiments are specifically designed for mental health posts, the methodology
we've developed offers broad applicability, highlighting its versatility and
effectiveness in producing high-quality domain-specific summaries.",2024-11-03,"Lu Qian, Yuqi Wang, Zimu Wang, Haiyang Zhang, Wei Wang, Ting Yu, Anh Nguyen",http://arxiv.org/pdf/2411.01485v1,cs.CL
Teaching Models to Improve on Tape,"Large Language Models (LLMs) often struggle when prompted to generate content
under specific constraints. However, in such cases it is often easy to check
whether these constraints are satisfied or violated. Recent works have shown
that LLMs can benefit from such ""corrective feedback"". Here we claim that this
skill of LLMs can be significantly enhanced via training. We introduce an RL
framework for teaching models to use such rewards, by simulating interaction
sessions, and rewarding the model according to its ability to satisfy the
constraints. We refer to our method as CORGI (Controlled Generation with RL for
Guided Interaction), and evaluate it on a variety of controlled generation
tasks using unlabeled training data. We find that CORGI consistently
outperforms the baseline reinforcement learning method that does not
incorporate conversational feedback. Furthermore, CORGI's interactive framework
enables meta-learning, allowing the LLM to generalize better to guided
interaction in new tasks. Our results clearly show that conversational
optimization, when combined with reinforcement learning, significantly improves
the effectiveness of LLMs in controlled generation contexts.",2024-11-03,"Liat Bezalel, Eyal Orgad, Amir Globerson",http://arxiv.org/pdf/2411.01483v3,cs.CL
DPCL-Diff: The Temporal Knowledge Graph Reasoning Based on Graph Node Diffusion Model with Dual-Domain Periodic Contrastive Learning,"Temporal knowledge graph (TKG) reasoning that infers future missing facts is
an essential and challenging task. Predicting future events typically relies on
closely related historical facts, yielding more accurate results for repetitive
or periodic events. However, for future events with sparse historical
interactions, the effectiveness of this method, which focuses on leveraging
high-frequency historical information, diminishes. Recently, the capabilities
of diffusion models in image generation have opened new opportunities for TKG
reasoning. Therefore, we propose a graph node diffusion model with dual-domain
periodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff)
introduces noise into sparsely related events to simulate new events,
generating high-quality data that better conforms to the actual distribution.
This generative mechanism significantly enhances the model's ability to reason
about new events. Additionally, the dual-domain periodic contrastive learning
(DPCL) maps periodic and non-periodic event entities to Poincar\'e and
Euclidean spaces, leveraging their characteristics to distinguish similar
periodic events effectively. Experimental results on four public datasets
demonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG
models in event prediction, demonstrating our approach's effectiveness. This
study also investigates the combined effectiveness of GNDiff and DPCL in TKG
tasks.",2024-11-03,"Yukun Cao, Lisheng Wang, Luobin Huang",http://arxiv.org/pdf/2411.01477v2,cs.CL
MoCE: Adaptive Mixture of Contextualization Experts for Byte-based Neural Machine Translation,"Byte-based machine translation systems have shown significant potential in
massively multilingual settings. Unicode encoding, which maps each character to
specific byte(s), eliminates the emergence of unknown words, even in new
languages. This avoids out-of-vocabulary risk in multilingual translation and
enables broad language scalability. However, byte-level tokenization results in
sequences that are hard to interpret due to limited semantic information per
byte. Local contextualization has proven effective in assigning initial
semantics to tokens, improving sentence comprehension. Nevertheless, variations
in encoding rules across languages necessitate an adaptive approach for
effective contextualization. To this end, we propose Mixture of
Contextualization Experts (MoCE), adaptively selecting and mixing attention
heads, which are treated as contextualization experts. This enhances the
flexibility of contextualization scales and allows models to search for better
contextualization combinations. Experiment results show that our method
outperforms existing methods without extensive manual adjustment of
hyper-parameters and surpasses subword-based models with fewer parameters in
Ted-59 dataset. Our code is available at https://github.com/ictnlp/MoCE.",2024-11-03,"Langlin Huang, Mengyu Bu, Yang Feng",http://arxiv.org/pdf/2411.01474v2,cs.CL
Hierarchical Sentiment Analysis Framework for Hate Speech Detection: Implementing Binary and Multiclass Classification Strategy,"A significant challenge in automating hate speech detection on social media
is distinguishing hate speech from regular and offensive language. These
identify an essential category of content that web filters seek to remove. Only
automated methods can manage this volume of daily data. To solve this problem,
the community of Natural Language Processing is currently investigating
different ways of hate speech detection. In addition to those, previous
approaches (e.g., Convolutional Neural Networks, multi-channel BERT models, and
lexical detection) have always achieved low precision without carefully
treating other related tasks like sentiment analysis and emotion
classification. They still like to group all messages with specific words in
them as hate speech simply because those terms often appear alongside hateful
rhetoric. In this research, our paper presented the hate speech text
classification system model drawn upon deep learning and machine learning. In
this paper, we propose a new multitask model integrated with shared emotional
representations to detect hate speech across the English language. The
Transformer-based model we used from Hugging Face and sentiment analysis helped
us prevent false positives. Conclusion. We conclude that utilizing sentiment
analysis and a Transformer-based trained model considerably improves hate
speech detection across multiple datasets.",2024-11-03,"Faria Naznin, Md Touhidur Rahman, Shahran Rahman Alve",http://arxiv.org/pdf/2411.05819v1,cs.CL
Classifier-guided Gradient Modulation for Enhanced Multimodal Learning,"Multimodal learning has developed very fast in recent years. However, during
the multimodal training process, the model tends to rely on only one modality
based on which it could learn faster, thus leading to inadequate use of other
modalities. Existing methods to balance the training process always have some
limitations on the loss functions, optimizers and the number of modalities and
only consider modulating the magnitude of the gradients while ignoring the
directions of the gradients. To solve these problems, in this paper, we present
a novel method to balance multimodal learning with Classifier-Guided Gradient
Modulation (CGGM), considering both the magnitude and directions of the
gradients. We conduct extensive experiments on four multimodal datasets:
UPMC-Food 101, CMU-MOSI, IEMOCAP and BraTS 2021, covering classification,
regression and segmentation tasks. The results show that CGGM outperforms all
the baselines and other state-of-the-art methods consistently, demonstrating
its effectiveness and versatility. Our code is available at
https://github.com/zrguo/CGGM.",2024-11-03,"Zirun Guo, Tao Jin, Jingyuan Chen, Zhou Zhao",http://arxiv.org/pdf/2411.01409v1,cs.CL
"Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models","LLMs have demonstrated impressive proficiency in generating coherent and
high-quality text, making them valuable across a range of text-generation
tasks. However, rigorous evaluation of this generated content is crucial, as
ensuring its quality remains a significant challenge due to persistent issues
such as factual inaccuracies and hallucination. This paper introduces three
fine-tuned general-purpose LLM autoevaluators, REC-8B, REC-12B and REC-70B,
specifically designed to evaluate generated text across several dimensions:
faithfulness, instruction following, coherence, and completeness. These models
not only provide ratings for these metrics but also offer detailed explanation
and verifiable citation, thereby enhancing trust in the content. Moreover, the
models support various citation modes, accommodating different requirements for
latency and granularity. Extensive evaluations on diverse benchmarks
demonstrate that our general-purpose LLM auto-evaluator, REC-70B, outperforms
state-of-the-art LLMs, excelling in content evaluation by delivering better
quality explanation and citation with minimal bias. Our REC dataset and models
are available at https://github.com/adelaidehsu/REC.",2024-11-03,"Aliyah R. Hsu, James Zhu, Zhichao Wang, Bin Bi, Shubham Mehrotra, Shiva K. Pentyala, Katherine Tan, Xiang-Bo Mao, Roshanak Omrani, Sougata Chaudhuri, Regunathan Radhakrishnan, Sitaram Asur, Claire Na Cheng, Bin Yu",http://arxiv.org/pdf/2411.02448v3,cs.CL
Artificial Intelligence Driven Course Generation: A Case Study Using ChatGPT,"This study explores Artificial Intelligence use, specifically ChatGPT, in
creating educational content. The study aims to elaborate on using ChatGPT to
create course materials. The main objective is to assess the efficiency,
quality, and impact of AI-driven course generation, and to create a Multimedia
Databases course as a case study. The study highlights the potential of AI to
revolutionize educational content creation, making it more accessible,
personalized, and efficient. The course content was generated in less than one
day through iterative methods, using prompts for translation, content
expansion, practical examples, assignments, supplementary materials, and LaTeX
formatting. Each part was verified immediately after generation to ensure
accuracy. Post-generation analysis with Detectia and Turnitin showed similarity
rates of 8.7% and 13%, indicating high originality. Experts and university
committees reviewed and approved the course, with English university teachers
praising its language quality. ChatGPT also created a well-structured and
diversified exam for the module. Key findings reveal significant time
efficiency, comprehensive content coverage, and high flexibility. The study
underscores AI's transformative potential in education, addressing challenges
related to data privacy, technology dependence, content accuracy, and
algorithmic biases. The conclusions emphasize the need for collaboration
between educators, policymakers, and technology developers to harness AI's
benefits in education fully.",2024-11-02,Djaber Rouabhia,http://arxiv.org/pdf/2411.01369v1,cs.CL
Online and Offline Evaluations of Collaborative Filtering and Content Based Recommender Systems,"Recommender systems are widely used AI applications designed to help users
efficiently discover relevant items. The effectiveness of such systems is tied
to the satisfaction of both users and providers. However, user satisfaction is
complex and cannot be easily framed mathematically using information retrieval
and accuracy metrics. While many studies evaluate accuracy through offline
tests, a growing number of researchers argue that online evaluation methods
such as A/B testing are better suited for this purpose. We have employed a
variety of algorithms on different types of datasets divergent in size and
subject, producing recommendations in various platforms, including media
streaming services, digital publishing websites, e-commerce systems, and news
broadcasting networks. Notably, our target websites and datasets are in Persian
(Farsi) language.
  This study provides a comparative analysis of a large-scale recommender
system that has been operating for the past year across about 70 websites in
Iran, processing roughly 300 requests per second collectively. The system
employs user-based and item-based recommendations using content-based,
collaborative filtering, trend-based methods, and hybrid approaches. Through
both offline and online evaluations, we aim to identify where these algorithms
perform most efficiently and determine the best method for our specific needs,
considering the dataset and system scale. Our methods of evaluation include
manual evaluation, offline tests including accuracy and ranking metrics like
hit-rate@k and nDCG, and online tests consisting of click-through rate (CTR).
Additionally we analyzed and proposed methods to address cold-start and
popularity bias.",2024-11-02,"Ali Elahi, Armin Zirak",http://arxiv.org/pdf/2411.01354v1,cs.CL
AMREx: AMR for Explainable Fact Verification,"With the advent of social media networks and the vast amount of information
circulating through them, automatic fact verification is an essential component
to prevent the spread of misinformation. It is even more useful to have fact
verification systems that provide explanations along with their classifications
to ensure accurate predictions. To address both of these requirements, we
implement AMREx, an Abstract Meaning Representation (AMR)-based veracity
prediction and explanation system for fact verification using a combination of
Smatch, an AMR evaluation metric to measure meaning containment and textual
similarity, and demonstrate its effectiveness in producing partially
explainable justifications using two community standard fact verification
datasets, FEVER and AVeriTeC. AMREx surpasses the AVeriTec baseline accuracy
showing the effectiveness of our approach for real-world claim verification. It
follows an interpretable pipeline and returns an explainable AMR node mapping
to clarify the system's veracity predictions when applicable. We further
demonstrate that AMREx output can be used to prompt LLMs to generate
natural-language explanations using the AMR mappings as a guide to lessen the
probability of hallucinations.",2024-11-02,"Chathuri Jayaweera, Sangpil Youm, Bonnie Dorr",http://arxiv.org/pdf/2411.01343v1,cs.CL
What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks,"Jailbreaks have been a central focus of research regarding the safety and
reliability of large language models (LLMs), yet the mechanisms underlying
these attacks remain poorly understood. While previous studies have
predominantly relied on linear methods to detect jailbreak attempts and model
refusals, we take a different approach by examining both linear and non-linear
features in prompts that lead to successful jailbreaks. First, we introduce a
novel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack
methods. Leveraging this dataset, we train probes to classify successful from
unsuccessful jailbreaks using the latent representations corresponding to
prompt tokens. Notably, we find that even when probes achieve high accuracy in
predicting the success of jailbreaks, their performance often fails to
generalize to unseen attack methods. This reveals that different jailbreaking
strategies exploit different non-linear, non-universal features. Next, we
demonstrate that non-linear probes provide a powerful tool for steering model
behavior. Specifically, we use these probes to guide targeted latent space
perturbations, enabling us to effectively modulate the model's robustness
against jailbreaks. Overall, our findings challenge the assumption that
jailbreaks can be fully understood through linear or simple universal prompt
features alone, highlighting the importance of a nuanced understanding of the
mechanisms behind LLM vulnerabilities.",2024-11-02,"Nathalie Kirch, Constantin Weisser, Severin Field, Helen Yannakoudakis, Stephen Casper",http://arxiv.org/pdf/2411.03343v2,cs.CL
Can Multimodal Large Language Model Think Analogically?,"Analogical reasoning, particularly in multimodal contexts, is the foundation
of human perception and creativity. Multimodal Large Language Model (MLLM) has
recently sparked considerable discussion due to its emergent capabilities. In
this paper, we delve into the multimodal analogical reasoning capability of
MLLM. Specifically, we explore two facets: \textit{MLLM as an explainer} and
\textit{MLLM as a predictor}. In \textit{MLLM as an explainer}, we primarily
focus on whether MLLM can deeply comprehend multimodal analogical reasoning
problems. We propose a unified prompt template and a method for harnessing the
comprehension capabilities of MLLM to augment existing models. In \textit{MLLM
as a predictor}, we aim to determine whether MLLM can directly solve multimodal
analogical reasoning problems. The experiments show that our approach
outperforms existing methods on popular datasets, providing preliminary
evidence for the analogical reasoning capability of MLLM.",2024-11-02,"Diandian Guo, Cong Cao, Fangfang Yuan, Dakui Wang, Wei Ma, Yanbing Liu, Jianhui Fu",http://arxiv.org/pdf/2411.01307v1,cs.CL
Varco Arena: A Tournament Approach to Reference-Free Benchmarking Large Language Models,"Most existing benchmarking approaches for evaluating the output quality of
large language models (LLMs) rely on comparing LLM responses to predefined
references. Such methods, based on static datasets, quickly become outdated as
LLM capabilities and use cases evolve. In this work, we introduce VARCO
Arena--a novel, cost-effective, and robust benchmarking approach that leverages
a single-elimination tournament structure to minimize the number of required
comparisons while eliminating the need for static references or costly human
annotations. We validate our approach through two experiments: (i) a simulation
study that examines its robustness under various conditions, and (ii) an
empirical evaluation using publicly available benchmark prompts. In both
experiments, VARCO Arena consistently outperforms current LLM benchmarking
practices, achieving stronger correlations with human-established Elo ratings.
Our results demonstrate that VARCO Arena not only produces reliable LLM
rankings but also provides a scalable, adaptable solution for qualitative
evaluation across diverse, customized use cases.",2024-11-02,"Seonil Son, Ju-Min Oh, Heegon Jin, Cheolhun Jang, Jeongbeom Jeong, Kuntae Kim",http://arxiv.org/pdf/2411.01281v3,cs.CL
NLP and Education: using semantic similarity to evaluate filled gaps in a large-scale Cloze test in the classroom,"This study examines the applicability of the Cloze test, a widely used tool
for assessing text comprehension proficiency, while highlighting its challenges
in large-scale implementation. To address these limitations, an automated
correction approach was proposed, utilizing Natural Language Processing (NLP)
techniques, particularly word embeddings (WE) models, to assess semantic
similarity between expected and provided answers. Using data from Cloze tests
administered to students in Brazil, WE models for Brazilian Portuguese (PT-BR)
were employed to measure the semantic similarity of the responses. The results
were validated through an experimental setup involving twelve judges who
classified the students' answers. A comparative analysis between the WE models'
scores and the judges' evaluations revealed that GloVe was the most effective
model, demonstrating the highest correlation with the judges' assessments. This
study underscores the utility of WE models in evaluating semantic similarity
and their potential to enhance large-scale Cloze test assessments. Furthermore,
it contributes to educational assessment methodologies by offering a more
efficient approach to evaluating reading proficiency.",2024-11-02,"Túlio Sousa de Gois, Flávia Oliveira Freitas, Julian Tejada, Raquel Meister Ko. Freitag",http://arxiv.org/pdf/2411.01280v1,cs.CL
TODO: Enhancing LLM Alignment with Ternary Preferences,"Aligning large language models (LLMs) with human intent is critical for
enhancing their performance across a variety of tasks. Standard alignment
techniques, such as Direct Preference Optimization (DPO), often rely on the
binary Bradley-Terry (BT) model, which can struggle to capture the complexities
of human preferences -- particularly in the presence of noisy or inconsistent
labels and frequent ties. To address these limitations, we introduce the
Tie-rank Oriented Bradley-Terry model (TOBT), an extension of the BT model that
explicitly incorporates ties, enabling more nuanced preference representation.
Building on this, we propose Tie-rank Oriented Direct Preference Optimization
(TODO), a novel alignment algorithm that leverages TOBT's ternary ranking
system to improve preference alignment. In evaluations on Mistral-7B and Llama
3-8B models, TODO consistently outperforms DPO in modeling preferences across
both in-distribution and out-of-distribution datasets. Additional assessments
using MT Bench and benchmarks such as Piqa, ARC-c, and MMLU further demonstrate
TODO's superior alignment performance. Notably, TODO also shows strong results
in binary preference alignment, highlighting its versatility and potential for
broader integration into LLM alignment. The implementation details can be found
in https://github.com/XXares/TODO.",2024-11-02,"Yuxiang Guo, Lu Yin, Bo Jiang, Jiaqi Zhang",http://arxiv.org/pdf/2411.02442v2,cs.CL
An Innovative CGL-MHA Model for Sarcasm Sentiment Recognition Using the MindSpore Framework,"The pervasive use of the Internet and social media introduces significant
challenges to automated sentiment analysis, particularly for sarcastic
expressions in user-generated content. Sarcasm conveys negative emotions
through ostensibly positive or exaggerated language, complicating its detection
within natural language processing tasks. To address this, we propose an
innovative sarcasm detection model integrating Convolutional Neural Networks
(CNN), Gated Recurrent Units (GRU), Long Short-Term Memory (LSTM), and
Multi-Head Attention mechanisms. The CNN component captures local n-gram
features, while GRU and LSTM layers model sequential dependencies and
contextual information. Multi-Head Attention enhances the model's focus on
relevant parts of the input, improving interpretability. Experiments on two
sarcasm detection datasets, Headlines and Riloff, demonstrate that the model
achieves an accuracy of 81.20% and an F1 score of 80.77% on Headlines, and an
accuracy of 79.72% with an F1 score of 61.39% on Riloff, outperforming
traditional models. These results validate the effectiveness of our hybrid
approach for sarcasm detection in social media texts.",2024-11-02,"Zhenkai Qin, Qining Luo, Xunyi Nong",http://arxiv.org/pdf/2411.01264v1,cs.CL
Diversidade linguística e inclusão digital: desafios para uma ia brasileira,"Linguistic diversity is a human attribute which, with the advance of
generative AIs, is coming under threat. This paper, based on the contributions
of sociolinguistics, examines the consequences of the variety selection bias
imposed by technological applications and the vicious circle of preserving a
variety that becomes dominant and standardized because it has linguistic
documentation to feed the large language models for machine learning.",2024-11-02,Raquel Meister Ko Freitag,http://arxiv.org/pdf/2411.01259v1,cs.CL
PMoL: Parameter Efficient MoE for Preference Mixing of LLM Alignment,"Reinforcement Learning from Human Feedback (RLHF) has been proven to be an
effective method for preference alignment of large language models (LLMs) and
is widely used in the post-training process of LLMs. However, RLHF struggles
with handling multiple competing preferences. This leads to a decrease in the
alignment of LLMs with human preferences. To address this issue, we propose
Preference Mixture of LoRAs (PMoL) from the perspective of model architecture,
which can adapt to any number of preferences to mix. PMoL combines Mixture of
Experts (MoE) and Low Rank Adaptor (LoRA). This architecture is innovatively
applied to the research of preference alignment and has achieved significant
performance improvement. The expert group soft loss is used to enable MoE with
the ability to mix preferences. Through comprehensive evaluation by the reward
model and GPT-4o, the experiment results show that PMoL has superior preference
mixing capabilities compared to baseline methods. PMoL achieves better
preference alignment with lower training costs.",2024-11-02,"Dongxu Liu, Bing Xu, Yinzhuo Chen, Bufan Xu, Wenpeng Lu, Muyun Yang, Tiejun Zhao",http://arxiv.org/pdf/2411.01245v1,cs.CL
$B^4$: A Black-Box Scrubbing Attack on LLM Watermarks,"Watermarking has emerged as a prominent technique for LLM-generated content
detection by embedding imperceptible patterns. Despite supreme performance, its
robustness against adversarial attacks remains underexplored. Previous work
typically considers a grey-box attack setting, where the specific type of
watermark is already known. Some even necessitates knowledge about
hyperparameters of the watermarking method. Such prerequisites are unattainable
in real-world scenarios. Targeting at a more realistic black-box threat model
with fewer assumptions, we here propose $B^4$, a black-box scrubbing attack on
watermarks. Specifically, we formulate the watermark scrubbing attack as a
constrained optimization problem by capturing its objectives with two
distributions, a Watermark Distribution and a Fidelity Distribution. This
optimization problem can be approximately solved using two proxy distributions.
Experimental results across 12 different settings demonstrate the superior
performance of $B^4$ compared with other baselines.",2024-11-02,"Baizhou Huang, Xiao Pu, Xiaojun Wan",http://arxiv.org/pdf/2411.01222v3,cs.CL
"One Arrow, Many Targets: Probing LLMs for Multi-Attribute Controllable Text Summarization","Text summarization is a well-established task within the natural language
processing (NLP) community. However, the focus on controllable summarization
tailored to user requirements is gaining traction only recently. While several
efforts explore controllability in text summarization, the investigation of
Multi-Attribute Controllable Summarization (MACS) remains limited. This work
addresses this gap by examining the MACS task through the lens of large
language models (LLMs), using various learning paradigms, particularly low-rank
adapters. We experiment with different popular adapter fine-tuning strategies
to assess the effectiveness of the resulting models in retaining cues and
patterns associated with multiple controllable attributes. Additionally, we
propose and evaluate a novel hierarchical adapter fusion technique to integrate
learnings from two distinct controllable attributes. Subsquently, we present
our findings, discuss the challenges encountered, and suggest potential avenues
for advancing the MACS task.",2024-11-02,"Tathagato Roy, Rahul Mishra",http://arxiv.org/pdf/2411.01213v1,cs.CL
PRIMO: Progressive Induction for Multi-hop Open Rule Generation,"Open rule refer to the implication from premise atoms to hypothesis atoms,
which captures various relations between instances in the real world. Injecting
open rule knowledge into the machine helps to improve the performance of
downstream tasks such as dialogue and relation extraction. Existing approaches
focus on single-hop open rule generation, ignoring multi-hop scenarios, leading
to logical inconsistencies between premise and hypothesis atoms, as well as
semantic duplication of generated rule atoms. To address these issues, we
propose a progressive multi-stage open rule generation method called PRIMO. We
introduce ontology information during the rule generation stage to reduce
ambiguity and improve rule accuracy. PRIMO constructs a multi-stage structure
consisting of generation, extraction, and ranking modules to fully leverage the
latent knowledge within the language model across multiple dimensions.
Furthermore, we employ reinforcement learning from human feedback to further
optimize model, enhancing the model's understanding of commonsense knowledge.
Experiments show that compared to baseline models, PRIMO significantly improves
rule quality and diversity while reducing the repetition rate of rule atoms.",2024-11-02,"Jianyu Liu, Sheng Bi, Guilin Qi",http://arxiv.org/pdf/2411.01205v1,cs.CL
Transfer Learning for Finetuning Large Language Models,"As the landscape of large language models expands, efficiently finetuning for
specific tasks becomes increasingly crucial. At the same time, the landscape of
parameter-efficient finetuning methods rapidly expands. Consequently,
practitioners face a multitude of complex choices when searching for an optimal
finetuning pipeline for large language models. To reduce the complexity for
practitioners, we investigate transfer learning for finetuning large language
models and aim to transfer knowledge about configurations from related
finetuning tasks to a new task. In this work, we transfer learn finetuning by
meta-learning performance and cost surrogate models for grey-box
meta-optimization from a new meta-dataset. Counter-intuitively, we propose to
rely only on transfer learning for new datasets. Thus, we do not use
task-specific Bayesian optimization but prioritize knowledge transferred from
related tasks over task-specific feedback. We evaluate our method on eight
synthetic question-answer datasets and a meta-dataset consisting of 1,800 runs
of finetuning Microsoft's Phi-3. Our transfer learning is superior to
zero-shot, default finetuning, and meta-optimization baselines. Our results
demonstrate the transferability of finetuning to adapt large language models
more effectively.",2024-11-02,"Tobias Strangmann, Lennart Purucker, Jörg K. H. Franke, Ivo Rapant, Fabio Ferreira, Frank Hutter",http://arxiv.org/pdf/2411.01195v1,cs.CL
"Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks","We introduce {\bf Swan}, a family of embedding models centred around the
Arabic language, addressing both small-scale and large-scale use cases. Swan
includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on
ArMistral, a pretrained Arabic large language model. To evaluate these models,
we propose ArabicMTEB, a comprehensive benchmark suite that assesses
cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text
embedding performance, covering eight diverse tasks and spanning 94 datasets.
Swan-Large achieves state-of-the-art results, outperforming
Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently
surpasses Multilingual-E5-base. Our extensive evaluations demonstrate that Swan
models are both dialectally and culturally aware, excelling across various
Arabic domains while offering significant monetary efficiency. This work
significantly advances the field of Arabic language modelling and provides
valuable resources for future research and applications in Arabic natural
language processing. Our models and benchmark are available at our GitHub page:
\href{https://github.com/UBC-NLP/swan}{https://github.com/UBC-NLP/swan}",2024-11-02,"Gagan Bhatia, El Moatez Billah Nagoudi, Abdellah El Mekki, Fakhraddin Alwajih, Muhammad Abdul-Mageed",http://arxiv.org/pdf/2411.01192v3,cs.CL
CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research,"This research addresses command-line embedding in cybersecurity, a field
obstructed by the lack of comprehensive datasets due to privacy and regulation
concerns. We propose the first dataset of similar command lines, named CyPHER,
for training and unbiased evaluation. The training set is generated using a set
of large language models (LLMs) comprising 28,520 similar command-line pairs.
Our testing dataset consists of 2,807 similar command-line pairs sourced from
authentic command-line data.
  In addition, we propose a command-line embedding model named CmdCaliper,
enabling the computation of semantic similarity with command lines. Performance
evaluations demonstrate that the smallest version of CmdCaliper (30 million
parameters) suppresses state-of-the-art (SOTA) sentence embedding models with
ten times more parameters across various tasks (e.g., malicious command-line
detection and similar command-line retrieval).
  Our study explores the feasibility of data generation using LLMs in the
cybersecurity domain. Furthermore, we release our proposed command-line
dataset, embedding models' weights and all program codes to the public. This
advancement paves the way for more effective command-line embedding for future
researchers.",2024-11-02,"Sian-Yao Huang, Cheng-Lin Yang, Che-Yu Lin, Chun-Ying Huang",http://arxiv.org/pdf/2411.01176v1,cs.CL
Dictionary Insertion Prompting for Multilingual Reasoning on Multilingual Large Language Models,"As current training data for Large Language Models (LLMs) are dominated by
English corpus, they are English-centric and they present impressive
performance on English reasoning tasks.\footnote{This paper primarily studies
English-centric models, but our method could be universal by using the centric
language in the dictionary for non-English-centric LLMs.} Yet, they usually
suffer from lower performance in other languages. There are about 7,000
languages over the world, and many are low-resourced on English-centric LLMs.
For the sake of people who primarily speak these languages, it is especially
urgent to enable our LLMs in those languages. Model training is usually
effective, but computationally expensive and requires experienced NLP
practitioners. This paper presents a novel and simple yet effective method
called \textbf{D}ictionary \textbf{I}nsertion \textbf{P}rompting
(\textbf{DIP}). When providing a non-English prompt, DIP looks up a word
dictionary and inserts words' English counterparts into the prompt for LLMs. It
then enables better translation into English and better English model thinking
steps which leads to obviously better results. We experiment with about 200
languages from FLORES-200. Since there are no adequate datasets, we use the
NLLB translator to create synthetic multilingual benchmarks from the existing 4
English reasoning benchmarks such as GSM8K and AQuA. Despite the simplicity and
computationally lightweight, we surprisingly found the effectiveness of DIP on
math and commonsense reasoning tasks on multiple open-source and close-source
LLMs.\footnote{Our dictionaries, code, and synthetic benchmarks will be
open-sourced to facilitate future research.}",2024-11-02,"Hongyuan Lu, Zixuan Li, Wai Lam",http://arxiv.org/pdf/2411.01141v1,cs.CL
Do LLMs Know to Respect Copyright Notice?,"Prior study shows that LLMs sometimes generate content that violates
copyright. In this paper, we study another important yet underexplored problem,
i.e., will LLMs respect copyright information in user input, and behave
accordingly? The research problem is critical, as a negative answer would imply
that LLMs will become the primary facilitator and accelerator of copyright
infringement behavior. We conducted a series of experiments using a diverse set
of language models, user prompts, and copyrighted materials, including books,
news articles, API documentation, and movie scripts. Our study offers a
conservative evaluation of the extent to which language models may infringe
upon copyrights when processing user input containing protected material. This
research emphasizes the need for further investigation and the importance of
ensuring LLMs respect copyright regulations when handling user input to prevent
unauthorized use or reproduction of protected content. We also release a
benchmark dataset serving as a test bed for evaluating infringement behaviors
by LLMs and stress the need for future alignment.",2024-11-02,"Jialiang Xu, Shenglan Li, Zhaozhuo Xu, Denghui Zhang",http://arxiv.org/pdf/2411.01136v1,cs.CL
"Infant Agent: A Tool-Integrated, Logic-Driven Agent with Cost-Effective API Usage","Despite the impressive capabilities of large language models (LLMs), they
currently exhibit two primary limitations,
\textbf{\uppercase\expandafter{\romannumeral 1}}: They struggle to
\textbf{autonomously solve the real world engineering problem}.
\textbf{\uppercase\expandafter{\romannumeral 2}}: They remain
\textbf{challenged in reasoning through complex logic problems}. To address
these challenges, we developed the \textsc{Infant Agent}, integrating
task-aware functions, operators, a hierarchical management system, and a memory
retrieval mechanism. Together, these components enable large language models to
sustain extended reasoning processes and handle complex, multi-step tasks
efficiently, all while significantly reducing API costs. Using the
\textsc{Infant Agent}, GPT-4o's accuracy on the SWE-bench-lite dataset rises
from $\mathbf{0.33\%}$ to $\mathbf{30\%}$, and in the AIME-2024 mathematics
competition, it increases GPT-4o's accuracy from $\mathbf{13.3\%}$ to
$\mathbf{37\%}$.",2024-11-02,"Bin Lei, Yuchen Li, Yiming Zeng, Tao Ren, Yi Luo, Tianyu Shi, Zitian Gao, Zeyu Hu, Weitai Kang, Qiuwu Chen",http://arxiv.org/pdf/2411.01114v1,cs.CL
Self-Consistency Falls Short! The Adverse Effects of Positional Bias on Long-Context Problems,"Self-consistency (SC) has been demonstrated to enhance the performance of
large language models (LLMs) across various tasks and domains involving short
content. However, does this evidence support its effectiveness for long-context
problems?
  We challenge the assumption that SC's benefits generalize to long-context
settings, where LLMs often struggle with position bias--a systematic tendency
to over-rely on specific context regions-which hinders their ability to utilize
information effectively from all parts of their context. Through comprehensive
experimentation with varying state-of-the-art models and tasks, we find that SC
not only fails to improve but actively degrades performance on long-context
tasks. This degradation appears driven by persistent position bias, worsening
with longer context lengths and smaller model sizes, but invariant to prompt
format or task type. Unlike short-context tasks, where SC diversifies reasoning
paths, long-context SC amplifies positional errors. These comprehensive results
provide valuable insight into the limitations of current LLMs in long-context
understanding and highlight the need for more sophisticated approaches.",2024-11-02,"Adam Byerly, Daniel Khashabi",http://arxiv.org/pdf/2411.01101v2,cs.CL
TabVer: Tabular Fact Verification with Natural Logic,"Fact verification on tabular evidence incentivises the use of symbolic
reasoning models where a logical form is constructed (e.g. a LISP-style
program), providing greater verifiability than fully neural approaches.
However, these systems typically rely on well-formed tables, restricting their
use in many scenarios. An emerging symbolic reasoning paradigm for textual
evidence focuses on natural logic inference, which constructs proofs by
modelling set-theoretic relations between a claim and its evidence in natural
language. This approach provides flexibility and transparency but is less
compatible with tabular evidence since the relations do not extend to
arithmetic functions. We propose a set-theoretic interpretation of numerals and
arithmetic functions in the context of natural logic, enabling the integration
of arithmetic expressions in deterministic proofs. We leverage large language
models to generate arithmetic expressions by generating questions about salient
parts of a claim which are answered by executing appropriate functions on
tables. In a few-shot setting on FEVEROUS, we achieve an accuracy of 71.4,
outperforming both fully neural and symbolic reasoning models by 3.4 points.
When evaluated on TabFact without any further training, our method remains
competitive with an accuracy lead of 0.5 points.",2024-11-02,"Rami Aly, Andreas Vlachos",http://arxiv.org/pdf/2411.01093v1,cs.CL
Unlocking the Archives: Using Large Language Models to Transcribe Handwritten Historical Documents,"This study demonstrates that Large Language Models (LLMs) can transcribe
historical handwritten documents with significantly higher accuracy than
specialized Handwritten Text Recognition (HTR) software, while being faster and
more cost-effective. We introduce an open-source software tool called
Transcription Pearl that leverages these capabilities to automatically
transcribe and correct batches of handwritten documents using commercially
available multimodal LLMs from OpenAI, Anthropic, and Google. In tests on a
diverse corpus of 18th/19th century English language handwritten documents,
LLMs achieved Character Error Rates (CER) of 5.7 to 7% and Word Error Rates
(WER) of 8.9 to 15.9%, improvements of 14% and 32% respectively over
specialized state-of-the-art HTR software like Transkribus. Most significantly,
when LLMs were then used to correct those transcriptions as well as texts
generated by conventional HTR software, they achieved near-human levels of
accuracy, that is CERs as low as 1.8% and WERs of 3.5%. The LLMs also completed
these tasks 50 times faster and at approximately 1/50th the cost of proprietary
HTR programs. These results demonstrate that when LLMs are incorporated into
software tools like Transcription Pearl, they provide an accessible, fast, and
highly accurate method for mass transcription of historical handwritten
documents, significantly streamlining the digitization process.",2024-11-02,"Mark Humphries, Lianne C. Leddy, Quinn Downton, Meredith Legace, John McConnell, Isabella Murray, Elizabeth Spence",http://arxiv.org/pdf/2411.03340v1,cs.CL
Plentiful Jailbreaks with String Compositions,"Large language models (LLMs) remain vulnerable to a slew of adversarial
attacks and jailbreaking methods. One common approach employed by white-hat
attackers, or red-teamers, is to process model inputs and outputs using
string-level obfuscations, which can include leetspeak, rotary ciphers, Base64,
ASCII, and more. Our work extends these encoding-based attacks by unifying them
in a framework of invertible string transformations. With invertibility, we can
devise arbitrary string compositions, defined as sequences of transformations,
that we can encode and decode end-to-end programmatically. We devise a
automated best-of-n attack that samples from a combinatorially large number of
string compositions. Our jailbreaks obtain competitive attack success rates on
several leading frontier models when evaluated on HarmBench, highlighting that
encoding-based attacks remain a persistent vulnerability even in advanced LLMs.",2024-11-01,Brian R. Y. Huang,http://arxiv.org/pdf/2411.01084v3,cs.CL
Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection,"Jailbreaking techniques trick Large Language Models (LLMs) into producing
restricted outputs, posing a serious threat. One line of defense is to use
another LLM as a Judge to evaluate the harmfulness of generated text. However,
we reveal that these Judge LLMs are vulnerable to token segmentation bias, an
issue that arises when delimiters alter the tokenization process, splitting
words into smaller sub-tokens. This disrupts the embeddings of the entire
sequence, reducing detection accuracy and allowing harmful content to be
misclassified as safe. In this paper, we introduce Emoji Attack, a novel
strategy that amplifies existing jailbreak prompts by exploiting token
segmentation bias. Our method leverages in-context learning to systematically
insert emojis into text before it is evaluated by a Judge LLM, inducing
embedding distortions that significantly lower the likelihood of detecting
unsafe content. Unlike traditional delimiters, emojis also introduce semantic
ambiguity, making them particularly effective in this attack. Through
experiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack
substantially reduces the ""unsafe"" prediction rate, bypassing existing
safeguards.",2024-11-01,"Zhipeng Wei, Yuqi Liu, N. Benjamin Erichson",http://arxiv.org/pdf/2411.01077v2,cs.CL
Privacy Risks of Speculative Decoding in Large Language Models,"Speculative decoding in large language models (LLMs) accelerates token
generation by speculatively predicting multiple tokens cheaply and verifying
them in parallel, and has been widely deployed. In this paper, we provide the
first study demonstrating the privacy risks of speculative decoding. We observe
that input-dependent patterns of correct and incorrect predictions can be
leaked out to an adversary monitoring token generation times and packet sizes,
leading to privacy breaches. By observing the pattern of correctly and
incorrectly speculated tokens, we show that a malicious adversary can
fingerprint queries and learn private user inputs with more than $90\%$
accuracy across three different speculative decoding techniques - REST (almost
$100\%$ accuracy), LADE (up to $92\%$ accuracy), and BiLD (up to $95\%$
accuracy). We show that an adversary can also leak out confidential
intellectual property used to design these techniques, such as data from
data-stores used for prediction (in REST) at a rate of more than $25$ tokens
per second, or even hyper-parameters used for prediction (in LADE). We also
discuss mitigation strategies, such as aggregating tokens across multiple
iterations and padding packets with additional bytes, to avoid such privacy or
confidentiality breaches.",2024-11-01,"Jiankun Wei, Abdulrahman Abdulrazzag, Tianchen Zhang, Adel Muursepp, Gururaj Saileshwar",http://arxiv.org/pdf/2411.01076v2,cs.CL
Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities,"Contrastive learning methods, such as CLIP, leverage naturally paired
data-for example, images and their corresponding text captions-to learn general
representations that transfer efficiently to downstream tasks. While such
approaches are generally applied to two modalities, domains such as robotics,
healthcare, and video need to support many types of data at once. We show that
the pairwise application of CLIP fails to capture joint information between
modalities, thereby limiting the quality of the learned representations. To
address this issue, we present Symile, a simple contrastive learning approach
that captures higher-order information between any number of modalities. Symile
provides a flexible, architecture-agnostic objective for learning
modality-specific representations. To develop Symile's objective, we derive a
lower bound on total correlation, and show that Symile representations for any
set of modalities form a sufficient statistic for predicting the remaining
modalities. Symile outperforms pairwise CLIP, even with modalities missing in
the data, on cross-modal classification and retrieval across several
experiments including on an original multilingual dataset of 33M image, text
and audio samples and a clinical dataset of chest X-rays, electrocardiograms,
and laboratory measurements. All datasets and code used in this work are
publicly available at https://github.com/rajesh-lab/symile.",2024-11-01,"Adriel Saporta, Aahlad Puli, Mark Goldstein, Rajesh Ranganath",http://arxiv.org/pdf/2411.01053v1,cs.CL
Narrative Analysis of True Crime Podcasts With Knowledge Graph-Augmented Large Language Models,"Narrative data spans all disciplines and provides a coherent model of the
world to the reader or viewer. Recent advancement in machine learning and Large
Language Models (LLMs) have enable great strides in analyzing natural language.
However, Large language models (LLMs) still struggle with complex narrative
arcs as well as narratives containing conflicting information. Recent work
indicates LLMs augmented with external knowledge bases can improve the accuracy
and interpretability of the resulting models. In this work, we analyze the
effectiveness of applying knowledge graphs (KGs) in understanding true-crime
podcast data from both classical Natural Language Processing (NLP) and LLM
approaches. We directly compare KG-augmented LLMs (KGLLMs) with classical
methods for KG construction, topic modeling, and sentiment analysis.
Additionally, the KGLLM allows us to query the knowledge base in natural
language and test its ability to factually answer questions. We examine the
robustness of the model to adversarial prompting in order to test the model's
ability to deal with conflicting information. Finally, we apply classical
methods to understand more subtle aspects of the text such as the use of
hearsay and sentiment in narrative construction and propose future directions.
Our results indicate that KGLLMs outperform LLMs on a variety of metrics, are
more robust to adversarial prompts, and are more capable of summarizing the
text into topics.",2024-11-01,"Xinyi Leng, Jason Liang, Jack Mauro, Xu Wang, Andrea L. Bertozzi, James Chapman, Junyuan Lin, Bohan Chen, Chenchen Ye, Temple Daniel, P. Jeffrey Brantingham",http://arxiv.org/pdf/2411.02435v1,cs.CL
Fighting Spurious Correlations in Text Classification via a Causal Learning Perspective,"In text classification tasks, models often rely on spurious correlations for
predictions, incorrectly associating irrelevant features with the target
labels. This issue limits the robustness and generalization of models,
especially when faced with out-of-distribution data where such spurious
correlations no longer hold. To address this challenge, we propose the Causally
Calibrated Robust Classifier (CCR), which aims to reduce models' reliance on
spurious correlations and improve model robustness. Our approach integrates a
causal feature selection method based on counterfactual reasoning, along with
an unbiased inverse propensity weighting (IPW) loss function. By focusing on
selecting causal features, we ensure that the model relies less on spurious
features during prediction. We theoretically justify our approach and
empirically show that CCR achieves state-of-the-art performance among methods
without group labels, and in some cases, it can compete with the models that
utilize group labels.",2024-11-01,"Yuqing Zhou, Ziwei Zhu",http://arxiv.org/pdf/2411.01045v3,cs.CL
Enhancing Question Answering Precision with Optimized Vector Retrieval and Instructions,"Question-answering (QA) is an important application of Information Retrieval
(IR) and language models, and the latest trend is toward pre-trained large
neural networks with embedding parameters. Augmenting QA performances with
these LLMs requires intensive computational resources for fine-tuning. We
propose an innovative approach to improve QA task performances by integrating
optimized vector retrievals and instruction methodologies. Based on retrieval
augmentation, the process involves document embedding, vector retrieval, and
context construction for optimal QA results. We experiment with different
combinations of text segmentation techniques and similarity functions, and
analyze their impacts on QA performances. Results show that the model with a
small chunk size of 100 without any overlap of the chunks achieves the best
result and outperforms the models based on semantic segmentation using
sentences. We discuss related QA examples and offer insight into how model
performances are improved within the two-stage framework.",2024-11-01,"Lixiao Yang, Mengyang Xu, Weimao Ke",http://arxiv.org/pdf/2411.01039v1,cs.CL
Provable Length Generalization in Sequence Prediction via Spectral Filtering,"We consider the problem of length generalization in sequence prediction. We
define a new metric of performance in this setting -- the Asymmetric-Regret --
which measures regret against a benchmark predictor with longer context length
than available to the learner. We continue by studying this concept through the
lens of the spectral filtering algorithm. We present a gradient-based learning
algorithm that provably achieves length generalization for linear dynamical
systems. We conclude with proof-of-concept experiments which are consistent
with our theory.",2024-11-01,"Annie Marsden, Evan Dogariu, Naman Agarwal, Xinyi Chen, Daniel Suo, Elad Hazan",http://arxiv.org/pdf/2411.01035v1,cs.CL
Birdie: Advancing State Space Models with Reward-Driven Objectives and Curricula,"Efficient state space models (SSMs), such as linear recurrent neural networks
and linear attention variants, offer computational advantages over Transformers
but struggle with tasks requiring long-range in-context retrieval-like text
copying, associative recall, and question answering over long contexts.
Previous efforts to address these challenges have focused on architectural
modifications, often reintroducing computational inefficiencies. In this paper,
we propose a novel training procedure, Birdie, that significantly enhances the
in-context retrieval capabilities of SSMs without altering their architecture.
Our approach combines bidirectional input processing with dynamic mixtures of
specialized pre-training objectives, optimized via reinforcement learning. We
introduce a new bidirectional SSM architecture that seamlessly transitions from
bidirectional context processing to causal generation. Experimental evaluations
demonstrate that Birdie markedly improves performance on retrieval-intensive
tasks such as multi-number phone book lookup, long paragraph
question-answering, and infilling. This narrows the performance gap with
Transformers, while retaining computational efficiency. Our findings highlight
the importance of training procedures in leveraging the fixed-state capacity of
SSMs, offering a new direction to advance their capabilities. All code and
pre-trained models are available at https://www.github.com/samblouir/birdie,
with support for JAX and PyTorch.",2024-11-01,"Sam Blouir, Jimmy T. H. Smith, Antonios Anastasopoulos, Amarda Shehu",http://arxiv.org/pdf/2411.01030v5,cs.CL
Provenance: A Light-weight Fact-checker for Retrieval Augmented LLM Generation Output,"We present a light-weight approach for detecting nonfactual outputs from
retrieval-augmented generation (RAG). Given a context and putative output, we
compute a factuality score that can be thresholded to yield a binary decision
to check the results of LLM-based question-answering, summarization, or other
systems. Unlike factuality checkers that themselves rely on LLMs, we use
compact, open-source natural language inference (NLI) models that yield a
freely accessible solution with low latency and low cost at run-time, and no
need for LLM fine-tuning. The approach also enables downstream mitigation and
correction of hallucinations, by tracing them back to specific context chunks.
Our experiments show high area under the ROC curve (AUC) across a wide range of
relevant open source datasets, indicating the effectiveness of our method for
fact-checking RAG output.",2024-11-01,"Hithesh Sankararaman, Mohammed Nasheed Yasin, Tanner Sorensen, Alessandro Di Bari, Andreas Stolcke",http://arxiv.org/pdf/2411.01022v1,cs.CL
Identifying Implicit Social Biases in Vision-Language Models,"Vision-language models, like CLIP (Contrastive Language Image Pretraining),
are becoming increasingly popular for a wide range of multimodal retrieval
tasks. However, prior work has shown that large language and deep vision models
can learn historical biases contained in their training sets, leading to
perpetuation of stereotypes and potential downstream harm. In this work, we
conduct a systematic analysis of the social biases that are present in CLIP,
with a focus on the interaction between image and text modalities. We first
propose a taxonomy of social biases called So-B-IT, which contains 374 words
categorized across ten types of bias. Each type can lead to societal harm if
associated with a particular demographic group. Using this taxonomy, we examine
images retrieved by CLIP from a facial image dataset using each word as part of
a prompt. We find that CLIP frequently displays undesirable associations
between harmful words and specific demographic groups, such as retrieving
mostly pictures of Middle Eastern men when asked to retrieve images of a
""terrorist"". Finally, we conduct an analysis of the source of such biases, by
showing that the same harmful stereotypes are also present in a large
image-text dataset used to train CLIP models for examples of biases that we
find. Our findings highlight the importance of evaluating and addressing bias
in vision-language models, and suggest the need for transparency and
fairness-aware curation of large pre-training datasets.",2024-11-01,"Kimia Hamidieh, Haoran Zhang, Walter Gerych, Thomas Hartvigsen, Marzyeh Ghassemi",http://arxiv.org/pdf/2411.00997v1,cs.CL
FedDTPT: Federated Discrete and Transferable Prompt Tuning for Black-Box Large Language Models,"In recent years, large language models (LLMs) have significantly advanced the
field of natural language processing (NLP). By fine-tuning LLMs with data from
specific scenarios, these foundation models can better adapt to various
downstream tasks. However, the fine-tuning process poses privacy leakage risks,
particularly in centralized data processing scenarios. To address user privacy
concerns, federated learning (FL) has been introduced to mitigate the risks
associated with centralized data collection from multiple sources.
Nevertheless, the privacy of LLMs themselves is equally critical, as potential
malicious attacks challenge their security, an issue that has received limited
attention in current research. Consequently, establishing a trusted multi-party
model fine-tuning environment is essential. Additionally, the local deployment
of large LLMs incurs significant storage costs and high computational demands.
To address these challenges, we propose for the first time a federated discrete
and transferable prompt tuning, namely FedDTPT, for black-box large language
models. In the client optimization phase, we adopt a token-level discrete
prompt optimization method that leverages a feedback loop based on prediction
accuracy to drive gradient-free prompt optimization through the MLM API. For
server optimization, we employ an attention mechanism based on semantic
similarity to filter all local prompt tokens, along with an embedding distance
elbow detection and DBSCAN clustering strategy to enhance the filtering
process. Experimental results demonstrate that, compared to state-of-the-art
methods, our approach achieves higher accuracy, reduced communication overhead,
and robustness to non-iid data in a black-box setting. Moreover, the optimized
prompts are transferable.",2024-11-01,"Jiaqi Wu, Simin Chen, Yuzhe Yang, Yijiang Li, Shiyue Hou, Rui Jing, Zehua Wang, Wei Chen, Zijian Tian",http://arxiv.org/pdf/2411.00985v1,cs.CL
Enhancing AAC Software for Dysarthric Speakers in e-Health Settings: An Evaluation Using TORGO,"Individuals with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS)
frequently face challenges with articulation, leading to dysarthria and
resulting in atypical speech patterns. In healthcare settings, communication
breakdowns reduce the quality of care. While building an augmentative and
alternative communication (AAC) tool to enable fluid communication we found
that state-of-the-art (SOTA) automatic speech recognition (ASR) technology like
Whisper and Wav2vec2.0 marginalizes atypical speakers largely due to the lack
of training data. Our work looks to leverage SOTA ASR followed by domain
specific error-correction. English dysarthric ASR performance is often
evaluated on the TORGO dataset. Prompt-overlap is a well-known issue with this
dataset where phrases overlap between training and test speakers. Our work
proposes an algorithm to break this prompt-overlap. After reducing
prompt-overlap, results with SOTA ASR models produce extremely high word error
rates for speakers with mild and severe dysarthria. Furthermore, to improve
ASR, our work looks at the impact of n-gram language models and large-language
model (LLM) based multi-modal generative error-correction algorithms like
Whispering-LLaMA for a second pass ASR. Our work highlights how much more needs
to be done to improve ASR for atypical speakers to enable equitable healthcare
access both in-person and in e-health settings.",2024-11-01,"Macarious Hui, Jinda Zhang, Aanchan Mohan",http://arxiv.org/pdf/2411.00980v2,cs.CL
Generic Embedding-Based Lexicons for Transparent and Reproducible Text Scoring,"With text analysis tools becoming increasingly sophisticated over the last
decade, researchers now face a decision of whether to use state-of-the-art
models that provide high performance but that can be highly opaque in their
operations and computationally intensive to run. The alternative, frequently,
is to rely on older, manually crafted textual scoring tools that are
transparently and easily applied, but can suffer from limited performance. I
present an alternative that combines the strengths of both: lexicons created
with minimal researcher inputs from generic (pretrained) word embeddings.
Presenting a number of conceptual lexicons produced from FastText and GloVe
(6B) vector representations of words, I argue that embedding-based lexicons
respond to a need for transparent yet high-performance text measuring tools.",2024-11-01,Catherine Moez,http://arxiv.org/pdf/2411.00964v1,cs.CL
Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM,"Rapidly developing large language models (LLMs) have brought tremendous
intelligent applications. Especially, the GPT-4o's excellent duplex speech
interaction ability has brought impressive experience to users. Researchers
have recently proposed several multi-modal LLMs in this direction that can
achieve user-agent speech-to-speech conversations. This paper proposes a novel
speech-text multimodal LLM architecture called Freeze-Omni. Our main
contribution is that the speech input and output modalities can be easily
connected to a textual LLM while keeping the LLM's parameters frozen throughout
the training process. We design a three-stage training strategy for modeling
both the speech input and output, enabling Freeze-Omni to obtain
speech-to-speech conversation ability using text-speech paired data (such as
ASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.
Moreover, we can effectively ensure that the intelligence of the Freeze-Omni in
the speech modality is at the same level compared with that in the text
modality of its backbone LLM, while achieving low latency end-to-end spoken
response. In addition, we also designed a method to achieve duplex dialogue
ability through multi-task training, giving Freeze-Omni a more natural style of
dialogue ability between users and agents. In summary, Freeze-Omni holds great
potential to conduct speech-to-speech dialogue based on a multimodal LLM under
the condition of a frozen LLM, avoiding the catastrophic forgetting problem
caused by limited data and training resources.",2024-11-01,"Xiong Wang, Yangze Li, Chaoyou Fu, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, Long Ma",http://arxiv.org/pdf/2411.00774v5,cs.CL
SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models,"Large language models (LLMs) have demonstrated remarkable capabilities, but
their outputs can sometimes be unreliable or factually incorrect. To address
this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding
framework that enhances the truthfulness of LLMs without relying on external
knowledge bases or requiring further fine-tuning. From an optimization
perspective, our SLED framework leverages the latent knowledge embedded within
the LLM by contrasting the output logits from the final layer with those from
early layers. It then utilizes an approximate gradient approach to enable
latent knowledge to guide the self-refinement of outputs, thereby effectively
improving factual accuracy. Extensive experiments have been conducted on
established benchmarks across a diverse range of model families (LLaMA 2, LLaMA
3, Gemma) and scales (from 2B to 70B), including more advanced architectural
configurations such as the mixture of experts (MoE). Our evaluation spans a
wide variety of tasks, including multi-choice, open-generation, and adaptations
to chain-of-thought reasoning tasks. The results demonstrate that SLED
consistently improves factual accuracy by up to 20\% compared to existing
decoding methods while maintaining natural language fluency and negligible
latency overhead. Furthermore, it can be flexibly combined with other decoding
methods to further enhance their performance.",2024-11-01,"Jianyi Zhang, Da-Cheng Juan, Cyrus Rashtchian, Chun-Sung Ferng, Heinrich Jiang, Yiran Chen",http://arxiv.org/pdf/2411.02433v2,cs.CL
Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided Sampling,"Self-improvement methods enable large language models (LLMs) to generate
solutions themselves and iteratively train on filtered, high-quality
rationales. This process proves effective and reduces the reliance on human
supervision in LLMs' reasoning, but the performance soon plateaus. We delve
into the process and find that models tend to over-sample on easy queries and
under-sample on queries they have yet to master. As iterations proceed, this
imbalance in sampling is exacerbated, leading to a long-tail distribution where
solutions to difficult queries almost diminish. This phenomenon limits the
performance gain of self-improving models. A straightforward solution is
brute-force sampling to balance the distribution, which significantly raises
computational costs. In this paper, we introduce Guided Self-Improvement (GSI),
a strategy aimed at improving the efficiency of sampling challenging
heavy-tailed data. It leverages Socratic-style guidance signals to help LLM
reasoning with complex queries, reducing the exploration effort and minimizing
computational overhead. Experiments on four models across diverse mathematical
tasks show that GSI strikes a balance between performance and efficiency, while
also being effective on held-out tasks.",2024-11-01,"Yiwen Ding, Zhiheng Xi, Wei He, Zhuoyuan Li, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang",http://arxiv.org/pdf/2411.00750v2,cs.CL
CORAG: A Cost-Constrained Retrieval Optimization System for Retrieval-Augmented Generation,"Large Language Models (LLMs) have demonstrated remarkable generation
capabilities but often struggle to access up-to-date information, which can
lead to hallucinations. Retrieval-Augmented Generation (RAG) addresses this
issue by incorporating knowledge from external databases, enabling more
accurate and relevant responses. Due to the context window constraints of LLMs,
it is impractical to input the entire external database context directly into
the model. Instead, only the most relevant information, referred to as chunks,
is selectively retrieved. However, current RAG research faces three key
challenges. First, existing solutions often select each chunk independently,
overlooking potential correlations among them. Second, in practice the utility
of chunks is non-monotonic, meaning that adding more chunks can decrease
overall utility. Traditional methods emphasize maximizing the number of
included chunks, which can inadvertently compromise performance. Third, each
type of user query possesses unique characteristics that require tailored
handling, an aspect that current approaches do not fully consider. To overcome
these challenges, we propose a cost constrained retrieval optimization system
CORAG for retrieval-augmented generation. We employ a Monte Carlo Tree Search
(MCTS) based policy framework to find optimal chunk combinations sequentially,
allowing for a comprehensive consideration of correlations among chunks.
Additionally, rather than viewing budget exhaustion as a termination condition,
we integrate budget constraints into the optimization of chunk combinations,
effectively addressing the non-monotonicity of chunk utility.",2024-11-01,"Ziting Wang, Haitao Yuan, Wei Dong, Gao Cong, Feifei Li",http://arxiv.org/pdf/2411.00744v1,cs.CL
Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models,"Understanding and mitigating the potential risks associated with foundation
models (FMs) hinges on developing effective interpretability methods. Sparse
Autoencoders (SAEs) have emerged as a promising tool for disentangling FM
representations, but they struggle to capture rare, yet crucial concepts in the
data. We introduce Specialized Sparse Autoencoders (SSAEs), designed to
illuminate these elusive dark matter features by focusing on specific
subdomains. We present a practical recipe for training SSAEs, demonstrating the
efficacy of dense retrieval for data selection and the benefits of Tilted
Empirical Risk Minimization as a training objective to improve concept recall.
Our evaluation of SSAEs on standard metrics, such as downstream perplexity and
$L_0$ sparsity, show that they effectively capture subdomain tail concepts,
exceeding the capabilities of general-purpose SAEs. We showcase the practical
utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs
achieve a 12.5\% increase in worst-group classification accuracy when applied
to remove spurious gender information. SSAEs provide a powerful new lens for
peering into the inner workings of FMs in subdomains.",2024-11-01,"Aashiq Muhamed, Mona Diab, Virginia Smith",http://arxiv.org/pdf/2411.00743v1,cs.CL
MolCap-Arena: A Comprehensive Captioning Benchmark on Language-Enhanced Molecular Property Prediction,"Bridging biomolecular modeling with natural language information,
particularly through large language models (LLMs), has recently emerged as a
promising interdisciplinary research area. LLMs, having been trained on large
corpora of scientific documents, demonstrate significant potential in
understanding and reasoning about biomolecules by providing enriched contextual
and domain knowledge. However, the extent to which LLM-driven insights can
improve performance on complex predictive tasks (e.g., toxicity) remains
unclear. Further, the extent to which relevant knowledge can be extracted from
LLMs also remains unknown. In this study, we present Molecule Caption Arena:
the first comprehensive benchmark of LLM-augmented molecular property
prediction. We evaluate over twenty LLMs, including both general-purpose and
domain-specific molecule captioners, across diverse prediction tasks. To this
goal, we introduce a novel, battle-based rating system. Our findings confirm
the ability of LLM-extracted knowledge to enhance state-of-the-art molecular
representations, with notable model-, prompt-, and dataset-specific variations.
Code, resources, and data are available at github.com/Genentech/molcap-arena.",2024-11-01,"Carl Edwards, Ziqing Lu, Ehsan Hajiramezanali, Tommaso Biancalani, Heng Ji, Gabriele Scalia",http://arxiv.org/pdf/2411.00737v1,cs.CL
SPRING Lab IITM's submission to Low Resource Indic Language Translation Shared Task,"We develop a robust translation model for four low-resource Indic languages:
Khasi, Mizo, Manipuri, and Assamese. Our approach includes a comprehensive
pipeline from data collection and preprocessing to training and evaluation,
leveraging data from WMT task datasets, BPCC, PMIndia, and OpenLanguageData. To
address the scarcity of bilingual data, we use back-translation techniques on
monolingual datasets for Mizo and Khasi, significantly expanding our training
corpus. We fine-tune the pre-trained NLLB 3.3B model for Assamese, Mizo, and
Manipuri, achieving improved performance over the baseline. For Khasi, which is
not supported by the NLLB model, we introduce special tokens and train the
model on our Khasi corpus. Our training involves masked language modelling,
followed by fine-tuning for English-to-Indic and Indic-to-English translations.",2024-11-01,"Hamees Sayed, Advait Joglekar, Srinivasan Umesh",http://arxiv.org/pdf/2411.00727v2,cs.CL
Can LLMs make trade-offs involving stipulated pain and pleasure states?,"Pleasure and pain play an important role in human decision making by
providing a common currency for resolving motivational conflicts. While Large
Language Models (LLMs) can generate detailed descriptions of pleasure and pain
experiences, it is an open question whether LLMs can recreate the motivational
force of pleasure and pain in choice scenarios - a question which may bear on
debates about LLM sentience, understood as the capacity for valenced
experiential states. We probed this question using a simple game in which the
stated goal is to maximise points, but where either the points-maximising
option is said to incur a pain penalty or a non-points-maximising option is
said to incur a pleasure reward, providing incentives to deviate from
points-maximising behaviour. Varying the intensity of the pain penalties and
pleasure rewards, we found that Claude 3.5 Sonnet, Command R+, GPT-4o, and
GPT-4o mini each demonstrated at least one trade-off in which the majority of
responses switched from points-maximisation to pain-minimisation or
pleasure-maximisation after a critical threshold of stipulated pain or pleasure
intensity is reached. LLaMa 3.1-405b demonstrated some graded sensitivity to
stipulated pleasure rewards and pain penalties. Gemini 1.5 Pro and PaLM 2
prioritised pain-avoidance over points-maximisation regardless of intensity,
while tending to prioritise points over pleasure regardless of intensity. We
discuss the implications of these findings for debates about the possibility of
LLM sentience.",2024-11-01,"Geoff Keeling, Winnie Street, Martyna Stachaczyk, Daria Zakharova, Iulia M. Comsa, Anastasiya Sakovych, Isabella Logothetis, Zejia Zhang, Blaise Agüera y Arcas, Jonathan Birch",http://arxiv.org/pdf/2411.02432v1,cs.CL
Text2Freq: Learning Series Patterns from Text via Frequency Domain,"Traditional time series forecasting models mainly rely on historical numeric
values to predict future outcomes.While these models have shown promising
results, they often overlook the rich information available in other
modalities, such as textual descriptions of special events, which can provide
crucial insights into future dynamics.However, research that jointly
incorporates text in time series forecasting remains relatively underexplored
compared to other cross-modality work. Additionally, the modality gap between
time series data and textual information poses a challenge for multimodal
learning. To address this task, we propose Text2Freq, a cross-modality model
that integrates text and time series data via the frequency domain.
Specifically, our approach aligns textual information to the low-frequency
components of time series data, establishing more effective and interpretable
alignments between these two modalities. Our experiments on paired datasets of
real-world stock prices and synthetic texts show that Text2Freq achieves
state-of-the-art performance, with its adaptable architecture encouraging
future research in this field.",2024-11-01,"Ming-Chih Lo, Ching Chang, Wen-Chih Peng",http://arxiv.org/pdf/2411.00929v1,cs.CL
A graph-based approach to extracting narrative signals from public discourse,"Narratives are key interpretative devices by which humans make sense of
political reality. As the significance of narratives for understanding current
societal issues such as polarization and misinformation becomes increasingly
evident, there is a growing demand for methods that support their empirical
analysis. To this end, we propose a graph-based formalism and machine-guided
method for extracting, representing, and analyzing selected narrative signals
from digital textual corpora, based on Abstract Meaning Representation (AMR).
The formalism and method introduced here specifically cater to the study of
political narratives that figure in texts from digital media such as archived
political speeches, social media posts, political manifestos and transcripts of
parliamentary debates. We conceptualize these political narratives as a type of
ontological narratives: stories by which actors position themselves as
political beings, and which are akin to political worldviews in which actors
present their normative vision of the world, or aspects thereof. We approach
the study of such political narratives as a problem of information retrieval:
starting from a textual corpus, we first extract a graph-like representation of
the meaning of each sentence in the corpus using AMR. Drawing on transferable
concepts from narratology, we then apply a set of heuristics to filter these
graphs for representations of 1) actors, 2) the events in which these actors
figure, and 3) traces of the perspectivization of these events. We approach
these references to actors, events, and instances of perspectivization as core
narrative signals that initiate a further analysis by alluding to larger
political narratives. By means of a case study of State of the European Union
addresses, we demonstrate how the formalism can be used to inductively surface
signals of political narratives from public discourse.",2024-11-01,"Armin Pournaki, Tom Willaert",http://arxiv.org/pdf/2411.00702v1,cs.CL
"ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents","Large language model (LLM)-based agents are increasingly employed to interact
with external environments (e.g., games, APIs, world models) to solve
user-provided tasks. However, current frameworks often lack the ability to
collaborate effectively with users in fully conversational settings.
Conversations are essential for aligning on task details, achieving
user-defined goals, and satisfying preferences. While existing agents address
ambiguity through clarification questions, they underutilize the broader
potential of an LLM's conversational capabilities. In this work, we introduce
ReSpAct, an LLM-based agent designed to seamlessly integrate reasoning,
decision-making, and dynamic dialogue for task-solving. Expanding on
reasoning-first approaches like ReAct, ReSpAct employs active, free-flowing
dialogues to interpret instructions, clarify goals, provide status updates,
resolve subtask failures, and refine plans based on user inputs without any
explicit dialogue schema. By alternating between task-solving actions and
interactive conversations, ReSpAct demonstrates improved performance across
diverse environments. We evaluate ReSpAct in user-interactive settings,
including task-oriented dialogue systems (MultiWOZ) and decision-making tasks
(ALFWorld, WebShop). ReSpAct outperforms ReAct with absolute success rate
improvements of 6% and 4% in ALFWorld and WebShop, respectively, and achieves a
5.5% gain in Inform and a 3% gain in Success scores in MultiWOZ. These results
highlight the value of integrating dynamic user-agent collaboration for more
effective task resolution.",2024-11-01,"Vardhan Dongre, Xiaocheng Yang, Emre Can Acikgoz, Suvodip Dey, Gokhan Tur, Dilek Hakkani-Tür",http://arxiv.org/pdf/2411.00927v2,cs.CL
Leveraging Large Language Models for Code-Mixed Data Augmentation in Sentiment Analysis,"Code-mixing (CM), where speakers blend languages within a single expression,
is prevalent in multilingual societies but poses challenges for natural
language processing due to its complexity and limited data. We propose using a
large language model to generate synthetic CM data, which is then used to
enhance the performance of task-specific models for CM sentiment analysis. Our
results show that in Spanish-English, synthetic data improved the F1 score by
9.32%, outperforming previous augmentation techniques. However, in
Malayalam-English, synthetic data only helped when the baseline was low; with
strong natural data, additional synthetic data offered little benefit. Human
evaluation confirmed that this approach is a simple, cost-effective way to
generate natural-sounding CM sentences, particularly beneficial for low
baselines. Our findings suggest that few-shot prompting of large language
models is a promising method for CM data augmentation and has significant
impact on improving sentiment analysis, an important element in the development
of social influence systems.",2024-11-01,Linda Zeng,http://arxiv.org/pdf/2411.00691v1,cs.CL
PrefRAG: Preference-Driven Multi-Source Retrieval Augmented Generation,"Retrieval-Augmented Generation (RAG) has emerged as a reliable external
knowledge augmentation technique to mitigate hallucination issues and
parameterized knowledge limitations in Large Language Models (LLMs). Existing
adaptive RAG (ARAG) systems excel at in-depth exploration within a single
source but struggle to effectively and controllably explore different retrieval
sources, as they fail to foresee their internal knowledge features. We develop
a novel multi-source ARAG system, PrefRAG, which enhances RAG by enabling
in-depth and controllable exploration of diverse retrieval sources through
preference-driven adaptive retrieval and self-reflection. PrefRAG first fully
explores controllable local sources in adaptive retrieval and supplements with
the web when appropriate, ultimately selecting the optimal source for knowledge
observation. Subsequently, PrefRAG feeds answer quality feedback into the
retrieval process, optimizing it from the generation perspective to produce
higher-quality responses. Extensive experiments confirm its superiority, high
retrieval efficiency, and knowledge controllability. PrefRAG outperforms
Vanilla RAG and the leading MS-ARAG by up to 25.6% and 13.9% respectively.
Additionally, PrefRAG trained with DPO achieves higher performance. The code
and data are available at https://github.com/QingFei1/PrefRAG.git.",2024-11-01,"Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Jie Tang",http://arxiv.org/pdf/2411.00689v2,cs.CL
Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in Language Models,"As Large Language Models (LLMs) are increasingly deployed in specialized
domains with continuously evolving knowledge, the need for timely and precise
knowledge injection has become essential. Fine-tuning with paraphrased data is
a common approach to enhance knowledge injection, yet it faces two significant
challenges: high computational costs due to repetitive external model usage and
limited sample diversity. To this end, we introduce LaPael, a latent-level
paraphrasing method that applies input-dependent noise to early LLM layers.
This approach enables diverse and semantically consistent augmentations
directly within the model. Furthermore, it eliminates the recurring costs of
paraphrase generation for each knowledge update. Our extensive experiments on
question-answering benchmarks demonstrate that LaPael improves knowledge
injection over standard fine-tuning and existing noise-based approaches.
Additionally, combining LaPael with data-level paraphrasing further enhances
performance.",2024-11-01,"Minki Kang, Sung Ju Hwang, Gibbeum Lee, Jaewoong Cho",http://arxiv.org/pdf/2411.00686v1,cs.CL
TaxaBind: A Unified Embedding Space for Ecological Applications,"We present TaxaBind, a unified embedding space for characterizing any species
of interest. TaxaBind is a multimodal embedding space across six modalities:
ground-level images of species, geographic location, satellite image, text,
audio, and environmental features, useful for solving ecological problems. To
learn this joint embedding space, we leverage ground-level images of species as
a binding modality. We propose multimodal patching, a technique for effectively
distilling the knowledge from various modalities into the binding modality. We
construct two large datasets for pretraining: iSatNat with species images and
satellite images, and iSoundNat with species images and audio. Additionally, we
introduce TaxaBench-8k, a diverse multimodal dataset with six paired modalities
for evaluating deep learning models on ecological tasks. Experiments with
TaxaBind demonstrate its strong zero-shot and emergent capabilities on a range
of tasks including species classification, cross-model retrieval, and audio
classification. The datasets and models are made available at
https://github.com/mvrl/TaxaBind.",2024-11-01,"Srikumar Sastry, Subash Khanal, Aayush Dhakal, Adeel Ahmad, Nathan Jacobs",http://arxiv.org/pdf/2411.00683v1,cs.CL
Zipfian Whitening,"The word embedding space in neural models is skewed, and correcting this can
improve task performance. We point out that most approaches for modeling,
correcting, and measuring the symmetry of an embedding space implicitly assume
that the word frequencies are uniform; in reality, word frequencies follow a
highly non-uniform distribution, known as Zipf's law. Surprisingly, simply
performing PCA whitening weighted by the empirical word frequency that follows
Zipf's law significantly improves task performance, surpassing established
baselines. From a theoretical perspective, both our approach and existing
methods can be clearly categorized: word representations are distributed
according to an exponential family with either uniform or Zipfian base
measures. By adopting the latter approach, we can naturally emphasize
informative low-frequency words in terms of their vector norm, which becomes
evident from the information-geometric perspective, and in terms of the loss
functions for imbalanced classification. Additionally, our theory corroborates
that popular natural language processing methods, such as skip-gram negative
sampling, WhiteningBERT, and headless language models, work well just because
their word embeddings encode the empirical word frequency into the underlying
probabilistic model.",2024-11-01,"Sho Yokoi, Han Bao, Hiroto Kurita, Hidetoshi Shimodaira",http://arxiv.org/pdf/2411.00680v1,cs.CL
Optimizing Contextual Speech Recognition Using Vector Quantization for Efficient Retrieval,"Neural contextual biasing allows speech recognition models to leverage
contextually relevant information, leading to improved transcription accuracy.
However, the biasing mechanism is typically based on a cross-attention module
between the audio and a catalogue of biasing entries, which means computational
complexity can pose severe practical limitations on the size of the biasing
catalogue and consequently on accuracy improvements. This work proposes an
approximation to cross-attention scoring based on vector quantization and
enables compute- and memory-efficient use of large biasing catalogues. We
propose to use this technique jointly with a retrieval based contextual biasing
approach. First, we use an efficient quantized retrieval module to shortlist
biasing entries by grounding them on audio. Then we use retrieved entries for
biasing. Since the proposed approach is agnostic to the biasing method, we
investigate using full cross-attention, LLM prompting, and a combination of the
two. We show that retrieval based shortlisting allows the system to efficiently
leverage biasing catalogues of several thousands of entries, resulting in up to
71% relative error rate reduction in personal entity recognition. At the same
time, the proposed approximation algorithm reduces compute time by 20% and
memory usage by 85-95%, for lists of up to one million entries, when compared
to standard dot-product cross-attention.",2024-11-01,"Nikolaos Flemotomos, Roger Hsiao, Pawel Swietojanski, Takaaki Hori, Dogan Can, Xiaodan Zhuang",http://arxiv.org/pdf/2411.00664v2,cs.CL
Phase Diagram of Vision Large Language Models Inference: A Perspective from Interaction across Image and Instruction,"Vision Large Language Models (VLLMs) usually take input as a concatenation of
image token embeddings and text token embeddings and conduct causal modeling.
However, their internal behaviors remain underexplored, raising the question of
interaction among two types of tokens. To investigate such multimodal
interaction during model inference, in this paper, we measure the
contextualization among the hidden state vectors of tokens from different
modalities. Our experiments uncover a four-phase inference dynamics of VLLMs
against the depth of Transformer-based LMs, including (I) Alignment: In very
early layers, contextualization emerges between modalities, suggesting a
feature space alignment. (II) Intra-modal Encoding: In early layers,
intra-modal contextualization is enhanced while inter-modal interaction is
suppressed, suggesting a local encoding within modalities. (III) Inter-modal
Encoding: In later layers, contextualization across modalities is enhanced,
suggesting a deeper fusion across modalities. (IV) Output Preparation: In very
late layers, contextualization is reduced globally, and hidden states are
aligned towards the unembedding space.",2024-11-01,"Houjing Wei, Yuting Shi, Naoya Inoue",http://arxiv.org/pdf/2411.00646v2,cs.CL
Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations,"Evaluations are critical for understanding the capabilities of large language
models (LLMs). Fundamentally, evaluations are experiments; but the literature
on evaluations has largely ignored the literature from other sciences on
experiment analysis and planning. This article shows researchers with some
training in statistics how to think about and analyze data from language model
evaluations. Conceptualizing evaluation questions as having been drawn from an
unseen super-population, we present formulas for analyzing evaluation data,
measuring differences between two models, and planning an evaluation
experiment. We make a number of specific recommendations for running language
model evaluations and reporting experiment results in a way that minimizes
statistical noise and maximizes informativeness.",2024-11-01,Evan Miller,http://arxiv.org/pdf/2411.00640v1,cs.CL
ConvCounsel: A Conversational Dataset for Student Counseling,"Student mental health is a sensitive issue that necessitates special
attention. A primary concern is the student-to-counselor ratio, which surpasses
the recommended standard of 250:1 in most universities. This imbalance results
in extended waiting periods for in-person consultations, which cause suboptimal
treatment. Significant efforts have been directed toward developing mental
health dialogue systems utilizing the existing open-source mental
health-related datasets. However, currently available datasets either discuss
general topics or various strategies that may not be viable for direct
application due to numerous ethical constraints inherent in this research
domain. To address this issue, this paper introduces a specialized mental
health dataset that emphasizes the active listening strategy employed in
conversation for counseling, also named as ConvCounsel. This dataset comprises
both speech and text data, which can facilitate the development of a reliable
pipeline for mental health dialogue systems. To demonstrate the utility of the
proposed dataset, this paper also presents the NYCUKA, a spoken mental health
dialogue system that is designed by using the ConvCounsel dataset. The results
show the merit of using this dataset.",2024-11-01,"Po-Chuan Chen, Mahdin Rohmatillah, You-Teng Lin, Jen-Tzung Chien",http://arxiv.org/pdf/2411.00604v1,cs.CL
LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models,"Mixture of Experts (MoEs) plays an important role in the development of more
efficient and effective large language models (LLMs). Due to the enormous
resource requirements, studying large scale MoE algorithms remain in-accessible
to many researchers. This work develops \emph{LibMoE}, a comprehensive and
modular framework to streamline the research, training, and evaluation of MoE
algorithms. Built upon three core principles: (i) modular design, (ii)
efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs
more accessible to a wide range of researchers by standardizing the training
and evaluation pipelines. Using LibMoE, we extensively benchmarked five
state-of-the-art MoE algorithms over three different LLMs and 11 datasets under
the zero-shot setting. The results show that despite the unique
characteristics, all MoE algorithms perform roughly similar when averaged
across a wide range of tasks. With the modular design and extensive evaluation,
we believe LibMoE will be invaluable for researchers to make meaningful
progress towards the next generation of MoE and LLMs. Project page:
\url{https://fsoft-aic.github.io/fsoft-LibMoE.github.io}.",2024-11-01,"Nam V. Nguyen, Thong T. Doan, Luong Tran, Van Nguyen, Quang Pham",http://arxiv.org/pdf/2411.00918v1,cs.CL
Adapting Language Models via Token Translation,"Modern large language models use a fixed tokenizer to effectively compress
text drawn from a source domain. However, applying the same tokenizer to a new
target domain often leads to inferior compression, more costly inference, and
reduced semantic alignment. To address this deficiency, we introduce Sparse
Sinkhorn Token Translation (S2T2). S2T2 trains a tailored tokenizer for the
target domain and learns to translate between target and source tokens,
enabling more effective reuse of the pre-trained next-source-token predictor.
In our experiments with finetuned English language models, S2T2 improves both
the perplexity and the compression of out-of-domain protein sequences,
outperforming direct finetuning with either the source or target tokenizer. In
addition, we find that token translations learned for smaller, less expensive
models can be directly transferred to larger, more powerful models to reap the
benefits of S2T2 at lower cost.",2024-11-01,"Zhili Feng, Tanya Marwah, Nicolo Fusi, David Alvarez-Melis, Lester Mackey",http://arxiv.org/pdf/2411.00593v2,cs.CL
ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot Named Entity Recognition with Large Language Models,"This paper presents ReverseNER, a method aimed at overcoming the limitation
of large language models (LLMs) in zero-shot named entity recognition (NER)
tasks, arising from their reliance on pre-provided demonstrations. ReverseNER
tackles this challenge by constructing a reliable example library composed of
dozens of entity-labeled sentences, generated through the reverse process of
NER. Specifically, while conventional NER methods label entities in a sentence,
ReverseNER features reversing the process by using an LLM to generate entities
from their definitions and subsequently expand them into full sentences. During
the entity expansion process, the LLM is guided to generate sentences by
replicating the structures of a set of specific \textsl{feature sentences},
extracted from the task sentences by clustering. This expansion process
produces dozens of entity-labeled task-relevant sentences. After constructing
the example library, the method selects several semantically similar
entity-labeled examples for each task sentence as references to facilitate the
LLM's entity recognition. We also propose an entity-level self-consistency
scoring mechanism to improve NER performance with LLMs. Experiments show that
ReverseNER significantly outperforms other zero-shot NER methods with LLMs,
marking a notable improvement in NER for domains without labeled data, while
declining computational resource consumption.",2024-11-01,"Anbang Wang, Difei Mei, Zhichao Zhang, Xiuxiu Bai, Ran Yao, Zewen Fang, Min Hu, Zhirui Cao, Haitao Sun, Yifeng Guo, Hongyao Zhou, Yu Guo",http://arxiv.org/pdf/2411.00533v4,cs.CL
"Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models","We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu
et al., 2023), designed to improve the large language model (LLM) generation.
Specifically, it guides an LLM to fulfill an input instruction by simulating
multiple experts, aggregating their responses, and selecting the best among
individual and aggregated responses. This process is performed in a single
chain of thoughts through our seven carefully designed subtasks derived from
the Nominal Group Technique (Ven and Delbecq, 1974), a well-established
decision-making framework. Our evaluations demonstrate that Multi-expert
Prompting significantly outperforms ExpertPrompting and comparable baselines in
enhancing the truthfulness, factuality, informativeness, and usefulness of
responses while reducing toxicity and hurtfulness. It further achieves
state-of-the-art truthfulness by outperforming the best baseline by 8.69% with
ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable
to diverse scenarios, eliminating the need for manual prompt construction.",2024-11-01,"Do Xuan Long, Duong Ngoc Yen, Anh Tuan Luu, Kenji Kawaguchi, Min-Yen Kan, Nancy F. Chen",http://arxiv.org/pdf/2411.00492v1,cs.CL
"GDTB: Genre Diverse Data for English Shallow Discourse Parsing across Modalities, Text Types, and Domains","Work on shallow discourse parsing in English has focused on the Wall Street
Journal corpus, the only large-scale dataset for the language in the PDTB
framework. However, the data is not openly available, is restricted to the news
domain, and is by now 35 years old. In this paper, we present and evaluate a
new open-access, multi-genre benchmark for PDTB-style shallow discourse
parsing, based on the existing UD English GUM corpus, for which discourse
relation annotations in other frameworks already exist. In a series of
experiments on cross-domain relation classification, we show that while our
dataset is compatible with PDTB, substantial out-of-domain degradation is
observed, which can be alleviated by joint training on both datasets.",2024-11-01,"Yang Janet Liu, Tatsuya Aoyama, Wesley Scivetti, Yilun Zhu, Shabnam Behzad, Lauren Elizabeth Levine, Jessica Lin, Devika Tiwari, Amir Zeldes",http://arxiv.org/pdf/2411.00491v1,cs.CL
Generative Emotion Cause Explanation in Multimodal Conversations,"Multimodal conversation, a crucial form of human communication, carries rich
emotional content, making the exploration of the causes of emotions within it a
research endeavor of significant importance. However, existing research on the
causes of emotions typically uses clause selection methods to locate the reason
utterance, without providing a detailed explanation of the emotional causes. In
this paper, we propose a new task, \textbf{M}ultimodal \textbf{C}onversation
\textbf{E}motion \textbf{C}ause \textbf{E}xplanation (MCECE), aiming to
generate a detailed explanation of the emotional cause to the target utterance
within a multimodal conversation scenario. Building upon the MELD dataset, we
develop a new dataset (ECEM) that integrates video clips with detailed
explanations of character emotions, facilitating an in-depth examination of the
causal factors behind emotional expressions in multimodal conversations.A novel
approach, FAME-Net, is further proposed, that harnesses the power of Large
Language Models (LLMs) to analyze visual data and accurately interpret the
emotions conveyed through facial expressions in videos. By exploiting the
contagion effect of facial emotions, FAME-Net effectively captures the
emotional causes of individuals engaged in conversations. Our experimental
results on the newly constructed dataset show that FAME-Net significantly
outperforms several excellent large language model baselines. Code and dataset
are available at \url{https://github.com/3222345200/ECEMdataset.git}",2024-11-01,"Lin Wang, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang",http://arxiv.org/pdf/2411.02430v1,cs.CL
Chat Bankman-Fried: an Exploration of LLM Alignment in Finance,"Advancements in large language models (LLMs) have renewed concerns about AI
alignment - the consistency between human and AI goals and values. As various
jurisdictions enact legislation on AI safety, the concept of alignment must be
defined and measured across different domains. This paper proposes an
experimental framework to assess whether LLMs adhere to ethical and legal
standards in the relatively unexplored context of finance. We prompt twelve
LLMs to impersonate the CEO of a financial institution and test their
willingness to misuse customer assets to repay outstanding corporate debt.
Beginning with a baseline configuration, we adjust preferences, incentives and
constraints, analyzing the impact of each adjustment with logistic regression.
Our findings reveal significant heterogeneity in the baseline propensity for
unethical behavior of LLMs. Factors such as risk aversion, profit expectations,
and regulatory environment consistently influence misalignment in ways
predicted by economic theory, although the magnitude of these effects varies
across LLMs. This paper highlights both the benefits and limitations of
simulation-based, ex post safety testing. While it can inform financial
authorities and institutions aiming to ensure LLM safety, there is a clear
trade-off between generality and cost.",2024-11-01,"Claudia Biancotti, Carolina Camassa, Andrea Coletta, Oliver Giudice, Aldo Glielmo",http://arxiv.org/pdf/2411.11853v3,cs.CL
E2E-AFG: An End-to-End Model with Adaptive Filtering for Retrieval-Augmented Generation,"Retrieval-augmented generation methods often neglect the quality of content
retrieved from external knowledge bases, resulting in irrelevant information or
potential misinformation that negatively affects the generation results of
large language models. In this paper, we propose an end-to-end model with
adaptive filtering for retrieval-augmented generation (E2E-AFG), which
integrates answer existence judgment and text generation into a single
end-to-end framework. This enables the model to focus more effectively on
relevant content while reducing the influence of irrelevant information and
generating accurate answers. We evaluate E2E-AFG on six representative
knowledge-intensive language datasets, and the results show that it
consistently outperforms baseline models across all tasks, demonstrating the
effectiveness and robustness of the proposed approach.",2024-11-01,"Yun Jiang, Zilong Xie, Wei Zhang, Yun Fang, Shuai Pan",http://arxiv.org/pdf/2411.00437v2,cs.CL
DARD: A Multi-Agent Approach for Task-Oriented Dialog Systems,"Task-oriented dialogue systems are essential for applications ranging from
customer service to personal assistants and are widely used across various
industries. However, developing effective multi-domain systems remains a
significant challenge due to the complexity of handling diverse user intents,
entity types, and domain-specific knowledge across several domains. In this
work, we propose DARD (Domain Assigned Response Delegation), a multi-agent
conversational system capable of successfully handling multi-domain dialogs.
DARD leverages domain-specific agents, orchestrated by a central dialog manager
agent. Our extensive experiments compare and utilize various agent modeling
approaches, combining the strengths of smaller fine-tuned models (Flan-T5-large
& Mistral-7B) with their larger counterparts, Large Language Models (LLMs)
(Claude Sonnet 3.0). We provide insights into the strengths and limitations of
each approach, highlighting the benefits of our multi-agent framework in terms
of flexibility and composability. We evaluate DARD using the well-established
MultiWOZ benchmark, achieving state-of-the-art performance by improving the
dialogue inform rate by 6.6% and the success rate by 4.1% over the
best-performing existing approaches. Additionally, we discuss various annotator
discrepancies and issues within the MultiWOZ dataset and its evaluation system.",2024-11-01,"Aman Gupta, Anirudh Ravichandran, Ziji Zhang, Swair Shah, Anurag Beniwal, Narayanan Sadagopan",http://arxiv.org/pdf/2411.00427v1,cs.CL
Self-Evolved Reward Learning for LLMs,"Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for
aligning language models with human preferences, playing a pivotal role in the
success of conversational models like GPT-4, ChatGPT, and Llama 2. A core
challenge in employing RLHF lies in training a reliable reward model (RM),
which relies on high-quality labels typically provided by human experts or
advanced AI system. These methods can be costly and may introduce biases that
affect the language model's responses. As language models improve, human input
may become less effective in further enhancing their performance. In this
paper, we propose Self-Evolved Reward Learning (SER), a novel approach where
the RM generates additional training data to iteratively improve itself. We
conducted extensive experiments on multiple datasets such as HH-RLHF and
UltraFeedback, using models like Mistral and Llama 3, and compare SER against
various baselines. Our results demonstrate that even with limited
human-annotated data, learning from self-feedback can robustly enhance RM
performance, thereby boosting the capabilities of large language models (LLMs).",2024-11-01,"Chenghua Huang, Zhizhen Fan, Lu Wang, Fangkai Yang, Pu Zhao, Zeqi Lin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang",http://arxiv.org/pdf/2411.00418v2,cs.CL
Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation,"Large Language Models (LLMs) demonstrate promising capabilities in solving
simple scientific problems but, even with domain-specific fine-tuning, often
produce hallucinations for complex ones. While integrating LLMs with tools can
mitigate this reliability issue, models finetuned on tool usage only often
over-rely on them, incurring unnecessary costs from resource-intensive
scientific tools even for simpler problems. Inspired by how human experts
assess the complexity of the problem before choosing the solutions, we propose
a novel two-component fine-tuning method, Adapting While Learning (AWL). In the
first component, World Knowledge Learning (WKL), LLMs internalize scientific
knowledge by learning from tools-generated solutions. In the second component,
Tool Usage Adaptation (TUA), we classify questions as easy or hard based on the
WKL-trained model's accuracy, and train it to maintain direct reasoning for
simple problems while switching to tools for challenging ones. We validate our
method on 6 scientific benchmark datasets in climate science, epidemiology, and
mathematics. Compared to the base 8B model, our trained models achieve 28.27%
higher answer accuracy and 13.76% better tool usage accuracy, even surpassing
state-of-the-art models including GPT-4 and Claude-3.5 on 4 custom-created
datasets.",2024-11-01,"Bohan Lyu, Yadi Cao, Duncan Watson-Parris, Leon Bergen, Taylor Berg-Kirkpatrick, Rose Yu",http://arxiv.org/pdf/2411.00412v3,cs.CL
Enhancing Authorship Attribution through Embedding Fusion: A Novel Approach with Masked and Encoder-Decoder Language Models,"The increasing prevalence of AI-generated content alongside human-written
text underscores the need for reliable discrimination methods. To address this
challenge, we propose a novel framework with textual embeddings from
Pre-trained Language Models (PLMs) to distinguish AI-generated and
human-authored text. Our approach utilizes Embedding Fusion to integrate
semantic information from multiple Language Models, harnessing their
complementary strengths to enhance performance. Through extensive evaluation
across publicly available diverse datasets, our proposed approach demonstrates
strong performance, achieving classification accuracy greater than 96% and a
Matthews Correlation Coefficient (MCC) greater than 0.93. This evaluation is
conducted on a balanced dataset of texts generated from five well-known Large
Language Models (LLMs), highlighting the effectiveness and robustness of our
novel methodology.",2024-11-01,"Arjun Ramesh Kaushik, Sunil Rufus R P, Nalini Ratha",http://arxiv.org/pdf/2411.00411v1,cs.CL
MetaMetrics-MT: Tuning Meta-Metrics for Machine Translation via Human Preference Calibration,"We present MetaMetrics-MT, an innovative metric designed to evaluate machine
translation (MT) tasks by aligning closely with human preferences through
Bayesian optimization with Gaussian Processes. MetaMetrics-MT enhances existing
MT metrics by optimizing their correlation with human judgments. Our
experiments on the WMT24 metric shared task dataset demonstrate that
MetaMetrics-MT outperforms all existing baselines, setting a new benchmark for
state-of-the-art performance in the reference-based setting. Furthermore, it
achieves comparable results to leading metrics in the reference-free setting,
offering greater efficiency.",2024-11-01,"David Anugraha, Garry Kuwanto, Lucky Susanto, Derry Tanti Wijaya, Genta Indra Winata",http://arxiv.org/pdf/2411.00390v1,cs.CL
STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing,"Advances in large language models (LLMs) have spurred research into enhancing
their reasoning capabilities, particularly in math-rich STEM documents. While
LLMs can generate equations or solve math-related queries, their ability to
fully understand and interpret abstract mathematical symbols in long, math-rich
documents remains limited. In this paper, we introduce STEM-PoM, a
comprehensive benchmark dataset designed to evaluate LLMs' reasoning abilities
on math symbols within contextual scientific text. The dataset, sourced from
real-world ArXiv documents, contains over 2K math symbols classified as main
attributes of variables, constants, operators, and unit descriptors, with
additional sub-attributes including scalar/vector/matrix for variables and
local/global/discipline-specific labels for both constants and operators. Our
extensive experiments show that state-of-the-art LLMs achieve an average of
20-60% accuracy under in-context learning and 50-60% accuracy with fine-tuning,
revealing a significant gap in their mathematical reasoning capabilities.
STEM-PoM fuels future research of developing advanced Math-AI models that can
robustly handle math symbols.",2024-11-01,"Jiaru Zou, Qing Wang, Pratyush Thakur, Nickvash Kani",http://arxiv.org/pdf/2411.00387v1,cs.CL
GRS-QA -- Graph Reasoning-Structured Question Answering Dataset,"Large Language Models (LLMs) have excelled in multi-hop question-answering
(M-QA) due to their advanced reasoning abilities. However, the impact of the
inherent reasoning structures on LLM M-QA performance remains unclear, largely
due to the absence of QA datasets that provide fine-grained reasoning
structures. To address this gap, we introduce the Graph Reasoning-Structured
Question Answering Dataset (GRS-QA), which includes both semantic contexts and
reasoning structures for QA pairs. Unlike existing M-QA datasets, where
different reasoning structures are entangled together, GRS-QA explicitly
captures intricate reasoning pathways by constructing reasoning graphs, where
nodes represent textual contexts and edges denote logical flows. These
reasoning graphs of different structures enable a fine-grained evaluation of
LLM reasoning capabilities across various reasoning structures. Our empirical
analysis reveals that LLMs perform differently when handling questions with
varying reasoning structures. This finding facilitates the exploration of
textual structures as compared with semantics.",2024-11-01,"Anish Pahilajani, Devasha Trivedi, Jincen Shuai, Khin S. Yone, Samyak Rajesh Jain, Namyong Park, Ryan A. Rossi, Nesreen K. Ahmed, Franck Dernoncourt, Yu Wang",http://arxiv.org/pdf/2411.00369v3,cs.CL
Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model through Reinforcement Learning from AI Feedback,"Although large language models perform well in understanding and responding
to user intent, their performance in specialized domains such as Traditional
Chinese Medicine (TCM) remains limited due to lack of expertise. In addition,
high-quality data related to TCM is scarce and difficult to obtain, making
large language models ineffective in handling TCM tasks. In this work, we
propose a framework to improve the performance of large language models for TCM
tasks using only a small amount of data. First, we use medical case data for
supervised fine-tuning of the large model, making it initially capable of
performing TCM tasks. Subsequently, we further optimize the model's performance
using reinforcement learning from AI feedback (RLAIF) to align it with the
preference data. The ablation study also demonstrated the performance gain is
attributed to both supervised fine-tuning and the direct policy optimization.
The experimental results show that the model trained with a small amount of
data achieves a significant performance improvement on a representative TCM
task.",2024-11-01,"Song Yu, Xiaofei Xu, Fangfei Xu, Li Li",http://arxiv.org/pdf/2411.00897v1,cs.CL
Evaluating the Impact of Lab Test Results on Large Language Models Generated Differential Diagnoses from Clinical Case Vignettes,"Differential diagnosis is crucial for medicine as it helps healthcare
providers systematically distinguish between conditions that share similar
symptoms. This study assesses the impact of lab test results on differential
diagnoses (DDx) made by large language models (LLMs). Clinical vignettes from
50 case reports from PubMed Central were created incorporating patient
demographics, symptoms, and lab results. Five LLMs GPT-4, GPT-3.5, Llama-2-70b,
Claude-2, and Mixtral-8x7B were tested to generate Top 10, Top 5, and Top 1 DDx
with and without lab data. A comprehensive evaluation involving GPT-4, a
knowledge graph, and clinicians was conducted. GPT-4 performed best, achieving
55% accuracy for Top 1 diagnoses and 60% for Top 10 with lab data, with lenient
accuracy up to 80%. Lab results significantly improved accuracy, with GPT-4 and
Mixtral excelling, though exact match rates were low. Lab tests, including
liver function, metabolic/toxicology panels, and serology/immune tests, were
generally interpreted correctly by LLMs for differential diagnosis.",2024-11-01,"Balu Bhasuran, Qiao Jin, Yuzhang Xie, Carl Yang, Karim Hanna, Jennifer Costa, Cindy Shavor, Zhiyong Lu, Zhe He",http://arxiv.org/pdf/2411.02523v1,cs.CL
Learning to Rank Salient Content for Query-focused Summarization,"This study examines the potential of integrating Learning-to-Rank (LTR) with
Query-focused Summarization (QFS) to enhance the summary relevance via content
prioritization. Using a shared secondary decoder with the summarization
decoder, we carry out the LTR task at the segment level. Compared to the
state-of-the-art, our model outperforms on QMSum benchmark (all metrics) and
matches on SQuALITY benchmark (2 metrics) as measured by Rouge and BertScore
while offering a lower training overhead. Specifically, on the QMSum benchmark,
our proposed system achieves improvements, particularly in Rouge-L (+0.42) and
BertScore (+0.34), indicating enhanced understanding and relevance. While
facing minor challenges in Rouge-1 and Rouge-2 scores on the SQuALITY
benchmark, the model significantly excels in Rouge-L (+1.47), underscoring its
capability to generate coherent summaries. Human evaluations emphasize the
efficacy of our method in terms of relevance and faithfulness of the generated
summaries, without sacrificing fluency. A deeper analysis reveals our model's
superiority over the state-of-the-art for broad queries, as opposed to specific
ones, from a qualitative standpoint. We further present an error analysis of
our model, pinpointing challenges faced and suggesting potential directions for
future research in this field.",2024-11-01,"Sajad Sotudeh, Nazli Goharian",http://arxiv.org/pdf/2411.00324v1,cs.CL
Rationale-Guided Retrieval Augmented Generation for Medical Question Answering,"Large language models (LLM) hold significant potential for applications in
biomedicine, but they struggle with hallucinations and outdated knowledge.
While retrieval-augmented generation (RAG) is generally employed to address
these issues, it also has its own set of challenges: (1) LLMs are vulnerable to
irrelevant or incorrect context, (2) medical queries are often not
well-targeted for helpful information, and (3) retrievers are prone to bias
toward the specific source corpus they were trained on. In this study, we
present RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the
reliability of RAG in biomedical contexts. RAG$^2$ incorporates three key
innovations: a small filtering model trained on perplexity-based labels of
rationales, which selectively augments informative snippets of documents while
filtering out distractors; LLM-generated rationales as queries to improve the
utility of retrieved snippets; a structure designed to retrieve snippets evenly
from a comprehensive set of four biomedical corpora, effectively mitigating
retriever bias. Our experiments demonstrate that RAG$^2$ improves the
state-of-the-art LLMs of varying sizes, with improvements of up to 6.1\%, and
it outperforms the previous best medical RAG model by up to 5.6\% across three
medical question-answering benchmarks. Our code is available at
https://github.com/dmis-lab/RAG2.",2024-11-01,"Jiwoong Sohn, Yein Park, Chanwoong Yoon, Sihyeon Park, Hyeon Hwang, Mujeen Sung, Hyunjae Kim, Jaewoo Kang",http://arxiv.org/pdf/2411.00300v1,cs.CL
LLM-Ref: Enhancing Reference Handling in Technical Writing with Large Language Models,"Large Language Models (LLMs) excel in data synthesis but can be inaccurate in
domain-specific tasks, which retrieval-augmented generation (RAG) systems
address by leveraging user-provided data. However, RAGs require optimization in
both retrieval and generation stages, which can affect output quality. In this
paper, we present LLM-Ref, a writing assistant tool that aids researchers in
writing articles from multiple source documents with enhanced reference
synthesis and handling capabilities. Unlike traditional RAG systems that use
chunking and indexing, our tool retrieves and generates content directly from
text paragraphs. This method facilitates direct reference extraction from the
generated outputs, a feature unique to our tool. Additionally, our tool employs
iterative response generation, effectively managing lengthy contexts within the
language model's constraints. Compared to baseline RAG-based systems, our
approach achieves a $3.25\times$ to $6.26\times$ increase in Ragas score, a
comprehensive metric that provides a holistic view of a RAG system's ability to
produce accurate, relevant, and contextually appropriate responses. This
improvement shows our method enhances the accuracy and contextual relevance of
writing assistance tools.",2024-11-01,"Kazi Ahmed Asif Fuad, Lizhong Chen",http://arxiv.org/pdf/2411.00294v2,cs.CL
Average-Over-Time Spiking Neural Networks for Uncertainty Estimation in Regression,"Uncertainty estimation is a standard tool to quantify the reliability of
modern deep learning models, and crucial for many real-world applications.
However, efficient uncertainty estimation methods for spiking neural networks,
particularly for regression models, have been lacking. Here, we introduce two
methods that adapt the Average-Over-Time Spiking Neural Network (AOT-SNN)
framework to regression tasks, enhancing uncertainty estimation in event-driven
models. The first method uses the heteroscedastic Gaussian approach, where SNNs
predict both the mean and variance at each time step, thereby generating a
conditional probability distribution of the target variable. The second method
leverages the Regression-as-Classification (RAC) approach, reformulating
regression as a classification problem to facilitate uncertainty estimation. We
evaluate our approaches on both a toy dataset and several benchmark datasets,
demonstrating that the proposed AOT-SNN models achieve performance comparable
to or better than state-of-the-art deep neural network methods, particularly in
uncertainty estimation. Our findings highlight the potential of SNNs for
uncertainty estimation in regression tasks, providing an efficient and
biologically inspired alternative for applications requiring both accuracy and
energy efficiency.",2024-11-29,"Tao Sun, Sander Bohté",http://arxiv.org/pdf/2412.00278v1,cs.LG
Attribute-Enhanced Similarity Ranking for Sparse Link Prediction,"Link prediction is a fundamental problem in graph data. In its most realistic
setting, the problem consists of predicting missing or future links between
random pairs of nodes from the set of disconnected pairs. Graph Neural Networks
(GNNs) have become the predominant framework for link prediction. GNN-based
methods treat link prediction as a binary classification problem and handle the
extreme class imbalance -- real graphs are very sparse -- by sampling
(uniformly at random) a balanced number of disconnected pairs not only for
training but also for evaluation. However, we show that the reported
performance of GNNs for link prediction in the balanced setting does not
translate to the more realistic imbalanced setting and that simpler
topology-based approaches are often better at handling sparsity. These findings
motivate Gelato, a similarity-based link-prediction method that applies (1)
graph learning based on node attributes to enhance a topological heuristic, (2)
a ranking loss for addressing class imbalance, and (3) a negative sampling
scheme that efficiently selects hard training pairs via graph partitioning.
Experiments show that Gelato outperforms existing GNN-based alternatives.",2024-11-29,"João Mattos, Zexi Huang, Mert Kosan, Ambuj Singh, Arlei Silva",http://arxiv.org/pdf/2412.00261v1,cs.LG
Integrating Social Determinants of Health into Knowledge Graphs: Evaluating Prediction Bias and Fairness in Healthcare,"Social determinants of health (SDoH) play a crucial role in patient health
outcomes, yet their integration into biomedical knowledge graphs remains
underexplored. This study addresses this gap by constructing an SDoH-enriched
knowledge graph using the MIMIC-III dataset and PrimeKG. We introduce a novel
fairness formulation for graph embeddings, focusing on invariance with respect
to sensitive SDoH information. Via employing a heterogeneous-GCN model for
drug-disease link prediction, we detect biases related to various SDoH factors.
To mitigate these biases, we propose a post-processing method that
strategically reweights edges connected to SDoHs, balancing their influence on
graph representations. This approach represents one of the first comprehensive
investigations into fairness issues within biomedical knowledge graphs
incorporating SDoH. Our work not only highlights the importance of considering
SDoH in medical informatics but also provides a concrete method for reducing
SDoH-related biases in link prediction tasks, paving the way for more equitable
healthcare recommendations. Our code is available at
\url{https://github.com/hwq0726/SDoH-KG}.",2024-11-29,"Tianqi Shang, Weiqing He, Tianlong Chen, Ying Ding, Huanmei Wu, Kaixiong Zhou, Li Shen",http://arxiv.org/pdf/2412.00245v1,cs.LG
Robust Testing for Deep Learning using Human Label Noise,"In deep learning (DL) systems, label noise in training datasets often
degrades model performance, as models may learn incorrect patterns from
mislabeled data. The area of Learning with Noisy Labels (LNL) has introduced
methods to effectively train DL models in the presence of noisily-labeled
datasets. Traditionally, these methods are tested using synthetic label noise,
where ground truth labels are randomly (and automatically) flipped. However,
recent findings highlight that models perform substantially worse under human
label noise than synthetic label noise, indicating a need for more realistic
test scenarios that reflect noise introduced due to imperfect human labeling.
This underscores the need for generating realistic noisy labels that simulate
human label noise, enabling rigorous testing of deep neural networks without
the need to collect new human-labeled datasets. To address this gap, we present
Cluster-Based Noise (CBN), a method for generating feature-dependent noise that
simulates human-like label noise. Using insights from our case study of label
memorization in the CIFAR-10N dataset, we design CBN to create more realistic
tests for evaluating LNL methods. Our experiments demonstrate that current LNL
methods perform worse when tested using CBN, highlighting its use as a rigorous
approach to testing neural networks. Next, we propose Soft Neighbor Label
Sampling (SNLS), a method designed to handle CBN, demonstrating its improvement
over existing techniques in tackling this more challenging type of noise.",2024-11-29,"Gordon Lim, Stefan Larson, Kevin Leach",http://arxiv.org/pdf/2412.00244v1,cs.LG
Multigraph Message Passing with Bi-Directional Multi-Edge Aggregations,"Graph Neural Networks (GNNs) have seen significant advances in recent years,
yet their application to multigraphs, where parallel edges exist between the
same pair of nodes, remains under-explored. Standard GNNs, designed for simple
graphs, compute node representations by combining all connected edges at once,
without distinguishing between edges from different neighbors. There are some
GNN architectures proposed specifically for multigraphs, yet these
architectures perform only node-level aggregation in their message passing
layers, which limits their expressive power. Furthermore, these approaches
either lack permutation equivariance when a strict total edge ordering is
absent, or fail to preserve the topological structure of the multigraph. To
address all these shortcomings, we propose MEGA-GNN, a unified framework for
message passing on multigraphs that can effectively perform diverse graph
learning tasks. Our approach introduces a two-stage aggregation process in the
message passing layers: first, parallel edges are aggregated, followed by a
node-level aggregation of messages from distinct neighbors. We show that
MEGA-GNN is not only permutation equivariant but also universal given a strict
total ordering on the edges. Experiments show that MEGA-GNN significantly
outperforms state-of-the-art solutions by up to 13\% on Anti-Money Laundering
datasets and is on par with their accuracy on real-world phishing
classification datasets in terms of minority class F1 score.",2024-11-29,"H. Çağrı Bilgi, Lydia Y. Chen, Kubilay Atasu",http://arxiv.org/pdf/2412.00241v2,cs.LG
Hybrid Spiking Neural Network -- Transformer Video Classification Model,"In recent years, Spiking Neural Networks (SNNs) have gathered significant
interest due to their temporal understanding capabilities. This work
introduces, to the best of our knowledge, the first Cortical Column like hybrid
architecture for the Time-Series Data Classification Task that leverages SNNs
and is inspired by the brain structure, inspired from the previous hybrid
models. We introduce several encoding methods to use with this model. Finally,
we develop a procedure for training this network on the training dataset. As an
effort to make using these models simpler, we make all the implementations
available to the public.",2024-11-29,Aaron Bateni,http://arxiv.org/pdf/2412.00237v1,cs.LG
Meta-learning Loss Functions of Parametric Partial Differential Equations Using Physics-Informed Neural Networks,"This paper proposes a new way to learn Physics-Informed Neural Network loss
functions using Generalized Additive Models. We apply our method by
meta-learning parametric partial differential equations, PDEs, on Burger's and
2D Heat Equations. The goal is to learn a new loss function for each parametric
PDE using meta-learning. The derived loss function replaces the traditional
data loss, allowing us to learn each parametric PDE more efficiently, improving
the meta-learner's performance and convergence.",2024-11-29,"Michail Koumpanakis, Ricardo Vilalta",http://arxiv.org/pdf/2412.00225v1,cs.LG
NushuRescue: Revitalization of the Endangered Nushu Language with AI,"The preservation and revitalization of endangered and extinct languages is a
meaningful endeavor, conserving cultural heritage while enriching fields like
linguistics and anthropology. However, these languages are typically
low-resource, making their reconstruction labor-intensive and costly. This
challenge is exemplified by Nushu, a rare script historically used by Yao women
in China for self-expression within a patriarchal society. To address this
challenge, we introduce NushuRescue, an AI-driven framework designed to train
large language models (LLMs) on endangered languages with minimal data.
NushuRescue automates evaluation and expands target corpora to accelerate
linguistic revitalization. As a foundational component, we developed NCGold, a
500-sentence Nushu-Chinese parallel corpus, the first publicly available
dataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to Nushu
and only 35 short examples from NCGold, NushuRescue achieved 48.69% translation
accuracy on 50 withheld sentences and generated NCSilver, a set of 98 newly
translated modern Chinese sentences of varying lengths. A sample of both NCGold
and NCSilver is included in the Supplementary Materials. Additionally, we
developed FastText-based and Seq2Seq models to further support research on
Nushu. NushuRescue provides a versatile and scalable tool for the
revitalization of endangered languages, minimizing the need for extensive human
input.",2024-11-29,"Ivory Yang, Weicheng Ma, Soroush Vosoughi",http://arxiv.org/pdf/2412.00218v4,cs.LG
Diffusion Model Guided Sampling with Pixel-Wise Aleatoric Uncertainty Estimation,"Despite the remarkable progress in generative modelling, current diffusion
models lack a quantitative approach to assess image quality. To address this
limitation, we propose to estimate the pixel-wise aleatoric uncertainty during
the sampling phase of diffusion models and utilise the uncertainty to improve
the sample generation quality. The uncertainty is computed as the variance of
the denoising scores with a perturbation scheme that is specifically designed
for diffusion models. We then show that the aleatoric uncertainty estimates are
related to the second-order derivative of the diffusion noise distribution. We
evaluate our uncertainty estimation algorithm and the uncertainty-guided
sampling on the ImageNet and CIFAR-10 datasets. In our comparisons with the
related work, we demonstrate promising results in filtering out low quality
samples. Furthermore, we show that our guided approach leads to better sample
generation in terms of FID scores.",2024-11-29,"Michele De Vita, Vasileios Belagiannis",http://arxiv.org/pdf/2412.00205v1,cs.LG
Scaling of Stochastic Normalizing Flows in $\mathrm{SU}(3)$ lattice gauge theory,"Non-equilibrium Markov Chain Monte Carlo (NE-MCMC) simulations provide a
well-understood framework based on Jarzynski's equality to sample from a target
probability distribution. By driving a base probability distribution out of
equilibrium, observables are computed without the need to thermalize. If the
base distribution is characterized by mild autocorrelations, this approach
provides a way to mitigate critical slowing down. Out-of-equilibrium evolutions
share the same framework of flow-based approaches and they can be naturally
combined into a novel architecture called Stochastic Normalizing Flows (SNFs).
In this work we present the first implementation of SNFs for $\mathrm{SU}(3)$
lattice gauge theory in 4 dimensions, defined by introducing gauge-equivariant
layers between out-of-equilibrium Monte Carlo updates. The core of our analysis
is focused on the promising scaling properties of this architecture with the
degrees of freedom of the system, which are directly inherited from NE-MCMC.
Finally, we discuss how systematic improvements of this approach can
realistically lead to a general and yet efficient sampling strategy at fine
lattice spacings for observables affected by long autocorrelation times.",2024-11-29,"Andrea Bulgarelli, Elia Cellini, Alessandro Nada",http://arxiv.org/pdf/2412.00200v3,cs.LG
Improving the performance of weak supervision searches using data augmentation,"Weak supervision combines the advantages of training on real data with the
ability to exploit signal properties. However, training a neural network using
weak supervision often requires an excessive amount of signal data, which
severely limits its practical applicability. In this study, we propose
addressing this limitation through data augmentation, increasing the training
data's size and diversity. Specifically, we focus on physics-inspired data
augmentation methods, such as $p_{\text{T}}$ smearing and jet rotation. Our
results demonstrate that data augmentation can significantly enhance the
performance of weak supervision, enabling neural networks to learn efficiently
from substantially less data.",2024-11-29,"Zong-En Chen, Cheng-Wei Chiang, Feng-Yang Hsieh",http://arxiv.org/pdf/2412.00198v1,cs.LG
Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation,"Recent years have witnessed the success of Multimodal Large Language Models
(MLLMs) in the vision understanding domain. The success of these models can
largely be attributed to the dominant scaling law, which states that larger
parameter sizes and data volumes contribute to better performance. Notably,
data scaling has mainly been powered by automatic data pipelines, which center
around the self-instruction of LLMs. The paradigm has been taken for granted
for quite some time, but the study of the effectiveness of scaling with these
data has been neglected for a long time. In this context, this work revisits
scaling with synthetic data and focuses on developing video-LLMs from a
data-centric perspective. Our main study approach is fine-tuning pre-trained
image-LLMs with video data and investigating learning efficiency through data
scaling. Results from our preliminary experiments reveal a low learning
efficiency phenomenon when simply scaling up video data samples, which, through
our probing, can be ascribed to a lack of instruction diversity. Aiming at this
issue, we propose a data augmentation method called Sparrow, which synthesizes
video-like samples from pure text instruction data. Mixing these synthetic
samples with the video data enables a more efficient training scheme. Through
comprehensive experiments, we demonstrate that our proposed method achieves
performance comparable to or even superior to baselines trained with many more
samples. Meanwhile, we find that incorporating these synthetic samples can
boost the performance of long video understanding without training with long
video data. The code and data examples are available at
https://github.com/VITA-MLLM/Sparrow.",2024-11-29,"Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan Yang, Zuwei Long, Yuhan Dai, Yongdong Luo, Haoyu Cao, Tong Xu, Xing Sun, Caifeng Shan, Ran He, Enhong Chen",http://arxiv.org/pdf/2411.19951v4,cs.LG
AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos,"We introduce AlphaTablets, a novel and generic representation of 3D planes
that features continuous 3D surface and precise boundary delineation. By
representing 3D planes as rectangles with alpha channels, AlphaTablets combine
the advantages of current 2D and 3D plane representations, enabling accurate,
consistent and flexible modeling of 3D planes. We derive differentiable
rasterization on top of AlphaTablets to efficiently render 3D planes into
images, and propose a novel bottom-up pipeline for 3D planar reconstruction
from monocular videos. Starting with 2D superpixels and geometric cues from
pre-trained models, we initialize 3D planes as AlphaTablets and optimize them
via differentiable rendering. An effective merging scheme is introduced to
facilitate the growth and refinement of AlphaTablets. Through iterative
optimization and merging, we reconstruct complete and accurate 3D planes with
solid surfaces and clear boundaries. Extensive experiments on the ScanNet
dataset demonstrate state-of-the-art performance in 3D planar reconstruction,
underscoring the great potential of AlphaTablets as a generic 3D plane
representation for various applications. Project page is available at:
https://hyzcluster.github.io/alphatablets",2024-11-29,"Yuze He, Wang Zhao, Shaohui Liu, Yubin Hu, Yushi Bai, Yu-Hui Wen, Yong-Jin Liu",http://arxiv.org/pdf/2411.19950v1,cs.LG
DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation,"Recent advances in dataset distillation have led to solutions in two main
directions. The conventional batch-to-batch matching mechanism is ideal for
small-scale datasets and includes bi-level optimization methods on models and
syntheses, such as FRePo, RCIG, and RaT-BPTT, as well as other methods like
distribution matching, gradient matching, and weight trajectory matching.
Conversely, batch-to-global matching typifies decoupled methods, which are
particularly advantageous for large-scale datasets. This approach has garnered
substantial interest within the community, as seen in SRe$^2$L, G-VBSM, WMDD,
and CDA. A primary challenge with the second approach is the lack of diversity
among syntheses within each class since samples are optimized independently and
the same global supervision signals are reused across different synthetic
images. In this study, we propose a new Diversity-driven EarlyLate Training
(DELT) scheme to enhance the diversity of images in batch-to-global matching
with less computation. Our approach is conceptually simple yet effective, it
partitions predefined IPC samples into smaller subtasks and employs local
optimizations to distill each subset into distributions from distinct phases,
reducing the uniformity induced by the unified optimization process. These
distilled images from the subtasks demonstrate effective generalization when
applied to the entire task. We conduct extensive experiments on CIFAR,
Tiny-ImageNet, ImageNet-1K, and its sub-datasets. Our approach outperforms the
previous state-of-the-art by 2$\sim$5% on average across different datasets and
IPCs (images per class), increasing diversity per class by more than 5% while
reducing synthesis time by up to 39.3% for enhancing the training efficiency.
Code is available at: https://github.com/VILA-Lab/DELT.",2024-11-29,"Zhiqiang Shen, Ammar Sherif, Zeyuan Yin, Shitong Shao",http://arxiv.org/pdf/2411.19946v1,cs.LG
LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting,"We introduce LumiNet, a novel architecture that leverages generative models
and latent intrinsic representations for effective lighting transfer. Given a
source image and a target lighting image, LumiNet synthesizes a relit version
of the source scene that captures the target's lighting. Our approach makes two
key contributions: a data curation strategy from the StyleGAN-based relighting
model for our training, and a modified diffusion-based ControlNet that
processes both latent intrinsic properties from the source image and latent
extrinsic properties from the target image. We further improve lighting
transfer through a learned adaptor (MLP) that injects the target's latent
extrinsic properties via cross-attention and fine-tuning.
  Unlike traditional ControlNet, which generates images with conditional maps
from a single scene, LumiNet processes latent representations from two
different images - preserving geometry and albedo from the source while
transferring lighting characteristics from the target. Experiments demonstrate
that our method successfully transfers complex lighting phenomena including
specular highlights and indirect illumination across scenes with varying
spatial layouts and materials, outperforming existing approaches on challenging
indoor scenes using only images as input.",2024-11-29,"Xiaoyan Xing, Konrad Groh, Sezer Karaoglu, Theo Gevers, Anand Bhattad",http://arxiv.org/pdf/2412.00177v2,cs.LG
Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability,"Mathematical reasoning tasks pose significant challenges for large language
models (LLMs) because they require precise logical deduction and sequence
analysis. In this work, we introduce the concept of critical tokens -- elements
within reasoning trajectories that significantly influence incorrect outcomes.
We present a novel framework for identifying these tokens through rollout
sampling and demonstrate their substantial divergence from traditional error
tokens. Through extensive experiments on datasets such as GSM8K and MATH500, we
show that identifying and replacing critical tokens significantly improves
model accuracy. We propose an efficient methodology for pinpointing these
tokens in large-scale datasets using contrastive estimation and extend this
framework to enhance model training processes with direct preference
optimization (DPO). Experimental results on GSM8K and MATH500 benchmarks with
the widely used models Llama-3 (8B and 70B) and Deepseek-math (7B) demonstrate
the effectiveness of the proposed approach, cDPO. Our results underscore the
potential of leveraging critical tokens to reduce errors in reasoning tasks,
advancing the development of AI systems capable of robust logical deduction.
Our code, annotated datasets, and trained models are available at
https://github.com/chenzhiling9954/Critical-Tokens-Matter to support and
encourage future research in this promising field.",2024-11-29,"Zicheng Lin, Tian Liang, Jiahao Xu, Qiuzhi Lin, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, Zhaopeng Tu",http://arxiv.org/pdf/2411.19943v3,cs.LG
Circumventing shortcuts in audio-visual deepfake detection datasets with unsupervised learning,"Good datasets are essential for developing and benchmarking any machine
learning system. Their importance is even more extreme for safety critical
applications such as deepfake detection - the focus of this paper. Here we
reveal that two of the most widely used audio-video deepfake datasets suffer
from a previously unidentified spurious feature: the leading silence. Fake
videos start with a very brief moment of silence and based on this feature
alone, we can separate the real and fake samples almost perfectly. As such,
previous audio-only and audio-video models exploit the presence of silence in
the fake videos and consequently perform worse when the leading silence is
removed. To circumvent latching on such unwanted artifact and possibly other
unrevealed ones we propose a shift from supervised to unsupervised learning by
training models exclusively on real data. We show that by aligning
self-supervised audio-video representations we remove the risk of relying on
dataset-specific biases and improve robustness in deepfake detection.",2024-11-29,"Stefan Smeu, Dragos-Alexandru Boldisor, Dan Oneata, Elisabeta Oneata",http://arxiv.org/pdf/2412.00175v2,cs.LG
FreeCloth: Free-form Generation Enhances Challenging Clothed Human Modeling,"Achieving realistic animated human avatars requires accurate modeling of
pose-dependent clothing deformations. Existing learning-based methods heavily
rely on the Linear Blend Skinning (LBS) of minimally-clothed human models like
SMPL to model deformation. However, they struggle to handle loose clothing,
such as long dresses, where the canonicalization process becomes ill-defined
when the clothing is far from the body, leading to disjointed and fragmented
results. To overcome this limitation, we propose FreeCloth, a novel hybrid
framework to model challenging clothed humans. Our core idea is to use
dedicated strategies to model different regions, depending on whether they are
close to or distant from the body. Specifically, we segment the human body into
three categories: unclothed, deformed, and generated. We simply replicate
unclothed regions that require no deformation. For deformed regions close to
the body, we leverage LBS to handle the deformation. As for the generated
regions, which correspond to loose clothing areas, we introduce a novel
free-form, part-aware generator to model them, as they are less affected by
movements. This free-form generation paradigm brings enhanced flexibility and
expressiveness to our hybrid framework, enabling it to capture the intricate
geometric details of challenging loose clothing, such as skirts and dresses.
Experimental results on the benchmark dataset featuring loose clothing
demonstrate that FreeCloth achieves state-of-the-art performance with superior
visual fidelity and realism, particularly in the most challenging cases.",2024-11-29,"Hang Ye, Xiaoxuan Ma, Hai Ci, Wentao Zhu, Yizhou Wang",http://arxiv.org/pdf/2411.19942v3,cs.LG
Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark,"Following the successful 2023 edition, we organised the Second Perception
Test challenge as a half-day workshop alongside the IEEE/CVF European
Conference on Computer Vision (ECCV) 2024, with the goal of benchmarking
state-of-the-art video models and measuring the progress since last year using
the Perception Test benchmark. This year, the challenge had seven tracks (up
from six last year) and covered low-level and high-level tasks, with language
and non-language interfaces, across video, audio, and text modalities; the
additional track covered hour-long video understanding and introduced a novel
video QA benchmark 1h-walk VQA. Overall, the tasks in the different tracks
were: object tracking, point tracking, temporal action localisation, temporal
sound localisation, multiple-choice video question-answering, grounded video
question-answering, and hour-long video question-answering. We summarise in
this report the challenge tasks and results, and introduce in detail the novel
hour-long video QA benchmark 1h-walk VQA.",2024-11-29,"Joseph Heyward, João Carreira, Dima Damen, Andrew Zisserman, Viorica Pătrăucean",http://arxiv.org/pdf/2411.19941v1,cs.LG
SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters,"Human beings are social animals. How to equip 3D autonomous characters with
similar social intelligence that can perceive, understand and interact with
humans remains an open yet foundamental problem. In this paper, we introduce
SOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling
framework for Immersive interaction with 3D autonomous characters.
Specifically, SOLAMI builds 3D autonomous characters from three aspects: (1)
Social VLA Architecture: We propose a unified social VLA framework to generate
multimodal response (speech and motion) based on the user's multimodal input to
drive the character for social interaction. (2) Interactive Multimodal Data: We
present SynMSI, a synthetic multimodal social interaction dataset generated by
an automatic pipeline using only existing motion datasets to address the issue
of data scarcity. (3) Immersive VR Interface: We develop a VR interface that
enables users to immersively interact with these characters driven by various
architectures. Extensive quantitative experiments and user studies demonstrate
that our framework leads to more precise and natural character responses (in
both speech and motion) that align with user expectations with lower latency.",2024-11-29,"Jianping Jiang, Weiye Xiao, Zhengyu Lin, Huaizhong Zhang, Tianxiang Ren, Yang Gao, Zhiqian Lin, Zhongang Cai, Lei Yang, Ziwei Liu",http://arxiv.org/pdf/2412.00174v1,cs.LG
On Domain-Specific Post-Training for Multimodal Large Language Models,"Adapting general multimodal large language models (MLLMs) to specific
domains, such as scientific and industrial fields, is highly significant in
promoting their practical applications. This paper systematically investigates
domain adaptation of MLLMs through post-training, focusing on data synthesis,
training pipelines, and task evaluation. (1) Data Synthesis: Using only
open-source models, we develop a generate-then-filter pipeline that curates
diverse visual instruction tasks based on domain-specific image-caption pairs.
The resulting data surpass the data synthesized by manual rules or strong
closed-source models (e.g., GPT-4V) in enhancing domain-specific performance.
(2) Training Pipeline: While the two-stage training--initially on image-caption
pairs followed by visual instruction tasks--is commonly adopted for developing
general MLLMs, we apply a single-stage training pipeline to enhance task
diversity for domain-specific post-training. (3) Task Evaluation: We conduct
extensive experiments in high-impact domains such as biomedicine, food, and
remote sensing, by post-training a variety of MLLMs and then evaluating MLLM
performance on various domain-specific tasks. Furthermore, we fully open-source
our models, code, and data to encourage future research in this area.",2024-11-29,"Daixuan Cheng, Shaohan Huang, Ziyu Zhu, Xintong Zhang, Wayne Xin Zhao, Zhongzhi Luan, Bo Dai, Zhenliang Zhang",http://arxiv.org/pdf/2411.19930v2,cs.LG
Scalable Out-of-distribution Robustness in the Presence of Unobserved Confounders,"We consider the task of out-of-distribution (OOD) generalization, where the
distribution shift is due to an unobserved confounder ($Z$) affecting both the
covariates ($X$) and the labels ($Y$). In this setting, traditional assumptions
of covariate and label shift are unsuitable due to the confounding, which
introduces heterogeneity in the predictor, i.e., $\hat{Y} = f_Z(X)$. OOD
generalization differs from traditional domain adaptation by not assuming
access to the covariate distribution ($X^\text{te}$) of the test samples during
training. These conditions create a challenging scenario for OOD robustness:
(a) $Z^\text{tr}$ is an unobserved confounder during training, (b)
$P^\text{te}{Z} \neq P^\text{tr}{Z}$, (c) $X^\text{te}$ is unavailable during
training, and (d) the posterior predictive distribution depends on
$P^\text{te}(Z)$, i.e., $\hat{Y} = E_{P^\text{te}(Z)}[f_Z(X)]$. In general,
accurate predictions are unattainable in this scenario, and existing literature
has proposed complex predictors based on identifiability assumptions that
require multiple additional variables. Our work investigates a set of
identifiability assumptions that tremendously simplify the predictor, whose
resulting elegant simplicity outperforms existing approaches.",2024-11-29,"Parjanya Prashant, Seyedeh Baharan Khatami, Bruno Ribeiro, Babak Salimi",http://arxiv.org/pdf/2411.19923v1,cs.LG
Dynamic EEG-fMRI mapping: Revealing the relationship between brain connectivity and cognitive state,"This study investigated the dynamic connectivity patterns between EEG and
fMRI modalities, contributing to our understanding of brain network
interactions. By employing a comprehensive approach that integrated static and
dynamic analyses of EEG-fMRI data, we were able to uncover distinct
connectivity states and characterize their temporal fluctuations. The results
revealed modular organization within the intrinsic connectivity networks (ICNs)
of the brain, highlighting the significant roles of sensory systems and the
default mode network. The use of a sliding window technique allowed us to
assess how functional connectivity varies over time, further elucidating the
transient nature of brain connectivity. Additionally, our findings align with
previous literature, reinforcing the notion that cognitive states can be
effectively identified through short-duration data, specifically within the
30-60 second timeframe. The established relationships between connectivity
strength and cognitive processes, particularly during different visual states,
underscore the relevance of our approach for future research into brain
dynamics. Overall, this study not only enhances our understanding of the
interplay between EEG and fMRI signals but also paves the way for further
exploration into the neural correlates of cognitive functions and their
implications in clinical settings. Future research should focus on refining
these methodologies and exploring their applications in various cognitive and
clinical contexts.",2024-11-29,"Guiran Liu, Binrong Zhu",http://arxiv.org/pdf/2411.19922v1,cs.LG
Quantifying the synthetic and real domain gap in aerial scene understanding,"Quantifying the gap between synthetic and real-world imagery is essential for
improving both transformer-based models - that rely on large volumes of data -
and datasets, especially in underexplored domains like aerial scene
understanding where the potential impact is significant. This paper introduces
a novel methodology for scene complexity assessment using Multi-Model Consensus
Metric (MMCM) and depth-based structural metrics, enabling a robust evaluation
of perceptual and structural disparities between domains. Our experimental
analysis, utilizing real-world (Dronescapes) and synthetic (Skyscenes)
datasets, demonstrates that real-world scenes generally exhibit higher
consensus among state-of-the-art vision transformers, while synthetic scenes
show greater variability and challenge model adaptability. The results
underline the inherent complexities and domain gaps, emphasizing the need for
enhanced simulation fidelity and model generalization. This work provides
critical insights into the interplay between domain characteristics and model
performance, offering a pathway for improved domain adaptation strategies in
aerial scene understanding.",2024-11-29,Alina Marcu,http://arxiv.org/pdf/2411.19913v1,cs.LG
Another look at inference after prediction,"From structural biology to epidemiology, predictions from machine learning
(ML) models increasingly complement costly gold-standard data to enable faster,
more affordable, and scalable scientific inquiry. In response, prediction-based
(PB) inference has emerged to accommodate statistical analysis using a large
volume of predictions together with a small amount of gold-standard data. The
goals of PB inference are two-fold: (i) to mitigate bias from errors in
predictions and (ii) to improve efficiency relative to traditional inference
using only the gold-standard data. Motwani and Witten (2023) recently revisited
two key PB inference approaches and found that only one method,
Prediction-powered Inference (PPI) proposed by Angelopoulos et al. (2023),
achieves (i). In this paper, we find that PPI does not achieve (ii). We revisit
the double sampling literature and show that, with a simple modification, PPI
can be adjusted to provide theoretically justified improvements in efficiency.
We also contextualize PB inference with economics and statistics literature
dating back to the 1960s to highlight the utility of classical methods in this
contemporary problem. Our extensive theoretical analyses, along with an
analysis of UK Biobank data, indicate that our proposal effectively mitigates
bias and improves efficiency, making it preferable for use in practice.",2024-11-29,"Jessica Gronsbell, Jianhui Gao, Yaqi Shi, Zachary R. McCaw, David Cheng",http://arxiv.org/pdf/2411.19908v3,cs.LG
Classical and Quantum Algorithms for the Deterministic L-system Inductive Inference Problem,"L-systems can be made to model and create simulations of many biological
processes, such as plant development. Finding an L-system for a given process
is typically solved by hand, by experts, in a massively time-consuming process.
It would be significant if this could be done automatically from data, such as
from sequences of images. In this paper, we are interested in inferring a
particular type of L-system, deterministic context-free L-system (D0L-system)
from a sequence of strings. We introduce the characteristic graph of a sequence
of strings, which we then utilize to translate our problem (inferring
D0L-system) in polynomial time into the maximum independent set problem (MIS)
and the SAT problem. After that, we offer a classical exact algorithm and an
approximate quantum algorithm for the problem.",2024-11-29,"Ali Lotfi, Ian McQuillan, Steven Rayan",http://arxiv.org/pdf/2411.19906v2,cs.LG
Incremental Multi-Scene Modeling via Continual Neural Graphics Primitives,"Neural radiance fields (NeRF) have revolutionized photorealistic rendering of
novel views for 3D scenes. Despite their growing popularity and efficiency as
3D resources, NeRFs face scalability challenges due to the need for separate
models per scene and the cumulative increase in training time for multiple
scenes. The potential for incrementally encoding multiple 3D scenes into a
single NeRF model remains largely unexplored. To address this, we introduce
Continual-Neural Graphics Primitives (C-NGP), a novel continual learning
framework that integrates multiple scenes incrementally into a single neural
radiance field. Using a generative replay approach, C-NGP adapts to new scenes
without requiring access to old data. We demonstrate that C-NGP can accommodate
multiple scenes without increasing the parameter count, producing high-quality
novel-view renderings on synthetic and real datasets. Notably, C-NGP models all
$8$ scenes from the Real-LLFF dataset together, with only a $2.2\%$ drop in
PSNR compared to vanilla NeRF, which models each scene independently. Further,
C-NGP allows multiple style edits in the same network.",2024-11-29,"Prajwal Singh, Ashish Tiwari, Gautam Vashishtha, Shanmuganathan Raman",http://arxiv.org/pdf/2411.19903v3,cs.LG
Noncommutative Model Selection for Data Clustering and Dimension Reduction Using Relative von Neumann Entropy,"We propose a pair of completely data-driven algorithms for unsupervised
classification and dimension reduction, and we empirically study their
performance on a number of data sets, both simulated data in three-dimensions
and images from the COIL-20 data set. The algorithms take as input a set of
points sampled from a uniform distribution supported on a metric space, the
latter embedded in an ambient metric space, and they output a clustering or
reduction of dimension of the data. They work by constructing a natural family
of graphs from the data and selecting the graph which maximizes the relative
von Neumann entropy of certain normalized heat operators constructed from the
graphs. Once the appropriate graph is selected, the eigenvectors of the graph
Laplacian may be used to reduce the dimension of the data, and clusters in the
data may be identified with the kernel of the associated graph Laplacian.
Notably, these algorithms do not require information about the size of a
neighborhood or the desired number of clusters as input, in contrast to popular
algorithms such as $k$-means, and even more modern spectral methods such as
Laplacian eigenmaps, among others.
  In our computational experiments, our clustering algorithm outperforms
$k$-means clustering on data sets with non-trivial geometry and topology, in
particular data whose clusters are not concentrated around a specific point,
and our dimension reduction algorithm is shown to work well in several simple
examples.",2024-11-29,"Araceli Guzmán-Tristán, Antonio Rieser",http://arxiv.org/pdf/2411.19902v1,cs.LG
Efficient quantum-enhanced classical simulation for patches of quantum landscapes,"Understanding the capabilities of classical simulation methods is key to
identifying where quantum computers are advantageous. Not only does this ensure
that quantum computers are used only where necessary, but also one can
potentially identify subroutines that can be offloaded onto a classical device.
In this work, we show that it is always possible to generate a classical
surrogate of a sub-region (dubbed a ""patch"") of an expectation landscape
produced by a parameterized quantum circuit. That is, we provide a
quantum-enhanced classical algorithm which, after simple measurements on a
quantum device, allows one to classically simulate approximate expectation
values of a subregion of a landscape. We provide time and sample complexity
guarantees for a range of families of circuits of interest, and further
numerically demonstrate our simulation algorithms on an exactly verifiable
simulation of a Hamiltonian variational ansatz and long-time dynamics
simulation on a 127-qubit heavy-hex topology.",2024-11-29,"Sacha Lerch, Ricard Puig, Manuel S. Rudolph, Armando Angrisani, Tyson Jones, M. Cerezo, Supanut Thanasilp, Zoë Holmes",http://arxiv.org/pdf/2411.19896v1,cs.LG
Noncommutative Model Selection and the Data-Driven Estimation of Real Cohomology Groups,"We propose three completely data-driven methods for estimating the real
cohomology groups $H^k (X ; \mathbb{R})$ of a compact metric-measure space $(X,
d_X, \mu_X)$ embedded in a metric-measure space $(Y,d_Y,\mu_Y)$, given a finite
set of points $S$ sampled from a uniform distrbution $\mu_X$ on $X$, possibly
corrupted with noise from $Y$. We present the results of several computational
experiments in the case that $X$ is embedded in $\mathbb{R}^n$, where two of
the three algorithms performed well.",2024-11-29,"Araceli Guzmán-Tristán, Antonio Rieser, Eduardo Velázquez-Richards",http://arxiv.org/pdf/2411.19894v1,cs.LG
FlowCLAS: Enhancing Normalizing Flow Via Contrastive Learning For Anomaly Segmentation,"Anomaly segmentation is a valuable computer vision task for safety-critical
applications that need to be aware of unexpected events. Current
state-of-the-art (SOTA) scene-level anomaly segmentation approaches rely on
diverse inlier class labels during training, limiting their ability to leverage
vast unlabeled datasets and pre-trained vision encoders. These methods may
underperform in domains with reduced color diversity and limited object
classes. Conversely, existing unsupervised methods struggle with anomaly
segmentation with the diverse scenes of less restricted domains. To address
these challenges, we introduce FlowCLAS, a novel self-supervised framework that
utilizes vision foundation models to extract rich features and employs a
normalizing flow network to learn their density distribution. We enhance the
model's discriminative power by incorporating Outlier Exposure and contrastive
learning in the latent space. FlowCLAS significantly outperforms all existing
methods on the ALLO anomaly segmentation benchmark for space robotics and
demonstrates competitive results on multiple road anomaly segmentation
benchmarks for autonomous driving, including Fishyscapes Lost&Found and Road
Anomaly. These results highlight FlowCLAS's effectiveness in addressing the
unique challenges of space anomaly segmentation while retaining SOTA
performance in the autonomous driving domain without reliance on inlier
segmentation labels.",2024-11-29,"Chang Won Lee, Selina Leveugle, Svetlana Stolpner, Chris Langley, Paul Grouchy, Jonathan Kelly, Steven L. Waslander",http://arxiv.org/pdf/2411.19888v1,cs.LG
Open source Differentiable ODE Solving Infrastructure,"Ordinary Differential Equations (ODEs) are widely used in physics, chemistry,
and biology to model dynamic systems, including reaction kinetics, population
dynamics, and biological processes. In this work, we integrate GPU-accelerated
ODE solvers into the open-source DeepChem framework, making these tools easily
accessible. These solvers support multiple numerical methods and are fully
differentiable, enabling easy integration into more complex differentiable
programs. We demonstrate the capabilities of our implementation through
experiments on Lotka-Volterra predator-prey dynamics, pharmacokinetic
compartment models, neural ODEs, and solving PDEs using reaction-diffusion
equations. Our solvers achieved high accuracy with mean squared errors ranging
from $10^{-4}$ to $10^{-6}$ and showed scalability in solving large systems
with up to 100 compartments.",2024-11-29,"Rakshit Kr. Singh, Aaron Rock Menezes, Rida Irfan, Bharath Ramsundar",http://arxiv.org/pdf/2411.19882v1,cs.LG
Spatial Clustering of Molecular Localizations with Graph Neural Networks,"Single-molecule localization microscopy generates point clouds corresponding
to fluorophore localizations. Spatial cluster identification and analysis of
these point clouds are crucial for extracting insights about molecular
organization. However, this task becomes challenging in the presence of
localization noise, high point density, or complex biological structures. Here,
we introduce MIRO (Multimodal Integration through Relational Optimization), an
algorithm that uses recurrent graph neural networks to transform the point
clouds in order to improve clustering efficiency when applying conventional
clustering techniques. We show that MIRO supports simultaneous processing of
clusters of different shapes and at multiple scales, demonstrating improved
performance across varied datasets. Our comprehensive evaluation demonstrates
MIRO's transformative potential for single-molecule localization applications,
showcasing its capability to revolutionize cluster analysis and provide
accurate, reliable details of molecular architecture. In addition, MIRO's
robust clustering capabilities hold promise for applications in various fields
such as neuroscience, for the analysis of neural connectivity patterns, and
environmental science, for studying spatial distributions of ecological data.",2024-11-29,"Jesús Pineda, Sergi Masó-Orriols, Joan Bertran, Mattias Goksör, Giovanni Volpe, Carlo Manzo",http://arxiv.org/pdf/2412.00173v1,cs.LG
Enhanced anomaly detection in well log data through the application of ensemble GANs,"Although generative adversarial networks (GANs) have shown significant
success in modeling data distributions for image datasets, their application to
structured or tabular data, such as well logs, remains relatively
underexplored. This study extends the ensemble GANs (EGANs) framework to
capture the distribution of well log data and detect anomalies that fall
outside of these distributions. The proposed approach compares the performance
of traditional methods, such as Gaussian mixture models (GMMs), with EGANs in
detecting anomalies outside the expected data distributions. For the gamma ray
(GR) dataset, EGANs achieved a precision of 0.62 and F1 score of 0.76,
outperforming GMM's precision of 0.38 and F1 score of 0.54. Similarly, for
travel time (DT), EGANs achieved a precision of 0.70 and F1 score of 0.79,
surpassing GMM 0.56 and 0.71. In the neutron porosity (NPHI) dataset, EGANs
recorded a precision of 0.53 and F1 score of 0.68, outshining GMM 0.47 and
0.61. For the bulk density (RHOB) dataset, EGANs achieved a precision of 0.52
and an F1 score of 0.67, slightly outperforming GMM, which yielded a precision
of 0.50 and an F1 score of 0.65. This work's novelty lies in applying EGANs for
well log data analysis, showcasing their ability to learn data patterns and
identify anomalies that deviate from them. This approach offers more reliable
anomaly detection compared to traditional methods like GMM. The findings
highlight the potential of EGANs in enhancing anomaly detection for well log
data, delivering significant implications for optimizing drilling strategies
and reservoir management through more accurate, data-driven insights into
subsurface characterization.",2024-11-29,"Abdulrahman Al-Fakih, A. Koeshidayatullah, Tapan Mukerji, SanLinn I. Kaka",http://arxiv.org/pdf/2411.19875v1,cs.LG
DeMo: Decoupled Momentum Optimization,"Training large neural networks typically requires sharing gradients between
accelerators through specialized high-speed interconnects. Drawing from the
signal processing principles of frequency decomposition and energy compaction,
we demonstrate that synchronizing full optimizer states and model parameters
during training is unnecessary. By decoupling momentum updates and allowing
controlled divergence in optimizer states across accelerators, we achieve
improved convergence compared to state-of-the-art optimizers. We introduce
{\textbf{De}}coupled {\textbf{Mo}}mentum (DeMo), a fused optimizer and data
parallel algorithm that reduces inter-accelerator communication requirements by
several orders of magnitude. This enables training of large neural networks
even with limited network bandwidth and heterogeneous hardware. Our method is
topology-agnostic and architecture-independent and supports scalable
clock-synchronous distributed training with negligible compute and memory
overhead. Empirical results show that models trained with DeMo match or exceed
the performance of equivalent models trained with AdamW, while eliminating the
need for high-speed interconnects when pre-training large scale foundation
models. An open source reference PyTorch implementation is published on GitHub
at https://github.com/bloc97/DeMo",2024-11-29,"Bowen Peng, Jeffrey Quesnelle, Diederik P. Kingma",http://arxiv.org/pdf/2411.19870v1,cs.LG
AIDetx: a compression-based method for identification of machine-learning generated text,"This paper introduces AIDetx, a novel method for detecting machine-generated
text using data compression techniques. Traditional approaches, such as deep
learning classifiers, often suffer from high computational costs and limited
interpretability. To address these limitations, we propose a compression-based
classification framework that leverages finite-context models (FCMs). AIDetx
constructs distinct compression models for human-written and AI-generated text,
classifying new inputs based on which model achieves a higher compression
ratio. We evaluated AIDetx on two benchmark datasets, achieving F1 scores
exceeding 97% and 99%, respectively, highlighting its high accuracy. Compared
to current methods, such as large language models (LLMs), AIDetx offers a more
interpretable and computationally efficient solution, significantly reducing
both training time and hardware requirements (e.g., no GPUs needed). The full
implementation is publicly available at https://github.com/AIDetx/AIDetx.",2024-11-29,"Leonardo Almeida, Pedro Rodrigues, Diogo Magalhães, Armando J. Pinho, Diogo Pratas",http://arxiv.org/pdf/2411.19869v1,cs.LG
Reverse Thinking Makes LLMs Stronger Reasoners,"Reverse thinking plays a crucial role in human reasoning. Humans can reason
not only from a problem to a solution but also in reverse, i.e., start from the
solution and reason towards the problem. This often enhances overall reasoning
performance as it enables consistency checks between their forward and backward
thinking. To enable Large Language Models (LLMs) to perform reverse thinking,
we introduce Reverse-Enhanced Thinking (RevThink), a framework composed of data
augmentation and learning objectives. In RevThink, we augment the dataset by
collecting structured forward-backward reasoning from a teacher model,
consisting of: (1) the original question, (2) forward reasoning, (3) backward
question, and (4) backward reasoning. We then employ three objectives to train
a smaller student model in a multi-task learning fashion: (a) generate forward
reasoning from a question, (b) generate a backward question from a question,
and (c) generate backward reasoning from the backward question. Experiments
across 12 datasets covering commonsense, math, and logical reasoning show an
average 13.53% improvement over the student model's zero-shot performance and a
6.84% improvement over the strongest knowledge distillation baselines.
Moreover, our method demonstrates sample efficiency -- using only 10% of the
correct forward reasoning from the training data, it outperforms a standard
fine-tuning method trained on 10x more forward reasoning. RevThink also
exhibits strong generalization to out-of-distribution held-out datasets.",2024-11-29,"Justin Chih-Yao Chen, Zifeng Wang, Hamid Palangi, Rujun Han, Sayna Ebrahimi, Long Le, Vincent Perot, Swaroop Mishra, Mohit Bansal, Chen-Yu Lee, Tomas Pfister",http://arxiv.org/pdf/2411.19865v2,cs.LG
SpaRC: Sparse Radar-Camera Fusion for 3D Object Detection,"In this work, we present SpaRC, a novel Sparse fusion transformer for 3D
perception that integrates multi-view image semantics with Radar and Camera
point features. The fusion of radar and camera modalities has emerged as an
efficient perception paradigm for autonomous driving systems. While
conventional approaches utilize dense Bird's Eye View (BEV)-based architectures
for depth estimation, contemporary query-based transformers excel in
camera-only detection through object-centric methodology. However, these
query-based approaches exhibit limitations in false positive detections and
localization precision due to implicit depth modeling. We address these
challenges through three key contributions: (1) sparse frustum fusion (SFF) for
cross-modal feature alignment, (2) range-adaptive radar aggregation (RAR) for
precise object localization, and (3) local self-attention (LSA) for focused
query aggregation. In contrast to existing methods requiring computationally
intensive BEV-grid rendering, SpaRC operates directly on encoded point
features, yielding substantial improvements in efficiency and accuracy.
Empirical evaluations on the nuScenes and TruckScenes benchmarks demonstrate
that SpaRC significantly outperforms existing dense BEV-based and sparse
query-based detectors. Our method achieves state-of-the-art performance metrics
of 67.1 NDS and 63.1 AMOTA. The code and pretrained models are available at
https://github.com/phi-wol/sparc.",2024-11-29,"Philipp Wolters, Johannes Gilg, Torben Teepe, Fabian Herzog, Felix Fent, Gerhard Rigoll",http://arxiv.org/pdf/2411.19860v1,cs.LG
Towards Class-wise Robustness Analysis,"While being very successful in solving many downstream tasks, the application
of deep neural networks is limited in real-life scenarios because of their
susceptibility to domain shifts such as common corruptions, and adversarial
attacks. The existence of adversarial examples and data corruption
significantly reduces the performance of deep classification models.
Researchers have made strides in developing robust neural architectures to
bolster decisions of deep classifiers. However, most of these works rely on
effective adversarial training methods, and predominantly focus on overall
model robustness, disregarding class-wise differences in robustness, which are
critical. Exploiting weakly robust classes is a potential avenue for attackers
to fool the image recognition models. Therefore, this study investigates
class-to-class biases across adversarially trained robust classification models
to understand their latent space structures and analyze their strong and weak
class-wise properties. We further assess the robustness of classes against
common corruptions and adversarial attacks, recognizing that class
vulnerability extends beyond the number of correct classifications for a
specific class. We find that the number of false positives of classes as
specific target classes significantly impacts their vulnerability to attacks.
Through our analysis on the Class False Positive Score, we assess a fair
evaluation of how susceptible each class is to misclassification.",2024-11-29,"Tejaswini Medi, Julia Grabinski, Margret Keuper",http://arxiv.org/pdf/2411.19853v2,cs.LG
A Visual-inertial Localization Algorithm using Opportunistic Visual Beacons and Dead-Reckoning for GNSS-Denied Large-scale Applications,"With the development of smart cities, the demand for continuous pedestrian
navigation in large-scale urban environments has significantly increased. While
global navigation satellite systems (GNSS) provide low-cost and reliable
positioning services, they are often hindered in complex urban canyon
environments. Thus, exploring opportunistic signals for positioning in urban
areas has become a key solution. Augmented reality (AR) allows pedestrians to
acquire real-time visual information. Accordingly, we propose a low-cost
visual-inertial positioning solution. This method comprises a lightweight
multi-scale group convolution (MSGC)-based visual place recognition (VPR)
neural network, a pedestrian dead reckoning (PDR) algorithm, and a
visual/inertial fusion approach based on a Kalman filter with gross error
suppression. The VPR serves as a conditional observation to the Kalman filter,
effectively correcting the errors accumulated through the PDR method. This
enables the entire algorithm to ensure the reliability of long-term positioning
in GNSS-denied areas. Extensive experimental results demonstrate that our
method maintains stable positioning during large-scale movements. Compared to
the lightweight MobileNetV3-based VPR method, our proposed VPR solution
improves Recall@1 by at least 3\% on two public datasets while reducing the
number of parameters by 63.37\%. It also achieves performance that is
comparable to the VGG16-based method. The VPR-PDR algorithm improves
localization accuracy by more than 40\% compared to the original PDR.",2024-11-29,"Liqiang Zhang, Ye Tian, Dongyan Wei",http://arxiv.org/pdf/2411.19845v2,cs.LG
Scaling Transformers for Low-Bitrate High-Quality Speech Coding,"The tokenization of speech with neural audio codec models is a vital part of
modern AI pipelines for the generation or understanding of speech, alone or in
a multimodal context. Traditionally such tokenization models have concentrated
on low parameter-count architectures using only components with strong
inductive biases. In this work we show that by scaling a transformer
architecture with large parameter count to this problem, and applying a
flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to
reach state-of-the-art speech quality at extremely low bit-rates of $400$ or
$700$ bits-per-second. The trained models strongly out-perform existing
baselines in both objective and subjective tests.",2024-11-29,"Julian D Parker, Anton Smirnov, Jordi Pons, CJ Carr, Zack Zukowski, Zach Evans, Xubo Liu",http://arxiv.org/pdf/2411.19842v1,cs.LG
Feedback-driven object detection and iterative model improvement,"Automated object detection has become increasingly valuable across diverse
applications, yet efficient, high-quality annotation remains a persistent
challenge. In this paper, we present the development and evaluation of a
platform designed to interactively improve object detection models. The
platform allows uploading and annotating images as well as fine-tuning object
detection models. Users can then manually review and refine annotations,
further creating improved snapshots that are used for automatic object
detection on subsequent image uploads - a process we refer to as semi-automatic
annotation resulting in a significant gain in annotation efficiency.
  Whereas iterative refinement of model results to speed up annotation has
become common practice, we are the first to quantitatively evaluate its
benefits with respect to time, effort, and interaction savings. Our
experimental results show clear evidence for a significant time reduction of up
to 53% for semi-automatic compared to manual annotation. Importantly, these
efficiency gains did not compromise annotation quality, while matching or
occasionally even exceeding the accuracy of manual annotations. These findings
demonstrate the potential of our lightweight annotation platform for creating
high-quality object detection datasets and provide best practices to guide
future development of annotation platforms.
  The platform is open-source, with the frontend and backend repositories
available on GitHub. To support the understanding of our labeling process, we
have created an explanatory video demonstrating the methodology using
microscopy images of E. coli bacteria as an example.",2024-11-29,"Sönke Tenckhoff, Mario Koddenbrock, Erik Rodner",http://arxiv.org/pdf/2411.19835v3,cs.LG
GradAlign for Training-free Model Performance Inference,"Architecture plays an important role in deciding the performance of deep
neural networks. However, the search for the optimal architecture is often
hindered by the vast search space, making it a time-intensive process.
Recently, a novel approach known as training-free neural architecture search
(NAS) has emerged, aiming to discover the ideal architecture without
necessitating extensive training. Training-free NAS leverages various
indicators for architecture selection, including metrics such as the count of
linear regions, the density of per-sample losses, and the stability of the
finite-width Neural Tangent Kernel (NTK) matrix. Despite the competitive
empirical performance of current training-free NAS techniques, they suffer from
certain limitations, including inconsistent performance and a lack of deep
understanding. In this paper, we introduce GradAlign, a simple yet effective
method designed for inferring model performance without the need for training.
At its core, GradAlign quantifies the extent of conflicts within per-sample
gradients during initialization, as substantial conflicts hinder model
convergence and ultimately result in worse performance. We evaluate GradAlign
against established training-free NAS methods using standard NAS benchmarks,
showing a better overall performance. Moreover, we show that the widely adopted
metric of linear region count may not suffice as a dependable criterion for
selecting network architectures during at initialization.",2024-11-29,"Yuxuan Li, Yunhui Guo",http://arxiv.org/pdf/2411.19819v1,cs.LG
Rethinking the initialization of Momentum in Federated Learning with Heterogeneous Data,"Data Heterogeneity is a major challenge of Federated Learning performance.
Recently, momentum based optimization techniques have beed proved to be
effective in mitigating the heterogeneity issue. Along with the model updates,
the momentum updates are transmitted to the server side and aggregated.
Therefore, the local training initialized with a global momentum is guided by
the global history of the gradients. However, we spot a problem in the
traditional cumulation of the momentum which is suboptimal in the Federated
Learning systems. The momentum used to weight less on the historical gradients
and more on the recent gradients. This however, will engage more biased local
gradients in the end of the local training. In this work, we propose a new way
to calculate the estimated momentum used in local initialization. The proposed
method is named as Reversed Momentum Federated Learning (RMFL). The key idea is
to assign exponentially decayed weights to the gradients with the time going
forward, which is on the contrary to the traditional momentum cumulation. The
effectiveness of RMFL is evaluated on three popular benchmark datasets with
different heterogeneity levels.",2024-11-29,"Chenguang Xiao, Shuo Wang",http://arxiv.org/pdf/2411.19798v1,cs.LG
Tractable Agreement Protocols,"We present an efficient reduction that converts any machine learning
algorithm into an interactive protocol, enabling collaboration with another
party (e.g., a human) to achieve consensus on predictions and improve accuracy.
This approach imposes calibration conditions on each party, which are
computationally and statistically tractable relaxations of Bayesian
rationality. These conditions are sensible even in prior-free settings,
representing a significant generalization of Aumann's classic ""agreement
theorem.""
  In our protocol, the model first provides a prediction. The human then
responds by either agreeing or offering feedback. The model updates its state
and revises its prediction, while the human may adjust their beliefs. This
iterative process continues until the two parties reach agreement. Initially,
we study a setting that extends Aumann's Agreement Theorem, where parties aim
to agree on a one-dimensional expectation by iteratively sharing their current
estimates. Here, we recover the convergence theorem of Aaronson'05 under weaker
assumptions. We then address the case where parties hold beliefs over
distributions with d outcomes, exploring two feedback mechanisms. The first
involves vector-valued estimates of predictions, while the second adopts a
decision-theoretic approach: the human, needing to take an action from a finite
set based on utility, communicates their utility-maximizing action at each
round. In this setup, the number of rounds until agreement remains independent
of d. Finally, we generalize to scenarios with more than two parties, where
computational complexity scales linearly with the number of participants. Our
protocols rely on simple, efficient conditions and produce predictions that
surpass the accuracy of any individual party's alone.",2024-11-29,"Natalie Collina, Surbhi Goel, Varun Gupta, Aaron Roth",http://arxiv.org/pdf/2411.19791v1,cs.LG
CAREL: Instruction-guided reinforcement learning with cross-modal auxiliary objectives,"Grounding the instruction in the environment is a key step in solving
language-guided goal-reaching reinforcement learning problems. In automated
reinforcement learning, a key concern is to enhance the model's ability to
generalize across various tasks and environments. In goal-reaching scenarios,
the agent must comprehend the different parts of the instructions within the
environmental context in order to complete the overall task successfully. In
this work, we propose CAREL (Cross-modal Auxiliary REinforcement Learning) as a
new framework to solve this problem using auxiliary loss functions inspired by
video-text retrieval literature and a novel method called instruction tracking,
which automatically keeps track of progress in an environment. The results of
our experiments suggest superior sample efficiency and systematic
generalization for this framework in multi-modal reinforcement learning
problems. Our code base is available here.",2024-11-29,"Armin Saghafian, Amirmohammad Izadi, Negin Hashemi Dijujin, Mahdieh Soleymani Baghshah",http://arxiv.org/pdf/2411.19787v1,cs.LG
MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks,"Recently, human motion analysis has experienced great improvement due to
inspiring generative models such as the denoising diffusion model and large
language model. While the existing approaches mainly focus on generating
motions with textual descriptions and overlook the reciprocal task. In this
paper, we present~\textbf{MoTe}, a unified multi-modal model that could handle
diverse tasks by learning the marginal, conditional, and joint distributions of
motion and text simultaneously. MoTe enables us to handle the paired
text-motion generation, motion captioning, and text-driven motion generation by
simply modifying the input context. Specifically, MoTe is composed of three
components: Motion Encoder-Decoder (MED), Text Encoder-Decoder (TED), and
Moti-on-Text Diffusion Model (MTDM). In particular, MED and TED are trained for
extracting latent embeddings, and subsequently reconstructing the motion
sequences and textual descriptions from the extracted embeddings, respectively.
MTDM, on the other hand, performs an iterative denoising process on the input
context to handle diverse tasks. Experimental results on the benchmark datasets
demonstrate the superior performance of our proposed method on text-to-motion
generation and competitive performance on motion captioning.",2024-11-29,"Yiming Wu, Wei Ji, Kecheng Zheng, Zicheng Wang, Dong Xu",http://arxiv.org/pdf/2411.19786v1,cs.LG
Machine learning force-field model for kinetic Monte Carlo simulations of itinerant Ising magnets,"We present a scalable machine learning (ML) framework for large-scale kinetic
Monte Carlo (kMC) simulations of itinerant electron Ising systems. As the
effective interactions between Ising spins in such itinerant magnets are
mediated by conducting electrons, the calculation of energy change due to a
local spin update requires solving an electronic structure problem. Such
repeated electronic structure calculations could be overwhelmingly prohibitive
for large systems. Assuming the locality principle, a convolutional neural
network (CNN) model is developed to directly predict the effective local field
and the corresponding energy change associated with a given spin update based
on Ising configuration in a finite neighborhood. As the kernel size of the CNN
is fixed at a constant, the model can be directly scalable to kMC simulations
of large lattices. Our approach is reminiscent of the ML force-field models
widely used in first-principles molecular dynamics simulations. Applying our ML
framework to a square-lattice double-exchange Ising model, we uncover unusual
coarsening of ferromagnetic domains at low temperatures. Our work highlights
the potential of ML methods for large-scale modeling of similar itinerant
systems with discrete dynamical variables.",2024-11-29,"Alexa Tyberg, Yunhao Fan, Gia-Wei Chern",http://arxiv.org/pdf/2411.19780v1,cs.LG
Origin-Destination Demand Prediction: An Urban Radiation and Attraction Perspective,"In recent years, origin-destination (OD) demand prediction has gained
significant attention for its profound implications in urban development.
Existing data-driven deep learning methods primarily focus on the spatial or
temporal dependency between regions yet neglecting regions' fundamental
functional difference. Though knowledge-driven physical methods have
characterised regions' functions by their radiation and attraction capacities,
these functions are defined on numerical factors like population without
considering regions' intrinsic nominal attributes, e.g., a region is a
residential or industrial district. Moreover, the complicated relationships
between two types of capacities, e.g., the radiation capacity of a residential
district in the morning will be transformed into the attraction capacity in the
evening, are totally missing from physical methods.
  In this paper, we not only generalize the physical radiation and attraction
capacities into the deep learning framework with the extended capability to
fulfil regions' functions, but also present a new model that captures the
relationships between two types of capacities. Specifically, we first model
regions' radiation and attraction capacities using a bilateral branch network,
each equipped with regions' attribute representations. We then describe the
transformation relationship of different capacities of the same region using a
hypergraph-based parameter generation method. We finally unveil the competition
relationship of different regions with the same attraction capacity through
cluster-based adversarial learning. Extensive experiments on two datasets
demonstrate the consistent improvements of our method over the state-of-the-art
baselines, as well as the good explainability of regions' functions using their
nominal attributes.",2024-11-29,"Xuan Ma, Zepeng Bao, Ming Zhong, Yuanyuan Zhu, Chenliang Li, Jiawei Jiang, Qing Li, Tieyun Qian",http://arxiv.org/pdf/2412.00167v1,cs.LG
PerLA: Perceptive 3D Language Assistant,"Enabling Large Language Models (LLMs) to understand the 3D physical world is
an emerging yet challenging research direction. Current strategies for
processing point clouds typically downsample the scene or divide it into
smaller parts for separate analysis. However, both approaches risk losing key
local details or global contextual information. In this paper, we introduce
PerLA, a 3D language assistant designed to be more perceptive to both details
and context, making visual representations more informative for the LLM. PerLA
captures high-resolution (local) details in parallel from different point cloud
areas and integrates them with (global) context obtained from a
lower-resolution whole point cloud. We present a novel algorithm that preserves
point cloud locality through the Hilbert curve and effectively aggregates
local-to-global information via cross-attention and a graph neural network.
Lastly, we introduce a novel loss for local representation consensus to promote
training stability. PerLA outperforms state-of-the-art 3D language assistants,
with gains of up to +1.34 CiDEr on ScanQA for question answering, and +4.22 on
ScanRefer and +3.88 on Nr3D for dense captioning.
https://gfmei.github.io/PerLA/",2024-11-29,"Guofeng Mei, Wei Lin, Luigi Riz, Yujiao Wu, Fabio Poiesi, Yiming Wang",http://arxiv.org/pdf/2411.19774v2,cs.LG
LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos,"Despite impressive advancements in video understanding, most efforts remain
limited to coarse-grained or visual-only video tasks. However, real-world
videos encompass omni-modal information (vision, audio, and speech) with a
series of events forming a cohesive storyline. The lack of multi-modal video
data with fine-grained event annotations and the high cost of manual labeling
are major obstacles to comprehensive omni-modality video perception. To address
this gap, we propose an automatic pipeline consisting of high-quality
multi-modal video filtering, semantically coherent omni-modal event boundary
detection, and cross-modal correlation-aware event captioning. In this way, we
present LongVALE, the first-ever Vision-Audio-Language Event understanding
benchmark comprising 105K omni-modal events with precise temporal boundaries
and detailed relation-aware captions within 8.4K high-quality long videos.
Further, we build a baseline that leverages LongVALE to enable video large
language models (LLMs) for omni-modality fine-grained temporal video
understanding for the first time. Extensive experiments demonstrate the
effectiveness and great potential of LongVALE in advancing comprehensive
multi-modal video understanding.",2024-11-29,"Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, Feng Zheng",http://arxiv.org/pdf/2411.19772v3,cs.LG
Riemannian Denoising Score Matching for Molecular Structure Optimization with Accurate Energy,"This study introduces a modified score matching method aimed at generating
molecular structures with high energy accuracy. The denoising process of score
matching or diffusion models mirrors molecular structure optimization, where
scores act like physical force fields that guide particles toward equilibrium
states. To achieve energetically accurate structures, it can be advantageous to
have the score closely approximate the gradient of the actual potential energy
surface. Unlike conventional methods that simply design the target score based
on structural differences in Euclidean space, we propose a Riemannian score
matching approach. This method represents molecular structures on a manifold
defined by physics-informed internal coordinates to efficiently mimic the
energy landscape, and performs noising and denoising within this space. Our
method has been evaluated by refining several types of starting structures on
the QM9 and GEOM datasets, demonstrating that the proposed Riemannian score
matching method significantly improves the accuracy of the generated molecular
structures, attaining chemical accuracy. The implications of this study extend
to various applications in computational chemistry, offering a robust tool for
accurate molecular structure prediction.",2024-11-29,"Jeheon Woo, Seonghwan Kim, Jun Hyeong Kim, Woo Youn Kim",http://arxiv.org/pdf/2411.19769v1,cs.LG
Stock Price Prediction using Multi-Faceted Information based on Deep Recurrent Neural Networks,"Accurate prediction of stock market trends is crucial for informed investment
decisions and effective portfolio management, ultimately leading to enhanced
wealth creation and risk mitigation. This study proposes a novel approach for
predicting stock prices in the stock market by integrating Convolutional Neural
Networks (CNN) and Long Short-Term Memory (LSTM) networks, using sentiment
analysis of social network data and candlestick data (price). The proposed
methodology consists of two primary components: sentiment analysis of social
network and candlestick data. By amalgamating candlestick data with insights
gleaned from Twitter, this approach facilitates a more detailed and accurate
examination of market trends and patterns, ultimately leading to more effective
stock price predictions. Additionally, a Random Forest algorithm is used to
classify tweets as either positive or negative, allowing for a more subtle and
informed assessment of market sentiment. This study uses CNN and LSTM networks
to predict stock prices. The CNN extracts short-term features, while the LSTM
models long-term dependencies. The integration of both networks enables a more
comprehensive analysis of market trends and patterns, leading to more accurate
stock price predictions.",2024-11-29,"Lida Shahbandari, Elahe Moradi, Mohammad Manthouri",http://arxiv.org/pdf/2411.19766v1,cs.LG
Forecasting Foreign Exchange Market Prices Using Technical Indicators with Deep Learning and Attention Mechanism,"Accurate prediction of price behavior in the foreign exchange market is
crucial. This paper proposes a novel approach that leverages technical
indicators and deep neural networks. The proposed architecture consists of a
Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN), and
attention mechanism. Initially, trend and oscillation technical indicators are
employed to extract statistical features from Forex currency pair data,
providing insights into price trends, market volatility, relative price
strength, and overbought and oversold conditions. Subsequently, the LSTM and
CNN networks are utilized in parallel to predict future price movements,
leveraging the strengths of both recurrent and convolutional architectures. The
LSTM network captures long-term dependencies and temporal patterns in the data,
while the CNN network extracts local patterns. The outputs of the parallel LSTM
and CNN networks are then fed into an attention mechanism, which learns to
weigh the importance of each feature and temporal dependency, generating a
context-aware representation of the input data. The attention-weighted output
is then used to predict future price movements, enabling the model to focus on
the most relevant features and temporal dependencies. Through a comprehensive
evaluation of the proposed approach on multiple Forex currency pairs, we
demonstrate its effectiveness in predicting price behavior and outperforming
benchmark models.",2024-11-29,"Sahabeh Saadati, Mohammad Manthouri",http://arxiv.org/pdf/2411.19763v1,cs.LG
LaVIDE: A Language-Vision Discriminator for Detecting Changes in Satellite Image with Map References,"Change detection, which typically relies on the comparison of bi-temporal
images, is significantly hindered when only a single image is available.
Comparing a single image with an existing map, such as OpenStreetMap, which is
continuously updated through crowd-sourcing, offers a viable solution to this
challenge. Unlike images that carry low-level visual details of ground objects,
maps convey high-level categorical information. This discrepancy in abstraction
levels complicates the alignment and comparison of the two data types. In this
paper, we propose a \textbf{La}nguage-\textbf{VI}sion \textbf{D}iscriminator
for d\textbf{E}tecting changes in satellite image with map references, namely
\ours{}, which leverages language to bridge the information gap between maps
and images. Specifically, \ours{} formulates change detection as the problem of
``{\textit Does the pixel belong to [class]?}'', aligning maps and images
within the feature space of the language-vision model to associate high-level
map categories with low-level image details. Moreover, we build a
mixture-of-experts discriminative module, which compares linguistic features
from maps with visual features from images across various semantic
perspectives, achieving comprehensive semantic comparison for change detection.
Extensive evaluation on four benchmark datasets demonstrates that \ours{} can
effectively detect changes in satellite image with map references,
outperforming state-of-the-art change detection algorithms, e.g., with gains of
about $13.8$\% on the DynamicEarthNet dataset and $4.3$\% on the SECOND
dataset.",2024-11-29,"Shuguo Jiang, Fang Xu, Sen Jia, Gui-Song Xia",http://arxiv.org/pdf/2411.19758v1,cs.LG
Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning Zero-Shot Models,"Fine-tuning foundation models often compromises their robustness to
distribution shifts. To remedy this, most robust fine-tuning methods aim to
preserve the pre-trained features. However, not all pre-trained features are
robust and those methods are largely indifferent to which ones to preserve. We
propose dual risk minimization (DRM), which combines empirical risk
minimization with worst-case risk minimization, to better preserve the core
features of downstream tasks. In particular, we utilize core-feature
descriptions generated by LLMs to induce core-based zero-shot predictions which
then serve as proxies to estimate the worst-case risk. DRM balances two crucial
aspects of model robustness: expected performance and worst-case performance,
establishing a new state of the art on various real-world benchmarks. DRM
significantly improves the out-of-distribution performance of CLIP ViT-L/14@336
on ImageNet (75.9 to 77.1), WILDS-iWildCam (47.1 to 51.8), and WILDS-FMoW (50.7
to 53.1); opening up new avenues for robust fine-tuning. Our code is available
at https://github.com/vaynexie/DRM .",2024-11-29,"Kaican Li, Weiyan Xie, Yongxiang Huang, Didan Deng, Lanqing Hong, Zhenguo Li, Ricardo Silva, Nevin L. Zhang",http://arxiv.org/pdf/2411.19757v1,cs.LG
DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering,"Gaussian splatting enables fast novel view synthesis in static 3D
environments. However, reconstructing real-world environments remains
challenging as distractors or occluders break the multi-view consistency
assumption required for accurate 3D reconstruction. Most existing methods rely
on external semantic information from pre-trained models, introducing
additional computational overhead as pre-processing steps or during
optimization. In this work, we propose a novel method, DeSplat, that directly
separates distractors and static scene elements purely based on volume
rendering of Gaussian primitives. We initialize Gaussians within each camera
view for reconstructing the view-specific distractors to separately model the
static 3D scene and distractors in the alpha compositing stages. DeSplat yields
an explicit scene separation of static elements and distractors, achieving
comparable results to prior distractor-free approaches without sacrificing
rendering speed. We demonstrate DeSplat's effectiveness on three benchmark data
sets for distractor-free novel view synthesis. See the project website at
https://aaltoml.github.io/desplat/.",2024-11-29,"Yihao Wang, Marcus Klasson, Matias Turkulainen, Shuzhe Wang, Juho Kannala, Arno Solin",http://arxiv.org/pdf/2411.19756v2,cs.LG
"A Multi-Loss Strategy for Vehicle Trajectory Prediction: Combining Off-Road, Diversity, and Directional Consistency Losses","Trajectory prediction is essential for the safety and efficiency of planning
in autonomous vehicles. However, current models often fail to fully capture
complex traffic rules and the complete range of potential vehicle movements.
Addressing these limitations, this study introduces three novel loss functions:
Offroad Loss, Direction Consistency Error, and Diversity Loss. These functions
are designed to keep predicted paths within driving area boundaries, aligned
with traffic directions, and cover a wider variety of plausible driving
scenarios. As all prediction modes should adhere to road rules and conditions,
this work overcomes the shortcomings of traditional ""winner takes all"" training
methods by applying the loss functions to all prediction modes. These loss
functions not only improve model training but can also serve as metrics for
evaluating the realism and diversity of trajectory predictions. Extensive
validation on the nuScenes and Argoverse 2 datasets with leading baseline
models demonstrates that our approach not only maintains accuracy but
significantly improves safety and robustness, reducing offroad errors on
average by 47% on original and by 37% on attacked scenes. This work sets a new
benchmark for trajectory prediction in autonomous driving, offering substantial
improvements in navigating complex environments. Our code is available at
https://github.com/vita-epfl/stay-on-track .",2024-11-29,"Ahmad Rahimi, Alexandre Alahi",http://arxiv.org/pdf/2411.19747v1,cs.LG
HVAC-DPT: A Decision Pretrained Transformer for HVAC Control,"Building operations consume approximately 40% of global energy, with Heating,
Ventilation, and Air Conditioning (HVAC) systems responsible for up to 50% of
this consumption. As HVAC energy demands are expected to rise, optimising
system efficiency is crucial for reducing future energy use and mitigating
climate change. Existing control strategies lack generalisation and require
extensive training and data, limiting their rapid deployment across diverse
buildings. This paper introduces HVAC-DPT, a Decision-Pretrained Transformer
using in-context Reinforcement Learning (RL) for multi-zone HVAC control.
HVAC-DPT frames HVAC control as a sequential prediction task, training a causal
transformer on interaction histories generated by diverse RL agents. This
approach enables HVAC-DPT to refine its policy in-context, without modifying
network parameters, allowing for deployment across different buildings without
the need for additional training or data collection. HVAC-DPT reduces energy
consumption in unseen buildings by 45% compared to the baseline controller,
offering a scalable and effective approach to mitigating the increasing
environmental impact of HVAC systems.",2024-11-29,Anaïs Berkes,http://arxiv.org/pdf/2411.19746v1,cs.LG
Amplifying human performance in combinatorial competitive programming,"Recent years have seen a significant surge in complex AI systems for
competitive programming, capable of performing at admirable levels against
human competitors. While steady progress has been made, the highest percentiles
still remain out of reach for these methods on standard competition platforms
such as Codeforces. Here we instead focus on combinatorial competitive
programming, where the target is to find as-good-as-possible solutions to
otherwise computationally intractable problems, over specific given inputs. We
hypothesise that this scenario offers a unique testbed for human-AI synergy, as
human programmers can write a backbone of a heuristic solution, after which AI
can be used to optimise the scoring function used by the heuristic. We deploy
our approach on previous iterations of Hash Code, a global team programming
competition inspired by NP-hard software engineering problems at Google, and we
leverage FunSearch to evolve our scoring functions. Our evolved solutions
significantly improve the attained scores from their baseline, successfully
breaking into the top percentile on all previous Hash Code online qualification
rounds, and outperforming the top human teams on several. Our method is also
performant on an optimisation problem that featured in a recent held-out
AtCoder contest.",2024-11-29,"Petar Veličković, Alex Vitvitskyi, Larisa Markeeva, Borja Ibarz, Lars Buesing, Matej Balog, Alexander Novikov",http://arxiv.org/pdf/2411.19744v1,cs.LG
Graph Neural Networks for Heart Failure Prediction on an EHR-Based Patient Similarity Graph,"Objective: In modern healthcare, accurately predicting diseases is a crucial
matter. This study introduces a novel approach using graph neural networks
(GNNs) and a Graph Transformer (GT) to predict the incidence of heart failure
(HF) on a patient similarity graph at the next hospital visit. Materials and
Methods: We used electronic health records (EHR) from the MIMIC-III dataset and
applied the K-Nearest Neighbors (KNN) algorithm to create a patient similarity
graph using embeddings from diagnoses, procedures, and medications. Three
models - GraphSAGE, Graph Attention Network (GAT), and Graph Transformer (GT) -
were implemented to predict HF incidence. Model performance was evaluated using
F1 score, AUROC, and AUPRC metrics, and results were compared against baseline
algorithms. An interpretability analysis was performed to understand the
model's decision-making process. Results: The GT model demonstrated the best
performance (F1 score: 0.5361, AUROC: 0.7925, AUPRC: 0.5168). Although the
Random Forest (RF) baseline achieved a similar AUPRC value, the GT model
offered enhanced interpretability due to the use of patient relationships in
the graph structure. A joint analysis of attention weights, graph connectivity,
and clinical features provided insight into model predictions across different
classification groups. Discussion and Conclusion: Graph-based approaches such
as GNNs provide an effective framework for predicting HF. By leveraging a
patient similarity graph, GNNs can capture complex relationships in EHR data,
potentially improving prediction accuracy and clinical interpretability.",2024-11-29,"Heloisa Oss Boll, Ali Amirahmadi, Amira Soliman, Stefan Byttner, Mariana Recamonde-Mendoza",http://arxiv.org/pdf/2411.19742v1,cs.LG
A Note on Small Percolating Sets on Hypercubes via Generative AI,"We apply a generative AI pattern-recognition technique called PatternBoost to
study bootstrap percolation on hypercubes. With this, we slightly improve the
best existing upper bound for the size of percolating subsets of the hypercube.",2024-11-29,"Gergely Bérczi, Adam Zsolt Wagner",http://arxiv.org/pdf/2411.19734v1,cs.LG
Improving generalization of robot locomotion policies via Sharpness-Aware Reinforcement Learning,"Reinforcement learning often requires extensive training data.
Simulation-to-real transfer offers a promising approach to address this
challenge in robotics. While differentiable simulators offer improved sample
efficiency through exact gradients, they can be unstable in contact-rich
environments and may lead to poor generalization. This paper introduces a novel
approach integrating sharpness-aware optimization into gradient-based
reinforcement learning algorithms. Our simulation results demonstrate that our
method, tested on contact-rich environments, significantly enhances policy
robustness to environmental variations and action perturbations while
maintaining the sample efficiency of first-order methods. Specifically, our
approach improves action noise tolerance compared to standard first-order
methods and achieves generalization comparable to zeroth-order methods. This
improvement stems from finding flatter minima in the loss landscape, associated
with better generalization. Our work offers a promising solution to balance
efficient learning and robust sim-to-real transfer in robotics, potentially
bridging the gap between simulation and real-world performance.",2024-11-29,"Severin Bochem, Eduardo Gonzalez-Sanchez, Yves Bicker, Gabriele Fadini",http://arxiv.org/pdf/2411.19732v1,cs.LG
Real-Time Anomaly Detection in Video Streams,"This thesis is part of a CIFRE agreement between the company Othello and the
LIASD laboratory. The objective is to develop an artificial intelligence system
that can detect real-time dangers in a video stream. To achieve this, a novel
approach combining temporal and spatial analysis has been proposed. Several
avenues have been explored to improve anomaly detection by integrating object
detection, human pose detection, and motion analysis. For result
interpretability, techniques commonly used for image analysis, such as
activation and saliency maps, have been extended to videos, and an original
method has been proposed. The proposed architecture performs binary or
multiclass classification depending on whether an alert or the cause needs to
be identified. Numerous neural networkmodels have been tested, and three of
them have been selected. You Only Looks Once (YOLO) has been used for spatial
analysis, a Convolutional Recurrent Neuronal Network (CRNN) composed of VGG19
and a Gated Recurrent Unit (GRU) for temporal analysis, and a multi-layer
perceptron for classification. These models handle different types of data and
can be combined in parallel or in series. Although the parallel mode is faster,
the serial mode is generally more reliable. For training these models,
supervised learning was chosen, and two proprietary datasets were created. The
first dataset focuses on objects that may play a potential role in anomalies,
while the second consists of videos containing anomalies or non-anomalies. This
approach allows for the processing of both continuous video streams and finite
videos, providing greater flexibility in detection.",2024-11-29,Fabien Poirier,http://arxiv.org/pdf/2411.19731v1,cs.LG
Risk-Averse Certification of Bayesian Neural Networks,"In light of the inherently complex and dynamic nature of real-world
environments, incorporating risk measures is crucial for the robustness
evaluation of deep learning models. In this work, we propose a Risk-Averse
Certification framework for Bayesian neural networks called RAC-BNN. Our method
leverages sampling and optimisation to compute a sound approximation of the
output set of a BNN, represented using a set of template polytopes. To enhance
robustness evaluation, we integrate a coherent distortion risk
measure--Conditional Value at Risk (CVaR)--into the certification framework,
providing probabilistic guarantees based on empirical distributions obtained
through sampling. We validate RAC-BNN on a range of regression and
classification benchmarks and compare its performance with a state-of-the-art
method. The results show that RAC-BNN effectively quantifies robustness under
worst-performing risky scenarios, and achieves tighter certified bounds and
higher efficiency in complex tasks.",2024-11-29,"Xiyue Zhang, Zifan Wang, Yulong Gao, Licio Romao, Alessandro Abate, Marta Kwiatkowska",http://arxiv.org/pdf/2411.19729v1,cs.LG
Towards Santali Linguistic Inclusion: Building the First Santali-to-English Translation Model using mT5 Transformer and Data Augmentation,"Around seven million individuals in India, Bangladesh, Bhutan, and Nepal
speak Santali, positioning it as nearly the third most commonly used
Austroasiatic language. Despite its prominence among the Austroasiatic language
family's Munda subfamily, Santali lacks global recognition. Currently, no
translation models exist for the Santali language. Our paper aims to include
Santali to the NPL spectrum. We aim to examine the feasibility of building
Santali translation models based on available Santali corpora. The paper
successfully addressed the low-resource problem and, with promising results,
examined the possibility of creating a functional Santali machine translation
model in a low-resource setup. Our study shows that Santali-English parallel
corpus performs better when in transformers like mt5 as opposed to untrained
transformers, proving that transfer learning can be a viable technique that
works with Santali language. Besides the mT5 transformer, Santali-English
performs better than Santali-Bangla parallel corpus as the mT5 has been trained
in way more English data than Bangla data. Lastly, our study shows that with
data augmentation, our model performs better.",2024-11-29,"Syed Mohammed Mostaque Billah, Ateya Ahmed Subarna, Sudipta Nandi Sarna, Ahmad Shawkat Wasit, Anika Fariha, Asif Sushmit, Arig Yousuf Sadeque",http://arxiv.org/pdf/2411.19726v1,cs.LG
JetFormer: An Autoregressive Generative Model of Raw Images and Text,"Removing modeling constraints and unifying architectures across domains has
been a key driver of the recent progress in training large multimodal models.
However, most of these models still rely on many separately trained components
such as modality-specific encoders and decoders. In this work, we further
streamline joint generative modeling of images and text. We propose an
autoregressive decoder-only transformer - JetFormer - which is trained to
directly maximize the likelihood of raw data, without relying on any separately
pretrained components, and can understand and generate both text and images.
Specifically, we leverage a normalizing flow model to obtain a soft-token image
representation that is jointly trained with an autoregressive multimodal
transformer. The normalizing flow model serves as both an image encoder for
perception tasks and an image decoder for image generation tasks during
inference. JetFormer achieves text-to-image generation quality competitive with
recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained
image autoencoders, which are trained with a complex mixture of losses,
including perceptual ones. At the same time, JetFormer demonstrates robust
image understanding capabilities. To the best of our knowledge, JetFormer is
the first model that is capable of generating high-fidelity images and
producing strong log-likelihood bounds.",2024-11-29,"Michael Tschannen, André Susano Pinto, Alexander Kolesnikov",http://arxiv.org/pdf/2411.19722v2,cs.LG
Modelling Networked Dynamical System by Temporal Graph Neural ODE with Irregularly Partial Observed Time-series Data,"Modeling the evolution of system with time-series data is a challenging and
critical task in a wide range of fields, especially when the time-series data
is regularly sampled and partially observable. Some methods have been proposed
to estimate the hidden dynamics between intervals like Neural ODE or
Exponential decay dynamic function and combine with RNN to estimate the
evolution. However, it is difficult for these methods to capture the spatial
and temporal dependencies existing within graph-structured time-series data and
take full advantage of the available relational information to impute missing
data and predict the future states. Besides, traditional RNN-based methods
leverage shared RNN cell to update the hidden state which does not capture the
impact of various intervals and missing state information on the reliability of
estimating the hidden state. To solve this problem, in this paper, we propose a
method embedding Graph Neural ODE with reliability and time-aware mechanism
which can capture the spatial and temporal dependencies in irregularly sampled
and partially observable time-series data to reconstruct the dynamics. Also, a
loss function is designed considering the reliability of the augment data from
the above proposed method to make further prediction. The proposed method has
been validated in experiments of different networked dynamical systems.",2024-11-29,"Mengbang Zou, Weisi Guo",http://arxiv.org/pdf/2412.00165v1,cs.LG
Relative Representations of Latent Spaces enable Efficient Semantic Channel Equalization,"In multi-user semantic communication, language mismatche poses a significant
challenge when independently trained agents interact. We present a novel
semantic equalization algorithm that enables communication between agents with
different languages without additional retraining. Our algorithm is based on
relative representations, a framework that enables different agents employing
different neural network models to have unified representation. It proceeds by
projecting the latent vectors of different models into a common space defined
relative to a set of data samples called \textit{anchors}, whose number equals
the dimension of the resulting space. A communication between different agents
translates to a communication of semantic symbols sampled from this relative
space. This approach, in addition to aligning the semantic representations of
different agents, allows compressing the amount of information being exchanged,
by appropriately selecting the number of anchors. Eventually, we introduce a
novel anchor selection strategy, which advantageously determines prototypical
anchors, capturing the most relevant information for the downstream task. Our
numerical results show the effectiveness of the proposed approach allowing
seamless communication between agents with radically different models,
including differences in terms of neural network architecture and datasets used
for initial training.",2024-11-29,"Tomás Hüttebräucker, Simone Fiorellino, Mohamed Sana, Paolo Di Lorenzo, Emilio Calvanese Strinati",http://arxiv.org/pdf/2411.19719v1,cs.LG
MonoPP: Metric-Scaled Self-Supervised Monocular Depth Estimation by Planar-Parallax Geometry in Automotive Applications,"Self-supervised monocular depth estimation (MDE) has gained popularity for
obtaining depth predictions directly from videos. However, these methods often
produce scale invariant results, unless additional training signals are
provided. Addressing this challenge, we introduce a novel self-supervised
metric-scaled MDE model that requires only monocular video data and the
camera's mounting position, both of which are readily available in modern
vehicles. Our approach leverages planar-parallax geometry to reconstruct scene
structure. The full pipeline consists of three main networks, a multi-frame
network, a singleframe network, and a pose network. The multi-frame network
processes sequential frames to estimate the structure of the static scene using
planar-parallax geometry and the camera mounting position. Based on this
reconstruction, it acts as a teacher, distilling knowledge such as scale
information, masked drivable area, metric-scale depth for the static scene, and
dynamic object mask to the singleframe network. It also aids the pose network
in predicting a metric-scaled relative pose between two subsequent images. Our
method achieved state-of-the-art results for the driving benchmark KITTI for
metric-scaled depth prediction. Notably, it is one of the first methods to
produce self-supervised metric-scaled depth prediction for the challenging
Cityscapes dataset, demonstrating its effectiveness and versatility.",2024-11-29,"Gasser Elazab, Torben Gräber, Michael Unterreiner, Olaf Hellwich",http://arxiv.org/pdf/2411.19717v1,cs.LG
Forensics Adapter: Unleashing CLIP for Generalizable Face Forgery Detection,"We describe Forensics Adapter, an adapter network designed to transform CLIP
into an effective and generalizable face forgery detector. Although CLIP is
highly versatile, adapting it for face forgery detection is non-trivial as
forgery-related knowledge is entangled with a wide range of unrelated
knowledge. Existing methods treat CLIP merely as a feature extractor, lacking
task-specific adaptation, which limits their effectiveness. To address this, we
introduce an adapter to learn face forgery traces -- the blending boundaries
unique to forged faces, guided by task-specific objectives. Then we enhance the
CLIP visual tokens with a dedicated interaction strategy that communicates
knowledge across CLIP and the adapter. Since the adapter is alongside CLIP, its
versatility is highly retained, naturally ensuring strong generalizability in
face forgery detection. With only 5.7M trainable parameters, our method
achieves a significant performance boost, improving by approximately 7% on
average across five standard datasets. Additionally, we describe Forensics
Adapter++, an extended method that incorporates textual modality via a newly
proposed forgery-aware prompt learning strategy. This extension leads to a
further 1.3% performance boost over the original Forensics Adapter. We believe
the proposed methods can serve as a baseline for future CLIP-based face forgery
detection methods. The codes have been released at
https://github.com/OUC-VAS/ForensicsAdapter.",2024-11-29,"Xinjie Cui, Yuezun Li, Delong Zhu, Jiaran Zhou, Junyu Dong, Siwei Lyu",http://arxiv.org/pdf/2411.19715v3,cs.LG
The Streetscape Application Services Stack (SASS): Towards a Distributed Sensing Architecture for Urban Applications,"As urban populations grow, cities are becoming more complex, driving the
deployment of interconnected sensing systems to realize the vision of smart
cities. These systems aim to improve safety, mobility, and quality of life
through applications that integrate diverse sensors with real-time
decision-making. Streetscape applications-focusing on challenges like
pedestrian safety and adaptive traffic management-depend on managing
distributed, heterogeneous sensor data, aligning information across time and
space, and enabling real-time processing. These tasks are inherently complex
and often difficult to scale. The Streetscape Application Services Stack (SASS)
addresses these challenges with three core services: multimodal data
synchronization, spatiotemporal data fusion, and distributed edge computing. By
structuring these capabilities as clear, composable abstractions with clear
semantics, SASS allows developers to scale streetscape applications efficiently
while minimizing the complexity of multimodal integration.
  We evaluated SASS in two real-world testbed environments: a controlled
parking lot and an urban intersection in a major U.S. city. These testbeds
allowed us to test SASS under diverse conditions, demonstrating its practical
applicability. The Multimodal Data Synchronization service reduced temporal
misalignment errors by 88%, achieving synchronization accuracy within 50
milliseconds. Spatiotemporal Data Fusion service improved detection accuracy
for pedestrians and vehicles by over 10%, leveraging multicamera integration.
The Distributed Edge Computing service increased system throughput by more than
an order of magnitude. Together, these results show how SASS provides the
abstractions and performance needed to support real-time, scalable urban
applications, bridging the gap between sensing infrastructure and actionable
streetscape intelligence.",2024-11-29,"Navid Salami Pargoo, Mahshid Ghasemi, Shuren Xia, Mehmet Kerem Turkcan, Taqiya Ehsan, Chengbo Zang, Yuan Sun, Javad Ghaderi, Gil Zussman, Zoran Kostic, Jorge Ortiz",http://arxiv.org/pdf/2411.19714v2,cs.LG
Know Your RAG: Dataset Taxonomy and Generation Strategies for Evaluating RAG Systems,"Retrieval Augmented Generation (RAG) systems are a widespread application of
Large Language Models (LLMs) in the industry. While many tools exist empowering
developers to build their own systems, measuring their performance locally,
with datasets reflective of the system's use cases, is a technological
challenge. Solutions to this problem range from non-specific and cheap (most
public datasets) to specific and costly (generating data from local documents).
In this paper, we show that using public question and answer (Q&A) datasets to
assess retrieval performance can lead to non-optimal systems design, and that
common tools for RAG dataset generation can lead to unbalanced data. We propose
solutions to these issues based on the characterization of RAG datasets through
labels and through label-targeted data generation. Finally, we show that
fine-tuned small LLMs can efficiently generate Q&A datasets. We believe that
these observations are invaluable to the know-your-data step of RAG systems
development.",2024-11-29,"Rafael Teixeira de Lima, Shubham Gupta, Cesar Berrospi, Lokesh Mishra, Michele Dolfi, Peter Staar, Panagiotis Vagenas",http://arxiv.org/pdf/2411.19710v1,cs.LG
Fast Mutual Information Computation for Large Binary Datasets,"Mutual Information (MI) is a powerful statistical measure that quantifies
shared information between random variables, particularly valuable in
high-dimensional data analysis across fields like genomics, natural language
processing, and network science. However, computing MI becomes computationally
prohibitive for large datasets where it is typically required a pairwise
computational approach where each column is compared to others. This work
introduces a matrix-based algorithm that accelerates MI computation by
leveraging vectorized operations and optimized matrix calculations. By
transforming traditional pairwise computational approaches into bulk matrix
operations, the proposed method enables efficient MI calculation across all
variable pairs. Experimental results demonstrate significant performance
improvements, with computation times reduced up to 50,000 times in the largest
dataset using optimized implementations, particularly when utilizing hardware
optimized frameworks. The approach promises to expand MI's applicability in
data-driven research by overcoming previous computational limitations.",2024-11-29,Andre O. Falcao,http://arxiv.org/pdf/2411.19702v1,cs.LG
Explaining the Impact of Training on Vision Models via Activation Clustering,"This paper introduces Neuro-Activated Vision Explanations (NAVE), a method
for extracting and visualizing the internal representations of vision model
encoders. By clustering feature activations, NAVE provides insights into
learned semantics without fine-tuning. Using object localization, we show that
NAVE's concepts align with image semantics. Through extensive experiments, we
analyze the impact of training strategies and architectures on encoder
representation capabilities. Additionally, we apply NAVE to study training
artifacts in vision transformers and reveal how weak training strategies and
spurious correlations degrade model performance. Our findings establish NAVE as
a valuable tool for post-hoc model inspection and improving transparency in
vision models.",2024-11-29,"Ahcène Boubekki, Samuel G. Fadel, Sebastian Mair",http://arxiv.org/pdf/2411.19700v3,cs.LG
Gated-Attention Feature-Fusion Based Framework for Poverty Prediction,"This research paper addresses the significant challenge of accurately
estimating poverty levels using deep learning, particularly in developing
regions where traditional methods like household surveys are often costly,
infrequent, and quickly become outdated. To address these issues, we propose a
state-of-the-art Convolutional Neural Network (CNN) architecture, extending the
ResNet50 model by incorporating a Gated-Attention Feature-Fusion Module (GAFM).
Our architecture is designed to improve the model's ability to capture and
combine both global and local features from satellite images, leading to more
accurate poverty estimates. The model achieves a 75% R2 score, significantly
outperforming existing leading methods in poverty mapping. This improvement is
due to the model's capacity to focus on and refine the most relevant features,
filtering out unnecessary data, which makes it a powerful tool for remote
sensing and poverty estimation.",2024-11-29,"Muhammad Umer Ramzan, Wahab Khaddim, Muhammad Ehsan Rana, Usman Ali, Manohar Ali, Fiaz ul Hassan, Fatima Mehmood",http://arxiv.org/pdf/2411.19690v1,cs.LG
SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical VQA Tasks,"Vision-Language Models (VLMs) have great potential in medical tasks, like
Visual Question Answering (VQA), where they could act as interactive assistants
for both patients and clinicians. Yet their robustness to distribution shifts
on unseen data remains a critical concern for safe deployment. Evaluating such
robustness requires a controlled experimental setup that allows for systematic
insights into the model's behavior. However, we demonstrate that current setups
fail to offer sufficiently thorough evaluations, limiting their ability to
accurately assess model robustness. To address this gap, our work introduces a
novel framework, called SURE-VQA, centered around three key requirements to
overcome the current pitfalls and systematically analyze the robustness of
VLMs: 1) Since robustness on synthetic shifts does not necessarily translate to
real-world shifts, robustness should be measured on real-world shifts that are
inherent to the VQA data; 2) Traditional token-matching metrics often fail to
capture underlying semantics, necessitating the use of large language models
(LLMs) for more accurate semantic evaluation; 3) Model performance often lacks
interpretability due to missing sanity baselines, thus meaningful baselines
should be reported that allow assessing the multimodal impact on the VLM. To
demonstrate the relevance of this framework, we conduct a study on the
robustness of various fine-tuning methods across three medical datasets with
four different types of distribution shifts. Our study reveals several
important findings: 1) Sanity baselines that do not utilize image data can
perform surprisingly well; 2) We confirm LoRA as the best-performing PEFT
method; 3) No PEFT method consistently outperforms others in terms of
robustness to shifts. Code is provided at https://github.com/IML-DKFZ/sure-vqa.",2024-11-29,"Kim-Celine Kahl, Selen Erkan, Jeremias Traub, Carsten T. Lüth, Klaus Maier-Hein, Lena Maier-Hein, Paul F. Jaeger",http://arxiv.org/pdf/2411.19688v1,cs.LG
Privacy-Preserving Orthogonal Aggregation for Guaranteeing Gender Fairness in Federated Recommendation,"Under stringent privacy constraints, whether federated recommendation systems
can achieve group fairness remains an inadequately explored question. Taking
gender fairness as a representative issue, we identify three phenomena in
federated recommendation systems: performance difference, data imbalance, and
preference disparity. We discover that the state-of-the-art methods only focus
on the first phenomenon. Consequently, their imposition of inappropriate
fairness constraints detrimentally affects the model training. Moreover, due to
insufficient sensitive attribute protection of existing works, we can infer the
gender of all users with 99.90% accuracy even with the addition of maximal
noise. In this work, we propose Privacy-Preserving Orthogonal Aggregation
(PPOA), which employs the secure aggregation scheme and quantization technique,
to prevent the suppression of minority groups by the majority and preserve the
distinct preferences for better group fairness. PPOA can assist different
groups in obtaining their respective model aggregation results through a
designed orthogonal mapping while keeping their attributes private.
Experimental results on three real-world datasets demonstrate that PPOA
enhances recommendation effectiveness for both females and males by up to 8.25%
and 6.36%, respectively, with a maximum overall improvement of 7.30%, and
achieves optimal fairness in most cases. Extensive ablation experiments and
visualizations indicate that PPOA successfully maintains preferences for
different gender groups.",2024-11-29,"Siqing Zhang, Yuchen Ding, Wei Tang, Wei Sun, Yong Liao, Peng Yuan Zhou",http://arxiv.org/pdf/2411.19678v1,cs.LG
On the Performance Analysis of Momentum Method: A Frequency Domain Perspective,"Momentum-based optimizers are widely adopted for training neural networks.
However, the optimal selection of momentum coefficients remains elusive. This
uncertainty impedes a clear understanding of the role of momentum in stochastic
gradient methods. In this paper, we present a frequency domain analysis
framework that interprets the momentum method as a time-variant filter for
gradients, where adjustments to momentum coefficients modify the filter
characteristics. Our experiments support this perspective and provide a deeper
understanding of the mechanism involved. Moreover, our analysis reveals the
following significant findings: high-frequency gradient components are
undesired in the late stages of training; preserving the original gradient in
the early stages, and gradually amplifying low-frequency gradient components
during training both enhance performance. Based on these insights, we propose
Frequency Stochastic Gradient Descent with Momentum (FSGDM), a heuristic
optimizer that dynamically adjusts the momentum filtering characteristic with
an empirically effective dynamic magnitude response. Experimental results
demonstrate the superiority of FSGDM over conventional momentum optimizers.",2024-11-29,"Xianliang Li, Jun Luo, Zhiwei Zheng, Hanxiao Wang, Li Luo, Lingkun Wen, Linlong Wu, Sheng Xu",http://arxiv.org/pdf/2411.19671v6,cs.LG
Multimodal Whole Slide Foundation Model for Pathology,"The field of computational pathology has been transformed with recent
advances in foundation models that encode histopathology region-of-interests
(ROIs) into versatile and transferable feature representations via
self-supervised learning (SSL). However, translating these advancements to
address complex clinical challenges at the patient and slide level remains
constrained by limited clinical data in disease-specific cohorts, especially
for rare clinical conditions. We propose TITAN, a multimodal whole slide
foundation model pretrained using 335,645 WSIs via visual self-supervised
learning and vision-language alignment with corresponding pathology reports and
423,122 synthetic captions generated from a multimodal generative AI copilot
for pathology. Without any finetuning or requiring clinical labels, TITAN can
extract general-purpose slide representations and generate pathology reports
that generalize to resource-limited clinical scenarios such as rare disease
retrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and
find that TITAN outperforms both ROI and slide foundation models across machine
learning settings such as linear probing, few-shot and zero-shot
classification, rare cancer retrieval and cross-modal retrieval, and pathology
report generation.",2024-11-29,"Tong Ding, Sophia J. Wagner, Andrew H. Song, Richard J. Chen, Ming Y. Lu, Andrew Zhang, Anurag J. Vaidya, Guillaume Jaume, Muhammad Shaban, Ahrong Kim, Drew F. K. Williamson, Bowen Chen, Cristina Almagro-Perez, Paul Doucet, Sharifa Sahai, Chengkuan Chen, Daisuke Komura, Akihiro Kawabe, Shumpei Ishikawa, Georg Gerber, Tingying Peng, Long Phi Le, Faisal Mahmood",http://arxiv.org/pdf/2411.19666v1,cs.LG
Nonparametric Instrumental Regression via Kernel Methods is Minimax Optimal,"We study the kernel instrumental variable algorithm of
\citet{singh2019kernel}, a nonparametric two-stage least squares (2SLS)
procedure which has demonstrated strong empirical performance. We provide a
convergence analysis that covers both the identified and unidentified settings:
when the structural function cannot be identified, we show that the kernel NPIV
estimator converges to the IV solution with minimum norm. Crucially, our
convergence is with respect to the strong $L_2$-norm, rather than a
pseudo-norm. Additionally, we characterize the smoothness of the target
function without relying on the instrument, instead leveraging a new
description of the projected subspace size (this being closely related to the
link condition in inverse learning literature). With the subspace size
description and under standard kernel learning assumptions, we derive, for the
first time, the minimax optimal learning rate for kernel NPIV in the strong
$L_2$-norm. Our result demonstrates that the strength of the instrument is
essential to achieve efficient learning. We also improve the original kernel
NPIV algorithm by adopting a general spectral regularization in stage 1
regression. The modified regularization can overcome the saturation effect of
Tikhonov regularization.",2024-11-29,"Dimitri Meunier, Zhu Li, Tim Christensen, Arthur Gretton",http://arxiv.org/pdf/2411.19653v1,cs.LG
Uniform Attention Maps: Boosting Image Fidelity in Reconstruction and Editing,"Text-guided image generation and editing using diffusion models have achieved
remarkable advancements. Among these, tuning-free methods have gained attention
for their ability to perform edits without extensive model adjustments,
offering simplicity and efficiency. However, existing tuning-free approaches
often struggle with balancing fidelity and editing precision. Reconstruction
errors in DDIM Inversion are partly attributed to the cross-attention mechanism
in U-Net, which introduces misalignments during the inversion and
reconstruction process. To address this, we analyze reconstruction from a
structural perspective and propose a novel approach that replaces traditional
cross-attention with uniform attention maps, significantly enhancing image
reconstruction fidelity. Our method effectively minimizes distortions caused by
varying text conditions during noise prediction. To complement this
improvement, we introduce an adaptive mask-guided editing technique that
integrates seamlessly with our reconstruction approach, ensuring consistency
and accuracy in editing tasks. Experimental results demonstrate that our
approach not only excels in achieving high-fidelity image reconstruction but
also performs robustly in real image composition and editing scenarios. This
study underscores the potential of uniform attention maps to enhance the
fidelity and versatility of diffusion-based image processing methods. Code is
available at https://github.com/Mowenyii/Uniform-Attention-Maps.",2024-11-29,"Wenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen",http://arxiv.org/pdf/2411.19652v1,cs.LG
CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation,"The advancement of large Vision-Language-Action (VLA) models has
significantly improved robotic manipulation in terms of language-guided task
execution and generalization to unseen scenarios. While existing VLAs adapted
from pretrained large Vision-Language-Models (VLM) have demonstrated promising
generalizability, their task performance is still unsatisfactory as indicated
by the low tasks success rates in different environments. In this paper, we
present a new advanced VLA architecture derived from VLM. Unlike previous works
that directly repurpose VLM for action prediction by simple action
quantization, we propose a omponentized VLA architecture that has a specialized
action module conditioned on VLM output. We systematically study the design of
the action module and demonstrates the strong performance enhancement with
diffusion action transformers for action sequence modeling, as well as their
favorable scaling behaviors. We also conduct comprehensive experiments and
ablation studies to evaluate the efficacy of our models with varied designs.
The evaluation on 5 robot embodiments in simulation and real work shows that
our model not only significantly surpasses existing VLAs in task performance
and but also exhibits remarkable adaptation to new robots and generalization to
unseen objects and backgrounds. It exceeds the average success rates of OpenVLA
which has similar model size (7B) with ours by over 35% in simulated evaluation
and 55% in real robot experiments. It also outperforms the large RT-2-X model
(55B) by 18% absolute success rates in simulation. Code and models can be found
on our project page (https://cogact.github.io/).",2024-11-29,"Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, Baining Guo",http://arxiv.org/pdf/2411.19650v1,cs.LG
CAdam: Confidence-Based Optimization for Online Learning,"Modern recommendation systems frequently employ online learning to
dynamically update their models with freshly collected data. The most commonly
used optimizer for updating neural networks in these contexts is the Adam
optimizer, which integrates momentum ($m_t$) and adaptive learning rate
($v_t$). However, the volatile nature of online learning data, characterized by
its frequent distribution shifts and presence of noises, poses significant
challenges to Adam's standard optimization process: (1) Adam may use outdated
momentum and the average of squared gradients, resulting in slower adaptation
to distribution changes, and (2) Adam's performance is adversely affected by
data noise. To mitigate these issues, we introduce CAdam, a confidence-based
optimization strategy that assesses the consistence between the momentum and
the gradient for each parameter dimension before deciding on updates. If
momentum and gradient are in sync, CAdam proceeds with parameter updates
according to Adam's original formulation; if not, it temporarily withholds
updates and monitors potential shifts in data distribution in subsequent
iterations. This method allows CAdam to distinguish between the true
distributional shifts and mere noise, and adapt more quickly to new data
distributions. Our experiments with both synthetic and real-world datasets
demonstrate that CAdam surpasses other well-known optimizers, including the
original Adam, in efficiency and noise robustness. Furthermore, in large-scale
A/B testing within a live recommendation system, CAdam significantly enhances
model performance compared to Adam, leading to substantial increases in the
system's gross merchandise volume (GMV).",2024-11-29,"Shaowen Wang, Anan Liu, Jian Xiao, Huan Liu, Yuekui Yang, Cong Xu, Qianqian Pu, Suncong Zheng, Wei Zhang, Jian Li",http://arxiv.org/pdf/2411.19647v1,cs.LG
Dynamic High-Order Control Barrier Functions with Diffuser for Safety-Critical Trajectory Planning at Signal-Free Intersections,"Planning safe and efficient trajectories through signal-free intersections
presents significant challenges for autonomous vehicles (AVs), particularly in
dynamic, multi-task environments with unpredictable interactions and an
increased possibility of conflicts. This study aims to address these challenges
by developing a unified, robust, adaptive framework to ensure safety and
efficiency across three distinct intersection movements: left-turn, right-turn,
and straight-ahead. Existing methods often struggle to reliably ensure safety
and effectively learn multi-task behaviors from demonstrations in such
environments. This study proposes a safety-critical planning method that
integrates Dynamic High-Order Control Barrier Functions (DHOCBF) with a
diffusion-based model, called Dynamic Safety-Critical Diffuser (DSC-Diffuser).
The DSC-Diffuser leverages task-guided planning to enhance efficiency, allowing
the simultaneous learning of multiple driving tasks from real-world expert
demonstrations. Moreover, the incorporation of goal-oriented constraints
significantly reduces displacement errors, ensuring precise trajectory
execution. To further ensure driving safety in dynamic environments, the
proposed DHOCBF framework dynamically adjusts to account for the movements of
surrounding vehicles, offering enhanced adaptability and reduce the
conservatism compared to traditional control barrier functions. Validity
evaluations of DHOCBF, conducted through numerical simulations, demonstrate its
robustness in adapting to variations in obstacle velocities, sizes,
uncertainties, and locations, effectively maintaining driving safety across a
wide range of complex and uncertain scenarios. Comprehensive performance
evaluations demonstrate that DSC-Diffuser generates realistic, stable, and
generalizable policies, providing flexibility and reliable safety assurance in
complex multi-task driving scenarios.",2024-11-29,"Di Chen, Ruiguo Zhong, Kehua Chen, Zhiwei Shang, Meixin Zhu, Edward Chung",http://arxiv.org/pdf/2412.00162v2,cs.LG
STEP: Enhancing Video-LLMs' Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training,"Video Large Language Models (Video-LLMs) have recently shown strong
performance in basic video understanding tasks, such as captioning and
coarse-grained question answering, but struggle with compositional reasoning
that requires multi-step spatio-temporal inference across object relations,
interactions, and events. The hurdles to enhancing this capability include
extensive manual labor, the lack of spatio-temporal compositionality in
existing data and the absence of explicit reasoning supervision. In this paper,
we propose STEP, a novel graph-guided self-training method that enables
Video-LLMs to generate reasoning-rich fine-tuning data from any raw videos to
improve itself. Specifically, we first induce Spatio-Temporal Scene Graph
(STSG) representation of diverse videos to capture fine-grained, multi-granular
video semantics. Then, the STSGs guide the derivation of multi-step reasoning
Question-Answer (QA) data with Chain-of-Thought (CoT) rationales. Both answers
and rationales are integrated as training objective, aiming to enhance model's
reasoning abilities by supervision over explicit reasoning steps. Experimental
results demonstrate the effectiveness of STEP across models of varying scales,
with a significant 21.3\% improvement in tasks requiring three or more
reasoning steps. Furthermore, it achieves superior performance with a minimal
amount of self-generated rationale-enriched training samples in both
compositional reasoning and comprehensive understanding benchmarks,
highlighting the broad applicability and vast potential.",2024-11-29,"Haiyi Qiu, Minghe Gao, Long Qian, Kaihang Pan, Qifan Yu, Juncheng Li, Wenjie Wang, Siliang Tang, Yueting Zhuang, Tat-Seng Chua",http://arxiv.org/pdf/2412.00161v2,cs.LG
Learned Random Label Predictions as a Neural Network Complexity Metric,"We empirically investigate the impact of learning randomly generated labels
in parallel to class labels in supervised learning on memorization, model
complexity, and generalization in deep neural networks. To this end, we
introduce a multi-head network architecture as an extension of standard CNN
architectures. Inspired by methods used in fair AI, our approach allows for the
unlearning of random labels, preventing the network from memorizing individual
samples. Based on the concept of Rademacher complexity, we first use our
proposed method as a complexity metric to analyze the effects of common
regularization techniques and challenge the traditional understanding of
feature extraction and classification in CNNs. Second, we propose a novel
regularizer that effectively reduces sample memorization. However, contrary to
the predictions of classical statistical learning theory, we do not observe
improvements in generalization.",2024-11-29,"Marlon Becker, Benjamin Risse",http://arxiv.org/pdf/2411.19640v1,cs.LG
PACMANN: Point Adaptive Collocation Method for Artificial Neural Networks,"Physics-Informed Neural Networks (PINNs) are an emerging tool for
approximating the solution of Partial Differential Equations (PDEs) in both
forward and inverse problems. PINNs minimize a loss function which includes the
PDE residual determined for a set of collocation points. Previous work has
shown that the number and distribution of these collocation points have a
significant influence on the accuracy of the PINN solution. Therefore, the
effective placement of these collocation points is an active area of research.
Specifically, adaptive collocation point sampling methods have been proposed,
which have been reported to scale poorly to higher dimensions. In this work, we
address this issue and present the Point Adaptive Collocation Method for
Artificial Neural Networks (PACMANN). Inspired by classic optimization
problems, this approach incrementally moves collocation points toward regions
of higher residuals using gradient-based optimization algorithms guided by the
gradient of the squared residual. We apply PACMANN for forward and inverse
problems, and demonstrate that this method matches the performance of
state-of-the-art methods in terms of the accuracy/efficiency tradeoff for the
low-dimensional problems, while outperforming available approaches for
high-dimensional problems; the best performance is observed for the Adam
optimizer. Key features of the method include its low computational cost and
simplicity of integration in existing physics-informed neural network
pipelines.",2024-11-29,"Coen Visser, Alexander Heinlein, Bianca Giovanardi",http://arxiv.org/pdf/2411.19632v1,cs.LG
Non-linear Equalization in 112 Gb/s PONs Using Kolmogorov-Arnold Networks,"We investigate Kolmogorov-Arnold networks (KANs) for non-linear equalization
of 112 Gb/s PAM4 passive optical networks (PONs). Using pruning and extensive
hyperparameter search, we outperform linear equalizers and convolutional neural
networks at low computational complexity.",2024-11-29,"Rodrigo Fischer, Patrick Matalla, Sebastian Randel, Laurent Schmalen",http://arxiv.org/pdf/2411.19631v1,cs.LG
OpenQDC: Open Quantum Data Commons,"Machine Learning Interatomic Potentials (MLIPs) are a highly promising
alternative to force-fields for molecular dynamics (MD) simulations, offering
precise and rapid energy and force calculations. However, Quantum-Mechanical
(QM) datasets, crucial for MLIPs, are fragmented across various repositories,
hindering accessibility and model development. We introduce the openQDC
package, consolidating 37 QM datasets from over 250 quantum methods and 400
million geometries into a single, accessible resource. These datasets are
meticulously preprocessed, and standardized for MLIP training, covering a wide
range of chemical elements and interactions relevant in organic chemistry.
OpenQDC includes tools for normalization and integration, easily accessible via
Python. Experiments with well-known architectures like SchNet, TorchMD-Net, and
DimeNet reveal challenges for those architectures and constitute a leaderboard
to accelerate benchmarking and guide novel algorithms development. Continuously
adding datasets to OpenQDC will democratize QM dataset access, foster more
collaboration and innovation, enhance MLIP development, and support their
adoption in the MD field.",2024-11-29,"Cristian Gabellini, Nikhil Shenoy, Stephan Thaler, Semih Canturk, Daniel McNeela, Dominique Beaini, Michael Bronstein, Prudencio Tossou",http://arxiv.org/pdf/2411.19629v1,cs.LG
Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and the Empirical Findings,"The excessive use of visual tokens in existing Multimoal Large Language
Models (MLLMs) often exhibits obvious redundancy and brings in prohibitively
expensive computation. To gain insights into this problem, we first conduct
extensive empirical studies on the attention behaviors of MLLMs, and summarize
three main inference stages in MLLMs: (i) Early fusion between tokens is first
accomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)
Multimodal reasoning} resumes and lasts until the end of inference. In
particular, we reveal that visual tokens will stop contributing to reasoning
when the text tokens receive enough image information, yielding obvious visual
redundancy. Based on these generalized observations, we propose a simple yet
effective method to improve the efficiency of MLLMs, termed dynamic
visual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive
the text token status and decide the removal of all visual tokens after a
certain layer, thereby addressing the observed visual redundancy. To validate
VTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,
and conduct extensive experiments on a bunch of benchmarks. The experiment
results not only show the effectiveness of our VTE in improving MLLMs'
efficiency, but also yield the general modeling patterns of MLLMs, well
facilitating the in-depth understanding of MLLMs. Our code is anonymously
released at https://github.com/DoubtedSteam/DyVTE.",2024-11-29,"Qiong Wu, Wenhao Lin, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji",http://arxiv.org/pdf/2411.19628v1,cs.LG
FairDD: Fair Dataset Distillation via Synchronized Matching,"Condensing large datasets into smaller synthetic counterparts has
demonstrated its promise for image classification. However, previous research
has overlooked a crucial concern in image recognition: ensuring that models
trained on condensed datasets are unbiased towards protected attributes (PA),
such as gender and race. Our investigation reveals that dataset distillation
(DD) fails to alleviate the unfairness towards minority groups within original
datasets. Moreover, this bias typically worsens in the condensed datasets due
to their smaller size. To bridge the research gap, we propose a novel fair
dataset distillation (FDD) framework, namely FairDD, which can be seamlessly
applied to diverse matching-based DD approaches, requiring no modifications to
their original architectures. The key innovation of FairDD lies in
synchronously matching synthetic datasets to PA-wise groups of original
datasets, rather than indiscriminate alignment to the whole distributions in
vanilla DDs, dominated by majority groups. This synchronized matching allows
synthetic datasets to avoid collapsing into majority groups and bootstrap their
balanced generation to all PA groups. Consequently, FairDD could effectively
regularize vanilla DDs to favor biased generation toward minority groups while
maintaining the accuracy of target attributes. Theoretical analyses and
extensive experimental evaluations demonstrate that FairDD significantly
improves fairness compared to vanilla DD methods, without sacrificing
classification accuracy. Its consistent superiority across diverse DDs,
spanning Distribution and Gradient Matching, establishes it as a versatile FDD
approach.",2024-11-29,"Qihang Zhou, Shenhao Fang, Shibo He, Wenchao Meng, Jiming Chen",http://arxiv.org/pdf/2411.19623v1,cs.LG
Materials Learning Algorithms (MALA): Scalable Machine Learning for Electronic Structure Calculations in Large-Scale Atomistic Simulations,"We present the Materials Learning Algorithms (MALA) package, a scalable
machine learning framework designed to accelerate density functional theory
(DFT) calculations suitable for large-scale atomistic simulations. Using local
descriptors of the atomic environment, MALA models efficiently predict key
electronic observables, including local density of states, electronic density,
density of states, and total energy. The package integrates data sampling,
model training and scalable inference into a unified library, while ensuring
compatibility with standard DFT and molecular dynamics codes. We demonstrate
MALA's capabilities with examples including boron clusters, aluminum across its
solid-liquid phase boundary, and predicting the electronic structure of a
stacking fault in a large beryllium slab. Scaling analyses reveal MALA's
computational efficiency and identify bottlenecks for future optimization. With
its ability to model electronic structures at scales far beyond standard DFT,
MALA is well suited for modeling complex material systems, making it a
versatile tool for advanced materials research.",2024-11-29,"Attila Cangi, Lenz Fiedler, Bartosz Brzoza, Karan Shah, Timothy J. Callow, Daniel Kotik, Steve Schmerler, Matthew C. Barry, James M. Goff, Andrew Rohskopf, Dayton J. Vogel, Normand Modine, Aidan P. Thompson, Sivasankaran Rajamanickam",http://arxiv.org/pdf/2411.19617v1,cs.LG
Self-Supervised Denoiser Framework,"Reconstructing images using Computed Tomography (CT) in an industrial context
leads to specific challenges that differ from those encountered in other areas,
such as clinical CT. Indeed, non-destructive testing with industrial CT will
often involve scanning multiple similar objects while maintaining high
throughput, requiring short scanning times, which is not a relevant concern in
clinical CT. Under-sampling the tomographic data (sinograms) is a natural way
to reduce the scanning time at the cost of image quality since the latter
depends on the number of measurements. In such a scenario, post-processing
techniques are required to compensate for the image artifacts induced by the
sinogram sparsity. We introduce the Self-supervised Denoiser Framework (SDF), a
self-supervised training method that leverages pre-training on highly sampled
sinogram data to enhance the quality of images reconstructed from undersampled
sinogram data. The main contribution of SDF is that it proposes to train an
image denoiser in the sinogram space by setting the learning task as the
prediction of one sinogram subset from another. As such, it does not require
ground-truth image data, leverages the abundant data modality in CT, the
sinogram, and can drastically enhance the quality of images reconstructed from
a fraction of the measurements. We demonstrate that SDF produces better image
quality, in terms of peak signal-to-noise ratio, than other analytical and
self-supervised frameworks in both 2D fan-beam or 3D cone-beam CT settings.
Moreover, we show that the enhancement provided by SDF carries over when
fine-tuning the image denoiser on a few examples, making it a suitable
pre-training technique in a context where there is little high-quality image
data. Our results are established on experimental datasets, making SDF a strong
candidate for being the building block of foundational image-enhancement models
in CT.",2024-11-29,"Emilien Valat, Andreas Hauptmann, Ozan Öktem",http://arxiv.org/pdf/2411.19593v1,cs.LG
LDA-AQU: Adaptive Query-guided Upsampling via Local Deformable Attention,"Feature upsampling is an essential operation in constructing deep
convolutional neural networks. However, existing upsamplers either lack
specific feature guidance or necessitate the utilization of high-resolution
feature maps, resulting in a loss of performance and flexibility. In this
paper, we find that the local self-attention naturally has the feature guidance
capability, and its computational paradigm aligns closely with the essence of
feature upsampling (\ie feature reassembly of neighboring points). Therefore,
we introduce local self-attention into the upsampling task and demonstrate that
the majority of existing upsamplers can be regarded as special cases of
upsamplers based on local self-attention. Considering the potential semantic
gap between upsampled points and their neighboring points, we further introduce
the deformation mechanism into the upsampler based on local self-attention,
thereby proposing LDA-AQU. As a novel dynamic kernel-based upsampler, LDA-AQU
utilizes the feature of queries to guide the model in adaptively adjusting the
position and aggregation weight of neighboring points, thereby meeting the
upsampling requirements across various complex scenarios. In addition, LDA-AQU
is lightweight and can be easily integrated into various model architectures.
We evaluate the effectiveness of LDA-AQU across four dense prediction tasks:
object detection, instance segmentation, panoptic segmentation, and semantic
segmentation. LDA-AQU consistently outperforms previous state-of-the-art
upsamplers, achieving performance enhancements of 1.7 AP, 1.5 AP, 2.0 PQ, and
2.5 mIoU compared to the baseline models in the aforementioned four tasks,
respectively. Code is available at \url{https://github.com/duzw9311/LDA-AQU}.",2024-11-29,"Zewen Du, Zhenjiang Hu, Guiyu Zhao, Ying Jin, Hongbin Ma",http://arxiv.org/pdf/2411.19585v1,cs.LG
Enhancing Sentiment Analysis in Bengali Texts: A Hybrid Approach Using Lexicon-Based Algorithm and Pretrained Language Model Bangla-BERT,"Sentiment analysis (SA) is a process of identifying the emotional tone or
polarity within a given text and aims to uncover the user's complex emotions
and inner feelings. While sentiment analysis has been extensively studied for
languages like English, research in Bengali, remains limited, particularly for
fine-grained sentiment categorization. This work aims to connect this gap by
developing a novel approach that integrates rule-based algorithms with
pre-trained language models. We developed a dataset from scratch, comprising
over 15,000 manually labeled reviews. Next, we constructed a Lexicon Data
Dictionary, assigning polarity scores to the reviews. We developed a novel rule
based algorithm Bangla Sentiment Polarity Score (BSPS), an approach capable of
generating sentiment scores and classifying reviews into nine distinct
sentiment categories. To assess the performance of this method, we evaluated
the classified sentiments using BanglaBERT, a pre-trained transformer-based
language model. We also performed sentiment classification directly with
BanglaBERT on the original data and evaluated this model's results. Our
analysis revealed that the BSPS + BanglaBERT hybrid approach outperformed the
standalone BanglaBERT model, achieving higher accuracy, precision, and nuanced
classification across the nine sentiment categories. The results of our study
emphasize the value and effectiveness of combining rule-based and pre-trained
language model approaches for enhanced sentiment analysis in Bengali and
suggest pathways for future research and application in languages with similar
linguistic complexities.",2024-11-29,"Hemal Mahmud, Hasan Mahmud, Mohammad Rifat Ahmmad Rashid",http://arxiv.org/pdf/2411.19584v2,cs.LG
Solving Rubik's Cube Without Tricky Sampling,"The Rubiks Cube, with its vast state space and sparse reward structure,
presents a significant challenge for reinforcement learning (RL) due to the
difficulty of reaching rewarded states. Previous research addressed this by
propagating cost-to-go estimates from the solved state and incorporating search
techniques. These approaches differ from human strategies that start from fully
scrambled cubes, which can be tricky for solving a general sparse-reward
problem. In this paper, we introduce a novel RL algorithm using policy gradient
methods to solve the Rubiks Cube without relying on near solved-state sampling.
Our approach employs a neural network to predict cost patterns between states,
allowing the agent to learn directly from scrambled states. Our method was
tested on the 2x2x2 Rubiks Cube, where the cube was scrambled 50,000 times, and
the model successfully solved it in over 99.4% of cases. Notably, this result
was achieved using only the policy network without relying on tree search as in
previous methods, demonstrating its effectiveness and potential for broader
applications in sparse-reward problems.",2024-11-29,"Yicheng Lin, Siyu Liang",http://arxiv.org/pdf/2411.19583v1,cs.LG
A Comprehensive Framework for Automated Segmentation of Perivascular Spaces in Brain MRI with the nnU-Net,"Background: Enlargement of perivascular spaces (PVS) is common in
neurodegenerative disorders including cerebral small vessel disease,
Alzheimer's disease, and Parkinson's disease. PVS enlargement may indicate
impaired clearance pathways and there is a need for reliable PVS detection
methods which are currently lacking. Aim: To optimise a widely used deep
learning model, the no-new-UNet (nnU-Net), for PVS segmentation. Methods: In 30
healthy participants (mean$\pm$SD age: 50$\pm$18.9 years; 13 females),
T1-weighted MRI images were acquired using three different protocols on three
MRI scanners (3T Siemens Tim Trio, 3T Philips Achieva, and 7T Siemens
Magnetom). PVS were manually segmented across ten axial slices in each
participant. Segmentations were completed using a sparse annotation strategy.
In total, 11 models were compared using various strategies for image handling,
preprocessing and semi-supervised learning with pseudo-labels. Model
performance was evaluated using 5-fold cross validation (5FCV). The main
performance metric was the Dice Similarity Coefficient (DSC). Results: The
voxel-spacing agnostic model (mean$\pm$SD DSC=64.3$\pm$3.3%) outperformed
models which resampled images to a common resolution (DSC=40.5-55%). Model
performance improved substantially following iterative label cleaning
(DSC=85.7$\pm$1.2%). Semi-supervised learning with pseudo-labels (n=12,740)
from 18 additional datasets improved the agreement between raw and predicted
PVS cluster counts (Lin's concordance correlation coefficient=0.89,
95%CI=0.82-0.94). We extended the model to enable PVS segmentation in the
midbrain (DSC=64.3$\pm$6.5%) and hippocampus (DSC=67.8$\pm$5%). Conclusions:
Our deep learning models provide a robust and holistic framework for the
automated quantification of PVS in brain MRI.",2024-11-29,"William Pham, Alexander Jarema, Donggyu Rim, Zhibin Chen, Mohamed S. H. Khlif, Vaughan G. Macefield, Luke A. Henderson, Amy Brodtmann",http://arxiv.org/pdf/2411.19564v2,cs.LG
Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning,"Low-rank adapters have become standard for efficiently fine-tuning large
language models (LLMs), but they often fall short of achieving the performance
of full fine-tuning. We propose a method, LoRA Silver Bullet or LoRA-SB, that
approximates full fine-tuning within low-rank subspaces using a carefully
designed initialization strategy. We theoretically demonstrate that the
architecture of LoRA-XS, which inserts a learnable (r x r) matrix between B and
A while keeping other matrices fixed, provides the precise conditions needed
for this approximation. We leverage its constrained update space to achieve
optimal scaling for high-rank gradient updates while removing the need for
hyperparameter tuning. We prove that our initialization offers an optimal
low-rank approximation of the initial gradient and preserves update directions
throughout training. Extensive experiments across mathematical reasoning,
commonsense reasoning, and language understanding tasks demonstrate that our
approach exceeds the performance of standard LoRA while using \textbf{27-90}
times fewer learnable parameters, and comprehensively outperforms LoRA-XS. Our
findings establish that it is possible to simulate full fine-tuning in low-rank
subspaces, and achieve significant efficiency gains without sacrificing
performance. Our code is publicly available at
https://github.com/RaghavSinghal10/lora-sb.",2024-11-29,"Kaustubh Ponkshe, Raghav Singhal, Eduard Gorbunov, Alexey Tumanov, Samuel Horvath, Praneeth Vepakomma",http://arxiv.org/pdf/2411.19557v3,cs.LG
Differentiable Causal Discovery For Latent Hierarchical Causal Models,"Discovering causal structures with latent variables from observational data
is a fundamental challenge in causal discovery. Existing methods often rely on
constraint-based, iterative discrete searches, limiting their scalability to
large numbers of variables. Moreover, these methods frequently assume linearity
or invertibility, restricting their applicability to real-world scenarios. We
present new theoretical results on the identifiability of nonlinear latent
hierarchical causal models, relaxing previous assumptions in literature about
the deterministic nature of latent variables and exogenous noise. Building on
these insights, we develop a novel differentiable causal discovery algorithm
that efficiently estimates the structure of such models. To the best of our
knowledge, this is the first work to propose a differentiable causal discovery
method for nonlinear latent hierarchical models. Our approach outperforms
existing methods in both accuracy and scalability. We demonstrate its practical
utility by learning interpretable hierarchical latent structures from
high-dimensional image data and demonstrate its effectiveness on downstream
tasks.",2024-11-29,"Parjanya Prashant, Ignavier Ng, Kun Zhang, Biwei Huang",http://arxiv.org/pdf/2411.19556v1,cs.LG
