title,summary,published,authors,pdf_url,category
Calibrating Language Models with Adaptive Temperature Scaling,"The effectiveness of large language models (LLMs) is not only measured by
their ability to generate accurate outputs but also by their calibration-how
well their confidence scores reflect the probability of their outputs being
correct. While unsupervised pre-training has been shown to yield LLMs with
well-calibrated conditional probabilities, recent studies have shown that after
fine-tuning with reinforcement learning from human feedback (RLHF), the
calibration of these models degrades significantly. In this work, we introduce
Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts
a temperature scaling parameter for each token prediction. The predicted
temperature values adapt based on token-level features and are fit over a
standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS
addresses the varying degrees of calibration shift that can occur after RLHF
fine-tuning. ATS improves calibration by over 10-50% across three downstream
natural language evaluation benchmarks compared to prior calibration methods
and does not impede performance improvements from RLHF.",2024-09-29,"Johnathan Xie, Annie S. Chen, Yoonho Lee, Eric Mitchell, Chelsea Finn",http://arxiv.org/pdf/2409.19817v1,cs.CL
Transforming Hidden States into Binary Semantic Features,"Large language models follow a lineage of many NLP applications that were
directly inspired by distributional semantics, but do not seem to be closely
related to it anymore. In this paper, we propose to employ the distributional
theory of meaning once again. Using Independent Component Analysis to overcome
some of its challenging aspects, we show that large language models represent
semantic features in their hidden states.",2024-09-29,"Tomáš Musil, David Mareček",http://arxiv.org/pdf/2409.19813v1,cs.CL
Can Models Learn Skill Composition from Examples?,"As large language models (LLMs) become increasingly advanced, their ability
to exhibit compositional generalization -- the capacity to combine learned
skills in novel ways not encountered during training -- has garnered
significant attention. This type of generalization, particularly in scenarios
beyond training data, is also of great interest in the study of AI safety and
alignment. A recent study introduced the SKILL-MIX evaluation, where models are
tasked with composing a short paragraph demonstrating the use of a specified
$k$-tuple of language skills. While small models struggled with composing even
with $k=3$, larger models like GPT-4 performed reasonably well with $k=5$ and
$6$.
  In this paper, we employ a setup akin to SKILL-MIX to evaluate the capacity
of smaller models to learn compositional generalization from examples.
Utilizing a diverse set of language skills -- including rhetorical, literary,
reasoning, theory of mind, and common sense -- GPT-4 was used to generate text
samples that exhibit random subsets of $k$ skills. Subsequent fine-tuning of 7B
and 13B parameter models on these combined skill texts, for increasing values
of $k$, revealed the following findings: (1) Training on combinations of $k=2$
and $3$ skills results in noticeable improvements in the ability to compose
texts with $k=4$ and $5$ skills, despite models never having seen such examples
during training. (2) When skill categories are split into training and held-out
groups, models significantly improve at composing texts with held-out skills
during testing despite having only seen training skills during fine-tuning,
illustrating the efficacy of the training approach even with previously unseen
skills. This study also suggests that incorporating skill-rich (potentially
synthetic) text into training can substantially enhance the compositional
capabilities of models.",2024-09-29,"Haoyu Zhao, Simran Kaur, Dingli Yu, Anirudh Goyal, Sanjeev Arora",http://arxiv.org/pdf/2409.19808v2,cs.CL
Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented Generation Systems,"Retrieval-Augmented Generation (RAG) has recently gained significant
attention for its enhanced ability to integrate external knowledge sources into
open-domain question answering (QA) tasks. However, it remains unclear how
these models address fairness concerns, particularly with respect to sensitive
attributes such as gender, geographic location, and other demographic factors.
First, as language models evolve to prioritize utility, like improving exact
match accuracy, fairness considerations may have been largely overlooked.
Second, the complex, multi-component architecture of RAG methods poses
challenges in identifying and mitigating biases, as each component is optimized
for distinct objectives. In this paper, we aim to empirically evaluate fairness
in several RAG methods. We propose a fairness evaluation framework tailored to
RAG, using scenario-based questions and analyzing disparities across
demographic attributes. Our experimental results indicate that, despite recent
advances in utility-driven optimization, fairness issues persist in both the
retrieval and generation stages. These findings underscore the need for
targeted interventions to address fairness concerns throughout the RAG
pipeline. The dataset and code used in this study are publicly available at
this GitHub Repository https://github.com/elviswxy/RAG_fairness .",2024-09-29,"Xuyang Wu, Shuowei Li, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang",http://arxiv.org/pdf/2409.19804v2,cs.CL
CRScore: Grounding Automated Evaluation of Code Review Comments in Code Claims and Smells,"The task of automated code review has recently gained a lot of attention from
the machine learning community. However, current review comment evaluation
metrics rely on comparisons with a human-written reference for a given code
change (also called a diff). Furthermore, code review is a one-to-many problem,
like generation and summarization, with many ""valid reviews"" for a diff. Thus,
we develop CRScore - a reference-free metric to measure dimensions of review
quality like conciseness, comprehensiveness, and relevance. We design CRScore
to evaluate reviews in a way that is grounded in claims and potential issues
detected in the code by LLMs and static analyzers. We demonstrate that CRScore
can produce valid, fine-grained scores of review quality that have the greatest
alignment with human judgment among open source metrics (0.54 Spearman
correlation) and are more sensitive than reference-based metrics. We also
release a corpus of 2.9k human-annotated review quality scores for
machine-generated and GitHub review comments to support the development of
automated metrics.",2024-09-29,"Atharva Naik, Marcus Alenius, Daniel Fried, Carolyn Rose",http://arxiv.org/pdf/2409.19801v2,cs.CL
Black-Box Segmentation of Electronic Medical Records,"Electronic medical records (EMRs) contain the majority of patients'
healthcare details. It is an abundant resource for developing an automatic
healthcare system. Most of the natural language processing (NLP) studies on EMR
processing, such as concept extraction, are adversely affected by the
inaccurate segmentation of EMR sections. At the same time, not enough attention
has been given to the accurate sectioning of EMRs. The information that may
occur in section structures is unvalued. This work focuses on the segmentation
of EMRs and proposes a black-box segmentation method using a simple sentence
embedding model and neural network, along with a proper training method. To
achieve universal adaptivity, we train our model on the dataset with different
section headings formats. We compare several advanced deep learning-based NLP
methods, and our method achieves the best segmentation accuracies (above 98%)
on various test data with a proper training corpus.",2024-09-29,"Hongyi Yuan, Sheng Yu",http://arxiv.org/pdf/2409.19796v1,cs.CL
Exploring Adversarial Robustness in Classification tasks using DNA Language Models,"DNA Language Models, such as GROVER, DNABERT2 and the Nucleotide Transformer,
operate on DNA sequences that inherently contain sequencing errors, mutations,
and laboratory-induced noise, which may significantly impact model performance.
Despite the importance of this issue, the robustness of DNA language models
remains largely underexplored. In this paper, we comprehensivly investigate
their robustness in DNA classification by applying various adversarial attack
strategies: the character (nucleotide substitutions), word (codon
modifications), and sentence levels (back-translation-based transformations) to
systematically analyze model vulnerabilities. Our results demonstrate that DNA
language models are highly susceptible to adversarial attacks, leading to
significant performance degradation. Furthermore, we explore adversarial
training method as a defense mechanism, which enhances both robustness and
classification accuracy. This study highlights the limitations of DNA language
models and underscores the necessity of robustness in bioinformatics.",2024-09-29,"Hyunwoo Yoo, Haebin Shin, Kaidi Xu, Gail Rosen",http://arxiv.org/pdf/2409.19788v2,cs.CL
"Realtime, multimodal invasive ventilation risk monitoring using language models and BoXHED","Objective: realtime monitoring of invasive ventilation (iV) in intensive care
units (ICUs) plays a crucial role in ensuring prompt interventions and better
patient outcomes. However, conventional methods often overlook valuable
insights embedded within clinical notes, relying solely on tabular data. In
this study, we propose an innovative approach to enhance iV risk monitoring by
incorporating clinical notes into the monitoring pipeline through using
language models for text summarization. Results: We achieve superior
performance in all metrics reported by the state-of-the-art in iV risk
monitoring, namely: an AUROC of 0.86, an AUC-PR of 0.35, and an AUCt of up to
0.86. We also demonstrate that our methodology allows for more lead time in
flagging iV for certain time buckets. Conclusion: Our study underscores the
potential of integrating clinical notes and language models into realtime iV
risk monitoring, paving the way for improved patient care and informed clinical
decision-making in ICU settings.",2024-09-29,"Arash Pakbin, Aaron Su, Donald K. K. Lee, Bobak J. Mortazavi",http://arxiv.org/pdf/2410.03725v1,cs.CL
Towards Robust Extractive Question Answering Models: Rethinking the Training Methodology,"This paper proposes a novel training method to improve the robustness of
Extractive Question Answering (EQA) models. Previous research has shown that
existing models, when trained on EQA datasets that include unanswerable
questions, demonstrate a significant lack of robustness against distribution
shifts and adversarial attacks. Despite this, the inclusion of unanswerable
questions in EQA training datasets is essential for ensuring real-world
reliability. Our proposed training method includes a novel loss function for
the EQA problem and challenges an implicit assumption present in numerous EQA
datasets. Models trained with our method maintain in-domain performance while
achieving a notable improvement on out-of-domain datasets. This results in an
overall F1 score improvement of 5.7 across all testing sets. Furthermore, our
models exhibit significantly enhanced robustness against two types of
adversarial attacks, with a performance decrease of only about a third compared
to the default models.",2024-09-29,"Son Quoc Tran, Matt Kretchmar",http://arxiv.org/pdf/2409.19766v1,cs.CL
Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs,"As large language models (LLMs) are applied to more use cases, creating high
quality, task-specific datasets for fine-tuning becomes a bottleneck for model
improvement. Using high quality human data has been the most common approach to
unlock model performance, but is prohibitively expensive in many scenarios.
Several alternative methods have also emerged, such as generating synthetic or
hybrid data, but the effectiveness of these approaches remain unclear,
especially in resource-constrained scenarios and tasks that are not easily
verified. To investigate this, we group various synthetic data generation
strategies into three representative categories -- Answer Augmentation,
Question Rephrase and New Question -- and study the performance of student LLMs
trained under various constraints, namely seed instruction set size and query
budget. We demonstrate that these strategies are not equally effective across
settings. Notably, the optimal data generation strategy depends strongly on the
ratio between the available teacher query budget and the size of the seed
instruction set. When this ratio is low, generating new answers to existing
questions proves most effective, but as this ratio increases, generating new
questions becomes optimal. Across all tasks, we find that choice of
augmentation method and other design choices matter substantially more in low
to mid data regimes than in high data regimes. We provide a practical framework
for selecting the appropriate augmentation method across settings, taking into
account additional factors such as the scalability of each method, the
importance of verifying synthetic data, and the use of different LLMs for
synthetic data generation.",2024-09-29,"Yung-Chieh Chan, George Pu, Apaar Shanker, Parth Suresh, Penn Jenks, John Heyer, Sam Denton",http://arxiv.org/pdf/2409.19759v3,cs.CL
CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering,"Recent studies have explored the use of Large Language Models (LLMs) with
Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering
(KGQA). They typically require rewriting retrieved subgraphs into natural
language formats comprehensible to LLMs. However, when tackling complex
questions, the knowledge rewritten by existing methods may include irrelevant
information, omit crucial details, or fail to align with the question's
semantics. To address them, we propose a novel rewriting method CoTKR,
Chain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces
and corresponding knowledge in an interleaved manner, thereby mitigating the
limitations of single-step knowledge rewriting. Additionally, to bridge the
preference gap between the knowledge rewriter and the question answering (QA)
model, we propose a training strategy PAQAF, Preference Alignment from Question
Answering Feedback, for leveraging feedback from the QA model to further
optimize the knowledge rewriter. We conduct experiments using various LLMs
across several KGQA benchmarks. Experimental results demonstrate that, compared
with previous knowledge rewriting methods, CoTKR generates the most beneficial
knowledge representation for QA models, which significantly improves the
performance of LLMs in KGQA.",2024-09-29,"Yike Wu, Yi Huang, Nan Hu, Yuncheng Hua, Guilin Qi, Jiaoyan Chen, Jeff Z. Pan",http://arxiv.org/pdf/2409.19753v3,cs.CL
AstroMLab 2: AstroLLaMA-2-70B Model and Benchmarking Specialised LLMs for Astronomy,"Continual pretraining of large language models on domain-specific data has
been proposed to enhance performance on downstream tasks. In astronomy, the
previous absence of astronomy-focused benchmarks has hindered objective
evaluation of these specialized LLM models. Leveraging a recent initiative to
curate high-quality astronomical MCQs, this study aims to quantitatively assess
specialized LLMs in astronomy. We find that the previously released AstroLLaMA
series, based on LLaMA-2-7B, underperforms compared to the base model. We
demonstrate that this performance degradation can be partially mitigated by
utilizing high-quality data for continual pretraining, such as summarized text
from arXiv. Despite the observed catastrophic forgetting in smaller models, our
results indicate that continual pretraining on the 70B model can yield
significant improvements. However, the current supervised fine-tuning dataset
still constrains the performance of instruct models. In conjunction with this
study, we introduce a new set of models, AstroLLaMA-3-8B and AstroLLaMA-2-70B,
building upon the previous AstroLLaMA series.",2024-09-29,"Rui Pan, Tuan Dung Nguyen, Hardik Arora, Alberto Accomazzi, Tirthankar Ghosal, Yuan-Sen Ting",http://arxiv.org/pdf/2409.19750v1,cs.CL
NeuroMax: Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization,"Recent advances in neural topic models have concentrated on two primary
directions: the integration of the inference network (encoder) with a
pre-trained language model (PLM) and the modeling of the relationship between
words and topics in the generative model (decoder). However, the use of large
PLMs significantly increases inference costs, making them less practical for
situations requiring low inference times. Furthermore, it is crucial to
simultaneously model the relationships between topics and words as well as the
interrelationships among topics themselves. In this work, we propose a novel
framework called NeuroMax (Neural Topic Model with Maximizing Mutual
Information with Pretrained Language Model and Group Topic Regularization) to
address these challenges. NeuroMax maximizes the mutual information between the
topic representation obtained from the encoder in neural topic models and the
representation derived from the PLM. Additionally, NeuroMax employs optimal
transport to learn the relationships between topics by analyzing how
information is transported among them. Experimental results indicate that
NeuroMax reduces inference time, generates more coherent topics and topic
groups, and produces more representative document embeddings, thereby enhancing
performance on downstream tasks.",2024-09-29,"Duy-Tung Pham, Thien Trang Nguyen Vu, Tung Nguyen, Linh Ngo Van, Duc Anh Nguyen, Thien Huu Nguyen",http://arxiv.org/pdf/2409.19749v1,cs.CL
"Natural Language Generation for Visualizations: State of the Art, Challenges and Future Directions","Natural language and visualization are two complementary modalities of human
communication that play a crucial role in conveying information effectively.
While visualizations help people discover trends, patterns, and anomalies in
data, natural language descriptions help explain these insights. Thus,
combining text with visualizations is a prevalent technique for effectively
delivering the core message of the data. Given the rise of natural language
generation (NLG), there is a growing interest in automatically creating natural
language descriptions for visualizations, which can be used as chart captions,
answering questions about charts, or telling data-driven stories. In this
survey, we systematically review the state of the art on NLG for visualizations
and introduce a taxonomy of the problem. The NLG tasks fall within the domain
of Natural Language Interfaces (NLI) for visualization, an area that has
garnered significant attention from both the research community and industry.
To narrow down the scope of the survey, we primarily concentrate on the
research works that focus on text generation for visualizations. To
characterize the NLG problem and the design space of proposed solutions, we
pose five Wh-questions, why and how NLG tasks are performed for visualizations,
what the task inputs and outputs are, as well as where and when the generated
texts are integrated with visualizations. We categorize the solutions used in
the surveyed papers based on these ""five Wh-questions."" Finally, we discuss the
key challenges and potential avenues for future research in this domain.",2024-09-29,"Enamul Hoque, Mohammed Saidul Islam",http://arxiv.org/pdf/2409.19747v1,cs.CL
PEAR: Position-Embedding-Agnostic Attention Re-weighting Enhances Retrieval-Augmented Generation with Zero Inference Overhead,"Large language models (LLMs) enhanced with retrieval-augmented generation
(RAG) have introduced a new paradigm for web search. However, the limited
context awareness of LLMs degrades their performance on RAG tasks. Existing
methods to enhance context awareness are often inefficient, incurring time or
memory overhead during inference, and many are tailored to specific position
embeddings. In this paper, we propose Position-Embedding-Agnostic attention
Re-weighting (PEAR), which enhances the context awareness of LLMs with zero
inference overhead. Specifically, on a proxy task focused on context copying,
we first detect heads which suppress the models' context awareness thereby
diminishing RAG performance. To weaken the impact of these heads, we re-weight
their outputs with learnable coefficients. The LLM (with frozen parameters) is
optimized by adjusting these coefficients to minimize loss on the proxy task.
As a result, the coefficients are optimized to values less than one, thereby
reducing their tendency to suppress RAG performance. During inference, the
optimized coefficients are fixed to re-weight these heads, regardless of the
specific task at hand. Our proposed PEAR offers two major advantages over
previous approaches: (1) It introduces zero additional inference overhead in
terms of memory usage or inference time, while outperforming competitive
baselines in accuracy and efficiency across various RAG tasks. (2) It is
independent of position embedding algorithms, ensuring broader applicability.",2024-09-29,"Tao Tan, Yining Qian, Ang Lv, Hongzhan Lin, Songhao Wu, Yongbo Wang, Feng Wang, Jingtong Wu, Xin Lu, Rui Yan",http://arxiv.org/pdf/2409.19745v2,cs.CL
"A Systematic Review of NLP for Dementia -- Tasks, Datasets and Opportunities","The close link between cognitive decline and language has fostered
long-standing collaboration between the NLP and medical communities in dementia
research. To examine this, we reviewed over 240 papers applying NLP to
dementia-related efforts, drawing from medical, technological, and NLP-focused
literature. We identify key research areas, including dementia detection,
linguistic biomarker extraction, caregiver support, and patient assistance,
showing that half of all papers focus solely on dementia detection using
clinical data. Yet, many directions remain unexplored -- artificially degraded
language models, synthetic data, digital twins, and more. We highlight gaps and
opportunities around trust, scientific rigor, applicability and cross-community
collaboration. We raise ethical dilemmas in the field, and highlight the
diverse datasets encountered throughout our review -- recorded, written,
structured, spontaneous, synthetic, clinical, social media-based, and more.
This review aims to inspire more creative, impactful, and rigorous research on
NLP for dementia.",2024-09-29,"Lotem Peled-Cohen, Roi Reichart",http://arxiv.org/pdf/2409.19737v2,cs.CL
Scrambled text: training Language Models to correct OCR errors using synthetic data,"OCR errors are common in digitised historical archives significantly
affecting their usability and value. Generative Language Models (LMs) have
shown potential for correcting these errors using the context provided by the
corrupted text and the broader socio-cultural context, a process called Context
Leveraging OCR Correction (CLOCR-C). However, getting sufficient training data
for fine-tuning such models can prove challenging. This paper shows that
fine-tuning a language model on synthetic data using an LM and using a
character level Markov corruption process can significantly improve the ability
to correct OCR errors. Models trained on synthetic data reduce the character
error rate by 55% and word error rate by 32% over the base LM and outperform
models trained on real data. Key findings include; training on under-corrupted
data is better than over-corrupted data; non-uniform character level corruption
is better than uniform corruption; More tokens-per-observation outperforms more
observations for a fixed token budget. The outputs for this paper are a set of
8 heuristics for training effective CLOCR-C models, a dataset of 11,000
synthetic 19th century newspaper articles and scrambledtext a python library
for creating synthetic corrupted data.",2024-09-29,Jonathan Bourne,http://arxiv.org/pdf/2409.19735v1,cs.CL
Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues,"Personality recognition aims to identify the personality traits implied in
user data such as dialogues and social media posts. Current research
predominantly treats personality recognition as a classification task, failing
to reveal the supporting evidence for the recognized personality. In this
paper, we propose a novel task named Explainable Personality Recognition,
aiming to reveal the reasoning process as supporting evidence of the
personality trait. Inspired by personality theories, personality traits are
made up of stable patterns of personality state, where the states are
short-term characteristic patterns of thoughts, feelings, and behaviors in a
concrete situation at a specific moment in time. We propose an explainable
personality recognition framework called Chain-of-Personality-Evidence (CoPE),
which involves a reasoning process from specific contexts to short-term
personality states to long-term personality traits. Furthermore, based on the
CoPE framework, we construct an explainable personality recognition dataset
from dialogues, PersonalityEvd. We introduce two explainable personality state
recognition and explainable personality trait recognition tasks, which require
models to recognize the personality state and trait labels and their
corresponding support evidence. Our extensive experiments based on Large
Language Models on the two tasks show that revealing personality traits is very
challenging and we present some insights for future research. Our data and code
are available at https://github.com/Lei-Sun-RUC/PersonalityEvd.",2024-09-29,"Lei Sun, Jinming Zhao, Qin Jin",http://arxiv.org/pdf/2409.19723v1,cs.CL
Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code,"This paper presents Coffee-Gym, a comprehensive RL environment for training
models that provide feedback on code editing. Coffee-Gym includes two major
components: (1) Coffee, a dataset containing humans' code edit traces for
coding questions and machine-written feedback for editing erroneous code; (2)
CoffeeEval, a reward function that faithfully reflects the helpfulness of
feedback by assessing the performance of the revised code in unit tests. With
them, Coffee-Gym addresses the unavailability of high-quality datasets for
training feedback models with RL, and provides more accurate rewards than the
SOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback
models that outperform baselines in enhancing open-source code LLMs' code
editing, making them comparable with closed-source LLMs. We make the dataset
and the model checkpoint publicly available.",2024-09-29,"Hyungjoo Chae, Taeyoon Kwon, Seungjun Moon, Yongho Song, Dongjin Kang, Kai Tzu-iunn Ong, Beong-woo Kwak, Seonghyeon Bae, Seung-won Hwang, Jinyoung Yeo",http://arxiv.org/pdf/2409.19715v2,cs.CL
A multimodal LLM for the non-invasive decoding of spoken text from brain recordings,"Brain-related research topics in artificial intelligence have recently gained
popularity, particularly due to the expansion of what multimodal architectures
can do from computer vision to natural language processing. Our main goal in
this work is to explore the possibilities and limitations of these
architectures in spoken text decoding from non-invasive fMRI recordings.
Contrary to vision and textual data, fMRI data represent a complex modality due
to the variety of brain scanners, which implies (i) the variety of the recorded
signal formats, (ii) the low resolution and noise of the raw signals, and (iii)
the scarcity of pretrained models that can be leveraged as foundation models
for generative learning. These points make the problem of the non-invasive
decoding of text from fMRI recordings very challenging. In this paper, we
propose and end-to-end multimodal LLM for decoding spoken text from fMRI
signals. The proposed architecture is founded on (i) an encoder derived from a
specific transformer incorporating an augmented embedding layer for the encoder
and a better-adjusted attention mechanism than that present in the state of the
art, and (ii) a frozen large language model adapted to align the embedding of
the input text and the encoded embedding of brain activity to decode the output
text. A benchmark in performed on a corpus consisting of a set of interactions
human-human and human-robot interactions where fMRI and conversational signals
are recorded synchronously. The obtained results are very promising, as our
proposal outperforms the evaluated models, and is able to generate text
capturing more accurate semantics present in the ground truth. The
implementation code is provided in https://github.com/Hmamouche/brain_decode.",2024-09-29,"Youssef Hmamouche, Ismail Chihab, Lahoucine Kdouri, Amal El Fallah Seghrouchni",http://arxiv.org/pdf/2409.19710v1,cs.CL
2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding for Large Language Models,"Tables are ubiquitous across various domains for concisely representing
structured information. Empowering large language models (LLMs) to reason over
tabular data represents an actively explored direction. However, since typical
LLMs only support one-dimensional~(1D) inputs, existing methods often flatten
the two-dimensional~(2D) table structure into a sequence of tokens, which can
severely disrupt the spatial relationships and result in an inevitable loss of
vital contextual information. In this paper, we first empirically demonstrate
the detrimental impact of such flattening operations on the performance of LLMs
in capturing the spatial information of tables through two elaborate proxy
tasks. Subsequently, we introduce a simple yet effective positional encoding
method, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to
address this challenge. 2D-TPE enables each attention head to dynamically
select a permutation order of tokens within the context for attending to them,
where each permutation represents a distinct traversal mode for the table, such
as column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of
losing essential spatial information while preserving computational efficiency,
thus better preserving the table structure. Extensive experiments across five
benchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring
the importance of preserving the table structure for accurate table
comprehension. Comprehensive analysis further reveals the substantially better
scalability of 2D-TPE to large tables than baselines.",2024-09-29,"Jia-Nan Li, Jian Guan, Wei Wu, Zhengtao Yu, Rui Yan",http://arxiv.org/pdf/2409.19700v3,cs.CL
CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays,"Existing rhetorical understanding and generation datasets or corpora
primarily focus on single coarse-grained categories or fine-grained categories,
neglecting the common interrelations between different rhetorical devices by
treating them as independent sub-tasks. In this paper, we propose the Chinese
Essay Rhetoric Dataset (CERD), consisting of 4 commonly used coarse-grained
categories including metaphor, personification, hyperbole and parallelism and
23 fine-grained categories across both form and content levels. CERD is a
manually annotated and comprehensive Chinese rhetoric dataset with five
interrelated sub-tasks. Unlike previous work, our dataset aids in understanding
various rhetorical devices, recognizing corresponding rhetorical components,
and generating rhetorical sentences under given conditions, thereby improving
the author's writing proficiency and language usage skills. Extensive
experiments are conducted to demonstrate the interrelations between multiple
tasks in CERD, as well as to establish a benchmark for future research on
rhetoric. The experimental results indicate that Large Language Models achieve
the best performance across most tasks, and jointly fine-tuning with multiple
tasks further enhances performance.",2024-09-29,"Nuowei Liu, Xinhao Chen, Hongyi Wu, Changzhi Sun, Man Lan, Yuanbin Wu, Xiaopeng Bai, Shaoguang Mao, Yan Xia",http://arxiv.org/pdf/2409.19691v1,cs.CL
Instruction Embedding: Latent Representations of Instructions Towards Task Identification,"Instruction data is crucial for improving the capability of Large Language
Models (LLMs) to align with human-level performance. Recent research LIMA
demonstrates that alignment is essentially a process where the model adapts
instructions' interaction style or format to solve various tasks, leveraging
pre-trained knowledge and skills. Therefore, for instructional data, the most
important aspect is the task it represents, rather than the specific semantics
and knowledge information. The latent representations of instructions play
roles for some instruction-related tasks like data selection and demonstrations
retrieval. However, they are always derived from text embeddings, encompass
overall semantic information that influences the representation of task
categories. In this work, we introduce a new concept, instruction embedding,
and construct Instruction Embedding Benchmark (IEB) for its training and
evaluation. Then, we propose a baseline Prompt-based Instruction Embedding
(PIE) method to make the representations more attention on tasks. The
evaluation of PIE, alongside other embedding methods on IEB with two designed
tasks, demonstrates its superior performance in accurately identifying task
categories. Moreover, the application of instruction embeddings in four
downstream tasks showcases its effectiveness and suitability for
instruction-related tasks.",2024-09-29,"Yiwei Li, Jiayi Shi, Shaoxiong Feng, Peiwen Yuan, Xinglin Wang, Boyuan Pan, Heda Wang, Yao Hu, Kan Li",http://arxiv.org/pdf/2409.19680v1,cs.CL
Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding,"Modeling and leveraging layout reading order in visually-rich documents
(VrDs) is critical in document intelligence as it captures the rich structure
semantics within documents. Previous works typically formulated layout reading
order as a permutation of layout elements, i.e. a sequence containing all the
layout elements. However, we argue that this formulation does not adequately
convey the complete reading order information in the layout, which may
potentially lead to performance decline in downstream VrD tasks. To address
this issue, we propose to model the layout reading order as ordering relations
over the set of layout elements, which have sufficient expressive capability
for the complete reading order information. To enable empirical evaluation on
methods towards the improved form of reading order prediction (ROP), we
establish a comprehensive benchmark dataset including the reading order
annotation as relations over layout elements, together with a
relation-extraction-based method that outperforms previous methods. Moreover,
to highlight the practical benefits of introducing the improved form of layout
reading order, we propose a reading-order-relation-enhancing pipeline to
improve model performance on any arbitrary VrD task by introducing additional
reading order relation inputs. Comprehensive results demonstrate that the
pipeline generally benefits downstream VrD tasks: (1) with utilizing the
reading order relation information, the enhanced downstream models achieve SOTA
results on both two task settings of the targeted dataset; (2) with utilizing
the pseudo reading order information generated by the proposed ROP model, the
performance of the enhanced models has improved across all three models and
eight cross-domain VrD-IE/QA task settings without targeted optimization.",2024-09-29,"Chong Zhang, Yi Tu, Yixi Zhao, Chenshu Yuan, Huan Chen, Yue Zhang, Mingxu Chai, Ya Guo, Huijia Zhu, Qi Zhang, Tao Gui",http://arxiv.org/pdf/2409.19672v1,cs.CL
"Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models","The need to analyze graphs is ubiquitous across various fields, from social
networks to biological research and recommendation systems. Therefore, enabling
the ability of large language models (LLMs) to process graphs is an important
step toward more advanced general intelligence. However, current LLM benchmarks
on graph analysis require models to directly reason over the prompts describing
graph topology, and are thus limited to small graphs with only a few dozens of
nodes. In contrast, human experts typically write programs based on popular
libraries for task solving, and can thus handle graphs with different scales.
To this end, a question naturally arises: can LLMs analyze graphs like
professionals? In this paper, we introduce ProGraph, a manually crafted
benchmark containing 3 categories of graph tasks. The benchmark expects
solutions based on programming instead of directly reasoning over raw inputs.
Our findings reveal that the performance of current LLMs is unsatisfactory,
with the best model achieving only 36% accuracy. To bridge this gap, we propose
LLM4Graph datasets, which include crawled documents and auto-generated codes
based on 6 widely used graph libraries. By augmenting closed-source LLMs with
document retrieval and fine-tuning open-source ones on the codes, we show
11-32% absolute improvements in their accuracies. Our results underscore that
the capabilities of LLMs in handling structured data are still under-explored,
and show the effectiveness of LLM4Graph in enhancing LLMs' proficiency of graph
analysis. The benchmark, datasets and enhanced open-source models are available
at https://github.com/BUPT-GAMMA/ProGraph.",2024-09-29,"Xin Sky Li, Weize Chen, Qizhi Chu, Haopeng Li, Zhaojun Sun, Ran Li, Chen Qian, Yiwei Wei, Zhiyuan Liu, Chuan Shi, Maosong Sun, Cheng Yang",http://arxiv.org/pdf/2409.19667v3,cs.CL
Identifying Knowledge Editing Types in Large Language Models,"Knowledge editing has emerged as an efficient technique for updating the
knowledge of large language models (LLMs), attracting increasing attention in
recent years. However, there is a lack of effective measures to prevent the
malicious misuse of this technique, which could lead to harmful edits in LLMs.
These malicious modifications could cause LLMs to generate toxic content,
misleading users into inappropriate actions. In front of this risk, we
introduce a new task, $\textbf{K}$nowledge $\textbf{E}$diting $\textbf{T}$ype
$\textbf{I}$dentification (KETI), aimed at identifying different types of edits
in LLMs, thereby providing timely alerts to users when encountering illicit
edits. As part of this task, we propose KETIBench, which includes five types of
harmful edits covering the most popular toxic types, as well as one benign
factual edit. We develop five classical classification models and three
BERT-based models as baseline identifiers for both open-source and
closed-source LLMs. Our experimental results, across 92 trials involving four
models and three knowledge editing methods, demonstrate that all eight baseline
identifiers achieve decent identification performance, highlighting the
feasibility of identifying malicious edits in LLMs. Additional analyses reveal
that the performance of the identifiers is independent of the reliability of
the knowledge editing methods and exhibits cross-domain generalization,
enabling the identification of edits from unknown sources. All data and code
are available in https://github.com/xpq-tech/KETI.",2024-09-29,"Xiaopeng Li, Shasha Li, Shangwen Wang, Shezheng Song, Bin Ji, Huijun Liu, Jun Ma, Jie Yu",http://arxiv.org/pdf/2409.19663v3,cs.CL
Multimodal Misinformation Detection by Learning from Synthetic Data with Multimodal LLMs,"Detecting multimodal misinformation, especially in the form of image-text
pairs, is crucial. Obtaining large-scale, high-quality real-world fact-checking
datasets for training detectors is costly, leading researchers to use synthetic
datasets generated by AI technologies. However, the generalizability of
detectors trained on synthetic data to real-world scenarios remains unclear due
to the distribution gap. To address this, we propose learning from synthetic
data for detecting real-world multimodal misinformation through two
model-agnostic data selection methods that match synthetic and real-world data
distributions. Experiments show that our method enhances the performance of a
small MLLM (13B) on real-world fact-checking datasets, enabling it to even
surpass GPT-4V~\cite{GPT-4V}.",2024-09-29,"Fengzhu Zeng, Wenqian Li, Wei Gao, Yan Pang",http://arxiv.org/pdf/2409.19656v1,cs.CL
Assessment and manipulation of latent constructs in pre-trained language models using psychometric scales,"Human-like personality traits have recently been discovered in large language
models, raising the hypothesis that their (known and as yet undiscovered)
biases conform with human latent psychological constructs. While large
conversational models may be tricked into answering psychometric
questionnaires, the latent psychological constructs of thousands of simpler
transformers, trained for other tasks, cannot be assessed because appropriate
psychometric methods are currently lacking. Here, we show how standard
psychological questionnaires can be reformulated into natural language
inference prompts, and we provide a code library to support the psychometric
assessment of arbitrary models. We demonstrate, using a sample of 88 publicly
available models, the existence of human-like mental health-related constructs
(including anxiety, depression, and Sense of Coherence) which conform with
standard theories in human psychology and show similar correlations and
mitigation strategies. The ability to interpret and rectify the performance of
language models by using psychological tools can boost the development of more
explainable, controllable, and trustworthy models.",2024-09-29,"Maor Reuben, Ortal Slobodin, Aviad Elyshar, Idan-Chaim Cohen, Orna Braun-Lewensohn, Odeya Cohen, Rami Puzis",http://arxiv.org/pdf/2409.19655v2,cs.CL
Learning Attentional Mixture of LoRAs for Language Model Continual Learning,"Fine-tuning large language models (LLMs) with Low-Rank adaption (LoRA) is
widely acknowledged as an effective approach for continual learning for new
tasks. However, it often suffers from catastrophic forgetting when dealing with
multiple tasks sequentially. To this end, we propose Attentional Mixture of
LoRAs (AM-LoRA), a continual learning approach tailored for LLMs. Specifically,
AM-LoRA learns a sequence of LoRAs for a series of tasks to continually learn
knowledge from different tasks. The key of our approach is that we devise an
attention mechanism as a knowledge mixture module to adaptively integrate
information from each LoRA. With the attention mechanism, AM-LoRA can
efficiently leverage the distinctive contributions of each LoRA, while
mitigating the risk of mutually negative interactions among them that may lead
to catastrophic forgetting. Moreover, we further introduce $L1$ norm in the
learning process to make the attention vector more sparse. The sparse
constraints can enable the model to lean towards selecting a few highly
relevant LoRAs, rather than aggregating and weighting all LoRAs collectively,
which can further reduce the impact stemming from mutual interference.
Experimental results on continual learning benchmarks indicate the superiority
of our proposed method.",2024-09-29,"Jialin Liu, Jianhua Wu, Jie Liu, Yutai Duan",http://arxiv.org/pdf/2409.19611v1,cs.CL
Federated Learning from Vision-Language Foundation Models: Theoretical Analysis and Method,"Integrating pretrained vision-language foundation models like CLIP into
federated learning has attracted significant attention for enhancing
generalization across diverse tasks. Typically, federated learning of
vision-language models employs prompt learning to reduce communication and
computational costs, i.e., prompt-based federated learning. However, there is
limited theoretical analysis to understand the performance of prompt-based
federated learning. In this work, we construct a theoretical analysis framework
for prompt-based federated learning via feature learning theory. Specifically,
we monitor the evolution of signal learning and noise memorization in
prompt-based federated learning, demonstrating that performance can be assessed
by the ratio of task-relevant to task-irrelevant coefficients. Furthermore, we
draw an analogy between income and risk in portfolio optimization and the
task-relevant and task-irrelevant terms in feature learning. Leveraging
inspiration from portfolio optimization that combining two independent assets
will maintain the income while reducing the risk, we introduce two prompts:
global prompt and local prompt to construct a prompt portfolio to balance the
generalization and personalization. Consequently, we showed the performance
advantage of the prompt portfolio and derived the optimal mixing coefficient.
These theoretical claims have been further supported by empirical experiments.",2024-09-29,"Bikang Pan, Wei Huang, Ye Shi",http://arxiv.org/pdf/2409.19610v1,cs.CL
Hyper-Connections,"We present hyper-connections, a simple yet effective method that can serve as
an alternative to residual connections. This approach specifically addresses
common drawbacks observed in residual connection variants, such as the seesaw
effect between gradient vanishing and representation collapse. Theoretically,
hyper-connections allow the network to adjust the strength of connections
between features at different depths and dynamically rearrange layers. We
conduct experiments focusing on the pre-training of large language models,
including dense and sparse models, where hyper-connections show significant
performance improvements over residual connections. Additional experiments
conducted on vision tasks also demonstrate similar improvements. We anticipate
that this method will be broadly applicable and beneficial across a wide range
of AI problems.",2024-09-29,"Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu, Qiyang Min, Xun Zhou",http://arxiv.org/pdf/2409.19606v3,cs.CL
The Crucial Role of Samplers in Online Direct Preference Optimization,"Direct Preference Optimization (DPO) has emerged as a stable, scalable, and
efficient solution for language model alignment. Despite its empirical success,
the optimization properties, particularly the impact of samplers on its
convergence rates, remain under-explored. In this paper, we provide a rigorous
analysis of DPO's convergence rates with different sampling strategies under
the exact gradient setting, revealing a surprising separation: uniform sampling
achieves $\textbf{linear}$ convergence, while our proposed online sampler
achieves $\textbf{quadratic}$ convergence. We further adapt the sampler to
practical settings by incorporating posterior distributions and logit mixing,
demonstrating improvements over previous methods. For example, it outperforms
vanilla DPO by over $7.4$% on Safe-RLHF dataset. Our results not only offer
insights into the theoretical understanding of DPO but also pave the way for
further algorithm designs.",2024-09-29,"Ruizhe Shi, Runlong Zhou, Simon S. Du",http://arxiv.org/pdf/2409.19605v3,cs.CL
Two-stage Framework for Robust Speech Emotion Recognition Using Target Speaker Extraction in Human Speech Noise Conditions,"Developing a robust speech emotion recognition (SER) system in noisy
conditions faces challenges posed by different noise properties. Most previous
studies have not considered the impact of human speech noise, thus limiting the
application scope of SER. In this paper, we propose a novel two-stage framework
for the problem by cascading target speaker extraction (TSE) method and SER. We
first train a TSE model to extract the speech of target speaker from a mixture.
Then, in the second stage, we utilize the extracted speech for SER training.
Additionally, we explore a joint training of TSE and SER models in the second
stage. Our developed system achieves a 14.33% improvement in unweighted
accuracy (UA) compared to a baseline without using TSE method, demonstrating
the effectiveness of our framework in mitigating the impact of human speech
noise. Moreover, we conduct experiments considering speaker gender, showing
that our framework performs particularly well in different-gender mixture.",2024-09-29,"Jinyi Mi, Xiaohan Shi, Ding Ma, Jiajun He, Takuya Fujimura, Tomoki Toda",http://arxiv.org/pdf/2409.19585v2,cs.CL
DiMB-RE: Mining the Scientific Literature for Diet-Microbiome Associations,"Objective: To develop a corpus annotated for diet-microbiome associations
from the biomedical literature and train natural language processing (NLP)
models to identify these associations, thereby improving the understanding of
their role in health and disease, and supporting personalized nutrition
strategies. Materials and Methods: We constructed DiMB-RE, a comprehensive
corpus annotated with 15 entity types (e.g., Nutrient, Microorganism) and 13
relation types (e.g., INCREASES, IMPROVES) capturing diet-microbiome
associations. We fine-tuned and evaluated state-of-the-art NLP models for named
entity, trigger, and relation extraction as well as factuality detection using
DiMB-RE. In addition, we benchmarked two generative large language models
(GPT-4o-mini and GPT-4o) on a subset of the dataset in zero- and one-shot
settings. Results: DiMB-RE consists of 14,450 entities and 4,206 relationships
from 165 publications (including 30 full-text Results sections). Fine-tuned NLP
models performed reasonably well for named entity recognition (0.800 F1 score),
while end-to-end relation extraction performance was modest (0.445 F1). The use
of Results section annotations improved relation extraction. The impact of
trigger detection was mixed. Generative models showed lower accuracy compared
to fine-tuned models. Discussion: To our knowledge, DiMB-RE is the largest and
most diverse corpus focusing on diet-microbiome interactions. NLP models
fine-tuned on DiMB-RE exhibit lower performance compared to similar corpora,
highlighting the complexity of information extraction in this domain.
Misclassified entities, missed triggers, and cross-sentence relations are the
major sources of relation extraction errors. Conclusions: DiMB-RE can serve as
a benchmark corpus for biomedical literature mining. DiMB-RE and the NLP models
are available at https://github.com/ScienceNLP-Lab/DiMB-RE.",2024-09-29,"Gibong Hong, Veronica Hindle, Nadine M. Veasley, Hannah D. Holscher, Halil Kilicoglu",http://arxiv.org/pdf/2409.19581v2,cs.CL
Quantitative Analysis of Audio-Visual Tasks: An Information-Theoretic Perspective,"In the field of spoken language processing, audio-visual speech processing is
receiving increasing research attention. Key components of this research
include tasks such as lip reading, audio-visual speech recognition, and
visual-to-speech synthesis. Although significant success has been achieved,
theoretical analysis is still insufficient for audio-visual tasks. This paper
presents a quantitative analysis based on information theory, focusing on
information intersection between different modalities. Our results show that
this analysis is valuable for understanding the difficulties of audio-visual
processing tasks as well as the benefits that could be obtained by modality
integration.",2024-09-29,"Chen Chen, Xiaolou Li, Zehua Liu, Lantian Li, Dong Wang",http://arxiv.org/pdf/2409.19575v1,cs.CL
Mitigating the Negative Impact of Over-association for Conversational Query Production,"Conversational query generation aims at producing search queries from
dialogue histories, which are then used to retrieve relevant knowledge from a
search engine to help knowledge-based dialogue systems. Trained to maximize the
likelihood of gold queries, previous models suffer from the data hunger issue,
and they tend to both drop important concepts from dialogue histories and
generate irrelevant concepts at inference time. We attribute these issues to
the over-association phenomenon where a large number of gold queries are
indirectly related to the dialogue topics, because annotators may unconsciously
perform reasoning with their background knowledge when generating these gold
queries. We carefully analyze the negative effects of this phenomenon on
pretrained Seq2seq query producers and then propose effective instance-level
weighting strategies for training to mitigate these issues from multiple
perspectives. Experiments on two benchmarks, Wizard-of-Internet and DuSinc,
show that our strategies effectively alleviate the negative effects and lead to
significant performance gains (2%-5% across automatic metrics and human
evaluation). Further analysis shows that our model selects better concepts from
dialogue histories and is 10 times more data efficient than the baseline. The
code is available at https://github.com/DeepLearnXMU/QG-OverAsso.",2024-09-29,"Ante Wang, Linfeng Song, Zijun Min, Ge Xu, Xiaoli Wang, Junfeng Yao, Jinsong Su",http://arxiv.org/pdf/2409.19572v1,cs.CL
Abstractive Summarization of Low resourced Nepali language using Multilingual Transformers,"Automatic text summarization in Nepali language is an unexplored area in
natural language processing (NLP). Although considerable research has been
dedicated to extractive summarization, the area of abstractive summarization,
especially for low-resource languages such as Nepali, remains largely
unexplored. This study explores the use of multilingual transformer models,
specifically mBART and mT5, for generating headlines for Nepali news articles
through abstractive summarization. The research addresses key challenges
associated with summarizing texts in Nepali by first creating a summarization
dataset through web scraping from various Nepali news portals. These
multilingual models were then fine-tuned using different strategies. The
performance of the fine-tuned models were then assessed using ROUGE scores and
human evaluation to ensure the generated summaries were coherent and conveyed
the original meaning. During the human evaluation, the participants were asked
to select the best summary among those generated by the models, based on
criteria such as relevance, fluency, conciseness, informativeness, factual
accuracy, and coverage. During the evaluation with ROUGE scores, the 4-bit
quantized mBART with LoRA model was found to be effective in generating better
Nepali news headlines in comparison to other models and also it was selected
34.05% of the time during the human evaluation, outperforming all other
fine-tuned models created for Nepali News headline generation.",2024-09-29,"Prakash Dhakal, Daya Sagar Baral",http://arxiv.org/pdf/2409.19566v1,cs.CL
Human Bias in the Face of AI: The Role of Human Judgement in AI Generated Text Evaluation,"As AI advances in text generation, human trust in AI generated content
remains constrained by biases that go beyond concerns of accuracy. This study
explores how bias shapes the perception of AI versus human generated content.
Through three experiments involving text rephrasing, news article
summarization, and persuasive writing, we investigated how human raters respond
to labeled and unlabeled content. While the raters could not differentiate the
two types of texts in the blind test, they overwhelmingly favored content
labeled as ""Human Generated,"" over those labeled ""AI Generated,"" by a
preference score of over 30%. We observed the same pattern even when the labels
were deliberately swapped. This human bias against AI has broader societal and
cognitive implications, as it undervalues AI performance. This study highlights
the limitations of human judgment in interacting with AI and offers a
foundation for improving human-AI collaboration, especially in creative fields.",2024-09-29,"Tiffany Zhu, Iain Weissburg, Kexun Zhang, William Yang Wang",http://arxiv.org/pdf/2410.03723v1,cs.CL
Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization,"Language models frequently inherit societal biases from their training data.
Numerous techniques have been proposed to mitigate these biases during both the
pre-training and fine-tuning stages. However, fine-tuning a pre-trained
debiased language model on a downstream task can reintroduce biases into the
model. Additionally, existing debiasing methods for downstream tasks either (i)
require labels of protected attributes (e.g., age, race, or political views)
that are often not available or (ii) rely on indicators of bias, which
restricts their applicability to gender debiasing since they rely on
gender-specific words. To address this, we introduce a novel debiasing
regularization technique based on the class-wise variance of embeddings.
Crucially, our method does not require attribute labels and targets any
attribute, thus addressing the shortcomings of existing debiasing methods. Our
experiments on encoder language models and three datasets demonstrate that our
method outperforms existing strong debiasing baselines that rely on target
attribute labels while maintaining performance on the target task.",2024-09-29,"Shahed Masoudian, Markus Frohmann, Navid Rekabsaz, Markus Schedl",http://arxiv.org/pdf/2409.19541v3,cs.CL
Mixed Chain-of-Psychotherapies for Emotional Support Chatbot,"In the realm of mental health support chatbots, it is vital to show empathy
and encourage self-exploration to provide tailored solutions. However, current
approaches tend to provide general insights or solutions without fully
understanding the help-seeker's situation. Therefore, we propose PsyMix, a
chatbot that integrates the analyses of the seeker's state from the perspective
of a psychotherapy approach (Chain-of-Psychotherapies, CoP) before generating
the response, and learns to incorporate the strength of various psychotherapies
by fine-tuning on a mixture of CoPs. Through comprehensive evaluation, we found
that PsyMix can outperform the ChatGPT baseline, and demonstrate a comparable
level of empathy in its responses to that of human counselors.",2024-09-29,"Siyuan Chen, Cong Ming, Zhiling Zhang, Yanyi Chen, Kenny Q. Zhu, Mengyue Wu",http://arxiv.org/pdf/2409.19533v1,cs.CL
Video DataFlywheel: Resolving the Impossible Data Trinity in Video-Language Understanding,"Recently, video-language understanding has achieved great success through
large-scale pre-training. However, data scarcity remains a prevailing
challenge. This study quantitatively reveals an ""impossible trinity"" among data
quantity, diversity, and quality in pre-training datasets. Recent efforts seek
to refine large-scale, diverse ASR datasets compromised by low quality through
synthetic annotations. These methods successfully leverage useful information
in multimodal video content (frames, tags, ASR transcripts, etc.) to refine the
original annotations. Nevertheless, they struggle to mitigate noise within
synthetic annotations and lack scalability as the dataset size expands. To
address these issues, we introduce the Video DataFlywheel framework, which
iteratively refines video annotations with improved noise control methods. For
iterative refinement, we first leverage a video-language model to generate
synthetic annotations, resulting in a refined dataset. Then, we pre-train on it
and fine-tune on human refinement examples for a stronger model. These
processes are repeated for continuous improvement. For noise control, we
present AdaTaiLr, a novel noise control method that requires weaker assumptions
on noise distribution, thereby proving more effective in large datasets with
theoretical guarantees. The combination of iterative refinement and AdaTaiLr
can achieve better scalability in video-language understanding. Extensive
experiments show that our framework outperforms existing data refinement
baselines, delivering a 3% performance boost and improving dataset quality with
minimal diversity loss. Furthermore, our refined dataset facilitates
significant improvements in various video-language understanding tasks,
including video question answering and text-video retrieval.",2024-09-29,"Xiao Wang, Jianlong Wu, Zijia Lin, Fuzheng Zhang, Di Zhang, Liqiang Nie",http://arxiv.org/pdf/2409.19532v1,cs.CL
LANDeRMT: Detecting and Routing Language-Aware Neurons for Selectively Finetuning LLMs to Machine Translation,"Recent advancements in large language models (LLMs) have shown promising
results in multilingual translation even with limited bilingual supervision.
The major challenges are catastrophic forgetting and parameter interference for
finetuning LLMs when provided parallel training data. To address these
challenges, we propose LANDeRMT, a \textbf{L}anguage-\textbf{A}ware
\textbf{N}euron \textbf{De}tecting and \textbf{R}outing framework that
selectively finetunes LLMs to \textbf{M}achine \textbf{T}ranslation with
diverse translation training data. In LANDeRMT, we evaluate the awareness of
neurons to MT tasks and categorize them into language-general and
language-specific neurons. This categorization enables selective parameter
updates during finetuning, mitigating parameter interference and catastrophic
forgetting issues. For the detected neurons, we further propose a conditional
awareness-based routing mechanism to dynamically adjust language-general and
language-specific capacity within LLMs, guided by translation signals.
Experimental results demonstrate that the proposed LANDeRMT is very effective
in learning translation knowledge, significantly improving translation quality
over various strong baselines for multiple language pairs.",2024-09-29,"Shaolin Zhu, Leiyu Pan, Bo Li, Deyi Xiong",http://arxiv.org/pdf/2409.19523v1,cs.CL
CoT-ST: Enhancing LLM-based Speech Translation with Multimodal Chain-of-Thought,"Speech Language Models (SLMs) have demonstrated impressive performance on
speech translation tasks. However, existing research primarily focuses on
direct instruction fine-tuning and often overlooks the inherent reasoning
capabilities of SLMs. In this paper, we introduce a three-stage training
framework designed to activate the chain-of-thought (CoT) capabilities of SLMs.
We propose CoT-ST, a speech translation model that utilizes multimodal CoT to
decompose speech translation into sequential steps of speech recognition and
translation. We validated the effectiveness of our method on two datasets: the
CoVoST-2 dataset and MuST-C dataset. The experimental results demonstrate that
CoT-ST outperforms previous state-of-the-art methods, achieving higher BLEU
scores (CoVoST-2 en-ja: 30.5->30.8, en-zh: 45.2->47.7, MuST-C en-zh:
19.6->21.2). This work is open sourced at
https://github.com/X-LANCE/SLAM-LLM/tree/main/examples/st_covost2 .",2024-09-29,"Yexing Du, Ziyang Ma, Yifan Yang, Keqi Deng, Xie Chen, Bo Yang, Yang Xiang, Ming Liu, Bing Qin",http://arxiv.org/pdf/2409.19510v1,cs.CL
Transforming Scholarly Landscapes: Influence of Large Language Models on Academic Fields beyond Computer Science,"Large Language Models (LLMs) have ushered in a transformative era in Natural
Language Processing (NLP), reshaping research and extending NLP's influence to
other fields of study. However, there is little to no work examining the degree
to which LLMs influence other research fields. This work empirically and
systematically examines the influence and use of LLMs in fields beyond NLP. We
curate $106$ LLMs and analyze $\sim$$148k$ papers citing LLMs to quantify their
influence and reveal trends in their usage patterns. Our analysis reveals not
only the increasing prevalence of LLMs in non-CS fields but also the
disparities in their usage, with some fields utilizing them more frequently
than others since 2018, notably Linguistics and Engineering together accounting
for $\sim$$45\%$ of LLM citations. Our findings further indicate that most of
these fields predominantly employ task-agnostic LLMs, proficient in zero or
few-shot learning without requiring further fine-tuning, to address their
domain-specific problems. This study sheds light on the cross-disciplinary
impact of NLP through LLMs, providing a better understanding of the
opportunities and challenges.",2024-09-29,"Aniket Pramanick, Yufang Hou, Saif M. Mohammad, Iryna Gurevych",http://arxiv.org/pdf/2409.19508v1,cs.CL
A Critical Look at Meta-evaluating Summarisation Evaluation Metrics,"Effective summarisation evaluation metrics enable researchers and
practitioners to compare different summarisation systems efficiently.
Estimating the effectiveness of an automatic evaluation metric, termed
meta-evaluation, is a critically important research question. In this position
paper, we review recent meta-evaluation practices for summarisation evaluation
metrics and find that (1) evaluation metrics are primarily meta-evaluated on
datasets consisting of examples from news summarisation datasets, and (2) there
has been a noticeable shift in research focus towards evaluating the
faithfulness of generated summaries. We argue that the time is ripe to build
more diverse benchmarks that enable the development of more robust evaluation
metrics and analyze the generalization ability of existing evaluation metrics.
In addition, we call for research focusing on user-centric quality dimensions
that consider the generated summary's communicative goal and the role of
summarisation in the workflow.",2024-09-29,"Xiang Dai, Sarvnaz Karimi, Biaoyan Fang",http://arxiv.org/pdf/2409.19507v1,cs.CL
The Nature of NLP: Analyzing Contributions in NLP Papers,"Natural Language Processing (NLP) is a dynamic, interdisciplinary field that
integrates intellectual traditions from computer science, linguistics, social
science, and more. Despite its established presence, the definition of what
constitutes NLP research remains debated. In this work, we quantitatively
investigate what constitutes NLP by examining research papers. For this
purpose, we propose a taxonomy and introduce NLPContributions, a dataset of
nearly $2k$ research paper abstracts, expertly annotated to identify scientific
contributions and classify their types according to this taxonomy. We also
propose a novel task to automatically identify these elements, for which we
train a strong baseline on our dataset. We present experimental results from
this task and apply our model to $\sim$$29k$ NLP research papers to analyze
their contributions, aiding in the understanding of the nature of NLP research.
Our findings reveal a rising involvement of machine learning in NLP since the
early nineties, alongside a declining focus on adding knowledge about language
or people; again, in post-2020, there has been a resurgence of focus on
language and people. We hope this work will spark discussions on our community
norms and inspire efforts to consciously shape the future.",2024-09-29,"Aniket Pramanick, Yufang Hou, Saif M. Mohammad, Iryna Gurevych",http://arxiv.org/pdf/2409.19505v1,cs.CL
MedHalu: Hallucinations in Responses to Healthcare Queries by Large Language Models,"The remarkable capabilities of large language models (LLMs) in language
understanding and generation have not rendered them immune to hallucinations.
LLMs can still generate plausible-sounding but factually incorrect or
fabricated information. As LLM-empowered chatbots become popular, laypeople may
frequently ask health-related queries and risk falling victim to these LLM
hallucinations, resulting in various societal and healthcare implications. In
this work, we conduct a pioneering study of hallucinations in LLM-generated
responses to real-world healthcare queries from patients. We propose MedHalu, a
carefully crafted first-of-its-kind medical hallucination dataset with a
diverse range of health-related topics and the corresponding hallucinated
responses from LLMs with labeled hallucination types and hallucinated text
spans. We also introduce MedHaluDetect framework to evaluate capabilities of
various LLMs in detecting hallucinations. We also employ three groups of
evaluators -- medical experts, LLMs, and laypeople -- to study who are more
vulnerable to these medical hallucinations. We find that LLMs are much worse
than the experts. They also perform no better than laypeople and even worse in
few cases in detecting hallucinations. To fill this gap, we propose
expert-in-the-loop approach to improve hallucination detection through LLMs by
infusing expert reasoning. We observe significant performance gains for all the
LLMs with an average macro-F1 improvement of 6.3 percentage points for GPT-4.",2024-09-29,"Vibhor Agarwal, Yiqiao Jin, Mohit Chandra, Munmun De Choudhury, Srijan Kumar, Nishanth Sastry",http://arxiv.org/pdf/2409.19492v1,cs.CL
HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare Conversations,"Effective patient care in digital healthcare requires large language models
(LLMs) that not only answer questions but also actively gather critical
information through well-crafted inquiries. This paper introduces HealthQ, a
novel framework for evaluating the questioning capabilities of LLM healthcare
chains. By implementing advanced LLM chains, including Retrieval-Augmented
Generation (RAG), Chain of Thought (CoT), and reflective chains, HealthQ
assesses how effectively these chains elicit comprehensive and relevant patient
information. To achieve this, we integrate an LLM judge to evaluate generated
questions across metrics such as specificity, relevance, and usefulness, while
aligning these evaluations with traditional Natural Language Processing (NLP)
metrics like ROUGE and Named Entity Recognition (NER)-based set comparisons. We
validate HealthQ using two custom datasets constructed from public medical
datasets, ChatDoctor and MTS-Dialog, and demonstrate its robustness across
multiple LLM judge models, including GPT-3.5, GPT-4, and Claude. Our
contributions are threefold: we present the first systematic framework for
assessing questioning capabilities in healthcare conversations, establish a
model-agnostic evaluation methodology, and provide empirical evidence linking
high-quality questions to improved patient information elicitation.",2024-09-28,"Ziyu Wang, Hao Li, Di Huang, Hye-Sung Kim, Chae-Won Shin, Amir M. Rahmani",http://arxiv.org/pdf/2409.19487v4,cs.CL
MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation,"Segmentation of anatomical structures and pathological regions in medical
images is essential for modern clinical diagnosis, disease research, and
treatment planning. While significant advancements have been made in deep
learning-based segmentation techniques, many of these methods still suffer from
limitations in data efficiency, generalizability, and interactivity. As a
result, developing precise segmentation methods that require fewer labeled
datasets remains a critical challenge in medical image analysis. Recently, the
introduction of foundation models like CLIP and Segment-Anything-Model (SAM),
with robust cross-domain representations, has paved the way for interactive and
universal image segmentation. However, further exploration of these models for
data-efficient segmentation in medical imaging is still needed and highly
relevant. In this paper, we introduce MedCLIP-SAMv2, a novel framework that
integrates the CLIP and SAM models to perform segmentation on clinical scans
using text prompts, in both zero-shot and weakly supervised settings. Our
approach includes fine-tuning the BiomedCLIP model with a new Decoupled Hard
Negative Noise Contrastive Estimation (DHN-NCE) loss, and leveraging the
Multi-modal Information Bottleneck (M2IB) to create visual prompts for
generating segmentation masks from SAM in the zero-shot setting. We also
investigate using zero-shot segmentation labels within a weakly supervised
paradigm to enhance segmentation quality further. Extensive testing across four
diverse segmentation tasks and medical imaging modalities (breast tumor
ultrasound, brain tumor MRI, lung X-ray, and lung CT) demonstrates the high
accuracy of our proposed framework. Our code is available at
https://github.com/HealthX-Lab/MedCLIP-SAMv2.",2024-09-28,"Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao",http://arxiv.org/pdf/2409.19483v4,cs.CL
Overriding Safety protections of Open-source Models,"LLMs(Large Language Models) nowadays have widespread adoption as a tool for
solving issues across various domain/tasks. These models since are susceptible
to produce harmful or toxic results, inference-time adversarial attacks,
therefore they do undergo safety alignment training and Red teaming for putting
in safety guardrails. For using these models, usually fine-tuning is done for
model alignment on the desired tasks, which can make model more aligned but
also make it more susceptible to produce unsafe responses, if fine-tuned with
harmful data.In this paper, we study how much of impact introduction of harmful
data in fine-tuning can make, and if it can override the safety protection of
those models. Conversely,it was also explored that if model is fine-tuned on
safety data can make the model produce more safer responses. Further we explore
if fine-tuning the model on harmful data makes it less helpful or less
trustworthy because of increase in model uncertainty leading to knowledge
drift. Our extensive experimental results shown that Safety protection in an
open-source can be overridden, when fine-tuned with harmful data as observed by
ASR increasing by 35% when compared to basemodel's ASR. Also, as observed,
fine-tuning a model with harmful data made the harmful fine-tuned model highly
uncertain with huge knowledge drift and less truthfulness in its responses.
Furthermore, for the safe fine-tuned model, ASR decreases by 51.68% as compared
to the basemodel, and Safe model also shown in minor drop in uncertainty and
truthfulness as compared to basemodel. This paper's code is available at:
https://github.com/techsachinkr/Overriding_Model_Safety_Protections",2024-09-28,Sachin Kumar,http://arxiv.org/pdf/2409.19476v1,cs.CL
SELP: Generating Safe and Efficient Task Plans for Robot Agents with Large Language Models,"Despite significant advancements in large language models (LLMs) that enhance
robot agents' understanding and execution of natural language (NL) commands,
ensuring the agents adhere to user-specified constraints remains challenging,
particularly for complex commands and long-horizon tasks. To address this
challenge, we present three key insights, equivalence voting, constrained
decoding, and domain-specific fine-tuning, which significantly enhance LLM
planners' capability in handling complex tasks. Equivalence voting ensures
consistency by generating and sampling multiple Linear Temporal Logic (LTL)
formulas from NL commands, grouping equivalent LTL formulas, and selecting the
majority group of formulas as the final LTL formula. Constrained decoding then
uses the generated LTL formula to enforce the autoregressive inference of
plans, ensuring the generated plans conform to the LTL. Domain-specific
fine-tuning customizes LLMs to produce safe and efficient plans within specific
task domains. Our approach, Safe Efficient LLM Planner (SELP), combines these
insights to create LLM planners to generate plans adhering to user commands
with high confidence. We demonstrate the effectiveness and generalizability of
SELP across different robot agents and tasks, including drone navigation and
robot manipulation. For drone navigation tasks, SELP outperforms
state-of-the-art planners by 10.8% in safety rate (i.e., finishing tasks
conforming to NL commands) and by 19.8% in plan efficiency. For robot
manipulation tasks, SELP achieves 20.4% improvement in safety rate. Our
datasets for evaluating NL-to-LTL and robot task planning will be released in
github.com/lt-asset/selp.",2024-09-28,"Yi Wu, Zikang Xiong, Yiran Hu, Shreyash S. Iyengar, Nan Jiang, Aniket Bera, Lin Tan, Suresh Jagannathan",http://arxiv.org/pdf/2409.19471v2,cs.CL
INSIGHTBUDDY-AI: Medication Extraction and Entity Linking using Large Language Models and Ensemble Learning,"Medication Extraction and Mining play an important role in healthcare NLP
research due to its practical applications in hospital settings, such as their
mapping into standard clinical knowledge bases (SNOMED-CT, BNF, etc.). In this
work, we investigate state-of-the-art LLMs in text mining tasks on medications
and their related attributes such as dosage, route, strength, and adverse
effects. In addition, we explore different ensemble learning methods
(\textsc{Stack-Ensemble} and \textsc{Voting-Ensemble}) to augment the model
performances from individual LLMs. Our ensemble learning result demonstrated
better performances than individually fine-tuned base models BERT, RoBERTa,
RoBERTa-L, BioBERT, BioClinicalBERT, BioMedRoBERTa, ClinicalBERT, and
PubMedBERT across general and specific domains. Finally, we build up an entity
linking function to map extracted medical terminologies into the SNOMED-CT
codes and the British National Formulary (BNF) codes, which are further mapped
to the Dictionary of Medicines and Devices (dm+d), and ICD. Our model's toolkit
and desktop applications are publicly available (at
\url{https://github.com/HECTA-UoM/ensemble-NER}).",2024-09-28,"Pablo Romero, Lifeng Han, Goran Nenadic",http://arxiv.org/pdf/2409.19467v2,cs.CL
Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach,"We study the problem of fine-tuning a language model (LM) for a target task
by optimally using the information from $n$ auxiliary tasks. This problem has
broad applications in NLP, such as targeted instruction tuning and data
selection in chain-of-thought fine-tuning. The key challenge of this problem is
that not all auxiliary tasks are useful to improve the performance of the
target task. Thus, choosing the right subset of auxiliary tasks is crucial.
Conventional subset selection methods, such as forward and backward stepwise
selection, are unsuitable for LM fine-tuning because they require repeated
training on subsets of auxiliary tasks. This paper introduces a new algorithm
to estimate model fine-tuning performances without repeated training. Our
algorithm first performs multitask training using the data of all the tasks to
obtain a meta initialization. Then, we approximate the model fine-tuning loss
of a subset using functional values and gradients from the meta initialization.
Empirically, we find that this gradient-based approximation holds with
remarkable accuracy for twelve transformer-based LMs. Thus, we can now estimate
fine-tuning performances on CPUs within a few seconds. Finally, we fine-tune
the pretrained base model for once on the selected subset of tasks. We conduct
extensive experiments to validate this approach, delivering a speedup of
$30\times$ over conventional subset selection while incurring only $1\%$ error
of the true fine-tuning performances. In downstream evaluations involving both
instruction tuning and chain-of-thought fine-tuning, this loss-based selection
approach improves over prior gradient or representation similarity-based
methods for subset selection by up to $3.8\%$.",2024-09-28,"Dongyue Li, Ziniu Zhang, Lu Wang, Hongyang R. Zhang",http://arxiv.org/pdf/2409.19458v2,cs.CL
Investigating the Impact of Text Summarization on Topic Modeling,"Topic models are used to identify and group similar themes in a set of
documents. Recent advancements in deep learning based neural topic models has
received significant research interest. In this paper, an approach is proposed
that further enhances topic modeling performance by utilizing a pre-trained
large language model (LLM) to generate summaries of documents before inputting
them into the topic model. Few shot prompting is used to generate summaries of
different lengths to compare their impact on topic modeling. This approach is
particularly effective for larger documents because it helps capture the most
essential information while reducing noise and irrelevant details that could
obscure the overall theme. Additionally, it is observed that datasets exhibit
an optimal summary length that leads to improved topic modeling performance.
The proposed method yields better topic diversity and comparable coherence
values compared to previous models.",2024-09-28,Trishia Khandelwal,http://arxiv.org/pdf/2410.09063v1,cs.CL
Thematic Analysis with Open-Source Generative AI and Machine Learning: A New Method for Inductive Qualitative Codebook Development,"This paper aims to answer one central question: to what extent can
open-source generative text models be used in a workflow to approximate
thematic analysis in social science research? To answer this question, we
present the Generative AI-enabled Theme Organization and Structuring (GATOS)
workflow, which uses open-source machine learning techniques, natural language
processing tools, and generative text models to facilitate thematic analysis.
To establish validity of the method, we present three case studies applying the
GATOS workflow, leveraging these models and techniques to inductively create
codebooks similar to traditional procedures using thematic analysis.
Specifically, we investigate the extent to which a workflow comprising
open-source models and tools can inductively produce codebooks that approach
the known space of themes and sub-themes. To address the challenge of gleaning
insights from these texts, we combine open-source generative text models,
retrieval-augmented generation, and prompt engineering to identify codes and
themes in large volumes of text, i.e., generate a qualitative codebook. The
process mimics an inductive coding process that researchers might use in
traditional thematic analysis by reading text one unit of analysis at a time,
considering existing codes already in the codebook, and then deciding whether
or not to generate a new code based on whether the extant codebook provides
adequate thematic coverage. We demonstrate this workflow using three synthetic
datasets from hypothetical organizational research settings: a study of
teammate feedback in teamwork settings, a study of organizational cultures of
ethical behavior, and a study of employee perspectives about returning to their
offices after the pandemic. We show that the GATOS workflow is able to identify
themes in the text that were used to generate the original synthetic datasets.",2024-09-28,"Andrew Katz, Gabriella Coloyan Fleming, Joyce Main",http://arxiv.org/pdf/2410.03721v1,cs.CL
'Simulacrum of Stories': Examining Large Language Models as Qualitative Research Participants,"The recent excitement around generative models has sparked a wave of
proposals suggesting the replacement of human participation and labor in
research and development--e.g., through surveys, experiments, and
interviews--with synthetic research data generated by large language models
(LLMs). We conducted interviews with 19 qualitative researchers to understand
their perspectives on this paradigm shift. Initially skeptical, researchers
were surprised to see similar narratives emerge in the LLM-generated data when
using the interview probe. However, over several conversational turns, they
went on to identify fundamental limitations, such as how LLMs foreclose
participants' consent and agency, produce responses lacking in palpability and
contextual depth, and risk delegitimizing qualitative research methods. We
argue that the use of LLMs as proxies for participants enacts the surrogate
effect, raising ethical and epistemological concerns that extend beyond the
technical limitations of current models to the core of whether LLMs fit within
qualitative ways of knowing.",2024-09-28,"Shivani Kapania, William Agnew, Motahhare Eslami, Hoda Heidari, Sarah Fox",http://arxiv.org/pdf/2409.19430v1,cs.CL
Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs,"In the age of mobile internet, user data, often referred to as memories, is
continuously generated on personal devices. Effectively managing and utilizing
this data to deliver services to users is a compelling research topic. In this
paper, we introduce a novel task of crafting personalized agents powered by
large language models (LLMs), which utilize a user's smartphone memories to
enhance downstream applications with advanced LLM capabilities. To achieve this
goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented
Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach
is further optimized using Reinforcement Learning to address three distinct
challenges: data collection, editability, and selectability. Extensive
experiments on a real-world dataset validate the effectiveness of EMG-RAG,
achieving an improvement of approximately 10% over the best existing approach.
Additionally, the personalized agents have been transferred into a real
smartphone AI assistant, which leads to enhanced usability.",2024-09-28,"Zheng Wang, Zhongyang Li, Zeren Jiang, Dandan Tu, Wei Shi",http://arxiv.org/pdf/2409.19401v1,cs.CL
Zero-Shot Multi-Hop Question Answering via Monte-Carlo Tree Search with Large Language Models,"Recent advances in large language models (LLMs) have significantly impacted
the domain of multi-hop question answering (MHQA), where systems are required
to aggregate information and infer answers from disparate pieces of text.
However, the autoregressive nature of LLMs inherently poses a challenge as
errors may accumulate if mistakes are made in the intermediate reasoning steps.
This paper introduces Monte-Carlo tree search for Zero-shot multi-hop Question
Answering (MZQA), a framework based on Monte-Carlo tree search (MCTS) to
identify optimal reasoning paths in MHQA tasks, mitigating the error
propagation from sequential reasoning processes. Unlike previous works, we
propose a zero-shot prompting method, which relies solely on instructions
without the support of hand-crafted few-shot examples that typically require
domain expertise. We also introduce a behavioral cloning approach (MZQA-BC)
trained on self-generated MCTS inference trajectories, achieving an over
10-fold increase in reasoning speed with bare compromise in performance. The
efficacy of our method is validated on standard benchmarks such as HotpotQA,
2WikiMultihopQA, and MuSiQue, demonstrating that it outperforms existing
frameworks.",2024-09-28,"Seongmin Lee, Jaewook Shin, Youngjin Ahn, Seokin Seo, Ohjoon Kwon, Kee-Eung Kim",http://arxiv.org/pdf/2409.19382v2,cs.CL
HYBRIDMIND: Meta Selection of Natural Language and Symbolic Language for Enhanced LLM Reasoning,"LLMs approach logical and mathematical reasoning through natural or symbolic
languages. While natural language offers human-accessible flexibility but
suffers from ambiguity, symbolic reasoning provides precise, machine-executable
inferences at the cost of strict domain constraints. We introduce HYBRIDMIND,
an adaptive strategy that selects the optimal reasoning approach for each
reasoning problem. Through extensive experiments, we evaluate both
prompting-based approaches with state-of-the-art LLMs and fine-tuned
open-source models. We find that fine-tuning LLaMA-3.1-8B-Instruct as a
meta-selector outperforms GPT-4o's natural language reasoning by 4.4\% on FOLIO
and 1.3\% on MATH. More notably, using GPT-3.5-turbo as a prompted
meta-selector yields a 10\% improvement on FOLIO's challenging subset compared
to GPT-4o. We will release our code and data to support future research.",2024-09-28,"Simeng Han, Tianyu Liu, Chuhan Li, Xuyuan Xiong, Arman Cohan",http://arxiv.org/pdf/2409.19381v5,cs.CL
DOTA: Distributional Test-Time Adaptation of Vision-Language Models,"Vision-language foundation models (e.g., CLIP) have shown remarkable
performance across a wide range of tasks. However, deploying these models may
be unreliable when significant distribution gaps exist between the training and
test data. The training-free test-time dynamic adapter (TDA) is a promising
approach to address this issue by storing representative test samples to guide
the classification of subsequent ones. However, TDA only naively maintains a
limited number of reference samples in the cache, leading to severe test-time
catastrophic forgetting when the cache is updated by dropping samples. In this
paper, we propose a simple yet effective method for DistributiOnal Test-time
Adaptation (Dota). Instead of naively memorizing representative test samples,
Dota continually estimates the distributions of test samples, allowing the
model to continually adapt to the deployment environment. The test-time
posterior probabilities are then computed using the estimated distributions
based on Bayes' theorem for adaptation purposes. To further enhance the
adaptability on the uncertain samples, we introduce a new human-in-the-loop
paradigm which identifies uncertain samples, collects human-feedback, and
incorporates it into the Dota framework. Extensive experiments validate that
Dota enables CLIP to continually learn, resulting in a significant improvement
compared to current state-of-the-art methods.",2024-09-28,"Zongbo Han, Jialong Yang, Junfan Li, Qinghua Hu, Qianli Xu, Mike Zheng Shou, Changqing Zhang",http://arxiv.org/pdf/2409.19375v1,cs.CL
Visual Question Decomposition on Multimodal Large Language Models,"Question decomposition has emerged as an effective strategy for prompting
Large Language Models (LLMs) to answer complex questions. However, while
existing methods primarily focus on unimodal language models, the question
decomposition capability of Multimodal Large Language Models (MLLMs) has yet to
be explored. To this end, this paper explores visual question decomposition on
MLLMs. Specifically, we introduce a systematic evaluation framework including a
dataset and several evaluation criteria to assess the quality of the decomposed
sub-questions, revealing that existing MLLMs struggle to produce high-quality
sub-questions. To address this limitation, we propose a specific finetuning
dataset, DecoVQA+, for enhancing the model's question decomposition capability.
Aiming at enabling models to perform appropriate selective decomposition, we
propose an efficient finetuning pipeline. The finetuning pipeline consists of
our proposed dataset and a training objective for selective decomposition.
Finetuned MLLMs demonstrate significant improvements in the quality of
sub-questions and the policy of selective question decomposition. Additionally,
the models also achieve higher accuracy with selective decomposition on VQA
benchmark datasets.",2024-09-28,"Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, Zhiqiang Xu, Jindong Gu",http://arxiv.org/pdf/2409.19339v2,cs.CL
Decoding Echo Chambers: LLM-Powered Simulations Revealing Polarization in Social Networks,"The impact of social media on critical issues such as echo chambers needs to
be addressed, as these phenomena can have disruptive consequences for our
society. Traditional research often oversimplifies emotional tendencies and
opinion evolution into numbers and formulas, neglecting that news and
communication are conveyed through text, which limits these approaches. Hence,
in this work, we propose an LLM-based simulation for the social opinion network
to evaluate and counter polarization phenomena. We first construct three
typical network structures to simulate different characteristics of social
interactions. Then, agents interact based on recommendation algorithms and
update their strategies through reasoning and analysis. By comparing these
interactions with the classic Bounded Confidence Model (BCM), the Friedkin
Johnsen (FJ) model, and using echo chamber-related indices, we demonstrate the
effectiveness of our framework in simulating opinion dynamics and reproducing
phenomena such as opinion polarization and echo chambers. We propose two
mitigation methods, active and passive nudges, that can help reduce echo
chambers, specifically within language-based simulations. We hope our work will
offer valuable insights and guidance for social polarization mitigation.",2024-09-28,"Chenxi Wang, Zongfang Liu, Dequan Yang, Xiuying Chen",http://arxiv.org/pdf/2409.19338v2,cs.CL
A Generalized Model for Multidimensional Intransitivity,"Intransitivity is a critical issue in pairwise preference modeling. It refers
to the intransitive pairwise preferences between a group of players or objects
that potentially form a cyclic preference chain and has been long discussed in
social choice theory in the context of the dominance relationship. However,
such multifaceted intransitivity between players and the corresponding player
representations in high dimensions is difficult to capture. In this paper, we
propose a probabilistic model that jointly learns each player's d-dimensional
representation (d>1) and a dataset-specific metric space that systematically
captures the distance metric in Rd over the embedding space. Interestingly, by
imposing additional constraints in the metric space, our proposed model
degenerates to former models used in intransitive representation learning.
Moreover, we present an extensive quantitative investigation of the vast
existence of intransitive relationships between objects in various real-world
benchmark datasets. To our knowledge, this investigation is the first of this
type. The predictive performance of our proposed method on different real-world
datasets, including social choice, election, and online game datasets, shows
that our proposed method outperforms several competing methods in terms of
prediction accuracy.",2024-09-28,"Jiuding Duan, Jiyi Li, Yukino Baba, Hisashi Kashima",http://arxiv.org/pdf/2409.19325v1,cs.CL
Designing Domain-Specific Large Language Models: The Critical Role of Fine-Tuning in Public Opinion Simulation,"Large language models (LLMs) have transformed natural language processing,
yet face challenges in specialized tasks such as simulating opinions on
environmental policies. This paper introduces a novel fine-tuning approach that
integrates socio-demographic data from the UK Household Longitudinal Study,
uniquely using profiling factors, such as age, gender, income, education, and
region. This method enhances the accuracy and representation of generated
views. By emulating diverse synthetic profiles, the fine-tuned models
significantly outperform pre-trained counterparts, achieving measurable
improvements in capturing demographic nuances. Evaluation metrics, including
Chi-Squared, Cosine Similarity, Jaccard Index, and KL-divergence, reveal a
strong alignment between synthetic and real-world opinions. This work
demonstrates the potential of fine-tuned LLMs tailored to societal contexts to
enable more ethical and precise policy simulations. Its broader implications
include deploying LLMs in domains like healthcare and education, fostering
inclusive and data-driven decision-making in both research and practice.",2024-09-28,Haocheng Lin,http://arxiv.org/pdf/2409.19308v2,cs.CL
FluentEditor2: Text-based Speech Editing by Modeling Multi-Scale Acoustic and Prosody Consistency,"Text-based speech editing (TSE) allows users to edit speech by modifying the
corresponding text directly without altering the original recording. Current
TSE techniques often focus on minimizing discrepancies between generated speech
and reference within edited regions during training to achieve fluent TSE
performance. However, the generated speech in the edited region should maintain
acoustic and prosodic consistency with the unedited region and the original
speech at both the local and global levels. To maintain speech fluency, we
propose a new fluency speech editing scheme based on our previous
\textit{FluentEditor} model, termed \textit{\textbf{FluentEditor2}}, by
modeling the multi-scale acoustic and prosody consistency training criterion in
TSE training. Specifically, for local acoustic consistency, we propose
\textit{hierarchical local acoustic smoothness constraint} to align the
acoustic properties of speech frames, phonemes, and words at the boundary
between the generated speech in the edited region and the speech in the
unedited region. For global prosody consistency, we propose \textit{contrastive
global prosody consistency constraint} to keep the speech in the edited region
consistent with the prosody of the original utterance. Extensive experiments on
the VCTK and LibriTTS datasets show that \textit{FluentEditor2} surpasses
existing neural networks-based TSE methods, including Editspeech, Campnet,
A$^3$T, FluentSpeech, and our Fluenteditor, in both subjective and objective.
Ablation studies further highlight the contributions of each module to the
overall effectiveness of the system. Speech demos are available at:
\url{https://github.com/Ai-S2-Lab/FluentEditor2}.",2024-09-28,"Rui Liu, Jiatian Xi, Ziyue Jiang, Haizhou Li",http://arxiv.org/pdf/2410.03719v2,cs.CL
Perception Compressor: A Training-Free Prompt Compression Framework in Long Context Scenarios,"Large language models (LLMs) demonstrate exceptional capabilities in various
scenarios. However, they suffer from much redundant information and are
sensitive to the position of key information in long context scenarios. To
address these challenges, we present Perception Compressor, a training-free
prompt compression framework. It includes a perception retriever that leverages
guiding questions and instruction to retrieve the most relevant demonstrations,
a dual-slope ratio allocator to dynamically allocate compression ratios and
open-book ratios, and a semi-guided iterative compression that retains key
information at the token level while removing tokens that distract the LLM. We
conduct extensive experiments on long context benchmarks, i.e.,
NaturalQuestions, LongBench, and MuSiQue. Experiment results show that
Perception Compressor outperforms existing methods by a large margin, achieving
state-of-the-art performance.",2024-09-28,"Jiwei Tang, Jin Xu, Tingwei Lu, Zhicheng Zhang, Yiming Zhao, Lin Hai, Hai-Tao Zheng",http://arxiv.org/pdf/2409.19272v5,cs.CL
LISTN: Lexicon induction with socio-temporal nuance,"In-group language is an important signifier of group dynamics. This paper
proposes a novel method for inducing lexicons of in-group language, which
incorporates its socio-temporal context. Existing methods for lexicon induction
do not capture the evolving nature of in-group language, nor the social
structure of the community. Using dynamic word and user embeddings trained on
conversations from online anti-women communities, our approach outperforms
prior methods for lexicon induction. We develop a test set for the task of
lexicon induction and a new lexicon of manosphere language, validated by human
experts, which quantifies the relevance of each term to a specific
sub-community at a given point in time. Finally, we present novel insights on
in-group language which illustrate the utility of this approach.",2024-09-28,Christine de Kock,http://arxiv.org/pdf/2409.19257v2,cs.CL
DENEB: A Hallucination-Robust Automatic Evaluation Metric for Image Captioning,"In this work, we address the challenge of developing automatic evaluation
metrics for image captioning, with a particular focus on robustness against
hallucinations. Existing metrics are often inadequate for handling
hallucinations, primarily due to their limited ability to compare candidate
captions with multifaceted reference captions. To address this shortcoming, we
propose DENEB, a novel supervised automatic evaluation metric specifically
robust against hallucinations. DENEB incorporates the Sim-Vec Transformer, a
mechanism that processes multiple references simultaneously, thereby
efficiently capturing the similarity between an image, a candidate caption, and
reference captions. To train DENEB, we construct the diverse and balanced
Nebula dataset comprising 32,978 images, paired with human judgments provided
by 805 annotators. We demonstrated that DENEB achieves state-of-the-art
performance among existing LLM-free metrics on the FOIL, Composite,
Flickr8K-Expert, Flickr8K-CF, Nebula, and PASCAL-50S datasets, validating its
effectiveness and robustness against hallucinations.",2024-09-28,"Kazuki Matsuda, Yuiga Wada, Komei Sugiura",http://arxiv.org/pdf/2409.19255v2,cs.CL
Edit-Constrained Decoding for Sentence Simplification,"We propose edit operation based lexically constrained decoding for sentence
simplification. In sentence simplification, lexical paraphrasing is one of the
primary procedures for rewriting complex sentences into simpler
correspondences. While previous studies have confirmed the efficacy of
lexically constrained decoding on this task, their constraints can be loose and
may lead to sub-optimal generation. We address this problem by designing
constraints that replicate the edit operations conducted in simplification and
defining stricter satisfaction conditions. Our experiments indicate that the
proposed method consistently outperforms the previous studies on three English
simplification corpora commonly used in this task.",2024-09-28,"Tatsuya Zetsu, Yuki Arase, Tomoyuki Kajiwara",http://arxiv.org/pdf/2409.19247v1,cs.CL
Jointly modelling the evolution of community structure and language in online extremist groups,"Group interactions take place within a particular socio-temporal context,
which should be taken into account when modelling communities. We propose a
method for jointly modelling community structure and language over time, and
apply it in the context of extremist anti-women online groups (collectively
known as the manosphere). Our model derives temporally grounded embeddings for
words and users, which evolve over the training window. We show that this
approach outperforms prior models which lacked one of these components (i.e.
not incorporating social structure, or using static word embeddings). Using
these embeddings, we investigate the evolution of users and words within these
communities in three ways: (i) we model a user as a sequence of embeddings and
forecast their affinity groups beyond the training window, (ii) we illustrate
how word evolution is useful in the context of temporal events, and (iii) we
characterise the propensity for violent language within subgroups of the
manosphere.",2024-09-28,Christine de Kock,http://arxiv.org/pdf/2409.19243v1,cs.CL
SciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement,"Automating the creation of scientific diagrams from academic papers can
significantly streamline the development of tutorials, presentations, and
posters, thereby saving time and accelerating the process. Current
text-to-image models struggle with generating accurate and visually appealing
diagrams from long-context inputs. We propose SciDoc2Diagram, a task that
extracts relevant information from scientific papers and generates diagrams,
along with a benchmarking dataset, SciDoc2DiagramBench. We develop a multi-step
pipeline SciDoc2Diagrammer that generates diagrams based on user intentions
using intermediate code generation. We observed that initial diagram drafts
were often incomplete or unfaithful to the source, leading us to develop
SciDoc2Diagrammer-Multi-Aspect-Feedback (MAF), a refinement strategy that
significantly enhances factual correctness and visual appeal and outperforms
existing models on both automatic and human judgement.",2024-09-28,"Ishani Mondal, Zongxia Li, Yufang Hou, Anandhavelu Natarajan, Aparna Garimella, Jordan Boyd-Graber",http://arxiv.org/pdf/2409.19242v2,cs.CL
Performance Evaluation of Tokenizers in Large Language Models for the Assamese Language,"Training of a tokenizer plays an important role in the performance of deep
learning models. This research aims to understand the performance of tokenizers
in five state-of-the-art (SOTA) large language models (LLMs) in the Assamese
language of India. The research is important to understand the multi-lingual
support for a low-resourced language such as Assamese. Our research reveals
that the tokenizer of SUTRA from Two AI performs the best with an average
Normalized Sequence Length (NSL) value of 0.45, closely followed by the
tokenizer of GPT-4o from Open AI with an average NSL value of 0.54, followed by
Gemma 2, Meta Llama 3.1, and Mistral Large Instruct 2407 with an average NSL
value of 0.82, 1.4, and 1.48 respectively.",2024-09-28,"Sagar Tamang, Dibya Jyoti Bora",http://arxiv.org/pdf/2410.03718v1,cs.CL
Test Case-Informed Knowledge Tracing for Open-ended Coding Tasks,"Open-ended coding tasks, which ask students to construct programs according
to certain specifications, are common in computer science education. Student
modeling can be challenging since their open-ended nature means that student
code can be diverse. Traditional knowledge tracing (KT) models that only
analyze response correctness may not fully capture nuances in student knowledge
from student code. In this paper, we introduce Test case-Informed Knowledge
Tracing for Open-ended Coding (TIKTOC), a framework to simultaneously analyze
and predict both open-ended student code and whether the code passes each test
case. We augment the existing CodeWorkout dataset with the test cases used for
a subset of the open-ended coding questions, and propose a multi-task learning
KT method to simultaneously analyze and predict 1) whether a student's code
submission passes each test case and 2) the student's open-ended code, using a
large language model as the backbone. We quantitatively show that these methods
outperform existing KT methods for coding that only use the overall score a
code submission receives. We also qualitatively demonstrate how test case
information, combined with open-ended code, helps us gain fine-grained insights
into student knowledge.",2024-09-28,"Zhangqi Duan, Nigel Fernandez, Alexander Hicks, Andrew Lan",http://arxiv.org/pdf/2410.10829v3,cs.CL
Evidence Is All You Need: Ordering Imaging Studies via Language Model Alignment with the ACR Appropriateness Criteria,"Diagnostic imaging studies are an increasingly important component of the
workup and management of acutely presenting patients. However, ordering
appropriate imaging studies according to evidence-based medical guidelines is a
challenging task with a high degree of variability between healthcare
providers. To address this issue, recent work has investigated if generative AI
and large language models can be leveraged to help clinicians order relevant
imaging studies for patients. However, it is challenging to ensure that these
tools are correctly aligned with medical guidelines, such as the American
College of Radiology's Appropriateness Criteria (ACR AC). In this study, we
introduce a framework to intelligently leverage language models by recommending
imaging studies for patient cases that are aligned with evidence-based
guidelines. We make available a novel dataset of patient ""one-liner"" scenarios
to power our experiments, and optimize state-of-the-art language models to
achieve an accuracy on par with clinicians in image ordering. Finally, we
demonstrate that our language model-based pipeline can be used as intelligent
assistants by clinicians to support image ordering workflows and improve the
accuracy of imaging study ordering according to the ACR AC. Our work
demonstrates and validates a strategy to leverage AI-based software to improve
trustworthy clinical decision making in alignment with expert evidence-based
guidelines.",2024-09-27,"Michael S. Yao, Allison Chae, Charles E. Kahn Jr., Walter R. Witschey, James C. Gee, Hersh Sagreiya, Osbert Bastani",http://arxiv.org/pdf/2409.19177v2,cs.CL
A GEN AI Framework for Medical Note Generation,"The increasing administrative burden of medical documentation, particularly
through Electronic Health Records (EHR), significantly reduces the time
available for direct patient care and contributes to physician burnout. To
address this issue, we propose MediNotes, an advanced generative AI framework
designed to automate the creation of SOAP (Subjective, Objective, Assessment,
Plan) notes from medical conversations. MediNotes integrates Large Language
Models (LLMs), Retrieval-Augmented Generation (RAG), and Automatic Speech
Recognition (ASR) to capture and process both text and voice inputs in real
time or from recorded audio, generating structured and contextually accurate
medical notes. The framework also incorporates advanced techniques like
Quantized Low-Rank Adaptation (QLoRA) and Parameter-Efficient Fine-Tuning
(PEFT) for efficient model fine-tuning in resource-constrained environments.
Additionally, MediNotes offers a query-based retrieval system, allowing
healthcare providers and patients to access relevant medical information
quickly and accurately. Evaluations using the ACI-BENCH dataset demonstrate
that MediNotes significantly improves the accuracy, efficiency, and usability
of automated medical documentation, offering a robust solution to reduce the
administrative burden on healthcare professionals while improving the quality
of clinical workflows.",2024-09-27,"Hui Yi Leong, Yi Fan Gao, Shuai Ji, Bora Kalaycioglu, Uktu Pamuksuz",http://arxiv.org/pdf/2410.01841v1,cs.CL
HM3: Heterogeneous Multi-Class Model Merging,"Foundation language model deployments often include auxiliary guard-rail
models to filter or classify text, detecting jailbreak attempts, biased or
toxic output, or ensuring topic adherence. These additional models increase the
complexity and cost of model inference, especially since many are also large
language models. To address this issue, we explore training-free model merging
techniques to consolidate these models into a single, multi-functional model.
We propose Heterogeneous Multi-Class Model Merging (HM3) as a simple technique
for merging multi-class classifiers with heterogeneous label spaces. Unlike
parameter-efficient fine-tuning techniques like LoRA, which require extensive
training and add complexity during inference, recent advancements allow models
to be merged in a training-free manner. We report promising results for merging
BERT-based guard models, some of which attain an average F1-score higher than
the source models while reducing the inference time by up to 44%. We introduce
self-merging to assess the impact of reduced task-vector density, finding that
the more poorly performing hate speech classifier benefits from self-merging
while higher-performing classifiers do not, which raises questions about using
task vector reduction for model tuning.",2024-09-27,Stefan Hackmann,http://arxiv.org/pdf/2409.19173v1,cs.CL
Revisiting the Superficial Alignment Hypothesis,"The Superficial Alignment Hypothesis posits that almost all of a language
model's abilities and knowledge are learned during pre-training, while
post-training is about giving a model the right style and format. We re-examine
these claims by empirically studying the scaling behavior of post-training with
increasing finetuning examples and evaluating them using objective
task-specific standardized benchmarks. Through experiments with the Llama-3,
Mistral, and Llama-2 model families of multiple sizes, we observe that, similar
to the pre-training scaling laws, post-training task performance scales as a
power law against the number of finetuning examples. This power law
relationship holds across a broad array of capabilities, including mathematical
reasoning, coding, instruction following, and multihop-reasoning. In addition,
for tasks like math and multihop reasoning, we observe that a handful of
examples merely align the model stylistically but do not saturate performance
on the benchmarks. Model performance is instead correlated with its reasoning
ability and it improves significantly with more examples, illustrating the need
for holistic evaluation programs leveraging objective benchmarks in addition to
measurement of alignment to human preferences. We also observe that language
models are not necessarily limited to using knowledge learned during
pre-training. With appropriate post-training, a model's ability to integrate
new knowledge greatly improves on downstream tasks like multihop
question-answering. Taken together, these results shed new light on the
Superficial Alignment Hypothesis, suggesting that it is, at best, an
over-simplification.",2024-09-27,"Mohit Raghavendra, Vaskar Nath, Sean Hendryx",http://arxiv.org/pdf/2410.03717v1,cs.CL
Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?,"Extremely low-resource (XLR) languages lack substantial corpora for training
NLP models, motivating the use of all available resources such as dictionaries
and grammar books. Machine Translation from One Book (Tanzer et al., 2024)
suggests that prompting long-context LLMs with one grammar book enables
English-Kalamang translation, an XLR language unseen by LLMs - a noteworthy
case of linguistics helping an NLP task. We investigate the source of this
translation ability, finding almost all improvements stem from the book's
parallel examples rather than its grammatical explanations. We find similar
results for Nepali and Guarani, seen low-resource languages, and we achieve
performance comparable to an LLM with a grammar book by simply fine-tuning an
encoder-decoder translation model. We then investigate where grammar books help
by testing two linguistic tasks, grammaticality judgment and gloss prediction,
and we explore what kind of grammatical knowledge helps by introducing a
typological feature prompt that achieves leading results on these more relevant
tasks. We thus emphasise the importance of task-appropriate data for XLR
languages: parallel examples for translation, and grammatical data for
linguistic tasks. As we find no evidence that long-context LLMs can make
effective use of grammatical explanations for XLR translation, we conclude data
collection for multilingual XLR tasks such as translation is best focused on
parallel data over linguistic description.",2024-09-27,"Seth Aycock, David Stap, Di Wu, Christof Monz, Khalil Sima'an",http://arxiv.org/pdf/2409.19151v2,cs.CL
On the Power of Decision Trees in Auto-Regressive Language Modeling,"Originally proposed for handling time series data, Auto-regressive Decision
Trees (ARDTs) have not yet been explored for language modeling. This paper
delves into both the theoretical and practical applications of ARDTs in this
new context. We theoretically demonstrate that ARDTs can compute complex
functions, such as simulating automata, Turing machines, and sparse circuits,
by leveraging ""chain-of-thought"" computations. Our analysis provides bounds on
the size, depth, and computational efficiency of ARDTs, highlighting their
surprising computational power. Empirically, we train ARDTs on simple language
generation tasks, showing that they can learn to generate coherent and
grammatically correct text on par with a smaller Transformer model.
Additionally, we show that ARDTs can be used on top of transformer
representations to solve complex reasoning tasks. This research reveals the
unique computational abilities of ARDTs, aiming to broaden the architectural
diversity in language model development.",2024-09-27,"Yulu Gan, Tomer Galanti, Tomaso Poggio, Eran Malach",http://arxiv.org/pdf/2409.19150v1,cs.CL
Uncovering Differences in Persuasive Language in Russian versus English Wikipedia,"We study how differences in persuasive language across Wikipedia articles,
written in either English and Russian, can uncover each culture's distinct
perspective on different subjects. We develop a large language model (LLM)
powered system to identify instances of persuasive language in multilingual
texts. Instead of directly prompting LLMs to detect persuasion, which is
subjective and difficult, we propose to reframe the task to instead ask
high-level questions (HLQs) which capture different persuasive aspects.
Importantly, these HLQs are authored by LLMs themselves. LLMs over-generate a
large set of HLQs, which are subsequently filtered to a small set aligned with
human labels for the original task. We then apply our approach to a
large-scale, bilingual dataset of Wikipedia articles (88K total), using a
two-stage identify-then-extract prompting strategy to find instances of
persuasion.
  We quantify the amount of persuasion per article, and explore the differences
in persuasion through several experiments on the paired articles. Notably, we
generate rankings of articles by persuasion in both languages. These rankings
match our intuitions on the culturally-salient subjects; Russian Wikipedia
highlights subjects on Ukraine, while English Wikipedia highlights the Middle
East. Grouping subjects into larger topics, we find politically-related events
contain more persuasion than others. We further demonstrate that HLQs obtain
similar performance when posed in either English or Russian. Our methodology
enables cross-lingual, cross-cultural understanding at scale, and we release
our code, prompts, and data.",2024-09-27,"Bryan Li, Aleksey Panasyuk, Chris Callison-Burch",http://arxiv.org/pdf/2409.19148v1,cs.CL
Confidential Prompting: Protecting User Prompts from Cloud LLM Providers,"Our work tackles the challenge of securing user inputs in cloud-hosted large
language model (LLM) serving while ensuring model confidentiality, output
invariance, and compute efficiency. We introduce Secure Partitioned Decoding
(SPD), which uses confidential computing to confine user prompts to a trusted
execution environment (TEE), namely a confidential virtual machine (CVM), while
allowing service providers to generate tokens efficiently. We also introduce a
novel cryptographic method, Prompt Obfuscation (PO), to ensure robustness
against reconstruction attacks on SPD. We demonstrate our approach preserves
both prompt confidentiality and LLM serving efficiency. Our solution enables
privacy-preserving cloud LLM serving that handles sensitive prompts, such as
clinical records, financial data, and personal information.",2024-09-27,"In Gim, Caihua Li, Lin Zhong",http://arxiv.org/pdf/2409.19134v3,cs.CL
Meta-RTL: Reinforcement-Based Meta-Transfer Learning for Low-Resource Commonsense Reasoning,"Meta learning has been widely used to exploit rich-resource source tasks to
improve the performance of low-resource target tasks. Unfortunately, most
existing meta learning approaches treat different source tasks equally,
ignoring the relatedness of source tasks to the target task in knowledge
transfer. To mitigate this issue, we propose a reinforcement-based multi-source
meta-transfer learning framework (Meta-RTL) for low-resource commonsense
reasoning. In this framework, we present a reinforcement-based approach to
dynamically estimating source task weights that measure the contribution of the
corresponding tasks to the target task in the meta-transfer learning. The
differences between the general loss of the meta model and task-specific losses
of source-specific temporal meta models on sampled target data are fed into the
policy network of the reinforcement learning module as rewards. The policy
network is built upon LSTMs that capture long-term dependencies on source task
weight estimation across meta learning iterations. We evaluate the proposed
Meta-RTL using both BERT and ALBERT as the backbone of the meta model on three
commonsense reasoning benchmark datasets. Experimental results demonstrate that
Meta-RTL substantially outperforms strong baselines and previous task selection
strategies and achieves larger improvements on extremely low-resource settings.",2024-09-27,"Yu Fu, Jie He, Yifan Yang, Qun Liu, Deyi Xiong",http://arxiv.org/pdf/2409.19075v4,cs.CL
Show and Guide: Instructional-Plan Grounded Vision and Language Model,"Guiding users through complex procedural plans is an inherently multimodal
task in which having visually illustrated plan steps is crucial to deliver an
effective plan guidance. However, existing works on plan-following language
models (LMs) often are not capable of multimodal input and output. In this
work, we present MM-PlanLLM, the first multimodal LLM designed to assist users
in executing instructional tasks by leveraging both textual plans and visual
information. Specifically, we bring cross-modality through two key tasks:
Conversational Video Moment Retrieval, where the model retrieves relevant
step-video segments based on user queries, and Visually-Informed Step
Generation, where the model generates the next step in a plan, conditioned on
an image of the user's current progress. MM-PlanLLM is trained using a novel
multitask-multistage approach, designed to gradually expose the model to
multimodal instructional-plans semantic layers, achieving strong performance on
both multimodal and textual dialogue in a plan-grounded setting. Furthermore,
we show that the model delivers cross-modal temporal and plan-structure
representations aligned between textual plan steps and instructional video
moments.",2024-09-27,"Diogo Glória-Silva, David Semedo, João Magalhães",http://arxiv.org/pdf/2409.19074v3,cs.CL
CLLMate: A Multimodal Benchmark for Weather and Climate Events Forecasting,"Forecasting weather and climate events is crucial for making appropriate
measures to mitigate environmental hazards and minimize losses. However,
existing environmental forecasting research focuses narrowly on predicting
numerical meteorological variables (e.g., temperature), neglecting the
translation of these variables into actionable textual narratives of events and
their consequences. To bridge this gap, we proposed Weather and Climate Event
Forecasting (WCEF), a new task that leverages numerical meteorological raster
data and textual event data to predict weather and climate events. This task is
challenging to accomplish due to difficulties in aligning multimodal data and
the lack of supervised datasets. To address these challenges, we present
CLLMate, the first multimodal dataset for WCEF, using 26,156 environmental news
articles aligned with ERA5 reanalysis data. We systematically benchmark 23
existing MLLMs on CLLMate, including closed-source, open-source, and our
fine-tuned models. Our experiments reveal the advantages and limitations of
existing MLLMs and the value of CLLMate for the training and benchmarking of
the WCEF task.",2024-09-27,"Haobo Li, Zhaowei Wang, Jiachen Wang, Yueya Wang, Alexis Kai Hon Lau, Huamin Qu",http://arxiv.org/pdf/2409.19058v2,cs.CL
LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction,"Classification tasks are typically handled using Machine Learning (ML)
models, which lack a balance between accuracy and interpretability. This paper
introduces a new approach for classification tasks using Large Language Models
(LLMs) in an explainable method. Unlike ML models, which rely heavily on data
cleaning and feature engineering, this method streamlines the process using
LLMs. This paper proposes a method called ""Language Model Learning (LML)""
powered by a new method called ""Data-Augmented Prediction (DAP)."" The
classification is performed by LLMs using a method similar to that used by
humans who manually explore and understand the data to decide classifications.
In the process of LML, a dataset is summarized and evaluated to determine the
features leading to each label the most. In the DAP process, the system uses
the data summary and a row of the testing dataset to automatically generate a
query to retrieve relevant rows from the dataset for context-aware
classification. LML and DAP unlock new possibilities in areas that require
explainable and context-aware decisions by ensuring satisfactory accuracy even
with complex data. The system scored an accuracy above 90% in some test cases,
confirming the effectiveness and potential of the system to outperform ML
models in various scenarios. The source code is available at
https://github.com/Pro-GenAI/LML-DAP",2024-09-27,Praneeth Vadlapati,http://arxiv.org/pdf/2409.18957v3,cs.CL
On the Inductive Bias of Stacking Towards Improving Reasoning,"Given the increasing scale of model sizes, novel training strategies like
gradual stacking [Gong et al., 2019, Reddi et al., 2023] have garnered
interest. Stacking enables efficient training by gradually growing the depth of
a model in stages and using layers from a smaller model in an earlier stage to
initialize the next stage. Although efficient for training, the model biases
induced by such growing approaches are largely unexplored. In this work, we
examine this fundamental aspect of gradual stacking, going beyond its
efficiency benefits. We propose a variant of gradual stacking called MIDAS that
can speed up language model training by up to 40%. Furthermore we discover an
intriguing phenomenon: MIDAS is not only training-efficient but surprisingly
also has an inductive bias towards improving downstream tasks, especially tasks
that require reasoning abilities like reading comprehension and math problems,
despite having similar or slightly worse perplexity compared to baseline
training. To further analyze this inductive bias, we construct reasoning
primitives -- simple synthetic tasks that are building blocks for reasoning --
and find that a model pretrained with stacking is significantly better than
standard pretraining on these primitives, with and without fine-tuning. This
provides stronger and more robust evidence for this inductive bias towards
reasoning. These findings of training efficiency and inductive bias towards
reasoning are verified at 1B, 2B and 8B parameter language models. Finally, we
conjecture the underlying reason for this inductive bias by exploring the
connection of stacking to looped models and provide strong supporting empirical
analysis.",2024-09-27,"Nikunj Saunshi, Stefani Karp, Shankar Krishnan, Sobhan Miryoosefi, Sashank J. Reddi, Sanjiv Kumar",http://arxiv.org/pdf/2409.19044v1,cs.CL
Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models,"The instruction-following ability of large language models enables humans to
interact with AI agents in a natural way. However, when required to generate
responses of a specific length, large language models often struggle to meet
users' needs due to their inherent difficulty in accurately perceiving
numerical constraints. To explore the ability of large language models to
control the length of generated responses, we propose the Target Length
Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible
Match (FM) to evaluate the model's performance in adhering to specified
response lengths. Furthermore, we introduce a novel, model-agnostic approach
called Ruler, which employs Meta Length Tokens (MLTs) to enhance the
instruction-following ability of large language models under length-constrained
instructions. Specifically, Ruler equips LLMs with the ability to generate
responses of a specified length based on length constraints within the
instructions. Moreover, Ruler can automatically generate appropriate MLT when
length constraints are not explicitly provided, demonstrating excellent
versatility and generalization. Comprehensive experiments show the
effectiveness of Ruler across different LLMs on Target Length Generation Task,
e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In
addition, we conduct extensive ablation experiments to further substantiate the
efficacy and generalization of Ruler. Our code and data is available at
https://github.com/Geaming2002/Ruler.",2024-09-27,"Jiaming Li, Lei Zhang, Yunshui Li, Ziqiang Liu, yuelin bai, Run Luo, Longze Chen, Min Yang",http://arxiv.org/pdf/2409.18943v2,cs.CL
AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow,"Simulated patient systems play a crucial role in modern medical education and
research, providing safe, integrative learning environments and enabling
clinical decision-making simulations. Large Language Models (LLM) could advance
simulated patient systems by replicating medical conditions and patient-doctor
interactions with high fidelity and low cost. However, ensuring the
effectiveness and trustworthiness of these systems remains a challenge, as they
require a large, diverse, and precise patient knowledgebase, along with a
robust and stable knowledge diffusion to users. Here, we developed AIPatient,
an advanced simulated patient system with AIPatient Knowledge Graph (AIPatient
KG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning
RAG) agentic workflow as the generation backbone. AIPatient KG samples data
from Electronic Health Records (EHRs) in the Medical Information Mart for
Intensive Care (MIMIC)-III database, producing a clinically diverse and
relevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89).
Reasoning RAG leverages six LLM powered agents spanning tasks including
retrieval, KG query generation, abstraction, checker, rewrite, and
summarization. This agentic framework reaches an overall accuracy of 94.15% in
EHR-based medical Question Answering (QA), outperforming benchmarks that use
either no agent or only partial agent integration. Our system also presents
high readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade
5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value
0.782, p>0.1). The promising performance of the AIPatient system highlights its
potential to support a wide range of applications, including medical education,
model evaluation, and system integration.",2024-09-27,"Huizi Yu, Jiayan Zhou, Lingyao Li, Shan Chen, Jack Gallifant, Anye Shi, Xiang Li, Wenyue Hua, Mingyu Jin, Guang Chen, Yang Zhou, Zhao Li, Trisha Gupte, Ming-Li Chen, Zahra Azizi, Yongfeng Zhang, Themistocles L. Assimes, Xin Ma, Danielle S. Bitterman, Lin Lu, Lizhou Fan",http://arxiv.org/pdf/2409.18924v2,cs.CL
Soft Measures for Extracting Causal Collective Intelligence,"Understanding and modeling collective intelligence is essential for
addressing complex social systems. Directed graphs called fuzzy cognitive maps
(FCMs) offer a powerful tool for encoding causal mental models, but extracting
high-integrity FCMs from text is challenging. This study presents an approach
using large language models (LLMs) to automate FCM extraction. We introduce
novel graph-based similarity measures and evaluate them by correlating their
outputs with human judgments through the Elo rating system. Results show
positive correlations with human evaluations, but even the best-performing
measure exhibits limitations in capturing FCM nuances. Fine-tuning LLMs
improves performance, but existing measures still fall short. This study
highlights the need for soft similarity measures tailored to FCM extraction,
advancing collective intelligence modeling with NLP.",2024-09-27,"Maryam Berijanian, Spencer Dork, Kuldeep Singh, Michael Riley Millikan, Ashlin Riggs, Aadarsh Swaminathan, Sarah L. Gibbs, Scott E. Friedman, Nathan Brugnone",http://arxiv.org/pdf/2409.18911v1,cs.CL
IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation,"As Large Language Models (LLMs) grow increasingly adept at managing complex
tasks, the evaluation set must keep pace with these advancements to ensure it
remains sufficiently discriminative. Item Discrimination (ID) theory, which is
widely used in educational assessment, measures the ability of individual test
items to differentiate between high and low performers. Inspired by this
theory, we propose an ID-induced prompt synthesis framework for evaluating LLMs
to ensure the evaluation set can continually update and refine according to
model abilities. Our data synthesis framework prioritizes both breadth and
specificity. It can generate prompts that comprehensively evaluate the
capabilities of LLMs while revealing meaningful performance differences between
models, allowing for effective discrimination of their relative strengths and
weaknesses across various tasks and domains. To produce high-quality data, we
incorporate a self-correct mechanism into our generalization framework, and
develop two models to predict prompt discrimination and difficulty score to
facilitate our data synthesis framework, contributing valuable tools to
evaluation data synthesis research. We apply our generated data to evaluate
five SOTA models. Our data achieves an average score of 51.92, accompanied by a
variance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and
WizardLM) obtain an average score exceeding 67, with a variance below 3.2. The
results demonstrate that the data generated by our framework is more
challenging and discriminative compared to previous works. We will release a
dataset of over 3,000 carefully crafted prompts to facilitate evaluation
research of LLMs.",2024-09-27,"Fan Lin, Shuyi Xie, Yong Dai, Wenlin Yao, Tianjiao Lang, Zishan Xu, Zhichao Hu, Xiao Xiao, Yuhong Liu, Yu Zhang",http://arxiv.org/pdf/2409.18892v2,cs.CL
Suicide Phenotyping from Clinical Notes in Safety-Net Psychiatric Hospital Using Multi-Label Classification with Pre-Trained Language Models,"Accurate identification and categorization of suicidal events can yield
better suicide precautions, reducing operational burden, and improving care
quality in high-acuity psychiatric settings. Pre-trained language models offer
promise for identifying suicidality from unstructured clinical narratives. We
evaluated the performance of four BERT-based models using two fine-tuning
strategies (multiple single-label and single multi-label) for detecting
coexisting suicidal events from 500 annotated psychiatric evaluation notes. The
notes were labeled for suicidal ideation (SI), suicide attempts (SA), exposure
to suicide (ES), and non-suicidal self-injury (NSSI). RoBERTa outperformed
other models using multiple single-label classification strategy (acc=0.86,
F1=0.78). MentalBERT (acc=0.83, F1=0.74) also exceeded BioClinicalBERT
(acc=0.82, F1=0.72) which outperformed BERT (acc=0.80, F1=0.70). RoBERTa
fine-tuned with single multi-label classification further improved the model
performance (acc=0.88, F1=0.81). The findings highlight that the model
optimization, pretraining with domain-relevant data, and the single multi-label
classification strategy enhance the model performance of suicide phenotyping.
Keywords: EHR-based Phenotyping; Natural Language Processing; Secondary Use of
EHR Data; Suicide Classification; BERT-based Model; Psychiatry; Mental Health",2024-09-27,"Zehan Li, Yan Hu, Scott Lane, Salih Selek, Lokesh Shahani, Rodrigo Machado-Vieira, Jair Soares, Hua Xu, Hongfang Liu, Ming Huang",http://arxiv.org/pdf/2409.18878v2,cs.CL
Individuation in Neural Models with and without Visual Grounding,"We show differences between a language-and-vision model CLIP and two
text-only models - FastText and SBERT - when it comes to the encoding of
individuation information. We study latent representations that CLIP provides
for substrates, granular aggregates, and various numbers of objects. We
demonstrate that CLIP embeddings capture quantitative differences in
individuation better than models trained on text-only data. Moreover, the
individuation hierarchy we deduce from the CLIP embeddings agrees with the
hierarchies proposed in linguistics and cognitive science.",2024-09-27,"Alexey Tikhonov, Lisa Bylinina, Ivan P. Yamshchikov",http://arxiv.org/pdf/2409.18868v1,cs.CL
Local Transcription Models in Home Care Nursing in Switzerland: an Interdisciplinary Case Study,"Latest advances in the field of natural language processing (NLP) enable new
use cases for different domains, including the medical sector. In particular,
transcription can be used to support automation in the nursing documentation
process and give nurses more time to interact with the patients. However,
different challenges including (a) data privacy, (b) local languages and
dialects, and (c) domain-specific vocabulary need to be addressed. In this case
study, we investigate the case of home care nursing documentation in
Switzerland. We assessed different transcription tools and models, and
conducted several experiments with OpenAI Whisper, involving different
variations of German (i.e., dialects, foreign accent) and manually curated
example texts by a domain expert of home care nursing. Our results indicate
that even the used out-of-the-box model performs sufficiently well to be a good
starting point for future research in the field.",2024-09-27,"Jeremy Kramer, Tetiana Kravchenko, Beatrice Kaufmann, Friederike J. S. Thilo, Mascha Kurpicz-Briki",http://arxiv.org/pdf/2409.18819v1,cs.CL
LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis,"In response to the growing complexity and volume of scientific literature,
this paper introduces the LLMs4Synthesis framework, designed to enhance the
capabilities of Large Language Models (LLMs) in generating high-quality
scientific syntheses. This framework addresses the need for rapid, coherent,
and contextually rich integration of scientific insights, leveraging both
open-source and proprietary LLMs. It also examines the effectiveness of LLMs in
evaluating the integrity and reliability of these syntheses, alleviating
inadequacies in current quantitative metrics. Our study contributes to this
field by developing a novel methodology for processing scientific papers,
defining new synthesis types, and establishing nine detailed quality criteria
for evaluating syntheses. The integration of LLMs with reinforcement learning
and AI feedback is proposed to optimize synthesis quality, ensuring alignment
with established criteria. The LLMs4Synthesis framework and its components are
made available, promising to enhance both the generation and evaluation
processes in scientific research synthesis.",2024-09-27,"Hamed Babaei Giglou, Jennifer D'Souza, Sören Auer",http://arxiv.org/pdf/2409.18812v1,cs.CL
A Survey on the Honesty of Large Language Models,"Honesty is a fundamental principle for aligning large language models (LLMs)
with human values, requiring these models to recognize what they know and don't
know and be able to faithfully express their knowledge. Despite promising,
current LLMs still exhibit significant dishonest behaviors, such as confidently
presenting wrong answers or failing to express what they know. In addition,
research on the honesty of LLMs also faces challenges, including varying
definitions of honesty, difficulties in distinguishing between known and
unknown knowledge, and a lack of comprehensive understanding of related
research. To address these issues, we provide a survey on the honesty of LLMs,
covering its clarification, evaluation approaches, and strategies for
improvement. Moreover, we offer insights for future research, aiming to inspire
further exploration in this important area.",2024-09-27,"Siheng Li, Cheng Yang, Taiqiang Wu, Chufan Shi, Yuji Zhang, Xinyu Zhu, Zesen Cheng, Deng Cai, Mo Yu, Lemao Liu, Jie Zhou, Yujiu Yang, Ngai Wong, Xixin Wu, Wai Lam",http://arxiv.org/pdf/2409.18786v1,cs.CL
Charting the Future: Using Chart Question-Answering for Scalable Evaluation of LLM-Driven Data Visualizations,"We propose a novel framework that leverages Visual Question Answering (VQA)
models to automate the evaluation of LLM-generated data visualizations.
Traditional evaluation methods often rely on human judgment, which is costly
and unscalable, or focus solely on data accuracy, neglecting the effectiveness
of visual communication. By employing VQA models, we assess data representation
quality and the general communicative clarity of charts. Experiments were
conducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with
visualizations generated by OpenAI's GPT-3.5 Turbo and Meta's Llama 3.1
70B-Instruct models. Our results indicate that LLM-generated charts do not
match the accuracy of the original non-LLM-generated charts based on VQA
performance measures. Moreover, while our results demonstrate that few-shot
prompting significantly boosts the accuracy of chart generation, considerable
progress remains to be made before LLMs can fully match the precision of
human-generated graphs. This underscores the importance of our work, which
expedites the research process by enabling rapid iteration without the need for
human annotation, thus accelerating advancements in this field.",2024-09-27,"James Ford, Xingmeng Zhao, Dan Schumacher, Anthony Rios",http://arxiv.org/pdf/2409.18764v1,cs.CL
Cross-Domain Keyword Extraction with Keyness Patterns,"Domain dependence and annotation subjectivity pose challenges for supervised
keyword extraction. Based on the premises that second-order keyness patterns
are existent at the community level and learnable from annotated keyword
extraction datasets, this paper proposes a supervised ranking approach to
keyword extraction that ranks keywords with keyness patterns consisting of
independent features (such as sublanguage domain and term length) and three
categories of dependent features -- heuristic features, specificity features,
and representavity features. The approach uses two convolutional-neural-network
based models to learn keyness patterns from keyword datasets and overcomes
annotation subjectivity by training the two models with bootstrap sampling
strategy. Experiments demonstrate that the approach not only achieves
state-of-the-art performance on ten keyword datasets in general supervised
keyword extraction with an average top-10-F-measure of 0.316 , but also robust
cross-domain performance with an average top-10-F-measure of 0.346 on four
datasets that are excluded in the training process. Such cross-domain
robustness is attributed to the fact that community-level keyness patterns are
limited in number and temperately independent of language domains, the
distinction between independent features and dependent features, and the
sampling training strategy that balances excess risk and lack of negative
training data.",2024-09-27,"Dongmei Zhou, Xuri Tang",http://arxiv.org/pdf/2409.18724v1,cs.CL
Read Over the Lines: Attacking LLMs and Toxicity Detection Systems with ASCII Art to Mask Profanity,"We introduce a novel family of adversarial attacks that exploit the inability
of language models to interpret ASCII art. To evaluate these attacks, we
propose the ToxASCII benchmark and develop two custom ASCII art fonts: one
leveraging special tokens and another using text-filled letter shapes. Our
attacks achieve a perfect 1.0 Attack Success Rate across ten models, including
OpenAI's o1-preview and LLaMA 3.1.
  Warning: this paper contains examples of toxic language used for research
purposes.",2024-09-27,"Sergey Berezin, Reza Farahbakhsh, Noel Crespi",http://arxiv.org/pdf/2409.18708v4,cs.CL
KALE-LM: Unleash The Power Of AI For Science Via Knowledge And Logic Enhanced Large Model,"Artificial intelligence is gradually demonstrating its immense potential, and
increasing attention is being given to how AI can be harnessed to advance
scientific research. In this vision paper, we present our perspectives on how
AI can better assist scientific inquiry and explore corresponding technical
approach. We have proposed and open-sourced two large models of our KALE-LM
model series, KALE-LM-Chem(-1.5), which have achieved outstanding performance
in tasks related to the field of chemistry. We hope that our work serves as a
strong starting point, helping to realize more intelligent AI and promoting the
advancement of human science and technology, as well as societal development.",2024-09-27,"Weichen Dai, Yezeng Chen, Zijie Dai, Yubo Liu, Zhijie Huang, Yixuan Pan, Baiyang Song, Chengli Zhong, Xinhe Li, Zeyu Wang, Zhuoying Feng, Yi Zhou",http://arxiv.org/pdf/2409.18695v2,cs.CL
Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models,"Various audio-LLMs (ALLMs) have been explored recently for tackling different
audio tasks simultaneously using a single, unified model. While existing
evaluations of ALLMs primarily focus on single-audio tasks, real-world
applications often involve processing multiple audio streams simultaneously. To
bridge this gap, we propose the first multi-audio evaluation (MAE) benchmark
that consists of 20 datasets from 11 multi-audio tasks encompassing both speech
and sound scenarios. Comprehensive experiments on MAE demonstrate that the
existing ALLMs, while being powerful in comprehending primary audio elements in
individual audio inputs, struggling to handle multi-audio scenarios. To this
end, we propose a novel multi-audio-LLM (MALLM) to capture audio context among
multiple similar audios using discriminative learning on our proposed synthetic
data. The results demonstrate that the proposed MALLM outperforms all baselines
and achieves high data efficiency using synthetic data without requiring human
annotations. The proposed MALLM opens the door for ALLMs towards multi-audio
processing era and brings us closer to replicating human auditory capabilities
in machines.",2024-09-27,"Yiming Chen, Xianghu Yue, Xiaoxue Gao, Chen Zhang, Luis Fernando D'Haro, Robby T. Tan, Haizhou Li",http://arxiv.org/pdf/2409.18680v3,cs.CL
"""Why"" Has the Least Side Effect on Model Editing","Training large language models (LLMs) from scratch is an expensive endeavor,
particularly as world knowledge continually evolves. To maintain relevance and
accuracy of LLMs, model editing has emerged as a pivotal research area. While
these methods hold promise, they can also produce unintended side effects.
Their underlying factors and causes remain largely unexplored. This paper
delves into a critical factor-question type-by categorizing model editing
questions. Our findings reveal that the extent of performance degradation
varies significantly across different question types, providing new insights
for experimental design in knowledge editing. Furthermore, we investigate
whether insights from smaller models can be extrapolated to larger models. Our
results indicate discrepancies in findings between models of different sizes,
suggesting that insights from smaller models may not necessarily apply to
larger models. Additionally, we examine the impact of batch size on side
effects, discovering that increasing the batch size can mitigate performance
drops.",2024-09-27,"Tsung-Hsuan Pan, Chung-Chi Chen, Hen-Hsen Huang, Hsin-Hsi Chen",http://arxiv.org/pdf/2409.18679v1,cs.CL
Rehearsing Answers to Probable Questions with Perspective-Taking,"Question answering (QA) has been a long-standing focus in the NLP field,
predominantly addressing reading comprehension and common sense QA. However,
scenarios involving the preparation of answers to probable questions during
professional oral presentations remain underexplored. In this paper, we pioneer
the examination of this crucial yet overlooked topic by utilizing real-world QA
conversation transcripts between company managers and professional analysts. We
explore the proposed task using three causal knowledge graphs (KGs) and three
large language models (LLMs). This work provides foundational insights into the
application of LLMs in professional QA scenarios, highlighting the importance
of causal KGs and perspective-taking in generating effective responses.",2024-09-27,"Yung-Yu Shih, Ziwei Xu, Hiroya Takamura, Yun-Nung Chen, Chung-Chi Chen",http://arxiv.org/pdf/2409.18678v1,cs.CL
Co-Trained Retriever-Generator Framework for Question Generation in Earnings Calls,"In diverse professional environments, ranging from academic conferences to
corporate earnings calls, the ability to anticipate audience questions stands
paramount. Traditional methods, which rely on manual assessment of an
audience's background, interests, and subject knowledge, often fall short -
particularly when facing large or heterogeneous groups, leading to imprecision
and inefficiency. While NLP has made strides in text-based question generation,
its primary focus remains on academic settings, leaving the intricate
challenges of professional domains, especially earnings call conferences,
underserved. Addressing this gap, our paper pioneers the multi-question
generation (MQG) task specifically designed for earnings call contexts. Our
methodology involves an exhaustive collection of earnings call transcripts and
a novel annotation technique to classify potential questions. Furthermore, we
introduce a retriever-enhanced strategy to extract relevant information. With a
core aim of generating a spectrum of potential questions that analysts might
pose, we derive these directly from earnings call content. Empirical
evaluations underscore our approach's edge, revealing notable excellence in the
accuracy, consistency, and perplexity of the questions generated.",2024-09-27,"Yining Juan, Chung-Chi Chen, Hen-Hsen Huang, Hsin-Hsi Chen",http://arxiv.org/pdf/2409.18677v1,cs.CL
HiCuLR: Hierarchical Curriculum Learning for Rhetorical Role Labeling of Legal Documents,"Rhetorical Role Labeling (RRL) of legal documents is pivotal for various
downstream tasks such as summarization, semantic case search and argument
mining. Existing approaches often overlook the varying difficulty levels
inherent in legal document discourse styles and rhetorical roles. In this work,
we propose HiCuLR, a hierarchical curriculum learning framework for RRL. It
nests two curricula: Rhetorical Role-level Curriculum (RC) on the outer layer
and Document-level Curriculum (DC) on the inner layer. DC categorizes documents
based on their difficulty, utilizing metrics like deviation from a standard
discourse structure and exposes the model to them in an easy-to-difficult
fashion. RC progressively strengthens the model to discern
coarse-to-fine-grained distinctions between rhetorical roles. Our experiments
on four RRL datasets demonstrate the efficacy of HiCuLR, highlighting the
complementary nature of DC and RC.",2024-09-27,"T. Y. S. S. Santosh, Apolline Isaia, Shiyu Hong, Matthias Grabmair",http://arxiv.org/pdf/2409.18647v1,cs.CL
The Craft of Selective Prediction: Towards Reliable Case Outcome Classification -- An Empirical Study on European Court of Human Rights Cases,"In high-stakes decision-making tasks within legal NLP, such as Case Outcome
Classification (COC), quantifying a model's predictive confidence is crucial.
Confidence estimation enables humans to make more informed decisions,
particularly when the model's certainty is low, or where the consequences of a
mistake are significant. However, most existing COC works prioritize high task
performance over model reliability. This paper conducts an empirical
investigation into how various design choices including pre-training corpus,
confidence estimator and fine-tuning loss affect the reliability of COC models
within the framework of selective prediction. Our experiments on the
multi-label COC task, focusing on European Court of Human Rights (ECtHR) cases,
highlight the importance of a diverse yet domain-specific pre-training corpus
for better calibration. Additionally, we demonstrate that larger models tend to
exhibit overconfidence, Monte Carlo dropout methods produce reliable confidence
estimates, and confident error regularization effectively mitigates
overconfidence. To our knowledge, this is the first systematic exploration of
selective prediction in legal NLP. Our findings underscore the need for further
research on enhancing confidence measurement and improving the trustworthiness
of models in the legal domain.",2024-09-27,"T. Y. S. S. Santosh, Irtiza Chowdhury, Shanshan Xu, Matthias Grabmair",http://arxiv.org/pdf/2409.18645v1,cs.CL
Incorporating Precedents for Legal Judgement Prediction on European Court of Human Rights Cases,"Inspired by the legal doctrine of stare decisis, which leverages precedents
(prior cases) for informed decision-making, we explore methods to integrate
them into LJP models. To facilitate precedent retrieval, we train a retriever
with a fine-grained relevance signal based on the overlap ratio of alleged
articles between cases. We investigate two strategies to integrate precedents:
direct incorporation at inference via label interpolation based on case
proximity and during training via a precedent fusion module using a
stacked-cross attention model. We employ joint training of the retriever and
LJP models to address latent space divergence between them. Our experiments on
LJP tasks from the ECHR jurisdiction reveal that integrating precedents during
training coupled with joint training of the retriever and LJP model,
outperforms models without precedents or with precedents incorporated only at
inference, particularly benefiting sparser articles.",2024-09-27,"T. Y. S. S. Santosh, Mohamed Hesham Elganayni, Stanisław Sójka, Matthias Grabmair",http://arxiv.org/pdf/2409.18644v1,cs.CL
Model-based Preference Optimization in Abstractive Summarization without Human Feedback,"In abstractive summarization, the challenge of producing concise and accurate
summaries arises from the vast amount of information contained in the source
document. Consequently, although Large Language Models (LLMs) can generate
fluent text, they often introduce inaccuracies by hallucinating content not
found in the original source. While supervised fine-tuning methods that
maximize likelihood contribute to this issue, they do not consistently enhance
the faithfulness of the summaries. Preference-based optimization methods, such
as Direct Preference Optimization (DPO), can further refine the model to align
with human preferences. However, these methods still heavily depend on costly
human feedback. In this work, we introduce a novel and straightforward approach
called Model-based Preference Optimization (MPO) to fine-tune LLMs for improved
summarization abilities without any human feedback. By leveraging the model's
inherent summarization capabilities, we create a preference dataset that is
fully generated by the model using different decoding strategies. Our
experiments on standard summarization datasets and various metrics demonstrate
that our proposed MPO significantly enhances the quality of generated summaries
without relying on human feedback.",2024-09-27,"Jaepill Choi, Kyubyung Chae, Jiwoo Song, Yohan Jo, Taesup Kim",http://arxiv.org/pdf/2409.18618v3,cs.CL
Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to Addressee Recognition and Response Selection in Conversations,"Assessing the performance of systems to classify Multi-Party Conversations
(MPC) is challenging due to the interconnection between linguistic and
structural characteristics of conversations. Conventional evaluation methods
often overlook variances in model behavior across different levels of
structural complexity on interaction graphs. In this work, we propose a
methodological pipeline to investigate model performance across specific
structural attributes of conversations. As a proof of concept we focus on
Response Selection and Addressee Recognition tasks, to diagnose model
weaknesses. To this end, we extract representative diagnostic subdatasets with
a fixed number of users and a good structural variety from a large and open
corpus of online MPCs. We further frame our work in terms of data minimization,
avoiding the use of original usernames to preserve privacy, and propose
alternatives to using original text messages. Results show that response
selection relies more on the textual content of conversations, while addressee
recognition requires capturing their structural dimension. Using an LLM in a
zero-shot setting, we further highlight how sensitivity to prompt variations is
task-dependent.",2024-09-27,"Nicolò Penzo, Maryam Sajedinia, Bruno Lepri, Sara Tonelli, Marco Guerini",http://arxiv.org/pdf/2409.18602v1,cs.CL
ASAG2024: A Combined Benchmark for Short Answer Grading,"Open-ended questions test a more thorough understanding than closed-ended
questions and are often a preferred assessment method. However, open-ended
questions are tedious to grade and subject to personal bias. Therefore, there
have been efforts to speed up the grading process through automation. Short
Answer Grading (SAG) systems aim to automatically score students' answers.
Despite growth in SAG methods and capabilities, there exists no comprehensive
short-answer grading benchmark across different subjects, grading scales, and
distributions. Thus, it is hard to assess the capabilities of current automated
grading methods in terms of their generalizability. In this preliminary work,
we introduce the combined ASAG2024 benchmark to facilitate the comparison of
automated grading systems. Combining seven commonly used short-answer grading
datasets in a common structure and grading scale. For our benchmark, we
evaluate a set of recent SAG methods, revealing that while LLM-based approaches
reach new high scores, they still are far from reaching human performance. This
opens up avenues for future research on human-machine SAG systems.",2024-09-27,"Gérôme Meyer, Philip Breuer, Jonathan Fürst",http://arxiv.org/pdf/2409.18596v1,cs.CL
"""Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree"": Zero-Shot Decision Tree Induction and Embedding with Large Language Models","Large language models (LLMs) provide powerful means to leverage prior
knowledge for predictive modeling when data is limited. In this work, we
demonstrate how LLMs can use their compressed world knowledge to generate
intrinsically interpretable machine learning models, i.e., decision trees,
without any training data. We find that these zero-shot decision trees can
surpass data-driven trees on some small-sized tabular datasets and that
embeddings derived from these trees perform on par with data-driven tree-based
embeddings on average. Our knowledge-driven decision tree induction and
embedding approaches therefore serve as strong new baselines for data-driven
machine learning methods in the low-data regime.",2024-09-27,"Ricardo Knauer, Mario Koddenbrock, Raphael Wallsberger, Nicholas M. Brisson, Georg N. Duda, Deborah Falla, David W. Evans, Erik Rodner",http://arxiv.org/pdf/2409.18594v1,cs.CL
Hit the Sweet Spot! Span-Level Ensemble for Large Language Models,"Ensembling various LLMs to unlock their complementary potential and leverage
their individual strengths is highly valuable. Previous studies typically focus
on two main paradigms: sample-level and token-level ensembles. Sample-level
ensemble methods either select or blend fully generated outputs, which hinders
dynamic correction and enhancement of outputs during the generation process. On
the other hand, token-level ensemble methods enable real-time correction
through fine-grained ensemble at each generation step. However, the information
carried by an individual token is quite limited, leading to suboptimal
decisions at each step. To address these issues, we propose SweetSpan, a
span-level ensemble method that effectively balances the need for real-time
adjustments and the information required for accurate ensemble decisions. Our
approach involves two key steps: First, we have each candidate model
independently generate candidate spans based on the shared prefix. Second, we
calculate perplexity scores to facilitate mutual evaluation among the candidate
models and achieve robust span selection by filtering out unfaithful scores. To
comprehensively evaluate ensemble methods, we propose a new challenging setting
(ensemble models with significant performance gaps) in addition to the standard
setting (ensemble the best-performing models) to assess the performance of
model ensembles in more realistic scenarios. Experimental results in both
standard and challenging settings across various language generation tasks
demonstrate the effectiveness, robustness, and versatility of our approach
compared with previous ensemble methods.",2024-09-27,"Yangyifan Xu, Jianghao Chen, Junhong Wu, Jiajun Zhang",http://arxiv.org/pdf/2409.18583v1,cs.CL
Research on Predicting Public Opinion Event Heat Levels Based on Large Language Models,"In recent years, with the rapid development of large language models, serval
models such as GPT-4o have demonstrated extraordinary capabilities, surpassing
human performance in various language tasks. As a result, many researchers have
begun exploring their potential applications in the field of public opinion
analysis. This study proposes a novel large-language-models-based method for
public opinion event heat level prediction. First, we preprocessed and
classified 62,836 Chinese hot event data collected between July 2022 and
December 2023. Then, based on each event's online dissemination heat index, we
used the MiniBatchKMeans algorithm to automatically cluster the events and
categorize them into four heat levels (ranging from low heat to very high
heat). Next, we randomly selected 250 events from each heat level, totalling
1,000 events, to build the evaluation dataset. During the evaluation process,
we employed various large language models to assess their accuracy in
predicting event heat levels in two scenarios: without reference cases and with
similar case references. The results showed that GPT-4o and DeepseekV2
performed the best in the latter case, achieving prediction accuracies of 41.4%
and 41.5%, respectively. Although the overall prediction accuracy remains
relatively low, it is worth noting that for low-heat (Level 1) events, the
prediction accuracies of these two models reached 73.6% and 70.4%,
respectively. Additionally, the prediction accuracy showed a downward trend
from Level 1 to Level 4, which correlates with the uneven distribution of data
across the heat levels in the actual dataset. This suggests that with the more
robust dataset, public opinion event heat level prediction based on large
language models will have significant research potential for the future.",2024-09-27,"Yi Ren, Tianyi Zhang, Weibin Li, DuoMu Zhou, Chenhao Qin, FangCheng Dong",http://arxiv.org/pdf/2409.18548v1,cs.CL
Self-Replicating Mechanical Universal Turing Machine,"This paper presents the implementation of a self-replicating finite-state
machine (FSM) and a self-replicating Turing Machine (TM) using bio-inspired
mechanisms. Building on previous work that introduced self-replicating
structures capable of sorting, copying, and reading information, this study
demonstrates the computational power of these mechanisms by explicitly
constructing a functioning FSM and TM. This study demonstrates the universality
of the system by emulating the UTM(5,5) of Neary and Woods.",2024-09-27,Ralph P. Lano,http://arxiv.org/pdf/2409.19037v1,cs.CL
A Survey on Complex Tasks for Goal-Directed Interactive Agents,"Goal-directed interactive agents, which autonomously complete tasks through
interactions with their environment, can assist humans in various domains of
their daily lives. Recent advances in large language models (LLMs) led to a
surge of new, more and more challenging tasks to evaluate such agents. To
properly contextualize performance across these tasks, it is imperative to
understand the different challenges they pose to agents. To this end, this
survey compiles relevant tasks and environments for evaluating goal-directed
interactive agents, structuring them along dimensions relevant for
understanding current obstacles. An up-to-date compilation of relevant
resources can be found on our project website:
https://coli-saar.github.io/interactive-agents.",2024-09-27,"Mareike Hartmann, Alexander Koller",http://arxiv.org/pdf/2409.18538v1,cs.CL
EmoPro: A Prompt Selection Strategy for Emotional Expression in LM-based Speech Synthesis,"Recent advancements in speech synthesis models, trained on extensive
datasets, have demonstrated remarkable zero-shot capabilities. These models can
control content, timbre, and emotion in generated speech based on prompt
inputs. Despite these advancements, the choice of prompts significantly impacts
the output quality, yet most existing selection schemes do not adequately
address the control of emotional intensity. To address this question, this
paper proposes a two-stage prompt selection strategy EmoPro, which is
specifically designed for emotionally controllable speech synthesis. This
strategy focuses on selecting highly expressive and high-quality prompts by
evaluating them from four perspectives: emotional expression strength, speech
quality, text-emotion consistency, and model generation performance.
Experimental results show that prompts selected using the proposed method
result in more emotionally expressive and engaging synthesized speech compared
to those obtained through baseline. Audio samples and codes will be available
at https://whyrrrrun.github.io/EmoPro/.",2024-09-27,"Haoyu Wang, Chunyu Qiang, Tianrui Wang, Cheng Gong, Qiuyu Liu, Yu Jiang, Xiaobao Wang, Chenyang Wang, Chen Zhang",http://arxiv.org/pdf/2409.18512v1,cs.CL
Do We Need Domain-Specific Embedding Models? An Empirical Investigation,"Embedding models play a crucial role in representing and retrieving
information across various NLP applications. Recent advancements in Large
Language Models (LLMs) have further enhanced the performance of embedding
models, which are trained on massive amounts of text covering almost every
domain. These models are often benchmarked on general-purpose datasets like
Massive Text Embedding Benchmark (MTEB), where they demonstrate superior
performance. However, a critical question arises: Is the development of
domain-specific embedding models necessary when general-purpose models are
trained on vast corpora that already include specialized domain texts? In this
paper, we empirically investigate this question, choosing the finance domain as
an example. We introduce the Finance Massive Text Embedding Benchmark
(FinMTEB), a counterpart to MTEB that consists of financial domain-specific
text datasets. We evaluate the performance of seven state-of-the-art embedding
models on FinMTEB and observe a significant performance drop compared to their
performance on MTEB. To account for the possibility that this drop is driven by
FinMTEB's higher complexity, we propose four measures to quantify dataset
complexity and control for this factor in our analysis. Our analysis provides
compelling evidence that state-of-the-art embedding models struggle to capture
domain-specific linguistic and semantic patterns. Moreover, we find that the
performance of general-purpose embedding models on MTEB is not correlated with
their performance on FinMTEB, indicating the need for domain-specific embedding
benchmarks for domain-specific embedding models. This study sheds light on
developing domain-specific embedding models in the LLM era. FinMTEB comes with
open-source code at https://github.com/yixuantt/FinMTEB",2024-09-27,"Yixuan Tang, Yi Yang",http://arxiv.org/pdf/2409.18511v4,cs.CL
Evaluation of OpenAI o1: Opportunities and Challenges of AGI,"This comprehensive study evaluates the performance of OpenAI's o1-preview
large language model across a diverse array of complex reasoning tasks,
spanning multiple domains, including computer science, mathematics, natural
sciences, medicine, linguistics, and social sciences. Through rigorous testing,
o1-preview demonstrated remarkable capabilities, often achieving human-level or
superior performance in areas ranging from coding challenges to scientific
reasoning and from language processing to creative problem-solving. Key
findings include:
  -83.3% success rate in solving complex competitive programming problems,
surpassing many human experts.
  -Superior ability in generating coherent and accurate radiology reports,
outperforming other evaluated models.
  -100% accuracy in high school-level mathematical reasoning tasks, providing
detailed step-by-step solutions.
  -Advanced natural language inference capabilities across general and
specialized domains like medicine.
  -Impressive performance in chip design tasks, outperforming specialized
models in areas such as EDA script generation and bug analysis.
  -Remarkable proficiency in anthropology and geology, demonstrating deep
understanding and reasoning in these specialized fields.
  -Strong capabilities in quantitative investing. O1 has comprehensive
financial knowledge and statistical modeling skills.
  -Effective performance in social media analysis, including sentiment analysis
and emotion recognition.
  The model excelled particularly in tasks requiring intricate reasoning and
knowledge integration across various fields. While some limitations were
observed, including occasional errors on simpler problems and challenges with
certain highly specialized concepts, the overall results indicate significant
progress towards artificial general intelligence.",2024-09-27,"Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, Chao Cao, Hanqi Jiang, Hanxu Chen, Yiwei Li, Junhao Chen, Huawen Hu, Yihen Liu, Huaqin Zhao, Shaochen Xu, Haixing Dai, Lin Zhao, Ruidong Zhang, Wei Zhao, Zhenyuan Yang, Jingyuan Chen, Peilong Wang, Wei Ruan, Hui Wang, Huan Zhao, Jing Zhang, Yiming Ren, Shihuan Qin, Tong Chen, Jiaxi Li, Arif Hassan Zidan, Afrar Jahin, Minheng Chen, Sichen Xia, Jason Holmes, Yan Zhuang, Jiaqi Wang, Bochen Xu, Weiran Xia, Jichao Yu, Kaibo Tang, Yaxuan Yang, Bolun Sun, Tao Yang, Guoyu Lu, Xianqiao Wang, Lilong Chai, He Li, Jin Lu, Lichao Sun, Xin Zhang, Bao Ge, Xintao Hu, Lian Zhang, Hua Zhou, Lu Zhang, Shu Zhang, Ninghao Liu, Bei Jiang, Linglong Kong, Zhen Xiang, Yudan Ren, Jun Liu, Xi Jiang, Yu Bao, Wei Zhang, Xiang Li, Gang Li, Wei Liu, Dinggang Shen, Andrea Sikora, Xiaoming Zhai, Dajiang Zhu, Tianming Liu",http://arxiv.org/pdf/2409.18486v1,cs.CL
URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological and Multilingual Knowledge Base,"URIEL is a knowledge base offering geographical, phylogenetic, and
typological vector representations for 7970 languages. It includes distance
measures between these vectors for 4005 languages, which are accessible via the
lang2vec tool. Despite being frequently cited, URIEL is limited in terms of
linguistic inclusion and overall usability. To tackle these challenges, we
introduce URIEL+, an enhanced version of URIEL and lang2vec that addresses
these limitations. In addition to expanding typological feature coverage for
2898 languages, URIEL+ improves the user experience with robust, customizable
distance calculations to better suit the needs of users. These upgrades also
offer competitive performance on downstream tasks and provide distances that
better align with linguistic distance studies.",2024-09-27,"Aditya Khan, Mason Shipton, David Anugraha, Kaiyao Duan, Phuong H. Hoang, Eric Khiu, A. Seza Doğruöz, En-Shiun Annie Lee",http://arxiv.org/pdf/2409.18472v3,cs.CL
Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications,"The rapid increase in unstructured data across various fields has made
multi-document comprehension and summarization a critical task. Traditional
approaches often fail to capture relevant context, maintain logical
consistency, and extract essential information from lengthy documents. This
paper explores the use of Long-context Large Language Models (LLMs) for
multi-document summarization, demonstrating their exceptional capacity to grasp
extensive connections, provide cohesive summaries, and adapt to various
industry domains and integration with enterprise applications/systems. The
paper discusses the workflow of multi-document summarization for effectively
deploying long-context LLMs, supported by case studies in legal applications,
enterprise functions such as HR, finance, and sourcing, as well as in the
medical and news domains. These case studies show notable enhancements in both
efficiency and accuracy. Technical obstacles, such as dataset diversity, model
scalability, and ethical considerations like bias mitigation and factual
accuracy, are carefully analyzed. Prospective research avenues are suggested to
augment the functionalities and applications of long-context LLMs, establishing
them as pivotal tools for transforming information processing across diverse
sectors and enterprise applications.",2024-09-27,"Aditi Godbole, Jabin Geevarghese George, Smita Shandilya",http://arxiv.org/pdf/2409.18454v1,cs.CL
Exploring Language Model Generalization in Low-Resource Extractive QA,"In this paper, we investigate Extractive Question Answering (EQA) with Large
Language Models (LLMs) under domain drift, i.e., can LLMs generalize to domains
that require specific knowledge such as medicine and law in a zero-shot fashion
without additional in-domain training? To this end, we devise a series of
experiments to explain the performance gap empirically. Our findings suggest
that: (a) LLMs struggle with dataset demands of closed domains such as
retrieving long answer spans; (b) Certain LLMs, despite showing strong overall
performance, display weaknesses in meeting basic requirements as discriminating
between domain-specific senses of words which we link to pre-processing
decisions; (c) Scaling model parameters is not always effective for cross
domain generalization; and (d) Closed-domain datasets are quantitatively much
different than open-domain EQA datasets and current LLMs struggle to deal with
them. Our findings point out important directions for improving existing LLMs.",2024-09-27,"Saptarshi Sengupta, Wenpeng Yin, Preslav Nakov, Shreya Ghosh, Suhang Wang",http://arxiv.org/pdf/2409.18446v2,cs.CL
Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization,"While generalization over tasks from easy to hard is crucial to profile
language models (LLMs), the datasets with fine-grained difficulty annotations
for each problem across a broad range of complexity are still blank. Aiming to
address this limitation, we present Easy2Hard-Bench, a consistently formatted
collection of 6 benchmark datasets spanning various domains, such as
mathematics and programming problems, chess puzzles, and reasoning questions.
Each problem within these datasets is annotated with numerical difficulty
scores. To systematically estimate problem difficulties, we collect abundant
performance data on attempts to each problem by humans in the real world or
LLMs on the prominent leaderboard. Leveraging the rich performance data, we
apply well-established difficulty ranking systems, such as Item Response Theory
(IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to
problems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from
previous collections by a higher proportion of challenging problems. Through
extensive experiments with six state-of-the-art LLMs, we provide a
comprehensive analysis of their performance and generalization capabilities
across varying levels of difficulty, with the aim of inspiring future research
in LLM generalization. The datasets are available at
https://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.",2024-09-27,"Mucong Ding, Chenghao Deng, Jocelyn Choo, Zichu Wu, Aakriti Agrawal, Avi Schwarzschild, Tianyi Zhou, Tom Goldstein, John Langford, Anima Anandkumar, Furong Huang",http://arxiv.org/pdf/2409.18433v1,cs.CL
Improving Multilingual ASR in the Wild Using Simple N-best Re-ranking,"Multilingual Automatic Speech Recognition (ASR) models are typically
evaluated in a setting where the ground-truth language of the speech utterance
is known, however, this is often not the case for most practical settings.
Automatic Spoken Language Identification (SLID) models are not perfect and
misclassifications have a substantial impact on the final ASR accuracy. In this
paper, we present a simple and effective N-best re-ranking approach to improve
multilingual ASR accuracy for several prominent acoustic models by employing
external features such as language models and text-based language
identification models. Our results on FLEURS using the MMS and Whisper models
show spoken language identification accuracy improvements of 8.7% and 6.1%,
respectively and word error rates which are 3.3% and 2.0% lower on these
benchmarks.",2024-09-27,"Brian Yan, Vineel Pratap, Shinji Watanabe, Michael Auli",http://arxiv.org/pdf/2409.18428v1,cs.CL
VickreyFeedback: Cost-efficient Data Construction for Reinforcement Learning from Human Feedback,"This paper addresses the cost-efficiency aspect of Reinforcement Learning
from Human Feedback (RLHF). RLHF leverages datasets of human preferences over
outputs of large language models (LLM)s to instill human expectations into
LLMs. Although preference annotation comes with a monetized cost, the economic
utility of a preference dataset has not been considered by far. What
exacerbates this situation is that, given complex intransitive or cyclic
relationships in preference datasets, existing algorithms for fine-tuning LLMs
are still far from capturing comprehensive preferences. This raises severe
cost-efficiency concerns in production environments, where preference data
accumulate over time. In this paper, we discuss the fine-tuning of LLMs as a
monetized economy and introduce an auction mechanism to improve the efficiency
of preference data collection in dollar terms. We show that introducing an
auction mechanism can play an essential role in enhancing the cost-efficiency
of RLHF, while maintaining satisfactory model performance. Experimental results
demonstrate that our proposed auction-based protocol is cost-effective for
fine-tuning LLMs concentrating on high-quality feedback.",2024-09-27,"Guoxi Zhang, Jiuding Duan",http://arxiv.org/pdf/2409.18417v2,cs.CL
SciDFM: A Large Language Model with Mixture-of-Experts for Science,"Recently, there has been a significant upsurge of interest in leveraging
large language models (LLMs) to assist scientific discovery. However, most LLMs
only focus on general science, while they lack domain-specific knowledge, such
as chemical molecules and amino acid sequences. To bridge these gaps, we
introduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and
is able to conduct college-level scientific reasoning and understand molecules
and amino acid sequences. We collect a large-scale training corpus containing
numerous scientific papers and books from different disciplines as well as data
from domain-specific databases. We further fine-tune the pre-trained model on
lots of instruction data to improve performances on downstream benchmarks. From
experiment results, we show that SciDFM achieves strong performance on general
scientific benchmarks such as SciEval and SciQ, and it reaches a SOTA
performance on domain-specific benchmarks among models of similar size. We
further analyze the expert layers and show that the results of expert selection
vary with data from different disciplines. To benefit the broader research
community, we open-source SciDFM at
https://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.",2024-09-27,"Liangtai Sun, Danyu Luo, Da Ma, Zihan Zhao, Baocai Chen, Zhennan Shen, Su Zhu, Lu Chen, Xin Chen, Kai Yu",http://arxiv.org/pdf/2409.18412v3,cs.CL
Defect Prediction with Content-based Features,"Traditional defect prediction approaches often use metrics that measure the
complexity of the design or implementing code of a software system, such as the
number of lines of code in a source file. In this paper, we explore a different
approach based on content of source code. Our key assumption is that source
code of a software system contains information about its technical aspects and
those aspects might have different levels of defect-proneness. Thus,
content-based features such as words, topics, data types, and package names
extracted from a source code file could be used to predict its defects. We have
performed an extensive empirical evaluation and found that: i) such
content-based features have higher predictive power than code complexity
metrics and ii) the use of feature selection, reduction, and combination
further improves the prediction performance.",2024-09-27,"Hung Viet Pham, Tung Thanh Nguyen",http://arxiv.org/pdf/2409.18365v1,cs.CL
Building a Chinese Medical Dialogue System: Integrating Large-scale Corpora and Novel Models,"The global COVID-19 pandemic underscored major deficiencies in traditional
healthcare systems, hastening the advancement of online medical services,
especially in medical triage and consultation. However, existing studies face
two main challenges. First, the scarcity of large-scale, publicly available,
domain-specific medical datasets due to privacy concerns, with current datasets
being small and limited to a few diseases, limiting the effectiveness of triage
methods based on Pre-trained Language Models (PLMs). Second, existing methods
lack medical knowledge and struggle to accurately understand professional terms
and expressions in patient-doctor consultations. To overcome these obstacles,
we construct the Large-scale Chinese Medical Dialogue Corpora (LCMDC), thereby
addressing the data shortage in this field. Moreover, we further propose a
novel triage system that combines BERT-based supervised learning with prompt
learning, as well as a GPT-based medical consultation model. To enhance domain
knowledge acquisition, we pre-trained PLMs using our self-constructed
background corpus. Experimental results on the LCMDC demonstrate the efficacy
of our proposed systems.",2024-09-27,"Xinyuan Wang, Haozhou Li, Dingfang Zheng, Qinke Peng",http://arxiv.org/pdf/2410.03521v2,cs.CL
MultiClimate: Multimodal Stance Detection on Climate Change Videos,"Climate change (CC) has attracted increasing attention in NLP in recent
years. However, detecting the stance on CC in multimodal data is understudied
and remains challenging due to a lack of reliable datasets. To improve the
understanding of public opinions and communication strategies, this paper
presents MultiClimate, the first open-source manually-annotated stance
detection dataset with $100$ CC-related YouTube videos and $4,209$
frame-transcript pairs. We deploy state-of-the-art vision and language models,
as well as multimodal models for MultiClimate stance detection. Results show
that text-only BERT significantly outperforms image-only ResNet50 and ViT.
Combining both modalities achieves state-of-the-art, $0.747$/$0.749$ in
accuracy/F1. Our 100M-sized fusion models also beat CLIP and BLIP, as well as
the much larger 9B-sized multimodal IDEFICS and text-only Llama3 and Gemma2,
indicating that multimodal stance detection remains challenging for large
language models. Our code, dataset, as well as supplementary materials, are
available at https://github.com/werywjw/MultiClimate.",2024-09-26,"Jiawen Wang, Longfei Zuo, Siyao Peng, Barbara Plank",http://arxiv.org/pdf/2409.18346v1,cs.CL
A Generalized LLM-Augmented BIM Framework: Application to a Speech-to-BIM system,"Performing building information modeling (BIM) tasks is a complex process
that imposes a steep learning curve and a heavy cognitive load due to the
necessity of remembering sequences of numerous commands. With the rapid
advancement of large language models (LLMs), it is foreseeable that BIM tasks,
including querying and managing BIM data, 4D and 5D BIM, design compliance
checking, or authoring a design, using written or spoken natural language
(i.e., text-to-BIM or speech-to-BIM), will soon supplant traditional graphical
user interfaces. This paper proposes a generalized LLM-augmented BIM framework
to expedite the development of LLM-enhanced BIM applications by providing a
step-by-step development process. The proposed framework consists of six steps:
interpret-fill-match-structure-execute-check. The paper demonstrates the
applicability of the proposed framework through implementing a speech-to-BIM
application, NADIA-S (Natural-language-based Architectural Detailing through
Interaction with Artificial Intelligence via Speech), using exterior wall
detailing as an example.",2024-09-26,"Ghang Lee, Suhyung Jang, Seokho Hyun",http://arxiv.org/pdf/2409.18345v1,cs.CL
AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language Models,"Recent advancements in Large Language Models (LLMs) have demonstrated great
success in many Natural Language Processing (NLP) tasks. In addition to their
cognitive intelligence, exploring their capabilities in emotional intelligence
is also crucial, as it enables more natural and empathetic conversational AI.
Recent studies have shown LLMs' capability in recognizing emotions, but they
often focus on single emotion labels and overlook the complex and ambiguous
nature of human emotions. This study is the first to address this gap by
exploring the potential of LLMs in recognizing ambiguous emotions, leveraging
their strong generalization capabilities and in-context learning. We design
zero-shot and few-shot prompting and incorporate past dialogue as context
information for ambiguous emotion recognition. Experiments conducted using
three datasets indicate significant potential for LLMs in recognizing ambiguous
emotions, and highlight the substantial benefits of including context
information. Furthermore, our findings indicate that LLMs demonstrate a high
degree of effectiveness in recognizing less ambiguous emotions and exhibit
potential for identifying more ambiguous emotions, paralleling human perceptual
capabilities.",2024-09-26,"Xin Hong, Yuan Gong, Vidhyasaharan Sethu, Ting Dang",http://arxiv.org/pdf/2409.18339v2,cs.CL
A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies,"Despite recent advancements in AI and NLP, negotiation remains a difficult
domain for AI agents. Traditional game theoretic approaches that have worked
well for two-player zero-sum games struggle in the context of negotiation due
to their inability to learn human-compatible strategies. On the other hand,
approaches that only use human data tend to be domain-specific and lack the
theoretical guarantees provided by strategies grounded in game theory.
Motivated by the notion of fairness as a criterion for optimality in general
sum games, we propose a negotiation framework called FDHC which incorporates
fairness into both the reward design and search to learn human-compatible
negotiation strategies. Our method includes a novel, RL+search technique called
LGM-Zero which leverages a pre-trained language model to retrieve
human-compatible offers from large action spaces. Our results show that our
method is able to achieve more egalitarian negotiation outcomes and improve
negotiation quality.",2024-09-26,"Ryan Shea, Zhou Yu",http://arxiv.org/pdf/2409.18335v1,cs.CL
Development and Validation of a Dynamic-Template-Constrained Large Language Model for Generating Fully-Structured Radiology Reports,"Current LLMs for creating fully-structured reports face the challenges of
formatting errors, content hallucinations, and privacy leakage issues when
uploading data to external servers.We aim to develop an open-source, accurate
LLM for creating fully-structured and standardized LCS reports from varying
free-text reports across institutions and demonstrate its utility in automatic
statistical analysis and individual lung nodule retrieval. With IRB approvals,
our retrospective study included 5,442 de-identified LDCT LCS radiology reports
from two institutions. We constructed two evaluation datasets by labeling 500
pairs of free-text and fully-structured radiology reports and one large-scale
consecutive dataset from January 2021 to December 2023. Two radiologists
created a standardized template for recording 27 lung nodule features on LCS.
We designed a dynamic-template-constrained decoding method to enhance existing
LLMs for creating fully-structured reports from free-text radiology reports.
Using consecutive structured reports, we automated descriptive statistical
analyses and a nodule retrieval prototype. Our best LLM for creating
fully-structured reports achieved high performance on cross-institutional
datasets with an F1 score of about 97%, with neither formatting errors nor
content hallucinations. Our method consistently improved the best open-source
LLMs by up to 10.42%, and outperformed GPT-4o by 17.19%. The automatically
derived statistical distributions were consistent with prior findings regarding
attenuation, location, size, stability, and Lung-RADS. The retrieval system
with structured reports allowed flexible nodule-level search and complex
statistical analysis. Our developed software is publicly available for local
deployment and further research.",2024-09-26,"Chuang Niu, Parisa Kaviani, Qing Lyu, Mannudeep K. Kalra, Christopher T. Whitlow, Ge Wang",http://arxiv.org/pdf/2409.18319v2,cs.CL
Realistic Evaluation of Model Merging for Compositional Generalization,"Merging has become a widespread way to cheaply combine individual models into
a single model that inherits their capabilities and attains better performance.
This popularity has spurred rapid development of many new merging methods,
which are typically validated in disparate experimental settings and frequently
differ in the assumptions made about model architecture, data availability, and
computational budget. In this work, we characterize the relative merits of
different merging methods by evaluating them in a shared experimental setting
and precisely identifying the practical requirements of each method.
Specifically, our setting focuses on using merging for compositional
generalization of capabilities in image classification, image generation, and
natural language processing. Additionally, we measure the computational costs
of different merging methods as well as how they perform when scaling the
number of models being merged. Taken together, our results clarify the state of
the field of model merging and provide a comprehensive and rigorous
experimental setup to test new methods.",2024-09-26,"Derek Tam, Yash Kant, Brian Lester, Igor Gilitschenski, Colin Raffel",http://arxiv.org/pdf/2409.18314v1,cs.CL
Advancing Object Detection in Transportation with Multimodal Large Language Models (MLLMs): A Comprehensive Review and Empirical Testing,"This study aims to comprehensively review and empirically evaluate the
application of multimodal large language models (MLLMs) and Large Vision Models
(VLMs) in object detection for transportation systems. In the first fold, we
provide a background about the potential benefits of MLLMs in transportation
applications and conduct a comprehensive review of current MLLM technologies in
previous studies. We highlight their effectiveness and limitations in object
detection within various transportation scenarios. The second fold involves
providing an overview of the taxonomy of end-to-end object detection in
transportation applications and future directions. Building on this, we
proposed empirical analysis for testing MLLMs on three real-world
transportation problems that include object detection tasks namely, road safety
attributes extraction, safety-critical event detection, and visual reasoning of
thermal images. Our findings provide a detailed assessment of MLLM performance,
uncovering both strengths and areas for improvement. Finally, we discuss
practical limitations and challenges of MLLMs in enhancing object detection in
transportation, thereby offering a roadmap for future research and development
in this critical area.",2024-09-26,"Huthaifa I. Ashqar, Ahmed Jaber, Taqwa I. Alhadidi, Mohammed Elhenawy",http://arxiv.org/pdf/2409.18286v1,cs.CL
DisGeM: Distractor Generation for Multiple Choice Questions with Span Masking,"Recent advancements in Natural Language Processing (NLP) have impacted
numerous sub-fields such as natural language generation, natural language
inference, question answering, and more. However, in the field of question
generation, the creation of distractors for multiple-choice questions (MCQ)
remains a challenging task. In this work, we present a simple, generic
framework for distractor generation using readily available Pre-trained
Language Models (PLMs). Unlike previous methods, our framework relies solely on
pre-trained language models and does not require additional training on
specific datasets. Building upon previous research, we introduce a two-stage
framework consisting of candidate generation and candidate selection. Our
proposed distractor generation framework outperforms previous methods without
the need for training or fine-tuning. Human evaluations confirm that our
approach produces more effective and engaging distractors. The related codebase
is publicly available at https://github.com/obss/disgem.",2024-09-26,"Devrim Cavusoglu, Secil Sen, Ulas Sert",http://arxiv.org/pdf/2409.18263v1,cs.CL
MMMT-IF: A Challenging Multimodal Multi-Turn Instruction Following Benchmark,"Evaluating instruction following capabilities for multimodal, multi-turn
dialogue is challenging. With potentially multiple instructions in the input
model context, the task is time-consuming for human raters and we show LLM
based judges are biased towards answers from the same model. We propose
MMMT-IF, an image based multi-turn Q$\&$A evaluation set with added global
instructions between questions, constraining the answer format. This challenges
models to retrieve instructions dispersed across long dialogues and reason
under instruction constraints. All instructions are objectively verifiable
through code execution. We introduce the Programmatic Instruction Following
($\operatorname{PIF}$) metric to measure the fraction of the instructions that
are correctly followed while performing a reasoning task. The
$\operatorname{PIF-N-K}$ set of metrics further evaluates robustness by
measuring the fraction of samples in a corpus where, for each sample, at least
K out of N generated model responses achieve a $\operatorname{PIF}$ score of
one. The $\operatorname{PIF}$ metric aligns with human instruction following
ratings, showing 60 percent correlation. Experiments show Gemini 1.5 Pro,
GPT-4o, and Claude 3.5 Sonnet, have a $\operatorname{PIF}$ metric that drops
from 0.81 on average at turn 1 across the models, to 0.64 at turn 20. Across
all turns, when each response is repeated 4 times ($\operatorname{PIF-4-4}$),
GPT-4o and Gemini successfully follow all instructions only $11\%$ of the time.
When all the instructions are also appended to the end of the model input
context, the $\operatorname{PIF}$ metric improves by 22.3 points on average,
showing that the challenge with the task lies not only in following the
instructions, but also in retrieving the instructions spread out in the model
context. We plan to open source the MMMT-IF dataset and metric computation
code.",2024-09-26,"Elliot L. Epstein, Kaisheng Yao, Jing Li, Xinyi Bai, Hamid Palangi",http://arxiv.org/pdf/2409.18216v1,cs.CL
AI Policy Projector: Grounding LLM Policy Design in Iterative Mapmaking,"Whether a large language model policy is an explicit constitution or an
implicit reward model, it is challenging to assess coverage over the unbounded
set of real-world situations that a policy must contend with. We introduce an
AI policy design process inspired by mapmaking, which has developed tactics for
visualizing and iterating on maps even when full coverage is not possible. With
Policy Projector, policy designers can survey the landscape of model
input-output pairs, define custom regions (e.g., ""violence""), and navigate
these regions with rules that can be applied to LLM outputs (e.g., if output
contains ""violence"" and ""graphic details,"" then rewrite without ""graphic
details""). Policy Projector supports interactive policy authoring using LLM
classification and steering and a map visualization reflecting the policy
designer's work. In an evaluation with 12 AI safety experts, our system helps
policy designers to address problematic model behaviors extending beyond an
existing, comprehensive harm taxonomy.",2024-09-26,"Michelle S. Lam, Fred Hohman, Dominik Moritz, Jeffrey P. Bigham, Kenneth Holstein, Mary Beth Kery",http://arxiv.org/pdf/2409.18203v1,cs.CL
LangSAMP: Language-Script Aware Multilingual Pretraining,"Recent multilingual pretrained language models (mPLMs) often avoid using
language embeddings -- learnable vectors assigned to individual languages.
However, this places a significant burden on token representations to encode
all language-specific information, which may hinder language neutrality. To
address this limitation, we propose Language-Script Aware Multilingual
Pretraining (LangSAMP), a method that incorporates both language and script
embeddings to enhance representation learning. Specifically, we integrate these
embeddings into the output of the Transformer blocks before passing the final
representations to the language modeling head for prediction. We apply LangSAMP
to the continual pretraining of XLM-R on a highly multilingual corpus covering
more than 500 languages. The resulting model consistently outperforms the
baseline in zero-shot crosslingual transfer across diverse downstream tasks.
Extensive analysis reveals that language and script embeddings capture
language- and script-specific nuances, which benefits more language-neutral
representations, proven by improved pairwise cosine similarity. In our case
study, we also show that language and script embeddings can be used to select
better source languages for crosslingual transfer. We make our code and models
publicly available at https://github.com/cisnlp/LangSAMP.",2024-09-26,"Yihong Liu, Haotian Ye, Chunlan Ma, Mingyang Wang, Hinrich Schütze",http://arxiv.org/pdf/2409.18199v2,cs.CL
Exploring LLM-Driven Explanations for Quantum Algorithms,"Background: Quantum computing is a rapidly growing new programming paradigm
that brings significant changes to the design and implementation of algorithms.
Understanding quantum algorithms requires knowledge of physics and mathematics,
which can be challenging for software developers.
  Aims: In this work, we provide a first analysis of how LLMs can support
developers' understanding of quantum code. Method: We empirically analyse and
compare the quality of explanations provided by three widely adopted LLMs
(Gpt3.5, Llama2, and Tinyllama) using two different human-written prompt styles
for seven state-of-the-art quantum algorithms. We also analyse how consistent
LLM explanations are over multiple rounds and how LLMs can improve existing
descriptions of quantum algorithms.
  Results: Llama2 provides the highest quality explanations from scratch, while
Gpt3.5 emerged as the LLM best suited to improve existing explanations. In
addition, we show that adding a small amount of context to the prompt
significantly improves the quality of explanations. Finally, we observe how
explanations are qualitatively and syntactically consistent over multiple
rounds.
  Conclusions: This work highlights promising results, and opens challenges for
future research in the field of LLMs for quantum code explanation. Future work
includes refining the methods through prompt optimisation and parsing of
quantum code explanations, as well as carrying out a systematic assessment of
the quality of explanations.",2024-09-26,"Giordano d'Aloisio, Sophie Fortz, Carol Hanna, Daniel Fortunato, Avner Bensoussan, Eñaut Mendiluze Usandizaga, Federica Sarro",http://arxiv.org/pdf/2409.19028v1,cs.CL
GrEmLIn: A Repository of Green Baseline Embeddings for 87 Low-Resource Languages Injected with Multilingual Graph Knowledge,"Contextualized embeddings based on large language models (LLMs) are available
for various languages, but their coverage is often limited for lower resourced
languages. Using LLMs for such languages is often difficult due to a high
computational cost; not only during training, but also during inference. Static
word embeddings are much more resource-efficient (""green""), and thus still
provide value, particularly for very low-resource languages. There is, however,
a notable lack of comprehensive repositories with such embeddings for diverse
languages. To address this gap, we present GrEmLIn, a centralized repository of
green, static baseline embeddings for 87 mid- and low-resource languages. We
compute GrEmLIn embeddings with a novel method that enhances GloVe embeddings
by integrating multilingual graph knowledge, which makes our static embeddings
competitive with LLM representations, while being parameter-free at inference
time. Our experiments demonstrate that GrEmLIn embeddings outperform
state-of-the-art contextualized embeddings from E5 on the task of lexical
similarity. They remain competitive in extrinsic evaluation tasks like
sentiment analysis and natural language inference, with average performance
gaps of just 5-10\% or less compared to state-of-the-art models, given a
sufficient vocabulary overlap with the target task, and underperform only on
topic classification. Our code and embeddings are publicly available at
https://huggingface.co/DFKI.",2024-09-26,"Daniil Gurgurov, Rishu Kumar, Simon Ostermann",http://arxiv.org/pdf/2409.18193v3,cs.CL
Evaluation of Large Language Models for Summarization Tasks in the Medical Domain: A Narrative Review,"Large Language Models have advanced clinical Natural Language Generation,
creating opportunities to manage the volume of medical text. However, the
high-stakes nature of medicine requires reliable evaluation, which remains a
challenge. In this narrative review, we assess the current evaluation state for
clinical summarization tasks and propose future directions to address the
resource constraints of expert human evaluation.",2024-09-26,"Emma Croxford, Yanjun Gao, Nicholas Pellegrino, Karen K. Wong, Graham Wills, Elliot First, Frank J. Liao, Cherodeep Goswami, Brian Patterson, Majid Afshar",http://arxiv.org/pdf/2409.18170v1,cs.CL
Open-World Evaluation for Retrieving Diverse Perspectives,"We study retrieving a set of documents that covers various perspectives on a
complex and contentious question (e.g., will ChatGPT do more harm than good?).
We curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS),
where each example consists of a question and diverse perspectives associated
with the question, sourced from survey questions and debate websites. On this
data, retrievers paired with a corpus are evaluated to surface a document set
that contains diverse perspectives. Our framing diverges from most retrieval
tasks in that document relevancy cannot be decided by simple string matches to
references. Instead, we build a language model-based automatic evaluator that
decides whether each retrieved document contains a perspective. This allows us
to evaluate the performance of three different types of corpus (Wikipedia, web
snapshot, and corpus constructed on the fly with retrieved pages from the
search engine) paired with retrievers. Retrieving diverse documents remains
challenging, with the outputs from existing retrievers covering all
perspectives on only 40% of the examples. We further study the effectiveness of
query expansion and diversity-focused reranking approaches and analyze
retriever sycophancy.",2024-09-26,"Hung-Ting Chen, Eunsol Choi",http://arxiv.org/pdf/2409.18110v2,cs.CL
Data-Prep-Kit: getting your data ready for LLM application development,"Data preparation is the first and a very important step towards any Large
Language Model (LLM) development. This paper introduces an easy-to-use,
extensible, and scale-flexible open-source data preparation toolkit called Data
Prep Kit (DPK). DPK is architected and designed to enable users to scale their
data preparation to their needs. With DPK they can prepare data on a local
machine or effortlessly scale to run on a cluster with thousands of CPU Cores.
DPK comes with a highly scalable, yet extensible set of modules that transform
natural language and code data. If the user needs additional transforms, they
can be easily developed using extensive DPK support for transform creation.
These modules can be used independently or pipelined to perform a series of
operations. In this paper, we describe DPK architecture and show its
performance from a small scale to a very large number of CPUs. The modules from
DPK have been used for the preparation of Granite Models [1] [2]. We believe
DPK is a valuable contribution to the AI community to easily prepare data to
enhance the performance of their LLM models or to fine-tune models with
Retrieval-Augmented Generation (RAG).",2024-09-26,"David Wood, Boris Lublinsky, Alexy Roytman, Shivdeep Singh, Constantin Adam, Abdulhamid Adebayo, Sungeun An, Yuan Chi Chang, Xuan-Hong Dang, Nirmit Desai, Michele Dolfi, Hajar Emami-Gohari, Revital Eres, Takuya Goto, Dhiraj Joshi, Yan Koyfman, Mohammad Nassar, Hima Patel, Paramesvaran Selvam, Yousaf Shah, Saptha Surendran, Daiki Tsuzuku, Petros Zerfos, Shahrokh Daijavad",http://arxiv.org/pdf/2409.18164v2,cs.CL
Infer Human's Intentions Before Following Natural Language Instructions,"For AI agents to be helpful to humans, they should be able to follow natural
language instructions to complete everyday cooperative tasks in human
environments. However, real human instructions inherently possess ambiguity,
because the human speakers assume sufficient prior knowledge about their hidden
goals and intentions. Standard language grounding and planning methods fail to
address such ambiguities because they do not model human internal goals as
additional partially observable factors in the environment. We propose a new
framework, Follow Instructions with Social and Embodied Reasoning (FISER),
aiming for better natural language instruction following in collaborative
embodied tasks. Our framework makes explicit inferences about human goals and
intentions as intermediate reasoning steps. We implement a set of
Transformer-based models and evaluate them over a challenging benchmark,
HandMeThat. We empirically demonstrate that using social reasoning to
explicitly infer human intentions before making action plans surpasses purely
end-to-end approaches. We also compare our implementation with strong
baselines, including Chain of Thought prompting on the largest available
pre-trained language models, and find that FISER provides better performance on
the embodied social reasoning tasks under investigation, reaching the
state-of-the-art on HandMeThat.",2024-09-26,"Yanming Wan, Yue Wu, Yiping Wang, Jiayuan Mao, Natasha Jaques",http://arxiv.org/pdf/2409.18073v1,cs.CL
IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning,"Recent advancements in image captioning have explored text-only training
methods to overcome the limitations of paired image-text data. However,
existing text-only training methods often overlook the modality gap between
using text data during training and employing images during inference. To
address this issue, we propose a novel approach called Image-like Retrieval,
which aligns text features with visually relevant features to mitigate the
modality gap. Our method further enhances the accuracy of generated captions by
designing a Fusion Module that integrates retrieved captions with input
features. Additionally, we introduce a Frequency-based Entity Filtering
technique that significantly improves caption quality. We integrate these
methods into a unified framework, which we refer to as IFCap
($\textbf{I}$mage-like Retrieval and $\textbf{F}$requency-based Entity
Filtering for Zero-shot $\textbf{Cap}$tioning). Through extensive
experimentation, our straightforward yet powerful approach has demonstrated its
efficacy, outperforming the state-of-the-art methods by a significant margin in
both image captioning and video captioning compared to zero-shot captioning
based on text-only training.",2024-09-26,"Soeun Lee, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim",http://arxiv.org/pdf/2409.18046v1,cs.CL
Unveiling the Role of Pretraining in Direct Speech Translation,"Direct speech-to-text translation systems encounter an important drawback in
data scarcity. A common solution consists on pretraining the encoder on
automatic speech recognition, hence losing efficiency in the training process.
In this study, we compare the training dynamics of a system using a pretrained
encoder, the conventional approach, and one trained from scratch. We observe
that, throughout the training, the randomly initialized model struggles to
incorporate information from the speech inputs for its predictions. Hence, we
hypothesize that this issue stems from the difficulty of effectively training
an encoder for direct speech translation. While a model trained from scratch
needs to learn acoustic and semantic modeling simultaneously, a pretrained one
can just focus on the latter. Based on these findings, we propose a subtle
change in the decoder cross-attention to integrate source information from
earlier steps in training. We show that with this change, the model trained
from scratch can achieve comparable performance to the pretrained one, while
reducing the training time.",2024-09-26,"Belen Alastruey, Gerard I. Gállego, Marta R. Costa-jussà",http://arxiv.org/pdf/2409.18044v1,cs.CL
"EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions","GPT-4o, an omni-modal model that enables vocal conversations with diverse
emotions and tones, marks a milestone for omni-modal foundation models.
However, empowering Large Language Models to perceive and generate images,
texts, and speeches end-to-end with publicly available data remains challenging
for the open-source community. Existing vision-language models rely on external
tools for speech processing, while speech-language models still suffer from
limited or totally without vision-understanding capabilities. To address this
gap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable
Large Language Models with end-to-end speech abilities while maintaining the
leading vision-language performance. With a semantic-acoustic disentangled
speech tokenizer, we surprisingly notice that omni-modal alignment can further
enhance vision-language and speech abilities compared with the bi-modal aligned
counterparts. Moreover, a lightweight style module is introduced for the
flexible speech style controls including emotions and pitches. For the first
time, EMOVA achieves state-of-the-art performance on both the vision-language
and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue
with vivid emotions.",2024-09-26,"Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, Dingdong Wang, Kun Xiang, Haoyuan Li, Haoli Bai, Jianhua Han, Xiaohui Li, Weike Jin, Nian Xie, Yu Zhang, James T. Kwok, Hengshuang Zhao, Xiaodan Liang, Dit-Yan Yeung, Xiao Chen, Zhenguo Li, Wei Zhang, Qun Liu, Jun Yao, Lanqing Hong, Lu Hou, Hang Xu",http://arxiv.org/pdf/2409.18042v4,cs.CL
Automated Detection and Analysis of Power Words in Persuasive Text Using Natural Language Processing,"Power words are terms that evoke strong emotional responses and significantly
influence readers' behavior, playing a crucial role in fields like marketing,
politics, and motivational writing. This study proposes a methodology for the
automated detection and analysis of power words in persuasive text using a
custom lexicon created from a comprehensive dataset scraped from online
sources. A specialized Python package, The Text Monger, is created and employed
to identify the presence and frequency of power words within a given text. By
analyzing diverse datasets, including fictional excerpts, speeches, and
marketing materials,the aim is to classify and assess the impact of power words
on sentiment and reader engagement. The findings provide valuable insights into
the effectiveness of power words across various domains, offering practical
applications for content creators, advertisers, and policymakers looking to
enhance their messaging and engagement strategies.",2024-09-26,Sahil Garje,http://arxiv.org/pdf/2409.18033v2,cs.CL
Compositional Hardness of Code in Large Language Models -- A Probabilistic Perspective,"A common practice in large language model (LLM) usage for complex analytical
tasks such as code generation, is to sample a solution for the entire task
within the model's context window. Previous works have shown that subtask
decomposition within the model's context (chain of thought), is beneficial for
solving such tasks. In this work, we point a limitation of LLMs' ability to
perform several sub-tasks within the same context window - an in-context
hardness of composition, pointing to an advantage for distributing a decomposed
problem in a multi-agent system of LLMs. The hardness of composition is
quantified by a generation complexity metric, i.e., the number of LLM
generations required to sample at least one correct solution. We find a gap
between the generation complexity of solving a compositional problem within the
same context relative to distributing it among multiple agents, that increases
exponentially with the solution's length. We prove our results theoretically
and demonstrate them empirically.",2024-09-26,"Yotam Wolf, Binyamin Rothberg, Dorin Shteyman, Amnon Shashua",http://arxiv.org/pdf/2409.18028v3,cs.CL
An Adversarial Perspective on Machine Unlearning for AI Safety,"Large language models are finetuned to refuse questions about hazardous
knowledge, but these protections can often be bypassed. Unlearning methods aim
at completely removing hazardous capabilities from models and make them
inaccessible to adversaries. This work challenges the fundamental differences
between unlearning and traditional safety post-training from an adversarial
perspective. We demonstrate that existing jailbreak methods, previously
reported as ineffective against unlearning, can be successful when applied
carefully. Furthermore, we develop a variety of adaptive methods that recover
most supposedly unlearned capabilities. For instance, we show that finetuning
on 10 unrelated examples or removing specific directions in the activation
space can recover most hazardous capabilities for models edited with RMU, a
state-of-the-art unlearning method. Our findings challenge the robustness of
current unlearning approaches and question their advantages over safety
training.",2024-09-26,"Jakub Łucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tramèr, Javier Rando",http://arxiv.org/pdf/2409.18025v5,cs.CL
DARE: Diverse Visual Question Answering with Robustness Evaluation,"Vision Language Models (VLMs) extend remarkable capabilities of text-only
large language models and vision-only models, and are able to learn from and
process multi-modal vision-text input. While modern VLMs perform well on a
number of standard image classification and image-text matching tasks, they
still struggle with a number of crucial vision-language (VL) reasoning
abilities such as counting and spatial reasoning. Moreover, while they might be
very brittle to small variations in instructions and/or evaluation protocols,
existing benchmarks fail to evaluate their robustness (or rather the lack of
it). In order to couple challenging VL scenarios with comprehensive robustness
evaluation, we introduce DARE, Diverse Visual Question Answering with
Robustness Evaluation, a carefully created and curated multiple-choice VQA
benchmark. DARE evaluates VLM performance on five diverse categories and
includes four robustness-oriented evaluations based on the variations of:
prompts, the subsets of answer options, the output format and the number of
correct answers. Among a spectrum of other findings, we report that
state-of-the-art VLMs still struggle with questions in most categories and are
unable to consistently deliver their peak performance across the tested
robustness evaluations. The worst case performance across the subsets of
options is up to 34% below the performance in the standard case. The robustness
of the open-source VLMs such as LLaVA 1.6 and Idefics2 cannot match the
closed-source models such as GPT-4 and Gemini, but even the latter remain very
brittle to different variations.",2024-09-26,"Hannah Sterz, Jonas Pfeiffer, Ivan Vulić",http://arxiv.org/pdf/2409.18023v1,cs.CL
Evaluating Multilingual Long-Context Models for Retrieval and Reasoning,"Recent large language models (LLMs) demonstrate impressive capabilities in
handling long contexts, some exhibiting near-perfect recall on synthetic
retrieval tasks. However, these evaluations have mainly focused on English text
and involved a single target sentence within lengthy contexts. Our work
investigates how LLM performance generalizes to multilingual settings with
multiple hidden target sentences. We create a new dataset -- mLongRR -- to
comprehensively evaluate several multilingual long-context LLMs on retrieval
and reasoning tasks across five languages: English, Vietnamese, Indonesian,
Swahili, and Somali. These languages share the Latin script but belong to
distinct language families and resource levels. Our analysis reveals a
significant performance gap between languages. The best-performing models such
as Gemini-1.5 and GPT-4o, achieve around 96% accuracy in English to around 36%
in Somali with a single target sentence. However, this accuracy drops to 40% in
English and 0% in Somali when dealing with three target sentences. Our findings
highlight the challenges long-context LLMs face when processing longer
contexts, an increase in the number of target sentences, or languages of lower
resource levels.",2024-09-26,"Ameeta Agrawal, Andy Dang, Sina Bagheri Nezhad, Rhitabrat Pokharel, Russell Scheinberg",http://arxiv.org/pdf/2409.18006v3,cs.CL
Extracting Affect Aggregates from Longitudinal Social Media Data with Temporal Adapters for Large Language Models,"This paper proposes temporally aligned Large Language Models (LLMs) as a tool
for longitudinal analysis of social media data. We fine-tune Temporal Adapters
for Llama 3 8B on full timelines from a panel of British Twitter users, and
extract longitudinal aggregates of emotions and attitudes with established
questionnaires. We focus our analysis on the beginning of the COVID-19 pandemic
that had a strong impact on public opinion and collective emotions. We validate
our estimates against representative British survey data and find strong
positive, significant correlations for several collective emotions. The
obtained estimates are robust across multiple training seeds and prompt
formulations, and in line with collective emotions extracted using a
traditional classification model trained on labeled data. We demonstrate the
flexibility of our method on questions of public opinion for which no
pre-trained classifier is available. Our work extends the analysis of affect in
LLMs to a longitudinal setting through Temporal Adapters. It enables flexible,
new approaches towards the longitudinal analysis of social media data.",2024-09-26,"Georg Ahnert, Max Pellert, David Garcia, Markus Strohmaier",http://arxiv.org/pdf/2409.17990v2,cs.CL
BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and Adaptive Disambiguate based Efficient Tree Search,"Large Language Models (LLMs) have exhibited exceptional performance across a
broad range of tasks and domains. However, they still encounter difficulties in
solving mathematical problems due to the rigorous and logical nature of
mathematics. Previous studies have employed techniques such as supervised
fine-tuning (SFT), prompt engineering, and search-based methods to improve the
mathematical problem-solving abilities of LLMs. Despite these efforts, their
performance remains suboptimal and demands substantial computational resources.
To address this issue, we propose a novel approach, BEATS, to enhance
mathematical problem-solving abilities. Our method leverages newly designed
prompts that guide the model to iteratively rewrite, advance by one step, and
generate answers based on previous steps. Additionally, we introduce a new
back-verification technique that uses LLMs to validate the correctness of the
generated answers. Furthermore, we employ a pruning tree search to optimize
search time while achieving strong performance. Notably, our method improves
Qwen2-7b-Instruct's score from 36.94 to 61.52, outperforming GPT4's 42.5 on the
MATH benchmark.",2024-09-26,"Linzhuang Sun, Hao Liang, Jingxuan Wei, Bihui Yu, Conghui He, Zenan Zhou, Wentao Zhang",http://arxiv.org/pdf/2409.17972v2,cs.CL
The Hard Positive Truth about Vision-Language Compositionality,"Several benchmarks have concluded that our best vision-language models (e.g.,
CLIP) are lacking in compositionality. Given an image, these benchmarks probe a
model's ability to identify its associated caption amongst a set of
compositional distractors. In response, a surge of recent proposals show
improvements by finetuning CLIP with distractors as hard negatives. Our
investigations reveal that these improvements have, in fact, been significantly
overstated -- because existing benchmarks do not probe whether finetuned
vision-language models remain invariant to hard positives. By curating an
evaluation dataset with 112,382 hard negatives and hard positives, we uncover
that including hard positives decreases CLIP's performance by 12.9%, while
humans perform effortlessly at 99%. CLIP finetuned with hard negatives results
in an even larger decrease, up to 38.7%. With this finding, we then produce a
1,775,259 image-text training set with both hard negative and hard positive
captions. By training with both, we see improvements on existing benchmarks
while simultaneously improving performance on hard positives, indicating a more
robust improvement in compositionality. Our work suggests the need for future
research to rigorously test and improve CLIP's understanding of semantic
relationships between related ""positive"" concepts.",2024-09-26,"Amita Kamath, Cheng-Yu Hsieh, Kai-Wei Chang, Ranjay Krishna",http://arxiv.org/pdf/2409.17958v1,cs.CL
Weak-to-Strong Backdoor Attack for Large Language Models,"Despite being widely applied due to their exceptional capabilities, Large
Language Models (LLMs) have been proven to be vulnerable to backdoor attacks.
These attacks introduce targeted vulnerabilities into LLMs by poisoning
training samples and full-parameter fine-tuning. However, this kind of backdoor
attack is limited since they require significant computational resources,
especially as the size of LLMs increases. Besides, parameter-efficient
fine-tuning (PEFT) offers an alternative but the restricted parameter updating
may impede the alignment of triggers with target labels. In this study, we
first verify that backdoor attacks with PEFT may encounter challenges in
achieving feasible performance. To address these issues and improve the
effectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack
algorithm from weak to strong based on feature alignment-enhanced knowledge
distillation (W2SAttack). Specifically, we poison small-scale language models
through full-parameter fine-tuning to serve as the teacher model. The teacher
model then covertly transfers the backdoor to the large-scale student model
through feature alignment-enhanced knowledge distillation, which employs PEFT.
Theoretical analysis reveals that W2SAttack has the potential to augment the
effectiveness of backdoor attacks. We demonstrate the superior performance of
W2SAttack on classification tasks across four language models, four backdoor
attack algorithms, and two different architectures of teacher models.
Experimental results indicate success rates close to 100% for backdoor attacks
targeting PEFT.",2024-09-26,"Shuai Zhao, Leilei Gan, Zhongliang Guo, Xiaobao Wu, Luwei Xiao, Xiaoyu Xu, Cong-Duy Nguyen, Luu Anh Tuan",http://arxiv.org/pdf/2409.17946v3,cs.CL
On Translating Technical Terminology: A Translation Workflow for Machine-Translated Acronyms,"The typical workflow for a professional translator to translate a document
from its source language (SL) to a target language (TL) is not always focused
on what many language models in natural language processing (NLP) do - predict
the next word in a series of words. While high-resource languages like English
and French are reported to achieve near human parity using common metrics for
measurement such as BLEU and COMET, we find that an important step is being
missed: the translation of technical terms, specifically acronyms. Some
state-of-the art machine translation systems like Google Translate which are
publicly available can be erroneous when dealing with acronyms - as much as 50%
in our findings. This article addresses acronym disambiguation for MT systems
by proposing an additional step to the SL-TL (FR-EN) translation workflow where
we first offer a new acronym corpus for public consumption and then experiment
with a search-based thresholding algorithm that achieves nearly 10% increase
when compared to Google Translate and OpusMT.",2024-09-26,"Richard Yue, John E. Ortega, Kenneth Ward Church",http://arxiv.org/pdf/2409.17943v1,cs.CL
Predicting Anchored Text from Translation Memories for Machine Translation Using Deep Learning Methods,"Translation memories (TMs) are the backbone for professional translation
tools called computer-aided translation (CAT) tools. In order to perform a
translation using a CAT tool, a translator uses the TM to gather translations
similar to the desired segment to translate (s'). Many CAT tools offer a
fuzzy-match algorithm to locate segments (s) in the TM that are close in
distance to s'. After locating two similar segments, the CAT tool will present
parallel segments (s, t) that contain one segment in the source language along
with its translation in the target language. Additionally, CAT tools contain
fuzzy-match repair (FMR) techniques that will automatically use the parallel
segments from the TM to create new TM entries containing a modified version of
the original with the idea in mind that it will be the translation of s'. Most
FMR techniques use machine translation as a way of ""repairing"" those words that
have to be modified. In this article, we show that for a large part of those
words which are anchored, we can use other techniques that are based on machine
learning approaches such as Word2Vec. BERT, and even ChatGPT. Specifically, we
show that for anchored words that follow the continuous bag-of-words (CBOW)
paradigm, Word2Vec, BERT, and GPT-4 can be used to achieve similar and, for
some cases, better results than neural machine translation for translating
anchored words from French to English.",2024-09-26,"Richard Yue, John E. Ortega",http://arxiv.org/pdf/2409.17939v1,cs.CL
The Lou Dataset -- Exploring the Impact of Gender-Fair Language in German Text Classification,"Gender-fair language, an evolving German linguistic variation, fosters
inclusion by addressing all genders or using neutral forms. Nevertheless, there
is a significant lack of resources to assess the impact of this linguistic
shift on classification using language models (LMs), which are probably not
trained on such variations. To address this gap, we present Lou, the first
dataset featuring high-quality reformulations for German text classification
covering seven tasks, like stance detection and toxicity classification.
Evaluating 16 mono- and multi-lingual LMs on Lou shows that gender-fair
language substantially impacts predictions by flipping labels, reducing
certainty, and altering attention patterns. However, existing evaluations
remain valid, as LM rankings of original and reformulated instances do not
significantly differ. While we offer initial insights on the effect on German
text classification, the findings likely apply to other languages, as
consistent patterns were observed in multi-lingual and English LMs.",2024-09-26,"Andreas Waldis, Joel Birrer, Anne Lauscher, Iryna Gurevych",http://arxiv.org/pdf/2409.17929v1,cs.CL
Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion,"During pre-training, the Text-to-Image (T2I) diffusion models encode factual
knowledge into their parameters. These parameterized facts enable realistic
image generation, but they may become obsolete over time, thereby
misrepresenting the current state of the world. Knowledge editing techniques
aim to update model knowledge in a targeted way. However, facing the dual
challenges posed by inadequate editing datasets and unreliable evaluation
criterion, the development of T2I knowledge editing encounter difficulties in
effectively generalizing injected knowledge. In this work, we design a T2I
knowledge editing framework by comprehensively spanning on three phases: First,
we curate a dataset \textbf{CAKE}, comprising paraphrase and multi-object test,
to enable more fine-grained assessment on knowledge generalization. Second, we
propose a novel criterion, \textbf{adaptive CLIP threshold}, to effectively
filter out false successful images under the current criterion and achieve
reliable editing evaluation. Finally, we introduce \textbf{MPE}, a simple but
effective approach for T2I knowledge editing. Instead of tuning parameters, MPE
precisely recognizes and edits the outdated part of the conditioning
text-prompt to accommodate the up-to-date knowledge. A straightforward
implementation of MPE (Based on in-context learning) exhibits better overall
performance than previous model editors. We hope these efforts can further
promote faithful evaluation of T2I knowledge editing methods.",2024-09-26,"Hengrui Gu, Kaixiong Zhou, Yili Wang, Ruobing Wang, Xin Wang",http://arxiv.org/pdf/2409.17928v2,cs.CL
Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect,"We introduce Atlas-Chat, the first-ever collection of LLMs specifically
developed for dialectal Arabic. Focusing on Moroccan Arabic, also known as
Darija, we construct our instruction dataset by consolidating existing Darija
language resources, creating novel datasets both manually and synthetically,
and translating English instructions with stringent quality control.
Atlas-Chat-2B, 9B, and 27B models, fine-tuned on the dataset, exhibit superior
ability in following Darija instructions and performing standard NLP tasks.
Notably, our models outperform both state-of-the-art and Arabic-specialized
LLMs like LLaMa, Jais, and AceGPT, e.g., our 9B model gains a 13% performance
boost over a larger 13B model on DarijaMMLU, in our newly introduced evaluation
suite for Darija covering both discriminative and generative tasks.
Furthermore, we perform an experimental analysis of various fine-tuning
strategies and base model choices to determine optimal configurations. All our
resources are publicly accessible, and we believe our work offers comprehensive
design methodologies of instruction-tuning for low-resource languages, which
are often neglected in favor of data-rich languages by contemporary LLMs.",2024-09-26,"Guokan Shang, Hadi Abdine, Yousef Khoubrane, Amr Mohamed, Yassine Abbahaddou, Sofiane Ennadir, Imane Momayiz, Xuguang Ren, Eric Moulines, Preslav Nakov, Michalis Vazirgiannis, Eric Xing",http://arxiv.org/pdf/2409.17912v2,cs.CL
Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations,"Emotion recognition from speech and music shares similarities due to their
acoustic overlap, which has led to interest in transferring knowledge between
these domains. However, the shared acoustic cues between speech and music,
particularly those encoded by Self-Supervised Learning (SSL) models, remain
largely unexplored, given the fact that SSL models for speech and music have
rarely been applied in cross-domain research. In this work, we revisit the
acoustic similarity between emotion speech and music, starting with an analysis
of the layerwise behavior of SSL models for Speech Emotion Recognition (SER)
and Music Emotion Recognition (MER). Furthermore, we perform cross-domain
adaptation by comparing several approaches in a two-stage fine-tuning process,
examining effective ways to utilize music for SER and speech for MER. Lastly,
we explore the acoustic similarities between emotional speech and music using
Frechet audio distance for individual emotions, uncovering the issue of emotion
bias in both speech and music SSL models. Our findings reveal that while speech
and music SSL models do capture shared acoustic features, their behaviors can
vary depending on different emotions due to their training strategies and
domain-specificities. Additionally, parameter-efficient fine-tuning can enhance
SER and MER performance by leveraging knowledge from each other. This study
provides new insights into the acoustic similarity between emotional speech and
music, and highlights the potential for cross-domain generalization to improve
SER and MER systems.",2024-09-26,"Yujia Sun, Zeyu Zhao, Korin Richmond, Yuanchao Li",http://arxiv.org/pdf/2409.17899v2,cs.CL
EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models,"In this work, we introduce EMMA-500, a large-scale multilingual language
model continue-trained on texts across 546 languages designed for enhanced
multilingual performance, focusing on improving language coverage for
low-resource languages. To facilitate continual pre-training, we compile the
MaLA corpus, a comprehensive multilingual dataset enriched with curated
datasets across diverse domains. Leveraging this corpus, we conduct extensive
continual pre-training of the Llama 2 7B model, resulting in EMMA-500, which
demonstrates robust performance across a wide collection of benchmarks,
including a comprehensive set of multilingual tasks. Our results highlight the
effectiveness of continual pre-training in expanding large language models'
language capacity, particularly for underrepresented languages, demonstrating
significant gains in cross-lingual transfer, task generalization, and language
adaptability. We release the MaLA corpus, EMMA-500 model weights, scripts, and
model generations.",2024-09-26,"Shaoxiong Ji, Zihao Li, Indraneil Paul, Jaakko Paavola, Peiqin Lin, Pinzhen Chen, Dayyán O'Brien, Hengyu Luo, Hinrich Schütze, Jörg Tiedemann, Barry Haddow",http://arxiv.org/pdf/2409.17892v2,cs.CL
Implementing a Nordic-Baltic Federated Health Data Network: a case report,"Background: Centralized collection and processing of healthcare data across
national borders pose significant challenges, including privacy concerns, data
heterogeneity and legal barriers. To address some of these challenges, we
formed an interdisciplinary consortium to develop a feder-ated health data
network, comprised of six institutions across five countries, to facilitate
Nordic-Baltic cooperation on secondary use of health data. The objective of
this report is to offer early insights into our experiences developing this
network. Methods: We used a mixed-method ap-proach, combining both experimental
design and implementation science to evaluate the factors affecting the
implementation of our network. Results: Technically, our experiments indicate
that the network functions without significant performance degradation compared
to centralized simu-lation. Conclusion: While use of interdisciplinary
approaches holds a potential to solve challeng-es associated with establishing
such collaborative networks, our findings turn the spotlight on the uncertain
regulatory landscape playing catch up and the significant operational costs.",2024-09-26,"Taridzo Chomutare, Aleksandar Babic, Laura-Maria Peltonen, Silja Elunurm, Peter Lundberg, Arne Jönsson, Emma Eneling, Ciprian-Virgil Gerstenberger, Troels Siggaard, Raivo Kolde, Oskar Jerdhaf, Martin Hansson, Alexandra Makhlysheva, Miroslav Muzny, Erik Ylipää, Søren Brunak, Hercules Dalianis",http://arxiv.org/pdf/2409.17865v1,cs.CL
PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent Representation MOdification,"Due to their substantial sizes, large language models (LLMs) are typically
deployed within a single-backbone multi-tenant framework. In this setup, a
single instance of an LLM backbone must cater to multiple users or tasks
through the application of various parameter-efficient fine-tuning (PEFT)
models. Despite the availability of numerous effective PEFT techniques such as
LoRA, there remains a need for a PEFT approach that achieves both high
efficiency during inference and competitive performance on downstream tasks. In
this research, we introduce a new and straightforward PEFT methodology named
\underline{P}rompt D\underline{E}pen\underline{D}ent \underline{R}epresentation
M\underline{O}dification (PEDRO). The proposed method involves integrating a
lightweight vector generator into each Transformer layer, which generates
vectors contingent upon the input prompts. These vectors then modify the hidden
representations created by the LLM through a dot product operation, thereby
influencing the semantic output and generated content of the model. Extensive
experimentation across a variety of tasks indicates that: (a) PEDRO surpasses
recent PEFT benchmarks when using a similar number of tunable parameters. (b)
Under the single-backbone multi-tenant deployment model, PEDRO exhibits
superior efficiency compared to LoRA, indicating significant industrial
potential.",2024-09-26,"Tianfang Xie, Tianjing Li, Wei Zhu, Wei Han, Yi Zhao",http://arxiv.org/pdf/2409.17834v1,cs.CL
Code Generation and Algorithmic Problem Solving Using Llama 3.1 405B,"Code generation by Llama 3.1 models, such as Meta's Llama 3.1 405B,
represents a significant advancement in the field of artificial intelligence,
particularly in natural language processing and programming automation. This
paper explores the capabilities and applications of Llama-driven code
generation, highlighting its ability to translate natural language prompts into
executable code across multiple programming languages. Key features include
contextual awareness, multi-language support, and enhanced debugging and
optimization functionalities. By examining these aspects, we illustrate how
Llama can serve as a versatile tool for developers of all skill levels,
improving productivity and efficiency in software development. The potential
implications for education, industry, and the future of coding practices are
also discussed, underscoring the transformative impact of AI in programming.
Experimentation shows that while Llama 3.1 405B performs well with simple
algorithmic and data structure based problems, it still struggles with problems
on Quantum Computing, Bioinformatics, and Artificial Intelligence.",2024-09-26,"Aniket Deroy, Subhankar Maity",http://arxiv.org/pdf/2409.19027v2,cs.CL
"BeanCounter: A low-toxicity, large-scale, and open dataset of business-oriented text","Many of the recent breakthroughs in language modeling have resulted from
scaling effectively the same model architecture to larger datasets. In this
vein, recent work has highlighted performance gains from increasing training
dataset size and quality, suggesting a need for novel sources of large-scale
datasets. In this work, we introduce BeanCounter, a public dataset consisting
of more than 159B tokens extracted from businesses' disclosures. We show that
this data is indeed novel: less than 0.1% of BeanCounter appears in Common
Crawl-based datasets and it is an order of magnitude larger than datasets
relying on similar sources. Given the data's provenance, we hypothesize that
BeanCounter is comparatively more factual and less toxic than web-based
datasets. Exploring this hypothesis, we find that many demographic identities
occur with similar prevalence in BeanCounter but with significantly less toxic
context relative to other datasets. To demonstrate the utility of BeanCounter,
we evaluate and compare two LLMs continually pre-trained on BeanCounter with
their base models. We find an 18-33% reduction in toxic generation and improved
performance within the finance domain for the continually pretrained models.
Collectively, our work suggests that BeanCounter is a novel source of
low-toxicity and high-quality domain-specific data with sufficient scale to
train multi-billion parameter LLMs.",2024-09-26,"Siyan Wang, Bradford Levy",http://arxiv.org/pdf/2409.17827v2,cs.CL
Inference-Time Language Model Alignment via Integrated Value Guidance,"Large language models are typically fine-tuned to align with human
preferences, but tuning large models is computationally intensive and complex.
In this work, we introduce $\textit{Integrated Value Guidance}$ (IVG), a method
that uses implicit and explicit value functions to guide language model
decoding at token and chunk-level respectively, efficiently aligning large
language models purely at inference time. This approach circumvents the
complexities of direct fine-tuning and outperforms traditional methods.
Empirically, we demonstrate the versatility of IVG across various tasks. In
controlled sentiment generation and summarization tasks, our method
significantly improves the alignment of large models using inference-time
guidance from $\texttt{gpt2}$-based value functions. Moreover, in a more
challenging instruction-following benchmark AlpacaEval 2.0, we show that both
specifically tuned and off-the-shelf value functions greatly improve the
length-controlled win rates of large models against $\texttt{gpt-4-turbo}$
(e.g., $19.51\% \rightarrow 26.51\%$ for $\texttt{Mistral-7B-Instruct-v0.2}$
and $25.58\% \rightarrow 33.75\%$ for $\texttt{Mixtral-8x7B-Instruct-v0.1}$
with Tulu guidance).",2024-09-26,"Zhixuan Liu, Zhanhui Zhou, Yuanfu Wang, Chao Yang, Yu Qiao",http://arxiv.org/pdf/2409.17819v1,cs.CL
Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness,"Recently, there has been significant interest in replacing the reward model
in Reinforcement Learning with Human Feedback (RLHF) methods for Large Language
Models (LLMs), such as Direct Preference Optimization (DPO) and its variants.
These approaches commonly use a binary cross-entropy mechanism on pairwise
samples, i.e., minimizing and maximizing the loss based on preferred or
dis-preferred responses, respectively. However, while this training strategy
omits the reward model, it also overlooks the varying preference degrees within
different responses. We hypothesize that this is a key factor hindering LLMs
from sufficiently understanding human preferences. To address this problem, we
propose a novel Self-supervised Preference Optimization (SPO) framework, which
constructs a self-supervised preference degree loss combined with the alignment
loss, thereby helping LLMs improve their ability to understand the degree of
preference. Extensive experiments are conducted on two widely used datasets of
different tasks. The results demonstrate that SPO can be seamlessly integrated
with existing preference optimization methods and significantly boost their
performance to achieve state-of-the-art performance. We also conduct detailed
analyses to offer comprehensive insights into SPO, which verifies its
effectiveness. The code is available at https://github.com/lijian16/SPO.",2024-09-26,"Jian Li, Haojing Huang, Yujia Zhang, Pengfei Xu, Xi Chen, Rui Song, Lida Shi, Jingwen Wang, Hao Xu",http://arxiv.org/pdf/2409.17791v1,cs.CL
Faithfulness and the Notion of Adversarial Sensitivity in NLP Explanations,"Faithfulness is arguably the most critical metric to assess the reliability
of explainable AI. In NLP, current methods for faithfulness evaluation are
fraught with discrepancies and biases, often failing to capture the true
reasoning of models. We introduce Adversarial Sensitivity as a novel approach
to faithfulness evaluation, focusing on the explainer's response when the model
is under adversarial attack. Our method accounts for the faithfulness of
explainers by capturing sensitivity to adversarial input changes. This work
addresses significant limitations in existing evaluation techniques, and
furthermore, quantifies faithfulness from a crucial yet underexplored paradigm.",2024-09-26,"Supriya Manna, Niladri Sett",http://arxiv.org/pdf/2409.17774v2,cs.CL
Integrating Hierarchical Semantic into Iterative Generation Model for Entailment Tree Explanation,"Manifestly and logically displaying the line of reasoning from evidence to
answer is significant to explainable question answering (QA). The entailment
tree exhibits the lines structurally, which is different from the
self-explanation principle in large-scale language models. Existing methods
rarely consider the semantic association of sentences between and within
hierarchies within the tree structure, which is prone to apparent mistakes in
combinations. In this work, we propose an architecture of integrating the
Hierarchical Semantics of sentences under the framework of Controller-Generator
(HiSCG) to explain answers. The HiSCG designs a hierarchical mapping between
hypotheses and facts, discriminates the facts involved in tree constructions,
and optimizes single-step entailments. To the best of our knowledge, We are the
first to notice hierarchical semantics of sentences between the same layer and
adjacent layers to yield improvements. The proposed method achieves comparable
performance on all three settings of the EntailmentBank dataset. The
generalization results on two out-of-domain datasets also demonstrate the
effectiveness of our method.",2024-09-26,"Qin Wang, Jianzhou Feng, Yiming Xu",http://arxiv.org/pdf/2409.17757v1,cs.CL
SECURE: Semantics-aware Embodied Conversation under Unawareness for Lifelong Robot Learning,"This paper addresses a challenging interactive task learning scenario we call
rearrangement under unawareness: to manipulate a rigid-body environment in a
context where the agent is unaware of a concept that is key to solving the
instructed task. We propose SECURE, an interactive task learning framework
designed to solve such problems. It uses embodied conversation to fix its
deficient domain model -- through dialogue, the agent discovers and then learns
to exploit unforeseen possibilities. In particular, SECURE learns from the
user's embodied corrective feedback when it makes a mistake, and it makes
strategic dialogue decisions to reveal useful evidence about novel concepts for
solving the instructed task. Together, these abilities allow the agent to
generalise to subsequent tasks using newly acquired knowledge. We demonstrate
that learning to solve rearrangement under unawareness is more data efficient
when the agent is semantics-aware -- that is, during both learning and
inference it augments the evidence from the user's embodied conversation with
its logical consequences, stemming from semantic analysis.",2024-09-26,"Rimvydas Rubavicius, Peter David Fagan, Alex Lascarides, Subramanian Ramamoorthy",http://arxiv.org/pdf/2409.17755v2,cs.CL
Are Transformers in Pre-trained LM A Good ASR Encoder? An Empirical Study,"In this study, we delve into the efficacy of transformers within pre-trained
language models (PLMs) when repurposed as encoders for Automatic Speech
Recognition (ASR). Our underlying hypothesis posits that, despite being
initially trained on text-based corpora, these transformers possess a
remarkable capacity to extract effective features from the input sequence. This
inherent capability, we argue, is transferrable to speech data, thereby
augmenting the acoustic modeling ability of ASR. Through rigorous empirical
analysis, our findings reveal a notable improvement in Character Error Rate
(CER) and Word Error Rate (WER) across diverse ASR tasks when transformers from
pre-trained LMs are incorporated. Particularly, they serve as an advantageous
starting point for initializing ASR encoders. Furthermore, we uncover that
these transformers, when integrated into a well-established ASR encoder, can
significantly boost performance, especially in scenarios where profound
semantic comprehension is pivotal. This underscores the potential of leveraging
the semantic prowess embedded within pre-trained transformers to advance ASR
systems' capabilities.",2024-09-26,"Keyu An, Shiliang Zhang, Zhijie Yan",http://arxiv.org/pdf/2409.17750v1,cs.CL
Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model,"A supervised ranking model, despite its advantage of being effective, usually
involves complex processing - typically multiple stages of task-specific
pre-training and fine-tuning. This has motivated researchers to explore simpler
pipelines leveraging large language models (LLMs) that are capable of working
in a zero-shot manner. However, since zero-shot inference does not make use of
a training set of pairs of queries and their relevant documents, its
performance is mostly worse than that of supervised models, which are trained
on such example pairs. Motivated by the existing findings that training
examples generally improve zero-shot performance, in our work, we explore if
this also applies to ranking models. More specifically, given a query and a
pair of documents, the preference prediction task is improved by augmenting
examples of preferences for similar queries from a training set. Our proposed
pairwise few-shot ranker demonstrates consistent improvements over the
zero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)
retrieval benchmarks. Our method also achieves a close performance to that of a
supervised model without requiring any complex training pipeline.",2024-09-26,"Nilanjan Sinhababu, Andrew Parry, Debasis Ganguly, Debasis Samanta, Pabitra Mitra",http://arxiv.org/pdf/2409.17745v3,cs.CL
MIO: A Foundation Model on Multimodal Tokens,"In this paper, we introduce MIO, a novel foundation model built on multimodal
tokens, capable of understanding and generating speech, text, images, and
videos in an end-to-end, autoregressive manner. While the emergence of large
language models (LLMs) and multimodal large language models (MM-LLMs) propels
advancements in artificial general intelligence through their versatile
capabilities, they still lack true any-to-any understanding and generation.
Recently, the release of GPT-4o has showcased the remarkable potential of
any-to-any LLMs for complex real-world tasks, enabling omnidirectional input
and output across images, speech, and text. However, it is closed-source and
does not support the generation of multimodal interleaved sequences. To address
this gap, we present MIO, which is trained on a mixture of discrete tokens
across four modalities using causal multimodal modeling. MIO undergoes a
four-stage training process: (1) alignment pre-training, (2) interleaved
pre-training, (3) speech-enhanced pre-training, and (4) comprehensive
supervised fine-tuning on diverse textual, visual, and speech tasks. Our
experimental results indicate that MIO exhibits competitive, and in some cases
superior, performance compared to previous dual-modal baselines, any-to-any
model baselines, and even modality-specific baselines. Moreover, MIO
demonstrates advanced capabilities inherent to its any-to-any feature, such as
interleaved video-text generation, chain-of-visual-thought reasoning, visual
guideline generation, instructional image editing, etc.",2024-09-26,"Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang",http://arxiv.org/pdf/2409.17692v3,cs.CL
Zero- and Few-shot Named Entity Recognition and Text Expansion in Medication Prescriptions using ChatGPT,"Introduction: Medication prescriptions are often in free text and include a
mix of two languages, local brand names, and a wide range of idiosyncratic
formats and abbreviations. Large language models (LLMs) have shown promising
ability to generate text in response to input prompts. We use ChatGPT 3.5 to
automatically structure and expand medication statements in discharge summaries
and thus make them easier to interpret for people and machines. Methods:
Named-entity Recognition (NER) and Text Expansion (EX) are used in a zero- and
few-shot setting with different prompt strategies. 100 medication statements
were manually annotated and curated. NER performance was measured by using
strict and partial matching. For the task EX, two experts interpreted the
results by assessing semantic equivalence between original and expanded
statements. The model performance was measured by precision, recall, and F1
score. Results: For NER, the best-performing prompt reached an average F1 score
of 0.94 in the test set. For EX, the few-shot prompt showed superior
performance among other prompts, with an average F1 score of 0.87. Conclusion:
Our study demonstrates good performance for NER and EX tasks in free-text
medication statements using ChatGPT. Compared to a zero-shot baseline, a
few-shot approach prevented the system from hallucinating, which would be
unacceptable when processing safety-relevant medication data.",2024-09-26,"Natthanaphop Isaradech, Andrea Riedel, Wachiranun Sirikul, Markus Kreuzthaler, Stefan Schulz",http://arxiv.org/pdf/2409.17683v1,cs.CL
Cross-lingual Human-Preference Alignment for Neural Machine Translation with Direct Quality Optimization,"Reinforcement Learning from Human Feedback (RLHF) and derivative techniques
like Direct Preference Optimization (DPO) are task-alignment algorithms used to
repurpose general, foundational models for specific tasks. We show that
applying task-alignment to neural machine translation (NMT) addresses an
existing task--data mismatch in NMT, leading to improvements across all
languages of a multilingual model, even when task-alignment is only applied to
a subset of those languages. We do so by introducing Direct Quality
Optimization (DQO), a variant of DPO leveraging a pre-trained translation
quality estimation model as a proxy for human preferences, and verify the
improvements with both automatic metrics and human evaluation.",2024-09-26,"Kaden Uhlig, Joern Wuebker, Raphael Reinauer, John DeNero",http://arxiv.org/pdf/2409.17673v2,cs.CL
Digital Twin Ecosystem for Oncology Clinical Operations,"Artificial Intelligence (AI) and Large Language Models (LLMs) hold
significant promise in revolutionizing healthcare, especially in clinical
applications. Simultaneously, Digital Twin technology, which models and
simulates complex systems, has gained traction in enhancing patient care.
However, despite the advances in experimental clinical settings, the potential
of AI and digital twins to streamline clinical operations remains largely
untapped. This paper introduces a novel digital twin framework specifically
designed to enhance oncology clinical operations. We propose the integration of
multiple specialized digital twins, such as the Medical Necessity Twin, Care
Navigator Twin, and Clinical History Twin, to enhance workflow efficiency and
personalize care for each patient based on their unique data. Furthermore, by
synthesizing multiple data sources and aligning them with the National
Comprehensive Cancer Network (NCCN) guidelines, we create a dynamic Cancer Care
Path, a continuously evolving knowledge base that enables these digital twins
to provide precise, tailored clinical recommendations.",2024-09-26,"Himanshu Pandey, Akhil Amod, Shivang, Kshitij Jaggi, Ruchi Garg, Abheet Jain, Vinayak Tantia",http://arxiv.org/pdf/2409.17650v1,cs.CL
Efficient In-Domain Question Answering for Resource-Constrained Environments,"Retrieval Augmented Generation (RAG) is a common method for integrating
external knowledge into pretrained Large Language Models (LLMs) to enhance
accuracy and relevancy in question answering (QA) tasks. However, prompt
engineering and resource efficiency remain significant bottlenecks in
developing optimal and robust RAG solutions for real-world QA applications.
Recent studies have shown success in using fine tuning to address these
problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to
smaller 7B models has demonstrated superior performance compared to RAG setups
with much larger models such as GPT-3.5. The combination of RAFT with
parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation
(LoRA), promises an even more efficient solution, yet remains an unexplored
area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage
requirements and gain faster inference times while maintaining comparable RAG
performance. This results in a more compute-efficient RAFT, or CRAFT, which is
particularly useful for knowledge-intensive QA tasks in resource-constrained
environments where internet access may be restricted and hardware resources
limited.",2024-09-26,"Isaac Chung, Phat Vo, Arman C. Kizilkale, Aaron Reite",http://arxiv.org/pdf/2409.17648v3,cs.CL
T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training on an Assistant Task for a Target Task,"Long text summarization, gradually being essential for efficiently processing
large volumes of information, stays challenging for Large Language Models
(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced
training datasets and the high requirement of contextual details dealing. To
address the issue, we design a novel zero-shot transfer learning framework,
abbreviated as T3, to iteratively training a baseline LLM on an assistant task
for the target task, where the former should own richer data resources and
share structural or semantic similarity with the latter. In practice, T3 is
approached to deal with the long text summarization task by utilizing question
answering as the assistant task, and further validated its effectiveness on the
BBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%
improvement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore
compared to three baseline LLMs, demonstrating its potential for more
assistant-target task combinations.",2024-09-26,"Xindi Tong, Yujin Zhu, Shijian Fan, Liang Xu",http://arxiv.org/pdf/2409.17640v3,cs.CL
ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context Information in Multi-Turn Multimodal Medical Dialogue,"The rocketing prosperity of large language models (LLMs) in recent years has
boosted the prevalence of vision-language models (VLMs) in the medical sector.
In our online medical consultation scenario, a doctor responds to the texts and
images provided by a patient in multiple rounds to diagnose her/his health
condition, forming a multi-turn multimodal medical dialogue format. Unlike
high-quality images captured by professional equipment in traditional medical
visual question answering (Med-VQA), the images in our case are taken by
patients' mobile phones. These images have poor quality control, with issues
such as excessive background elements and the lesion area being significantly
off-center, leading to degradation of vision-language alignment in the model
training phase. In this paper, we propose ZALM3, a Zero-shot strategy to
improve vision-language ALignment in Multi-turn Multimodal Medical dialogue.
Since we observe that the preceding text conversations before an image can
infer the regions of interest (RoIs) in the image, ZALM3 employs an LLM to
summarize the keywords from the preceding context and a visual grounding model
to extract the RoIs. The updated images eliminate unnecessary background noise
and provide more effective vision-language alignment. To better evaluate our
proposed method, we design a new subjective assessment metric for multi-turn
unimodal/multimodal medical dialogue to provide a fine-grained performance
comparison. Our experiments across three different clinical departments
remarkably demonstrate the efficacy of ZALM3 with statistical significance.",2024-09-26,"Zhangpu Li, Changhong Zou, Suxue Ma, Zhicheng Yang, Chen Du, Youbao Tang, Zhenjie Cao, Ning Zhang, Jui-Hsin Lai, Ruei-Sung Lin, Yuan Ni, Xingzhi Sun, Jing Xiao, Jieke Hou, Kai Zhang, Mei Han",http://arxiv.org/pdf/2409.17610v2,cs.CL
"Deep CLAS: Deep Contextual Listen, Attend and Spell","Contextual-LAS (CLAS) has been shown effective in improving Automatic Speech
Recognition (ASR) of rare words. It relies on phrase-level contextual modeling
and attention-based relevance scoring without explicit contextual constraint
which lead to insufficient use of contextual information. In this work, we
propose deep CLAS to use contextual information better. We introduce bias loss
forcing model to focus on contextual information. The query of bias attention
is also enriched to improve the accuracy of the bias attention score. To get
fine-grained contextual information, we replace phrase-level encoding with
character-level encoding and encode contextual information with conformer
rather than LSTM. Moreover, we directly use the bias attention score to correct
the output probability distribution of the model. Experiments using the public
AISHELL-1 and AISHELL-NER. On AISHELL-1, compared to CLAS baselines, deep CLAS
obtains a 65.78% relative recall and a 53.49% relative F1-score increase in the
named entity recognition scene.",2024-09-26,"Mengzhi Wang, Shifu Xiong, Genshun Wan, Hang Chen, Jianqing Gao, Lirong Dai",http://arxiv.org/pdf/2409.17603v2,cs.CL
DualCoTs: Dual Chain-of-Thoughts Prompting for Sentiment Lexicon Expansion of Idioms,"Idioms represent a ubiquitous vehicle for conveying sentiments in the realm
of everyday discourse, rendering the nuanced analysis of idiom sentiment
crucial for a comprehensive understanding of emotional expression within
real-world texts. Nevertheless, the existing corpora dedicated to idiom
sentiment analysis considerably limit research in text sentiment analysis. In
this paper, we propose an innovative approach to automatically expand the
sentiment lexicon for idioms, leveraging the capabilities of large language
models through the application of Chain-of-Thought prompting. To demonstrate
the effectiveness of this approach, we integrate multiple existing resources
and construct an emotional idiom lexicon expansion dataset (called EmoIdiomE),
which encompasses a comprehensive repository of Chinese and English idioms.
Then we designed the Dual Chain-of-Thoughts (DualCoTs) method, which combines
insights from linguistics and psycholinguistics, to demonstrate the
effectiveness of using large models to automatically expand the sentiment
lexicon for idioms. Experiments show that DualCoTs is effective in idioms
sentiment lexicon expansion in both Chinese and English. For reproducibility,
we will release the data and code upon acceptance.",2024-09-26,"Fuqiang Niu, Minghuan Tan, Bowen Zhang, Min Yang, Ruifeng Xu",http://arxiv.org/pdf/2409.17588v1,cs.CL
Dealing with Controversy: An Emotion and Coping Strategy Corpus Based on Role Playing,"There is a mismatch between psychological and computational studies on
emotions. Psychological research aims at explaining and documenting internal
mechanisms of these phenomena, while computational work often simplifies them
into labels. Many emotion fundamentals remain under-explored in natural
language processing, particularly how emotions develop and how people cope with
them. To help reduce this gap, we follow theories on coping, and treat emotions
as strategies to cope with salient situations (i.e., how people deal with
emotion-eliciting events). This approach allows us to investigate the link
between emotions and behavior, which also emerges in language. We introduce the
task of coping identification, together with a corpus to do so, constructed via
role-playing. We find that coping strategies realize in text even though they
are challenging to recognize, both for humans and automatic systems trained and
prompted on the same task. We thus open up a promising research direction to
enhance the capability of models to better capture emotion mechanisms from
text.",2024-09-26,"Enrica Troiano, Sofie Labat, Marco Antonio Stranisci, Viviana Patti, Rossana Damiano, Roman Klinger",http://arxiv.org/pdf/2409.19025v1,cs.CL
Leveraging Annotator Disagreement for Text Classification,"It is common practice in text classification to only use one majority label
for model training even if a dataset has been annotated by multiple annotators.
Doing so can remove valuable nuances and diverse perspectives inherent in the
annotators' assessments. This paper proposes and compares three different
strategies to leverage annotator disagreement for text classification: a
probability-based multi-label method, an ensemble system, and instruction
tuning. All three approaches are evaluated on the tasks of hate speech and
abusive conversation detection, which inherently entail a high degree of
subjectivity. Moreover, to evaluate the effectiveness of embracing annotation
disagreements for model training, we conduct an online survey that compares the
performance of the multi-label model against a baseline model, which is trained
with the majority label.
  The results show that in hate speech detection, the multi-label method
outperforms the other two approaches, while in abusive conversation detection,
instruction tuning achieves the best performance. The results of the survey
also show that the outputs from the multi-label models are considered a better
representation of the texts than the single-label model.",2024-09-26,"Jin Xu, Mariët Theune, Daniel Braun",http://arxiv.org/pdf/2409.17577v1,cs.CL
Heuristics and Biases in AI Decision-Making: Implications for Responsible AGI,"We investigate the presence of cognitive biases in three large language
models (LLMs): GPT-4o, Gemma 2, and Llama 3.1. The study uses 1,500 experiments
across nine established cognitive biases to evaluate the models' responses and
consistency. GPT-4o demonstrated the strongest overall performance. Gemma 2
showed strengths in addressing the sunk cost fallacy and prospect theory,
however its performance varied across different biases. Llama 3.1 consistently
underperformed, relying on heuristics and exhibiting frequent inconsistencies
and contradictions. The findings highlight the challenges of achieving robust
and generalizable reasoning in LLMs, and underscore the need for further
development to mitigate biases in artificial general intelligence (AGI). The
study emphasizes the importance of integrating statistical reasoning and
ethical considerations in future AI development.",2024-09-26,"Payam Saeedi, Mahsa Goodarzi, M Abdullah Canbaz",http://arxiv.org/pdf/2410.02820v3,cs.CL
"Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult","Preference optimization methods typically begin training with a well-trained
SFT model as a reference model. In RLHF and DPO, a regularization term is used
during the preference optimization process to prevent the policy model from
deviating too far from the reference model's distribution, thereby avoiding the
generation of anomalous responses. When the reference model is already
well-aligned with the given data or only requires slight adjustments, this
approach can produce a well-aligned model. However, if the reference model is
not aligned with the given data and requires significant deviation from its
current state, a regularization term may actually hinder the model alignment.
In this study, we propose \textbf{Modulated Intervention Preference
Optimization (MIPO)} to address this issue. MIPO modulates the degree of
intervention from the reference model based on how well the given data is
aligned with it. If the data is well-aligned, the intervention is increased to
prevent the policy model from diverging significantly from reference model.
Conversely, if the alignment is poor, the interference is reduced to facilitate
more extensive training. We compare the performance of MIPO and DPO using
Mistral-7B and Llama3-8B in Alpaca Eval 2.0 and MT-Bench. The experimental
results demonstrate that MIPO consistently outperforms DPO across various
evaluation scenarios.",2024-09-26,Cheolhun Jang,http://arxiv.org/pdf/2409.17545v2,cs.CL
Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models,"Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks but their performance in complex logical reasoning tasks remains
unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can
improve the reasoning ability of LLMs to some extent, they suffer from an
unfaithful issue where derived conclusions may not align with the generated
reasoning chain. To address this issue, some studies employ the approach of
propositional logic to further enhance logical reasoning abilities of LLMs.
However, the potential omissions in the extraction of logical expressions in
these methods can cause information loss in the logical reasoning process,
thereby generating incorrect results. To this end, we propose Logic-of-Thought
(LoT) prompting which employs propositional logic to generate expanded logical
information descriptions and utilizes them as an additional augmentation to
original contexts, thereby ensuring information completeness and enhancing
logical reasoning ability. LoT is orthogonal to existing prompting methods and
can be seamlessly integrated with them. Extensive experiments demonstrate that
LoT boosts the performance of various prompting methods with a striking margin
across five logical reasoning tasks. In particular, LoT enhances
Chain-of-Thought's performance on the ReClor dataset by +4.35%, improves
Chain-of-Thought with Self-Consistency's performance on the RuleTaker dataset
by +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriter
dataset by +8%.",2024-09-26,"Tongxuan Liu, Wenjiang Xu, Weizhe Huang, Yuting Zeng, Jiaxing Wang, Xingyu Wang, Hailong Yang, Jing Li",http://arxiv.org/pdf/2409.17539v2,cs.CL
On the Implicit Relation Between Low-Rank Adaptation and Differential Privacy,"A significant approach in natural language processing involves large-scale
pre-training of models on general domain data followed by their adaptation to
specific tasks or domains. As models grow in size, full fine-tuning all of
their parameters becomes increasingly impractical. To address this, some
methods for low-rank task adaptation of language models have been proposed,
e.g., LoRA and FLoRA. These methods keep the pre-trained model weights fixed
and incorporate trainable low-rank decomposition matrices into some layers of
the transformer architecture, called adapters. This approach significantly
reduces the number of trainable parameters required for downstream tasks
compared to full fine-tuning all parameters. In this work, we look at low-rank
adaptation from the lens of data privacy. We show theoretically that the
low-rank adaptation used in LoRA and FLoRA leads to the injection of some
random noise into the batch gradients w.r.t the adapter parameters. We quantify
the variance of the injected noise and show that the smaller the adaptation
rank, the larger the noise variance. By establishing a Berry-Esseen type bound
on the total variation distance between distribution of the injected noise and
a Gaussian distribution with the same variance, we show that the dynamics of
low-rank adaptation is close to that of differentially private fine-tuning of
the adapters. Finally, using Johnson-Lindenstrauss lemma, we show that when
augmented with gradient scaling, low-rank adaptation is very close to
performing DPSGD algorithm with a fixed noise scale to fine-tune the adapters.
Suggested by our theoretical findings and approved by our experimental results,
we show that low-rank adaptation, besides mitigating the space and
computational complexities, implicitly provides a privacy protection w.r.t the
fine-tuning data, without inducing the high space complexity of DPSGD.",2024-09-26,"Saber Malekmohammadi, Golnoosh Farnadi",http://arxiv.org/pdf/2409.17538v5,cs.CL
MUSE: Integrating Multi-Knowledge for Knowledge Graph Completion,"Knowledge Graph Completion (KGC) aims to predict the missing [relation] part
of (head entity)--[relation]->(tail entity) triplet. Most existing KGC methods
focus on single features (e.g., relation types) or sub-graph aggregation.
However, they do not fully explore the Knowledge Graph (KG) features and
neglect the guidance of external semantic knowledge. To address these
shortcomings, we propose a knowledge-aware reasoning model (MUSE), which
designs a novel multi-knowledge representation learning mechanism for missing
relation prediction. Our model develops a tailored embedding space through
three parallel components: 1) Prior Knowledge Learning for enhancing the
triplets' semantic representation by fine-tuning BERT; 2) Context Message
Passing for enhancing the context messages of KG; 3) Relational Path
Aggregation for enhancing the path representation from the head entity to the
tail entity. The experimental results show that MUSE significantly outperforms
other baselines on four public datasets, achieving over 5.50% H@1 improvement
and 4.20% MRR improvement on the NELL995 dataset. The code and datasets will be
released via https://github.com/SUSTech-TP/ADMA2024-MUSE.git.",2024-09-26,Pengjie Liu,http://arxiv.org/pdf/2409.17536v1,cs.CL
Data Proportion Detection for Optimized Data Management for Large Language Models,"Large language models (LLMs) have demonstrated exceptional performance across
a wide range of tasks and domains, with data preparation playing a critical
role in achieving these results. Pre-training data typically combines
information from multiple domains. To maximize performance when integrating
data from various domains, determining the optimal data proportion is
essential. However, state-of-the-art (SOTA) LLMs rarely disclose details about
their pre-training data, making it difficult for researchers to identify ideal
data proportions. In this paper, we introduce a new topic, \textit{data
proportion detection}, which enables the automatic estimation of pre-training
data proportions by analyzing the generated outputs of LLMs. We provide
rigorous theoretical proofs, practical algorithms, and preliminary experimental
results for data proportion detection. Based on these findings, we offer
valuable insights into the challenges and future directions for effective data
proportion detection and data management.",2024-09-26,"Hao Liang, Keshi Zhao, Yajie Yang, Bin Cui, Guosheng Dong, Zenan Zhou, Wentao Zhang",http://arxiv.org/pdf/2409.17527v1,cs.CL
Elephant in the Room: Unveiling the Impact of Reward Model Quality in Alignment,"The demand for regulating potentially risky behaviors of large language
models (LLMs) has ignited research on alignment methods. Since LLM alignment
heavily relies on reward models for optimization or evaluation, neglecting the
quality of reward models may cause unreliable results or even misalignment.
Despite the vital role reward models play in alignment, previous works have
consistently overlooked their performance and used off-the-shelf reward models
arbitrarily without verification, rendering the reward model ``\emph{an
elephant in the room}''. To this end, this work first investigates the quality
of the widely-used preference dataset, HH-RLHF, and curates a clean version,
CHH-RLHF. Based on CHH-RLHF, we benchmark the accuracy of a broad range of
reward models used in previous alignment works, unveiling the unreliability of
using them both for optimization and evaluation. Furthermore, we systematically
study the impact of reward model quality on alignment performance in three
reward utilization paradigms. Extensive experiments reveal that better reward
models perform as better human preference proxies. This work aims to awaken
people to notice this huge elephant in alignment research. We call attention to
the following issues: (1) The reward model needs to be rigorously evaluated,
whether for alignment optimization or evaluation. (2) Considering the role of
reward models, research efforts should not only concentrate on alignment
algorithm, but also on developing more reliable human proxy.",2024-09-26,"Yan Liu, Xiaoyuan Yi, Xiaokang Chen, Jing Yao, Jingwei Yi, Daoguang Zan, Zheng Liu, Xing Xie, Tsung-Yi Ho",http://arxiv.org/pdf/2409.19024v1,cs.CL
When A Man Says He Is Pregnant: ERP Evidence for A Rational Account of Speaker-contextualized Language Comprehension,"Spoken language is often, if not always, understood in a context formed by
the identity of the speaker. For example, we can easily make sense of an
utterance such as ""I'm going to have a manicure this weekend"" or ""The first
time I got pregnant I had a hard time"" when spoken by a woman, but it would be
harder to understand when it is spoken by a man. Previous event-related
potential (ERP) studies have shown mixed results regarding the
neurophysiological responses to such speaker-content mismatches, with some
reporting an N400 effect and others a P600 effect. In an electroencephalography
(EEG) experiment involving 64 participants, we used social and biological
mismatches as test cases to demonstrate how these distinct ERP patterns reflect
different aspects of rational inference. We showed that when the mismatch
involves social stereotypes (e.g., men getting a manicure), listeners can
arrive at a ""literal"" interpretation by integrating the content with their
social knowledge, though this integration requires additional effort due to
stereotype violations-resulting in an N400 effect. In contrast, when the
mismatch involves biological knowledge (e.g., men getting pregnant), a
""literal"" interpretation becomes impossible, leading listeners to treat the
input as potentially containing errors and engage in correction
processes-resulting in a P600 effect. Supporting this rational inference
framework, we found that the social N400 effect decreased as a function of the
listener's personality trait of openness (as more open-minded individuals
maintain more flexible social expectations), while the biological P600 effect
remained robust (as biological constraints are recognized regardless of
individual personalities). Our findings help to reconcile the empirical
inconsistencies and show how rational inference shapes speaker-contextualized
language comprehension.",2024-09-26,"Hanlin Wu, Zhenguang G. Cai",http://arxiv.org/pdf/2409.17525v2,cs.CL
"Comparing Unidirectional, Bidirectional, and Word2vec Models for Discovering Vulnerabilities in Compiled Lifted Code","Ransomware and other forms of malware cause significant financial and
operational damage to organizations by exploiting long-standing and often
difficult-to-detect software vulnerabilities. To detect vulnerabilities such as
buffer overflows in compiled code, this research investigates the application
of unidirectional transformer-based embeddings, specifically GPT-2. Using a
dataset of LLVM functions, we trained a GPT-2 model to generate embeddings,
which were subsequently used to build LSTM neural networks to differentiate
between vulnerable and non-vulnerable code. Our study reveals that embeddings
from the GPT-2 model significantly outperform those from bidirectional models
of BERT and RoBERTa, achieving an accuracy of 92.5% and an F1-score of 89.7%.
LSTM neural networks were developed with both frozen and unfrozen embedding
model layers. The model with the highest performance was achieved when the
embedding layers were unfrozen. Further, the research finds that, in exploring
the impact of different optimizers within this domain, the SGD optimizer
demonstrates superior performance over Adam. Overall, these findings reveal
important insights into the potential of unidirectional transformer-based
approaches in enhancing cybersecurity defenses.",2024-09-26,"Gary A. McCully, John D. Hastings, Shengjie Xu, Adam Fortier",http://arxiv.org/pdf/2409.17513v2,cs.CL
HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection,"The surge in applications of large language models (LLMs) has prompted
concerns about the generation of misleading or fabricated information, known as
hallucinations. Therefore, detecting hallucinations has become critical to
maintaining trust in LLM-generated content. A primary challenge in learning a
truthfulness classifier is the lack of a large amount of labeled truthful and
hallucinated data. To address the challenge, we introduce HaloScope, a novel
learning framework that leverages the unlabeled LLM generations in the wild for
hallucination detection. Such unlabeled data arises freely upon deploying LLMs
in the open world, and consists of both truthful and hallucinated information.
To harness the unlabeled data, we present an automated membership estimation
score for distinguishing between truthful and untruthful generations within
unlabeled mixture data, thereby enabling the training of a binary truthfulness
classifier on top. Importantly, our framework does not require extra data
collection and human annotations, offering strong flexibility and practicality
for real-world applications. Extensive experiments show that HaloScope can
achieve superior hallucination detection performance, outperforming the
competitive rivals by a significant margin. Code is available at
https://github.com/deeplearningwisc/haloscope.",2024-09-26,"Xuefeng Du, Chaowei Xiao, Yixuan Li",http://arxiv.org/pdf/2409.17504v1,cs.CL
MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models,"Large Language Models (LLMs) are distinguished by their massive parameter
counts, which typically result in significant redundancy. This work introduces
MaskLLM, a learnable pruning method that establishes Semi-structured (or
``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during
inference. Instead of developing a new importance criterion, MaskLLM explicitly
models N:M patterns as a learnable distribution through Gumbel Softmax
sampling. This approach facilitates end-to-end training on large-scale datasets
and offers two notable advantages: 1) High-quality Masks - our method
effectively scales to large datasets and learns accurate masks; 2)
Transferability - the probabilistic modeling of mask distribution enables the
transfer learning of sparsity across domains or tasks. We assessed MaskLLM
using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3,
with sizes ranging from 843M to 15B parameters, and our empirical results show
substantial improvements over state-of-the-art methods. For instance, leading
approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to
the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL
solely by learning the masks with frozen weights. Furthermore, MaskLLM's
learnable nature allows customized masks for lossless application of 2:4
sparsity to downstream tasks or domains. Code is available at
https://github.com/NVlabs/MaskLLM.",2024-09-26,"Gongfan Fang, Hongxu Yin, Saurav Muralidharan, Greg Heinrich, Jeff Pool, Jan Kautz, Pavlo Molchanov, Xinchao Wang",http://arxiv.org/pdf/2409.17481v2,cs.CL
Reducing and Exploiting Data Augmentation Noise through Meta Reweighting Contrastive Learning for Text Classification,"Data augmentation has shown its effectiveness in resolving the data-hungry
problem and improving model's generalization ability. However, the quality of
augmented data can be varied, especially compared with the raw/original data.
To boost deep learning models' performance given augmented data/samples in text
classification tasks, we propose a novel framework, which leverages both meta
learning and contrastive learning techniques as parts of our design for
reweighting the augmented samples and refining their feature representations
based on their quality. As part of the framework, we propose novel
weight-dependent enqueue and dequeue algorithms to utilize augmented samples'
weight/quality information effectively. Through experiments, we show that our
framework can reasonably cooperate with existing deep learning models (e.g.,
RoBERTa-base and Text-CNN) and augmentation techniques (e.g., Wordnet and
Easydata) for specific supervised learning tasks. Experiment results show that
our framework achieves an average of 1.6%, up to 4.3% absolute improvement on
Text-CNN encoders and an average of 1.4%, up to 4.4% absolute improvement on
RoBERTa-base encoders on seven GLUE benchmark datasets compared with the best
baseline. We present an indepth analysis of our framework design, revealing the
non-trivial contributions of our network components. Our code is publicly
available for better reproducibility.",2024-09-26,"Guanyi Mou, Yichuan Li, Kyumin Lee",http://arxiv.org/pdf/2409.17474v1,cs.CL
Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards,"Recent advances in automated essay scoring (AES) have shifted towards
evaluating multiple traits to provide enriched feedback. Like typical AES
systems, multi-trait AES employs the quadratic weighted kappa (QWK) to measure
agreement with human raters, aligning closely with the rating schema; however,
its non-differentiable nature prevents its direct use in neural network
training. In this paper, we propose Scoring-aware Multi-reward Reinforcement
Learning (SaMRL), which integrates actual evaluation schemes into the training
process by designing QWK-based rewards with a mean-squared error penalty for
multi-trait AES. Existing reinforcement learning (RL) applications in AES are
limited to classification models despite associated performance degradation, as
RL requires probability distributions; instead, we adopt an autoregressive
score generation framework to leverage token generation probabilities for
robust multi-trait score predictions. Empirical analyses demonstrate that SaMRL
facilitates model training, notably enhancing scoring of previously inferior
prompts.",2024-09-26,"Heejin Do, Sangwon Ryu, Gary Geunbae Lee",http://arxiv.org/pdf/2409.17472v1,cs.CL
What is the social benefit of hate speech detection research? A Systematic Review,"While NLP research into hate speech detection has grown exponentially in the
last three decades, there has been minimal uptake or engagement from policy
makers and non-profit organisations. We argue the absence of ethical frameworks
have contributed to this rift between current practice and best practice. By
adopting appropriate ethical frameworks, NLP researchers may enable the social
impact potential of hate speech research. This position paper is informed by
reviewing forty-eight hate speech detection systems associated with
thirty-seven publications from different venues.",2024-09-26,Sidney Gig-Jan Wong,http://arxiv.org/pdf/2409.17467v1,cs.CL
RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking,"The rapid progress of Large Language Models (LLMs) has opened up new
opportunities across various domains and applications; yet it also presents
challenges related to potential misuse. To mitigate such risks, red teaming has
been employed as a proactive security measure to probe language models for
harmful outputs via jailbreak attacks. However, current jailbreak attack
approaches are single-turn with explicit malicious queries that do not fully
capture the complexity of real-world interactions. In reality, users can engage
in multi-turn interactions with LLM-based chat assistants, allowing them to
conceal their true intentions in a more covert manner. To bridge this gap, we,
first, propose a new jailbreak approach, RED QUEEN ATTACK. This method
constructs a multi-turn scenario, concealing the malicious intent under the
guise of preventing harm. We craft 40 scenarios that vary in turns and select
14 harmful categories to generate 56k multi-turn attack data points. We conduct
comprehensive experiments on the RED QUEEN ATTACK with four representative LLM
families of different sizes. Our experiments reveal that all LLMs are
vulnerable to RED QUEEN ATTACK, reaching 87.62% attack success rate on GPT-4o
and 75.4% on Llama3-70B. Further analysis reveals that larger models are more
susceptible to the RED QUEEN ATTACK, with multi-turn structures and concealment
strategies contributing to its success. To prioritize safety, we introduce a
straightforward mitigation strategy called RED QUEEN GUARD, which aligns LLMs
to effectively counter adversarial attacks. This approach reduces the attack
success rate to below 1% while maintaining the model's performance across
standard benchmarks. Full implementation and dataset are publicly accessible at
https://github.com/kriti-hippo/red_queen.",2024-09-26,"Yifan Jiang, Kriti Aggarwal, Tanmay Laud, Kashif Munir, Jay Pujara, Subhabrata Mukherjee",http://arxiv.org/pdf/2409.17458v1,cs.CL
A Novel Spinor-Based Embedding Model for Transformers,"This paper proposes a novel approach to word embeddings in Transformer models
by utilizing spinors from geometric algebra. Spinors offer a rich mathematical
framework capable of capturing complex relationships and transformations in
high-dimensional spaces. By encoding words as spinors, we aim to enhance the
expressiveness and robustness of language representations. We present the
theoretical foundations of spinors, detail their integration into Transformer
architectures, and discuss potential advantages and challenges.",2024-09-26,Rick White,http://arxiv.org/pdf/2410.00038v1,cs.CL
Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models,"Language models (LMs), despite their advances, often depend on spurious
correlations, undermining their accuracy and generalizability. This study
addresses the overlooked impact of subtler, more complex shortcuts that
compromise model reliability beyond oversimplified shortcuts. We introduce a
comprehensive benchmark that categorizes shortcuts into occurrence, style, and
concept, aiming to explore the nuanced ways in which these shortcuts influence
the performance of LMs. Through extensive experiments across traditional LMs,
large language models, and state-of-the-art robust models, our research
systematically investigates models' resilience and susceptibilities to
sophisticated shortcuts. Our benchmark and code can be found at:
https://github.com/yuqing-zhou/shortcut-learning-in-text-classification.",2024-09-26,"Yuqing Zhou, Ruixiang Tang, Ziyu Yao, Ziwei Zhu",http://arxiv.org/pdf/2409.17455v1,cs.CL
Description-based Controllable Text-to-Speech with Cross-Lingual Voice Control,"We propose a novel description-based controllable text-to-speech (TTS) method
with cross-lingual control capability. To address the lack of audio-description
paired data in the target language, we combine a TTS model trained on the
target language with a description control model trained on another language,
which maps input text descriptions to the conditional features of the TTS
model. These two models share disentangled timbre and style representations
based on self-supervised learning (SSL), allowing for disentangled voice
control, such as controlling speaking styles while retaining the original
timbre. Furthermore, because the SSL-based timbre and style representations are
language-agnostic, combining the TTS and description control models while
sharing the same embedding space effectively enables cross-lingual control of
voice characteristics. Experiments on English and Japanese TTS demonstrate that
our method achieves high naturalness and controllability for both languages,
even though no Japanese audio-description pairs are used.",2024-09-26,"Ryuichi Yamamoto, Yuma Shirahata, Masaya Kawamura, Kentaro Tachibana",http://arxiv.org/pdf/2409.17452v1,cs.CL
Enhancing Financial Sentiment Analysis with Expert-Designed Hint,"This paper investigates the role of expert-designed hint in enhancing
sentiment analysis on financial social media posts. We explore the capability
of large language models (LLMs) to empathize with writer perspectives and
analyze sentiments. Our findings reveal that expert-designed hint, i.e.,
pointing out the importance of numbers, significantly improve performances
across various LLMs, particularly in cases requiring perspective-taking skills.
Further analysis on tweets containing different types of numerical data
demonstrates that the inclusion of expert-designed hint leads to notable
improvements in sentiment analysis performance, especially for tweets with
monetary-related numbers. Our findings contribute to the ongoing discussion on
the applicability of Theory of Mind in NLP and open new avenues for improving
sentiment analysis in financial domains through the strategic use of expert
knowledge.",2024-09-26,"Chung-Chi Chen, Hiroya Takamura, Ichiro Kobayashi, Yusuke Miyao",http://arxiv.org/pdf/2409.17448v1,cs.CL
HDFlow: Enhancing LLM Complex Problem-Solving with Hybrid Thinking and Dynamic Workflows,"Despite recent advancements in large language models (LLMs), their
performance on complex reasoning problems requiring multi-step thinking and
combining various skills is still limited. To address this, we propose a novel
framework HDFlow for complex reasoning with LLMs that combines fast and slow
thinking modes in an adaptive manner. Our approach consists of two key
components: 1) a new approach for slow, deliberate reasoning called Dynamic
Workflow, which automatically decomposes complex problems into more manageable
sub-tasks and dynamically designs a workflow to assemble specialized LLM or
symbolic reasoning tools to solve sub-tasks; 2) Hybrid Thinking, a general
framework that dynamically combines fast and slow thinking based on problem
complexity. Finally, we propose an easy-to-scale method for automatically
synthesizing a large-scale dataset of 27K challenging reasoning problems for
complex reasoning and a hybrid thinking tuning method that trains smaller LLMs
on this dataset to internalize the fast/slow hybrid reasoning strategies.
Experiments on four reasoning benchmark datasets demonstrate that our slow
thinking with dynamic workflows significantly outperforms Chain-of-Thought, and
hybrid thinking achieves the highest accuracy while providing an effective
balance between computational efficiency and performance. Fine-tuning using our
hybrid thinking approach also significantly boosts the complex reasoning
capabilities of open-source language models. The results showcase the promise
of slow thinking, dynamic workflows, and hybrid thinking in expanding the
frontier of complex problem-solving with LLMs\footnote{Code and data will be
released at \url{https://github.com/wenlinyao/HDFlow}.}.",2024-09-25,"Wenlin Yao, Haitao Mi, Dong Yu",http://arxiv.org/pdf/2409.17433v1,cs.CL
On Extending Direct Preference Optimization to Accommodate Ties,"We derive and investigate two DPO variants that explicitly model the
possibility of declaring a tie in pair-wise comparisons. We replace the
Bradley-Terry model in DPO with two well-known modeling extensions, by Rao and
Kupper and by Davidson, that assign probability to ties as alternatives to
clear preferences. Our experiments in neural machine translation and
summarization show that explicitly labeled ties can be added to the datasets
for these DPO variants without the degradation in task performance that is
observed when the same tied pairs are presented to DPO. We find empirically
that the inclusion of ties leads to stronger regularization with respect to the
reference policy as measured by KL divergence, and we see this even for DPO in
its original form. These findings motivate and enable the inclusion of tied
pairs in preference optimization as opposed to simply discarding them.",2024-09-25,"Jinghong Chen, Guangyu Yang, Weizhe Lin, Jingbiao Mei, Bill Byrne",http://arxiv.org/pdf/2409.17431v1,cs.CL
Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction,"Large Language Models (LLMs) have demonstrated remarkable capabilities in
handling long context inputs, but this comes at the cost of increased
computational resources and latency. Our research introduces a novel approach
for the long context bottleneck to accelerate LLM inference and reduce GPU
memory consumption. Our research demonstrates that LLMs can identify relevant
tokens in the early layers before generating answers to a query. Leveraging
this insight, we propose an algorithm that uses early layers of an LLM as
filters to select and compress input tokens, significantly reducing the context
length for subsequent processing. Our method, GemFilter, demonstrates
substantial improvements in both speed and memory efficiency compared to
existing techniques, such as standard attention and SnapKV/H2O. Notably, it
achieves a 2.4$\times$ speedup and 30\% reduction in GPU memory usage compared
to SOTA methods. Evaluation on the Needle in a Haystack task shows that
GemFilter significantly outperforms standard attention, SnapKV and demonstrates
comparable performance on the LongBench challenge. GemFilter is simple,
training-free, and broadly applicable across different LLMs. Crucially, it
provides interpretability by allowing humans to inspect the selected input
sequence. These findings not only offer practical benefits for LLM deployment,
but also enhance our understanding of LLM internal mechanisms, paving the way
for further optimizations in LLM design and inference. Our code is available at
\url{https://github.com/SalesforceAIResearch/GemFilter}.",2024-09-25,"Zhenmei Shi, Yifei Ming, Xuan-Phi Nguyen, Yingyu Liang, Shafiq Joty",http://arxiv.org/pdf/2409.17422v1,cs.CL
Pre-Finetuning with Impact Duration Awareness for Stock Movement Prediction,"Understanding the duration of news events' impact on the stock market is
crucial for effective time-series forecasting, yet this facet is largely
overlooked in current research. This paper addresses this research gap by
introducing a novel dataset, the Impact Duration Estimation Dataset (IDED),
specifically designed to estimate impact duration based on investor opinions.
Our research establishes that pre-finetuning language models with IDED can
enhance performance in text-based stock movement predictions. In addition, we
juxtapose our proposed pre-finetuning task with sentiment analysis
pre-finetuning, further affirming the significance of learning impact duration.
Our findings highlight the promise of this novel research direction in stock
movement prediction, offering a new avenue for financial forecasting. We also
provide the IDED and pre-finetuned language models under the CC BY-NC-SA 4.0
license for academic use, fostering further exploration in this field.",2024-09-25,"Chr-Jr Chiu, Chung-Chi Chen, Hen-Hsen Huang, Hsin-Hsi Chen",http://arxiv.org/pdf/2409.17419v1,cs.CL
Enhancing Investment Opinion Ranking through Argument-Based Sentiment Analysis,"In the era of rapid Internet and social media platform development,
individuals readily share their viewpoints online. The overwhelming quantity of
these posts renders comprehensive analysis impractical. This necessitates an
efficient recommendation system to filter and present significant, relevant
opinions. Our research introduces a dual-pronged argument mining technique to
improve recommendation system effectiveness, considering both professional and
amateur investor perspectives. Our first strategy involves using the
discrepancy between target and closing prices as an opinion indicator. The
second strategy applies argument mining principles to score investors'
opinions, subsequently ranking them by these scores. Experimental results
confirm the effectiveness of our approach, demonstrating its ability to
identify opinions with higher profit potential. Beyond profitability, our
research extends to risk analysis, examining the relationship between
recommended opinions and investor behaviors. This offers a holistic view of
potential outcomes following the adoption of these recommended opinions.",2024-09-25,"Chung-Chi Chen, Hen-Hsen Huang, Hsin-Hsi Chen, Hiroya Takamura, Ichiro Kobayashi, Yusuke Miyao",http://arxiv.org/pdf/2409.17417v1,cs.CL
From Deception to Detection: The Dual Roles of Large Language Models in Fake News,"Fake news poses a significant threat to the integrity of information
ecosystems and public trust. The advent of Large Language Models (LLMs) holds
considerable promise for transforming the battle against fake news. Generally,
LLMs represent a double-edged sword in this struggle. One major concern is that
LLMs can be readily used to craft and disseminate misleading information on a
large scale. This raises the pressing questions: Can LLMs easily generate
biased fake news? Do all LLMs have this capability? Conversely, LLMs offer
valuable prospects for countering fake news, thanks to their extensive
knowledge of the world and robust reasoning capabilities. This leads to other
critical inquiries: Can we use LLMs to detect fake news, and do they outperform
typical detection models? In this paper, we aim to address these pivotal
questions by exploring the performance of various LLMs. Our objective is to
explore the capability of various LLMs in effectively combating fake news,
marking this as the first investigation to analyze seven such models. Our
results reveal that while some models adhere strictly to safety protocols,
refusing to generate biased or misleading content, other models can readily
produce fake news across a spectrum of biases. Additionally, our results show
that larger models generally exhibit superior detection abilities and that
LLM-generated fake news are less likely to be detected than human-written ones.
Finally, our findings demonstrate that users can benefit from LLM-generated
explanations in identifying fake news.",2024-09-25,"Dorsaf Sallami, Yuan-Chen Chang, Esma Aïmeur",http://arxiv.org/pdf/2409.17416v1,cs.CL
Post-hoc Reward Calibration: A Case Study on Length Bias,"Reinforcement Learning from Human Feedback aligns the outputs of Large
Language Models with human values and preferences. Central to this process is
the reward model (RM), which translates human feedback into training signals
for optimising LLM behaviour. However, RMs can develop biases by exploiting
spurious correlations in their training data, such as favouring outputs based
on length or style rather than true quality. These biases can lead to incorrect
output rankings, sub-optimal model evaluations, and the amplification of
undesirable behaviours in LLMs alignment. This paper addresses the challenge of
correcting such biases without additional data and training, introducing the
concept of Post-hoc Reward Calibration. We first propose an intuitive approach
to estimate the bias term and, thus, remove it to approximate the underlying
true reward. We then extend the approach to a more general and robust form with
the Locally Weighted Regression. Focusing on the prevalent length bias, we
validate our proposed approaches across three experimental settings,
demonstrating consistent improvements: (1) a 3.11 average performance gain
across 33 reward models on the RewardBench dataset; (2) enhanced alignment of
RM rankings with GPT-4 evaluations and human preferences based on the
AlpacaEval benchmark; and (3) improved Length-Controlled win rate of the RLHF
process in multiple LLM--RM combinations. Our method is computationally
efficient and generalisable to other types of bias and RMs, offering a scalable
and robust solution for mitigating biases in LLM alignment. Our code and
results are available at https://github.com/ZeroYuHuang/Reward-Calibration.",2024-09-25,"Zeyu Huang, Zihan Qiu, Zili Wang, Edoardo M. Ponti, Ivan Titov",http://arxiv.org/pdf/2409.17407v1,cs.CL
Building Multilingual Datasets for Predicting Mental Health Severity through LLMs: Prospects and Challenges,"Large Language Models (LLMs) are increasingly being integrated into various
medical fields, including mental health support systems. However, there is a
gap in research regarding the effectiveness of LLMs in non-English mental
health support applications. To address this problem, we present a novel
multilingual adaptation of widely-used mental health datasets, translated from
English into six languages (e.g., Greek, Turkish, French, Portuguese, German,
and Finnish). This dataset enables a comprehensive evaluation of LLM
performance in detecting mental health conditions and assessing their severity
across multiple languages. By experimenting with GPT and Llama, we observe
considerable variability in performance across languages, despite being
evaluated on the same translated dataset. This inconsistency underscores the
complexities inherent in multilingual mental health support, where
language-specific nuances and mental health data coverage can affect the
accuracy of the models. Through comprehensive error analysis, we emphasize the
risks of relying exclusively on LLMs in medical settings (e.g., their potential
to contribute to misdiagnoses). Moreover, our proposed approach offers
significant cost savings for multilingual tasks, presenting a major advantage
for broad-scale implementation.",2024-09-25,"Konstantinos Skianis, John Pavlopoulos, A. Seza Doğruöz",http://arxiv.org/pdf/2409.17397v2,cs.CL
Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia,"Though Large Language Models (LLMs) have shown remarkable abilities in
mathematics reasoning, they are still struggling with performing numeric
operations accurately, such as addition and multiplication. Numbers can be
tokenized into tokens in various ways by different LLMs and affect the numeric
operations performance. Currently, there are two representatives: 1) Tokenize
into $1$-digit, and 2) Tokenize into $1\sim 3$ digit. The difference is roughly
equivalent to using different numeral systems (namely base $10$ or base
$10^{3}$). In light of this, we study the scaling behavior of different numeral
systems in the context of transformer-based large language models. We
empirically show that a base $10$ system is consistently more data-efficient
than a base $10^{2}$ or $10^{3}$ system across training data scale, model sizes
under from-scratch training settings, while different number systems have very
similar fine-tuning performances. We attribute this to higher token frequencies
of a base $10$ system. Additionally, we reveal extrapolation behavior patterns
on addition and multiplication. We identify that base $100$ and base $1000$
systems struggle on token-level discernment and token-level operations. We also
sheds light on the mechanism learnt by the models.",2024-09-25,"Zhejian Zhou, Jiayu Wang, Dahua Lin, Kai Chen",http://arxiv.org/pdf/2409.17391v2,cs.CL
data2lang2vec: Data Driven Typological Features Completion,"Language typology databases enhance multi-lingual Natural Language Processing
(NLP) by improving model adaptability to diverse linguistic structures. The
widely-used lang2vec toolkit integrates several such databases, but its
coverage remains limited at 28.9\%. Previous work on automatically increasing
coverage predicts missing values based on features from other languages or
focuses on single features, we propose to use textual data for better-informed
feature prediction. To this end, we introduce a multi-lingual Part-of-Speech
(POS) tagger, achieving over 70\% accuracy across 1,749 languages, and
experiment with external statistical features and a variety of machine learning
algorithms. We also introduce a more realistic evaluation setup, focusing on
likely to be missing typology features, and show that our approach outperforms
previous work in both setups.",2024-09-25,"Hamidreza Amirzadeh, Sadegh Jafari, Anika Harju, Rob van der Goot",http://arxiv.org/pdf/2409.17373v1,cs.CL
Internalizing ASR with Implicit Chain of Thought for Efficient Speech-to-Speech Conversational LLM,"Current speech-based LLMs are predominantly trained on extensive ASR and TTS
datasets, excelling in tasks related to these domains. However, their ability
to handle direct speech-to-speech conversations remains notably constrained.
These models often rely on an ASR-to-TTS chain-of-thought pipeline, converting
speech into text for processing before generating audio responses, which
introduces latency and loses audio features. We propose a method that
implicitly internalizes ASR chain of thought into a speech LLM, enhancing its
native speech understanding capabilities. Our approach reduces latency and
improves the model's native understanding of speech, paving the way for more
efficient and natural real-time audio interactions. We also release a
large-scale synthetic conversational dataset to facilitate further research.",2024-09-25,"Robin Shing-Hei Yuen, Timothy Tin-Long Tse, Jian Zhu",http://arxiv.org/pdf/2409.17353v3,cs.CL
How Transliterations Improve Crosslingual Alignment,"Recent studies have shown that post-aligning multilingual pretrained language
models (mPLMs) using alignment objectives on both original and transliterated
data can improve crosslingual alignment. This improvement further leads to
better crosslingual transfer performance. However, it remains unclear how and
why a better crosslingual alignment is achieved, as this technique only
involves transliterations, and does not use any parallel data. This paper
attempts to explicitly evaluate the crosslingual alignment and identify the key
elements in transliteration-based approaches that contribute to better
performance. For this, we train multiple models under varying setups for two
pairs of related languages: (1) Polish and Ukrainian and (2) Hindi and Urdu. To
assess alignment, we define four types of similarities based on sentence
representations. Our experimental results show that adding transliterations
alone improves the overall similarities, even for random sentence pairs. With
the help of auxiliary transliteration-based alignment objectives, especially
the contrastive objective, the model learns to distinguish matched from random
pairs, leading to better crosslingual alignment. However, we also show that
better alignment does not always yield better downstream performance,
suggesting that further research is needed to clarify the connection between
alignment and performance. The code implementation is based on
\url{https://github.com/cisnlp/Transliteration-PPA}.",2024-09-25,"Yihong Liu, Mingyang Wang, Amir Hossein Kargaran, Ayyoob Imani, Orgest Xhelili, Haotian Ye, Chunlan Ma, François Yvon, Hinrich Schütze",http://arxiv.org/pdf/2409.17326v2,cs.CL
Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation,"This study presents a novel evaluation framework for the Vision-Language
Navigation (VLN) task. It aims to diagnose current models for various
instruction categories at a finer-grained level. The framework is structured
around the context-free grammar (CFG) of the task. The CFG serves as the basis
for the problem decomposition and the core premise of the instruction
categories design. We propose a semi-automatic method for CFG construction with
the help of Large-Language Models (LLMs). Then, we induct and generate data
spanning five principal instruction categories (i.e. direction change, landmark
recognition, region recognition, vertical movement, and numerical
comprehension). Our analysis of different models reveals notable performance
discrepancies and recurrent issues. The stagnation of numerical comprehension,
heavy selective biases over directional concepts, and other interesting
findings contribute to the development of future language-guided navigation
systems.",2024-09-25,"Zehao Wang, Minye Wu, Yixin Cao, Yubo Ma, Meiqi Chen, Tinne Tuytelaars",http://arxiv.org/pdf/2409.17313v1,cs.CL
BabyLlama-2: Ensemble-Distilled Models Consistently Outperform Teachers With Limited Data,"We present BabyLlama-2, a 345 million parameter model distillation-pretrained
from two teachers on a 10 million word corpus for the BabyLM competition. On
BLiMP and SuperGLUE benchmarks, BabyLlama-2 outperforms baselines trained on
both 10 and 100 million word datasets with the same data mix, as well as its
teacher models. Through an extensive hyperparameter sweep, we demonstrate that
the advantages of distillation cannot be attributed to suboptimal
hyperparameter selection of the teachers. Our findings underscore the need for
further investigation into distillation techniques, particularly in
data-limited settings.",2024-09-25,"Jean-Loup Tastet, Inar Timiryasov",http://arxiv.org/pdf/2409.17312v1,cs.CL
Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,"Large Language Models (LLMs) have revolutionized natural language processing,
yet they struggle with inconsistent reasoning, particularly in novel domains
and complex logical sequences. This research introduces Proof of Thought, a
framework that enhances the reliability and transparency of LLM outputs. Our
approach bridges LLM-generated ideas with formal logic verification, employing
a custom interpreter to convert LLM outputs into First Order Logic constructs
for theorem prover scrutiny. Central to our method is an intermediary
JSON-based Domain-Specific Language, which by design balances precise logical
structures with intuitive human concepts. This hybrid representation enables
both rigorous validation and accessible human comprehension of LLM reasoning
processes. Key contributions include a robust type system with sort management
for enhanced logical integrity, explicit representation of rules for clear
distinction between factual and inferential knowledge, and a flexible
architecture that allows for easy extension to various domain-specific
applications. We demonstrate Proof of Thought's effectiveness through
benchmarking on StrategyQA and a novel multimodal reasoning task, showing
improved performance in open-ended scenarios. By providing verifiable and
interpretable results, our technique addresses critical needs for AI system
accountability and sets a foundation for human-in-the-loop oversight in
high-stakes domains.",2024-09-25,"Debargha Ganguly, Srinivasan Iyengar, Vipin Chaudhary, Shivkumar Kalyanaraman",http://arxiv.org/pdf/2409.17270v2,cs.CL
Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The
strongest open-weight models rely heavily on synthetic data from proprietary
VLMs to achieve good performance, effectively distilling these closed VLMs into
open ones. As a result, the community has been missing foundational knowledge
about how to build performant VLMs from scratch. We present Molmo, a new family
of VLMs that are state-of-the-art in their class of openness. Our key
contribution is a collection of new datasets called PixMo, including a dataset
of highly detailed image captions for pre-training, a free-form image Q&A
dataset for fine-tuning, and an innovative 2D pointing dataset, all collected
without the use of external VLMs. The success of our approach relies on careful
modeling choices, a well-tuned training pipeline, and, most critically, the
quality of our newly collected datasets. Our best-in-class 72B model not only
outperforms others in the class of open weight and data models, but also
outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini
1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and
on a large human evaluation. Our model weights, new datasets, and source code
are available at https://molmo.allenai.org/blog.",2024-09-25,"Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi",http://arxiv.org/pdf/2409.17146v2,cs.CL
FineZip : Pushing the Limits of Large Language Models for Practical Lossless Text Compression,"While the language modeling objective has been shown to be deeply connected
with compression, it is surprising that modern LLMs are not employed in
practical text compression systems. In this paper, we provide an in-depth
analysis of neural network and transformer-based compression techniques to
answer this question. We compare traditional text compression systems with
neural network and LLM-based text compression methods. Although LLM-based
systems significantly outperform conventional compression methods, they are
highly impractical. Specifically, LLMZip, a recent text compression system
using Llama3-8B requires 9.5 days to compress just 10 MB of text, although with
huge improvements in compression ratios. To overcome this, we present FineZip -
a novel LLM-based text compression system that combines ideas of online
memorization and dynamic context to reduce the compression time immensely.
FineZip can compress the above corpus in approximately 4 hours compared to 9.5
days, a 54 times improvement over LLMZip and comparable performance. FineZip
outperforms traditional algorithmic compression methods with a large margin,
improving compression ratios by approximately 50\%. With this work, we take the
first step towards making lossless text compression with LLMs a reality. While
FineZip presents a significant step in that direction, LLMs are still not a
viable solution for large-scale text compression. We hope our work paves the
way for future research and innovation to solve this problem.",2024-09-25,"Fazal Mittu, Yihuan Bu, Akshat Gupta, Ashok Devireddy, Alp Eren Ozdarendeli, Anant Singh, Gopala Anumanchipalli",http://arxiv.org/pdf/2409.17141v1,cs.CL
Assessing the Level of Toxicity Against Distinct Groups in Bangla Social Media Comments: A Comprehensive Investigation,"Social media platforms have a vital role in the modern world, serving as
conduits for communication, the exchange of ideas, and the establishment of
networks. However, the misuse of these platforms through toxic comments, which
can range from offensive remarks to hate speech, is a concerning issue. This
study focuses on identifying toxic comments in the Bengali language targeting
three specific groups: transgender people, indigenous people, and migrant
people, from multiple social media sources. The study delves into the intricate
process of identifying and categorizing toxic language while considering the
varying degrees of toxicity: high, medium, and low. The methodology involves
creating a dataset, manual annotation, and employing pre-trained transformer
models like Bangla-BERT, bangla-bert-base, distil-BERT, and
Bert-base-multilingual-cased for classification. Diverse assessment metrics
such as accuracy, recall, precision, and F1-score are employed to evaluate the
model's effectiveness. The experimental findings reveal that Bangla-BERT
surpasses alternative models, achieving an F1-score of 0.8903. This research
exposes the complexity of toxicity in Bangla social media dialogues, revealing
its differing impacts on diverse demographic groups.",2024-09-25,"Mukaffi Bin Moin, Pronay Debnath, Usafa Akther Rifa, Rijeet Bin Anis",http://arxiv.org/pdf/2409.17130v1,cs.CL
Plurals: A System for Guiding LLMs Via Simulated Social Ensembles,"Recent debates raised concerns that language models may favor certain
viewpoints. But what if the solution is not to aim for a 'view from nowhere'
but rather to leverage different viewpoints? We introduce Plurals, a system and
Python library for pluralistic AI deliberation. Plurals consists of Agents
(LLMs, optionally with personas) which deliberate within customizable
Structures, with Moderators overseeing deliberation. Plurals is a generator of
simulated social ensembles. Plurals integrates with government datasets to
create nationally representative personas, includes deliberation templates
inspired by deliberative democracy, and allows users to customize both
information-sharing structures and deliberation behavior within Structures. Six
case studies demonstrate fidelity to theoretical constructs and efficacy. Three
randomized experiments show simulated focus groups produced output resonant
with an online sample of the relevant audiences (chosen over zero-shot
generation in 75% of trials). Plurals is both a paradigm and a concrete system
for pluralistic AI. The Plurals library is available at
https://github.com/josh-ashkinaze/plurals and will be continually updated.",2024-09-25,"Joshua Ashkinaze, Emily Fry, Narendra Edara, Eric Gilbert, Ceren Budak",http://arxiv.org/pdf/2409.17213v6,cs.CL
"Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Handy Appetizer","This book explores the role of Artificial Intelligence (AI), Machine Learning
(ML), and Deep Learning (DL) in driving the progress of big data analytics and
management. The book focuses on simplifying the complex mathematical concepts
behind deep learning, offering intuitive visualizations and practical case
studies to help readers understand how neural networks and technologies like
Convolutional Neural Networks (CNNs) work. It introduces several classic models
and technologies such as Transformers, GPT, ResNet, BERT, and YOLO,
highlighting their applications in fields like natural language processing,
image recognition, and autonomous driving. The book also emphasizes the
importance of pre-trained models and how they can enhance model performance and
accuracy, with instructions on how to apply these models in various real-world
scenarios. Additionally, it provides an overview of key big data management
technologies like SQL and NoSQL databases, as well as distributed computing
frameworks such as Apache Hadoop and Spark, explaining their importance in
managing and processing vast amounts of data. Ultimately, the book underscores
the value of mastering deep learning and big data management skills as critical
tools for the future workforce, making it an essential resource for both
beginners and experienced professionals.",2024-09-25,"Benji Peng, Xuanhe Pan, Yizhu Wen, Ziqian Bi, Keyu Chen, Ming Li, Ming Liu, Qian Niu, Junyu Liu, Jinlang Wang, Sen Zhang, Jiawei Xu, Pohsun Feng",http://arxiv.org/pdf/2409.17120v1,cs.CL
Programming Every Example: Lifting Pre-training Data Quality Like Experts at Scale,"Large language model pre-training has traditionally relied on human experts
to craft heuristics for improving the corpora quality, resulting in numerous
rules developed to date. However, these rules lack the flexibility to address
the unique characteristics of individual example effectively. Meanwhile,
applying tailored rules to every example is impractical for human experts. In
this paper, we demonstrate that even small language models, with as few as 0.3B
parameters, can exhibit substantial data refining capabilities comparable to
those of human experts. We introduce Programming Every Example (ProX), a novel
framework that treats data refinement as a programming task, enabling models to
refine corpora by generating and executing fine-grained operations, such as
string normalization, for each individual example at scale. Experimental
results show that models pre-trained on ProX-curated data outperform either
original data or data filtered by other selection methods by more than 2%
across various downstream benchmarks. Its effectiveness spans various model
sizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,
FineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in
domain-specific continual pre-training: without domain specific design, models
trained on OpenWebMath refined by ProX outperform human-crafted rule-based
methods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for
Llama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable
to models like Llemma-7B trained on 200B tokens. Further analysis highlights
that ProX significantly saves training FLOPs, offering a promising path for
efficient LLM pre-training. We are open-sourcing ProX with >500B corpus,
models, and sharing all training and implementation details for reproducible
research and future innovation. Code: https://github.com/GAIR-NLP/ProX",2024-09-25,"Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, Pengfei Liu",http://arxiv.org/pdf/2409.17115v2,cs.CL
Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?,"Large vision-language models (VLMs) have become state-of-the-art for many
computer vision tasks, with in-context learning (ICL) as a popular adaptation
strategy for new ones. But can VLMs learn novel concepts purely from visual
demonstrations, or are they limited to adapting to the output format of ICL
examples? We propose a new benchmark we call Spatial Visual Ambiguity Tasks
(SVAT) that challenges state-of-the-art VLMs to learn new visuospatial tasks
in-context. We find that VLMs fail to do this zero-shot, and sometimes continue
to fail after finetuning. However, adding simpler data to the training by
curriculum learning leads to improved ICL performance.",2024-09-25,"Bowen Zhao, Leo Parker Dirac, Paulina Varshavskaya",http://arxiv.org/pdf/2409.17080v1,cs.CL
Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition,"Accurately attributing answer text to its source document is crucial for
developing a reliable question-answering system. However, attribution for long
documents remains largely unexplored. Post-hoc attribution systems are designed
to map answer text back to the source document, yet the granularity of this
mapping has not been addressed. Furthermore, a critical question arises: What
exactly should be attributed? This involves identifying the specific
information units within an answer that require grounding. In this paper, we
propose and investigate a novel approach to the factual decomposition of
generated answers for attribution, employing template-based in-context
learning. To accomplish this, we utilize the question and integrate negative
sampling during few-shot in-context learning for decomposition. This approach
enhances the semantic understanding of both abstractive and extractive answers.
We examine the impact of answer decomposition by providing a thorough
examination of various attribution approaches, ranging from retrieval-based
techniques to LLM-based attributors.",2024-09-25,"Pritika Ramu, Koustava Goswami, Apoorv Saxena, Balaji Vasan Srinivasan",http://arxiv.org/pdf/2409.17073v4,cs.CL
Using LLM for Real-Time Transcription and Summarization of Doctor-Patient Interactions into ePuskesmas in Indonesia,"One of the key issues contributing to inefficiency in Puskesmas is the
time-consuming nature of doctor-patient interactions. Doctors need to conduct
thorough consultations, which include diagnosing the patient's condition,
providing treatment advice, and transcribing detailed notes into medical
records. In regions with diverse linguistic backgrounds, doctors often have to
ask clarifying questions, further prolonging the process. While diagnosing is
essential, transcription and summarization can often be automated using AI to
improve time efficiency and help doctors enhance care quality and enable early
diagnosis and intervention. This paper proposes a solution using a localized
large language model (LLM) to transcribe, translate, and summarize
doctor-patient conversations. We utilize the Whisper model for transcription
and GPT-3 to summarize them into the ePuskemas medical records format. This
system is implemented as an add-on to an existing web browser extension,
allowing doctors to fill out patient forms while talking. By leveraging this
solution for real-time transcription, translation, and summarization, doctors
can improve the turnaround time for patient care while enhancing the quality of
records, which become more detailed and insightful for future visits. This
innovation addresses challenges like overcrowded facilities and the
administrative burden on healthcare providers in Indonesia. We believe this
solution will help doctors save time, provide better care, and produce more
accurate medical records, representing a significant step toward modernizing
healthcare and ensuring patients receive timely, high-quality care, even in
resource-constrained settings.",2024-09-25,"Azmul Asmar Irfan, Nur Ahmad Khatim, Mansur M. Arief",http://arxiv.org/pdf/2409.17054v1,cs.CL
Detecting Temporal Ambiguity in Questions,"Detecting and answering ambiguous questions has been a challenging task in
open-domain question answering. Ambiguous questions have different answers
depending on their interpretation and can take diverse forms. Temporally
ambiguous questions are one of the most common types of such questions. In this
paper, we introduce TEMPAMBIQA, a manually annotated temporally ambiguous QA
dataset consisting of 8,162 open-domain questions derived from existing
datasets. Our annotations focus on capturing temporal ambiguity to study the
task of detecting temporally ambiguous questions. We propose a novel approach
by using diverse search strategies based on disambiguated versions of the
questions. We also introduce and test non-search, competitive baselines for
detecting temporal ambiguity using zero-shot and few-shot approaches.",2024-09-25,"Bhawna Piryani, Abdelrahman Abdallah, Jamshid Mozafari, Adam Jatowt",http://arxiv.org/pdf/2409.17046v1,cs.CL
How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not,"The remarkable performance achieved by Large Language Models (LLM) has driven
research efforts to leverage them for a wide range of tasks and input
modalities. In speech-to-text (S2T) tasks, the emerging solution consists of
projecting the output of the encoder of a Speech Foundational Model (SFM) into
the LLM embedding space through an adapter module. However, no work has yet
investigated how much the downstream-task performance depends on each component
(SFM, adapter, LLM) nor whether the best design of the adapter depends on the
chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter
modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on
two widespread S2T tasks, namely Automatic Speech Recognition and Speech
Translation. Our results demonstrate that the SFM plays a pivotal role in
downstream performance, while the adapter choice has moderate impact and
depends on the SFM and LLM.",2024-09-25,"Francesco Verdini, Pierfrancesco Melucci, Stefano Perna, Francesco Cariaggi, Marco Gaido, Sara Papi, Szymon Mazurek, Marek Kasztelnik, Luisa Bentivogli, Sébastien Bratières, Paolo Merialdo, Simone Scardapane",http://arxiv.org/pdf/2409.17044v2,cs.CL
Counterfactual Token Generation in Large Language Models,"""Sure, I am happy to generate a story for you: Captain Lyra stood at the helm
of her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]
Lyra's eyes welled up with tears as she realized the bitter truth - she had
sacrificed everything for fleeting riches, and lost the love of her crew, her
family, and herself."" Although this story, generated by a large language model,
is captivating, one may wonder -- how would the story have unfolded if the
model had chosen ""Captain Maeve"" as the protagonist instead? We cannot know.
State-of-the-art large language models are stateless -- they maintain no
internal memory or state. Given a prompt, they generate a sequence of tokens as
an output using an autoregressive process. As a consequence, they cannot reason
about counterfactual alternatives to tokens they have generated in the past. In
this work, our goal is to enhance them with this functionality. To this end, we
develop a causal model of token generation that builds upon the Gumbel-Max
structural causal model. Our model allows any large language model to perform
counterfactual token generation at almost no cost in comparison with vanilla
token generation, it is embarrassingly simple to implement, and it does not
require any fine-tuning nor prompt engineering. We implement our model on Llama
3 8B-Instruct and Ministral-8B-Instruct and conduct a qualitative and a
quantitative analysis of counterfactually generated text. We conclude with a
demonstrative application of counterfactual token generation for bias
detection, unveiling interesting insights about the model of the world
constructed by large language models.",2024-09-25,"Ivi Chatzi, Nina Corvelo Benz, Eleni Straitouri, Stratis Tsirtsis, Manuel Gomez-Rodriguez",http://arxiv.org/pdf/2409.17027v3,cs.CL
AutoLLM-CARD: Towards a Description and Landscape of Large Language Models,"With the rapid growth of the Natural Language Processing (NLP) field, a vast
variety of Large Language Models (LLMs) continue to emerge for diverse NLP
tasks. As more papers are published, researchers and developers face the
challenge of information overload. Thus, developing a system that can
automatically extract and organise key information about LLMs from academic
papers is particularly important. The standard format for documenting
information about LLMs is the LLM model card (\textbf{LLM-Card}). We propose a
method for automatically generating LLM model cards from scientific
publications. We use Named Entity Recognition (\textbf{NER}) and Relation
Extraction (\textbf{RE}) methods that automatically extract key information
about LLMs from the papers, helping researchers to access information about
LLMs efficiently. These features include model \textit{licence}, model
\textit{name}, and model \textit{application}. With these features, we can form
a model card for each paper. We processed 106 academic papers by defining three
dictionaries -- LLM's name, licence, and application. 11,051 sentences were
extracted through dictionary lookup, and the dataset was constructed through
manual review of the final selection of 129 sentences with a link between the
name and the \textit{licence}, and 106 sentences with a link between the model
name and the \textit{application}. The resulting resource is relevant for LLM
card illustrations using relational knowledge graphs. Our code and findings can
contribute to automatic LLM card generation. Data and code in
\textsc{autoLLM-Card} will be shared and freely available at
\url{https://github.com/shengwei-tian/dependency-parser-visualization}",2024-09-25,"Shengwei Tian, Lifeng Han, Goran Nenadic",http://arxiv.org/pdf/2409.17011v3,cs.CL
Models Can and Should Embrace the Communicative Nature of Human-Generated Math,"Math is constructed by people for people: just as natural language corpora
reflect not just propositions but the communicative goals of language users,
the math data that models are trained on reflects not just idealized
mathematical entities but rich communicative intentions. While there are
important advantages to treating math in a purely symbolic manner, we here
hypothesize that there are benefits to treating math as situated linguistic
communication and that language models are well suited for this goal, in ways
that are not fully appreciated. We illustrate these points with two case
studies. First, we ran an experiment in which we found that language models
interpret the equals sign in a humanlike way -- generating systematically
different word problems for the same underlying equation arranged in different
ways. Second, we found that language models prefer proofs to be ordered in
naturalistic ways, even though other orders would be logically equivalent. We
advocate for AI systems that learn from and represent the communicative
intentions latent in human-generated math.",2024-09-25,"Sasha Boguraev, Ben Lipkin, Leonie Weissweiler, Kyle Mahowald",http://arxiv.org/pdf/2409.17005v2,cs.CL
Application of AI-based Models for Online Fraud Detection and Analysis,"Fraud is a prevalent offence that extends beyond financial loss, causing
psychological and physical harm to victims. The advancements in online
communication technologies alowed for online fraud to thrive in this vast
network, with fraudsters increasingly using these channels for deception. With
the progression of technologies like AI, there is a growing concern that fraud
will scale up, using sophisticated methods, like deep-fakes in phishing
campaigns, all generated by language generation models like ChatGPT. However,
the application of AI in detecting and analyzing online fraud remains
understudied. We conduct a Systematic Literature Review on AI and NLP
techniques for online fraud detection. The review adhered the PRISMA-ScR
protocol, with eligibility criteria including relevance to online fraud, use of
text data, and AI methodologies. We screened 2,457 academic records, 350 met
our eligibility criteria, and included 223. We report the state-of-the-art NLP
techniques for analysing various online fraud categories; the training data
sources; the NLP algorithms and models built; and the performance metrics
employed for model evaluation. We find that current research on online fraud is
divided into various scam activitiesand identify 16 different frauds that
researchers focus on. This SLR enhances the academic understanding of AI-based
detection methods for online fraud and offers insights for policymakers, law
enforcement, and businesses on safeguarding against such activities. We
conclude that focusing on specific scams lacks generalization, as multiple
models are required for different fraud types. The evolving nature of scams
limits the effectiveness of models trained on outdated data. We also identify
issues in data limitations, training bias reporting, and selective presentation
of metrics in model performance reporting, which can lead to potential biases
in model evaluation.",2024-09-25,"Antonis Papasavva, Shane Johnson, Ed Lowther, Samantha Lundrigan, Enrico Mariconti, Anna Markovska, Nilufer Tuptuk",http://arxiv.org/pdf/2409.19022v2,cs.CL
AXCEL: Automated eXplainable Consistency Evaluation using LLMs,"Large Language Models (LLMs) are widely used in both industry and academia
for various tasks, yet evaluating the consistency of generated text responses
continues to be a challenge. Traditional metrics like ROUGE and BLEU show a
weak correlation with human judgment. More sophisticated metrics using Natural
Language Inference (NLI) have shown improved correlations but are complex to
implement, require domain-specific training due to poor cross-domain
generalization, and lack explainability. More recently, prompt-based metrics
using LLMs as evaluators have emerged; while they are easier to implement, they
still lack explainability and depend on task-specific prompts, which limits
their generalizability. This work introduces Automated eXplainable Consistency
Evaluation using LLMs (AXCEL), a prompt-based consistency metric which offers
explanations for the consistency scores by providing detailed reasoning and
pinpointing inconsistent text spans. AXCEL is also a generalizable metric which
can be adopted to multiple tasks without changing the prompt. AXCEL outperforms
both non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting
inconsistencies across summarization by 8.7%, free text generation by 6.2%, and
data-to-text conversion tasks by 29.4%. We also evaluate the influence of
underlying LLMs on prompt based metric performance and recalibrate the SOTA
prompt-based metrics with the latest LLMs for fair comparison. Further, we show
that AXCEL demonstrates strong performance using open source LLMs.",2024-09-25,"P Aditya Sreekar, Sahil Verma, Suransh Chopra, Sarik Ghazarian, Abhishek Persad, Narayanan Sadagopan",http://arxiv.org/pdf/2409.16984v1,cs.CL
"Decoding Large-Language Models: A Systematic Overview of Socio-Technical Impacts, Constraints, and Emerging Questions","There have been rapid advancements in the capabilities of large language
models (LLMs) in recent years, greatly revolutionizing the field of natural
language processing (NLP) and artificial intelligence (AI) to understand and
interact with human language. Therefore, in this work, we conduct a systematic
investigation of the literature to identify the prominent themes and directions
of LLM developments, impacts, and limitations. Our findings illustrate the
aims, methodologies, limitations, and future directions of LLM research. It
includes responsible development considerations, algorithmic improvements,
ethical challenges, and societal implications of LLM development. Overall, this
paper provides a rigorous and comprehensive overview of current research in LLM
and identifies potential directions for future development. The article
highlights the application areas that could have a positive impact on society
along with the ethical considerations.",2024-09-25,"Zeyneb N. Kaya, Souvick Ghosh",http://arxiv.org/pdf/2409.16974v1,cs.CL
Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM Personalization,"Large language models (LLMs) have revolutionized how we interact with
technology, but their personalization to individual user preferences remains a
significant challenge, particularly in on-device applications. Traditional
methods often depend heavily on labeled datasets and can be resource-intensive.
To address these issues, we present Adaptive Self-Supervised Learning
Strategies (ASLS), which utilizes self-supervised learning techniques to
personalize LLMs dynamically. The framework comprises a user profiling layer
for collecting interaction data and a neural adaptation layer for real-time
model fine-tuning. This innovative approach enables continuous learning from
user feedback, allowing the model to generate responses that align closely with
user-specific contexts. The adaptive mechanisms of ASLS minimize computational
demands and enhance personalization efficiency. Experimental results across
various user scenarios illustrate the superior performance of ASLS in boosting
user engagement and satisfaction, highlighting its potential to redefine LLMs
as highly responsive and context-aware systems on-device.",2024-09-25,"Rafael Mendoza, Isabella Cruz, Richard Liu, Aarav Deshmukh, David Williams, Jesscia Peng, Rohan Iyer",http://arxiv.org/pdf/2409.16973v1,cs.CL
Weighted Cross-entropy for Low-Resource Languages in Multilingual Speech Recognition,"This paper addresses the challenge of integrating low-resource languages into
multilingual automatic speech recognition (ASR) systems. We introduce a novel
application of weighted cross-entropy, typically used for unbalanced datasets,
to facilitate the integration of low-resource languages into pre-trained
multilingual ASR models within the context of continual multilingual learning.
We fine-tune the Whisper multilingual ASR model on five high-resource languages
and one low-resource language, employing language-weighted dynamic
cross-entropy and data augmentation. The results show a remarkable 6.69% word
error rate (WER) reduction for the low-resource language compared to the
fine-tuned model without applying our approach, and a 48.86% WER reduction
compared to the original Whisper model. In addition, our approach yields an
average WER reduction of 3.29% across the six languages, showing no degradation
for the high-resource languages.",2024-09-25,"Andrés Piñeiro-Martín, Carmen García-Mateo, Laura Docío-Fernández, María del Carmen López-Pérez, Georg Rehm",http://arxiv.org/pdf/2409.16954v1,cs.CL
Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling,"The lack of labeled data is a common challenge in speech classification
tasks, particularly those requiring extensive subjective assessment, such as
cognitive state classification. In this work, we propose a Semi-Supervised
Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method
that leverages both acoustic and linguistic characteristics to select the most
confident data for training the classification model. Acoustically, unlabeled
data are compared to labeled data using the Frechet audio distance, calculated
from embeddings generated by multiple audio encoders. Linguistically, large
language models are prompted to revise automatic speech recognition
transcriptions and predict labels based on our proposed task-specific
knowledge. High-confidence data are identified when pseudo-labels from both
sources align, while mismatches are treated as low-confidence data. A bimodal
classifier is then trained to iteratively label the low-confidence data until a
predefined criterion is met. We evaluate our SSL framework on emotion
recognition and dementia detection tasks. Experimental results demonstrate that
our method achieves competitive performance compared to fully supervised
learning using only 30% of the labeled data and significantly outperforms two
selected baselines.",2024-09-25,"Yuanchao Li, Zixing Zhang, Jing Han, Peter Bell, Catherine Lai",http://arxiv.org/pdf/2409.16937v3,cs.CL
Investigating OCR-Sensitive Neurons to Improve Entity Recognition in Historical Documents,"This paper investigates the presence of OCR-sensitive neurons within the
Transformer architecture and their influence on named entity recognition (NER)
performance on historical documents. By analysing neuron activation patterns in
response to clean and noisy text inputs, we identify and then neutralise
OCR-sensitive neurons to improve model performance. Based on two open access
large language models (Llama2 and Mistral), experiments demonstrate the
existence of OCR-sensitive regions and show improvements in NER performance on
historical newspapers and classical commentaries, highlighting the potential of
targeted neuron modulation to improve models' performance on noisy text.",2024-09-25,"Emanuela Boros, Maud Ehrmann",http://arxiv.org/pdf/2409.16934v3,cs.CL
Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models,"Utilizing Self-Supervised Learning (SSL) models for Speech Emotion
Recognition (SER) has proven effective, yet limited research has explored
cross-lingual scenarios. This study presents a comparative analysis between
human performance and SSL models, beginning with a layer-wise analysis and an
exploration of parameter-efficient fine-tuning strategies in monolingual,
cross-lingual, and transfer learning contexts. We further compare the SER
ability of models and humans at both utterance- and segment-levels.
Additionally, we investigate the impact of dialect on cross-lingual SER through
human evaluation. Our findings reveal that models, with appropriate knowledge
transfer, can adapt to the target language and achieve performance comparable
to native speakers. We also demonstrate the significant effect of dialect on
SER for individuals without prior linguistic and paralinguistic background.
Moreover, both humans and models exhibit distinct behaviors across different
emotions. These results offer new insights into the cross-lingual SER
capabilities of SSL models, underscoring both their similarities to and
differences from human emotion perception.",2024-09-25,"Zhichen Han, Tianqi Geng, Hui Feng, Jiahong Yuan, Korin Richmond, Yuanchao Li",http://arxiv.org/pdf/2409.16920v2,cs.CL
Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness,"The increasing capability and widespread usage of large language models
(LLMs) highlight the desirability of automatic detection of LLM-generated text.
Zero-shot detectors, due to their training-free nature, have received
considerable attention and notable success. In this paper, we identify a new
feature, token cohesiveness, that is useful for zero-shot detection, and we
demonstrate that LLM-generated text tends to exhibit higher token cohesiveness
than human-written text. Based on this observation, we devise TOCSIN, a generic
dual-channel detection paradigm that uses token cohesiveness as a plug-and-play
module to improve existing zero-shot detectors. To calculate token
cohesiveness, TOCSIN only requires a few rounds of random token deletion and
semantic difference measurement, making it particularly suitable for a
practical black-box setting where the source model used for generation is not
accessible. Extensive experiments with four state-of-the-art base detectors on
various datasets, source models, and evaluation settings demonstrate the
effectiveness and generality of the proposed approach. Code available at:
\url{https://github.com/Shixuan-Ma/TOCSIN}.",2024-09-25,"Shixuan Ma, Quan Wang",http://arxiv.org/pdf/2409.16914v1,cs.CL
Pruning Multilingual Large Language Models for Multilingual Inference,"Multilingual large language models (MLLMs), trained on multilingual balanced
data, demonstrate better zero-shot learning performance in non-English
languages compared to large language models trained on English-dominant data.
However, the disparity in performance between English and non-English languages
remains a challenge yet to be fully addressed. A distinctive characteristic of
MLLMs is their high-quality translation capabilities, indicating an acquired
proficiency in aligning between languages. This study explores how to enhance
the zero-shot performance of MLLMs in non-English languages by leveraging their
alignment capability between English and non-English languages. To achieve
this, we first analyze the behavior of MLLMs when performing translation and
reveal that there are large magnitude features that play a critical role in the
translation process. Inspired by these findings, we retain the weights
associated with operations involving the large magnitude features and prune
other weights to force MLLMs to rely on these features for tasks beyond
translation. We empirically demonstrate that this pruning strategy can enhance
the MLLMs' performance in non-English language.",2024-09-25,"Hwichan Kim, Jun Suzuki, Tosho Hirasawa, Mamoru Komachi",http://arxiv.org/pdf/2409.16911v2,cs.CL
Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question Answering,"Time-Sensitive Question Answering (TSQA) demands the effective utilization of
specific temporal contexts, encompassing multiple time-evolving facts, to
address time-sensitive questions. This necessitates not only the parsing of
temporal information within questions but also the identification and
understanding of time-evolving facts to generate accurate answers. However,
current large language models still have limited sensitivity to temporal
information and their inadequate temporal reasoning capabilities. In this
paper, we propose a novel framework that enhances temporal awareness and
reasoning through Temporal Information-Aware Embedding and Granular Contrastive
Reinforcement Learning. Experimental results on four TSQA datasets demonstrate
that our framework significantly outperforms existing LLMs in TSQA tasks,
marking a step forward in bridging the performance gap between machine and
human temporal understanding and reasoning.",2024-09-25,"Wanqi Yang, Yanda Li, Meng Fang, Ling Chen",http://arxiv.org/pdf/2409.16909v2,cs.CL
A Roadmap for Embodied and Social Grounding in LLMs,"The fusion of Large Language Models (LLMs) and robotic systems has led to a
transformative paradigm in the robotic field, offering unparalleled
capabilities not only in the communication domain but also in skills like
multimodal input handling, high-level reasoning, and plan generation. The
grounding of LLMs knowledge into the empirical world has been considered a
crucial pathway to exploit the efficiency of LLMs in robotics. Nevertheless,
connecting LLMs' representations to the external world with multimodal
approaches or with robots' bodies is not enough to let them understand the
meaning of the language they are manipulating. Taking inspiration from humans,
this work draws attention to three necessary elements for an agent to grasp and
experience the world. The roadmap for LLMs grounding is envisaged in an active
bodily system as the reference point for experiencing the environment, a
temporally structured experience for a coherent, self-related interaction with
the external world, and social skills to acquire a common-grounded shared
experience.",2024-09-25,"Sara Incao, Carlo Mazzola, Giulia Belgiovine, Alessandra Sciutti",http://arxiv.org/pdf/2409.16900v1,cs.CL
Robotic Backchanneling in Online Conversation Facilitation: A Cross-Generational Study,"Japan faces many challenges related to its aging society, including
increasing rates of cognitive decline in the population and a shortage of
caregivers. Efforts have begun to explore solutions using artificial
intelligence (AI), especially socially embodied intelligent agents and robots
that can communicate with people. Yet, there has been little research on the
compatibility of these agents with older adults in various everyday situations.
To this end, we conducted a user study to evaluate a robot that functions as a
facilitator for a group conversation protocol designed to prevent cognitive
decline. We modified the robot to use backchannelling, a natural human way of
speaking, to increase receptiveness of the robot and enjoyment of the group
conversation experience. We conducted a cross-generational study with young
adults and older adults. Qualitative analyses indicated that younger adults
perceived the backchannelling version of the robot as kinder, more trustworthy,
and more acceptable than the non-backchannelling robot. Finally, we found that
the robot's backchannelling elicited nonverbal backchanneling in older
participants.",2024-09-25,"Sota Kobuki, Katie Seaborn, Seiki Tokunaga, Kosuke Fukumori, Shun Hidaka, Kazuhiro Tamura, Koji Inoue, Tatsuya Kawahara, Mihoko Otake-Mastuura",http://arxiv.org/pdf/2409.16899v1,cs.CL
Shifting from endangerment to rebirth in the Artificial Intelligence Age: An Ensemble Machine Learning Approach for Hawrami Text Classification,"Hawrami, a dialect of Kurdish, is classified as an endangered language as it
suffers from the scarcity of data and the gradual loss of its speakers. Natural
Language Processing projects can be used to partially compensate for data
availability for endangered languages/dialects through a variety of approaches,
such as machine translation, language model building, and corpora development.
Similarly, NLP projects such as text classification are in language
documentation. Several text classification studies have been conducted for
Kurdish, but they were mainly dedicated to two particular dialects: Sorani
(Central Kurdish) and Kurmanji (Northern Kurdish). In this paper, we introduce
various text classification models using a dataset of 6,854 articles in Hawrami
labeled into 15 categories by two native speakers. We use K-nearest Neighbor
(KNN), Linear Support Vector Machine (Linear SVM), Logistic Regression (LR),
and Decision Tree (DT) to evaluate how well those methods perform the
classification task. The results indicate that the Linear SVM achieves a 96% of
accuracy and outperforms the other approaches.",2024-09-25,"Aram Khaksar, Hossein Hassani",http://arxiv.org/pdf/2409.16884v1,cs.CL
The Role of Language Models in Modern Healthcare: A Comprehensive Review,"The application of large language models (LLMs) in healthcare has gained
significant attention due to their ability to process complex medical data and
provide insights for clinical decision-making. These models have demonstrated
substantial capabilities in understanding and generating natural language,
which is crucial for medical documentation, diagnostics, and patient
interaction. This review examines the trajectory of language models from their
early stages to the current state-of-the-art LLMs, highlighting their strengths
in healthcare applications and discussing challenges such as data privacy,
bias, and ethical considerations. The potential of LLMs to enhance healthcare
delivery is explored, alongside the necessary steps to ensure their ethical and
effective integration into medical practice.",2024-09-25,"Amna Khalid, Ayma Khalid, Umar Khalid",http://arxiv.org/pdf/2409.16860v1,cs.CL
Exposing Assumptions in AI Benchmarks through Cognitive Modelling,"Cultural AI benchmarks often rely on implicit assumptions about measured
constructs, leading to vague formulations with poor validity and unclear
interrelations. We propose exposing these assumptions using explicit cognitive
models formulated as Structural Equation Models. Using cross-lingual alignment
transfer as an example, we show how this approach can answer key research
questions and identify missing datasets. This framework grounds benchmark
construction theoretically and guides dataset development to improve construct
measurement. By embracing transparency, we move towards more rigorous,
cumulative AI evaluation science, challenging researchers to critically examine
their assessment foundations.",2024-09-25,"Jonathan H. Rystrøm, Kenneth C. Enevoldsen",http://arxiv.org/pdf/2409.16849v1,cs.CL
CodeInsight: A Curated Dataset of Practical Coding Solutions from Stack Overflow,"We introduce a novel dataset tailored for code generation, aimed at aiding
developers in common tasks. Our dataset provides examples that include a
clarified intent, code snippets associated, and an average of three related
unit tests. It encompasses a range of libraries such as \texttt{Pandas},
\texttt{Numpy}, and \texttt{Regex}, along with more than 70 standard libraries
in Python code derived from Stack Overflow. Comprising 3,409 crafted examples
by Python experts, our dataset is designed for both model finetuning and
standalone evaluation. To complete unit tests evaluation, we categorize
examples in order to get more fine grained analysis, enhancing the
understanding of models' strengths and weaknesses in specific coding tasks. The
examples have been refined to reduce data contamination, a process confirmed by
the performance of three leading models: Mistral 7B, CodeLLaMa 13B, and
Starcoder 15B. We further investigate data-contamination testing GPT-4
performance on a part of our dataset. The benchmark can be accessed at
\url{https://github.com/NathanaelBeau/CodeInsight}.",2024-09-25,"Nathanaël Beau, Benoît Crabbé",http://arxiv.org/pdf/2409.16819v1,cs.CL
A Few Hypocrites: Few-Shot Learning and Subtype Definitions for Detecting Hypocrisy Accusations in Online Climate Change Debates,"The climate crisis is a salient issue in online discussions, and hypocrisy
accusations are a central rhetorical element in these debates. However, for
large-scale text analysis, hypocrisy accusation detection is an understudied
tool, most often defined as a smaller subtask of fallacious argument detection.
In this paper, we define hypocrisy accusation detection as an independent task
in NLP, and identify different relevant subtypes of hypocrisy accusations. Our
Climate Hypocrisy Accusation Corpus (CHAC) consists of 420 Reddit climate
debate comments, expert-annotated into two different types of hypocrisy
accusations: personal versus political hypocrisy. We evaluate few-shot
in-context learning with 6 shots and 3 instruction-tuned Large Language Models
(LLMs) for detecting hypocrisy accusations in this dataset. Results indicate
that the GPT-4o and Llama-3 models in particular show promise in detecting
hypocrisy accusations (F1 reaching 0.68, while previous work shows F1 of 0.44).
However, context matters for a complex semantic concept such as hypocrisy
accusations, and we find models struggle especially at identifying political
hypocrisy accusations compared to personal moral hypocrisy. Our study
contributes new insights in hypocrisy detection and climate change discourse,
and is a stepping stone for large-scale analysis of hypocrisy accusation in
online climate debates.",2024-09-25,"Paulina Garcia Corral, Avishai Green, Hendrik Meyer, Anke Stoll, Xiaoyue Yan, Myrthe Reuver",http://arxiv.org/pdf/2409.16807v1,cs.CL
Mitigating the Bias of Large Language Model Evaluation,"Recently, there has been a trend of evaluating the Large Language Model (LLM)
quality in the flavor of LLM-as-a-Judge, namely leveraging another LLM to
evaluate the current output quality. However, existing judges are proven to be
biased, namely they would favor answers which present better superficial
quality (such as verbosity, fluency) while ignoring the instruction following
ability. In this work, we propose systematic research about the bias of
LLM-as-a-Judge. Specifically, for closed-source judge models, we apply
calibration to mitigate the significance of superficial quality, both on
probability level and prompt level. For open-source judge models, we propose to
mitigate the bias by contrastive training, with curated negative samples that
deviate from instruction but present better superficial quality. We apply our
methods on the bias evaluation benchmark, and experiment results show our
methods mitigate the bias by a large margin while maintaining a satisfactory
evaluation accuracy.",2024-09-25,"Hongli Zhou, Hui Huang, Yunfei Long, Bing Xu, Conghui Zhu, Hailong Cao, Muyun Yang, Tiejun Zhao",http://arxiv.org/pdf/2409.16788v1,cs.CL
Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction,"Automated red teaming is an effective method for identifying misaligned
behaviors in large language models (LLMs). Existing approaches, however, often
focus primarily on improving attack success rates while overlooking the need
for comprehensive test case coverage. Additionally, most of these methods are
limited to single-turn red teaming, failing to capture the multi-turn dynamics
of real-world human-machine interactions. To overcome these limitations, we
propose HARM (Holistic Automated Red teaMing), which scales up the diversity of
test cases using a top-down approach based on an extensible, fine-grained risk
taxonomy. Our method also leverages a novel fine-tuning strategy and
reinforcement learning techniques to facilitate multi-turn adversarial probing
in a human-like manner. Experimental results demonstrate that our framework
enables a more systematic understanding of model vulnerabilities and offers
more targeted guidance for the alignment process.",2024-09-25,"Jinchuan Zhang, Yan Zhou, Yaxin Liu, Ziming Li, Songlin Hu",http://arxiv.org/pdf/2409.16783v1,cs.CL
E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL,"Translating Natural Language Queries into Structured Query Language
(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the
natural language processing and database communities, aimed at providing a
natural language interface to databases (NLIDB) and lowering the barrier for
non-experts. Despite recent advancements made through the use of Large Language
Models (LLMs), significant challenges remain. These include handling complex
database schemas, resolving ambiguity in user queries, and generating SQL
queries with intricate structures that accurately reflect the user's intent. In
this work, we introduce E-SQL, a novel pipeline specifically designed to
address these challenges through direct schema linking and candidate predicate
augmentation. E-SQL enhances the natural language query by incorporating
relevant database items (i.e., tables, columns, and values) and conditions
directly into the question and SQL construction plan, bridging the gap between
the query and the database structure. The pipeline leverages candidate
predicate augmentation to mitigate erroneous or incomplete predicates in
generated SQLs. Comprehensive evaluations on the BIRD benchmark illustrate that
E-SQL achieves competitive performance, particularly excelling in complex
queries with a 66.29% execution accuracy on the test set. A further observation
from our experiments reveals that incorporating schema filtering into the
translation pipeline does not have a positive impact on performance when the
most advanced proprietary LLMs are used. Additionally, our experiments with
small LLMs highlight the importance and positive impact of enriched questions
on their performance. Without fine-tuning, single-prompt SQL generation using
enriched questions with DeepSeek Coder 7B Instruct 1.5v achieves 56.45%
execution accuracy on the BIRD development set.",2024-09-25,"Hasan Alp Caferoğlu, Özgür Ulusoy",http://arxiv.org/pdf/2409.16751v2,cs.CL
RoleBreak: Character Hallucination as a Jailbreak Attack in Role-Playing Systems,"Role-playing systems powered by large language models (LLMs) have become
increasingly influential in emotional communication applications. However,
these systems are susceptible to character hallucinations, where the model
deviates from predefined character roles and generates responses that are
inconsistent with the intended persona. This paper presents the first
systematic analysis of character hallucination from an attack perspective,
introducing the RoleBreak framework. Our framework identifies two core
mechanisms-query sparsity and role-query conflict-as key factors driving
character hallucination. Leveraging these insights, we construct a novel
dataset, RoleBreakEval, to evaluate existing hallucination mitigation
techniques. Our experiments reveal that even enhanced models trained to
minimize hallucination remain vulnerable to attacks. To address these
vulnerabilities, we propose a novel defence strategy, the Narrator Mode, which
generates supplemental context through narration to mitigate role-query
conflicts and improve query generalization. Experimental results demonstrate
that Narrator Mode significantly outperforms traditional refusal-based
strategies by reducing hallucinations, enhancing fidelity to character roles
and queries, and improving overall narrative coherence.",2024-09-25,"Yihong Tang, Bo Wang, Xu Wang, Dongming Zhao, Jing Liu, Jijun Zhang, Ruifang He, Yuexian Hou",http://arxiv.org/pdf/2409.16727v1,cs.CL
PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning,"Low-rank adaptation (LoRA) and its variants have recently gained much
interest due to their ability to avoid excessive inference costs. However, LoRA
still encounters the following challenges: (1) Limitation of low-rank
assumption; and (2) Its initialization method may be suboptimal. To this end,
we propose PMSS(Pre-trained Matrices Skeleton Selection), which enables
high-rank updates with low costs while leveraging semantic and linguistic
information inherent in pre-trained weight. It achieves this by selecting
skeletons from the pre-trained weight matrix and only learning a small matrix
instead. Experiments demonstrate that PMSS outperforms LoRA and other
fine-tuning methods across tasks with much less trainable parameters. We
demonstrate its effectiveness, especially in handling complex tasks such as
DROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math
reasoning(+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of
GSM8K). The code and model will be released soon.",2024-09-25,"Qibin Wang, Xiaolin Hu, Weikai Xu, Wei Liu, Jian Luan, Bin Wang",http://arxiv.org/pdf/2409.16722v1,cs.CL
Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification,"Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed
the success of prompt tuning and adapter tuning, while the classic model
fine-tuning on inherent parameters seems to be overlooked. It is believed that
fine-tuning the parameters of VLMs with few-shot samples corrupts the
pre-trained knowledge since fine-tuning the CLIP model even degrades
performance. In this paper, we revisit this viewpoint, and propose a new
perspective: fine-tuning the specific parameters instead of all will uncover
the power of classic model fine-tuning on VLMs. Through our meticulous study,
we propose ClipFit, a simple yet effective method to fine-tune CLIP without
introducing any overhead of extra parameters. We demonstrate that by only
fine-tuning the specific bias terms and normalization layers, ClipFit can
improve the performance of zero-shot CLIP by 7.27\% average harmonic mean
accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the
pre-trained models, we conducted extensive experimental analyses w.r.t. changes
in internal parameters and representations. We found that low-level text bias
layers and the first layer normalization layer change much more than other
layers. The code is available at \url{https://github.com/minglllli/CLIPFit}.",2024-09-25,"Ming Li, Jike Zhong, Chenxin Li, Liuzhuozheng Li, Nie Lin, Masashi Sugiyama",http://arxiv.org/pdf/2409.16718v2,cs.CL
Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?,"In the post-Turing era, evaluating large language models (LLMs) involves
assessing generated text based on readers' reactions rather than merely its
indistinguishability from human-produced content. This paper explores how
LLM-generated text impacts readers' decisions, focusing on both amateur and
expert audiences. Our findings indicate that GPT-4 can generate persuasive
analyses affecting the decisions of both amateurs and professionals.
Furthermore, we evaluate the generated text from the aspects of grammar,
convincingness, logical coherence, and usefulness. The results highlight a high
correlation between real-world evaluation through audience reactions and the
current multi-dimensional evaluators commonly used for generative models.
Overall, this paper shows the potential and risk of using generated text to
sway human decisions and also points out a new direction for evaluating
generated text, i.e., leveraging the reactions and decisions of readers. We
release our dataset to assist future research.",2024-09-25,"Takehiro Takayanagi, Hiroya Takamura, Kiyoshi Izumi, Chung-Chi Chen",http://arxiv.org/pdf/2409.16710v2,cs.CL
Probing Omissions and Distortions in Transformer-based RDF-to-Text Models,"In Natural Language Generation (NLG), important information is sometimes
omitted in the output text. To better understand and analyse how this type of
mistake arises, we focus on RDF-to-Text generation and explore two methods of
probing omissions in the encoder output of BART (Lewis et al, 2020) and of T5
(Raffel et al, 2019): (i) a novel parameter-free probing method based on the
computation of cosine similarity between embeddings of RDF graphs and of RDF
graphs in which we removed some entities and (ii) a parametric probe which
performs binary classification on the encoder embeddings to detect omitted
entities. We also extend our analysis to distorted entities, i.e. entities that
are not fully correctly mentioned in the generated text (e.g. misspelling of
entity, wrong units of measurement). We found that both omitted and distorted
entities can be probed in the encoder's output embeddings. This suggests that
the encoder emits a weaker signal for these entities and therefore is
responsible for some loss of information. This also shows that probing methods
can be used to detect mistakes in the output of NLG models.",2024-09-25,"Juliette Faille, Albert Gatt, Claire Gardent",http://arxiv.org/pdf/2409.16707v1,cs.CL
"A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms","Large language models (LLMs) have achieved remarkable advancements in natural
language processing, showcasing exceptional performance across various tasks.
However, the expensive memory and computational requirements present
significant challenges for their practical deployment. Low-bit quantization has
emerged as a critical approach to mitigate these challenges by reducing the
bit-width of model parameters, activations, and gradients, thus decreasing
memory usage and computational demands. This paper presents a comprehensive
survey of low-bit quantization methods tailored for LLMs, covering the
fundamental principles, system implementations, and algorithmic strategies. An
overview of basic concepts and new data formats specific to low-bit LLMs is
first introduced, followed by a review of frameworks and systems that
facilitate low-bit LLMs across various hardware platforms. Then, we categorize
and analyze techniques and toolkits for efficient low-bit training and
inference of LLMs. Finally, we conclude with a discussion of future trends and
potential advancements of low-bit LLMs. Our systematic overview from basic,
system, and algorithm perspectives can offer valuable insights and guidelines
for future works to enhance the efficiency and applicability of LLMs through
low-bit quantization.",2024-09-25,"Ruihao Gong, Yifu Ding, Zining Wang, Chengtao Lv, Xingyu Zheng, Jinyang Du, Haotong Qin, Jinyang Guo, Michele Magno, Xianglong Liu",http://arxiv.org/pdf/2409.16694v2,cs.CL
MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making,"Long-term memory is significant for agents, in which insights play a crucial
role. However, the emergence of irrelevant insight and the lack of general
insight can greatly undermine the effectiveness of insight. To solve this
problem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an
embodied agent designed to improve LLMs' planning and decision-making ability
by summarizing and utilizing insight effectively across different scales. MSI
achieves this through the experience selector, insight generator, and insight
selector. Leveraging a three-part pipeline, MSI can generate task-specific and
high-level insight, store it in a database, and then use relevant insight from
it to aid in decision-making. Our experiments show that MSI outperforms another
insight strategy when planning by GPT3.5. Moreover, We delve into the
strategies for selecting seed experience and insight, aiming to provide LLM
with more useful and relevant insight for better decision-making. Our
observations also indicate that MSI exhibits better robustness when facing
domain-shifting scenarios.",2024-09-25,"Dayuan Fu, Biqing Qi, Yihuai Gao, Che Jiang, Guanting Dong, Bowen Zhou",http://arxiv.org/pdf/2409.16686v2,cs.CL
SynTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA,"Text-to-SQL parsing and end-to-end question answering (E2E TQA) are two main
approaches for Table-based Question Answering task. Despite success on multiple
benchmarks, they have yet to be compared and their synergy remains unexplored.
In this paper, we identify different strengths and weaknesses through
evaluating state-of-the-art models on benchmark datasets: Text-to-SQL
demonstrates superiority in handling questions involving arithmetic operations
and long tables; E2E TQA excels in addressing ambiguous questions, non-standard
table schema, and complex table contents. To combine both strengths, we propose
a Synergistic Table-based Question Answering approach that integrate different
models via answer selection, which is agnostic to any model types. Further
experiments validate that ensembling models by either feature-based or
LLM-based answer selector significantly improves the performance over
individual models.",2024-09-25,"Siyue Zhang, Anh Tuan Luu, Chen Zhao",http://arxiv.org/pdf/2409.16682v2,cs.CL
Emotional Dimension Control in Language Model-Based Text-to-Speech: Spanning a Broad Spectrum of Human Emotions,"Current emotional text-to-speech systems face challenges in conveying the
full spectrum of human emotions, largely due to the inherent complexity of
human emotions and the limited range of emotional labels in existing speech
datasets. To address these limitations, this paper introduces a TTS framework
that provides flexible user control over three emotional dimensions - pleasure,
arousal, and dominance - enabling the synthesis of a diverse array of emotional
styles. The framework leverages an emotional dimension predictor, trained soley
on categorical labels from speech data and grounded in earlier psychological
research, which is seamlessly integrated into a language model-based TTS
system. Experimental results demonstrates that the proposed framework
effectively learns emotional styles from expressive speech, eliminating the
need for explicit emotion labels during TTS training, while enhancing the
naturalness and diversity of synthesized emotional speech.",2024-09-25,"Kun Zhou, You Zhang, Shengkui Zhao, Hao Wang, Zexu Pan, Dianwen Ng, Chong Zhang, Chongjia Ni, Yukun Ma, Trung Hieu Nguyen, Jia Qi Yip, Bin Ma",http://arxiv.org/pdf/2409.16681v2,cs.CL
SWE2: SubWord Enriched and Significant Word Emphasized Framework for Hate Speech Detection,"Hate speech detection on online social networks has become one of the
emerging hot topics in recent years. With the broad spread and fast propagation
speed across online social networks, hate speech makes significant impacts on
society by increasing prejudice and hurting people. Therefore, there are
aroused attention and concern from both industry and academia. In this paper,
we address the hate speech problem and propose a novel hate speech detection
framework called SWE2, which only relies on the content of messages and
automatically identifies hate speech. In particular, our framework exploits
both word-level semantic information and sub-word knowledge. It is intuitively
persuasive and also practically performs well under a situation with/without
character-level adversarial attack. Experimental results show that our proposed
model achieves 0.975 accuracy and 0.953 macro F1, outperforming 7
state-of-the-art baselines under no adversarial attack. Our model robustly and
significantly performed well under extreme adversarial attack (manipulation of
50% messages), achieving 0.967 accuracy and 0.934 macro F1.",2024-09-25,"Guanyi Mou, Pengyi Ye, Kyumin Lee",http://arxiv.org/pdf/2409.16673v1,cs.CL
DiaSynth: Synthetic Dialogue Generation Framework for Low Resource Dialogue Applications,"The scarcity of domain-specific dialogue datasets limits the development of
dialogue systems across applications. Existing research is constrained by
general or niche datasets that lack sufficient scale for training dialogue
systems. To address this gap, we introduce DiaSynth - a synthetic dialogue
generation framework capable of generating high-quality, contextually rich
dialogues across a wide range of domains. Unlike existing frameworks, DiaSynth
uses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to
generate dynamic, domain-specific dialogues with simulated personas and diverse
conversational features. We perform our experiments by generating synthetic
data using different LLMs and few-shot examples from DialogSum and SAMSum. The
pretrained language models fine-tuned on the synthetic data outperform the base
models by 16.47% on dialogue summarization, while the comparison between models
fine-tuned on in-domain data and synthetic data shows that the synthetic data
is able to capture 90.48% of the performance distribution of the in-domain data
on dialogue summarization. The quality of the data generated also increases as
we increase the size of LLM from 3B to 8B. These results validate DiaSynth's
potential as a robust alternative to traditional data collection methods. We
open source the code and data generated for future research.",2024-09-25,"Sathya Krishnan Suresh, Wu Mengjun, Tushar Pranav, Eng Siong Chng",http://arxiv.org/pdf/2409.19020v3,cs.CL
"An Effective, Robust and Fairness-aware Hate Speech Detection Framework","With the widespread online social networks, hate speeches are spreading
faster and causing more damage than ever before. Existing hate speech detection
methods have limitations in several aspects, such as handling data
insufficiency, estimating model uncertainty, improving robustness against
malicious attacks, and handling unintended bias (i.e., fairness). There is an
urgent need for accurate, robust, and fair hate speech classification in online
social networks. To bridge the gap, we design a data-augmented, fairness
addressed, and uncertainty estimated novel framework. As parts of the
framework, we propose Bidirectional Quaternion-Quasi-LSTM layers to balance
effectiveness and efficiency. To build a generalized model, we combine five
datasets collected from three platforms. Experiment results show that our model
outperforms eight state-of-the-art methods under both no attack scenario and
various attack scenarios, indicating the effectiveness and robustness of our
model. We share our code along with combined dataset for better future research",2024-09-25,"Guanyi Mou, Kyumin Lee",http://arxiv.org/pdf/2409.17191v1,cs.CL
Topic-aware Causal Intervention for Counterfactual Detection,"Counterfactual statements, which describe events that did not or cannot take
place, are beneficial to numerous NLP applications. Hence, we consider the
problem of counterfactual detection (CFD) and seek to enhance the CFD models.
Previous models are reliant on clue phrases to predict counterfactuality, so
they suffer from significant performance drop when clue phrase hints do not
exist during testing. Moreover, these models tend to predict
non-counterfactuals over counterfactuals. To address these issues, we propose
to integrate neural topic model into the CFD model to capture the global
semantics of the input statement. We continue to causally intervene the hidden
representations of the CFD model to balance the effect of the class labels.
Extensive experiments show that our approach outperforms previous
state-of-the-art CFD and bias-resolving methods in both the CFD and other
bias-sensitive tasks.",2024-09-25,"Thong Nguyen, Truc-My Nguyen",http://arxiv.org/pdf/2409.16668v2,cs.CL
A Character-Centric Creative Story Generation via Imagination,"Creative story generation has long been a goal of NLP research. While
existing methodologies have aimed to generate long and coherent stories, they
fall significantly short of human capabilities in terms of diversity and
character depth. To address this, we introduce a novel story generation
framework called CCI (Character-centric Creative story generation via
Imagination). CCI features two modules for creative story generation: IG
(Image-Guided Imagination) and MW (Multi-Writer model). In the IG module, we
utilize a text-to-image model to create visual representations of key story
elements, such as characters, backgrounds, and main plots, in a more novel and
concrete manner than text-only approaches. The MW module uses these story
elements to generate multiple persona-description candidates and selects the
best one to insert into the story, thereby enhancing the richness and depth of
the narrative. We compared the stories generated by CCI and baseline models
through statistical analysis, as well as human and LLM evaluations. The results
showed that the IG and MW modules significantly improve various aspects of the
stories' creativity. Furthermore, our framework enables interactive multi-modal
story generation with users, opening up new possibilities for human-LLM
integration in cultural development. Project page : https://www.2024cci.p-e.kr/",2024-09-25,"Kyeongman Park, Minbeom Kim, Kyomin Jung",http://arxiv.org/pdf/2409.16667v3,cs.CL
Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts,"In this work, we show the pre-trained language models return distinguishable
generation probability and uncertainty distribution to unfaithfully
hallucinated texts, regardless of their size and structure. By examining 24
models on 6 data sets, we find out that 88-98% of cases return statistically
significantly distinguishable generation probability and uncertainty
distributions. Using this general phenomenon, we showcase a
hallucination-reducing training algorithm. Our algorithm outperforms other
baselines by achieving higher faithfulness metrics while maintaining sound
general text quality measures.",2024-09-25,"Taehun Cha, Donghun Lee",http://arxiv.org/pdf/2409.16658v1,cs.CL
Speech Recognition Rescoring with Large Speech-Text Foundation Models,"Large language models (LLM) have demonstrated the ability to understand human
language by leveraging large amount of text data. Automatic speech recognition
(ASR) systems are often limited by available transcribed speech data and
benefit from a second pass rescoring using LLM. Recently multi-modal large
language models, particularly speech and text foundational models have
demonstrated strong spoken language understanding. Speech-Text foundational
models leverage large amounts of unlabelled and labelled data both in speech
and text modalities to model human language. In this work, we propose novel
techniques to use multi-modal LLM for ASR rescoring. We also explore
discriminative training to further improve the foundational model rescoring
performance. We demonstrate cross-modal knowledge transfer in speech-text LLM
can benefit rescoring. Our experiments demonstrate up-to 20% relative
improvements over Whisper large ASR and up-to 15% relative improvements over
text-only LLM.",2024-09-25,"Prashanth Gurunath Shivakumar, Jari Kolehmainen, Aditya Gourav, Yi Gu, Ankur Gandhe, Ariya Rastrow, Ivan Bulyko",http://arxiv.org/pdf/2409.16654v1,cs.CL
Domain-Independent Automatic Generation of Descriptive Texts for Time-Series Data,"Due to scarcity of time-series data annotated with descriptive texts,
training a model to generate descriptive texts for time-series data is
challenging. In this study, we propose a method to systematically generate
domain-independent descriptive texts from time-series data. We identify two
distinct approaches for creating pairs of time-series data and descriptive
texts: the forward approach and the backward approach. By implementing the
novel backward approach, we create the Temporal Automated Captions for
Observations (TACO) dataset. Experimental results demonstrate that a
contrastive learning based model trained using the TACO dataset is capable of
generating descriptive texts for time-series data in novel domains.",2024-09-25,"Kota Dohi, Aoi Ito, Harsh Purohit, Tomoya Nishida, Takashi Endo, Yohei Kawaguchi",http://arxiv.org/pdf/2409.16647v1,cs.CL
Cross-Lingual and Cross-Cultural Variation in Image Descriptions,"Do speakers of different languages talk differently about what they see?
Behavioural and cognitive studies report cultural effects on perception;
however, these are mostly limited in scope and hard to replicate. In this work,
we conduct the first large-scale empirical study of cross-lingual variation in
image descriptions. Using a multimodal dataset with 31 languages and images
from diverse locations, we develop a method to accurately identify entities
mentioned in captions and present in the images, then measure how they vary
across languages. Our analysis reveals that pairs of languages that are
geographically or genetically closer tend to mention the same entities more
frequently. We also identify entity categories whose saliency is universally
high (such as animate beings), low (clothing accessories) or displaying high
variance across languages (landscape). In a case study, we measure the
differences in a specific language pair (e.g., Japanese mentions clothing far
more frequently than English). Furthermore, our method corroborates previous
small-scale studies, including 1) Rosch et al. (1976)'s theory of basic-level
categories, demonstrating a preference for entities that are neither too
generic nor too specific, and 2) Miyamoto et al. (2006)'s hypothesis that
environments afford patterns of perception, such as entity counts. Overall, our
work reveals the presence of both universal and culture-specific patterns in
entity mentions.",2024-09-25,"Uri Berger, Edoardo M. Ponti",http://arxiv.org/pdf/2409.16646v3,cs.CL
Enabling Auditory Large Language Models for Automatic Speech Quality Evaluation,"Speech quality assessment typically requires evaluating audio from multiple
aspects, such as mean opinion score (MOS) and speaker similarity (SIM) \etc.,
which can be challenging to cover using one small model designed for a single
task. In this paper, we propose leveraging recently introduced auditory large
language models (LLMs) for automatic speech quality assessment. By employing
task-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B
testing results, which are commonly used for evaluating text-to-speech systems.
Additionally, the finetuned auditory LLM is able to generate natural language
descriptions assessing aspects like noisiness, distortion, discontinuity, and
overall quality, providing more interpretable outputs. Extensive experiments
have been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality
datasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and
Qwen2-Audio. For the natural language descriptions task, a commercial model
Google Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory
LLMs achieve competitive performance compared to state-of-the-art task-specific
small models in predicting MOS and SIM, while also delivering promising results
in A/B testing and natural language descriptions. Our data processing scripts
and finetuned model checkpoints can be found at
https://github.com/bytedance/SALMONN.",2024-09-25,"Siyin Wang, Wenyi Yu, Yudong Yang, Changli Tang, Yixuan Li, Jimin Zhuang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang",http://arxiv.org/pdf/2409.16644v3,cs.CL
Training Language Models to Win Debates with Self-Play Improves Judge Accuracy,"We test the robustness of debate as a method of scalable oversight by
training models to debate with data generated via self-play. In a long-context
reading comprehension task, we find that language model based evaluators answer
questions more accurately when judging models optimized to win debates. By
contrast, we find no such relationship for consultancy models trained to
persuade a judge without an opposing debater present. In quantitative and
qualitative comparisons between our debate models and novel consultancy
baselines, we find evidence that debate training encourages stronger and more
informative arguments, showing promise that it can help provide high-quality
supervision for tasks that are difficult to directly evaluate.",2024-09-25,"Samuel Arnesen, David Rein, Julian Michael",http://arxiv.org/pdf/2409.16636v1,cs.CL
Claim-Guided Textual Backdoor Attack for Practical Applications,"Recent advances in natural language processing and the increased use of large
language models have exposed new security vulnerabilities, such as backdoor
attacks. Previous backdoor attacks require input manipulation after model
distribution to activate the backdoor, posing limitations in real-world
applicability. Addressing this gap, we introduce a novel Claim-Guided Backdoor
Attack (CGBA), which eliminates the need for such manipulations by utilizing
inherent textual claims as triggers. CGBA leverages claim extraction,
clustering, and targeted training to trick models to misbehave on targeted
claims without affecting their performance on clean data. CGBA demonstrates its
effectiveness and stealthiness across various datasets and models,
significantly enhancing the feasibility of practical backdoor attacks. Our code
and data will be available at https://github.com/PaperCGBA/CGBA.",2024-09-25,"Minkyoo Song, Hanna Kim, Jaehan Kim, Youngjin Jin, Seungwon Shin",http://arxiv.org/pdf/2409.16618v1,cs.CL
Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications,"Recent studies have evaluated the creativity/novelty of large language models
(LLMs) primarily from a semantic perspective, using benchmarks from cognitive
science. However, accessing the novelty in scholarly publications is a largely
unexplored area in evaluating LLMs. In this paper, we introduce a scholarly
novelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in
scholarly papers. SchNovel consists of 15000 pairs of papers across six fields
sampled from the arXiv dataset with publication dates spanning 2 to 10 years
apart. In each pair, the more recently published paper is assumed to be more
novel. Additionally, we propose RAG-Novelty, which simulates the review process
taken by human reviewers by leveraging the retrieval of similar papers to
assess novelty. Extensive experiments provide insights into the capabilities of
different LLMs to assess novelty and demonstrate that RAG-Novelty outperforms
recent baseline models.",2024-09-25,"Ethan Lin, Zhiyuan Peng, Yi Fang",http://arxiv.org/pdf/2409.16605v1,cs.CL
"Overview of the First Shared Task on Clinical Text Generation: RRG24 and ""Discharge Me!""","Recent developments in natural language generation have tremendous
implications for healthcare. For instance, state-of-the-art systems could
automate the generation of sections in clinical reports to alleviate physician
workload and streamline hospital documentation. To explore these applications,
we present a shared task consisting of two subtasks: (1) Radiology Report
Generation (RRG24) and (2) Discharge Summary Generation (""Discharge Me!"").
RRG24 involves generating the 'Findings' and 'Impression' sections of radiology
reports given chest X-rays. ""Discharge Me!"" involves generating the 'Brief
Hospital Course' and 'Discharge Instructions' sections of discharge summaries
for patients admitted through the emergency department. ""Discharge Me!""
submissions were subsequently reviewed by a team of clinicians. Both tasks
emphasize the goal of reducing clinician burnout and repetitive workloads by
generating documentation. We received 201 submissions from across 8 teams for
RRG24, and 211 submissions from across 16 teams for ""Discharge Me!"".",2024-09-25,"Justin Xu, Zhihong Chen, Andrew Johnston, Louis Blankemeier, Maya Varma, Jason Hom, William J. Collins, Ankit Modi, Robert Lloyd, Benjamin Hopkins, Curtis Langlotz, Jean-Benoit Delbrouck",http://arxiv.org/pdf/2409.16603v1,cs.CL
Disentangling Questions from Query Generation for Task-Adaptive Retrieval,"This paper studies the problem of information retrieval, to adapt to unseen
tasks. Existing work generates synthetic queries from domain-specific documents
to jointly train the retriever. However, the conventional query generator
assumes the query as a question, thus failing to accommodate general search
intents. A more lenient approach incorporates task-adaptive elements, such as
few-shot learning with an 137B LLM. In this paper, we challenge a trend
equating query and question, and instead conceptualize query generation task as
a ""compilation"" of high-level intent into task-adaptive query. Specifically, we
propose EGG, a query generator that better adapts to wide search intents
expressed in the BeIR benchmark. Our method outperforms baselines and existing
models on four tasks with underexplored intents, while utilizing a query
generator 47 times smaller than the previous state-of-the-art. Our findings
reveal that instructing the LM with explicit search intent is a key aspect of
modeling an effective query generator.",2024-09-25,"Yoonsang Lee, Minsoo Kim, Seung-won Hwang",http://arxiv.org/pdf/2409.16570v1,cs.CL
Understanding the Cognitive Complexity in Language Elicited by Product Images,"Product images (e.g., a phone) can be used to elicit a diverse set of
consumer-reported features expressed through language, including surface-level
perceptual attributes (e.g., ""white"") and more complex ones, like perceived
utility (e.g., ""battery""). The cognitive complexity of elicited language
reveals the nature of cognitive processes and the context required to
understand them; cognitive complexity also predicts consumers' subsequent
choices. This work offers an approach for measuring and validating the
cognitive complexity of human language elicited by product images, providing a
tool for understanding the cognitive processes of human as well as virtual
respondents simulated by Large Language Models (LLMs). We also introduce a
large dataset that includes diverse descriptive labels for product images,
including human-rated complexity. We demonstrate that human-rated cognitive
complexity can be approximated using a set of natural language models that,
combined, roughly capture the complexity construct. Moreover, this approach is
minimally supervised and scalable, even in use cases with limited human
assessment of complexity.",2024-09-25,"Yan-Ying Chen, Shabnam Hakimi, Monica Van, Francine Chen, Matthew Hong, Matt Klenk, Charlene Wu",http://arxiv.org/pdf/2409.16521v1,cs.CL
RAGProbe: An Automated Approach for Evaluating RAG Applications,"Retrieval Augmented Generation (RAG) is increasingly being used when building
Generative AI applications. Evaluating these applications and RAG pipelines is
mostly done manually, via a trial and error process. Automating evaluation of
RAG pipelines requires overcoming challenges such as context misunderstanding,
wrong format, incorrect specificity, and missing content. Prior works therefore
focused on improving evaluation metrics as well as enhancing components within
the pipeline using available question and answer datasets. However, they have
not focused on 1) providing a schema for capturing different types of
question-answer pairs or 2) creating a set of templates for generating
question-answer pairs that can support automation of RAG pipeline evaluation.
In this paper, we present a technique for generating variations in
question-answer pairs to trigger failures in RAG pipelines. We validate 5
open-source RAG pipelines using 3 datasets. Our approach revealed the highest
failure rates when prompts combine multiple questions: 91% for questions when
spanning multiple documents and 78% for questions from a single document;
indicating a need for developers to prioritise handling these combined
questions. 60% failure rate was observed in academic domain dataset and 53% and
62% failure rates were observed in open-domain datasets. Our automated approach
outperforms the existing state-of-the-art methods, by increasing the failure
rate by 51% on average per dataset. Our work presents an automated approach for
continuously monitoring the health of RAG pipelines, which can be integrated
into existing CI/CD pipelines, allowing for improved quality.",2024-09-24,"Shangeetha Sivasothy, Scott Barnett, Stefanus Kurniawan, Zafaryab Rasool, Rajesh Vasa",http://arxiv.org/pdf/2409.19019v1,cs.CL
A Unified Hallucination Mitigation Framework for Large Vision-Language Models,"Hallucination is a common problem for Large Vision-Language Models (LVLMs)
with long generations which is difficult to eradicate. The generation with
hallucinations is partially inconsistent with the image content. To mitigate
hallucination, current studies either focus on the process of model inference
or the results of model generation, but the solutions they design sometimes do
not deal appropriately with various types of queries and the hallucinations of
the generations about these queries. To accurately deal with various
hallucinations, we present a unified framework, Dentist, for hallucination
mitigation. The core step is to first classify the queries, then perform
different processes of hallucination mitigation based on the classification
result, just like a dentist first observes the teeth and then makes a plan. In
a simple deployment, Dentist can classify queries as perception or reasoning
and easily mitigate potential hallucinations in answers which has been
demonstrated in our experiments. On MMbench, we achieve a 13.44%/10.2%/15.8%
improvement in accuracy on Image Quality, a Coarse Perception visual question
answering (VQA) task, over the baseline InstructBLIP/LLaVA/VisualGLM.",2024-09-24,"Yue Chang, Liqiang Jing, Xiaopeng Zhang, Yue Zhang",http://arxiv.org/pdf/2409.16494v1,cs.CL
Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs,"Recent advances in large language models (LLMs) have led to the development
of artificial intelligence (AI)-powered tutoring chatbots, showing promise in
providing broad access to high-quality personalized education. Existing works
have studied how to make LLMs follow tutoring principles, but have not studied
broader uses of LLMs for supporting tutoring. Up until now, tracing student
knowledge and analyzing misconceptions has been difficult and time-consuming to
implement for open-ended dialogue tutoring. In this work, we investigate
whether LLMs can be supportive of this task: we first use LLM prompting methods
to identify the knowledge components/skills involved in each dialogue turn,
i.e., a tutor utterance posing a task or a student utterance that responds to
it. We also evaluate whether the student responds correctly to the tutor and
verify the LLM's accuracy using human expert annotations. We then apply a range
of knowledge tracing (KT) methods on the resulting labeled data to track
student knowledge levels over an entire dialogue. We conduct experiments on two
tutoring dialogue datasets, and show that a novel yet simple LLM-based method,
LLMKT, significantly outperforms existing KT methods in predicting student
response correctness in dialogues. We perform extensive qualitative analyses to
highlight the challenges in dialogueKT and outline multiple avenues for future
work.",2024-09-24,"Alexander Scarlatos, Ryan S. Baker, Andrew Lan",http://arxiv.org/pdf/2409.16490v2,cs.CL
Spelling Correction through Rewriting of Non-Autoregressive ASR Lattices,"For end-to-end Automatic Speech Recognition (ASR) models, recognizing
personal or rare phrases can be hard. A promising way to improve accuracy is
through spelling correction (or rewriting) of the ASR lattice, where
potentially misrecognized phrases are replaced with acoustically similar and
contextually relevant alternatives. However, rewriting is challenging for ASR
models trained with connectionist temporal classification (CTC) due to noisy
hypotheses produced by a non-autoregressive, context-independent beam search.
  We present a finite-state transducer (FST) technique for rewriting wordpiece
lattices generated by Transformer-based CTC models. Our algorithm performs
grapheme-to-phoneme (G2P) conversion directly from wordpieces into phonemes,
avoiding explicit word representations and exploiting the richness of the CTC
lattice. Our approach requires no retraining or modification of the ASR model.
We achieved up to a 15.2% relative reduction in sentence error rate (SER) on a
test set with contextually relevant entities.",2024-09-24,"Leonid Velikovich, Christopher Li, Diamantino Caseiro, Shankar Kumar, Pat Rondon, Kandarp Joshi, Xavier Velez",http://arxiv.org/pdf/2409.16469v1,cs.CL
"Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification","Logical reasoning is a fundamental task in natural language processing that
presents significant challenges to Large Language Models (LLMs). The inherent
characteristics of logical reasoning makes it well-suited for symbolic
representations such as first-order logic (FOL). Research in symbolic logical
reasoning explored FOL generation using state-of-the-art LLMs (i.e., GPT-4) to
produce FOL translations of natural language (NL) statements, but errors in
translation are usually not the focus. We address this by categorizing the
translation errors in FOL statements generated by LLMs. To make progress
towards improving the quality of FOL translations for smaller language models
such as LLaMA-2 13B and Mistral 7B, we create ProofFOL, a high-quality
FOL-annotated subset of ProofWriter dataset using GPT-4o. The models fine-tuned
on this silver standard data achieve a significant gain in performance when
compared to larger language models such as LLaMA-2 70B. In addition to
improving the model using large data, we also tackle the issue of data scarcity
and introduce an incremental framework encompassing of data augmentation and
verification steps. In the augmentation process, a single pair of (premises,
conclusion) is split into multiple new instances based on the predicates and
FOLs. This data is used for fine-tuning, and the inference on this model
generates FOLs with fewer errors over the model trained on the original data.
Our investigation on the translation errors leads to generation of a
perturbation dataset, which is used to train a verifier that corrects potential
syntactic and semantic FOL translation errors. We demonstrate an efficient
method for making the most of a limited existing human-annotated dataset. Our
results show state-of-the-art performance for ProofWriter and ProntoQA datasets
using ProofFOL on LLaMA-2 and Mistral models.",2024-09-24,"Ramya Keerthy Thatikonda, Jiuzhou Han, Wray Buntine, Ehsan Shareghi",http://arxiv.org/pdf/2409.16461v1,cs.CL
FMDLlama: Financial Misinformation Detection based on Large Language Models,"The emergence of social media has made the spread of misinformation easier.
In the financial domain, the accuracy of information is crucial for various
aspects of financial market, which has made financial misinformation detection
(FMD) an urgent problem that needs to be addressed. Large language models
(LLMs) have demonstrated outstanding performance in various fields. However,
current studies mostly rely on traditional methods and have not explored the
application of LLMs in the field of FMD. The main reason is the lack of FMD
instruction tuning datasets and evaluation benchmarks. In this paper, we
propose FMDLlama, the first open-sourced instruction-following LLMs for FMD
task based on fine-tuning Llama3.1 with instruction data, the first multi-task
FMD instruction dataset (FMDID) to support LLM instruction tuning, and a
comprehensive FMD evaluation benchmark (FMD-B) with classification and
explanation generation tasks to test the FMD ability of LLMs. We compare our
models with a variety of LLMs on FMD-B, where our model outperforms other
open-sourced LLMs as well as OpenAI's products. This project is available at
https://github.com/lzw108/FMD.",2024-09-24,"Zhiwei Liu, Xin Zhang, Kailai Yang, Qianqian Xie, Jimin Huang, Sophia Ananiadou",http://arxiv.org/pdf/2409.16452v2,cs.CL
A Comprehensive Survey of Bias in LLMs: Current Landscape and Future Directions,"Large Language Models(LLMs) have revolutionized various applications in
natural language processing (NLP) by providing unprecedented text generation,
translation, and comprehension capabilities. However, their widespread
deployment has brought to light significant concerns regarding biases embedded
within these models. This paper presents a comprehensive survey of biases in
LLMs, aiming to provide an extensive review of the types, sources, impacts, and
mitigation strategies related to these biases. We systematically categorize
biases into several dimensions. Our survey synthesizes current research
findings and discusses the implications of biases in real-world applications.
Additionally, we critically assess existing bias mitigation techniques and
propose future research directions to enhance fairness and equity in LLMs. This
survey serves as a foundational resource for researchers, practitioners, and
policymakers concerned with addressing and understanding biases in LLMs.",2024-09-24,"Rajesh Ranjan, Shailja Gupta, Surya Narayan Singh",http://arxiv.org/pdf/2409.16430v1,cs.CL
Revisiting Acoustic Features for Robust ASR,"Automatic Speech Recognition (ASR) systems must be robust to the myriad types
of noises present in real-world environments including environmental noise,
room impulse response, special effects as well as attacks by malicious actors
(adversarial attacks). Recent works seek to improve accuracy and robustness by
developing novel Deep Neural Networks (DNNs) and curating diverse training
datasets for them, while using relatively simple acoustic features. While this
approach improves robustness to the types of noise present in the training
data, it confers limited robustness against unseen noises and negligible
robustness to adversarial attacks. In this paper, we revisit the approach of
earlier works that developed acoustic features inspired by biological auditory
perception that could be used to perform accurate and robust ASR. In contrast,
Specifically, we evaluate the ASR accuracy and robustness of several
biologically inspired acoustic features. In addition to several features from
prior works, such as gammatone filterbank features (GammSpec), we also propose
two new acoustic features called frequency masked spectrogram (FreqMask) and
difference of gammatones spectrogram (DoGSpec) to simulate the
neuro-psychological phenomena of frequency masking and lateral suppression.
Experiments on diverse models and datasets show that (1) DoGSpec achieves
significantly better robustness than the highly popular log mel spectrogram
(LogMelSpec) with minimal accuracy degradation, and (2) GammSpec achieves
better accuracy and robustness to non-adversarial noises from the Speech Robust
Bench benchmark, but it is outperformed by DoGSpec against adversarial attacks.",2024-09-24,"Muhammad A. Shah, Bhiksha Raj",http://arxiv.org/pdf/2409.16399v1,cs.CL
RISCORE: Enhancing In-Context Riddle Solving in Language Models through Context-Reconstructed Example Augmentation,"Riddle-solving requires advanced reasoning skills, pushing LLMs to engage in
abstract thinking and creative problem-solving, often revealing limitations in
their cognitive abilities. In this paper, we examine the riddle-solving
capabilities of LLMs using a multiple-choice format, exploring how different
prompting techniques impact performance on riddles that demand diverse
reasoning skills. To enhance results, we introduce RISCORE (RIddle Solving with
COntext REcontruciton) a novel fully automated prompting method that generates
and utilizes contextually reconstructed sentence-based puzzles in conjunction
with the original examples to create few-shot exemplars. Our experiments
demonstrate that RISCORE significantly improves the performance of language
models in both vertical and lateral thinking tasks, surpassing traditional
exemplar selection strategies across a variety of few-shot settings.",2024-09-24,"Ioannis Panagiotopoulos, Giorgos Filandrianos, Maria Lymperaiou, Giorgos Stamou",http://arxiv.org/pdf/2409.16383v4,cs.CL
"Do the Right Thing, Just Debias! Multi-Category Bias Mitigation Using LLMs","This paper tackles the challenge of building robust and generalizable bias
mitigation models for language. Recognizing the limitations of existing
datasets, we introduce ANUBIS, a novel dataset with 1507 carefully curated
sentence pairs encompassing nine social bias categories. We evaluate
state-of-the-art models like T5, utilizing Supervised Fine-Tuning (SFT),
Reinforcement Learning (PPO, DPO), and In-Context Learning (ICL) for effective
bias mitigation. Our analysis focuses on multi-class social bias reduction,
cross-dataset generalizability, and environmental impact of the trained models.
ANUBIS and our findings offer valuable resources for building more equitable AI
systems and contribute to the development of responsible and unbiased
technologies with broad societal impact.",2024-09-24,"Amartya Roy, Danush Khanna, Devanshu Mahapatra, Vasanthakumar, Avirup Das, Kripabandhu Ghosh",http://arxiv.org/pdf/2409.16371v1,cs.CL
Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs,"Training large language models (LLMs) for external tool usage is a rapidly
expanding field, with recent research focusing on generating synthetic data to
address the shortage of available data. However, the absence of systematic data
quality checks poses complications for properly training and testing models. To
that end, we propose two approaches for assessing the reliability of data for
training LLMs to use external tools. The first approach uses intuitive,
human-defined correctness criteria. The second approach uses a model-driven
assessment with in-context evaluation. We conduct a thorough evaluation of data
quality on two popular benchmarks, followed by an extrinsic evaluation that
showcases the impact of data quality on model performance. Our results
demonstrate that models trained on high-quality data outperform those trained
on unvalidated data, even when trained with a smaller quantity of data. These
findings empirically support the significance of assessing and ensuring the
reliability of training data for tool-using LLMs.",2024-09-24,"Shadi Iskander, Nachshon Cohen, Zohar Karnin, Ori Shapira, Sofia Tolmach",http://arxiv.org/pdf/2409.16341v2,cs.CL
A fast and sound tagging method for discontinuous named-entity recognition,"We introduce a novel tagging scheme for discontinuous named entity
recognition based on an explicit description of the inner structure of
discontinuous mentions. We rely on a weighted finite state automaton for both
marginal and maximum a posteriori inference. As such, our method is sound in
the sense that (1) well-formedness of predicted tag sequences is ensured via
the automaton structure and (2) there is an unambiguous mapping between
well-formed sequences of tags and (discontinuous) mentions. We evaluate our
approach on three English datasets in the biomedical domain, and report
comparable results to state-of-the-art while having a way simpler and faster
model.",2024-09-24,Caio Corro,http://arxiv.org/pdf/2409.16243v1,cs.CL
EuroLLM: Multilingual Language Models for Europe,"The quality of open-weight LLMs has seen significant improvement, yet they
remain predominantly focused on English. In this paper, we introduce the
EuroLLM project, aimed at developing a suite of open-weight multilingual LLMs
capable of understanding and generating text in all official European Union
languages, as well as several additional relevant languages. We outline the
progress made to date, detailing our data collection and filtering process, the
development of scaling laws, the creation of our multilingual tokenizer, and
the data mix and modeling configurations. Additionally, we release our initial
models: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on
multilingual general benchmarks and machine translation.",2024-09-24,"Pedro Henrique Martins, Patrick Fernandes, João Alves, Nuno M. Guerreiro, Ricardo Rei, Duarte M. Alves, José Pombal, Amin Farajian, Manuel Faysse, Mateusz Klimaszewski, Pierre Colombo, Barry Haddow, José G. C. de Souza, Alexandra Birch, André F. T. Martins",http://arxiv.org/pdf/2409.16235v1,cs.CL
Towards Enhancing Linked Data Retrieval in Conversational UIs using Large Language Models,"Despite the recent broad adoption of Large Language Models (LLMs) across
various domains, their potential for enriching information systems in
extracting and exploring Linked Data (LD) and Resource Description Framework
(RDF) triplestores has not been extensively explored. This paper examines the
integration of LLMs within existing systems, emphasising the enhancement of
conversational user interfaces (UIs) and their capabilities for data extraction
by producing more accurate SPARQL queries without the requirement for model
retraining. Typically, conversational UI models necessitate retraining with the
introduction of new datasets or updates, limiting their functionality as
general-purpose extraction tools. Our approach addresses this limitation by
incorporating LLMs into the conversational UI workflow, significantly enhancing
their ability to comprehend and process user queries effectively. By leveraging
the advanced natural language understanding capabilities of LLMs, our method
improves RDF entity extraction within web systems employing conventional
chatbots. This integration facilitates a more nuanced and context-aware
interaction model, critical for handling the complex query patterns often
encountered in RDF datasets and Linked Open Data (LOD) endpoints. The
evaluation of this methodology shows a marked enhancement in system
expressivity and the accuracy of responses to user queries, indicating a
promising direction for future research in this area. This investigation not
only underscores the versatility of LLMs in enhancing existing information
systems but also sets the stage for further explorations into their potential
applications within more specialised domains of web information systems.",2024-09-24,"Omar Mussa, Omer Rana, Benoît Goossens, Pablo Orozco-Terwengel, Charith Perera",http://arxiv.org/pdf/2409.16220v1,cs.CL
HelloBench: Evaluating Long Text Generation Capabilities of Large Language Models,"In recent years, Large Language Models (LLMs) have demonstrated remarkable
capabilities in various tasks (e.g., long-context understanding), and many
benchmarks have been proposed. However, we observe that long text generation
capabilities are not well investigated. Therefore, we introduce the
Hierarchical Long Text Generation Benchmark (HelloBench), a comprehensive,
in-the-wild, and open-ended benchmark to evaluate LLMs' performance in
generating long text. Based on Bloom's Taxonomy, HelloBench categorizes long
text generation tasks into five subtasks: open-ended QA, summarization, chat,
text completion, and heuristic text generation. Besides, we propose
Hierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation
method that significantly reduces the time and effort required for human
evaluation while maintaining a high correlation with human evaluation. We have
conducted extensive experiments across around 30 mainstream LLMs and observed
that the current LLMs lack long text generation capabilities. Specifically,
first, regardless of whether the instructions include explicit or implicit
length constraints, we observe that most LLMs cannot generate text that is
longer than 4000 words. Second, we observe that while some LLMs can generate
longer text, many issues exist (e.g., severe repetition and quality
degradation). Third, to demonstrate the effectiveness of HelloEval, we compare
HelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge
methods, which show that HelloEval has the highest correlation with human
evaluation. We release our code in https://github.com/Quehry/HelloBench.",2024-09-24,"Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, Junran Peng, Zhaoxiang Zhang, Songyang Zhang, Kai Chen",http://arxiv.org/pdf/2409.16191v1,cs.CL
Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering,"Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning
large language models (LLMs) to various domains due to its modular design and
widespread availability on platforms like Huggingface. This modularity has
sparked interest in combining multiple LoRAs to enhance LLM capabilities.
However, existing methods for LoRA composition primarily focus on task-specific
adaptations that require additional training, and current model merging
techniques often fail to fully leverage LoRA's modular nature, leading to
parameter interference and performance degradation. In this paper, we
investigate the feasibility of disassembling and reassembling multiple LoRAs at
a finer granularity, analogous to assembling LEGO blocks. We introduce the
concept of Minimal Semantic Units (MSUs), where the parameters corresponding to
each rank in LoRA function as independent units. These MSUs demonstrate
permutation invariance and concatenation-summation equivalence properties,
enabling flexible combinations to create new LoRAs. Building on these insights,
we propose the LoRA-LEGO framework. This framework conducts rank-wise parameter
clustering by grouping MSUs from different LoRAs into $k$ clusters. The
centroid of each cluster serves as a representative MSU, enabling the assembly
of a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual
reweighting strategy to optimize the scale of the merged LoRA. Experiments
across various benchmarks demonstrate that our method outperforms existing
approaches in LoRA merging.",2024-09-24,"Ziyu Zhao, Tao Shen, Didi Zhu, Zexi Li, Jing Su, Xuwu Wang, Kun Kuang, Fei Wu",http://arxiv.org/pdf/2409.16167v3,cs.CL
Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework,"Retrieval-augmented generation (RAG) has emerged as a popular solution to
mitigate the hallucination issues of large language models. However, existing
studies on RAG seldom address the issue of predictive uncertainty, i.e., how
likely it is that a RAG model's prediction is incorrect, resulting in
uncontrollable risks in real-world applications. In this work, we emphasize the
importance of risk control, ensuring that RAG models proactively refuse to
answer questions with low confidence. Our research identifies two critical
latent factors affecting RAG's confidence in its predictions: the quality of
the retrieved results and the manner in which these results are utilized. To
guide RAG models in assessing their own confidence based on these two latent
factors, we develop a counterfactual prompting framework that induces the
models to alter these factors and analyzes the effect on their answers. We also
introduce a benchmarking procedure to collect answers with the option to
abstain, facilitating a series of experiments. For evaluation, we introduce
several risk-related metrics and the experimental results demonstrate the
effectiveness of our approach. Our code and benchmark dataset are available at
https://github.com/ict-bigdatalab/RC-RAG.",2024-09-24,"Lu Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, Xueqi Cheng",http://arxiv.org/pdf/2409.16146v2,cs.CL
HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear Composition for Open-Vocabulary Object Detection,"Open-vocabulary object detection (OVD) models are considered to be Large
Multi-modal Models (LMM), due to their extensive training data and a large
number of parameters. Mainstream OVD models prioritize object coarse-grained
category rather than focus on their fine-grained attributes, e.g., colors or
materials, thus failed to identify objects specified with certain attributes.
However, OVD models are pretrained on large-scale image-text pairs with rich
attribute words, whose latent feature space can represent the global text
feature as a linear composition of fine-grained attribute tokens without
highlighting them. Therefore, we propose in this paper a universal and explicit
approach for frozen mainstream OVD models that boosts their attribute-level
detection capabilities by highlighting fine-grained attributes in explicit
linear space. Firstly, a LLM is leveraged to highlight attribute words within
the input text as a zero-shot prompted task. Secondly, by strategically
adjusting the token masks, the text encoders of OVD models extract both global
text and attribute-specific features, which are then explicitly composited as
two vectors in linear space to form the new attribute-highlighted feature for
detection tasks, where corresponding scalars are hand-crafted or learned to
reweight both two vectors. Notably, these scalars can be seamlessly transferred
among different OVD models, which proves that such an explicit linear
composition is universal. Empirical evaluation on the FG-OVD dataset
demonstrates that our proposed method uniformly improves fine-grained
attribute-level OVD of various mainstream models and achieves new
state-of-the-art performance.",2024-09-24,"Yuqi Ma, Mengyin Liu, Chao Zhu, Xu-Cheng Yin",http://arxiv.org/pdf/2409.16136v1,cs.CL
Implicit assessment of language learning during practice as accurate as explicit testing,"Assessment of proficiency of the learner is an essential part of Intelligent
Tutoring Systems (ITS). We use Item Response Theory (IRT) in computer-aided
language learning for assessment of student ability in two contexts: in test
sessions, and in exercises during practice sessions. Exhaustive testing across
a wide range of skills can provide a detailed picture of proficiency, but may
be undesirable for a number of reasons. Therefore, we first aim to replace
exhaustive tests with efficient but accurate adaptive tests. We use learner
data collected from exhaustive tests under imperfect conditions, to train an
IRT model to guide adaptive tests. Simulations and experiments with real
learner data confirm that this approach is efficient and accurate. Second, we
explore whether we can accurately estimate learner ability directly from the
context of practice with exercises, without testing. We transform learner data
collected from exercise sessions into a form that can be used for IRT modeling.
This is done by linking the exercises to {\em linguistic constructs}; the
constructs are then treated as ""items"" within IRT. We present results from
large-scale studies with thousands of learners. Using teacher assessments of
student ability as ""ground truth,"" we compare the estimates obtained from tests
vs. those from exercises. The experiments confirm that the IRT models can
produce accurate ability estimation based on exercises.",2024-09-24,"Jue Hou, Anisia Katinskaia, Anh-Duc Vu, Roman Yangarber",http://arxiv.org/pdf/2409.16133v1,cs.CL
MOSS: Enabling Code-Driven Evolution and Context Management for AI Agents,"Developing AI agents powered by large language models (LLMs) faces
significant challenges in achieving true Turing completeness and adaptive,
code-driven evolution. Current approaches often generate code independently of
its runtime context, relying heavily on the LLM's memory, which results in
inefficiencies and limits adaptability. Manual protocol development in sandbox
environments further constrains the agent's autonomous adaptability. Crucially,
achieving consistency in code and context across multi-turn interactions and
ensuring isolation of local variables within each interaction remains an
unsolved problem.
  We introduce MOSS (llM-oriented Operating System Simulation), a novel
framework that addresses these challenges by integrating code generation with a
dynamic context management system. MOSS ensures consistency and adaptability by
using a mechanism that maintains the Python context across interactions,
including isolation of local variables and preservation of runtime integrity.
At its core, the framework employs an Inversion of Control (IoC) container in
conjunction with decorators to enforce the least knowledge principle, allowing
agents to focus on abstract interfaces rather than concrete implementations.
This facilitates seamless integration of new tools and libraries, enables
runtime instance replacement, and reduces prompt complexity, providing a ""what
you see is what you get"" environment for the agent.
  Through a series of case studies, we show how this framework can enhance the
efficiency and capabilities of agent development and highlight its advantages
in moving towards Turing-complete agents capable of evolving through code.",2024-09-24,"Ming Zhu, Yi Zhou",http://arxiv.org/pdf/2409.16120v1,cs.CL
Exploring Hint Generation Approaches in Open-Domain Question Answering,"Automatic Question Answering (QA) systems rely on contextual information to
provide accurate answers. Commonly, contexts are prepared through either
retrieval-based or generation-based methods. The former involves retrieving
relevant documents from a corpus like Wikipedia, whereas the latter uses
generative models such as Large Language Models (LLMs) to generate the context.
In this paper, we introduce a novel context preparation approach called HINTQA,
which employs Automatic Hint Generation (HG) techniques. Unlike traditional
methods, HINTQA prompts LLMs to produce hints about potential answers for the
question rather than generating relevant context. We evaluate our approach
across three QA datasets including TriviaQA, NaturalQuestions, and Web
Questions, examining how the number and order of hints impact performance. Our
findings show that the HINTQA surpasses both retrieval-based and
generation-based approaches. We demonstrate that hints enhance the accuracy of
answers more than retrieved and generated contexts.",2024-09-24,"Jamshid Mozafari, Abdelrahman Abdallah, Bhawna Piryani, Adam Jatowt",http://arxiv.org/pdf/2409.16096v1,cs.CL
Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering,"Users post numerous product-related questions on e-commerce platforms,
affecting their purchase decisions. Product-related question answering (PQA)
entails utilizing product-related resources to provide precise responses to
users. We propose a novel task of Multilingual Cross-market Product-based
Question Answering (MCPQA) and define the task as providing answers to
product-related questions in a main marketplace by utilizing information from
another resource-rich auxiliary marketplace in a multilingual context. We
introduce a large-scale dataset comprising over 7 million questions from 17
marketplaces across 11 languages. We then perform automatic translation on the
Electronics category of our dataset, naming it as McMarket. We focus on two
subtasks: review-based answer generation and product-related question ranking.
For each subtask, we label a subset of McMarket using an LLM and further
evaluate the quality of the annotations via human assessment. We then conduct
experiments to benchmark our dataset, using models ranging from traditional
lexical models to LLMs in both single-market and cross-market scenarios across
McMarket and the corresponding LLM subset. Results show that incorporating
cross-market information significantly enhances performance in both tasks.",2024-09-24,"Yifei Yuan, Yang Deng, Anders Søgaard, Mohammad Aliannejadi",http://arxiv.org/pdf/2409.16025v1,cs.CL
AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming in LLM-Based Batch Relevance Assessment,"Cognitive biases are systematic deviations in thinking that lead to
irrational judgments and problematic decision-making, extensively studied
across various fields. Recently, large language models (LLMs) have shown
advanced understanding capabilities but may inherit human biases from their
training data. While social biases in LLMs have been well-studied, cognitive
biases have received less attention, with existing research focusing on
specific scenarios. The broader impact of cognitive biases on LLMs in various
decision-making contexts remains underexplored. We investigated whether LLMs
are influenced by the threshold priming effect in relevance judgments, a core
task and widely-discussed research topic in the Information Retrieval (IR)
coummunity. The priming effect occurs when exposure to certain stimuli
unconsciously affects subsequent behavior and decisions. Our experiment
employed 10 topics from the TREC 2019 Deep Learning passage track collection,
and tested AI judgments under different document relevance scores, batch
lengths, and LLM models, including GPT-3.5, GPT-4, LLaMa2-13B and LLaMa2-70B.
Results showed that LLMs tend to give lower scores to later documents if
earlier ones have high relevance, and vice versa, regardless of the combination
and model used. Our finding demonstrates that LLM%u2019s judgments, similar to
human judgments, are also influenced by threshold priming biases, and suggests
that researchers and system engineers should take into account potential
human-like cognitive biases in designing, evaluating, and auditing LLMs in IR
tasks and beyond.",2024-09-24,"Nuo Chen, Jiqun Liu, Xiaoyu Dong, Qijiong Liu, Tetsuya Sakai, Xiao-Ming Wu",http://arxiv.org/pdf/2409.16022v2,cs.CL
Bridging Speech and Text: Enhancing ASR with Pinyin-to-Character Pre-training in LLMs,"The integration of large language models (LLMs) with pre-trained speech
models has opened up new avenues in automatic speech recognition (ASR). While
LLMs excel in multimodal understanding tasks, effectively leveraging their
capabilities for ASR remains a significant challenge. This paper presents a
novel training approach to enhance LLM performance in ASR tasks. We propose
pre-training LLMs on Pinyin embedding sequences, which represent pronunciation
features, to generate corresponding Chinese characters. This step enables the
LLM to adapt to generating text from pronunciation features before encountering
real speech data. Furthermore, we fine-tune the LoRA parameters to enhance the
LLM's understanding of speech modality information. In AISHELL-1 corpus, our
approach yields a 9.5% relative improvement in ASR tasks compared to the
baseline without Pinyi-to-Character pre-training. Additionally, incorporating
auxiliary text data for Pinyi-to-Character pre-training further boosts
performance, achieving a 19.0% relative improvement.",2024-09-24,"Yang Yuhang, Peng Yizhou, Eng Siong Chng, Xionghu Zhong",http://arxiv.org/pdf/2409.16005v1,cs.CL
Finetuning LLMs for Comparative Assessment Tasks,"Automated assessment in natural language generation is a challenging task.
Instruction-tuned large language models (LLMs) have shown promise in
reference-free evaluation, particularly through comparative assessment.
However, the quadratic computational complexity of pairwise comparisons limits
its scalability. To address this, efficient comparative assessment has been
explored by applying comparative strategies on zero-shot LLM probabilities. We
propose a framework for finetuning LLMs for comparative assessment to align the
model's output with the target distribution of comparative probabilities. By
training on soft probabilities, our approach improves state-of-the-art
performance while maintaining high performance with an efficient subset of
comparisons.",2024-09-24,"Vatsal Raina, Adian Liusie, Mark Gales",http://arxiv.org/pdf/2409.15979v1,cs.CL
TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control,"Zero-shot singing voice synthesis (SVS) with style transfer and style control
aims to generate high-quality singing voices with unseen timbres and styles
(including singing method, emotion, rhythm, technique, and pronunciation) from
audio and text prompts. However, the multifaceted nature of singing styles
poses a significant challenge for effective modeling, transfer, and control.
Furthermore, current SVS models often fail to generate singing voices rich in
stylistic nuances for unseen singers. To address these challenges, we introduce
TCSinger, the first zero-shot SVS model for style transfer across cross-lingual
speech and singing styles, along with multi-level style control. Specifically,
TCSinger proposes three primary modules: 1) the clustering style encoder
employs a clustering vector quantization model to stably condense style
information into a compact latent space; 2) the Style and Duration Language
Model (S\&D-LM) concurrently predicts style information and phoneme duration,
which benefits both; 3) the style adaptive decoder uses a novel mel-style
adaptive normalization method to generate singing voices with enhanced details.
Experimental results show that TCSinger outperforms all baseline models in
synthesis quality, singer similarity, and style controllability across various
tasks, including zero-shot style transfer, multi-level style control,
cross-lingual style transfer, and speech-to-singing style transfer. Singing
voice samples can be accessed at https://aaronz345.github.io/TCSingerDemo/.",2024-09-24,"Yu Zhang, Ziyue Jiang, Ruiqi Li, Changhao Pan, Jinzheng He, Rongjie Huang, Chuxin Wang, Zhou Zhao",http://arxiv.org/pdf/2409.15977v5,cs.CL
Tuning Into Bias: A Computational Study of Gender Bias in Song Lyrics,"The application of text mining methods is becoming increasingly prevalent,
particularly within Humanities and Computational Social Sciences, as well as in
a broader range of disciplines. This paper presents an analysis of gender bias
in English song lyrics using topic modeling and bias measurement techniques.
Leveraging BERTopic, we cluster a dataset of 537,553 English songs into
distinct topics and analyze their temporal evolution. Our results reveal a
significant thematic shift in song lyrics over time, transitioning from
romantic themes to a heightened focus on the sexualization of women.
Additionally, we observe a substantial prevalence of profanity and misogynistic
content across various topics, with a particularly high concentration in the
largest thematic cluster. To further analyse gender bias across topics and
genres in a quantitative way, we employ the Single Category Word Embedding
Association Test (SC-WEAT) to calculate bias scores for word embeddings trained
on the most prominent topics as well as individual genres. The results indicate
a consistent male bias in words associated with intelligence and strength,
while appearance and weakness words show a female bias. Further analysis
highlights variations in these biases across topics, illustrating the interplay
between thematic content and gender stereotypes in song lyrics.",2024-09-24,"Danqing Chen, Adithi Satish, Rasul Khanbayov, Carolin M. Schuster, Georg Groh",http://arxiv.org/pdf/2409.15949v2,cs.CL
Automated test generation to evaluate tool-augmented LLMs as conversational AI agents,"Tool-augmented LLMs are a promising approach to create AI agents that can
have realistic conversations, follow procedures, and call appropriate
functions. However, evaluating them is challenging due to the diversity of
possible conversations, and existing datasets focus only on single interactions
and function-calling. We present a test generation pipeline to evaluate LLMs as
conversational AI agents. Our framework uses LLMs to generate diverse tests
grounded on user-defined procedures. For that, we use intermediate graphs to
limit the LLM test generator's tendency to hallucinate content that is not
grounded on input procedures, and enforces high coverage of the possible
conversations. Additionally, we put forward ALMITA, a manually curated dataset
for evaluating AI agents in customer support, and use it to evaluate existing
LLMs. Our results show that while tool-augmented LLMs perform well in single
interactions, they often struggle to handle complete conversations. While our
focus is on customer support, our method is general and capable of AI agents
for different domains.",2024-09-24,"Samuel Arcadinho, David Aparicio, Mariana Almeida",http://arxiv.org/pdf/2409.15934v2,cs.CL
SLIMER-IT: Zero-Shot NER on Italian Language,"Traditional approaches to Named Entity Recognition (NER) frame the task into
a BIO sequence labeling problem. Although these systems often excel in the
downstream task at hand, they require extensive annotated data and struggle to
generalize to out-of-distribution input domains and unseen entity types. On the
contrary, Large Language Models (LLMs) have demonstrated strong zero-shot
capabilities. While several works address Zero-Shot NER in English, little has
been done in other languages. In this paper, we define an evaluation framework
for Zero-Shot NER, applying it to the Italian language. Furthermore, we
introduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning
approach for zero-shot NER leveraging prompts enriched with definition and
guidelines. Comparisons with other state-of-the-art models, demonstrate the
superiority of SLIMER-IT on never-seen-before entity tags.",2024-09-24,"Andrew Zamai, Leonardo Rigutini, Marco Maggini, Andrea Zugarini",http://arxiv.org/pdf/2409.15933v2,cs.CL
Multilingual Transfer and Domain Adaptation for Low-Resource Languages of Spain,"This article introduces the submission status of the Translation into
Low-Resource Languages of Spain task at (WMT 2024) by Huawei Translation
Service Center (HW-TSC). We participated in three translation tasks: spanish to
aragonese (es-arg), spanish to aranese (es-arn), and spanish to asturian
(es-ast). For these three translation tasks, we use training strategies such as
multilingual transfer, regularized dropout, forward translation and back
translation, labse denoising, transduction ensemble learning and other
strategies to neural machine translation (NMT) model based on training deep
transformer-big architecture. By using these enhancement strategies, our
submission achieved a competitive result in the final evaluation.",2024-09-24,"Yuanchang Luo, Zhanglin Wu, Daimeng Wei, Hengchao Shang, Zongyao Li, Jiaxin Guo, Zhiqiang Rao, Shaojun Li, Jinlong Yang, Yuhao Xie, Jiawei Zheng Bin Wei, Hao Yang",http://arxiv.org/pdf/2409.15924v2,cs.CL
Explaining word embeddings with perfect fidelity: Case study in research impact prediction,"Best performing approaches for scholarly document quality prediction are
based on embedding models, which do not allow direct explanation of classifiers
as distinct words no longer correspond to the input features for model
training. Although model-agnostic explanation methods such as Local
interpretable model-agnostic explanations (LIME) can be applied, these produce
results with questionable correspondence to the ML model. We introduce a new
feature importance method, Self-model Rated Entities (SMER), for logistic
regression-based classification models trained on word embeddings. We show that
SMER has theoretically perfect fidelity with the explained model, as its
prediction corresponds exactly to the average of predictions for individual
words in the text. SMER allows us to reliably determine which words or entities
positively contribute to predicting impactful articles. Quantitative and
qualitative evaluation is performed through five diverse experiments conducted
on 50.000 research papers from the CORD-19 corpus. Through an AOPC curve
analysis, we experimentally demonstrate that SMER produces better explanations
than LIME for logistic regression.",2024-09-24,"Lucie Dvorackova, Marcin P. Joachimiak, Michal Cerny, Adriana Kubecova, Vilem Sklenak, Tomas Kliegr",http://arxiv.org/pdf/2409.15912v1,cs.CL
A Modular-based Strategy for Mitigating Gradient Conflicts in Simultaneous Speech Translation,"Simultaneous Speech Translation (SimulST) involves generating target language
text while continuously processing streaming speech input, presenting
significant real-time challenges. Multi-task learning is often employed to
enhance SimulST performance but introduces optimization conflicts between
primary and auxiliary tasks, potentially compromising overall efficiency. The
existing model-level conflict resolution methods are not well-suited for this
task which exacerbates inefficiencies and leads to high GPU memory consumption.
To address these challenges, we propose a Modular Gradient Conflict Mitigation
(MGCM) strategy that detects conflicts at a finer-grained modular level and
resolves them utilizing gradient projection. Experimental results demonstrate
that MGCM significantly improves SimulST performance, particularly under medium
and high latency conditions, achieving a 0.68 BLEU score gain in offline tasks.
Additionally, MGCM reduces GPU memory consumption by over 95\% compared to
other conflict mitigation methods, establishing it as a robust solution for
SimulST tasks.",2024-09-24,"Xiaoqian Liu, Yangfan Du, Jianjin Wang, Yuan Ge, Chen Xu, Tong Xiao, Guocheng Chen, Jingbo Zhu",http://arxiv.org/pdf/2409.15911v3,cs.CL
Enhancing Text-to-SQL Capabilities of Large Language Models via Domain Database Knowledge Injection,"Text-to-SQL is a subtask in semantic parsing that has seen rapid progress
with the evolution of Large Language Models (LLMs). However, LLMs face
challenges due to hallucination issues and a lack of domain-specific database
knowledge(such as table schema and cell values). As a result, they can make
errors in generating table names, columns, and matching values to the correct
columns in SQL statements. This paper introduces a method of knowledge
injection to enhance LLMs' ability to understand schema contents by
incorporating prior knowledge. This approach improves their performance in
Text-to-SQL tasks. Experimental results show that pre-training LLMs on
domain-specific database knowledge and fine-tuning them on downstream
Text-to-SQL tasks significantly improves the Execution Match (EX) and Exact
Match (EM) metrics across various models. This effectively reduces errors in
generating column names and matching values to the columns. Furthermore, the
knowledge-injected models can be applied to many downstream Text-to-SQL tasks,
demonstrating the generalizability of the approach presented in this paper.",2024-09-24,"Xingyu Ma, Xin Tian, Lingxiang Wu, Xuepeng Wang, Xueming Tang, Jinqiao Wang",http://arxiv.org/pdf/2409.15907v2,cs.CL
Konstruktor: A Strong Baseline for Simple Knowledge Graph Question Answering,"While being one of the most popular question types, simple questions such as
""Who is the author of Cinderella?"", are still not completely solved.
Surprisingly, even the most powerful modern Large Language Models are prone to
errors when dealing with such questions, especially when dealing with rare
entities. At the same time, as an answer may be one hop away from the question
entity, one can try to develop a method that uses structured knowledge graphs
(KGs) to answer such questions. In this paper, we introduce Konstruktor - an
efficient and robust approach that breaks down the problem into three steps:
(i) entity extraction and entity linking, (ii) relation prediction, and (iii)
querying the knowledge graph. Our approach integrates language models and
knowledge graphs, exploiting the power of the former and the interpretability
of the latter. We experiment with two named entity recognition and entity
linking methods and several relation detection techniques. We show that for
relation detection, the most challenging step of the workflow, a combination of
relation classification/generation and ranking outperforms other methods. We
report Konstruktor's strong results on four datasets.",2024-09-24,"Maria Lysyuk, Mikhail Salnikov, Pavel Braslavski, Alexander Panchenko",http://arxiv.org/pdf/2409.15902v1,cs.CL
HLB: Benchmarking LLMs' Humanlikeness in Language Use,"As synthetic data becomes increasingly prevalent in training language models,
particularly through generated dialogue, concerns have emerged that these
models may deviate from authentic human language patterns, potentially losing
the richness and creativity inherent in human communication. This highlights
the critical need to assess the humanlikeness of language models in real-world
language use. In this paper, we present a comprehensive humanlikeness benchmark
(HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic
experiments designed to probe core linguistic aspects, including sound, word,
syntax, semantics, and discourse (see
https://huggingface.co/spaces/XufengDuan/HumanLikeness). To anchor these
comparisons, we collected responses from over 2,000 human participants and
compared them to outputs from the LLMs in these experiments.
  For rigorous evaluation, we developed a coding algorithm that accurately
identified language use patterns, enabling the extraction of response
distributions for each task. By comparing the response distributions between
human participants and LLMs, we quantified humanlikeness through distributional
similarity. Our results reveal fine-grained differences in how well LLMs
replicate human responses across various linguistic levels. Importantly, we
found that improvements in other performance metrics did not necessarily lead
to greater humanlikeness, and in some cases, even resulted in a decline. By
introducing psycholinguistic methods to model evaluation, this benchmark offers
the first framework for systematically assessing the humanlikeness of LLMs in
language use.",2024-09-24,"Xufeng Duan, Bei Xiao, Xuemei Tang, Zhenguang G. Cai",http://arxiv.org/pdf/2409.15890v1,cs.CL
Machine Translation Advancements of Low-Resource Indian Languages by Transfer Learning,"This paper introduces the submission by Huawei Translation Center (HW-TSC) to
the WMT24 Indian Languages Machine Translation (MT) Shared Task. To develop a
reliable machine translation system for low-resource Indian languages, we
employed two distinct knowledge transfer strategies, taking into account the
characteristics of the language scripts and the support available from existing
open-source models for Indian languages. For Assamese(as) and Manipuri(mn), we
fine-tuned the existing IndicTrans2 open-source model to enable bidirectional
translation between English and these languages. For Khasi (kh) and Mizo (mz),
We trained a multilingual model as a baseline using bilingual data from these
four language pairs, along with an additional about 8kw English-Bengali
bilingual data, all of which share certain linguistic features. This was
followed by fine-tuning to achieve bidirectional translation between English
and Khasi, as well as English and Mizo. Our transfer learning experiments
produced impressive results: 23.5 BLEU for en-as, 31.8 BLEU for en-mn, 36.2
BLEU for as-en, and 47.9 BLEU for mn-en on their respective test sets.
Similarly, the multilingual model transfer learning experiments yielded
impressive outcomes, achieving 19.7 BLEU for en-kh, 32.8 BLEU for en-mz, 16.1
BLEU for kh-en, and 33.9 BLEU for mz-en on their respective test sets. These
results not only highlight the effectiveness of transfer learning techniques
for low-resource languages but also contribute to advancing machine translation
capabilities for low-resource Indian languages.",2024-09-24,"Bin Wei, Jiawei Zhen, Zongyao Li, Zhanglin Wu, Daimeng Wei, Jiaxin Guo, Zhiqiang Rao, Shaojun Li, Yuanchang Luo, Hengchao Shang, Jinlong Yang, Yuhao Xie, Hao Yang",http://arxiv.org/pdf/2409.15879v1,cs.CL
Exploring the traditional NMT model and Large Language Model for chat translation,"This paper describes the submissions of Huawei Translation Services
Center(HW-TSC) to WMT24 chat translation shared task on
English$\leftrightarrow$Germany (en-de) bidirection. The experiments involved
fine-tuning models using chat data and exploring various strategies, including
Minimum Bayesian Risk (MBR) decoding and self-training. The results show
significant performance improvements in certain directions, with the MBR
self-training method achieving the best results. The Large Language Model also
discusses the challenges and potential avenues for further research in the
field of chat translation.",2024-09-24,"Jinlong Yang, Hengchao Shang, Daimeng Wei, Jiaxin Guo, Zongyao Li, Zhanglin Wu, Zhiqiang Rao, Shaojun Li, Yuhao Xie, Yuanchang Luo, Jiawei Zheng, Bin Wei, Hao Yang",http://arxiv.org/pdf/2409.16331v1,cs.CL
Privacy Evaluation Benchmarks for NLP Models,"By inducing privacy attacks on NLP models, attackers can obtain sensitive
information such as training data and model parameters, etc. Although
researchers have studied, in-depth, several kinds of attacks in NLP models,
they are non-systematic analyses. It lacks a comprehensive understanding of the
impact caused by the attacks. For example, we must consider which scenarios can
apply to which attacks, what the common factors are that affect the performance
of different attacks, the nature of the relationships between different
attacks, and the influence of various datasets and models on the effectiveness
of the attacks, etc. Therefore, we need a benchmark to holistically assess the
privacy risks faced by NLP models. In this paper, we present a privacy attack
and defense evaluation benchmark in the field of NLP, which includes the
conventional/small models and large language models (LLMs). This benchmark
supports a variety of models, datasets, and protocols, along with standardized
modules for comprehensive evaluation of attacks and defense strategies. Based
on the above framework, we present a study on the association between auxiliary
data from different domains and the strength of privacy attacks. And we provide
an improved attack method in this scenario with the help of Knowledge
Distillation (KD). Furthermore, we propose a chained framework for privacy
attacks. Allowing a practitioner to chain multiple attacks to achieve a
higher-level attack objective. Based on this, we provide some defense and
enhanced attack strategies. The code for reproducing the results can be found
at https://github.com/user2311717757/nlp_doctor.",2024-09-24,"Wei Huang, Yinggui Wang, Cen Chen",http://arxiv.org/pdf/2409.15868v3,cs.CL
BeSimulator: A Large Language Model Powered Text-based Behavior Simulator,"Traditional robot simulators focus on physical process modeling and realistic
rendering, often suffering from high computational costs, inefficiencies, and
limited adaptability. To handle this issue, we propose Behavior Simulation in
robotics to emphasize checking the behavior logic of robots and achieving
sufficient alignment between the outcome of robot actions and real scenarios.
In this paper, we introduce BeSimulator, a modular and novel LLM-powered
framework, as an attempt towards behavior simulation in the context of
text-based environments. By constructing text-based virtual environments and
performing semantic-level simulation, BeSimulator can generalize across
scenarios and achieve long-horizon complex simulation. Inspired by human
cognition processes, it employs a ""consider-decide-capture-transfer""
methodology, termed Chain of Behavior Simulation, which excels at analyzing
action feasibility and state transitions. Additionally, BeSimulator
incorporates code-driven reasoning to enable arithmetic operations and enhance
reliability, as well as integrates reflective feedback to refine simulation.
Based on our manually constructed behavior-tree-based simulation benchmark
BTSIMBENCH, our experiments show a significant performance improvement in
behavior simulation compared to baselines, ranging from 14.7% to 26.6%.",2024-09-24,"Jianan Wang, Bin Li, Xueying Wang, Fu Li, Yunlong Wu, Juan Chen, Xiaodong Yi",http://arxiv.org/pdf/2409.15865v1,cs.CL
A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding,"Dialogue State Tracking (DST) is crucial for understanding user needs and
executing appropriate system actions in task-oriented dialogues. Majority of
existing DST methods are designed to work within predefined ontologies and
assume the availability of gold domain labels, struggling with adapting to new
slots values. While Large Language Models (LLMs)-based systems show promising
zero-shot DST performance, they either require extensive computational
resources or they underperform existing fully-trained systems, limiting their
practicality. To address these limitations, we propose a zero-shot,
open-vocabulary system that integrates domain classification and DST in a
single pipeline. Our approach includes reformulating DST as a
question-answering task for less capable models and employing self-refining
prompts for more adaptable ones. Our system does not rely on fixed slot values
defined in the ontology allowing the system to adapt dynamically. We compare
our approach with existing SOTA, and show that it provides up to 20% better
Joint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,
with up to 90% fewer requests to the LLM API.",2024-09-24,"Abdulfattah Safa, Gözde Gül Şahin",http://arxiv.org/pdf/2409.15861v3,cs.CL
iGAiVA: Integrated Generative AI and Visual Analytics in a Machine Learning Workflow for Text Classification,"In developing machine learning (ML) models for text classification, one
common challenge is that the collected data is often not ideally distributed,
especially when new classes are introduced in response to changes of data and
tasks. In this paper, we present a solution for using visual analytics (VA) to
guide the generation of synthetic data using large language models. As VA
enables model developers to identify data-related deficiency, data synthesis
can be targeted to address such deficiency. We discuss different types of data
deficiency, describe different VA techniques for supporting their
identification, and demonstrate the effectiveness of targeted data synthesis in
improving model accuracy. In addition, we present a software tool, iGAiVA,
which maps four groups of ML tasks into four VA views, integrating generative
AI and VA into an ML workflow for developing and improving text classification
models.",2024-09-24,"Yuanzhe Jin, Adrian Carrasco-Revilla, Min Chen",http://arxiv.org/pdf/2409.15848v2,cs.CL
Textless NLP -- Zero Resource Challenge with Low Resource Compute,"This work addresses the persistent challenges of substantial training time
and GPU resource requirements even when training lightweight encoder-vocoder
models for Textless NLP. We reduce training steps significantly while improving
performance by a) leveraging learning rate schedulers for efficient and faster
convergence b) optimizing hop length and c) tuning the interpolation scale
factors for better audio quality. Additionally, we explore the latent space
representation for Indian languages such as Tamil and Bengali for the acoustic
unit discovery and voice conversion task. Our approach leverages a quantized
encoder architecture, in conjunction with a vocoder which utilizes the proposed
mixture of optimized hop length, tuned interpolation scale factors and a cyclic
learning rate scheduler. We obtain consistently good results across English,
Tamil and Bengali datasets. The proposed method excels in capturing complex
linguistic patterns, resulting in clear reconstructed audio during voice
conversion with significantly reduced training time.",2024-09-24,"Krithiga Ramadass, Abrit Pal Singh, Srihari J, Sheetal Kalyani",http://arxiv.org/pdf/2409.19015v1,cs.CL
Unveiling Language Competence Neurons: A Psycholinguistic Approach to Model Interpretability,"As large language models (LLMs) advance in their linguistic capacity,
understanding how they capture aspects of language competence remains a
significant challenge. This study therefore employs psycholinguistic paradigms
in English, which are well-suited for probing deeper cognitive aspects of
language processing, to explore neuron-level representations in language model
across three tasks: sound-shape association, sound-gender association, and
implicit causality. Our findings indicate that while GPT-2-XL struggles with
the sound-shape task, it demonstrates human-like abilities in both sound-gender
association and implicit causality. Targeted neuron ablation and activation
manipulation reveal a crucial relationship: When GPT-2-XL displays a linguistic
ability, specific neurons correspond to that competence; conversely, the
absence of such an ability indicates a lack of specialized neurons. This study
is the first to utilize psycholinguistic experiments to investigate deep
language competence at the neuron level, providing a new level of granularity
in model interpretability and insights into the internal mechanisms driving
language ability in the transformer-based LLM.",2024-09-24,"Xufeng Duan, Xinyu Zhou, Bei Xiao, Zhenguang G. Cai",http://arxiv.org/pdf/2409.15827v2,cs.CL
60 Data Points are Sufficient to Fine-Tune LLMs for Question-Answering,"Large language models (LLMs) encode extensive world knowledge through
pre-training on massive datasets, which can then be fine-tuned for the
question-answering (QA) task. However, effective strategies for fine-tuning
LLMs for the QA task remain largely unexplored. To address this gap, we
categorize supervised fine-tuning (SFT) data based on the extent of knowledge
memorized by the pretrained LLMs and conduct a series of empirical analyses.
Our experiments, involving four LLMs from three different model families, focus
on three key factors: the amount of data required for SFT, the impact of
different SFT datasets on model performance, and how data requirements vary
across LLMs. The results show that as few as 60 data points during the SFT
stage can activate the knowledge encoded during pre-training, enabling LLMs to
perform the QA task. Additionally, SFT with data of varying memory levels has a
significant impact on LLM performance, with the optimal dataset differing based
on the specific model being fine-tuned. Future research will delve deeper into
the mechanisms underlying these phenomena.",2024-09-24,"Junjie Ye, Yuming Yang, Qi Zhang, Tao Gui, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan",http://arxiv.org/pdf/2409.15825v2,cs.CL
Supervised Fine-Tuning Achieve Rapid Task Adaption Via Alternating Attention Head Activation Patterns,"LLMs' performance on complex tasks is still unsatisfactory. A key issue is
that presently LLMs learn in a data-driven schema, while the instructions about
these complex tasks are both scarce and hard to collect or construct. On the
contrary, a prominent phenomenon is that LLMs can learn rather fast on simpler
tasks with adequate prior knowledge captured during pretraining stage. Thus, if
the prerequisite and mechanism of such rapid generalization could be
elucidated, it could enhance the efficiency and effectiveness of the LLM's
ability to learn complex tasks. Thus, in this paper, we employ a gradient-based
method, to dissect the process that the SFT process adapts LLMs to downstream
tasks via the perspective of attention patterns. We find that: (1) LLMs
selectively activate task-specific attention heads during SFT; (2) activation
patterns for complex tasks are combinations of basic task patterns; and (3)
changes in a few parameters can significantly impact activation patterns after
SFT on a small number of samples.Based on these insights, experiments are
conducted to actually enhance the efficiency and effectiveness of SFT.",2024-09-24,"Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Ting Liu, Bing Qin",http://arxiv.org/pdf/2409.15820v2,cs.CL
"AsthmaBot: Multi-modal, Multi-Lingual Retrieval Augmented Generation For Asthma Patient Support","Asthma rates have risen globally, driven by environmental and lifestyle
factors. Access to immediate medical care is limited, particularly in
developing countries, necessitating automated support systems. Large Language
Models like ChatGPT (Chat Generative Pre-trained Transformer) and Gemini have
advanced natural language processing in general and question answering in
particular, however, they are prone to producing factually incorrect responses
(i.e. hallucinations). Retrieval-augmented generation systems, integrating
curated documents, can improve large language models' performance and reduce
the incidence of hallucination. We introduce AsthmaBot, a multi-lingual,
multi-modal retrieval-augmented generation system for asthma support.
Evaluation of an asthma-related frequently asked questions dataset shows
AsthmaBot's efficacy. AsthmaBot has an added interactive and intuitive
interface that integrates different data modalities (text, images, videos) to
make it accessible to the larger public. AsthmaBot is available online via
\url{asthmabot.datanets.org}.",2024-09-24,"Adil Bahaj, Mounir Ghogho",http://arxiv.org/pdf/2409.15815v1,cs.CL
NER-Luxury: Named entity recognition for the fashion and luxury domain,"In this study, we address multiple challenges of developing a named-entity
recognition model in English for the fashion and luxury industry, namely the
entity disambiguation, French technical jargon in multiple sub-sectors,
scarcity of the ESG methodology, and a disparate company structures of the
sector with small and medium-sized luxury houses to large conglomerate
leveraging economy of scale.
  In this work, we introduce a taxonomy of 36+ entity types with a
luxury-oriented annotation scheme, and create a dataset of more than 40K
sentences respecting a clear hierarchical classification. We also present five
supervised fine-tuned models NER-Luxury for fashion, beauty, watches, jewelry,
fragrances, cosmetics, and overall luxury, focusing equally on the aesthetic
side and the quantitative side.
  In an additional experiment, we compare in a quantitative empirical
assessment of the NER performance of our models against the state-of-the-art
open-source large language models that show promising results and highlights
the benefits of incorporating a bespoke NER model in existing machine learning
pipelines.",2024-09-24,Akim Mousterou,http://arxiv.org/pdf/2409.15804v1,cs.CL
"Small Language Models: Survey, Measurements, and Insights","Small language models (SLMs), despite their widespread adoption in modern
smart devices, have received significantly less academic attention compared to
their large language model (LLM) counterparts, which are predominantly deployed
in data centers and cloud environments. While researchers continue to improve
the capabilities of LLMs in the pursuit of artificial general intelligence, SLM
research aims to make machine intelligence more accessible, affordable, and
efficient for everyday tasks. Focusing on transformer-based, decoder-only
language models with 100M-5B parameters, we survey 70 state-of-the-art
open-source SLMs, analyzing their technical innovations across three axes:
architectures, training datasets, and training algorithms. In addition, we
evaluate their capabilities in various domains, including commonsense
reasoning, mathematics, in-context learning, and long context. To gain further
insight into their on-device runtime costs, we benchmark their inference
latency and memory footprints. Through in-depth analysis of our benchmarking
data, we offer valuable insights to advance research in this field.",2024-09-24,"Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, Mengwei Xu",http://arxiv.org/pdf/2409.15790v3,cs.CL
CHBench: A Chinese Dataset for Evaluating Health in Large Language Models,"With the rapid development of large language models (LLMs), assessing their
performance on health-related inquiries has become increasingly essential. The
use of these models in real-world contexts-where misinformation can lead to
serious consequences for individuals seeking medical advice and
support-necessitates a rigorous focus on safety and trustworthiness. In this
work, we introduce CHBench, the first comprehensive safety-oriented Chinese
health-related benchmark designed to evaluate LLMs' capabilities in
understanding and addressing physical and mental health issues with a safety
perspective across diverse scenarios. CHBench comprises 6,493 entries on mental
health and 2,999 entries on physical health, spanning a wide range of topics.
Our extensive evaluations of four popular Chinese LLMs highlight significant
gaps in their capacity to deliver safe and accurate health information,
underscoring the urgent need for further advancements in this critical domain.
The code is available at https://github.com/TracyGuo2001/CHBench.",2024-09-24,"Chenlu Guo, Nuo Xu, Yi Chang, Yuan Wu",http://arxiv.org/pdf/2409.15766v2,cs.CL
XTRUST: On the Multilingual Trustworthiness of Large Language Models,"Large language models (LLMs) have demonstrated remarkable capabilities across
a range of natural language processing (NLP) tasks, capturing the attention of
both practitioners and the broader public. A key question that now preoccupies
the AI community concerns the capabilities and limitations of these models,
with trustworthiness emerging as a central issue, particularly as LLMs are
increasingly applied in sensitive fields like healthcare and finance, where
errors can have serious consequences. However, most previous studies on the
trustworthiness of LLMs have been limited to a single language, typically the
predominant one in the dataset, such as English. In response to the growing
global deployment of LLMs, we introduce XTRUST, the first comprehensive
multilingual trustworthiness benchmark. XTRUST encompasses a diverse range of
topics, including illegal activities, hallucination, out-of-distribution (OOD)
robustness, physical and mental health, toxicity, fairness, misinformation,
privacy, and machine ethics, across 10 different languages. Using XTRUST, we
conduct an empirical evaluation of the multilingual trustworthiness of five
widely used LLMs, offering an in-depth analysis of their performance across
languages and tasks. Our results indicate that many LLMs struggle with certain
low-resource languages, such as Arabic and Russian, highlighting the
considerable room for improvement in the multilingual trustworthiness of
current language models. The code is available at
https://github.com/LluckyYH/XTRUST.",2024-09-24,"Yahan Li, Yi Wang, Yi Chang, Yuan Wu",http://arxiv.org/pdf/2409.15762v1,cs.CL
Hypothesis Clustering and Merging: Novel MultiTalker Speech Recognition with Speaker Tokens,"In many real-world scenarios, such as meetings, multiple speakers are present
with an unknown number of participants, and their utterances often overlap. We
address these multi-speaker challenges by a novel attention-based
encoder-decoder method augmented with special speaker class tokens obtained by
speaker clustering. During inference, we select multiple recognition hypotheses
conditioned on predicted speaker cluster tokens, and these hypotheses are
merged by agglomerative hierarchical clustering (AHC) based on the normalized
edit distance. The clustered hypotheses result in the multi-speaker
transcriptions with the appropriate number of speakers determined by AHC. Our
experiments on the LibriMix dataset demonstrate that our proposed method was
particularly effective in complex 3-mix environments, achieving a 55% relative
error reduction on clean data and a 36% relative error reduction on noisy data
compared with conventional serialized output training.",2024-09-24,"Yosuke Kashiwagi, Hayato Futami, Emiru Tsunoo, Siddhant Arora, Shinji Watanabe",http://arxiv.org/pdf/2409.15732v1,cs.CL
Federated Large Language Models: Current Progress and Future Directions,"Large language models are rapidly gaining popularity and have been widely
adopted in real-world applications. While the quality of training data is
essential, privacy concerns arise during data collection. Federated learning
offers a solution by allowing multiple clients to collaboratively train LLMs
without sharing local data. However, FL introduces new challenges, such as
model convergence issues due to heterogeneous data and high communication
costs. A comprehensive study is required to address these challenges and guide
future research. This paper surveys Federated learning for LLMs (FedLLM),
highlighting recent advances and future directions. We focus on two key
aspects: fine-tuning and prompt learning in a federated setting, discussing
existing work and associated research challenges. We finally propose potential
research directions for federated LLMs, including pre-training and how LLMs can
further enhance federated learning.",2024-09-24,"Yuhang Yao, Jianyi Zhang, Junda Wu, Chengkai Huang, Yu Xia, Tong Yu, Ruiyi Zhang, Sungchul Kim, Ryan Rossi, Ang Li, Lina Yao, Julian McAuley, Yiran Chen, Carlee Joe-Wong",http://arxiv.org/pdf/2409.15723v1,cs.CL
Making Text Embedders Few-Shot Learners,"Large language models (LLMs) with decoder-only architectures demonstrate
remarkable in-context learning (ICL) capabilities. This feature enables them to
effectively handle both familiar and novel tasks by utilizing examples provided
within their input context. Recognizing the potential of this capability, we
propose leveraging the ICL feature in LLMs to enhance the process of text
embedding generation. To this end, we introduce a novel model bge-en-icl, which
employs few-shot examples to produce high-quality text embeddings. Our approach
integrates task-related examples directly into the query side, resulting in
significant improvements across various tasks. Additionally, we have
investigated how to effectively utilize LLMs as embedding models, including
various attention mechanisms, pooling methods, etc. Our findings suggest that
retaining the original framework often yields the best results, underscoring
that simplicity is best. Experimental results on the MTEB and AIR-Bench
benchmarks demonstrate that our approach sets new state-of-the-art (SOTA)
performance. Our model, code and dataset are freely available at
https://github.com/FlagOpen/FlagEmbedding .",2024-09-24,"Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingxia Shao, Defu Lian, Zheng Liu",http://arxiv.org/pdf/2409.15700v1,cs.CL
Lighter And Better: Towards Flexible Context Adaptation For Retrieval Augmented Generation,"The existing Retrieval-Augmented Generation (RAG) systems face significant
challenges in terms of cost and effectiveness. On one hand, they need to encode
the lengthy retrieved contexts before responding to the input tasks, which
imposes substantial computational overhead. On the other hand, directly using
generic Large Language Models (LLMs) often leads to sub-optimal answers, while
task-specific fine-tuning may compromise the LLMs' general capabilities. To
address these challenges, we introduce a novel approach called FlexRAG
(Flexible Context Adaptation for RAG). In this approach, the retrieved contexts
are compressed into compact embeddings before being encoded by the LLMs.
Simultaneously, these compressed embeddings are optimized to enhance downstream
RAG performance. A key feature of FlexRAG is its flexibility, which enables
effective support for diverse compression ratios and selective preservation of
important contexts. Thanks to these technical designs, FlexRAG achieves
superior generation quality while significantly reducing running costs.
Comprehensive experiments on various question-answering datasets validate our
approach as a cost-effective and flexible solution for RAG systems.",2024-09-24,"Zheng Liu, Chenyuan Wu, Ninglu Shao, Shitao Xiao, Chaozhuo Li, Defu Lian",http://arxiv.org/pdf/2409.15699v1,cs.CL
dnaGrinder: a lightweight and high-capacity genomic foundation model,"The task of understanding and interpreting the complex information encoded
within genomic sequences remains a grand challenge in biological research and
clinical applications. In this context, recent advancements in large language
model research have led to the development of both encoder-only and
decoder-only foundation models designed to decode intricate information in DNA
sequences. However, several issues persist, particularly regarding the
efficient management of long-range dependencies inherent in genomic sequences,
the effective representation of nucleotide variations, and the considerable
computational costs associated with large model architectures and extensive
pretraining datasets. Current genomic foundation models often face a critical
tradeoff: smaller models with mediocre performance versus large models with
improved performance. To address these challenges, we introduce dnaGrinder, a
unique and efficient genomic foundation model. dnaGrinder excels at managing
long-range dependencies within genomic sequences while minimizing computational
costs without compromising performance. It achieves results that are not just
comparable but often superior to leading DNA models such as Nucleotide
Transformer and DNABERT-2. Furthermore, dnaGrinder is designed for easy
fine-tuning on workstation-grade GPUs, accommodating input lengths exceeding
17,000 tokens. On a single high-performance GPU, it supports sequences longer
than 140,000 tokens, making it a highly efficient and accessible tool for both
basic biological research and clinical applications.",2024-09-24,"Qihang Zhao, Chi Zhang, Weixiong Zhang",http://arxiv.org/pdf/2409.15697v1,cs.CL
A Survey of Stance Detection on Social Media: New Directions and Perspectives,"In modern digital environments, users frequently express opinions on
contentious topics, providing a wealth of information on prevailing attitudes.
The systematic analysis of these opinions offers valuable insights for
decision-making in various sectors, including marketing and politics. As a
result, stance detection has emerged as a crucial subfield within affective
computing, enabling the automatic detection of user stances in social media
conversations and providing a nuanced understanding of public sentiment on
complex issues. Recent years have seen a surge of research interest in
developing effective stance detection methods, with contributions from multiple
communities, including natural language processing, web science, and social
computing. This paper provides a comprehensive survey of stance detection
techniques on social media, covering task definitions, datasets, approaches,
and future works. We review traditional stance detection models, as well as
state-of-the-art methods based on large language models, and discuss their
strengths and limitations. Our survey highlights the importance of stance
detection in understanding public opinion and sentiment, and identifies gaps in
current research. We conclude by outlining potential future directions for
stance detection on social media, including the need for more robust and
generalizable models, and the importance of addressing emerging challenges such
as multi-modal stance detection and stance detection in low-resource languages.",2024-09-24,"Bowen Zhang, Genan Dai, Fuqiang Niu, Nan Yin, Xiaomao Fan, Senzhang Wang, Xiaochun Cao, Hu Huang",http://arxiv.org/pdf/2409.15690v2,cs.CL
Language-based Audio Moment Retrieval,"In this paper, we propose and design a new task called audio moment retrieval
(AMR). Unlike conventional language-based audio retrieval tasks that search for
short audio clips from an audio database, AMR aims to predict relevant moments
in untrimmed long audio based on a text query. Given the lack of prior work in
AMR, we first build a dedicated dataset, Clotho-Moment, consisting of
large-scale simulated audio recordings with moment annotations. We then propose
a DETR-based model, named Audio Moment DETR (AM-DETR), as a fundamental
framework for AMR tasks. This model captures temporal dependencies within audio
features, inspired by similar video moment retrieval tasks, thus surpassing
conventional clip-level audio retrieval methods. Additionally, we provide
manually annotated datasets to properly measure the effectiveness and
robustness of our methods on real data. Experimental results show that AM-DETR,
trained with Clotho-Moment, outperforms a baseline model that applies a
clip-level audio retrieval method with a sliding window on all metrics,
particularly improving Recall1@0.7 by 9.00 points. Our datasets and code are
publicly available in
https://h-munakata.github.io/Language-based-Audio-Moment-Retrieval.",2024-09-24,"Hokuto Munakata, Taichi Nishimura, Shota Nakada, Tatsuya Komatsu",http://arxiv.org/pdf/2409.15672v2,cs.CL
Mitigating Semantic Leakage in Cross-lingual Embeddings via Orthogonality Constraint,"Accurately aligning contextual representations in cross-lingual sentence
embeddings is key for effective parallel data mining. A common strategy for
achieving this alignment involves disentangling semantics and language in
sentence embeddings derived from multilingual pre-trained models. However, we
discover that current disentangled representation learning methods suffer from
semantic leakage - a term we introduce to describe when a substantial amount of
language-specific information is unintentionally leaked into semantic
representations. This hinders the effective disentanglement of semantic and
language representations, making it difficult to retrieve embeddings that
distinctively represent the meaning of the sentence. To address this challenge,
we propose a novel training objective, ORthogonAlity Constraint LEarning
(ORACLE), tailored to enforce orthogonality between semantic and language
embeddings. ORACLE builds upon two components: intra-class clustering and
inter-class separation. Through experiments on cross-lingual retrieval and
semantic textual similarity tasks, we demonstrate that training with the ORACLE
objective effectively reduces semantic leakage and enhances semantic alignment
within the embedding space.",2024-09-24,"Dayeon Ki, Cheonbok Park, Hyunjoong Kim",http://arxiv.org/pdf/2409.15664v1,cs.CL
FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL Benchmark,"Text-to-SQL systems have become crucial for translating natural language into
SQL queries in various industries, enabling non-technical users to perform
complex data operations. The need for accurate evaluation methods has increased
as these systems have grown more sophisticated. However, the Execution Accuracy
(EX), the most prevalent evaluation metric, still shows many false positives
and negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel
approach to evaluating text-to-SQL systems using large language models (LLMs)
to emulate human expert-level evaluation of SQL queries. Our metric improves
agreement with human experts (from 62 to 87.04 in Cohen's kappa) with
comprehensive context and sophisticated criteria. Our extensive experiments
yield several key insights: (1) Models' performance increases by over 2.6
points on average, substantially affecting rankings on Spider and BIRD
benchmarks; (2) The underestimation of models in EX primarily stems from
annotation quality issues; and (3) Model performance on particularly
challenging questions tends to be overestimated. This work contributes to a
more accurate and nuanced evaluation of text-to-SQL systems, potentially
reshaping our understanding of state-of-the-art performance in this field.",2024-09-24,"Heegyu Kim, Taeyang Jeon, Seunghwan Choi, Seungtaek Choi, Hyunsouk Cho",http://arxiv.org/pdf/2409.19014v4,cs.CL
M$^2$PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning,"Multimodal Large Language Models (MLLMs) demonstrate remarkable performance
across a wide range of domains, with increasing emphasis on enhancing their
zero-shot generalization capabilities for unseen tasks across various
modalities. Instruction tuning has emerged as an effective strategy for
achieving zero-shot generalization by finetuning pretrained models on diverse
multimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient
finetuning becomes increasingly critical. However, most existing
parameter-efficient approaches focus only on single modalities and often
overlook the multimodal characteristics during finetuning. In this work, we
introduce a novel Multimodal Prompt Tuning (M$^2$PT) approach for efficient
instruction tuning of MLLMs. M$^2$PT effectively integrates visual and textual
prompts into the vision encoder and language processor respectively during
finetuning, facilitating the extraction and alignment of features across
modalities. Empirical results on various multimodal evaluation datasets
demonstrate the superior performance of our approach compared to several
state-of-the-art baselines. A comprehensive set of ablation studies validates
the effectiveness of our prompt design and the efficiency of our approach.",2024-09-24,"Taowen Wang, Yiyang Liu, James Chenhao Liang, junhan zhao, Yiming Cui, Yuning Mao, Shaoliang Nie, Jiahao Liu, Fuli Feng, Zenglin Xu, Cheng Han, Lifu Huang, Qifan Wang, Dongfang Liu",http://arxiv.org/pdf/2409.15657v4,cs.CL
English offensive text detection using CNN based Bi-GRU model,"Over the years, the number of users of social media has increased
drastically. People frequently share their thoughts through social platforms,
and this leads to an increase in hate content. In this virtual community,
individuals share their views, express their feelings, and post photos, videos,
blogs, and more. Social networking sites like Facebook and Twitter provide
platforms to share vast amounts of content with a single click. However, these
platforms do not impose restrictions on the uploaded content, which may include
abusive language and explicit images unsuitable for social media. To resolve
this issue, a new idea must be implemented to divide the inappropriate content.
Numerous studies have been done to automate the process. In this paper, we
propose a new Bi-GRU-CNN model to classify whether the text is offensive or
not. The combination of the Bi-GRU and CNN models outperforms the existing
model.",2024-09-24,"Tonmoy Roy, Md Robiul Islam, Asif Ahammad Miazee, Anika Antara, Al Amin, Sunjim Hossain",http://arxiv.org/pdf/2409.15652v3,cs.CL
Qualitative Insights Tool (QualIT): LLM Enhanced Topic Modeling,"Topic modeling is a widely used technique for uncovering thematic structures
from large text corpora. However, most topic modeling approaches e.g. Latent
Dirichlet Allocation (LDA) struggle to capture nuanced semantics and contextual
understanding required to accurately model complex narratives. Recent
advancements in this area include methods like BERTopic, which have
demonstrated significantly improved topic coherence and thus established a new
standard for benchmarking. In this paper, we present a novel approach, the
Qualitative Insights Tool (QualIT) that integrates large language models (LLMs)
with existing clustering-based topic modeling approaches. Our method leverages
the deep contextual understanding and powerful language generation capabilities
of LLMs to enrich the topic modeling process using clustering. We evaluate our
approach on a large corpus of news articles and demonstrate substantial
improvements in topic coherence and topic diversity compared to baseline topic
modeling techniques. On the 20 ground-truth topics, our method shows 70% topic
coherence (vs 65% & 57% benchmarks) and 95.5% topic diversity (vs 85% & 72%
benchmarks). Our findings suggest that the integration of LLMs can unlock new
opportunities for topic modeling of dynamic and complex text data, as is common
in talent management research contexts.",2024-09-24,"Satya Kapoor, Alex Gil, Sreyoshi Bhaduri, Anshul Mittal, Rutu Mulkar",http://arxiv.org/pdf/2409.15626v1,cs.CL
Improving Academic Skills Assessment with NLP and Ensemble Learning,"This study addresses the critical challenges of assessing foundational
academic skills by leveraging advancements in natural language processing
(NLP). Traditional assessment methods often struggle to provide timely and
comprehensive feedback on key cognitive and linguistic aspects, such as
coherence, syntax, and analytical reasoning. Our approach integrates multiple
state-of-the-art NLP models, including BERT, RoBERTa, BART, DeBERTa, and T5,
within an ensemble learning framework. These models are combined through
stacking techniques using LightGBM and Ridge regression to enhance predictive
accuracy. The methodology involves detailed data preprocessing, feature
extraction, and pseudo-label learning to optimize model performance. By
incorporating sophisticated NLP techniques and ensemble learning, this study
significantly improves the accuracy and efficiency of assessments, offering a
robust solution that surpasses traditional methods and opens new avenues for
educational technology research focused on enhancing core academic
competencies.",2024-09-23,"Xinyi Huang, Yingyi Wu, Danyang Zhang, Jiacheng Hu, Yujian Long",http://arxiv.org/pdf/2409.19013v3,cs.CL
Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents,"Despite broad interest in modeling spoken dialogue agents, most approaches
are inherently ""half-duplex"" -- restricted to turn-based interaction with
responses requiring explicit prompting by the user or implicit tracking of
interruption or silence events. Human dialogue, by contrast, is ""full-duplex""
allowing for rich synchronicity in the form of quick and dynamic turn-taking,
overlapping speech, and backchanneling. Technically, the challenge of achieving
full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs
do not have a sense of ""time"". To bridge this gap, we propose Synchronous LLMs
for full-duplex spoken dialogue modeling. We design a novel mechanism to
integrate time information into Llama3-8b so that they run synchronously with
the real-world clock. We also introduce a training recipe that uses 212k hours
of synthetic spoken dialogue data generated from text dialogue data to create a
model that generates meaningful and natural spoken dialogue, with just 2k hours
of real-world spoken dialogue data. Synchronous LLMs outperform
state-of-the-art in dialogue meaningfulness while maintaining naturalness.
Finally, we demonstrate the model's ability to participate in full-duplex
dialogue by simulating interaction between two agents trained on different
datasets, while considering Internet-scale latencies of up to 240 ms. Webpage:
https://syncllm.cs.washington.edu/.",2024-09-23,"Bandhav Veluri, Benjamin N Peloquin, Bokai Yu, Hongyu Gong, Shyamnath Gollakota",http://arxiv.org/pdf/2409.15594v1,cs.CL
Unveiling the Potential of Graph Neural Networks in SME Credit Risk Assessment,"This paper takes the graph neural network as the technical framework,
integrates the intrinsic connections between enterprise financial indicators,
and proposes a model for enterprise credit risk assessment. The main research
work includes: Firstly, based on the experience of predecessors, we selected 29
enterprise financial data indicators, abstracted each indicator as a vertex,
deeply analyzed the relationships between the indicators, constructed a
similarity matrix of indicators, and used the maximum spanning tree algorithm
to achieve the graph structure mapping of enterprises; secondly, in the
representation learning phase of the mapped graph, a graph neural network model
was built to obtain its embedded representation. The feature vector of each
node was expanded to 32 dimensions, and three GraphSAGE operations were
performed on the graph, with the results pooled using the Pool operation, and
the final output of three feature vectors was averaged to obtain the graph's
embedded representation; finally, a classifier was constructed using a
two-layer fully connected network to complete the prediction task. Experimental
results on real enterprise data show that the model proposed in this paper can
well complete the multi-level credit level estimation of enterprises.
Furthermore, the tree-structured graph mapping deeply portrays the intrinsic
connections of various indicator data of the company, and according to the ROC
and other evaluation criteria, the model's classification effect is significant
and has good ""robustness"".",2024-09-23,"Bingyao Liu, Iris Li, Jianhua Yao, Yuan Chen, Guanming Huang, Jiajing Wang",http://arxiv.org/pdf/2409.17909v1,cs.CL
Optimizing News Text Classification with Bi-LSTM and Attention Mechanism for Efficient Data Processing,"The development of Internet technology has led to a rapid increase in news
information. Filtering out valuable content from complex information has become
an urgentproblem that needs to be solved. In view of the shortcomings of
traditional manual classification methods that are time-consuming and
inefficient, this paper proposes an automaticclassification scheme for news
texts based on deep learning. This solution achieves efficient classification
and management of news texts by introducing advanced machine learning
algorithms, especially an optimization model that combines Bi-directional Long
Short-Term Memory Network (Bi-LSTM) and Attention Mechanism. Experimental
results show that this solution can not only significantly improve the accuracy
and timeliness of classification, but also significantly reduce the need for
manual intervention. It has important practical significance for improving the
information processing capabilities of the news industry and accelerating the
speed of information flow. Through comparative analysis of multiple common
models, the effectiveness and advancement of the proposed method are proved,
laying a solid foundation for future news text classification research.",2024-09-23,"Bingyao Liu, Jiajing Chen, Rui Wang, Junming Huang, Yuanshuai Luo, Jianjun Wei",http://arxiv.org/pdf/2409.15576v1,cs.CL
Asking an AI for salary negotiation advice is a matter of concern: Controlled experimental perturbation of ChatGPT for protected and non-protected group discrimination on a contextual task with no clear ground truth answers,"We conducted controlled experimental bias audits for four versions of
ChatGPT, which we asked to recommend an opening offer in salary negotiations
for a new hire. We submitted 98,800 prompts to each version, systematically
varying the employee's gender, university, and major, and tested prompts in
voice of each side of the negotiation: the employee versus employer. We find
ChatGPT as a multi-model platform is not robust and consistent enough to be
trusted for such a task. We observed statistically significant salary offers
when varying gender for all four models, although with smaller gaps than for
other attributes tested. The largest gaps were different model versions and
between the employee- vs employer-voiced prompts. We also observed substantial
gaps when varying university and major, but many of the biases were not
consistent across model versions. We tested for fictional and fraudulent
universities and found wildly inconsistent results across cases and model
versions. We make broader contributions to the AI/ML fairness literature. Our
scenario and our experimental design differ from mainstream AI/ML auditing
efforts in key ways. Bias audits typically test discrimination for protected
classes like gender, which we contrast with testing non-protected classes of
university and major. Asking for negotiation advice includes how aggressive one
ought to be in a negotiation relative to known empirical salary distributions
and scales, which is a deeply contextual and personalized task that has no
objective ground truth to validate. These results raise concerns for the
specific model versions we tested and ChatGPT as a multi-model platform in
continuous development. Our epistemology does not permit us to definitively
certify these models as either generally biased or unbiased on the attributes
we test, but our study raises matters of concern for stakeholders to further
investigate.",2024-09-23,"R. Stuart Geiger, Flynn O'Sullivan, Elsie Wang, Jonathan Lo",http://arxiv.org/pdf/2409.15567v3,cs.CL
GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation,"The ability to form, retrieve, and reason about memories in response to
stimuli serves as the cornerstone for general intelligence - shaping entities
capable of learning, adaptation, and intuitive insight. Large Language Models
(LLMs) have proven their ability, given the proper memories or context, to
reason and respond meaningfully to stimuli. However, they are still unable to
optimally encode, store, and retrieve memories - the ability to do this would
unlock their full ability to operate as AI agents, and to specialize to niche
domains. To remedy this, one promising area of research is Retrieval Augmented
Generation (RAG), which aims to augment LLMs by providing them with rich
in-context examples and information. In question-answering (QA) applications,
RAG methods embed the text of interest in chunks, and retrieve the most
relevant chunks for a prompt using text embeddings. Motivated by human memory
encoding and retrieval, we aim to improve over standard RAG methods by
generating and encoding higher-level information and tagging the chunks by
their utility to answer questions. We introduce Graphical Eigen Memories For
Retrieval Augmented Generation (GEM-RAG). GEM-RAG works by tagging each chunk
of text in a given text corpus with LLM generated ``utility'' questions,
connecting chunks in a graph based on the similarity of both their text and
utility questions, and then using the eigendecomposition of the memory graph to
build higher level summary nodes that capture the main themes of the text. We
evaluate GEM-RAG, using both UnifiedQA and GPT-3.5 Turbo as the LLMs, with
SBERT, and OpenAI's text encoders on two standard QA tasks, showing that
GEM-RAG outperforms other state-of-the-art RAG methods on these tasks. We also
discuss the implications of having a robust RAG system and future directions.",2024-09-23,"Brendan Hogan Rappazzo, Yingheng Wang, Aaron Ferber, Carla Gomes",http://arxiv.org/pdf/2409.15566v1,cs.CL
Lost in the Logic: An Evaluation of Large Language Models' Reasoning Capabilities on LSAT Logic Games,"In this thesis, I evaluate the performance of Large Language Models (LLMs) on
the Law School Admissions Test (LSAT), specifically the Logic Games section of
the test. I focus on this section because it presents a complex logical
reasoning task and thus is a valuable source of data for evaluating how modern,
increasingly capable LLMs can handle hard logical reasoning tasks. I construct
a dataset of LSAT logic games and their associated metadata, and extensively
evaluate LLMs' performance in a Chain-of-Thought prompting setting. Given the
weak performance in this setting, I explore other prompting frameworks on a
smaller subset of the dataset, adapting ideas from Reflexion to this task. This
results in a substantially improved accuracy of 70 percent for GPT-4 and 46
percent for GPT-3.5 on this data subset, highlighting the capacity of LLMs to
revise their logical errors, despite initially weak performance. Finally, I
analyze the types of logic games that models perform better or worse on, as
well as the types of logical errors I observe from human annotation, providing
detailed insights on the logical reasoning capabilities of LLMs.",2024-09-23,Saumya Malik,http://arxiv.org/pdf/2409.19012v1,cs.CL
"Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction","Annotating and recognizing speech emotion using prompt engineering has
recently emerged with the advancement of Large Language Models (LLMs), yet its
efficacy and reliability remain questionable. In this paper, we conduct a
systematic study on this topic, beginning with the proposal of novel prompts
that incorporate emotion-specific knowledge from acoustics, linguistics, and
psychology. Subsequently, we examine the effectiveness of LLM-based prompting
on Automatic Speech Recognition (ASR) transcription, contrasting it with
ground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize
prompting pipeline for robust LLM-based emotion recognition from spoken
language with ASR errors. Additionally, experiments on context-aware learning,
in-context learning, and instruction tuning are performed to examine the
usefulness of LLM training schemes in this direction. Finally, we investigate
the sensitivity of LLMs to minor prompt variations. Experimental results
demonstrate the efficacy of the emotion-specific prompts, ASR error correction,
and LLM training schemes for LLM-based emotion recognition. Our study aims to
refine the use of LLMs in emotion recognition and related domains.",2024-09-23,"Yuanchao Li, Yuan Gong, Chao-Han Huck Yang, Peter Bell, Catherine Lai",http://arxiv.org/pdf/2409.15551v2,cs.CL
Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet Audio Distance,"The complex nature of musical emotion introduces inherent bias in both
recognition and generation, particularly when relying on a single audio
encoder, emotion classifier, or evaluation metric. In this work, we conduct a
study on Music Emotion Recognition (MER) and Emotional Music Generation (EMG),
employing diverse audio encoders alongside Frechet Audio Distance (FAD), a
reference-free evaluation metric. Our study begins with a benchmark evaluation
of MER, highlighting the limitations of using a single audio encoder and the
disparities observed across different measurements. We then propose assessing
MER performance using FAD derived from multiple encoders to provide a more
objective measure of musical emotion. Furthermore, we introduce an enhanced EMG
approach designed to improve both the variability and prominence of generated
musical emotion, thereby enhancing its realism. Additionally, we investigate
the differences in realism between the emotions conveyed in real and synthetic
music, comparing our EMG model against two baseline models. Experimental
results underscore the issue of emotion bias in both MER and EMG and
demonstrate the potential of using FAD and diverse audio encoders to evaluate
musical emotion more objectively and effectively.",2024-09-23,"Yuanchao Li, Azalea Gui, Dimitra Emmanouilidou, Hannes Gamper",http://arxiv.org/pdf/2409.15545v3,cs.CL
"Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA","Augmenting Large Language Models (LLMs) with information retrieval
capabilities (i.e., Retrieval-Augmented Generation (RAG)) has proven beneficial
for knowledge-intensive tasks. However, understanding users' contextual search
intent when generating responses is an understudied topic for conversational
question answering (QA). This conversational extension leads to additional
concerns when compared to single-turn QA as it is more challenging for systems
to comprehend conversational context and manage retrieved passages over
multiple turns. In this work, we propose a method for enabling LLMs to decide
when to retrieve in RAG settings given a conversational context. When retrieval
is deemed necessary, the LLM then rewrites the conversation for passage
retrieval and judges the relevance of returned passages before response
generation. Operationally, we build on the single-turn SELF-RAG framework (Asai
et al., 2023) and propose SELF-multi-RAG for conversational settings.
SELF-multi-RAG demonstrates improved capabilities over single-turn variants
with respect to retrieving relevant passages (by using summarized
conversational context) and assessing the quality of generated responses.
Experiments on three conversational QA datasets validate the enhanced response
generation capabilities of SELF-multi-RAG, with improvements of ~13% measured
by human annotation.",2024-09-23,"Nirmal Roy, Leonardo F. R. Ribeiro, Rexhina Blloshmi, Kevin Small",http://arxiv.org/pdf/2409.15515v1,cs.CL
"A comprehensive study of on-device NLP applications -- VQA, automated Form filling, Smart Replies for Linguistic Codeswitching","Recent improvement in large language models, open doors for certain new
experiences for on-device applications which were not possible before. In this
work, we propose 3 such new experiences in 2 categories. First we discuss
experiences which can be powered in screen understanding i.e. understanding
whats on user screen namely - (1) visual question answering, and (2) automated
form filling based on previous screen. The second category of experience which
can be extended are smart replies to support for multilingual speakers with
code-switching. Code-switching occurs when a speaker alternates between two or
more languages. To the best of our knowledge, this is first such work to
propose these tasks and solutions to each of them, to bridge the gap between
latest research and real world impact of the research in on-device
applications.",2024-09-23,Naman Goyal,http://arxiv.org/pdf/2409.19010v1,cs.CL
RAM2C: A Liberal Arts Educational Chatbot based on Retrieval-augmented Multi-role Multi-expert Collaboration,"Recently, many studies focus on utilizing large language models (LLMs) into
educational dialogues. Especially, within liberal arts dialogues, educators
must balance \textbf{H}umanized communication, \textbf{T}eaching expertise, and
\textbf{S}afety-ethics (\textbf{HTS}), besides the subject knowledge itself.
However, due to collecting massive amounts of HTS-compliant teaching dialogues
from real world as training corpus is expensive, the outputs of existing LLMs
in teaching dialogues fall short of human standards. To address this, we design
a Retrieval-augmented Multi-role Multi-expert Collaboration (RAM2C) framework
to automatically generate such dialogues data. Specifically, we first establish
HTS-guided knowledge bases, encompassing three domain knowledge in teaching
skills, psychology, and safety ethics. Then, RAM2C organizes LLMs, which are
retrieval-augmented by the above different knowledge bases, into multi-experts
groups with distinct roles to generate the HTS-compliant educational dialogues
dataset. We then fine-tuned the LLMs using this dataset. Empirical evaluations
indicate that RM2C-empowered LLMs excel in Chinese reading teaching, offering
more personalized, and ethically safe teaching response, demonstrating RAM2C's
practicality and high quality. We release the experiments at
\hyperlink{https://github.com/ram2c/ram2c}{https://github.com/ram2c/ram2c}.",2024-09-23,"Haoyu Huang, Tong Niu, Rui Yang, Luping Shi",http://arxiv.org/pdf/2409.15461v1,cs.CL
In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models,"Recent advancements in artificial intelligence have led to the creation of
highly capable large language models (LLMs) that can perform tasks in a
human-like manner. However, LLMs exhibit only infant-level cognitive abilities
in certain areas. One such area is the A-Not-B error, a phenomenon seen in
infants where they repeat a previously rewarded behavior despite well-observed
changed conditions. This highlights their lack of inhibitory control -- the
ability to stop a habitual or impulsive response. In our work, we design a
text-based multi-choice QA scenario similar to the A-Not-B experimental
settings to systematically test the inhibitory control abilities of LLMs. We
found that state-of-the-art LLMs (like Llama3-8b) perform consistently well
with in-context learning (ICL) but make errors and show a significant drop of
as many as 83.3% in reasoning tasks when the context changes trivially. This
suggests that LLMs only have inhibitory control abilities on par with human
infants in this regard, often failing to suppress the previously established
response pattern during ICL.",2024-09-23,"Pengrui Han, Peiyang Song, Haofei Yu, Jiaxuan You",http://arxiv.org/pdf/2409.15454v1,cs.CL
CUTE: Measuring LLMs' Understanding of Their Tokens,"Large Language Models (LLMs) show remarkable performance on a wide variety of
tasks. Most LLMs split text into multi-character tokens and process them as
atomic units without direct access to individual characters. This raises the
question: To what extent can LLMs learn orthographic information? To answer
this, we propose a new benchmark, CUTE, which features a collection of tasks
designed to test the orthographic knowledge of LLMs. We evaluate popular LLMs
on CUTE, finding that most of them seem to know the spelling of their tokens,
yet fail to use this information effectively to manipulate text, calling into
question how much of this knowledge is generalizable.",2024-09-23,"Lukas Edman, Helmut Schmid, Alexander Fraser",http://arxiv.org/pdf/2409.15452v2,cs.CL
A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?,"Large language models (LLMs) have exhibited remarkable capabilities across
various domains and tasks, pushing the boundaries of our knowledge in learning
and cognition. The latest model, OpenAI's o1, stands out as the first LLM with
an internalized chain-of-thought technique using reinforcement learning
strategies. While it has demonstrated surprisingly strong capabilities on
various general language tasks, its performance in specialized fields such as
medicine remains unknown. To this end, this report provides a comprehensive
exploration of o1 on different medical scenarios, examining 3 key aspects:
understanding, reasoning, and multilinguality. Specifically, our evaluation
encompasses 6 tasks using data from 37 medical datasets, including two newly
constructed and more challenging question-answering (QA) tasks based on
professional medical quizzes from the New England Journal of Medicine (NEJM)
and The Lancet. These datasets offer greater clinical relevance compared to
standard medical QA benchmarks such as MedQA, translating more effectively into
real-world clinical utility. Our analysis of o1 suggests that the enhanced
reasoning ability of LLMs may (significantly) benefit their capability to
understand various medical instructions and reason through complex clinical
scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average
of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios.
But meanwhile, we identify several weaknesses in both the model capability and
the existing evaluation protocols, including hallucination, inconsistent
multilingual ability, and discrepant metrics for evaluation. We release our raw
data and model outputs at https://ucsc-vlaa.github.io/o1_medicine/ for future
research.",2024-09-23,"Yunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang, Bingchen Zhao, Yongshuo Zong, Qiao Jin, Cihang Xie, Yuyin Zhou",http://arxiv.org/pdf/2409.15277v1,cs.CL
OmniBench: Towards The Future of Universal Omni-Language Models,"Recent advancements in multimodal large language models (MLLMs) have focused
on integrating multiple modalities, yet their ability to simultaneously process
and reason across different inputs remains underexplored. We introduce
OmniBench, a novel benchmark designed to evaluate models' ability to recognize,
interpret, and reason across visual, acoustic, and textual inputs
simultaneously. We define language models capable of such tri-modal processing
as omni-language models (OLMs). OmniBench features high-quality human
annotations that require integrated understanding across all modalities. Our
evaluation reveals that: i) open-source OLMs show significant limitations in
instruction-following and reasoning in tri-modal contexts; and ii) most
baseline models perform poorly (around 50% accuracy) even with textual
alternatives to image/audio inputs. To address these limitations, we develop
OmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. We
advocate for developing more robust tri-modal integration techniques and
training strategies to enhance OLM performance. Codes and data could be found
at our repo (https://github.com/multimodal-art-projection/OmniBench).",2024-09-23,"Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Zekun Wang, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhenzhu Yang, Xiangzhou Wang, Zhaoxiang Zhang, Zachary Liu, Emmanouil Benetos, Wenhao Huang, Chenghua Lin",http://arxiv.org/pdf/2409.15272v4,cs.CL
Behavioral Bias of Vision-Language Models: A Behavioral Finance View,"Large Vision-Language Models (LVLMs) evolve rapidly as Large Language Models
(LLMs) was equipped with vision modules to create more human-like models.
However, we should carefully evaluate their applications in different domains,
as they may possess undesired biases. Our work studies the potential behavioral
biases of LVLMs from a behavioral finance perspective, an interdisciplinary
subject that jointly considers finance and psychology. We propose an end-to-end
framework, from data collection to new evaluation metrics, to assess LVLMs'
reasoning capabilities and the dynamic behaviors manifested in two established
human financial behavioral biases: recency bias and authority bias. Our
evaluations find that recent open-source LVLMs such as LLaVA-NeXT,
MobileVLM-V2, Mini-Gemini, MiniCPM-Llama3-V 2.5 and Phi-3-vision-128k suffer
significantly from these two biases, while the proprietary model GPT-4o is
negligibly impacted. Our observations highlight directions in which open-source
models can improve. The code is available at
https://github.com/mydcxiao/vlm_behavioral_fin.",2024-09-23,"Yuhang Xiao, Yudi Lin, Ming-Chang Chiu",http://arxiv.org/pdf/2409.15256v1,cs.CL
Archon: An Architecture Search Framework for Inference-Time Techniques,"Inference-time techniques are emerging as highly effective tools to enhance
large language model (LLM) capabilities. However, best practices for developing
systems that combine these techniques remain underdeveloped due to our limited
understanding of the utility of individual inference-time techniques and the
interactions between them. Additionally, efficiently and automatically
searching the space of model choices, inference-time techniques, and their
compositions is challenging due to the large design space. To address these
challenges, we introduce Archon, a modular framework for selecting, combining,
and stacking layers of inference-time techniques to construct optimized LLM
systems for target benchmarks. Rather than relying on a single LLM called once,
we leverage a diverse set of LLMs and inference-time techniques, creating LLM
systems greater than the sum of their parts. Archon defines an extensible
design space, encompassing techniques such as generation ensembling, repeated
sampling, ranking, fusion, critiquing, verification, and unit testing. It
transforms the problem of building LLM systems into a hyperparameter
optimization objective. Given the available LLMs, inference-time techniques,
and compute budget, Archon utilizes hyperparameter search techniques to
discover optimized architectures for target benchmark(s). We evaluate Archon
architectures across a range of instruction-following, reasoning, and coding
benchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval,
MixEval Hard, MATH, and CodeContests. Archon architectures outperform frontier
models, such as GPT-4o and Claude 3.5 Sonnet, on these benchmarks, achieving an
average accuracy increase of 15.1 percentage points by using all available
LLMs. We make our code and datasets available publicly on Github:
https://github.com/ScalingIntelligence/Archon.",2024-09-23,"Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha, E. Kelly Buchanan, Mayee Chen, Neel Guha, Christopher Ré, Azalia Mirhoseini",http://arxiv.org/pdf/2409.15254v5,cs.CL
Fully automatic extraction of morphological traits from the Web: utopia or reality?,"Plant morphological traits, their observable characteristics, are fundamental
to understand the role played by each species within their ecosystem. However,
compiling trait information for even a moderate number of species is a
demanding task that may take experts years to accomplish. At the same time,
massive amounts of information about species descriptions is available online
in the form of text, although the lack of structure makes this source of data
impossible to use at scale. To overcome this, we propose to leverage recent
advances in large language models (LLMs) and devise a mechanism for gathering
and processing information on plant traits in the form of unstructured textual
descriptions, without manual curation. We evaluate our approach by
automatically replicating three manually created species-trait matrices. Our
method managed to find values for over half of all species-trait pairs, with an
F1-score of over 75%. Our results suggest that large-scale creation of
structured trait databases from unstructured online text is currently feasible
thanks to the information extraction capabilities of LLMs, being limited by the
availability of textual descriptions covering all the traits of interest.",2024-09-23,"Diego Marcos, Robert van de Vlasakker, Ioannis N. Athanasiadis, Pierre Bonnet, Hervé Goeau, Alexis Joly, W. Daniel Kissling, César Leblanc, André S. J. van Proosdij, Konstantinos P. Panousis",http://arxiv.org/pdf/2409.17179v2,cs.CL
MADial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue Generation,"Long-term memory is important for chatbots and dialogue systems (DS) to
create consistent and human-like conversations, evidenced by numerous developed
memory-augmented DS (MADS). To evaluate the effectiveness of such MADS,
existing commonly used evaluation metrics, like retrieval accuracy and
perplexity (PPL), mainly focus on query-oriented factualness and language
quality assessment. However, these metrics often lack practical value.
Moreover, the evaluation dimensions are insufficient for human-like assessment
in DS. Regarding memory-recalling paradigms, current evaluation schemes only
consider passive memory retrieval while ignoring diverse memory recall with
rich triggering factors, e.g., emotions and surroundings, which can be
essential in emotional support scenarios. To bridge the gap, we construct a
novel Memory-Augmented Dialogue Benchmark (MADail-Bench) covering various
memory-recalling paradigms based on cognitive science and psychology theories.
The benchmark assesses two tasks separately: memory retrieval and memory
recognition with the incorporation of both passive and proactive memory recall
data. We introduce new scoring criteria to the evaluation, including memory
injection, emotion support (ES) proficiency, and intimacy, to comprehensively
assess generated responses. Results from cutting-edge embedding models and
large language models on this benchmark indicate the potential for further
advancement. Extensive testing further reveals correlations between memory
injection, ES proficiency, and intimacy.",2024-09-23,"Junqing He, Liang Zhu, Rui Wang, Xi Wang, Reza Haffari, Jiaxing Zhang",http://arxiv.org/pdf/2409.15240v2,cs.CL
ASTE Transformer Modelling Dependencies in Aspect-Sentiment Triplet Extraction,"Aspect-Sentiment Triplet Extraction (ASTE) is a recently proposed task of
aspect-based sentiment analysis that consists in extracting (aspect phrase,
opinion phrase, sentiment polarity) triples from a given sentence. Recent
state-of-the-art methods approach this task by first extracting all possible
text spans from a given text, then filtering the potential aspect and opinion
phrases with a classifier, and finally considering all their pairs with another
classifier that additionally assigns sentiment polarity to them. Although
several variations of the above scheme have been proposed, the common feature
is that the final result is constructed by a sequence of independent classifier
decisions. This hinders the exploitation of dependencies between extracted
phrases and prevents the use of knowledge about the interrelationships between
classifier predictions to improve performance. In this paper, we propose a new
ASTE approach consisting of three transformer-inspired layers, which enables
the modelling of dependencies both between phrases and between the final
classifier decisions. Experimental results show that the method achieves higher
performance in terms of F1 measure than other methods studied on popular
benchmarks. In addition, we show that a simple pre-training technique further
improves the performance of the model.",2024-09-23,"Iwo Naglik, Mateusz Lango",http://arxiv.org/pdf/2409.15202v2,cs.CL
Learning from Contrastive Prompts: Automated Optimization and Adaptation,"As LLMs evolve, significant effort is spent on manually crafting prompts.
While existing prompt optimization methods automate this process, they rely
solely on learning from incorrect samples, leading to a sub-optimal
performance. Additionally, an unexplored challenge in the literature is prompts
effective for prior models may not perform well on newer versions or different
languages. We propose the Learning from Contrastive Prompts (LCP) framework to
address these gaps, enhancing both prompt optimization and adaptation. LCP
employs contrastive learning to generate effective prompts by analyzing
patterns in good and bad prompt examples. Our evaluation on the Big-Bench Hard
dataset shows that LCP has a win rate of over 76% over existing methods in
prompt optimization and demonstrates strong adaptability across different model
versions, families, and languages. LCP offers a systematic approach to prompt
engineering, reducing manual effort in deploying LLMs across varied contexts.",2024-09-23,"Mingqi Li, Karan Aggarwal, Yong Xie, Aitzaz Ahmad, Stephen Lau",http://arxiv.org/pdf/2409.15199v1,cs.CL
PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models,"Effective patient-provider communication is crucial in clinical care,
directly impacting patient outcomes and quality of life. Traditional evaluation
methods, such as human ratings, patient feedback, and provider
self-assessments, are often limited by high costs and scalability issues.
Although existing natural language processing (NLP) techniques show promise,
they struggle with the nuances of clinical communication and require sensitive
clinical data for training, reducing their effectiveness in real-world
applications. Emerging large language models (LLMs) offer a new approach to
assessing complex communication metrics, with the potential to advance the
field through integration into passive sensing and just-in-time intervention
systems. This study explores LLMs as evaluators of palliative care
communication quality, leveraging their linguistic, in-context learning, and
reasoning capabilities. Specifically, using simulated scripts crafted and
labeled by healthcare professionals, we test proprietary models (e.g., GPT-4)
and fine-tune open-source LLMs (e.g., LLaMA2) with a synthetic dataset
generated by GPT-4 to evaluate clinical conversations, to identify key metrics
such as `understanding' and `empathy'. Our findings demonstrated LLMs' superior
performance in evaluating clinical communication, providing actionable feedback
with reasoning, and demonstrating the feasibility and practical viability of
developing in-house LLMs. This research highlights LLMs' potential to enhance
patient-provider interactions and lays the groundwork for downstream steps in
developing LLM-empowered clinical health systems.",2024-09-23,"Zhiyuan Wang, Fangxu Yuan, Virginia LeBaron, Tabor Flickinger, Laura E. Barnes",http://arxiv.org/pdf/2409.15188v2,cs.CL
Lessons Learned on Information Retrieval in Electronic Health Records: A Comparison of Embedding Models and Pooling Strategies,"Objective: Applying large language models (LLMs) to the clinical domain is
challenging due to the context-heavy nature of processing medical records.
Retrieval-augmented generation (RAG) offers a solution by facilitating
reasoning over large text sources. However, there are many parameters to
optimize in just the retrieval system alone. This paper presents an ablation
study exploring how different embedding models and pooling methods affect
information retrieval for the clinical domain.
  Methods: Evaluating on three retrieval tasks on two electronic health record
(EHR) data sources, we compared seven models, including medical- and
general-domain models, specialized encoder embedding models, and off-the-shelf
decoder LLMs. We also examine the choice of embedding pooling strategy for each
model, independently on the query and the text to retrieve.
  Results: We found that the choice of embedding model significantly impacts
retrieval performance, with BGE, a comparatively small general-domain model,
consistently outperforming all others, including medical-specific models.
However, our findings also revealed substantial variability across datasets and
query text phrasings. We also determined the best pooling methods for each of
these models to guide future design of retrieval systems.
  Discussion: The choice of embedding model, pooling strategy, and query
formulation can significantly impact retrieval performance and the performance
of these models on other public benchmarks does not necessarily transfer to new
domains. Further studies such as this one are vital for guiding
empirically-grounded development of retrieval frameworks, such as in the
context of RAG, for the clinical domain.",2024-09-23,"Skatje Myers, Timothy A. Miller, Yanjun Gao, Matthew M. Churpek, Anoop Mayampurath, Dmitriy Dligach, Majid Afshar",http://arxiv.org/pdf/2409.15163v1,cs.CL
Inferring Scientific Cross-Document Coreference and Hierarchy with Definition-Augmented Relational Reasoning,"We address the fundamental task of inferring cross-document coreference and
hierarchy in scientific texts, which has important applications in knowledge
graph construction, search, recommendation and discovery. LLMs can struggle
when faced with many long-tail technical concepts with nuanced variations. We
present a novel method which generates context-dependent definitions of concept
mentions by retrieving full-text literature, and uses the definitions to
enhance detection of cross-document relations. We further generate relational
definitions, which describe how two concept mentions are related or different,
and design an efficient re-ranking approach to address the combinatorial
explosion involved in inferring links across papers. In both fine-tuning and
in-context learning settings we achieve large gains in performance. We provide
analysis of generated definitions, shedding light on the relational reasoning
ability of LLMs over fine-grained scientific concepts.",2024-09-23,"Lior Forer, Tom Hope",http://arxiv.org/pdf/2409.15113v2,cs.CL
Efficiently Dispatching Flash Attention For Partially Filled Attention Masks,"Transformers are widely used across various applications, many of which yield
sparse or partially filled attention matrices. Examples include attention masks
designed to reduce the quadratic complexity of attention, sequence packing
techniques, and recent innovations like tree masking for fast validation in
MEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art
algorithm Flash Attention still processes them with quadratic complexity as
though they were dense. In this paper, we introduce Binary Block Masking, a
highly efficient modification that enhances Flash Attention by making it
mask-aware. We further propose two optimizations: one tailored for masks with
contiguous non-zero patterns and another for extremely sparse masks. Our
experiments on attention masks derived from real-world scenarios demonstrate up
to a 9x runtime improvement. The implementation will be publicly released to
foster further research and application.",2024-09-23,"Agniv Sharma, Jonas Geiping",http://arxiv.org/pdf/2409.15097v2,cs.CL
Using Similarity to Evaluate Factual Consistency in Summaries,"Cutting-edge abstractive summarisers generate fluent summaries, but the
factuality of the generated text is not guaranteed. Early summary factuality
evaluation metrics are usually based on n-gram overlap and embedding
similarity, but are reported fail to align with human annotations. Therefore,
many techniques for detecting factual inconsistencies build pipelines around
natural language inference (NLI) or question-answering (QA) models with
additional supervised learning steps. In this paper, we revisit
similarity-based metrics, showing that this failure stems from the comparison
text selection and its granularity. We propose a new zero-shot factuality
evaluation metric, Sentence-BERT Score (SBERTScore), which compares sentences
between the summary and the source document. It outperforms widely-used
word-word metrics including BERTScore and can compete with existing NLI and
QA-based factuality metrics on the benchmark without needing any fine-tuning.
Our experiments indicate that each technique has different strengths, with
SBERTScore particularly effective in identifying correct summaries. We
demonstrate how a combination of techniques is more effective in detecting
various types of error.",2024-09-23,"Yuxuan Ye, Edwin Simpson, Raul Santos Rodriguez",http://arxiv.org/pdf/2409.15090v1,cs.CL
Enhancing Scientific Reproducibility Through Automated BioCompute Object Creation Using Retrieval-Augmented Generation from Publications,"The exponential growth in computational power and accessibility has
transformed the complexity and scale of bioinformatics research, necessitating
standardized documentation for transparency, reproducibility, and regulatory
compliance. The IEEE BioCompute Object (BCO) standard addresses this need but
faces adoption challenges due to the overhead of creating compliant
documentation, especially for legacy research. This paper presents a novel
approach to automate the creation of BCOs from scientific papers using
Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs). We
describe the development of the BCO assistant tool that leverages RAG to
extract relevant information from source papers and associated code
repositories, addressing key challenges such as LLM hallucination and
long-context understanding. The implementation incorporates optimized retrieval
processes, including a two-pass retrieval with re-ranking, and employs
carefully engineered prompts for each BCO domain. We discuss the tool's
architecture, extensibility, and evaluation methods, including automated and
manual assessment approaches. The BCO assistant demonstrates the potential to
significantly reduce the time and effort required for retroactive documentation
of bioinformatics research while maintaining compliance with the standard. This
approach opens avenues for AI-assisted scientific documentation and knowledge
extraction from publications thereby enhancing scientific reproducibility. The
BCO assistant tool and documentation is available at
https://biocompute-objects.github.io/bco-rag/.",2024-09-23,"Sean Kim, Raja Mazumder",http://arxiv.org/pdf/2409.15076v1,cs.CL
Evaluating the Usability of LLMs in Threat Intelligence Enrichment,"Large Language Models (LLMs) have the potential to significantly enhance
threat intelligence by automating the collection, preprocessing, and analysis
of threat data. However, the usability of these tools is critical to ensure
their effective adoption by security professionals. Despite the advanced
capabilities of LLMs, concerns about their reliability, accuracy, and potential
for generating inaccurate information persist. This study conducts a
comprehensive usability evaluation of five LLMs ChatGPT, Gemini, Cohere,
Copilot, and Meta AI focusing on their user interface design, error handling,
learning curve, performance, and integration with existing tools in threat
intelligence enrichment. Utilizing a heuristic walkthrough and a user study
methodology, we identify key usability issues and offer actionable
recommendations for improvement. Our findings aim to bridge the gap between LLM
functionality and user experience, thereby promoting more efficient and
accurate threat intelligence practices by ensuring these tools are
user-friendly and reliable.",2024-09-23,"Sanchana Srikanth, Mohammad Hasanuzzaman, Farah Tasnur Meem",http://arxiv.org/pdf/2409.15072v1,cs.CL
Brotherhood at WMT 2024: Leveraging LLM-Generated Contextual Conversations for Cross-Lingual Image Captioning,"In this paper, we describe our system under the team name Brotherhood for the
English-to-Lowres Multi-Modal Translation Task. We participate in the
multi-modal translation tasks for English-Hindi, English-Hausa,
English-Bengali, and English-Malayalam language pairs. We present a method
leveraging multi-modal Large Language Models (LLMs), specifically GPT-4o and
Claude 3.5 Sonnet, to enhance cross-lingual image captioning without
traditional training or fine-tuning. Our approach utilizes instruction-tuned
prompting to generate rich, contextual conversations about cropped images,
using their English captions as additional context. These synthetic
conversations are then translated into the target languages. Finally, we employ
a weighted prompting strategy, balancing the original English caption with the
translated conversation to generate captions in the target language. This
method achieved competitive results, scoring 37.90 BLEU on the English-Hindi
Challenge Set and ranking first and second for English-Hausa on the Challenge
and Evaluation Leaderboards, respectively. We conduct additional experiments on
a subset of 250 images, exploring the trade-offs between BLEU scores and
semantic similarity across various weighting schemes.",2024-09-23,"Siddharth Betala, Ishan Chokshi",http://arxiv.org/pdf/2409.15052v1,cs.CL
Scaling Laws of Decoder-Only Models on the Multilingual Machine Translation Task,"Recent studies have showcased remarkable capabilities of decoder-only models
in many NLP tasks, including translation. Yet, the machine translation field
has been largely dominated by encoder-decoder models based on the Transformer
architecture. As a consequence, scaling laws of encoder-decoder models for
neural machine translation have already been well studied, but decoder-only
models have received less attention. This work explores the scaling laws of
decoder-only models on the multilingual and multidomain translation task. We
trained a collection of six decoder-only models, ranging from 70M to 7B
parameters, on a sentence-level, multilingual and multidomain dataset. We
conducted a series of experiments showing that the loss of decoder-only models
can be estimated using a scaling law similar to the one discovered for large
language models, but we also show that this scaling law has difficulties to
generalize to too large models or to a different data distribution. We also
study different scaling methods and show that scaling the depth and the width
of a model lead to similar test loss improvements, but with different impact on
the model's efficiency.",2024-09-23,"Gaëtan Caillaut, Raheel Qader, Mariam Nakhlé, Jingshu Liu, Jean-Gabriel Barthélemy",http://arxiv.org/pdf/2409.15051v1,cs.CL
Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP,"CLIP has demonstrated great versatility in adapting to various downstream
tasks, such as image editing and generation, visual question answering, and
video understanding. However, CLIP-based applications often suffer from
misunderstandings regarding user intent, leading to discrepancies between the
required number of objects and the actual outputs in image generation tasks. In
this work, we empirically investigate the quantity bias in CLIP. By carefully
designing different experimental settings and datasets, we comprehensively
evaluate CLIP's understanding of quantity from text, image, and cross-modal
perspectives. Our experimental results reveal a quantity bias in CLIP
embeddings, impacting the reliability of downstream tasks.",2024-09-23,"Zeliang Zhang, Zhuo Liu, Mingqian Feng, Chenliang Xu",http://arxiv.org/pdf/2409.15035v1,cs.CL
Generative LLM Powered Conversational AI Application for Personalized Risk Assessment: A Case Study in COVID-19,"Large language models (LLMs) have shown remarkable capabilities in various
natural language tasks and are increasingly being applied in healthcare
domains. This work demonstrates a new LLM-powered disease risk assessment
approach via streaming human-AI conversation, eliminating the need for
programming required by traditional machine learning approaches. In a COVID-19
severity risk assessment case study, we fine-tune pre-trained generative LLMs
(e.g., Llama2-7b and Flan-t5-xl) using a few shots of natural language
examples, comparing their performance with traditional classifiers (i.e.,
Logistic Regression, XGBoost, Random Forest) that are trained de novo using
tabular data across various experimental settings. We develop a mobile
application that uses these fine-tuned LLMs as its generative AI (GenAI) core
to facilitate real-time interaction between clinicians and patients, providing
no-code risk assessment through conversational interfaces. This integration not
only allows for the use of streaming Questions and Answers (QA) as inputs but
also offers personalized feature importance analysis derived from the LLM's
attention layers, enhancing the interpretability of risk assessments. By
achieving high Area Under the Curve (AUC) scores with a limited number of
fine-tuning samples, our results demonstrate the potential of generative LLMs
to outperform discriminative classification methods in low-data regimes,
highlighting their real-world adaptability and effectiveness. This work aims to
fill the existing gap in leveraging generative LLMs for interactive no-code
risk assessment and to encourage further research in this emerging field.",2024-09-23,"Mohammad Amin Roshani, Xiangyu Zhou, Yao Qiang, Srinivasan Suresh, Steve Hicks, Usha Sethuraman, Dongxiao Zhu",http://arxiv.org/pdf/2409.15027v1,cs.CL
Inference-Friendly Models With MixAttention,"The size of the key-value (KV) cache plays a critical role in determining
both the maximum context length and the number of concurrent requests supported
during inference in modern language models. The KV cache size grows
proportionally with the number of attention heads and the tokens processed,
leading to increased memory consumption and slower inference for long inputs.
In this work, we explore the use of MixAttention, a model architecture
modification closely related to a blog published by Character.AI. MixAttention
combines sliding window attention, where only a small subset of recent tokens
is stored in the KV cache, with KV cache sharing across layers. Our experiments
demonstrate that MixAttention significantly reduces memory usage and improves
inference speed without sacrificing model performance in both short and
long-context tasks. We also explore various configurations of this
architecture, identifying those that maintain quality across evaluation metrics
while optimizing resource efficiency.",2024-09-23,"Shashank Rajput, Ying Sheng, Sean Owen, Vitaliy Chiley",http://arxiv.org/pdf/2409.15012v1,cs.CL
ViBERTgrid BiLSTM-CRF: Multimodal Key Information Extraction from Unstructured Financial Documents,"Multimodal key information extraction (KIE) models have been studied
extensively on semi-structured documents. However, their investigation on
unstructured documents is an emerging research topic. The paper presents an
approach to adapt a multimodal transformer (i.e., ViBERTgrid previously
explored on semi-structured documents) for unstructured financial documents, by
incorporating a BiLSTM-CRF layer. The proposed ViBERTgrid BiLSTM-CRF model
demonstrates a significant improvement in performance (up to 2 percentage
points) on named entity recognition from unstructured documents in financial
domain, while maintaining its KIE performance on semi-structured documents. As
an additional contribution, we publicly released token-level annotations for
the SROIE dataset in order to pave the way for its use in multimodal sequence
labeling models.",2024-09-23,"Furkan Pala, Mehmet Yasin Akpınar, Onur Deniz, Gülşen Eryiğit",http://arxiv.org/pdf/2409.15004v1,cs.CL
Enhancing Aspect-based Sentiment Analysis in Tourism Using Large Language Models and Positional Information,"Aspect-Based Sentiment Analysis (ABSA) in tourism plays a significant role in
understanding tourists' evaluations of specific aspects of attractions, which
is crucial for driving innovation and development in the tourism industry.
However, traditional pipeline models are afflicted by issues such as error
propagation and incomplete extraction of sentiment elements. To alleviate this
issue, this paper proposes an aspect-based sentiment analysis model, ACOS_LLM,
for Aspect-Category-Opinion-Sentiment Quadruple Extraction (ACOSQE). The model
comprises two key stages: auxiliary knowledge generation and ACOSQE. Firstly,
Adalora is used to fine-tune large language models for generating high-quality
auxiliary knowledge. To enhance model efficiency, Sparsegpt is utilized to
compress the fine-tuned model to 50% sparsity. Subsequently, Positional
information and sequence modeling are employed to achieve the ACOSQE task, with
auxiliary knowledge and the original text as inputs. Experiments are conducted
on both self-created tourism datasets and publicly available datasets, Rest15
and Rest16. Results demonstrate the model's superior performance, with an F1
improvement of 7.49% compared to other models on the tourism dataset.
Additionally, there is an F1 improvement of 0.05% and 1.06% on the Rest15 and
Rest16 datasets, respectively.",2024-09-23,"Chun Xu, Mengmeng Wang, Yan Ren, Shaolin Zhu",http://arxiv.org/pdf/2409.14997v1,cs.CL
Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs,"Large Language Models (LLMs) have demonstrated significant potential in
transforming clinical applications. In this study, we investigate the efficacy
of four techniques in adapting LLMs for clinical use-cases: continuous
pretraining, instruct fine-tuning, NEFTune, and prompt engineering. We employ
these methods on Mistral 7B and Mixtral 8x7B models, leveraging a large-scale
clinical pretraining dataset of 50 billion tokens and an instruct fine-tuning
dataset of 500 million tokens. Our evaluation across various clinical tasks
reveals the impact of each technique. While continuous pretraining beyond 250
billion tokens yields marginal improvements on its own, it establishes a strong
foundation for instruct fine-tuning. Notably, NEFTune, designed primarily to
enhance generation quality, surprisingly demonstrates additional gains on our
benchmark. Complex prompt engineering methods further enhance performance.
These findings show the importance of tailoring fine-tuning strategies and
exploring innovative techniques to optimize LLM performance in the clinical
domain.",2024-09-23,"Clément Christophe, Tathagata Raha, Svetlana Maslenkova, Muhammad Umar Salman, Praveen K Kanithi, Marco AF Pimentel, Shadab Khan",http://arxiv.org/pdf/2409.14988v1,cs.CL
Evaluating Theory of (an uncertain) Mind: Predicting the Uncertain Beliefs of Others in Conversation Forecasting,"Typically, when evaluating Theory of Mind, we consider the beliefs of others
to be binary: held or not held. But what if someone is unsure about their own
beliefs? How can we quantify this uncertainty? We propose a new suite of tasks,
challenging language models (LMs) to model the uncertainty of others in
dialogue. We design these tasks around conversation forecasting, wherein an
agent forecasts an unobserved outcome to a conversation. Uniquely, we view
interlocutors themselves as forecasters, asking an LM to predict the
uncertainty of the interlocutors (a probability). We experiment with re-scaling
methods, variance reduction strategies, and demographic context, for this
regression task, conducting experiments on three dialogue corpora (social,
negotiation, task-oriented) with eight LMs. While LMs can explain up to 7%
variance in the uncertainty of others, we highlight the difficulty of the tasks
and room for future work, especially in practical applications, like
anticipating ``false",2024-09-23,"Anthony Sicilia, Malihe Alikhani",http://arxiv.org/pdf/2409.14986v1,cs.CL
Bilingual Rhetorical Structure Parsing with Large Parallel Annotations,"Discourse parsing is a crucial task in natural language processing that aims
to reveal the higher-level relations in a text. Despite growing interest in
cross-lingual discourse parsing, challenges persist due to limited parallel
data and inconsistencies in the Rhetorical Structure Theory (RST) application
across languages and corpora. To address this, we introduce a parallel Russian
annotation for the large and diverse English GUM RST corpus. Leveraging recent
advances, our end-to-end RST parser achieves state-of-the-art results on both
English and Russian corpora. It demonstrates effectiveness in both monolingual
and bilingual settings, successfully transferring even with limited
second-language annotation. To the best of our knowledge, this work is the
first to evaluate the potential of cross-lingual end-to-end RST parsing on a
manually annotated parallel corpus.",2024-09-23,Elena Chistova,http://arxiv.org/pdf/2409.14969v1,cs.CL
Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely,"Large language models (LLMs) augmented with external data have demonstrated
remarkable capabilities in completing real-world tasks. Techniques for
integrating external data into LLMs, such as Retrieval-Augmented Generation
(RAG) and fine-tuning, are gaining increasing attention and widespread
application. Nonetheless, the effective deployment of data-augmented LLMs
across various specialized fields presents substantial challenges. These
challenges encompass a wide range of issues, from retrieving relevant data and
accurately interpreting user intent to fully harnessing the reasoning
capabilities of LLMs for complex tasks. We believe that there is no
one-size-fits-all solution for data-augmented LLM applications. In practice,
underperformance often arises from a failure to correctly identify the core
focus of a task or because the task inherently requires a blend of multiple
capabilities that must be disentangled for better resolution. In this survey,
we propose a RAG task categorization method, classifying user queries into four
levels based on the type of external data required and primary focus of the
task: explicit fact queries, implicit fact queries, interpretable rationale
queries, and hidden rationale queries. We define these levels of queries,
provide relevant datasets, and summarize the key challenges and most effective
techniques for addressing these challenges. Finally, we discuss three main
forms of integrating external data into LLMs: context, small model, and
fine-tuning, highlighting their respective strengths, limitations, and the
types of problems they are suited to solve. This work aims to help readers
thoroughly understand and decompose the data requirements and key bottlenecks
in building LLM applications, offering solutions to the different challenges
and serving as a guide to systematically developing such applications.",2024-09-23,"Siyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He, Luna K. Qiu, Lili Qiu",http://arxiv.org/pdf/2409.14924v1,cs.CL
With Ears to See and Eyes to Hear: Sound Symbolism Experiments with Multimodal Large Language Models,"Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have
demonstrated aptitude as potential substitutes for human participants in
experiments testing psycholinguistic phenomena. However, an understudied
question is to what extent models that only have access to vision and text
modalities are able to implicitly understand sound-based phenomena via abstract
reasoning from orthography and imagery alone. To investigate this, we analyse
the ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise
a non-arbitrary link between sounds and concepts) as well as their ability to
""hear"" via the interplay of the language and vision modules of open and
closed-source multimodal models. We perform multiple experiments, including
replicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism
tasks, and comparing human judgements of linguistic iconicity with that of
LLMs. Our results show that VLMs demonstrate varying levels of agreement with
human labels, and more task information may be required for VLMs versus their
human counterparts for in silico experimentation. We additionally see through
higher maximum agreement levels that Magnitude Symbolism is an easier pattern
for VLMs to identify than Shape Symbolism, and that an understanding of
linguistic iconicity is highly dependent on model size.",2024-09-23,"Tyler Loakman, Yucheng Li, Chenghua Lin",http://arxiv.org/pdf/2409.14917v2,cs.CL
Towards a Realistic Long-Term Benchmark for Open-Web Research Agents,"We present initial results of a forthcoming benchmark for evaluating LLM
agents on white-collar tasks of economic value. We evaluate agents on
real-world ""messy"" open-web research tasks of the type that are routine in
finance and consulting. In doing so, we lay the groundwork for an LLM agent
evaluation suite where good performance directly corresponds to a large
economic and societal impact. We built and tested several agent architectures
with o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.
On average, LLM agents powered by Claude-3.5 Sonnet and o1-preview
substantially outperformed agents using GPT-4o, with agents based on Llama 3.1
(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct
architecture with the ability to delegate subtasks to subagents performed best.
In addition to quantitative evaluations, we qualitatively assessed the
performance of the LLM agents by inspecting their traces and reflecting on
their observations. Our evaluation represents the first in-depth assessment of
agents' abilities to conduct challenging, economically valuable analyst-style
research on the real open web.",2024-09-23,"Peter Mühlbacher, Nikos I. Bosse, Lawrence Phillips",http://arxiv.org/pdf/2409.14913v2,cs.CL
Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization,"In mental health counseling, condensing dialogues into concise and relevant
summaries (aka counseling notes) holds pivotal significance. Large Language
Models (LLMs) exhibit remarkable capabilities in various generative tasks;
however, their adaptation to domain-specific intricacies remains challenging,
especially within mental health contexts. Unlike standard LLMs, mental health
experts first plan to apply domain knowledge in writing summaries. Our work
enhances LLMs' ability by introducing a novel planning engine to orchestrate
structuring knowledge alignment. To achieve high-order planning, we divide
knowledge encapsulation into two major phases: (i) holding dialogue structure
and (ii) incorporating domain-specific knowledge. We employ a planning engine
on Llama-2, resulting in a novel framework, PIECE. Our proposed system employs
knowledge filtering-cum-scaffolding to encapsulate domain knowledge.
Additionally, PIECE leverages sheaf convolution learning to enhance its
understanding of the dialogue's structural nuances. We compare PIECE with 14
baseline methods and observe a significant improvement across ROUGE and Bleurt
scores. Further, expert evaluation and analyses validate the generation quality
to be effective, sometimes even surpassing the gold standard. We further
benchmark PIECE with other LLMs and report improvement, including Llama-2
(+2.72%), Mistral (+2.04%), and Zephyr (+1.59%), to justify the
generalizability of the planning engine.",2024-09-23,"Aseem Srivastava, Smriti Joshi, Tanmoy Chakraborty, Md Shad Akhtar",http://arxiv.org/pdf/2409.14907v1,cs.CL
DSG-KD: Knowledge Distillation from Domain-Specific to General Language Models,"The use of pre-trained language models fine-tuned to address specific
downstream tasks is a common approach in natural language processing (NLP).
However, acquiring domain-specific knowledge via fine-tuning is challenging.
Traditional methods involve pretraining language models using vast amounts of
domain-specific data before fine-tuning for particular tasks. This study
investigates emergency/non-emergency classification tasks based on electronic
medical record (EMR) data obtained from pediatric emergency departments (PEDs)
in Korea. Our findings reveal that existing domain-specific pre-trained
language models underperform compared to general language models in handling
N-lingual free-text data characteristics of non-English-speaking regions. To
address these limitations, we propose a domain knowledge transfer methodology
that leverages knowledge distillation to infuse general language models with
domain-specific knowledge via fine-tuning. This study demonstrates the
effective transfer of specialized knowledge between models by defining a
general language model as the student model and a domain-specific pre-trained
model as the teacher model. In particular, we address the complexities of EMR
data obtained from PEDs in non-English-speaking regions, such as Korea, and
demonstrate that the proposed method enhances classification performance in
such contexts. The proposed methodology not only outperforms baseline models on
Korean PED EMR data, but also promises broader applicability in various
professional and technical domains. In future works, we intend to extend this
methodology to include diverse non-English-speaking regions and address
additional downstream tasks, with the aim of developing advanced model
architectures using state-of-the-art KD techniques. The code is available in
https://github.com/JoSangYeon/DSG-KD.",2024-09-23,"Sangyeon Cho, Jangyeong Jeon, Dongjoon Lee, Changhee Lee, Junyeong Kim",http://arxiv.org/pdf/2409.14904v1,cs.CL
End-to-End Graph Flattening Method for Large Language Models,"In recent years, the breakthrough of Large Language Models (LLMs) offers new
ideas for achieving universal methods on graph data. The common practice of
converting graphs into natural language for LLMs, which refers to graph
flattening, exhibits good generalizability and interpretability. However, the
poor organization of the textual format results in poor performance in
long-distance scenario understanding. Inspired by human cognitive reasoning
habits, we propose a novel method for graph flattening to fit LLMs, termed as
End-to-End DAG-Path prompting (EEDP). Experiments on real-world datasets show
that EEDP enhances the reasoning performance of LLMs in long-distance scenarios
while maintaining excellent performance in short-distance scenarios,
demonstrating good robustness in the face of distance variations.",2024-09-23,"Bin Hong, Jinze Wu, Jiayu Liu, Liang Ding, Jing Sha, Kai Zhang, Shijin Wang, Zhenya Huang",http://arxiv.org/pdf/2409.14880v1,cs.CL
Privacy Policy Analysis through Prompt Engineering for LLMs,"Privacy policies are often obfuscated by their complexity, which impedes
transparency and informed consent. Conventional machine learning approaches for
automatically analyzing these policies demand significant resources and
substantial domain-specific training, causing adaptability issues. Moreover,
they depend on extensive datasets that may require regular maintenance due to
changing privacy concerns.
  In this paper, we propose, apply, and assess PAPEL (Privacy Policy Analysis
through Prompt Engineering for LLMs), a framework harnessing the power of Large
Language Models (LLMs) through prompt engineering to automate the analysis of
privacy policies. PAPEL aims to streamline the extraction, annotation, and
summarization of information from these policies, enhancing their accessibility
and comprehensibility without requiring additional model training. By
integrating zero-shot, one-shot, and few-shot learning approaches and the
chain-of-thought prompting in creating predefined prompts and prompt templates,
PAPEL guides LLMs to efficiently dissect, interpret, and synthesize the
critical aspects of privacy policies into user-friendly summaries. We
demonstrate the effectiveness of PAPEL with two applications: (i) annotation
and (ii) contradiction analysis. We assess the ability of several LLaMa and GPT
models to identify and articulate data handling practices, offering insights
comparable to existing automated analysis approaches while reducing training
efforts and increasing the adaptability to new analytical needs. The
experiments demonstrate that the LLMs PAPEL utilizes (LLaMA and Chat GPT
models) achieve robust performance in privacy policy annotation, with F1 scores
reaching 0.8 and above (using the OPP-115 gold standard), underscoring the
effectiveness of simpler prompts across various advanced language models.",2024-09-23,"Arda Goknil, Femke B. Gelderblom, Simeon Tverdal, Shukun Tokas, Hui Song",http://arxiv.org/pdf/2409.14879v1,cs.CL
The ParlaSpeech Collection of Automatically Generated Speech and Text Datasets from Parliamentary Proceedings,"Recent significant improvements in speech and language technologies come both
from self-supervised approaches over raw language data as well as various types
of explicit supervision. To ensure high-quality processing of spoken data, the
most useful type of explicit supervision is still the alignment between the
speech signal and its corresponding text transcript, which is a data type that
is not available for many languages. In this paper, we present our approach to
building large and open speech-and-text-aligned datasets of less-resourced
languages based on transcripts of parliamentary proceedings and their
recordings. Our starting point are the ParlaMint comparable corpora of
transcripts of parliamentary proceedings of 26 national European parliaments.
In the pilot run on expanding the ParlaMint corpora with aligned publicly
available recordings, we focus on three Slavic languages, namely Croatian,
Polish, and Serbian. The main challenge of our approach is the lack of any
global alignment between the ParlaMint texts and the available recordings, as
well as the sometimes varying data order in each of the modalities, which
requires a novel approach in aligning long sequences of text and audio in a
large search space. The results of this pilot run are three high-quality
datasets that span more than 5,000 hours of speech and accompanying text
transcripts. Although these datasets already make a huge difference in the
availability of spoken and textual data for the three languages, we want to
emphasize the potential of the presented approach in building similar datasets
for many more languages.",2024-09-23,"Nikola Ljubešić, Peter Rupnik, Danijel Koržinek",http://arxiv.org/pdf/2409.15397v2,cs.CL
HW-TSC's Submission to the CCMT 2024 Machine Translation Tasks,"This paper presents the submission of Huawei Translation Services Center
(HW-TSC) to machine translation tasks of the 20th China Conference on Machine
Translation (CCMT 2024). We participate in the bilingual machine translation
task and multi-domain machine translation task. For these two translation
tasks, we use training strategies such as regularized dropout, bidirectional
training, data diversification, forward translation, back translation,
alternated training, curriculum learning, and transductive ensemble learning to
train neural machine translation (NMT) models based on the deep Transformer-big
architecture. Furthermore, to explore whether large language model (LLM) can
help improve the translation quality of NMT systems, we use supervised
fine-tuning to train llama2-13b as an Automatic post-editing (APE) model to
improve the translation results of the NMT model on the multi-domain machine
translation task. By using these plyometric strategies, our submission achieves
a competitive result in the final evaluation.",2024-09-23,"Zhanglin Wu, Yuanchang Luo, Daimeng Wei, Jiawei Zheng, Bin Wei, Zongyao Li, Hengchao Shang, Jiaxin Guo, Shaojun Li, Weidong Zhang, Ning Xie, Hao Yang",http://arxiv.org/pdf/2409.14842v3,cs.CL
Orthogonal Finetuning for Direct Preference Optimization,"DPO is an effective preference optimization algorithm. However, the DPO-tuned
models tend to overfit on the dispreferred samples, manifested as overly long
generations lacking diversity. While recent regularization approaches have
endeavored to alleviate this issue by modifying the objective function, they
achieved that at the cost of alignment performance degradation. In this paper,
we innovatively incorporate regularization from the perspective of weight
updating to curb alignment overfitting. Through the pilot experiment, we
discovered that there exists a positive correlation between overfitting and the
hyperspherical energy fluctuation. Hence, we introduce orthogonal finetuning
for DPO via a weight-Rotated Preference Optimization (RoPO) method, which
merely conducts rotational and magnitude-stretching updates on the weight
parameters to maintain the hyperspherical energy invariant, thereby preserving
the knowledge encoded in the angle between neurons. Extensive experiments
demonstrate that our model aligns perfectly with human preferences while
retaining the original expressive capacity using only 0.0086% of the trainable
parameters, suggesting an effective regularization against overfitting.
Specifically, RoPO outperforms DPO by up to 10 points on MT-Bench and by up to
2.8 points on AlpacaEval 2, while enhancing the generation diversity by an
average of 6 points.",2024-09-23,"Chenxu Yang, Ruipeng Jia, Naibin Gu, Zheng Lin, Siyuan Chen, Chao Pang, Weichong Yin, Yu Sun, Hua Wu, Weiping Wang",http://arxiv.org/pdf/2409.14836v2,cs.CL
ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback,"Recently, tool-augmented LLMs have gained increasing attention. Given an
instruction, tool-augmented LLMs can interact with various external tools in
multiple rounds and provide a final answer. However, previous LLMs were trained
on overly detailed instructions, which included API names or parameters, while
real users would not explicitly mention these API details. This leads to a gap
between trained LLMs and real-world scenarios. In addition, most works ignore
whether the interaction process follows the instruction. To address these
issues, we constructed a training dataset called MGToolBench, which contains
statement and category-level instructions to better reflect real-world
scenarios. In addition, we propose ToolPlanner, a two-stage reinforcement
learning framework that utilizes path planning and two feedback mechanisms to
enhance the LLM's task completion and instruction-following capabilities.
Experimental results show that ToolPlanner significantly improves the Match
Rate, Pass Rate and Win Rate by 26.8%, 20.2%, and 5.6% compared to the SOTA
model. Human evaluation verifies that the multi-granularity instructions can
better align with users' usage habits. Our data and code will be released upon
acceptance.",2024-09-23,"Qinzhuo Wu, Wei Liu, Jian Luan, Bin Wang",http://arxiv.org/pdf/2409.14826v3,cs.CL
Past Meets Present: Creating Historical Analogy with Large Language Models,"Historical analogies, which compare known past events with contemporary but
unfamiliar events, are important abilities that help people make decisions and
understand the world. However, research in applied history suggests that people
have difficulty finding appropriate analogies. And previous studies in the AI
community have also overlooked historical analogies. To fill this gap, in this
paper, we focus on the historical analogy acquisition task, which aims to
acquire analogous historical events for a given event. We explore retrieval and
generation methods for acquiring historical analogies based on different large
language models (LLMs). Furthermore, we propose a self-reflection method to
mitigate hallucinations and stereotypes when LLMs generate historical
analogies. Through human evaluations and our specially designed automatic
multi-dimensional assessment, we find that LLMs generally have a good potential
for historical analogies. And the performance of the models can be further
improved by using our self-reflection method.",2024-09-23,"Nianqi Li, Siyu Yuan, Jiangjie Chen, Jiaqing Liang, Feng Wei, Zujie Liang, Deqing Yang, Yanghua Xiao",http://arxiv.org/pdf/2409.14820v1,cs.CL
MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding,"Recently, mobile AI agents based on VLMs have been gaining increasing
attention. These works typically utilize VLM as a foundation, fine-tuning it
with instruction-based mobile datasets. However, these VLMs are typically
pre-trained on general-domain data, which often results in a lack of
fundamental capabilities specific to the mobile domain. Therefore, they may
struggle to recognize specific UI elements and understand intra-UI fine-grained
information. In addition, the current fine-tuning task focuses on interacting
with the most relevant element for the given instruction. These fine-tuned VLMs
may still ignore the relationships between UI pages, neglect the roles of
elements in page transitions and lack inter-UI understanding. To address
issues, we propose a VLM called MobileVLM, which includes two additional
pre-training stages to enhance both intra- and inter-UI understanding. We
defined four UI-based pre-training tasks, enabling the model to better perceive
fine-grained elements and capture page transition actions. To address the lack
of mobile pre-training data, we built a large Chinese mobile dataset Mobile3M
from scratch, which contains 3 million UI pages, and real-world transition
actions, forming a directed graph structure. Experimental results show
MobileVLM excels on both our test set and public mobile benchmarks,
outperforming existing VLMs.",2024-09-23,"Qinzhuo Wu, Weikai Xu, Wei Liu, Tao Tan, Jianfeng Liu, Ang Li, Jian Luan, Bin Wang, Shuo Shang",http://arxiv.org/pdf/2409.14818v2,cs.CL
MTP: A Dataset for Multi-Modal Turning Points in Casual Conversations,"Detecting critical moments, such as emotional outbursts or changes in
decisions during conversations, is crucial for understanding shifts in human
behavior and their consequences. Our work introduces a novel problem setting
focusing on these moments as turning points (TPs), accompanied by a
meticulously curated, high-consensus, human-annotated multi-modal dataset. We
provide precise timestamps, descriptions, and visual-textual evidence
high-lighting changes in emotions, behaviors, perspectives, and decisions at
these turning points. We also propose a framework, TPMaven, utilizing
state-of-the-art vision-language models to construct a narrative from the
videos and large language models to classify and detect turning points in our
multi-modal dataset. Evaluation results show that TPMaven achieves an F1-score
of 0.88 in classification and 0.61 in detection, with additional explanations
aligning with human expectations.",2024-09-23,"Gia-Bao Dinh Ho, Chang Wei Tan, Zahra Zamanzadeh Darban, Mahsa Salehi, Gholamreza Haffari, Wray Buntine",http://arxiv.org/pdf/2409.14801v1,cs.CL
Towards Efficient and Robust VQA-NLE Data Generation with Large Vision-Language Models,"Natural Language Explanation (NLE) aims to elucidate the decision-making
process by providing detailed, human-friendly explanations in natural language.
It helps demystify the decision-making processes of large vision-language
models (LVLMs) through the use of language models. While existing methods for
creating a Vision Question-Answering with Natural Language Explanation
(VQA-NLE) datasets can provide explanations, they heavily rely on human
annotations that are time-consuming and costly. In this study, we propose a
novel approach that leverages LVLMs to efficiently generate high-quality
synthetic VQA-NLE datasets. By evaluating our synthetic data, we showcase how
advanced prompting techniques can lead to the production of high-quality
VQA-NLE data. Our findings indicate that this proposed method achieves up to
20x faster than human annotation, with only a minimal decrease in qualitative
metrics, achieving robust quality that is nearly equivalent to human-annotated
data. Furthermore, we show that incorporating visual prompts significantly
enhances the relevance of text generation. Our study paves the way for a more
efficient and robust automated generation of multi-modal NLE data, offering a
promising solution to the problem.",2024-09-23,"Patrick Amadeus Irawan, Genta Indra Winata, Samuel Cahyawijaya, Ayu Purwarianti",http://arxiv.org/pdf/2409.14785v2,cs.CL
Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method,"As the scale of training corpora for large language models (LLMs) grows,
model developers become increasingly reluctant to disclose details on their
data. This lack of transparency poses challenges to scientific evaluation and
ethical deployment. Recently, pretraining data detection approaches, which
infer whether a given text was part of an LLM's training data through black-box
access, have been explored. The Min-K\% Prob method, which has achieved
state-of-the-art results, assumes that a non-training example tends to contain
a few outlier words with low token probabilities. However, the effectiveness
may be limited as it tends to misclassify non-training texts that contain many
common words with high probabilities predicted by LLMs. To address this issue,
we introduce a divergence-based calibration method, inspired by the
divergence-from-randomness concept, to calibrate token probabilities for
pretraining data detection. We compute the cross-entropy (i.e., the divergence)
between the token probability distribution and the token frequency distribution
to derive a detection score. We have developed a Chinese-language benchmark,
PatentMIA, to assess the performance of detection approaches for LLMs on
Chinese text. Experimental results on English-language benchmarks and PatentMIA
demonstrate that our proposed method significantly outperforms existing
methods. Our code and PatentMIA benchmark are available at
https://github.com/zhang-wei-chao/DC-PDD.",2024-09-23,"Weichao Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng",http://arxiv.org/pdf/2409.14781v6,cs.CL
OMPar: Automatic Parallelization with AI-Driven Source-to-Source Compilation,"Manual parallelization of code remains a significant challenge due to the
complexities of modern software systems and the widespread adoption of
multi-core architectures. This paper introduces OMPar, an AI-driven tool
designed to automate the parallelization of C/C++ code using OpenMP pragmas.
OMPar integrates Large Language Models (LLMs) through two key components:
OMPify, which assesses loop parallelization potential, and MonoCoder-OMP, a new
fine-tuned model which generates precise OpenMP pragmas. The evaluation of
OMPar follows the same rigorous process applied to traditional tools like
source-to-source AutoPar and ICPC compilers: (1) ensuring the generated code
compiles and runs correctly in serial form, (2) assessing performance with the
gradual addition of threads and corresponding physical cores, and (3) verifying
and validating the correctness of the code's output. Benchmarks from HeCBench
and ParEval are used to evaluate accuracy and performance. Experimental results
demonstrate that OMPar significantly outperforms traditional methods, achieving
higher accuracy in identifying parallelizable loops and generating efficient
pragmas. Beyond accuracy, OMPar offers advantages such as the ability to work
on partial or incomplete codebases and the capacity to continuously learn from
new code patterns, enhancing its parallelization capabilities over time. These
results underscore the potential of LLMs in revolutionizing automatic
parallelization techniques, paving the way for more efficient and scalable
parallel computing systems.",2024-09-23,"Tal Kadosh, Niranjan Hasabnis, Prema Soundararajan, Vy A. Vo, Mihai Capota, Nesreen Ahmed, Yuval Pinter, Gal Oren",http://arxiv.org/pdf/2409.14771v1,cs.CL
Language-Agnostic Analysis of Speech Depression Detection,"The people with Major Depressive Disorder (MDD) exhibit the symptoms of tonal
variations in their speech compared to the healthy counterparts. However, these
tonal variations not only confine to the state of MDD but also on the language,
which has unique tonal patterns. This work analyzes automatic speech-based
depression detection across two languages, English and Malayalam, which
exhibits distinctive prosodic and phonemic characteristics. We propose an
approach that utilizes speech data collected along with self-reported labels
from participants reading sentences from IViE corpus, in both English and
Malayalam. The IViE corpus consists of five sets of sentences: simple
sentences, WH-questions, questions without morphosyntactic markers, inversion
questions and coordinations, that can naturally prompt speakers to speak in
different tonal patterns. Convolutional Neural Networks (CNNs) are employed for
detecting depression from speech. The CNN model is trained to identify acoustic
features associated with depression in speech, focusing on both languages. The
model's performance is evaluated on the collected dataset containing recordings
from both depressed and non-depressed speakers, analyzing its effectiveness in
detecting depression across the two languages. Our findings and collected data
could contribute to the development of language-agnostic speech-based
depression detection systems, thereby enhancing accessibility for diverse
populations.",2024-09-23,"Sona Binu, Jismi Jose, Fathima Shimna K V, Alino Luke Hans, Reni K. Cherian, Starlet Ben Alex, Priyanka Srivastava, Chiranjeevi Yarra",http://arxiv.org/pdf/2409.14769v1,cs.CL
Do Large Language Models have Problem-Solving Capability under Incomplete Information Scenarios?,"The evaluation of the problem-solving capability under incomplete information
scenarios of Large Language Models (LLMs) is increasingly important,
encompassing capabilities such as questioning, knowledge search, error
detection, and path planning. Current research mainly focus on LLMs'
problem-solving capability such as ``Twenty Questions''. However, these kinds
of games do not require recognizing misleading cues which are necessary in the
incomplete information scenario. Moreover, the existing game such as ``Who is
undercover'' are highly subjective, making it challenging for evaluation.
Therefore, in this paper, we introduce a novel game named BrainKing based on
the ``Who is undercover'' and ``Twenty Questions'' for evaluating LLM
capabilities under incomplete information scenarios. It requires LLMs to
identify target entities with limited yes-or-no questions and potential
misleading answers. By setting up easy, medium, and hard difficulty modes, we
comprehensively assess the performance of LLMs across various aspects. Our
results reveal the capabilities and limitations of LLMs in BrainKing, providing
significant insights of LLM problem-solving levels.",2024-09-23,"Yuyan Chen, Tianhao Yu, Yueze Li, Songzhou Yan, Sijia Liu, Jiaqing Liang, Yanghua Xiao",http://arxiv.org/pdf/2409.14762v1,cs.CL
FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension,"Referring Expression Comprehension (REC) is a crucial cross-modal task that
objectively evaluates the capabilities of language understanding, image
comprehension, and language-to-image grounding. Consequently, it serves as an
ideal testing ground for Multi-modal Large Language Models (MLLMs). In pursuit
of this goal, we have established a new REC dataset characterized by two key
features: Firstly, it is designed with controllable varying levels of
difficulty, necessitating multi-level fine-grained reasoning across object
categories, attributes, and multi-hop relationships. Secondly, it includes
negative text and images created through fine-grained editing and generation
based on existing data, thereby testing the model's ability to correctly reject
scenarios where the target object is not visible in the image--an essential
aspect often overlooked in existing datasets and approaches. Utilizing this
high-quality dataset, we conducted comprehensive evaluations of both
state-of-the-art specialist models and MLLMs. Our findings indicate that there
remains a significant gap in achieving satisfactory grounding performance. We
anticipate that our dataset will inspire new approaches to enhance visual
reasoning and develop more advanced cross-modal interaction strategies,
ultimately unlocking the full potential of MLLMs. Our code and the datasets are
available at https://github.com/liujunzhuo/FineCops-Ref.",2024-09-23,"Junzhuo Liu, Xuzheng Yang, Weiwei Li, Peng Wang",http://arxiv.org/pdf/2409.14750v2,cs.CL
LINKAGE: Listwise Ranking among Varied-Quality References for Non-Factoid QA Evaluation via LLMs,"Non-Factoid (NF) Question Answering (QA) is challenging to evaluate due to
diverse potential answers and no objective criterion. The commonly used
automatic evaluation metrics like ROUGE or BERTScore cannot accurately measure
semantic similarities or answers from different perspectives. Recently, Large
Language Models (LLMs) have been resorted to for NFQA evaluation due to their
compelling performance on various NLP tasks. Common approaches include
pointwise scoring of each candidate answer and pairwise comparisons between
answers. Inspired by the evolution from pointwise to pairwise to listwise in
learning-to-rank methods, we propose a novel listwise NFQA evaluation approach,
that utilizes LLMs to rank candidate answers in a list of reference answers
sorted by descending quality. Moreover, for NF questions that do not have
multi-grade or any golden answers, we leverage LLMs to generate the reference
answer list of various quality to facilitate the listwise evaluation. Extensive
experimental results on three NFQA datasets, i.e., ANTIQUE, the TREC-DL-NF, and
WebGLM show that our method has significantly higher correlations with human
annotations compared to automatic scores and common pointwise and pairwise
approaches.",2024-09-23,"Sihui Yang, Keping Bi, Wanqing Cui, Jiafeng Guo, Xueqi Cheng",http://arxiv.org/pdf/2409.14744v2,cs.CL
ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information,"In different NLP tasks, detecting harmful content is crucial for online
environments, especially with the growing influence of social media. However,
previous research has two main issues: 1) a lack of data in low-resource
settings, and 2) inconsistent definitions and criteria for judging harmful
content, requiring classification models to be robust to spurious features and
diverse. We propose Toxicraft, a novel framework for synthesizing datasets of
harmful information to address these weaknesses. With only a small amount of
seed data, our framework can generate a wide variety of synthetic, yet
remarkably realistic, examples of toxic information. Experimentation across
various datasets showcases a notable enhancement in detection model robustness
and adaptability, surpassing or close to the gold labels.",2024-09-23,"Zheng Hui, Zhaoxiao Guo, Hang Zhao, Juanyong Duan, Congrui Huang",http://arxiv.org/pdf/2409.14740v2,cs.CL
Parse Trees Guided LLM Prompt Compression,"Offering rich contexts to Large Language Models (LLMs) has shown to boost the
performance in various tasks, but the resulting longer prompt would increase
the computational cost and might exceed the input limit of LLMs. Recently, some
prompt compression methods have been suggested to shorten the length of prompts
by using language models to generate shorter prompts or by developing
computational models to select important parts of original prompt. The
generative compression methods would suffer from issues like hallucination,
while the selective compression methods have not involved linguistic rules and
overlook the global structure of prompt. To this end, we propose a novel
selective compression method called PartPrompt. It first obtains a parse tree
for each sentence based on linguistic rules, and calculates local information
entropy for each node in a parse tree. These local parse trees are then
organized into a global tree according to the hierarchical structure such as
the dependency of sentences, paragraphs, and sections. After that, the
root-ward propagation and leaf-ward propagation are proposed to adjust node
values over the global tree. Finally, a recursive algorithm is developed to
prune the global tree based on the adjusted node values. The experiments show
that PartPrompt receives the state-of-the-art performance across various
datasets, metrics, compression ratios, and target LLMs for inference. The
in-depth ablation studies confirm the effectiveness of designs in PartPrompt,
and other additional experiments also demonstrate its superiority in terms of
the coherence of compressed prompts and in the extreme long prompt scenario.",2024-09-23,"Wenhao Mao, Chengbin Hou, Tianyu Zhang, Xinyu Lin, Ke Tang, Hairong Lv",http://arxiv.org/pdf/2409.15395v1,cs.CL
ERABAL: Enhancing Role-Playing Agents through Boundary-Aware Learning,"Role-playing is an emerging application in the field of Human-Computer
Interaction (HCI), primarily implemented through the alignment training of a
large language model (LLM) with assigned characters. Despite significant
progress, role-playing agents (RPLAs) still struggle with maintaining
role-consistency across conversations, particularly when confronted with
boundary queries subtly related to character attributes. In this paper, we
present ERABAL, a framework aimed at enhancing RPLAs' role-playing capabilities
through boundary-aware learning. ERABAL encompasses a generation pipeline for
role-specific dialogues and a concomitant methodology for alignment training.
Through comprehensive evaluations, we demonstrate that ERABAL is both efficient
and effective. By training with significantly fewer dialogues than those used
in leading approaches, ERABAL achieves notable improvements across
WikiRoleEval, CharacterEval, and the role-playing subset of MT-Bench compared
to the generalist baseline models. Our code and datasets will be made publicly
available to support further research.",2024-09-23,"Yihong Tang, Jiao Ou, Che Liu, Fuzheng Zhang, Di Zhang, Kun Gai",http://arxiv.org/pdf/2409.14710v2,cs.CL
Target-Aware Language Modeling via Granular Data Sampling,"Language model pretraining generally targets a broad range of use cases and
incorporates data from diverse sources. However, there are instances where we
desire a model that excels in specific areas without markedly compromising
performance in other areas. A cost-effective and straightforward approach is
sampling with low-dimensional data features, which allows to select large-scale
pretraining data for domain-specific use cases. In this work, we revisit
importance sampling with n-gram features consisting of multi-granular tokens,
which strikes a good balance between sentence compression and representation
capabilities. We observed the sampled data to have a high correlation with the
target downstream task performance while preserving its effectiveness on other
tasks. This leads to the proposed data sampling paradigm where language models
can be pretrained more efficiently on selected documents. On eight benchmarks
we demonstrate with $\sim$1% of the data, pretrained models perform on par with
the full RefinedWeb data and outperform randomly selected samples for model
sizes ranging from 125M to 1.5B.",2024-09-23,"Ernie Chang, Pin-Jie Lin, Yang Li, Changsheng Zhao, Daeil Kim, Rastislav Rabatin, Zechun Liu, Yangyang Shi, Vikas Chandra",http://arxiv.org/pdf/2409.14705v1,cs.CL
VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models,"Progress in Text-to-Image (T2I) models has significantly improved the
generation of images from textual descriptions. However, existing evaluation
metrics do not adequately assess the models' ability to handle a diverse range
of textual prompts, which is crucial for their generalizability. To address
this, we introduce a new metric called Visual Language Evaluation Understudy
(VLEU). VLEU uses large language models to sample from the visual text domain,
the set of all possible input texts for T2I models, to generate a wide variety
of prompts. The images generated from these prompts are evaluated based on
their alignment with the input text using the CLIP model.VLEU quantifies a
model's generalizability by computing the Kullback-Leibler divergence between
the marginal distribution of the visual text and the conditional distribution
of the images generated by the model. This metric provides a quantitative way
to compare different T2I models and track improvements during model finetuning.
Our experiments demonstrate the effectiveness of VLEU in evaluating the
generalization capability of various T2I models, positioning it as an essential
metric for future research in text-to-image synthesis.",2024-09-23,"Jingtao Cao, Zheng Zhang, Hongru Wang, Kam-Fai Wong",http://arxiv.org/pdf/2409.14704v2,cs.CL
MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification,"The complexity of text-embedded images presents a formidable challenge in
machine learning given the need for multimodal understanding of multiple
aspects of expression conveyed by them. While previous research in multimodal
analysis has primarily focused on singular aspects such as hate speech and its
subclasses, this study expands this focus to encompass multiple aspects of
linguistics: hate, targets of hate, stance, and humor. We introduce a novel
dataset PrideMM comprising 5,063 text-embedded images associated with the
LGBTQ+ Pride movement, thereby addressing a serious gap in existing resources.
We conduct extensive experimentation on PrideMM by using unimodal and
multimodal baseline methods to establish benchmarks for each task.
Additionally, we propose a novel framework MemeCLIP for efficient downstream
learning while preserving the knowledge of the pre-trained CLIP model. The
results of our experiments show that MemeCLIP achieves superior performance
compared to previously proposed frameworks on two real-world datasets. We
further compare the performance of MemeCLIP and zero-shot GPT-4 on the hate
classification task. Finally, we discuss the shortcomings of our model by
qualitatively analyzing misclassified samples. Our code and dataset are
publicly available at: https://github.com/SiddhantBikram/MemeCLIP.",2024-09-23,"Siddhant Bikram Shah, Shuvam Shiwakoti, Maheep Chaudhary, Haohan Wang",http://arxiv.org/pdf/2409.14703v2,cs.CL
Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling,"Over the last few years, multi-vector retrieval methods, spearheaded by
ColBERT, have become an increasingly popular approach to Neural IR. By storing
representations at the token level rather than at the document level, these
methods have demonstrated very strong retrieval performance, especially in
out-of-domain settings. However, the storage and memory requirements necessary
to store the large number of associated vectors remain an important drawback,
hindering practical adoption. In this paper, we introduce a simple
clustering-based token pooling approach to aggressively reduce the number of
vectors that need to be stored. This method can reduce the space & memory
footprint of ColBERT indexes by 50% with virtually no retrieval performance
degradation. This method also allows for further reductions, reducing the
vector count by 66%-to-75% , with degradation remaining below 5% on a vast
majority of datasets. Importantly, this approach requires no architectural
change nor query-time processing, and can be used as a simple drop-in during
indexation with any ColBERT-like model.",2024-09-23,"Benjamin Clavié, Antoine Chaffin, Griffin Adams",http://arxiv.org/pdf/2409.14683v1,cs.CL
"FeruzaSpeech: A 60 Hour Uzbek Read Speech Corpus with Punctuation, Casing, and Context","This paper introduces FeruzaSpeech, a read speech corpus of the Uzbek
language, containing transcripts in both Cyrillic and Latin alphabets, freely
available for academic research purposes. This corpus includes 60 hours of
high-quality recordings from a single native female speaker from Tashkent,
Uzbekistan. These recordings consist of short excerpts from a book and BBC
News. This paper discusses the enhancement of the Word Error Rates (WERs) on
CommonVoice 16.1's Uzbek data, Uzbek Speech Corpus data, and FeruzaSpeech data
upon integrating FeruzaSpeech.",2024-09-23,"Anna Povey, Katherine Povey",http://arxiv.org/pdf/2410.00035v1,cs.CL
RACER: Rich Language-Guided Failure Recovery Policies for Imitation Learning,"Developing robust and correctable visuomotor policies for robotic
manipulation is challenging due to the lack of self-recovery mechanisms from
failures and the limitations of simple language instructions in guiding robot
actions. To address these issues, we propose a scalable data generation
pipeline that automatically augments expert demonstrations with failure
recovery trajectories and fine-grained language annotations for training. We
then introduce Rich languAge-guided failure reCovERy (RACER), a
supervisor-actor framework, which combines failure recovery data with rich
language descriptions to enhance robot control. RACER features a
vision-language model (VLM) that acts as an online supervisor, providing
detailed language guidance for error correction and task execution, and a
language-conditioned visuomotor policy as an actor to predict the next actions.
Our experimental results show that RACER outperforms the state-of-the-art
Robotic View Transformer (RVT) on RLbench across various evaluation settings,
including standard long-horizon tasks, dynamic goal-change tasks and zero-shot
unseen tasks, achieving superior performance in both simulated and real world
environments. Videos and code are available at:
https://rich-language-failure-recovery.github.io.",2024-09-23,"Yinpei Dai, Jayjun Lee, Nima Fazeli, Joyce Chai",http://arxiv.org/pdf/2409.14674v1,cs.CL
Instruction Tuning Vs. In-Context Learning: Revisiting Large Language Models in Few-Shot Computational Social Science,"Real-world applications of large language models (LLMs) in computational
social science (CSS) tasks primarily depend on the effectiveness of instruction
tuning (IT) or in-context learning (ICL). While IT has shown highly effective
at fine-tuning LLMs for various tasks, ICL offers a rapid alternative for task
adaptation by learning from examples without explicit gradient updates. In this
paper, we evaluate the classification performance of LLMs using IT versus ICL
in few-shot CSS tasks. The experimental results indicate that ICL consistently
outperforms IT in most CSS tasks. Additionally, we investigate the relationship
between the increasing number of training samples and LLM performance. Our
findings show that simply increasing the number of samples without considering
their quality does not consistently enhance the performance of LLMs with either
ICL or IT and can sometimes even result in a performance decline. Finally, we
compare three prompting strategies, demonstrating that ICL is more effective
than zero-shot and Chain-of-Thought (CoT). Our research highlights the
significant advantages of ICL in handling CSS tasks in few-shot settings and
emphasizes the importance of optimizing sample quality and prompting strategies
to improve LLM classification performance. The code will be made available.",2024-09-23,"Taihang Wang, Xiaoman Xu, Yimin Wang, Ye Jiang",http://arxiv.org/pdf/2409.14673v1,cs.CL
Direct Judgement Preference Optimization,"Auto-evaluation is crucial for assessing response quality and offering
feedback for model development. Recent studies have explored training large
language models (LLMs) as generative judges to evaluate and critique other
models' outputs. In this work, we investigate the idea of learning from both
positive and negative data with preference optimization to enhance the
evaluation capabilities of LLM judges across an array of different use cases.
We achieve this by employing three approaches to collect the preference pairs
for different use cases, each aimed at improving our generative judge from a
different perspective. Our comprehensive study over a wide range of benchmarks
demonstrates the effectiveness of our method. In particular, our generative
judge achieves the best performance on 10 out of 13 benchmarks, outperforming
strong baselines like GPT-4o and specialized judge models. Further analysis
show that our judge model robustly counters inherent biases such as position
and length bias, flexibly adapts to any evaluation protocol specified by
practitioners, and provides helpful language feedback for improving downstream
generator models.",2024-09-23,"Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, Shafiq Joty",http://arxiv.org/pdf/2409.14664v2,cs.CL
Building Tamil Treebanks,"Treebanks are important linguistic resources, which are structured and
annotated corpora with rich linguistic annotations. These resources are used in
Natural Language Processing (NLP) applications, supporting linguistic analyses,
and are essential for training and evaluating various computational models.
This paper discusses the creation of Tamil treebanks using three distinct
approaches: manual annotation, computational grammars, and machine learning
techniques. Manual annotation, though time-consuming and requiring linguistic
expertise, ensures high-quality and rich syntactic and semantic information.
Computational deep grammars, such as Lexical Functional Grammar (LFG), offer
deep linguistic analyses but necessitate significant knowledge of the
formalism. Machine learning approaches, utilising off-the-shelf frameworks and
tools like Stanza, UDpipe, and UUParser, facilitate the automated annotation of
large datasets but depend on the availability of quality annotated data,
cross-linguistic training resources, and computational power. The paper
discusses the challenges encountered in building Tamil treebanks, including
issues with Internet data, the need for comprehensive linguistic analysis, and
the difficulty of finding skilled annotators. Despite these challenges, the
development of Tamil treebanks is essential for advancing linguistic research
and improving NLP tools for Tamil.",2024-09-23,Kengatharaiyer Sarveswaran,http://arxiv.org/pdf/2409.14657v1,cs.CL
Harmonising the Clinical Melody: Tuning Large Language Models for Hospital Course Summarisation in Clinical Coding,"The increasing volume and complexity of clinical documentation in Electronic
Medical Records systems pose significant challenges for clinical coders, who
must mentally process and summarise vast amounts of clinical text to extract
essential information needed for coding tasks. While large language models have
been successfully applied to shorter summarisation tasks in recent years, the
challenge of summarising a hospital course remains an open area for further
research and development. In this study, we adapted three pre trained LLMs,
Llama 3, BioMistral, Mistral Instruct v0.1 for the hospital course
summarisation task, using Quantized Low Rank Adaptation fine tuning. We created
a free text clinical dataset from MIMIC III data by concatenating various
clinical notes as the input clinical text, paired with ground truth Brief
Hospital Course sections extracted from the discharge summaries for model
training. The fine tuned models were evaluated using BERTScore and ROUGE
metrics to assess the effectiveness of clinical domain fine tuning.
Additionally, we validated their practical utility using a novel hospital
course summary assessment metric specifically tailored for clinical coding. Our
findings indicate that fine tuning pre trained LLMs for the clinical domain can
significantly enhance their performance in hospital course summarisation and
suggest their potential as assistive tools for clinical coding. Future work
should focus on refining data curation methods to create higher quality
clinical datasets tailored for hospital course summary tasks and adapting more
advanced open source LLMs comparable to proprietary models to further advance
this research.",2024-09-23,"Bokang Bi, Leibo Liu, Sanja Lujic, Louisa Jorm, Oscar Perez-Concha",http://arxiv.org/pdf/2409.14638v2,cs.CL
Can a Neural Model Guide Fieldwork? A Case Study on Morphological Data Collection,"Linguistic fieldwork is an important component in language documentation and
preservation. However, it is a long, exhaustive, and time-consuming process.
This paper presents a novel model that guides a linguist during the fieldwork
and accounts for the dynamics of linguist-speaker interactions. We introduce a
novel framework that evaluates the efficiency of various sampling strategies
for obtaining morphological data and assesses the effectiveness of
state-of-the-art neural models in generalising morphological structures. Our
experiments highlight two key strategies for improving the efficiency: (1)
increasing the diversity of annotated data by uniform sampling among the cells
of the paradigm tables, and (2) using model confidence as a guide to enhance
positive interaction by providing reliable predictions during annotation.",2024-09-22,"Aso Mahmudi, Borja Herce, Demian Inostroza Amestica, Andreas Scherbakov, Eduard Hovy, Ekaterina Vylomova",http://arxiv.org/pdf/2409.14628v2,cs.CL
Can pre-trained language models generate titles for research papers?,"The title of a research paper communicates in a succinct style the main theme
and, sometimes, the findings of the paper. Coming up with the right title is
often an arduous task, and therefore, it would be beneficial to authors if
title generation can be automated. In this paper, we fine-tune pre-trained
language models to generate titles of papers from their abstracts.
Additionally, we use GPT-3.5-turbo in a zero-shot setting to generate paper
titles. The performance of the models is measured with ROUGE, METEOR,
MoverScore, BERTScore and SciBERTScore metrics. We find that fine-tuned
PEGASUS-large outperforms the other models, including fine-tuned LLaMA-3-8B and
GPT-3.5-turbo, across most metrics. We also demonstrate that ChatGPT can
generate creative titles for papers. Our observations suggest that AI-generated
paper titles are generally accurate and appropriate.",2024-09-22,"Tohida Rehman, Debarshi Kumar Sanyal, Samiran Chattopadhyay",http://arxiv.org/pdf/2409.14602v2,cs.CL
"EchoAtt: Attend, Copy, then Adjust for More Efficient Large Language Models","Large Language Models (LLMs), with their increasing depth and number of
parameters, have demonstrated outstanding performance across a variety of
natural language processing tasks. However, this growth in scale leads to
increased computational demands, particularly during inference and fine-tuning.
To address these challenges, we introduce EchoAtt, a novel framework aimed at
optimizing transformer-based models by analyzing and leveraging the similarity
of attention patterns across layers. Our analysis reveals that many inner
layers in LLMs, especially larger ones, exhibit highly similar attention
matrices. By exploiting this similarity, EchoAtt enables the sharing of
attention matrices in less critical layers, significantly reducing
computational requirements without compromising performance. We incorporate
this approach within a knowledge distillation setup, where a pre-trained
teacher model guides the training of a smaller student model. The student model
selectively shares attention matrices in layers with high similarity while
inheriting key parameters from the teacher. Our best results with
TinyLLaMA-1.1B demonstrate that EchoAtt improves inference speed by 15\%,
training speed by 25\%, and reduces the number of parameters by approximately
4\%, all while improving zero-shot performance. These findings highlight the
potential of attention matrix sharing to enhance the efficiency of LLMs, making
them more practical for real-time and resource-limited applications.",2024-09-22,"Hossein Rajabzadeh, Aref Jafari, Aman Sharma, Benyamin Jami, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh",http://arxiv.org/pdf/2409.14595v1,cs.CL
Backtracking Improves Generation Safety,"Text generation has a fundamental limitation almost by definition: there is
no taking back tokens that have been generated, even when they are clearly
problematic. In the context of language model safety, when a partial unsafe
generation is produced, language models by their nature tend to happily keep on
generating similarly unsafe additional text. This is in fact how safety
alignment of frontier models gets circumvented in the wild, despite great
efforts in improving their safety. Deviating from the paradigm of approaching
safety alignment as prevention (decreasing the probability of harmful
responses), we propose backtracking, a technique that allows language models to
""undo"" and recover from their own unsafe generation through the introduction of
a special [RESET] token. Our method can be incorporated into either SFT or DPO
training to optimize helpfulness and harmlessness. We show that models trained
to backtrack are consistently safer than baseline models: backtracking
Llama-3-8B is four times more safe than the baseline model (6.1\% $\to$ 1.5\%)
in our evaluations without regression in helpfulness. Our method additionally
provides protection against four adversarial attacks including an adaptive
attack, despite not being trained to do so.",2024-09-22,"Yiming Zhang, Jianfeng Chi, Hailey Nguyen, Kartikeya Upasani, Daniel M. Bikel, Jason Weston, Eric Michael Smith",http://arxiv.org/pdf/2409.14586v1,cs.CL
The X Types -- Mapping the Semantics of the Twitter Sphere,"Social networks form a valuable source of world knowledge, where influential
entities correspond to popular accounts. Unlike factual knowledge bases (KBs),
which maintain a semantic ontology, structured semantic information is not
available on social media. In this work, we consider a social KB of roughly
200K popular Twitter accounts, which denotes entities of interest. We elicit
semantic information about those entities. In particular, we associate them
with a fine-grained set of 136 semantic types, e.g., determine whether a given
entity account belongs to a politician, or a musical artist. In the lack of
explicit type information in Twitter, we obtain semantic labels for a subset of
the accounts via alignment with the KBs of DBpedia and Wikidata. Given the
labeled dataset, we finetune a transformer-based text encoder to generate
semantic embeddings of the entities based on the contents of their accounts. We
then exploit this evidence alongside network-based embeddings to predict the
entities semantic types. In our experiments, we show high type prediction
performance on the labeled dataset. Consequently, we apply our type
classification model to all of the entity accounts in the social KB. Our
analysis of the results offers insights about the global semantics of the
Twitter sphere. We discuss downstream applications that should benefit from
semantic type information and the semantic embeddings of social entities
generated in this work. In particular, we demonstrate enhanced performance on
the key task of entity similarity assessment using this information.",2024-09-22,"Ogen Schlachet Drukerman, Einat Minkov",http://arxiv.org/pdf/2409.14584v1,cs.CL
Evaluating the Performance and Robustness of LLMs in Materials Science Q&A and Property Predictions,"Large Language Models (LLMs) have the potential to revolutionize scientific
research, yet their robustness and reliability in domain-specific applications
remain insufficiently explored. In this study, we evaluate the performance and
robustness of LLMs for materials science, focusing on domain-specific question
answering and materials property prediction across diverse real-world and
adversarial conditions. Three distinct datasets are used in this study: 1) a
set of multiple-choice questions from undergraduate-level materials science
courses, 2) a dataset including various steel compositions and yield strengths,
and 3) a band gap dataset, containing textual descriptions of material crystal
structures and band gap values. The performance of LLMs is assessed using
various prompting strategies, including zero-shot chain-of-thought, expert
prompting, and few-shot in-context learning. The robustness of these models is
tested against various forms of 'noise', ranging from realistic disturbances to
intentionally adversarial manipulations, to evaluate their resilience and
reliability under real-world conditions. Additionally, the study showcases
unique phenomena of LLMs during predictive tasks, such as mode collapse
behavior when the proximity of prompt examples is altered and performance
recovery from train/test mismatch. The findings aim to provide informed
skepticism for the broad use of LLMs in materials science and to inspire
advancements that enhance their robustness and reliability for practical
applications.",2024-09-22,"Hongchen Wang, Kangming Li, Scott Ramsay, Yao Fehlis, Edward Kim, Jason Hattrick-Simpers",http://arxiv.org/pdf/2409.14572v2,cs.CL
Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training,"Emojis have gained immense popularity on social platforms, serving as a
common means to supplement or replace text. However, existing data mining
approaches generally either completely ignore or simply treat emojis as
ordinary Unicode characters, which may limit the model's ability to grasp the
rich semantic information in emojis and the interaction between emojis and
texts. Thus, it is necessary to release the emoji's power in social media data
mining. To this end, we first construct a heterogeneous graph consisting of
three types of nodes, i.e. post, word and emoji nodes to improve the
representation of different elements in posts. The edges are also well-defined
to model how these three elements interact with each other. To facilitate the
sharing of information among post, word and emoji nodes, we propose a graph
pre-train framework for text and emoji co-modeling, which contains two graph
pre-training tasks: node-level graph contrastive learning and edge-level link
reconstruction learning. Extensive experiments on the Xiaohongshu and Twitter
datasets with two types of downstream tasks demonstrate that our approach
proves significant improvement over previous strong baseline methods.",2024-09-22,"Zhou Zhang, Dongzeng Tan, Jiaan Wang, Yilong Chen, Jiarong Xu",http://arxiv.org/pdf/2409.14552v2,cs.CL
What Are They Doing? Joint Audio-Speech Co-Reasoning,"In audio and speech processing, tasks usually focus on either the audio or
speech modality, even when both sounds and human speech are present in the same
audio clip. Recent Auditory Large Language Models (ALLMs) have made it possible
to process audio and speech simultaneously within a single model, leading to
further considerations of joint audio-speech tasks.
  In this paper, we establish a novel benchmark to investigate how well ALLMs
can perform joint audio-speech processing. Specifically, we introduce Joint
Audio-Speech Co-Reasoning (JASCO), a novel task that unifies audio and speech
processing, strictly requiring co-reasoning across both modalities. We also
release a scene-reasoning dataset called ""What Are They Doing"". Additionally,
we provide deeper insights into the models' behaviors by analyzing their
dependence on each modality.",2024-09-22,"Yingzhi Wang, Pooneh Mousavi, Artem Ploujnikov, Mirco Ravanelli",http://arxiv.org/pdf/2409.14526v2,cs.CL
Beyond Words: Evaluating Large Language Models in Transportation Planning,"The resurgence and rapid advancement of Generative Artificial Intelligence
(GenAI) in 2023 has catalyzed transformative shifts across numerous industry
sectors, including urban transportation and logistics. This study investigates
the evaluation of Large Language Models (LLMs), specifically GPT-4 and
Phi-3-mini, to enhance transportation planning. The study assesses the
performance and spatial comprehension of these models through a
transportation-informed evaluation framework that includes general geospatial
skills, general transportation domain skills, and real-world transportation
problem-solving. Utilizing a mixed-methods approach, the research encompasses
an evaluation of the LLMs' general Geographic Information System (GIS) skills,
general transportation domain knowledge as well as abilities to support human
decision-making in the real-world transportation planning scenarios of
congestion pricing. Results indicate that GPT-4 demonstrates superior accuracy
and reliability across various GIS and transportation-specific tasks compared
to Phi-3-mini, highlighting its potential as a robust tool for transportation
planners. Nonetheless, Phi-3-mini exhibits competence in specific analytical
scenarios, suggesting its utility in resource-constrained environments. The
findings underscore the transformative potential of GenAI technologies in urban
transportation planning. Future work could explore the application of newer
LLMs and the impact of Retrieval-Augmented Generation (RAG) techniques, on a
broader set of real-world transportation planning and operations challenges, to
deepen the integration of advanced AI models in transportation management
practices.",2024-09-22,"Shaowei Ying, Zhenlong Li, Manzhu Yu",http://arxiv.org/pdf/2409.14516v1,cs.CL
Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving Human-AI Alignment in the Writing Process through Edits,"LLM-based applications are helping people write, and LLM-generated text is
making its way into social media, journalism, and our classrooms. However, the
differences between LLM-generated and human written text remain unclear. To
explore this, we hired professional writers to edit paragraphs in several
creative domains. We first found these writers agree on undesirable
idiosyncrasies in LLM generated text, formalizing it into a seven-category
taxonomy (e.g. clich\'es, unnecessary exposition). Second, we curated the LAMP
corpus: 1,057 LLM-generated paragraphs edited by professional writers according
to our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our
study (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms
of writing quality, revealing common limitations across model families. Third,
building on existing work in automatic editing we evaluated methods to improve
LLM-generated text. A large-scale preference annotation confirms that although
experts largely prefer text edited by other experts, automatic editing methods
show promise in improving alignment between LLM-generated and human-written
text.",2024-09-22,"Tuhin Chakrabarty, Philippe Laban, Chien-Sheng Wu",http://arxiv.org/pdf/2409.14509v5,cs.CL
A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders,"Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose
the activations of Large Language Models (LLMs) into human-interpretable
latents. In this paper, we pose two questions. First, to what extent do SAEs
extract monosemantic and interpretable latents? Second, to what extent does
varying the sparsity or the size of the SAE affect monosemanticity /
interpretability? By investigating these questions in the context of a simple
first-letter identification task where we have complete access to ground truth
labels for all tokens in the vocabulary, we are able to provide more detail
than prior investigations. Critically, we identify a problematic form of
feature-splitting we call feature absorption where seemingly monosemantic
latents fail to fire in cases where they clearly should. Our investigation
suggests that varying SAE size or sparsity is insufficient to solve this issue,
and that there are deeper conceptual issues in need of resolution.",2024-09-22,"David Chanin, James Wilken-Smith, Tomáš Dulka, Hardik Bhatnagar, Joseph Bloom",http://arxiv.org/pdf/2409.14507v4,cs.CL
Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation for Logical Reading Comprehension,"Logical reading comprehension is a challenging task that entails grasping the
underlying semantics of text and applying reasoning to deduce the correct
answer. Prior researches have primarily focused on enhancing logical reasoning
capabilities through Chain-of-Thought (CoT) or data augmentation. However,
previous work constructing chain-of-thought rationales concentrates solely on
analyzing correct options, neglecting the incorrect alternatives. Addtionally,
earlier efforts on data augmentation by altering contexts rely on rule-based
methods, which result in generated contexts that lack diversity and coherence.
To address these issues, we propose a Premise-Oriented Data Augmentation (PODA)
framework. This framework can generate CoT rationales including analyses for
both correct and incorrect options, while constructing diverse and high-quality
counterfactual contexts from incorrect candidate options. We integrate
summarizing premises and identifying premises for each option into rationales.
Subsequently, we employ multi-step prompts with identified premises to
construct counterfactual context. To facilitate the model's capabilities to
better differentiate the reasoning process associated with each option, we
introduce a novel thought-path contrastive learning method that compares
reasoning paths between the original and counterfactual samples. Experimental
results on three representative LLMs demonstrate that our method can improve
the baselines substantially across two challenging logical reasoning benchmarks
(ReClor and LogiQA 2.0). The data and code are released at
https://github.com/lalalamdbf/TPReasoner.",2024-09-22,"Chenxu Wang, Ping Jian, Zhen Yang",http://arxiv.org/pdf/2409.14495v3,cs.CL
Unsupervised Word Discovery: Boundary Detection with Clustering vs. Dynamic Programming,"We look at the long-standing problem of segmenting unlabeled speech into
word-like segments and clustering these into a lexicon. Several previous
methods use a scoring model coupled with dynamic programming to find an optimal
segmentation. Here we propose a much simpler strategy: we predict word
boundaries using the dissimilarity between adjacent self-supervised features,
then we cluster the predicted segments to construct a lexicon. For a fair
comparison, we update the older ES-KMeans dynamic programming method with
better features and boundary constraints. On the five-language ZeroSpeech
benchmarks, our simple approach gives similar state-of-the-art results compared
to the new ES-KMeans+ method, while being almost five times faster. Project
webpage: https://s-malan.github.io/prom-seg-clus.",2024-09-22,"Simon Malan, Benjamin van Niekerk, Herman Kamper",http://arxiv.org/pdf/2409.14486v2,cs.CL
A Large Language Model and Denoising Diffusion Framework for Targeted Design of Microstructures with Commands in Natural Language,"Microstructure plays a critical role in determining the macroscopic
properties of materials, with applications spanning alloy design, MEMS devices,
and tissue engineering, among many others. Computational frameworks have been
developed to capture the complex relationship between microstructure and
material behavior. However, despite these advancements, the steep learning
curve associated with domain-specific knowledge and complex algorithms
restricts the broader application of these tools. To lower this barrier, we
propose a framework that integrates Natural Language Processing (NLP), Large
Language Models (LLMs), and Denoising Diffusion Probabilistic Models (DDPMs) to
enable microstructure design using intuitive natural language commands. Our
framework employs contextual data augmentation, driven by a pretrained LLM, to
generate and expand a diverse dataset of microstructure descriptors. A
retrained NER model extracts relevant microstructure descriptors from
user-provided natural language inputs, which are then used by the DDPM to
generate microstructures with targeted mechanical properties and topological
features. The NLP and DDPM components of the framework are modular, allowing
for separate training and validation, which ensures flexibility in adapting the
framework to different datasets and use cases. A surrogate model system is
employed to rank and filter generated samples based on their alignment with
target properties. Demonstrated on a database of nonlinear hyperelastic
microstructures, this framework serves as a prototype for accessible inverse
design of microstructures, starting from intuitive natural language commands.",2024-09-22,"Nikita Kartashov, Nikolaos N. Vlassis",http://arxiv.org/pdf/2409.14473v1,cs.CL
Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints,"Semantic Parsing aims to capture the meaning of a sentence and convert it
into a logical, structured form. Previous studies show that semantic parsing
enhances the performance of smaller models (e.g., BERT) on downstream tasks.
However, it remains unclear whether the improvements extend similarly to LLMs.
In this paper, our empirical findings reveal that, unlike smaller models,
directly adding semantic parsing results into LLMs reduces their performance.
To overcome this, we propose SENSE, a novel prompting approach that embeds
semantic hints within the prompt. Experiments show that SENSE consistently
improves LLMs' performance across various tasks, highlighting the potential of
integrating semantic information to improve LLM capabilities.",2024-09-22,"Kaikai An, Shuzheng Si, Helan Hu, Haozhe Zhao, Yuchi Wang, Qingyan Guo, Baobao Chang",http://arxiv.org/pdf/2409.14469v1,cs.CL
AggregHate: An Efficient Aggregative Approach for the Detection of Hatemongers on Social Platforms,"Automatic detection of online hate speech serves as a crucial step in the
detoxification of the online discourse. Moreover, accurate classification can
promote a better understanding of the proliferation of hate as a social
phenomenon. While most prior work focus on the detection of hateful utterances,
we argue that focusing on the user level is as important, albeit challenging.
In this paper we consider a multimodal aggregative approach for the detection
of hate-mongers, taking into account the potentially hateful texts, user
activity, and the user network. We evaluate our methods on three unique
datasets X (Twitter), Gab, and Parler showing that a processing a user's texts
in her social context significantly improves the detection of hate mongers,
compared to previously used text and graph-based methods. Our method can be
then used to improve the classification of coded messages, dog-whistling, and
racial gas-lighting, as well as inform intervention measures. Moreover, our
approach is highly efficient even for very large datasets and networks.",2024-09-22,"Tom Marzea, Abraham Israeli, Oren Tsur",http://arxiv.org/pdf/2409.14464v1,cs.CL
Exploring Multilingual Probing in Large Language Models: A Cross-Language Analysis,"Probing techniques for large language models (LLMs) have primarily focused on
English, overlooking the vast majority of the world's languages. In this paper,
we extend these probing methods to a multilingual context, investigating the
behaviors of LLMs across diverse languages. We conduct experiments on several
open-source LLM models, analyzing probing accuracy, trends across layers, and
similarities between probing vectors for multiple languages. Our key findings
reveal: (1) a consistent performance gap between high-resource and low-resource
languages, with high-resource languages achieving significantly higher probing
accuracy; (2) divergent layer-wise accuracy trends, where high-resource
languages show substantial improvement in deeper layers similar to English; and
(3) higher representational similarities among high-resource languages, with
low-resource languages demonstrating lower similarities both among themselves
and with high-resource languages. These results highlight significant
disparities in LLMs' multilingual capabilities and emphasize the need for
improved modeling of low-resource languages.",2024-09-22,"Daoyang Li, Haiyan Zhao, Qingcheng Zeng, Mengnan Du",http://arxiv.org/pdf/2409.14459v2,cs.CL
SAC-KG: Exploiting Large Language Models as Skilled Automatic Constructors for Domain Knowledge Graphs,"Knowledge graphs (KGs) play a pivotal role in knowledge-intensive tasks
across specialized domains, where the acquisition of precise and dependable
knowledge is crucial. However, existing KG construction methods heavily rely on
human intervention to attain qualified KGs, which severely hinders the
practical applicability in real-world scenarios. To address this challenge, we
propose a general KG construction framework, named SAC-KG, to exploit large
language models (LLMs) as Skilled Automatic Constructors for domain Knowledge
Graph. SAC-KG effectively involves LLMs as domain experts to generate
specialized and precise multi-level KGs. Specifically, SAC-KG consists of three
components: Generator, Verifier, and Pruner. For a given entity, Generator
produces its relations and tails from raw domain corpora, to construct a
specialized single-level KG. Verifier and Pruner then work together to ensure
precision by correcting generation errors and determining whether newly
produced tails require further iteration for the next-level KG.Experiments
demonstrate that SAC-KG automatically constructs a domain KG at the scale of
over one million nodes and achieves a precision of 89.32%, leading to a
superior performance with over 20% increase in precision rate compared to
existing state-of-the-art methods for the KG construction task.",2024-09-22,"Hanzhu Chen, Xu Shen, Qitan Lv, Jie Wang, Xiaoqi Ni, Jieping Ye",http://arxiv.org/pdf/2410.02811v1,cs.CL
Automotive innovation landscaping using LLM,"The process of landscaping automotive innovation through patent analysis is
crucial for Research and Development teams. It aids in comprehending innovation
trends, technological advancements, and the latest technologies from
competitors. Traditionally, this process required intensive manual efforts.
However, with the advent of Large Language Models (LLMs), it can now be
automated, leading to faster and more efficient patent categorization &
state-of-the-art of inventive concept extraction. This automation can assist
various R\&D teams in extracting relevant information from extensive patent
databases. This paper introduces a method based on prompt engineering to
extract essential information for landscaping. The information includes the
problem addressed by the patent, the technology utilized, and the area of
innovation within the vehicle ecosystem (such as safety, Advanced Driver
Assistance Systems and more).The result demonstrates the implementation of this
method to create a landscape of fuel cell technology using open-source patent
data. This approach provides a comprehensive overview of the current state of
fuel cell technology, offering valuable insights for future research and
development in this field.",2024-09-22,"Raju Gorain, Omkar Salunke",http://arxiv.org/pdf/2409.14436v1,cs.CL
Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations,"With the aid of large language models, current conversational recommender
system (CRS) has gaining strong abilities to persuade users to accept
recommended items. While these CRSs are highly persuasive, they can mislead
users by incorporating incredible information in their explanations, ultimately
damaging the long-term trust between users and the CRS. To address this, we
propose a simple yet effective method, called PC-CRS, to enhance the
credibility of CRS's explanations during persuasion. It guides the explanation
generation through our proposed credibility-aware persuasive strategies and
then gradually refines explanations via post-hoc self-reflection. Experimental
results demonstrate the efficacy of PC-CRS in promoting persuasive and credible
explanations. Further analysis reveals the reason behind current methods
producing incredible explanations and the potential of credible explanations to
improve recommendation accuracy.",2024-09-22,"Peixin Qin, Chen Huang, Yang Deng, Wenqiang Lei, Tat-Seng Chua",http://arxiv.org/pdf/2409.14399v2,cs.CL
Predicting User Stances from Target-Agnostic Information using Large Language Models,"We investigate Large Language Models' (LLMs) ability to predict a user's
stance on a target given a collection of his/her target-agnostic social media
posts (i.e., user-level stance prediction). While we show early evidence that
LLMs are capable of this task, we highlight considerable variability in the
performance of the model across (i) the type of stance target, (ii) the
prediction strategy and (iii) the number of target-agnostic posts supplied.
Post-hoc analyses further hint at the usefulness of target-agnostic posts in
providing relevant information to LLMs through the presence of both
surface-level (e.g., target-relevant keywords) and user-level features (e.g.,
encoding users' moral values). Overall, our findings suggest that LLMs might
offer a viable method for determining public stances towards new topics based
on historical and target-agnostic data. At the same time, we also call for
further research to better understand LLMs' strong performance on the stance
prediction task and how their effectiveness varies across task contexts.",2024-09-22,"Siyuan Brandon Loh, Liang Ze Wong, Prasanta Bhattacharya, Joseph Simons, Wei Gao, Hong Zhang",http://arxiv.org/pdf/2409.14395v1,cs.CL
Investigating Layer Importance in Large Language Models,"Large language models (LLMs) have gained increasing attention due to their
prominent ability to understand and process texts. Nevertheless, LLMs largely
remain opaque. The lack of understanding of LLMs has obstructed the deployment
in safety-critical scenarios and hindered the development of better models. In
this study, we advance the understanding of LLM by investigating the
significance of individual layers in LLMs. We propose an efficient sampling
method to faithfully evaluate the importance of layers using Shapley values, a
widely used explanation framework in feature attribution and data valuation. In
addition, we conduct layer ablation experiments to assess the performance
degradation resulting from the exclusion of specific layers. Our findings
reveal the existence of cornerstone layers, wherein certain early layers can
exhibit a dominant contribution over others. Removing one cornerstone layer
leads to a drastic collapse of the model performance, often reducing it to
random guessing. Conversely, removing non-cornerstone layers results in only
marginal performance changes. This study identifies cornerstone layers in LLMs
and underscores their critical role for future research.",2024-09-22,"Yang Zhang, Yanfei Dong, Kenji Kawaguchi",http://arxiv.org/pdf/2409.14381v1,cs.CL
J2N -- Nominal Adjective Identification and its Application,"This paper explores the challenges posed by nominal adjectives (NAs) in
natural language processing (NLP) tasks, particularly in part-of-speech (POS)
tagging. We propose treating NAs as a distinct POS tag, ""JN,"" and investigate
its impact on POS tagging, BIO chunking, and coreference resolution. Our study
shows that reclassifying NAs can improve the accuracy of syntactic analysis and
structural understanding in NLP. We present experimental results using Hidden
Markov Models (HMMs), Maximum Entropy (MaxEnt) models, and Spacy, demonstrating
the feasibility and potential benefits of this approach. Additionally we
finetuned a bert model to identify the NA in untagged text.",2024-09-22,"Lemeng Qi, Yang Han, Zhuotong Xie",http://arxiv.org/pdf/2409.14374v3,cs.CL
The Ability of Large Language Models to Evaluate Constraint-satisfaction in Agent Responses to Open-ended Requests,"Generative AI agents are often expected to respond to complex user requests
that have No One Right Answer (NORA), e.g., ""design a vegetarian meal plan
below 1800 calories"". Such requests may entail a set of constraints that the
agent should adhere to. To successfully develop agents for NORA scenarios, an
accurate automatic evaluation framework is essential, and specifically - one
capable of validating the satisfaction of constraints in the agent's response.
Recently, large language models (LLMs) have been adopted as versatile
evaluators for many NORA tasks, but their ability to evaluate
constraint-satisfaction in generated text remains unclear. To study this, we
develop and release a novel Arithmetic Constraint-Satisfaction (ACS)
benchmarking dataset. The dataset consists of complex user requests with
corresponding constraints, agent responses and human labels indicating each
constraint's satisfaction level in the response. A unique property of this
dataset is that validating many of its constraints requires reviewing the
response as a whole (in contrast to many other benchmarks that require the
validation of a single independent item). Moreover, it assesses LLMs in
performing reasoning, in-context data extraction, arithmetic calculations, and
counting. We then benchmark both open and proprietary LLMs on evaluating
constraint-satisfaction, and show that most models still have a significant
headroom for improvement, and that errors primarily stem from reasoning issues.
In addition, most models exhibit a skewed constraint-satisfaction prediction
pattern, with higher accuracy where the ground-truth label is ""satisfied"".
Lastly, few-shot prompting for our task proved to be rather challenging, since
many of the studied models showed a degradation in performance when it was
introduced.",2024-09-22,"Lior Madmoni, Amir Zait, Ilia Labzovsky, Danny Karmon",http://arxiv.org/pdf/2409.14371v1,cs.CL
Position IDs Matter: An Enhanced Position Layout for Efficient Context Compression in Large Language Models,"Using special tokens (e.g., gist, memory, or compressed tokens) to compress
context information is a common practice for large language models (LLMs).
However, existing approaches often neglect that position encodings inherently
induce local inductive biases in models, causing the compression process to
ignore holistic contextual dependencies. We propose Enhanced Position Layout
(EPL), a simple yet effective method that improves the context compression
capability of LLMs by only adjusting position IDs, the numerical identifiers
that specify token positions. EPL minimizes the distance between context tokens
and their corresponding special tokens and at the same time maintains the
sequence order in position IDs between context tokens, special tokens, and the
subsequent tokens. Integrating EPL into our best performing context compression
model results in 1.9 ROUGE-1 F1 improvement on out-of-domain question answering
datasets in average. When extended to multimodal scenarios, EPL brings an
average accuracy gain of 2.6 to vision compression LLMs.",2024-09-22,"Runsong Zhao, Xin Liu, Xinyu Liu, Pengcheng Huang, Chunyang Xiao, Tong Xiao, Jingbo Zhu",http://arxiv.org/pdf/2409.14364v3,cs.CL
Using Natural Language Processing to find Indication for Burnout with Text Classification: From Online Data to Real-World Data,"Burnout, classified as a syndrome in the ICD-11, arises from chronic
workplace stress that has not been effectively managed. It is characterized by
exhaustion, cynicism, and reduced professional efficacy, and estimates of its
prevalence vary significantly due to inconsistent measurement methods. Recent
advancements in Natural Language Processing (NLP) and machine learning offer
promising tools for detecting burnout through textual data analysis, with
studies demonstrating high predictive accuracy. This paper contributes to
burnout detection in German texts by: (a) collecting an anonymous real-world
dataset including free-text answers and Oldenburg Burnout Inventory (OLBI)
responses; (b) demonstrating the limitations of a GermanBERT-based classifier
trained on online data; (c) presenting two versions of a curated
BurnoutExpressions dataset, which yielded models that perform well in
real-world applications; and (d) providing qualitative insights from an
interdisciplinary focus group on the interpretability of AI models used for
burnout detection. Our findings emphasize the need for greater collaboration
between AI researchers and clinical experts to refine burnout detection models.
Additionally, more real-world data is essential to validate and enhance the
effectiveness of current AI methods developed in NLP research, which are often
based on data automatically scraped from online sources and not evaluated in a
real-world context. This is essential for ensuring AI tools are well suited for
practical applications.",2024-09-22,"Mascha Kurpicz-Briki, Ghofrane Merhbene, Alexandre Puttick, Souhir Ben Souissi, Jannic Bieri, Thomas Jörg Müller, Christoph Golz",http://arxiv.org/pdf/2409.14357v1,cs.CL
MQM-APE: Toward High-Quality Error Annotation Predictors with Automatic Post-Editing in LLM Translation Evaluators,"Large Language Models (LLMs) have shown significant potential as judges for
Machine Translation (MT) quality assessment, providing both scores and
fine-grained feedback. Although approaches such as GEMBA-MQM have shown
state-of-the-art performance on reference-free evaluation, the predicted errors
do not align well with those annotated by human, limiting their
interpretability as feedback signals. To enhance the quality of error
annotations predicted by LLM evaluators, we introduce a universal and
training-free framework, $\textbf{MQM-APE}$, based on the idea of filtering out
non-impactful errors by Automatically Post-Editing (APE) the original
translation based on each error, leaving only those errors that contribute to
quality improvement. Specifically, we prompt the LLM to act as 1)
$\textit{evaluator}$ to provide error annotations, 2) $\textit{post-editor}$ to
determine whether errors impact quality improvement and 3) $\textit{pairwise
quality verifier}$ as the error filter. Experiments show that our approach
consistently improves both the reliability and quality of error spans against
GEMBA-MQM, across eight LLMs in both high- and low-resource languages.
Orthogonal to trained approaches, MQM-APE complements translation-specific
evaluators such as Tower, highlighting its broad applicability. Further
analysis confirms the effectiveness of each module and offers valuable insights
into evaluator design and LLMs selection.",2024-09-22,"Qingyu Lu, Liang Ding, Kanjian Zhang, Jinxia Zhang, Dacheng Tao",http://arxiv.org/pdf/2409.14335v2,cs.CL
Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses,"Large language models (LLMs) equipped with chain-of-thoughts (CoT) prompting
have shown significant multi-step reasoning capabilities in factual content
like mathematics, commonsense, and logic. However, their performance in
narrative reasoning, which demands greater abstraction capabilities, remains
unexplored. This study utilizes tropes in movie synopses to assess the abstract
reasoning abilities of state-of-the-art LLMs and uncovers their low
performance. We introduce a trope-wise querying approach to address these
challenges and boost the F1 score by 11.8 points. Moreover, while prior studies
suggest that CoT enhances multi-step reasoning, this study shows CoT can cause
hallucinations in narrative content, reducing GPT-4's performance. We also
introduce an Adversarial Injection method to embed trope-related text tokens
into movie synopses without explicit tropes, revealing CoT's heightened
sensitivity to such injections. Our comprehensive analysis provides insights
for future research directions.",2024-09-22,"Hung-Ting Su, Ya-Ching Hsu, Xudong Lin, Xiang-Qian Shi, Yulei Niu, Han-Yuan Hsu, Hung-yi Lee, Winston H. Hsu",http://arxiv.org/pdf/2409.14324v1,cs.CL
Reliable and diverse evaluation of LLM medical knowledge mastery,"Mastering medical knowledge is crucial for medical-specific LLMs. However,
despite the existence of medical benchmarks like MedQA, a unified framework
that fully leverages existing knowledge bases to evaluate LLMs' mastery of
medical knowledge is still lacking. In the study, we propose a novel framework
PretexEval that dynamically generates reliable and diverse test samples to
evaluate LLMs for any given medical knowledge base. We notice that test samples
produced directly from knowledge bases by templates or LLMs may introduce
factual errors and also lack diversity. To address these issues, we introduce a
novel schema into our proposed evaluation framework that employs predicate
equivalence transformations to produce a series of variants for any given
medical knowledge point. Finally, these produced predicate variants are
converted into textual language, resulting in a series of reliable and diverse
test samples to evaluate whether LLMs fully master the given medical factual
knowledge point. Here, we use our proposed framework to systematically
investigate the mastery of medical factual knowledge of 12 well-known LLMs,
based on two knowledge bases that are crucial for clinical diagnosis and
treatment. The evaluation results illustrate that current LLMs still exhibit
significant deficiencies in fully mastering medical knowledge, despite
achieving considerable success on some famous public benchmarks. These new
findings provide valuable insights for developing medical-specific LLMs,
highlighting that current LLMs urgently need to strengthen their comprehensive
and in-depth mastery of medical knowledge before being applied to real-world
medical scenarios.",2024-09-22,"Yuxuan Zhou, Xien Liu, Chen Ning, Xiao Zhang, Ji Wu",http://arxiv.org/pdf/2409.14302v2,cs.CL
Towards Within-Class Variation in Alzheimer's Disease Detection from Spontaneous Speech,"Alzheimer's Disease (AD) detection has emerged as a promising research area
that employs machine learning classification models to distinguish between
individuals with AD and those without. Unlike conventional classification
tasks, we identify within-class variation as a critical challenge in AD
detection: individuals with AD exhibit a spectrum of cognitive impairments.
Given that many AD detection tasks lack fine-grained labels, simplistic binary
classification may overlook two crucial aspects: within-class differences and
instance-level imbalance. The former compels the model to map AD samples with
varying degrees of impairment to a single diagnostic label, disregarding
certain changes in cognitive function. While the latter biases the model
towards overrepresented severity levels. This work presents early efforts to
address these challenges. We propose two novel methods: Soft Target
Distillation (SoTD) and Instance-level Re-balancing (InRe), targeting two
problems respectively. Experiments on the ADReSS and ADReSSo datasets
demonstrate that the proposed methods significantly improve detection accuracy.
Further analysis reveals that SoTD effectively harnesses the strengths of
multiple component models, while InRe substantially alleviates model
over-fitting. These findings provide insights for developing more robust and
reliable AD detection models.",2024-09-22,"Jiawen Kang, Dongrui Han, Lingwei Meng, Jingyan Zhou, Jinchao Li, Xixin Wu, Helen Meng",http://arxiv.org/pdf/2409.16322v1,cs.CL
Opinion Mining on Offshore Wind Energy for Environmental Engineering,"In this paper, we conduct sentiment analysis on social media data to study
mass opinion about offshore wind energy. We adapt three machine learning
models, namely, TextBlob, VADER, and SentiWordNet because different functions
are provided by each model. TextBlob provides subjectivity analysis as well as
polarity classification. VADER offers cumulative sentiment scores. SentiWordNet
considers sentiments with reference to context and performs classification
accordingly. Techniques in NLP are harnessed to gather meaning from the textual
data in social media. Data visualization tools are suitably deployed to display
the overall results. This work is much in line with citizen science and smart
governance via involvement of mass opinion to guide decision support. It
exemplifies the role of Machine Learning and NLP here.",2024-09-22,"Isabele Bittencourt, Aparna S. Varde, Pankaj Lal",http://arxiv.org/pdf/2409.14292v1,cs.CL
ESPERANTO: Evaluating Synthesized Phrases to Enhance Robustness in AI Detection for Text Origination,"While large language models (LLMs) exhibit significant utility across various
domains, they simultaneously are susceptible to exploitation for unethical
purposes, including academic misconduct and dissemination of misinformation.
Consequently, AI-generated text detection systems have emerged as a
countermeasure. However, these detection mechanisms demonstrate vulnerability
to evasion techniques and lack robustness against textual manipulations. This
paper introduces back-translation as a novel technique for evading detection,
underscoring the need to enhance the robustness of current detection systems.
The proposed method involves translating AI-generated text through multiple
languages before back-translating to English. We present a model that combines
these back-translated texts to produce a manipulated version of the original
AI-generated text. Our findings demonstrate that the manipulated text retains
the original semantics while significantly reducing the true positive rate
(TPR) of existing detection methods. We evaluate this technique on nine AI
detectors, including six open-source and three proprietary systems, revealing
their susceptibility to back-translation manipulation. In response to the
identified shortcomings of existing AI text detectors, we present a
countermeasure to improve the robustness against this form of manipulation. Our
results indicate that the TPR of the proposed method declines by only 1.85%
after back-translation manipulation. Furthermore, we build a large dataset of
720k texts using eight different LLMs. Our dataset contains both human-authored
and LLM-generated texts in various domains and writing styles to assess the
performance of our method and existing detectors. This dataset is publicly
shared for the benefit of the research community.",2024-09-22,"Navid Ayoobi, Lily Knab, Wen Cheng, David Pantoja, Hamidreza Alikhani, Sylvain Flamant, Jin Kim, Arjun Mukherjee",http://arxiv.org/pdf/2409.14285v1,cs.CL
Can-Do! A Dataset and Neuro-Symbolic Grounded Framework for Embodied Planning with Large Multimodal Models,"Large multimodal models have demonstrated impressive problem-solving
abilities in vision and language tasks, and have the potential to encode
extensive world knowledge. However, it remains an open challenge for these
models to perceive, reason, plan, and act in realistic environments. In this
work, we introduce Can-Do, a benchmark dataset designed to evaluate embodied
planning abilities through more diverse and complex scenarios than previous
datasets. Our dataset includes 400 multimodal samples, each consisting of
natural language user instructions, visual images depicting the environment,
state changes, and corresponding action plans. The data encompasses diverse
aspects of commonsense knowledge, physical understanding, and safety awareness.
Our fine-grained analysis reveals that state-of-the-art models, including
GPT-4V, face bottlenecks in visual perception, comprehension, and reasoning
abilities. To address these challenges, we propose NeuroGround, a neurosymbolic
framework that first grounds the plan generation in the perceived environment
states and then leverages symbolic planning engines to augment the
model-generated plans. Experimental results demonstrate the effectiveness of
our framework compared to strong baselines. Our code and dataset are available
at https://embodied-planning.github.io.",2024-09-22,"Yew Ken Chia, Qi Sun, Lidong Bing, Soujanya Poria",http://arxiv.org/pdf/2409.14277v1,cs.CL
Instruction Following without Instruction Tuning,"Instruction tuning commonly means finetuning a language model on
instruction-response pairs. We discover two forms of adaptation (tuning) that
are deficient compared to instruction tuning, yet still yield instruction
following; we call this implicit instruction tuning. We first find that
instruction-response pairs are not necessary: training solely on responses,
without any corresponding instructions, yields instruction following. This
suggests pretrained models have an instruction-response mapping which is
revealed by teaching the model the desired distribution of responses. However,
we then find it's not necessary to teach the desired distribution of responses:
instruction-response training on narrow-domain data like poetry still leads to
broad instruction-following behavior like recipe generation. In particular,
when instructions are very different from those in the narrow finetuning
domain, models' responses do not adhere to the style of the finetuning domain.
To begin to explain implicit instruction tuning, we hypothesize that very
simple changes to a language model's distribution yield instruction following.
We support this by hand-writing a rule-based language model which yields
instruction following in a product-of-experts with a pretrained model. The
rules are to slowly increase the probability of ending the sequence, penalize
repetition, and uniformly change 15 words' probabilities. In summary,
adaptations made without being designed to yield instruction following can do
so implicitly.",2024-09-21,"John Hewitt, Nelson F. Liu, Percy Liang, Christopher D. Manning",http://arxiv.org/pdf/2409.14254v1,cs.CL
Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models,"In dialogue, the addressee may initially misunderstand the speaker and
respond erroneously, often prompting the speaker to correct the
misunderstanding in the next turn with a Third Position Repair (TPR). The
ability to process and respond appropriately to such repair sequences is thus
crucial in conversational AI systems. In this paper, we first collect, analyse,
and publicly release BlockWorld-Repairs: a dataset of multi-modal TPR sequences
in an instruction-following manipulation task that is, by design, rife with
referential ambiguity. We employ this dataset to evaluate several
state-of-the-art Vision and Language Models (VLM) across multiple settings,
focusing on their capability to process and accurately respond to TPRs and thus
recover from miscommunication. We find that, compared to humans, all models
significantly underperform in this task. We then show that VLMs can benefit
from specialised losses targeting relevant tokens during fine-tuning, achieving
better performance and generalising better to new scenarios. Our results
suggest that these models are not yet ready to be deployed in multi-modal
collaborative settings where repairs are common, and highlight the need to
design training regimes and objectives that facilitate learning from
interaction. Our code and data are available at
www.github.com/JChiyah/blockworld-repairs",2024-09-21,"Javier Chiyah-Garcia, Alessandro Suglia, Arash Eshghi",http://arxiv.org/pdf/2409.14247v2,cs.CL
Data-centric NLP Backdoor Defense from the Lens of Memorization,"Backdoor attack is a severe threat to the trustworthiness of DNN-based
language models. In this paper, we first extend the definition of memorization
of language models from sample-wise to more fine-grained sentence element-wise
(e.g., word, phrase, structure, and style), and then point out that language
model backdoors are a type of element-wise memorization. Through further
analysis, we find that the strength of such memorization is positively
correlated to the frequency of duplicated elements in the training dataset. In
conclusion, duplicated sentence elements are necessary for successful backdoor
attacks. Based on this, we propose a data-centric defense. We first detect
trigger candidates in training data by finding memorizable elements, i.e.,
duplicated elements, and then confirm real triggers by testing if the
candidates can activate backdoor behaviors (i.e., malicious elements). Results
show that our method outperforms state-of-the-art defenses in defending against
different types of NLP backdoors.",2024-09-21,"Zhenting Wang, Zhizhi Wang, Mingyu Jin, Mengnan Du, Juan Zhai, Shiqing Ma",http://arxiv.org/pdf/2409.14200v1,cs.CL
"The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends","In the era of large language models (LLMs), a vast amount of conversation
logs will be accumulated thanks to the rapid development trend of language UI.
Conversation Analysis (CA) strives to uncover and analyze critical information
from conversation data, streamlining manual processes and supporting business
insights and decision-making. The need for CA to extract actionable insights
and drive empowerment is becoming increasingly prominent and attracting
widespread attention. However, the lack of a clear scope for CA leads to a
dispersion of various techniques, making it difficult to form a systematic
technical synergy to empower business applications. In this paper, we perform a
thorough review and systematize CA task to summarize the existing related work.
Specifically, we formally define CA task to confront the fragmented and chaotic
landscape in this field, and derive four key steps of CA from conversation
scene reconstruction, to in-depth attribution analysis, and then to performing
targeted training, finally generating conversations based on the targeted
training for achieving the specific goals. In addition, we showcase the
relevant benchmarks, discuss potential challenges and point out future
directions in both industry and academia. In view of current advancements, it
is evident that the majority of efforts are still concentrated on the analysis
of shallow conversation elements, which presents a considerable gap between the
research and business, and with the assist of LLMs, recent work has shown a
trend towards research on causality and strategic tasks which are sophisticated
and high-level. The analyzed experiences and insights will inevitably have
broader application value in business operations that target conversation logs.",2024-09-21,"Xinghua Zhang, Haiyang Yu, Yongbin Li, Minzheng Wang, Longze Chen, Fei Huang",http://arxiv.org/pdf/2409.14195v1,cs.CL
Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic Extraction,"Integrating structured knowledge from tabular formats poses significant
challenges within natural language processing (NLP), mainly when dealing with
complex, semi-structured tables like those found in the FeTaQA dataset. These
tables require advanced methods to interpret and generate meaningful responses
accurately. Traditional approaches, such as SQL and SPARQL, often fail to fully
capture the semantics of such data, especially in the presence of irregular
table structures like web tables. This paper addresses these challenges by
proposing a novel approach that extracts triples straightforward from tabular
data and integrates it with a retrieval-augmented generation (RAG) model to
enhance the accuracy, coherence, and contextual richness of responses generated
by a fine-tuned GPT-3.5-turbo-0125 model. Our approach significantly
outperforms existing baselines on the FeTaQA dataset, particularly excelling in
Sacre-BLEU and ROUGE metrics. It effectively generates contextually accurate
and detailed long-form answers from tables, showcasing its strength in complex
data interpretation.",2024-09-21,"Hossein Sholehrasa, Sanaz Saki Norouzi, Pascal Hitzler, Majid Jaberi-Douraki",http://arxiv.org/pdf/2409.14192v2,cs.CL
Rephrase and Contrast: Fine-Tuning Language Models for Enhanced Understanding of Communication and Computer Networks,"Large language models (LLMs) are being widely researched across various
disciplines, with significant recent efforts focusing on adapting LLMs for
understanding of how communication networks operate. However, over-reliance on
prompting techniques hinders the full exploitation of the generalization
ability of these models, and the lack of efficient fine-tuning methods prevents
the full realization of lightweight LLMs' potential. This paper addresses these
challenges by introducing our Rephrase and Contrast (RaC) framework, an
efficient fine-tuning framework. RaC enhances LLMs' comprehension and critical
thinking abilities by incorporating question reformulation and contrastive
analysis of correct and incorrect answers during the fine-tuning process.
Experimental results demonstrate a 63.73% accuracy improvement over the
foundational model when tested on a comprehensive networking problem set.
Moreover, to efficiently construct the dataset for RaC fine-tuning, we develop
a GPT-assisted data mining method for generating high-quality question-answer
(QA) pairs; furthermore, we introduce ChoiceBoost, a data augmentation
technique that expands dataset size while reducing answer-order bias. Apart
from these technical innovations, we contribute to the networking community by
open-sourcing valuable research resources, including: 1) the fine-tuned
networking model referred to as RaC-Net, 2) the training dataset used for
fine-tuning the model, 3) three testing problem sets of different difficulties
to serve as benchmarks for future research, and 4) code associated with the
above resources.",2024-09-21,"Liujianfu Wang, Yuyang Du, Jingqi Lin, Kexin Chen, Soung Chang Liew",http://arxiv.org/pdf/2409.19007v2,cs.CL
On Lexical Invariance on Multisets and Graphs,"In this draft, we study a novel problem, called lexical invariance, using the
medium of multisets and graphs. Traditionally in the NLP domain, lexical
invariance indicates that the semantic meaning of a sentence should remain
unchanged regardless of the specific lexical or word-based representation of
the input. For example, ``The movie was extremely entertaining'' would have the
same meaning as ``The film was very enjoyable''. In this paper, we study a more
challenging setting, where the output of a function is invariant to any
injective transformation applied to the input lexical space. For example,
multiset {1,2,3,2} is equivalent to multiset {a,b,c,b} if we specify an
injective transformation that maps 1 to a, 2 to b and 3 to c. We study the
sufficient and necessary conditions for a most expressive lexical invariant
(and permutation invariant) function on multisets and graphs, and proves that
for multisets, the function must have a form that only takes the multiset of
counts of the unique elements in the original multiset as input. For example, a
most expressive lexical invariant function on {a,b,c,b} must have a form that
only operates on {1,1,2} (meaning that there are 1, 1, 2 unique elements
corresponding to a,c,b). For graphs, we prove that a most expressive lexical
invariant and permutation invariant function must have a form that only takes
the adjacency matrix and a difference matrix as input, where the (i,j)th
element of the difference matrix is 1 if node i and node j have the same
feature and 0 otherwise. We perform synthetic experiments on TU datasets to
verify our theorems.",2024-09-21,Muhan Zhang,http://arxiv.org/pdf/2409.14179v1,cs.CL
QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling,"Large Language models (LLMs) have brought about substantial advancements in
the field of Question Answering (QA) systems. These models do remarkably well
in addressing intricate inquiries in a variety of disciplines. However, because
of domain-specific vocabulary, complex technological concepts, and the
requirement for exact responses applying LLMs to specialized sectors like
telecommunications presents additional obstacles. GPT-3.5 has been used in
recent work, to obtain noteworthy accuracy for telecom-related questions in a
Retrieval Augmented Generation (RAG) framework. Notwithstanding these
developments, the practical use of models such as GPT-3.5 is restricted by
their proprietary nature and high computing demands. This paper introduces
QMOS, an innovative approach which uses a Question-Masked loss and Option
Shuffling trick to enhance the performance of LLMs in answering Multiple-Choice
Questions in the telecommunications domain. Our focus was on using opensource,
smaller language models (Phi-2 and Falcon-7B) within an enhanced RAG framework.
Our multi-faceted approach involves several enhancements to the whole LLM-RAG
pipeline of finetuning, retrieval, prompt engineering and inference. Our
approaches significantly outperform existing results, achieving accuracy
improvements from baselines of 24.70% to 49.30% with Falcon-7B and from 42.07%
to 84.65% with Phi-2.",2024-09-21,"Blessed Guda, Gabrial Zencha Ashungafac, Lawrence Francis, Carlee Joe-Wong",http://arxiv.org/pdf/2409.14175v2,cs.CL
Towards Building Efficient Sentence BERT Models using Layer Pruning,"This study examines the effectiveness of layer pruning in creating efficient
Sentence BERT (SBERT) models. Our goal is to create smaller sentence embedding
models that reduce complexity while maintaining strong embedding similarity. We
assess BERT models like Muril and MahaBERT-v2 before and after pruning,
comparing them with smaller, scratch-trained models like MahaBERT-Small and
MahaBERT-Smaller. Through a two-phase SBERT fine-tuning process involving
Natural Language Inference (NLI) and Semantic Textual Similarity (STS), we
evaluate the impact of layer reduction on embedding quality. Our findings show
that pruned models, despite fewer layers, perform competitively with fully
layered versions. Moreover, pruned models consistently outperform similarly
sized, scratch-trained models, establishing layer pruning as an effective
strategy for creating smaller, efficient embedding models. These results
highlight layer pruning as a practical approach for reducing computational
demand while preserving high-quality embeddings, making SBERT models more
accessible for languages with limited technological resources.",2024-09-21,"Anushka Shelke, Riya Savant, Raviraj Joshi",http://arxiv.org/pdf/2409.14168v1,cs.CL
A Survey on Large Language Model-empowered Autonomous Driving,"Artificial intelligence (AI) plays a crucial role in autonomous driving (AD)
research, propelling its development towards intelligence and efficiency.
Currently, the development of AD technology follows two main technical paths:
modularization and end-to-end. Modularization decompose the driving task into
modules such as perception, prediction, planning, and control, and train them
separately. Due to the inconsistency of training objectives between modules,
the integrated effect suffers from bias. End-to-end attempts to address this
issue by utilizing a single model that directly maps from sensor data to
control signals. This path has limited learning capabilities in a comprehensive
set of features and struggles to handle unpredictable long-tail events and
complex urban traffic scenarios. In the face of challenges encountered in both
paths, many researchers believe that large language models (LLMs) with powerful
reasoning capabilities and extensive knowledge understanding may be the
solution, expecting LLMs to provide AD systems with deeper levels of
understanding and decision-making capabilities. In light of the challenges
faced by both paths, many researchers believe that LLMs, with their powerful
reasoning abilities and extensive knowledge, could offer a solution. To
understand if LLMs could enhance AD, this paper conducts a thorough analysis of
the potential applications of LLMs in AD systems, including exploring their
optimization strategies in both modular and end-to-end approaches, with a
particular focus on how LLMs can tackle the problems and challenges present in
current solutions. Furthermore, we discuss an important question: Can LLM-based
artificial general intelligence (AGI) be a key to achieve high-level AD? We
further analyze the potential limitations and challenges that LLMs may
encounter in promoting the development of AD technology.",2024-09-21,"Yuxuan Zhu, Shiyi Wang, Wenqing Zhong, Nianchen Shen, Yunqi Li, Siqi Wang, Zhiheng Li, Cathy Wu, Zhengbing He, Li Li",http://arxiv.org/pdf/2409.14165v3,cs.CL
PromptTA: Prompt-driven Text Adapter for Source-free Domain Generalization,"Source-free domain generalization (SFDG) tackles the challenge of adapting
models to unseen target domains without access to source domain data. To deal
with this challenging task, recent advances in SFDG have primarily focused on
leveraging the text modality of vision-language models such as CLIP. These
methods involve developing a transferable linear classifier based on diverse
style features extracted from the text and learned prompts or deriving
domain-unified text representations from domain banks. However, both style
features and domain banks have limitations in capturing comprehensive domain
knowledge. In this work, we propose Prompt-Driven Text Adapter (PromptTA)
method, which is designed to better capture the distribution of style features
and employ resampling to ensure thorough coverage of domain knowledge. To
further leverage this rich domain information, we introduce a text adapter that
learns from these style features for efficient domain information storage.
Extensive experiments conducted on four benchmark datasets demonstrate that
PromptTA achieves state-of-the-art performance. The code is available at
https://github.com/zhanghr2001/PromptTA.",2024-09-21,"Haoran Zhang, Shuanghao Bai, Wanqi Zhou, Jingwen Fu, Badong Chen",http://arxiv.org/pdf/2409.14163v1,cs.CL
On Importance of Pruning and Distillation for Efficient Low Resource NLP,"The rise of large transformer models has revolutionized Natural Language
Processing, leading to significant advances in tasks like text classification.
However, this progress demands substantial computational resources, escalating
training duration, and expenses with larger model sizes. Efforts have been made
to downsize and accelerate English models (e.g., Distilbert, MobileBert). Yet,
research in this area is scarce for low-resource languages.
  In this study, we explore the case of the low-resource Indic language
Marathi. Leveraging the marathi-topic-all-doc-v2 model as our baseline, we
implement optimization techniques to reduce computation time and memory usage.
Our focus is on enhancing the efficiency of Marathi transformer models while
maintaining top-tier accuracy and reducing computational demands. Using the
MahaNews document classification dataset and the marathi-topic-all-doc-v2 model
from L3Cube, we apply Block Movement Pruning, Knowledge Distillation, and Mixed
Precision methods individually and in combination to boost efficiency. We
demonstrate the importance of strategic pruning levels in achieving desired
efficiency gains. Furthermore, we analyze the balance between efficiency
improvements and environmental impact, highlighting how optimized model
architectures can contribute to a more sustainable computational ecosystem.
Implementing these techniques on a single GPU system, we determine that the
optimal configuration is 25\% pruning + knowledge distillation. This approach
yielded a 2.56x speedup in computation time while maintaining baseline accuracy
levels.",2024-09-21,"Aishwarya Mirashi, Purva Lingayat, Srushti Sonavane, Tejas Padhiyar, Raviraj Joshi, Geetanjali Kale",http://arxiv.org/pdf/2409.14162v1,cs.CL
Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis,"We find arithmetic ability resides within a limited number of attention
heads, with each head specializing in distinct operations. To delve into the
reason, we introduce the Comparative Neuron Analysis (CNA) method, which
identifies an internal logic chain consisting of four distinct stages from
input to prediction: feature enhancing with shallow FFN neurons, feature
transferring by shallow attention layers, feature predicting by arithmetic
heads, and prediction enhancing among deep FFN neurons. Moreover, we identify
the human-interpretable FFN neurons within both feature-enhancing and
feature-predicting stages. These findings lead us to investigate the mechanism
of LoRA, revealing that it enhances prediction probabilities by amplifying the
coefficient scores of FFN neurons related to predictions. Finally, we apply our
method in model pruning for arithmetic tasks and model editing for reducing
gender bias. Code is on https://github.com/zepingyu0512/arithmetic-mechanism.",2024-09-21,"Zeping Yu, Sophia Ananiadou",http://arxiv.org/pdf/2409.14144v1,cs.CL
Towards Automated Patent Workflows: AI-Orchestrated Multi-Agent Framework for Intellectual Property Management and Analysis,"Patents are the currency of innovation, and like any currency, they need to
be managed and protected (Gavin Potenza). Patents, as legal documents that
secure intellectual property rights, play a critical role in technological
innovation. The growing complexity of patent documents and the surge in patent
applications have created a need for automated solutions in patent analysis. In
this work, we present PatExpert, an autonomous multi-agent conversational
framework designed to streamline and optimize patent-related tasks. The
framework consists of a metaagent that coordinates task-specific expert agents
for various patent-related tasks and a critique agent for error handling and
feedback provision. The meta-agent orchestrates specialized expert agents, each
fine-tuned for specific tasks such as patent classification, acceptance, claim
generation, abstractive summarization, multi-patent analysis, and scientific
hypothesis generation. For multi-patent analysis, the framework incorporates
advanced methods like Graph Retrieval-Augmented Generation (GRAG) to enhance
response accuracy and relevance by combining semantic similarity with knowledge
graphs. Error handling is managed by critique agents (Gold-LLM-as-a-Judge and
Reward-LLM-as-a-Judge), which evaluate output responses for accuracy and
provide iterative feedback. The framework also prioritizes explainability,
ensuring transparent justifications for decisions made during patent analysis.
Its comprehensive capabilities make it a valuable tool for automating complex
patent workflows, enhancing efficiency, accuracy, and compliance in
patent-related tasks. Empirical evidence demonstrates significant improvements
in patent processing tasks, concluding that the framework offers a robust
solution for automating and optimizing patent analysis.",2024-09-21,"Sakhinana Sagar Srinivas, Vijay Sri Vaikunth, Venkataramana Runkana",http://arxiv.org/pdf/2409.19006v2,cs.CL
Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm,"Parameter-efficient fine-tuning (PEFT) has become a key training strategy for
large language models. However, its reliance on fewer trainable parameters
poses security risks, such as task-agnostic backdoors. Despite their severe
impact on a wide range of tasks, there is no practical defense solution
available that effectively counters task-agnostic backdoors within the context
of PEFT. In this study, we introduce Obliviate, a PEFT-integrable backdoor
defense. We develop two techniques aimed at amplifying benign neurons within
PEFT layers and penalizing the influence of trigger tokens. Our evaluations
across three major PEFT architectures show that our method can significantly
reduce the attack success rate of the state-of-the-art task-agnostic backdoors
(83.6%$\downarrow$). Furthermore, our method exhibits robust defense
capabilities against both task-specific backdoors and adaptive attacks. Source
code will be obtained at https://github.com/obliviateARR/Obliviate.",2024-09-21,"Jaehan Kim, Minkyoo Song, Seung Ho Na, Seungwon Shin",http://arxiv.org/pdf/2409.14119v3,cs.CL
Routing in Sparsely-gated Language Models responds to Context,"Language Models (LMs) recently incorporate mixture-of-experts layers
consisting of a router and a collection of experts to scale up their parameter
count given a fixed computational budget. Building on previous efforts
indicating that token-expert assignments are predominantly influenced by token
identities and positions, we trace routing decisions of similarity-annotated
text pairs to evaluate the context sensitivity of learned token-expert
assignments. We observe that routing in encoder layers mainly depends on
(semantic) associations, but contextual cues provide an additional layer of
refinement. Conversely, routing in decoder layers is more variable and markedly
less sensitive to context.",2024-09-21,"Stefan Arnold, Marian Fietta, Dilara Yesilbas",http://arxiv.org/pdf/2409.14107v1,cs.CL
Probing Context Localization of Polysemous Words in Pre-trained Language Model Sub-Layers,"In the era of high performing Large Language Models, researchers have widely
acknowledged that contextual word representations are one of the key drivers in
achieving top performances in downstream tasks. In this work, we investigate
the degree of contextualization encoded in the fine-grained sub-layer
representations of a Pre-trained Language Model (PLM) by empirical experiments
using linear probes. Unlike previous work, we are particularly interested in
identifying the strength of contextualization across PLM sub-layer
representations (i.e. Self-Attention, Feed-Forward Activation and Output
sub-layers). To identify the main contributions of sub-layers to
contextualisation, we first extract the sub-layer representations of polysemous
words in minimally different sentence pairs, and compare how these
representations change through the forward pass of the PLM network. Second, by
probing on a sense identification classification task, we try to empirically
localize the strength of contextualization information encoded in these
sub-layer representations. With these probing experiments, we also try to gain
a better understanding of the influence of context length and context richness
on the degree of contextualization. Our main conclusion is cautionary: BERT
demonstrates a high degree of contextualization in the top sub-layers if the
word in question is in a specific position in the sentence with a shorter
context window, but this does not systematically generalize across different
word positions and context sizes.",2024-09-21,"Soniya Vijayakumar, Josef van Genabith, Simon Ostermann",http://arxiv.org/pdf/2409.14097v1,cs.CL
PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL,"Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL
tasks, exhibiting remarkable reasoning capabilities. Different from tasks such
as math word problems and commonsense reasoning, SQL solutions have a
relatively fixed pattern. This facilitates the investigation of whether LLMs
can benefit from categorical thinking, mirroring how humans acquire knowledge
through inductive reasoning based on comparable examples. In this study, we
propose that employing query group partitioning allows LLMs to focus on
learning the thought processes specific to a single problem type, consequently
enhancing their reasoning abilities across diverse difficulty levels and
problem categories. Our experiments reveal that multiple advanced LLMs, when
equipped with PTD-SQL, can either surpass or match previous state-of-the-art
(SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with
varying initial performances have exhibited significant improvements, mainly at
the boundary of their capabilities after targeted drilling, suggesting a
parallel with human progress. Code is available at
https://github.com/lrlbbzl/PTD-SQL.",2024-09-21,"Ruilin Luo, Liyuan Wang, Binghuai Lin, Zicheng Lin, Yujiu Yang",http://arxiv.org/pdf/2409.14082v1,cs.CL
Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation,"Recent studies show that text-to-image (T2I) models are vulnerable to
adversarial attacks, especially with noun perturbations in text prompts. In
this study, we investigate the impact of adversarial attacks on different POS
tags within text prompts on the images generated by T2I models. We create a
high-quality dataset for realistic POS tag token swapping and perform
gradient-based attacks to find adversarial suffixes that mislead T2I models
into generating images with altered tokens. Our empirical results show that the
attack success rate (ASR) varies significantly among different POS tag
categories, with nouns, proper nouns, and adjectives being the easiest to
attack. We explore the mechanism behind the steering effect of adversarial
suffixes, finding that the number of critical tokens and content fusion vary
among POS tags, while features like suffix transferability are consistent
across categories. We have made our implementation publicly available at -
https://github.com/shahariar-shibli/Adversarial-Attack-on-POS-Tags.",2024-09-21,"G M Shahariar, Jia Chen, Jiachen Li, Yue Dong",http://arxiv.org/pdf/2409.15381v1,cs.CL
"What is a Digital Twin Anyway? Deriving the Definition for the Built Environment from over 15,000 Scientific Publications","The concept of digital twins has attracted significant attention across
various domains, particularly within the built environment. However, there is a
sheer volume of definitions and the terminological consensus remains out of
reach. The lack of a universally accepted definition leads to ambiguities in
their conceptualization and implementation, and may cause miscommunication for
both researchers and practitioners. We employed Natural Language Processing
(NLP) techniques to systematically extract and analyze definitions of digital
twins from a corpus of more than 15,000 full-text articles spanning diverse
disciplines. The study compares these findings with insights from an expert
survey that included 52 experts. The study identifies concurrence on the
components that comprise a ``Digital Twin'' from a practical perspective across
various domains, contrasting them with those that do not, to identify
deviations. We investigate the evolution of digital twin definitions over time
and across different scales, including manufacturing, building, and
urban/geospatial perspectives. We extracted the main components of Digital
Twins using Text Frequency Analysis and N-gram analysis. Subsequently, we
identified components that appeared in the literature and conducted a
Chi-square test to assess the significance of each component in different
domains. Our analysis identified key components of digital twins and revealed
significant variations in definitions based on application domains, such as
manufacturing, building, and urban contexts. The analysis of DT components
reveal two major groups of DT types: High-Performance Real-Time (HPRT) DTs, and
Long-Term Decision Support (LTDS) DTs. Contrary to common assumptions, we found
that components such as simulation, AI/ML, real-time capabilities, and
bi-directional data flow are not yet fully mature in the digital twins of the
built environment.",2024-09-21,"Mahmoud Abdelrahman, Edgardo Macatulad, Binyu Lei, Matias Quintana, Clayton Miller, Filip Biljecki",http://arxiv.org/pdf/2409.19005v3,cs.CL
MultiMed: Multilingual Medical Speech Recognition via Attention Encoder Decoder,"Multilingual automatic speech recognition (ASR) in the medical domain serves
as a foundational task for various downstream applications such as speech
translation, spoken language understanding, and voice-activated assistants.
This technology improves patient care by enabling efficient communication
across language barriers, alleviating specialized workforce shortages, and
facilitating improved diagnosis and treatment, particularly during pandemics.
In this work, we introduce MultiMed, the first multilingual medical ASR
dataset, along with the first collection of small-to-large end-to-end medical
ASR models, spanning five languages: Vietnamese, English, German, French, and
Mandarin Chinese. To our best knowledge, MultiMed stands as the world's largest
medical ASR dataset across all major benchmarks: total duration, number of
recording conditions, number of accents, and number of speaking roles.
Furthermore, we present the first multilinguality study for medical ASR, which
includes reproducible empirical baselines, a monolinguality-multilinguality
analysis, Attention Encoder Decoder (AED) vs Hybrid comparative study and a
linguistic analysis. We present practical ASR end-to-end training schemes
optimized for a fixed number of trainable parameters that are common in
industry settings. All code, data, and models are available online:
https://github.com/leduckhai/MultiMed/tree/master/MultiMed.",2024-09-21,"Khai Le-Duc, Phuc Phan, Tan-Hanh Pham, Bach Phan Tat, Minh-Huong Ngo, Chris Ngo, Thanh Nguyen-Tang, Truong-Son Hy",http://arxiv.org/pdf/2409.14074v3,cs.CL
Temporally Consistent Factuality Probing for Large Language Models,"The prolific use of Large Language Models (LLMs) as an alternate knowledge
base requires them to be factually consistent, necessitating both correctness
and consistency traits for paraphrased queries. Recently, significant attempts
have been made to benchmark datasets and metrics to evaluate LLMs for these
traits. However, structural simplicity (subject-relation-object) and
contemporary association in their query formulation limit the broader
definition of factuality and consistency. In this study, we introduce TeCFaP, a
novel Temporally Consistent Factuality Probe task to expand the consistent
factuality probe in the temporal dimension. To this end, we propose TEMP-COFAC,
a high-quality dataset of prefix-style English query paraphrases. Subsequently,
we extend the definitions of existing metrics to represent consistent
factuality across temporal dimension. We experiment with a diverse set of LLMs
and find most of them performing poorly on TeCFaP. Next, we propose a novel
solution CoTSeLF (Consistent-Time-Sensitive Learning Framework) combining
multi-task instruction tuning (MT-IT) with consistent-time-sensitive
reinforcement learning (CTSRL) to improve temporally consistent factuality in
LLMs. Our experiments demonstrate the efficacy of CoTSeLF over several
baselines.",2024-09-21,"Ashutosh Bajpai, Aaryan Goyal, Atif Anwer, Tanmoy Chakraborty",http://arxiv.org/pdf/2409.14065v2,cs.CL
Co-occurrence is not Factual Association in Language Models,"Pretrained language models can encode a large amount of knowledge and utilize
it for various reasoning tasks, yet they can still struggle to learn novel
factual knowledge effectively from finetuning on limited textual
demonstrations. In this work, we show that the reason for this deficiency is
that language models are biased to learn word co-occurrence statistics instead
of true factual associations. We identify the differences between two forms of
knowledge representation in language models: knowledge in the form of
co-occurrence statistics is encoded in the middle layers of the transformer
model and does not generalize well to reasoning scenarios beyond simple
question answering, while true factual associations are encoded in the lower
layers and can be freely utilized in various reasoning tasks. Based on these
observations, we propose two strategies to improve the learning of factual
associations in language models. We show that training on text with implicit
rather than explicit factual associations can force the model to learn factual
associations instead of co-occurrence statistics, significantly improving the
generalization of newly learned knowledge. We also propose a simple training
method to actively forget the learned co-occurrence statistics, which unblocks
and enhances the learning of factual associations when training on plain
narrative text. On both synthetic and real-world corpora, the two proposed
strategies improve the generalization of the knowledge learned during
finetuning to reasoning scenarios such as indirect and multi-hop question
answering.",2024-09-21,"Xiao Zhang, Miao Li, Ji Wu",http://arxiv.org/pdf/2409.14057v1,cs.CL
GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion,"In recent years, Large Language Models (LLMs) have demonstrated remarkable
capabilities across diverse NLP tasks. Extensive research has explored how to
enhance the logical reasoning abilities such as Chain-of-Thought,
Chain-of-Thought with Self-Consistency, Tree-Of-Thoughts, and multi-agent
debates. In the context of multi-agent debates, significant performance
improvements can be achieved with an increasing number of agents and debate
rounds. However, the escalation in the number of agents and debate rounds can
drastically raise the tokens cost of debates, thereby limiting the scalability
of the multi-agent debate technique. To better harness the advantages of
multi-agent debates in logical reasoning tasks, this paper proposes a method to
significantly reduce token cost in multi-agent debates. This approach involves
dividing all agents into multiple debate groups, with agents engaging in
debates within their respective groups and sharing interim debate results
between groups. Comparative experiments across multiple datasets have
demonstrated that this method can reduce the total tokens by up to 51.7% during
debates and while potentially enhancing accuracy by as much as 25%. Our method
significantly enhances the performance and efficiency of interactions in the
multi-agent debate.",2024-09-21,"Tongxuan Liu, Xingyu Wang, Weizhe Huang, Wenjiang Xu, Yuting Zeng, Lei Jiang, Hailong Yang, Jing Li",http://arxiv.org/pdf/2409.14051v1,cs.CL
OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching,"Hallucinations of large language models (LLMs) commonly occur in
domain-specific downstream tasks, with no exception in ontology matching (OM).
The prevalence of using LLMs for OM raises the need for benchmarks to better
understand LLM hallucinations. The OAEI-LLM dataset is an extended version of
the Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate
LLM-specific hallucinations in OM tasks. We outline the methodology used in
dataset construction and schema extension, and provide examples of potential
use cases.",2024-09-21,"Zhangcheng Qiang, Kerry Taylor, Weiqing Wang, Jing Jiang",http://arxiv.org/pdf/2409.14038v5,cs.CL
Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators,"Large Language Models (LLMs) and AI assistants driven by these models are
experiencing exponential growth in usage among both expert and amateur users.
In this work, we focus on evaluating the reliability of current LLMs as science
communicators. Unlike existing benchmarks, our approach emphasizes assessing
these models on scientific questionanswering tasks that require a nuanced
understanding and awareness of answerability. We introduce a novel dataset,
SCiPS-QA, comprising 742 Yes/No queries embedded in complex scientific
concepts, along with a benchmarking suite that evaluates LLMs for correctness
and consistency across various criteria. We benchmark three proprietary LLMs
from the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2,
Llama-3, and Mistral families. While most open-access models significantly
underperform compared to GPT-4 Turbo, our experiments identify Llama-3-70B as a
strong competitor, often surpassing GPT-4 Turbo in various evaluation aspects.
We also find that even the GPT models exhibit a general incompetence in
reliably verifying LLM responses. Moreover, we observe an alarming trend where
human evaluators are deceived by incorrect responses from GPT-4 Turbo.",2024-09-21,"Prasoon Bajpai, Niladri Chatterjee, Subhabrata Dutta, Tanmoy Chakraborty",http://arxiv.org/pdf/2409.14037v1,cs.CL
Uncovering Latent Chain of Thought Vectors in Language Models,"In this work, we examine how targeted perturbations in the activation space
of Language Models (LMs) can encode complex reasoning patterns. We inject
steering vectors, derived from LM activations, into LMs during inference time
and study whether these vectors can induce Chain-of-Thought (CoT) reasoning in
LMs without the need for natural language prompting. We demonstrate this
approach on Llama3 8B Instruct and Mistral 7B v0.2 Instruct and show that
activation-space interventions achieve competitive, if not superior,
performance compared to traditional CoT prompting across multiple reasoning
benchmarks, including GSM8k, MMLU, AGI Eval, and ARC AI2. These findings
suggest that neural network activations can encode reasoning patterns, offering
a new application of activation space manipulation as a tool for tuning model
behavior.",2024-09-21,"Jason Zhang, Scott Viteri",http://arxiv.org/pdf/2409.14026v3,cs.CL
StateAct: Enhancing LLM Base Agents via Self-prompting and State-tracking,"Large language models (LLMs) are increasingly used as autonomous agents,
tackling tasks from robotics to web navigation. Their performance depends on
the underlying base agent. Existing methods, however, struggle with
long-context reasoning and goal adherence. We introduce StateAct, a novel and
efficient base agent that enhances decision-making through (1) self-prompting,
which reinforces task goals at every step, and (2) chain-of-states, an
extension of chain-of-thought that tracks state information over time. StateAct
outperforms ReAct, the previous best base agent, by over 10% on Alfworld, 30%
on Textcraft, and 7% on Webshop across multiple frontier LLMs. We also
demonstrate that StateAct can be used as a drop-in replacement for ReAct with
advanced LLM agent methods such as test-time scaling, yielding an additional
12% gain on Textcraft. By improving efficiency and long-range reasoning without
requiring additional training or retrieval, StateAct provides a scalable
foundation for LLM agents. We open source our code to support further research
at https://github.com/ai-nikolai/stateact .",2024-09-21,"Nikolai Rozanov, Marek Rei",http://arxiv.org/pdf/2410.02810v3,cs.CL
Graph Neural Network Framework for Sentiment Analysis Using Syntactic Feature,"Amidst the swift evolution of social media platforms and e-commerce
ecosystems, the domain of opinion mining has surged as a pivotal area of
exploration within natural language processing. A specialized segment within
this field focuses on extracting nuanced evaluations tied to particular
elements within textual contexts. This research advances a composite framework
that amalgamates the positional cues of topical descriptors. The proposed
system converts syntactic structures into a matrix format, leveraging
convolutions and attention mechanisms within a graph to distill salient
characteristics. Incorporating the positional relevance of descriptors relative
to lexical items enhances the sequential integrity of the input. Trials have
substantiated that this integrated graph-centric scheme markedly elevates the
efficacy of evaluative categorization, showcasing preeminence.",2024-09-21,"Linxiao Wu, Yuanshuai Luo, Binrong Zhu, Guiran Liu, Rui Wang, Qian Yu",http://arxiv.org/pdf/2409.14000v1,cs.CL
Contrastive Learning for Knowledge-Based Question Generation in Large Language Models,"With the rapid development of artificial intelligence technology, especially
the increasingly widespread application of question-and-answer systems,
high-quality question generation has become a key component in supporting the
development of these systems. This article focuses on knowledge-based question
generation technology, which aims to enable computers to simulate the human
questioning process based on understanding specific texts or knowledge bases.
In light of the issues of hallucination and knowledge gaps present in
large-scale language models when applied to knowledge-intensive tasks, this
paper proposes an enhanced question generation method that incorporates
contrastive learning. This method utilizes multiple models to jointly mine
domain knowledge and uses contrastive learning to guide the model in reducing
noise and hallucinations in generation. Experimental results show that by
designing prompts containing contrasting examples, the model's performance in
question generation improves considerably, particularly when contrasting
instructions and examples are used simultaneously, leading to the highest
quality of generated questions and improved accuracy. These results demonstrate
that the method proposed in this study, which combines contrasting context and
chain-of-thought prompts, can effectively improve both the quality and the
practicality of question generation.",2024-09-21,"Zhenhong Zhang, Jiajing Chen, Weiyan Shi, Lingjie Yi, Chihang Wang, Qian Yu",http://arxiv.org/pdf/2409.13994v2,cs.CL
SMART-RAG: Selection using Determinantal Matrices for Augmented Retrieval,"Retrieval-Augmented Generation (RAG) has greatly improved large language
models (LLMs) by enabling them to generate accurate, contextually grounded
responses through the integration of external information. However,
conventional RAG approaches, which prioritize top-ranked documents based solely
on query-context relevance, often introduce redundancy and conflicting
information. This issue is particularly evident in unsupervised retrieval
settings, where there are no mechanisms to effectively mitigate these problems,
leading to suboptimal context selection. To address this, we propose Selection
using Matrices for Augmented Retrieval (SMART) in question answering tasks, a
fully unsupervised and training-free framework designed to optimize context
selection in RAG. SMART leverages Determinantal Point Processes (DPPs) to
simultaneously model relevance, diversity and conflict, ensuring the selection
of potentially high-quality contexts. Experimental results across multiple
datasets demonstrate that SMART significantly enhances QA performance and
surpasses previous unsupervised context selection methods, showing a promising
strategy for RAG.",2024-09-21,"Jiatao Li, Xinyu Hu, Xiaojun Wan",http://arxiv.org/pdf/2409.13992v1,cs.CL
ChemEval: A Comprehensive Multi-Level Chemical Evaluation for Large Language Models,"There is a growing interest in the role that LLMs play in chemistry which
lead to an increased focus on the development of LLMs benchmarks tailored to
chemical domains to assess the performance of LLMs across a spectrum of
chemical tasks varying in type and complexity. However, existing benchmarks in
this domain fail to adequately meet the specific requirements of chemical
research professionals. To this end, we propose \textbf{\textit{ChemEval}},
which provides a comprehensive assessment of the capabilities of LLMs across a
wide range of chemical domain tasks. Specifically, ChemEval identified 4
crucial progressive levels in chemistry, assessing 12 dimensions of LLMs across
42 distinct chemical tasks which are informed by open-source data and the data
meticulously crafted by chemical experts, ensuring that the tasks have
practical value and can effectively evaluate the capabilities of LLMs. In the
experiment, we evaluate 12 mainstream LLMs on ChemEval under zero-shot and
few-shot learning contexts, which included carefully selected demonstration
examples and carefully designed prompts. The results show that while general
LLMs like GPT-4 and Claude-3.5 excel in literature understanding and
instruction following, they fall short in tasks demanding advanced chemical
knowledge. Conversely, specialized LLMs exhibit enhanced chemical competencies,
albeit with reduced literary comprehension. This suggests that LLMs have
significant potential for enhancement when tackling sophisticated tasks in the
field of chemistry. We believe our work will facilitate the exploration of
their potential to drive progress in chemistry. Our benchmark and analysis will
be available at {\color{blue} \url{https://github.com/USTC-StarTeam/ChemEval}}.",2024-09-21,"Yuqing Huang, Rongyang Zhang, Xuesong He, Xuyang Zhi, Hao Wang, Xin Li, Feiyang Xu, Deguang Liu, Huadong Liang, Yi Li, Jian Cui, Zimu Liu, Shijin Wang, Guoping Hu, Guiquan Liu, Qi Liu, Defu Lian, Enhong Chen",http://arxiv.org/pdf/2409.13989v1,cs.CL
Role-Play Paradox in Large Language Models: Reasoning Performance Gains and Ethical Dilemmas,"Role-play in large language models (LLMs) enhances their ability to generate
contextually relevant and high-quality responses by simulating diverse
cognitive perspectives. However, our study identifies significant risks
associated with this technique. First, we demonstrate that autotuning, a method
used to auto-select models' roles based on the question, can lead to the
generation of harmful outputs, even when the model is tasked with adopting
neutral roles. Second, we investigate how different roles affect the likelihood
of generating biased or harmful content. Through testing on benchmarks
containing stereotypical and harmful questions, we find that role-play
consistently amplifies the risk of biased outputs. Our results underscore the
need for careful consideration of both role simulation and tuning processes
when deploying LLMs in sensitive or high-stakes contexts.",2024-09-21,"Jinman Zhao, Zifan Qian, Linbo Cao, Yining Wang, Yitian Ding, Yulan Hu, Zeyu Zhang, Zeyong Jin",http://arxiv.org/pdf/2409.13979v2,cs.CL
Can Language Model Understand Word Semantics as A Chatbot? An Empirical Study of Language Model Internal External Mismatch,"Current common interactions with language models is through full inference.
This approach may not necessarily align with the model's internal knowledge.
Studies show discrepancies between prompts and internal representations. Most
focus on sentence understanding. We study the discrepancy of word semantics
understanding in internal and external mismatch across Encoder-only,
Decoder-only, and Encoder-Decoder pre-trained language models.",2024-09-21,"Jinman Zhao, Xueyan Zhang, Xingyu Yue, Weizhe Chen, Zifan Qian, Ruiyu Wang",http://arxiv.org/pdf/2409.13972v1,cs.CL
Exploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank,"In this paper, we study an under-explored area of language and vocabulary
learning: keyword mnemonics, a technique for memorizing vocabulary through
memorable associations with a target word via a verbal cue. Typically, creating
verbal cues requires extensive human effort and is quite time-consuming,
necessitating an automated method that is more scalable. We propose a novel
overgenerate-and-rank method via prompting large language models (LLMs) to
generate verbal cues and then ranking them according to psycholinguistic
measures and takeaways from a pilot user study. To assess cue quality, we
conduct both an automated evaluation of imageability and coherence, as well as
a human evaluation involving English teachers and learners. Results show that
LLM-generated mnemonics are comparable to human-generated ones in terms of
imageability, coherence, and perceived usefulness, but there remains plenty of
room for improvement due to the diversity in background and preference among
language learners.",2024-09-21,"Jaewook Lee, Hunter McNichols, Andrew Lan",http://arxiv.org/pdf/2409.13952v1,cs.CL
Mufu: Multilingual Fused Learning for Low-Resource Translation with LLM,"Multilingual large language models (LLMs) are great translators, but this is
largely limited to high-resource languages. For many LLMs, translating in and
out of low-resource languages remains a challenging task. To maximize data
efficiency in this low-resource setting, we introduce Mufu, which includes a
selection of automatically generated multilingual candidates and an instruction
to correct inaccurate translations in the prompt. Mufu prompts turn a
translation task into a postediting one, and seek to harness the LLM's
reasoning capability with auxiliary translation candidates, from which the
model is required to assess the input quality, align the semantics
cross-lingually, copy from relevant inputs and override instances that are
incorrect. Our experiments on En-XX translations over the Flores-200 dataset
show LLMs finetuned against Mufu-style prompts are robust to poor quality
auxiliary translation candidates, achieving performance superior to NLLB 1.3B
distilled model in 64% of low- and very-low-resource language pairs. We then
distill these models to reduce inference cost, while maintaining on average 3.1
chrF improvement over finetune-only baseline in low-resource translations.",2024-09-20,"Zheng Wei Lim, Nitish Gupta, Honglin Yu, Trevor Cohn",http://arxiv.org/pdf/2409.13949v2,cs.CL
Aligning Language Models Using Follow-up Likelihood as Reward Signal,"In natural human-to-human conversations, participants often receive feedback
signals from one another based on their follow-up reactions. These reactions
can include verbal responses, facial expressions, changes in emotional state,
and other non-verbal cues. Similarly, in human-machine interactions, the
machine can leverage the user's follow-up utterances as feedback signals to
assess whether it has appropriately addressed the user's request. Therefore, we
propose using the likelihood of follow-up utterances as rewards to
differentiate preferred responses from less favored ones, without relying on
human or commercial LLM-based preference annotations. Our proposed reward
mechanism, ``Follow-up Likelihood as Reward"" (FLR), matches the performance of
strong reward models trained on large-scale human or GPT-4 annotated data on 8
pairwise-preference and 4 rating-based benchmarks. Building upon the FLR
mechanism, we propose to automatically mine preference data from the online
generations of a base policy model. The preference data are subsequently used
to boost the helpfulness of the base model through direct alignment from
preference (DAP) methods, such as direct preference optimization (DPO). Lastly,
we demonstrate that fine-tuning the language model that provides follow-up
likelihood with natural language feedback significantly enhances FLR's
performance on reward modeling benchmarks and effectiveness in aligning the
base policy model's helpfulness.",2024-09-20,"Chen Zhang, Dading Chong, Feng Jiang, Chengguang Tang, Anningzhe Gao, Guohua Tang, Haizhou Li",http://arxiv.org/pdf/2409.13948v3,cs.CL
MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models,"This study explores the effectiveness of Large Language Models (LLMs) in
creating personalized ""mirror stories"" that reflect and resonate with
individual readers' identities, addressing the significant lack of diversity in
literature. We present MirrorStories, a corpus of 1,500 personalized short
stories generated by integrating elements such as name, gender, age, ethnicity,
reader interest, and story moral. We demonstrate that LLMs can effectively
incorporate diverse identity elements into narratives, with human evaluators
identifying personalized elements in the stories with high accuracy. Through a
comprehensive evaluation involving 26 diverse human judges, we compare the
effectiveness of MirrorStories against generic narratives. We find that
personalized LLM-generated stories not only outscore generic human-written and
LLM-generated ones across all metrics of engagement (with average ratings of
4.22 versus 3.37 on a 5-point scale), but also achieve higher textual diversity
while preserving the intended moral. We also provide analyses that include bias
assessments and a study on the potential for integrating images into
personalized stories.",2024-09-20,"Sarfaroz Yunusov, Hamza Sidat, Ali Emami",http://arxiv.org/pdf/2409.13935v2,cs.CL
On-Device Collaborative Language Modeling via a Mixture of Generalists and Specialists,"On-device LLMs have gained increasing attention for their ability to enhance
privacy and provide a personalized user experience. To facilitate private
learning with scarce data, Federated Learning has become a standard approach.
However, it faces challenges such as computational resource heterogeneity and
data heterogeneity among end users. We propose CoMiGS ($\textbf{Co}$llaborative
learning with a $\textbf{Mi}$xture of $\textbf{G}$eneralists and
$\textbf{S}$pecialists), the first approach to address both challenges. A key
innovation of our method is the bi-level optimization formulation of the
Mixture-of-Experts learning objective, where the router is optimized using a
separate validation set to ensure alignment with the target distribution. We
solve our objective with alternating minimization, for which we provide a
theoretical analysis. Our method shares generalist experts across users while
localizing a varying number of specialist experts, thereby adapting to users'
computational resources and preserving privacy. Through extensive experiments,
we show CoMiGS effectively balances general and personalized knowledge for each
token generation. We demonstrate that CoMiGS remains robust against
overfitting-due to the generalists' regularizing effect-while adapting to local
data through specialist expertise. We open source our codebase for
collaborative LLMs.",2024-09-20,"Dongyang Fan, Bettina Messmer, Nikita Doikov, Martin Jaggi",http://arxiv.org/pdf/2409.13931v3,cs.CL
Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation,"We study the code generation behavior of instruction-tuned models built on
top of code pre-trained language models when they could access an auxiliary
function to implement a function. We design several ways to provide auxiliary
functions to the models by adding them to the query or providing a response
prefix to incorporate the ability to utilize auxiliary functions with the
instruction-following capability. Our experimental results show the
effectiveness of combining the base models' auxiliary function utilization
ability with the instruction following ability. In particular, the performance
of adopting our approaches with the open-sourced language models surpasses that
of the recent powerful proprietary language models, i.e., gpt-4o.",2024-09-20,"Seonghyeon Lee, Suyeon Kim, Joonwon Jang, Heejae Chon, Dongha Lee, Hwanjo Yu",http://arxiv.org/pdf/2409.13928v1,cs.CL
"One Model is All You Need: ByT5-Sanskrit, a Unified Model for Sanskrit NLP Tasks","Morphologically rich languages are notoriously challenging to process for
downstream NLP applications. This paper presents a new pretrained language
model, ByT5-Sanskrit, designed for NLP applications involving the
morphologically rich language Sanskrit. We evaluate ByT5-Sanskrit on
established Sanskrit word segmentation tasks, where it outperforms previous
data-driven approaches by a considerable margin and matches the performance of
the current best lexicon-based model. It is easier to deploy and more robust to
data not covered by external linguistic resources. It also achieves new
state-of-the-art results in Vedic Sanskrit dependency parsing and OCR
post-correction tasks. Additionally, based on the Digital Corpus of Sanskrit,
we introduce a novel multitask dataset for the joint training of Sanskrit word
segmentation, lemmatization, and morphosyntactic tagging tasks. We fine-tune
ByT5-Sanskrit on this dataset, creating a versatile multitask model for various
downstream Sanskrit applications. We have used this model in Sanskrit
linguistic annotation projects, in information retrieval setups, and as a
preprocessing step in a Sanskrit machine translation pipeline. We also show
that our approach yields new best scores for lemmatization and dependency
parsing of other morphologically rich languages. We thus demonstrate that
byte-level pretrained language models can achieve excellent performance for
morphologically rich languages, outperforming tokenizer-based models and
presenting an important vector of exploration when constructing NLP pipelines
for such languages.",2024-09-20,"Sebastian Nehrdich, Oliver Hellwig, Kurt Keutzer",http://arxiv.org/pdf/2409.13920v1,cs.CL
Target word activity detector: An approach to obtain ASR word boundaries without lexicon,"Obtaining word timestamp information from end-to-end (E2E) ASR models remains
challenging due to the lack of explicit time alignment during training. This
issue is further complicated in multilingual models. Existing methods, either
rely on lexicons or introduce additional tokens, leading to scalability issues
and increased computational costs. In this work, we propose a new approach to
estimate word boundaries without relying on lexicons. Our method leverages word
embeddings from sub-word token units and a pretrained ASR model, requiring only
word alignment information during training. Our proposed method can scale-up to
any number of languages without incurring any additional cost. We validate our
approach using a multilingual ASR model trained on five languages and
demonstrate its effectiveness against a strong baseline.",2024-09-20,"Sunit Sivasankaran, Eric Sun, Jinyu Li, Yan Huang, Jing Pan",http://arxiv.org/pdf/2409.13913v1,cs.CL
Enhancing Large Language Models with Domain-specific Retrieval Augment Generation: A Case Study on Long-form Consumer Health Question Answering in Ophthalmology,"Despite the potential of Large Language Models (LLMs) in medicine, they may
generate responses lacking supporting evidence or based on hallucinated
evidence. While Retrieval Augment Generation (RAG) is popular to address this
issue, few studies implemented and evaluated RAG in downstream domain-specific
applications. We developed a RAG pipeline with 70,000 ophthalmology-specific
documents that retrieve relevant documents to augment LLMs during inference
time. In a case study on long-form consumer health questions, we systematically
evaluated the responses including over 500 references of LLMs with and without
RAG on 100 questions with 10 healthcare professionals. The evaluation focuses
on factuality of evidence, selection and ranking of evidence, attribution of
evidence, and answer accuracy and completeness. LLMs without RAG provided 252
references in total. Of which, 45.3% hallucinated, 34.1% consisted of minor
errors, and 20.6% were correct. In contrast, LLMs with RAG significantly
improved accuracy (54.5% being correct) and reduced error rates (18.8% with
minor hallucinations and 26.7% with errors). 62.5% of the top 10 documents
retrieved by RAG were selected as the top references in the LLM response, with
an average ranking of 4.9. The use of RAG also improved evidence attribution
(increasing from 1.85 to 2.49 on a 5-point scale, P<0.001), albeit with slight
decreases in accuracy (from 3.52 to 3.23, P=0.03) and completeness (from 3.47
to 3.27, P=0.17). The results demonstrate that LLMs frequently exhibited
hallucinated and erroneous evidence in the responses, raising concerns for
downstream applications in the medical domain. RAG substantially reduced the
proportion of such evidence but encountered challenges.",2024-09-20,"Aidan Gilson, Xuguang Ai, Thilaka Arunachalam, Ziyou Chen, Ki Xiong Cheong, Amisha Dave, Cameron Duic, Mercy Kibe, Annette Kaminaka, Minali Prasad, Fares Siddig, Maxwell Singer, Wendy Wong, Qiao Jin, Tiarnan D. L. Keenan, Xia Hu, Emily Y. Chew, Zhiyong Lu, Hua Xu, Ron A. Adelman, Yih-Chung Tham, Qingyu Chen",http://arxiv.org/pdf/2409.13902v1,cs.CL
LLM for Everyone: Representing the Underrepresented in Large Language Models,"Natural language processing (NLP) has witnessed a profound impact of large
language models (LLMs) that excel in a multitude of tasks. However, the
limitation of LLMs in multilingual settings, particularly in underrepresented
languages, remains a significant hurdle. This thesis aims to bridge the gap in
NLP research and development by focusing on underrepresented languages. A
comprehensive evaluation of LLMs is conducted to assess their capabilities in
these languages, revealing the challenges of multilingual and multicultural
generalization. Addressing the multilingual generalization gap, this thesis
proposes data-and-compute-efficient methods to mitigate the disparity in LLM
ability in underrepresented languages, allowing better generalization on
underrepresented languages without the loss of task generalization ability. The
proposed solutions cover cross-lingual continual instruction tuning,
retrieval-based cross-lingual in-context learning, and in-context query
alignment. Furthermore, a novel method to measure cultural values alignment
between LLMs operating in different languages is proposed, ensuring cultural
sensitivity and inclusivity. These contributions aim to enhance the
multilingual and multicultural alignment of LLMs in underrepresented languages,
ultimately advancing the NLP field toward greater equality and inclusiveness.",2024-09-20,Samuel Cahyawijaya,http://arxiv.org/pdf/2409.13897v1,cs.CL
Transfer Learning with Clinical Concept Embeddings from Large Language Models,"Knowledge sharing is crucial in healthcare, especially when leveraging data
from multiple clinical sites to address data scarcity, reduce costs, and enable
timely interventions. Transfer learning can facilitate cross-site knowledge
transfer, but a major challenge is heterogeneity in clinical concepts across
different sites. Large Language Models (LLMs) show significant potential of
capturing the semantic meaning of clinical concepts and reducing heterogeneity.
This study analyzed electronic health records from two large healthcare systems
to assess the impact of semantic embeddings from LLMs on local, shared, and
transfer learning models. Results indicate that domain-specific LLMs, such as
Med-BERT, consistently outperform in local and direct transfer scenarios, while
generic models like OpenAI embeddings require fine-tuning for optimal
performance. However, excessive tuning of models with biomedical embeddings may
reduce effectiveness, emphasizing the need for balance. This study highlights
the importance of domain-specific embeddings and careful model tuning for
effective knowledge transfer in healthcare.",2024-09-20,"Yuhe Gao, Runxue Bao, Yuelyu Ji, Yiming Sun, Chenxi Song, Jeffrey P. Ferraro, Ye Ye",http://arxiv.org/pdf/2409.13893v1,cs.CL
A Multi-LLM Debiasing Framework,"Large Language Models (LLMs) are powerful tools with the potential to benefit
society immensely, yet, they have demonstrated biases that perpetuate societal
inequalities. Despite significant advancements in bias mitigation techniques
using data augmentation, zero-shot prompting, and model fine-tuning, biases
continuously persist, including subtle biases that may elude human detection.
Recent research has shown a growing interest in multi-LLM approaches, which
have been demonstrated to be effective in improving the quality of reasoning
and factuality in LLMs. Building on this approach, we propose a novel multi-LLM
debiasing framework aimed at reducing bias in LLMs. Our work is the first to
introduce and evaluate two distinct approaches within this framework for
debiasing LLMs: a centralized method, where the conversation is facilitated by
a single central LLM, and a decentralized method, where all models communicate
directly. Our findings reveal that our multi-LLM framework significantly
reduces bias in LLMs, outperforming the baseline method across several social
groups.",2024-09-20,"Deonna M. Owens, Ryan A. Rossi, Sungchul Kim, Tong Yu, Franck Dernoncourt, Xiang Chen, Ruiyi Zhang, Jiuxiang Gu, Hanieh Deilamsalehy, Nedim Lipka",http://arxiv.org/pdf/2409.13884v1,cs.CL
"""I Never Said That"": A dataset, taxonomy and baselines on response clarity classification","Equivocation and ambiguity in public speech are well-studied discourse
phenomena, especially in political science and analysis of political
interviews. Inspired by the well-grounded theory on equivocation, we aim to
resolve the closely related problem of response clarity in questions extracted
from political interviews, leveraging the capabilities of Large Language Models
(LLMs) and human expertise. To this end, we introduce a novel taxonomy that
frames the task of detecting and classifying response clarity and a
corresponding clarity classification dataset which consists of question-answer
(QA) pairs drawn from political interviews and annotated accordingly. Our
proposed two-level taxonomy addresses the clarity of a response in terms of the
information provided for a given question (high-level) and also provides a
fine-grained taxonomy of evasion techniques that relate to unclear, ambiguous
responses (lower-level). We combine ChatGPT and human annotators to collect,
validate and annotate discrete QA pairs from political interviews, to be used
for our newly introduced response clarity task. We provide a detailed analysis
and conduct several experiments with different model architectures, sizes and
adaptation methods to gain insights and establish new baselines over the
proposed dataset and task.",2024-09-20,"Konstantinos Thomas, Giorgos Filandrianos, Maria Lymperaiou, Chrysoula Zerva, Giorgos Stamou",http://arxiv.org/pdf/2409.13879v1,cs.CL
Instruct-Tuning Pretrained Causal Language Models for Ancient Greek Papyrology and Epigraphy,"This article presents an experiment in fine-tuning a pretrained causal
language model (Meta's Llama 3.1 8B Instruct) to assist with restoring missing
or illegible characters in ancient Greek inscriptions and documentary papyri.
Utilizing a straightforward instruction-based approach and a 95%/5% train/test
split, the papyrus restoration model achieved a character error rate (CER) of
14.9%, a top-1 accuracy of 73.5%, and a top-20 accuracy of 86.0% for sequences
up to 10 characters. A model was also fine-tuned for geographic attribution,
reaching a top-1 accuracy of 66.4% and a top-3 accuracy of 79.9%. In
chronological attribution, it demonstrated an average deviation of 21.7 years
from the actual terminus post/ante quem, with a median deviation of 0 years.
For inscriptions, the restoration model achieved a CER of 20.5%, a top-1
accuracy of 63.7%, and a top-20 accuracy of 83.0% for sequences up to 10
characters. In geographic attribution, it attained a top-1 accuracy of 75.0%
and a top-3 accuracy of 83.7%, while in dating, it had an average deviation of
37.1 years and a median deviation of 3 years from the actual date range.
Benchmarked against the state-of-the-art model (Ithaca) on a shared test set
and on recently edited inscriptions, the instruction-tuned models excelled in
text restoration, while also offering the practical advantage of ignoring
spaces during reconstruction, which aligns with the scriptio continua of
ancient textual artifacts. However, their performance in geographic and
chronological attribution was lower than Ithaca's. To evaluate the approach in
a more even setup, the instruction model was retrained with an 80%/10%/10%
train-validation-test split, and still outperformed Ithaca in text restoration.
The results suggest that fine-tuning larger pretrained causal language models
using instruction templates for emendations and conjectures to ancient texts
holds promise.",2024-09-20,Eric Cullhed,http://arxiv.org/pdf/2409.13870v3,cs.CL
"Generative AI Carries Non-Democratic Biases and Stereotypes: Representation of Women, Black Individuals, Age Groups, and People with Disability in AI-Generated Images across Occupations","AI governance and ethics in AI development have become critical concerns,
prompting active discussions among tech companies, governments, and researchers
about the potential risks AI poses to our democracies. This short essay aims to
highlight one such risk: how generative AI includes or excludes
equity-deserving groups in its outputs. The findings reveal that generative AI
is not equitably inclusive regarding gender, race, age, and visible disability.",2024-09-20,Ayoob Sadeghiani,http://arxiv.org/pdf/2409.13869v1,cs.CL
Unlocking Memorization in Large Language Models with Dynamic Soft Prompting,"Pretrained large language models (LLMs) have revolutionized natural language
processing (NLP) tasks such as summarization, question answering, and
translation. However, LLMs pose significant security risks due to their
tendency to memorize training data, leading to potential privacy breaches and
copyright infringement. Accurate measurement of this memorization is essential
to evaluate and mitigate these potential risks. However, previous attempts to
characterize memorization are constrained by either using prefixes only or by
prepending a constant soft prompt to the prefixes, which cannot react to
changes in input. To address this challenge, we propose a novel method for
estimating LLM memorization using dynamic, prefix-dependent soft prompts. Our
approach involves training a transformer-based generator to produce soft
prompts that adapt to changes in input, thereby enabling more accurate
extraction of memorized data. Our method not only addresses the limitations of
previous methods but also demonstrates superior performance in diverse
experimental settings compared to state-of-the-art techniques. In particular,
our method can achieve the maximum relative improvement of 112.75% and 32.26%
over the vanilla baseline in terms of discoverable memorization rate for the
text generation task and code generation task respectively.",2024-09-20,"Zhepeng Wang, Runxue Bao, Yawen Wu, Jackson Taylor, Cao Xiao, Feng Zheng, Weiwen Jiang, Shangqian Gao, Yanfu Zhang",http://arxiv.org/pdf/2409.13853v1,cs.CL
Do language models practice what they preach? Examining language ideologies about gendered language reform encoded in LLMs,"We study language ideologies in text produced by LLMs through a case study on
English gendered language reform (related to role nouns like
congressperson/-woman/-man, and singular they). First, we find political bias:
when asked to use language that is ""correct"" or ""natural"", LLMs use language
most similarly to when asked to align with conservative (vs. progressive)
values. This shows how LLMs' metalinguistic preferences can implicitly
communicate the language ideologies of a particular political group, even in
seemingly non-political contexts. Second, we find LLMs exhibit internal
inconsistency: LLMs use gender-neutral variants more often when more explicit
metalinguistic context is provided. This shows how the language ideologies
expressed in text produced by LLMs can vary, which may be unexpected to users.
We discuss the broader implications of these findings for value alignment.",2024-09-20,"Julia Watson, Sophia Lee, Barend Beekhuizen, Suzanne Stevenson",http://arxiv.org/pdf/2409.13852v1,cs.CL
STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions,"Mitigating explicit and implicit biases in Large Language Models (LLMs) has
become a critical focus in the field of natural language processing. However,
many current methodologies evaluate scenarios in isolation, without considering
the broader context or the spectrum of potential biases within each situation.
To address this, we introduce the Sensitivity Testing on Offensive Progressions
(STOP) dataset, which includes 450 offensive progressions containing 2,700
unique sentences of varying severity that progressively escalate from less to
more explicitly offensive. Covering a broad spectrum of 9 demographics and 46
sub-demographics, STOP ensures inclusivity and comprehensive coverage. We
evaluate several leading closed- and open-source models, including GPT-4,
Mixtral, and Llama 3. Our findings reveal that even the best-performing models
detect bias inconsistently, with success rates ranging from 19.3% to 69.8%. We
also demonstrate how aligning models with human judgments on STOP can improve
model answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs
by up to 191%, while maintaining or even improving performance. STOP presents a
novel framework for assessing the complex nature of biases in LLMs, which will
enable more effective bias mitigation strategies and facilitates the creation
of fairer language models.",2024-09-20,"Robert Morabito, Sangmitra Madhusudan, Tyler McDonald, Ali Emami",http://arxiv.org/pdf/2409.13843v2,cs.CL
GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music Scores for All Singing Tasks,"The scarcity of high-quality and multi-task singing datasets significantly
hinders the development of diverse controllable and personalized singing tasks,
as existing singing datasets suffer from low quality, limited diversity of
languages and singers, absence of multi-technique information and realistic
music scores, and poor task suitability. To tackle these problems, we present
GTSinger, a large global, multi-technique, free-to-use, high-quality singing
corpus with realistic music scores, designed for all singing tasks, along with
its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality
singing voices, forming the largest recorded singing dataset; (2) 20
professional singers across nine widely spoken languages offer diverse timbres
and styles; (3) we provide controlled comparison and phoneme-level annotations
of six commonly used singing techniques, helping technique modeling and
control; (4) GTSinger offers realistic music scores, assisting real-world
musical composition; (5) singing voices are accompanied by manual
phoneme-to-audio alignments, global style labels, and 16.16 hours of paired
speech for various singing tasks. Moreover, to facilitate the use of GTSinger,
we conduct four benchmark experiments: technique-controllable singing voice
synthesis, technique recognition, style transfer, and speech-to-singing
conversion. The corpus and demos can be found at
http://aaronz345.github.io/GTSingerDemo/. We provide the dataset and the code
for processing data and conducting benchmarks at
https://huggingface.co/datasets/AaronZ345/GTSinger and
https://github.com/AaronZ345/GTSinger.",2024-09-20,"Yu Zhang, Changhao Pan, Wenxiang Guo, Ruiqi Li, Zhiyuan Zhu, Jialei Wang, Wenhao Xu, Jingyu Lu, Zhiqing Hong, Chuxin Wang, LiChao Zhang, Jinzheng He, Ziyue Jiang, Yuxin Chen, Chen Yang, Jiecheng Zhou, Xinyu Cheng, Zhou Zhao",http://arxiv.org/pdf/2409.13832v6,cs.CL
Measuring Copyright Risks of Large Language Model via Partial Information Probing,"Exploring the data sources used to train Large Language Models (LLMs) is a
crucial direction in investigating potential copyright infringement by these
models. While this approach can identify the possible use of copyrighted
materials in training data, it does not directly measure infringing risks.
Recent research has shifted towards testing whether LLMs can directly output
copyrighted content. Addressing this direction, we investigate and assess LLMs'
capacity to generate infringing content by providing them with partial
information from copyrighted materials, and try to use iterative prompting to
get LLMs to generate more infringing content. Specifically, we input a portion
of a copyrighted text into LLMs, prompt them to complete it, and then analyze
the overlap between the generated content and the original copyrighted
material. Our findings demonstrate that LLMs can indeed generate content highly
overlapping with copyrighted materials based on these partial inputs.",2024-09-20,"Weijie Zhao, Huajie Shao, Zhaozhuo Xu, Suzhen Duan, Denghui Zhang",http://arxiv.org/pdf/2409.13831v1,cs.CL
The Impact of Large Language Models in Academia: from Writing to Speaking,"Large language models (LLMs) are increasingly impacting human society,
particularly in textual information. Based on more than 30,000 papers and 1,000
presentations from machine learning conferences, we examined and compared the
words used in writing and speaking, representing the first large-scale study of
how LLMs influence the two main modes of verbal communication and expression
within the same group of people. Our empirical results show that LLM-style
words such as ""significant"" have been used more frequently in abstracts and
oral presentations. The impact on speaking is beginning to emerge and is likely
to grow in the future, calling attention to the implicit influence and ripple
effect of LLMs on human society.",2024-09-20,"Mingmeng Geng, Caixi Chen, Yanru Wu, Dongping Chen, Yao Wan, Pan Zhou",http://arxiv.org/pdf/2409.13686v2,cs.CL
ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation,"Navigating and understanding complex environments over extended periods of
time is a significant challenge for robots. People interacting with the robot
may want to ask questions like where something happened, when it occurred, or
how long ago it took place, which would require the robot to reason over a long
history of their deployment. To address this problem, we introduce a
Retrieval-augmented Memory for Embodied Robots, or ReMEmbR, a system designed
for long-horizon video question answering for robot navigation. To evaluate
ReMEmbR, we introduce the NaVQA dataset where we annotate spatial, temporal,
and descriptive questions to long-horizon robot navigation videos. ReMEmbR
employs a structured approach involving a memory building and a querying phase,
leveraging temporal information, spatial information, and images to efficiently
handle continuously growing robot histories. Our experiments demonstrate that
ReMEmbR outperforms LLM and VLM baselines, allowing ReMEmbR to achieve
effective long-horizon reasoning with low latency. Additionally, we deploy
ReMEmbR on a robot and show that our approach can handle diverse queries. The
dataset, code, videos, and other material can be found at the following link:
https://nvidia-ai-iot.github.io/remembr",2024-09-20,"Abrar Anwar, John Welsh, Joydeep Biswas, Soha Pouya, Yan Chang",http://arxiv.org/pdf/2409.13682v1,cs.CL
Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning,"Large Language Models (LLMs) have demonstrated impressive performance across
various tasks. However, current training approaches combine standard
cross-entropy loss with extensive data, human feedback, or ad hoc methods to
enhance performance. These solutions are often not scalable or feasible due to
their associated costs, complexity, or resource requirements. This study
investigates the use of established semantic segmentation loss functions in
natural language generation to create a versatile, practical, and scalable
solution for fine-tuning different architectures. We evaluate their
effectiveness in solving Math Word Problems and question answering across
different models of varying sizes. For the analyzed tasks, we found that the
traditional Cross-Entropy loss represents a sub-optimal choice, while models
trained to minimize alternative (task-dependent) losses, such as Focal or
Lov\'asz, achieve a mean improvement of +42% on exact match without requiring
additional data or human feedback. These findings suggest a promising pathway
for more efficient and accessible training processes.",2024-09-20,"Daniele Rege Cambrin, Giuseppe Gallipoli, Irene Benedetto, Luca Cagliero, Paolo Garza",http://arxiv.org/pdf/2409.13641v1,cs.CL
Advancing Event Causality Identification via Heuristic Semantic Dependency Inquiry Network,"Event Causality Identification (ECI) focuses on extracting causal relations
between events in texts. Existing methods for ECI primarily rely on causal
features and external knowledge. However, these approaches fall short in two
dimensions: (1) causal features between events in a text often lack explicit
clues, and (2) external knowledge may introduce bias, while specific problems
require tailored analyses. To address these issues, we propose SemDI - a simple
and effective Semantic Dependency Inquiry Network for ECI. SemDI captures
semantic dependencies within the context using a unified encoder. Then, it
utilizes a Cloze Analyzer to generate a fill-in token based on comprehensive
context understanding. Finally, this fill-in token is used to inquire about the
causal relation between two events. Extensive experiments demonstrate the
effectiveness of SemDI, surpassing state-of-the-art methods on three widely
used benchmarks. Code is available at https://github.com/hrlics/SemDI.",2024-09-20,"Haoran Li, Qiang Gao, Hongmei Wu, Li Huang",http://arxiv.org/pdf/2409.13621v2,cs.CL
MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension,"Referring Expression Comprehension (REC), which aims to ground a local visual
region via natural language, is a task that heavily relies on multimodal
alignment. Most existing methods utilize powerful pre-trained models to
transfer visual/linguistic knowledge by full fine-tuning. However, full
fine-tuning the entire backbone not only breaks the rich prior knowledge
embedded in the pre-training, but also incurs significant computational costs.
Motivated by the recent emergence of Parameter-Efficient Transfer Learning
(PETL) methods, we aim to solve the REC task in an effective and efficient
manner. Directly applying these PETL methods to the REC task is inappropriate,
as they lack the specific-domain abilities for precise local visual perception
and visual-language alignment. Therefore, we propose a novel framework of
Multimodal Prior-guided Parameter Efficient Tuning, namely MaPPER.
Specifically, MaPPER comprises Dynamic Prior Adapters guided by an aligned
prior, and Local Convolution Adapters to extract precise local semantics for
better visual perception. Moreover, the Prior-Guided Text module is proposed to
further utilize the prior for facilitating the cross-modal alignment.
Experimental results on three widely-used benchmarks demonstrate that MaPPER
achieves the best accuracy compared to the full fine-tuning and other PETL
methods with only 1.41% tunable backbone parameters. Our code is available at
https://github.com/liuting20/MaPPER.",2024-09-20,"Ting Liu, Zunnan Xu, Yue Hu, Liangtao Shi, Zhiqiang Wang, Quanjun Yin",http://arxiv.org/pdf/2409.13609v3,cs.CL
"Cross-Target Stance Detection: A Survey of Techniques, Datasets, and Challenges","Stance detection is the task of determining the viewpoint expressed in a text
towards a given target. A specific direction within the task focuses on
cross-target stance detection, where a model trained on samples pertaining to
certain targets is then applied to a new, unseen target. With the increasing
need to analyze and mining viewpoints and opinions online, the task has
recently seen a significant surge in interest. This review paper examines the
advancements in cross-target stance detection over the last decade,
highlighting the evolution from basic statistical methods to contemporary
neural and LLM-based models. These advancements have led to notable
improvements in accuracy and adaptability. Innovative approaches include the
use of topic-grouped attention and adversarial learning for zero-shot
detection, as well as fine-tuning techniques that enhance model robustness.
Additionally, prompt-tuning methods and the integration of external knowledge
have further refined model performance. A comprehensive overview of the
datasets used for evaluating these models is also provided, offering valuable
insights into the progress and challenges in the field. We conclude by
highlighting emerging directions of research and by suggesting avenues for
future work in the task.",2024-09-20,"Parisa Jamadi Khiabani, Arkaitz Zubiaga",http://arxiv.org/pdf/2409.13594v1,cs.CL
YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models,"Understanding satire and humor is a challenging task for even current
Vision-Language models. In this paper, we propose the challenging tasks of
Satirical Image Detection (detecting whether an image is satirical),
Understanding (generating the reason behind the image being satirical), and
Completion (given one half of the image, selecting the other half from 2 given
options, such that the complete image is satirical) and release a high-quality
dataset YesBut, consisting of 2547 images, 1084 satirical and 1463
non-satirical, containing different artistic styles, to evaluate those tasks.
Each satirical image in the dataset depicts a normal scenario, along with a
conflicting scenario which is funny or ironic. Despite the success of current
Vision-Language Models on multimodal tasks such as Visual QA and Image
Captioning, our benchmarking experiments show that such models perform poorly
on the proposed tasks on the YesBut Dataset in Zero-Shot Settings w.r.t both
automated as well as human evaluation. Additionally, we release a dataset of
119 real, satirical photographs for further research. The dataset and code are
available at https://github.com/abhi1nandy2/yesbut_dataset.",2024-09-20,"Abhilash Nandy, Yash Agarwal, Ashish Patwa, Millon Madhur Das, Aman Bansal, Ankit Raj, Pawan Goyal, Niloy Ganguly",http://arxiv.org/pdf/2409.13592v1,cs.CL
"Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for Filipino","Multilingual large language models (LLMs) today may not necessarily provide
culturally appropriate and relevant responses to its Filipino users. We
introduce Kalahi, a cultural LLM evaluation suite collaboratively created by
native Filipino speakers. It is composed of 150 high-quality, handcrafted and
nuanced prompts that test LLMs for generations that are relevant to shared
Filipino cultural knowledge and values. Strong LLM performance in Kalahi
indicates a model's ability to generate responses similar to what an average
Filipino would say or do in a given situation. We conducted experiments on LLMs
with multilingual and Filipino language support. Results show that Kalahi,
while trivial for Filipinos, is challenging for LLMs, with the best model
answering only 46.0% of the questions correctly compared to native Filipino
performance of 89.10%. Thus, Kalahi can be used to accurately and reliably
evaluate Filipino cultural representation in LLMs.",2024-09-20,"Jann Railey Montalan, Jian Gang Ngui, Wei Qi Leong, Yosephine Susanto, Hamsawardhini Rengarajan, Alham Fikri Aji, William Chandra Tjhi",http://arxiv.org/pdf/2409.15380v3,cs.CL
Demystifying and Extracting Fault-indicating Information from Logs for Failure Diagnosis,"Logs are imperative in the maintenance of online service systems, which often
encompass important information for effective failure mitigation. While
existing anomaly detection methodologies facilitate the identification of
anomalous logs within extensive runtime data, manual investigation of log
messages by engineers remains essential to comprehend faults, which is
labor-intensive and error-prone. Upon examining the log-based troubleshooting
practices at CloudA, we find that engineers typically prioritize two categories
of log information for diagnosis. These include fault-indicating descriptions,
which record abnormal system events, and fault-indicating parameters, which
specify the associated entities. Motivated by this finding, we propose an
approach to automatically extract such faultindicating information from logs
for fault diagnosis, named LoFI. LoFI comprises two key stages. In the first
stage, LoFI performs coarse-grained filtering to collect logs related to the
faults based on semantic similarity. In the second stage, LoFI leverages a
pre-trained language model with a novel prompt-based tuning method to extract
fine-grained information of interest from the collected logs. We evaluate LoFI
on logs collected from Apache Spark and an industrial dataset from CloudA. The
experimental results demonstrate that LoFI outperforms all baseline methods by
a significant margin, achieving an absolute improvement of 25.8~37.9 in F1 over
the best baseline method, ChatGPT. This highlights the effectiveness of LoFI in
recognizing fault-indicating information. Furthermore, the successful
deployment of LoFI at CloudA and user studies validate the utility of our
method. The code and data are available at
https://github.com/Jun-jie-Huang/LoFI.",2024-09-20,"Junjie Huang, Zhihan Jiang, Jinyang Liu, Yintong Huo, Jiazhen Gu, Zhuangbin Chen, Cong Feng, Hui Dong, Zengyin Yang, Michael R. Lyu",http://arxiv.org/pdf/2409.13561v1,cs.CL
Generating Visual Stories with Grounded and Coreferent Characters,"Characters are important in narratives. They move the plot forward, create
emotional connections, and embody the story's themes. Visual storytelling
methods focus more on the plot and events relating to it, without building the
narrative around specific characters. As a result, the generated stories feel
generic, with character mentions being absent, vague, or incorrect. To mitigate
these issues, we introduce the new task of character-centric story generation
and present the first model capable of predicting visual stories with
consistently grounded and coreferent character mentions. Our model is finetuned
on a new dataset which we build on top of the widely used VIST benchmark.
Specifically, we develop an automated pipeline to enrich VIST with visual and
textual character coreference chains. We also propose new evaluation metrics to
measure the richness of characters and coreference in stories. Experimental
results show that our model generates stories with recurring characters which
are consistent and coreferent to larger extent compared to baselines and
state-of-the-art systems.",2024-09-20,"Danyang Liu, Mirella Lapata, Frank Keller",http://arxiv.org/pdf/2409.13555v2,cs.CL
Contextualized Data-Wrangling Code Generation in Computational Notebooks,"Data wrangling, the process of preparing raw data for further analysis in
computational notebooks, is a crucial yet time-consuming step in data science.
Code generation has the potential to automate the data wrangling process to
reduce analysts' overhead by translating user intents into executable code.
Precisely generating data wrangling code necessitates a comprehensive
consideration of the rich context present in notebooks, including textual
context, code context and data context. However, notebooks often interleave
multiple non-linear analysis tasks into linear sequence of code blocks, where
the contextual dependencies are not clearly reflected. Directly training models
with source code blocks fails to fully exploit the contexts for accurate
wrangling code generation.
  To bridge the gap, we aim to construct a high quality datasets with clear and
rich contexts to help training models for data wrangling code generation tasks.
In this work, we first propose an automated approach, CoCoMine to mine
data-wrangling code generation examples with clear multi-modal contextual
dependency. It first adopts data flow analysis to identify the code blocks
containing data wrangling codes. Then, CoCoMine extracts the contextualized
datawrangling code examples through tracing and replaying notebooks. With
CoCoMine, we construct CoCoNote, a dataset containing 58,221 examples for
Contextualized Data-wrangling Code generation in Notebooks. To demonstrate the
effectiveness of our dataset, we finetune a range of pretrained code models and
prompt various large language models on our task. Furthermore, we also propose
DataCoder, which encodes data context and code&textual contexts separately to
enhance code generation. Experiment results demonstrate the significance of
incorporating data context in data-wrangling code generation and the
effectiveness of our model. We release code and data at url...",2024-09-20,"Junjie Huang, Daya Guo, Chenglong Wang, Jiazhen Gu, Shuai Lu, Jeevana Priya Inala, Cong Yan, Jianfeng Gao, Nan Duan, Michael R. Lyu",http://arxiv.org/pdf/2409.13551v1,cs.CL
ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources,"Recent developments in large language models (LLMs) have led to significant
improvements in intelligent dialogue systems'ability to handle complex
inquiries. However, current LLMs still exhibit limitations in specialized
domain knowledge, particularly in technical fields such as agriculture. To
address this problem, we propose ShizishanGPT, an intelligent question
answering system for agriculture based on the Retrieval Augmented Generation
(RAG) framework and agent architecture. ShizishanGPT consists of five key
modules: including a generic GPT-4 based module for answering general
questions; a search engine module that compensates for the problem that the
large language model's own knowledge cannot be updated in a timely manner; an
agricultural knowledge graph module for providing domain facts; a retrieval
module which uses RAG to supplement domain knowledge; and an agricultural agent
module, which invokes specialized models for crop phenotype prediction, gene
expression analysis, and so on. We evaluated the ShizishanGPT using a dataset
containing 100 agricultural questions specially designed for this study. The
experimental results show that the tool significantly outperforms general LLMs
as it provides more accurate and detailed answers due to its modular design and
integration of different domain knowledge sources. Our source code, dataset,
and model weights are publicly available at https://github.com/Zaiwen/CropGPT.",2024-09-20,"Shuting Yang, Zehui Liu, Wolfgang Mayer",http://arxiv.org/pdf/2409.13537v1,cs.CL
Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist with Tertiary Memory,"Mental health issues, particularly depressive disorders, present significant
challenges in contemporary society, necessitating the development of effective
automated diagnostic methods. This paper introduces the Agent Mental Clinic
(AMC), a self-improving conversational agent system designed to enhance
depression diagnosis through simulated dialogues between patient and
psychiatrist agents. To enhance the dialogue quality and diagnosis accuracy, we
design a psychiatrist agent consisting of a tertiary memory structure, a
dialogue control and reflect plugin that acts as ``supervisor'' and a memory
sampling module, fully leveraging the skills reflected by the psychiatrist
agent, achieving great accuracy on depression risk and suicide risk diagnosis
via conversation. Experiment results on datasets collected in real-life
scenarios demonstrate that the system, simulating the procedure of training
psychiatrists, can be a promising optimization method for aligning LLMs with
real-life distribution in specific domains without modifying the weights of
LLMs, even when only a few representative labeled cases are available.",2024-09-20,"Kunyao Lan, Bingrui Jin, Zichen Zhu, Siyuan Chen, Shu Zhang, Kenny Q. Zhu, Mengyue Wu",http://arxiv.org/pdf/2409.15084v2,cs.CL
EMMeTT: Efficient Multimodal Machine Translation Training,"A rising interest in the modality extension of foundation language models
warrants discussion on the most effective, and efficient, multimodal training
approach. This work focuses on neural machine translation (NMT) and proposes a
joint multimodal training regime of Speech-LLM to include automatic speech
translation (AST). We investigate two different foundation model architectures,
decoder-only GPT and encoder-decoder T5, extended with Canary-1B's speech
encoder. To handle joint multimodal training, we propose a novel training
framework called EMMeTT. EMMeTT improves training efficiency with the
following: balanced sampling across languages, datasets, and modalities;
efficient sequential data iteration; and a novel 2D bucketing scheme for
multimodal data, complemented by a batch size optimizer (OOMptimizer). We show
that a multimodal training consistently helps with both architectures.
Moreover, SALM-T5 trained with EMMeTT retains the original NMT capability while
outperforming AST baselines on four-language subsets of FLORES and FLEURS. The
resultant Multimodal Translation Model produces strong text and speech
translation results at the same time.",2024-09-20,"Piotr Żelasko, Zhehuai Chen, Mengru Wang, Daniel Galvez, Oleksii Hrinchuk, Shuoyang Ding, Ke Hu, Jagadeesh Balam, Vitaly Lavrukhin, Boris Ginsburg",http://arxiv.org/pdf/2409.13523v1,cs.CL
A Survey on Moral Foundation Theory and Pre-Trained Language Models: Current Advances and Challenges,"Moral values have deep roots in early civilizations, codified within norms
and laws that regulated societal order and the common good. They play a crucial
role in understanding the psychological basis of human behavior and cultural
orientation. The Moral Foundation Theory (MFT) is a well-established framework
that identifies the core moral foundations underlying the manner in which
different cultures shape individual and social lives. Recent advancements in
natural language processing, particularly Pre-trained Language Models (PLMs),
have enabled the extraction and analysis of moral dimensions from textual data.
This survey presents a comprehensive review of MFT-informed PLMs, providing an
analysis of moral tendencies in PLMs and their application in the context of
the MFT. We also review relevant datasets and lexicons and discuss trends,
limitations, and future directions. By providing a structured overview of the
intersection between PLMs and MFT, this work bridges moral psychology insights
within the realm of PLMs, paving the way for further research and development
in creating morally aware AI systems.",2024-09-20,"Lorenzo Zangari, Candida M. Greco, Davide Picca, Andrea Tagarelli",http://arxiv.org/pdf/2409.13521v2,cs.CL
LM-assisted keyword biasing with Aho-Corasick algorithm for Transducer-based ASR,"Despite the recent success of end-to-end models for automatic speech
recognition, recognizing special rare and out-of-vocabulary words, as well as
fast domain adaptation with text, are still challenging. It often happens that
biasing to the special entities leads to a degradation in the overall
performance. We propose a light on-the-fly method to improve automatic speech
recognition performance by combining a bias list of named entities with a
word-level n-gram language model with the shallow fusion approach based on the
Aho-Corasick string matching algorithm. The Aho-Corasick algorithm has proved
to be more efficient than other methods and allows fast context adaptation. An
n-gram language model is introduced as a graph with fail and output arcs, where
the arc weights are adapted from the n-gram probabilities. The language model
is used as an additional support to keyword biasing when the language model is
combined with bias entities in a single context graph to take care of the
overall performance. We demonstrate our findings on 4 languages, 2 public and 1
private datasets including performance on named entities and out-of-vocabulary
entities. We achieve up to 21.6% relative improvement in the general word error
rate with no practical difference in the inverse real-time factor.",2024-09-20,"Iuliia Thorbecke, Juan Zuluaga-Gomez, Esaú Villatoro-Tello, Andres Carofilis, Shashi Kumar, Petr Motlicek, Karthik Pandia, Aravind Ganapathiraju",http://arxiv.org/pdf/2409.13514v1,cs.CL
"Sketching With Your Voice: ""Non-Phonorealistic"" Rendering of Sounds via Vocal Imitation","We present a method for automatically producing human-like vocal imitations
of sounds: the equivalent of ""sketching,"" but for auditory rather than visual
representation. Starting with a simulated model of the human vocal tract, we
first try generating vocal imitations by tuning the model's control parameters
to make the synthesized vocalization match the target sound in terms of
perceptually-salient auditory features. Then, to better match human intuitions,
we apply a cognitive theory of communication to take into account how human
speakers reason strategically about their listeners. Finally, we show through
several experiments and user studies that when we add this type of
communicative reasoning to our method, it aligns with human intuitions better
than matching auditory features alone does. This observation has broad
implications for the study of depiction in computer graphics.",2024-09-20,"Matthew Caren, Kartik Chandra, Joshua B. Tenenbaum, Jonathan Ragan-Kelley, Karima Ma",http://arxiv.org/pdf/2409.13507v1,cs.CL
End-Cloud Collaboration Framework for Advanced AI Customer Service in E-commerce,"In recent years, the e-commerce industry has seen a rapid increase in the
demand for advanced AI-driven customer service solutions. Traditional
cloud-based models face limitations in terms of latency, personalized services,
and privacy concerns. Furthermore, end devices often lack the computational
resources to deploy large AI models effectively. In this paper, we propose an
innovative End-Cloud Collaboration (ECC) framework for advanced AI customer
service in e-commerce. This framework integrates the advantages of large cloud
models and mid/small-sized end models by deeply exploring the generalization
potential of cloud models and effectively utilizing the computing power
resources of terminal chips, alleviating the strain on computing resources to
some extent. Specifically, the large cloud model acts as a teacher, guiding and
promoting the learning of the end model, which significantly reduces the end
model's reliance on large-scale, high-quality data and thereby addresses the
data bottleneck in traditional end model training, offering a new paradigm for
the rapid deployment of industry applications. Additionally, we introduce an
online evolutive learning strategy that enables the end model to continuously
iterate and upgrade based on guidance from the cloud model and real-time user
feedback. This strategy ensures that the model can flexibly adapt to the rapid
changes in application scenarios while avoiding the uploading of sensitive
information by performing local fine-tuning, achieving the dual goals of
privacy protection and personalized service. %We make systematic contributions
to the customized model fine-tuning methods in the e-commerce domain. To
conclude, we implement in-depth corpus collection (e.g., data organization,
cleaning, and preprocessing) and train an ECC-based industry-specific model for
e-commerce customer service.",2024-09-20,"Liangyu Teng, Yang Liu, Jing Liu, Liang Song",http://arxiv.org/pdf/2410.07122v1,cs.CL
HUT: A More Computation Efficient Fine-Tuning Method With Hadamard Updated Transformation,"Fine-tuning pre-trained language models for downstream tasks has achieved
impressive results in NLP. However, fine-tuning all parameters becomes
impractical due to the rapidly increasing size of model parameters. To address
this, Parameter Efficient Fine-Tuning (PEFT) methods update only a subset of
parameters. Most PEFT methods, such as LoRA, use incremental updates, which
involve adding learned weight matrix increments to the original parameters.
Although effective, these methods face limitations in capturing complex
parameter dynamics and do not maintain a strong correlation between the
original and updated parameters. To overcome these challenges, we propose the
direct Updated Transformation (UT) paradigm, which constructs a transformation
directly from the original to the updated parameters. This approach ensures
that the correlation between the original and updated parameters is preserved,
leveraging the semantic features learned during pre-training. Building on this
paradigm, we present the Hadamard Updated Transformation (HUT) method. HUT
efficiently updates the original weight matrix using the Hadamard
transformation with two low-rank matrices, offering a more expressive and
flexible update mechanism. This allows HUT to capture richer parameter features
through functional transformations, reducing computational complexity while
maintaining or improving model quality. Theoretical analysis and extensive
experiments on RoBERTa and GPT-2 validate the effectiveness of HUT. Results
show that HUT performs on par with or better than other PEFT methods in terms
of model quality, while significantly reducing computational complexity.",2024-09-20,"Geyuan Zhang, Xiaofei Zhou, Chuheng Chen",http://arxiv.org/pdf/2409.13501v1,cs.CL
Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper,"The training of automatic speech recognition (ASR) with little to no
supervised data remains an open question. In this work, we demonstrate that
streaming Transformer-Transducer (TT) models can be trained from scratch in
consumer and accessible GPUs in their entirety with pseudo-labeled (PL) speech
from foundational speech models (FSM). This allows training a robust ASR model
just in one stage and does not require large data and computational budget
compared to the two-step scenario with pre-training and fine-tuning. We perform
a comprehensive ablation on different aspects of PL-based streaming TT models
such as the impact of (1) shallow fusion of n-gram LMs, (2) contextual biasing
with named entities, (3) chunk-wise decoding for low-latency streaming
applications, and (4) TT overall performance as the function of the FSM size.
Our results demonstrate that TT can be trained from scratch without supervised
data, even with very noisy PLs. We validate the proposed framework on 6
languages from CommonVoice and propose multiple heuristics to filter out
hallucinated PLs.",2024-09-20,"Iuliia Thorbecke, Juan Zuluaga-Gomez, Esaú Villatoro-Tello, Shashi Kumar, Pradeep Rangappa, Sergio Burdisso, Petr Motlicek, Karthik Pandia, Aravind Ganapathiraju",http://arxiv.org/pdf/2409.13499v2,cs.CL
Constrained Reasoning Chains for Enhancing Theory-of-Mind in Large Language Models,"Theory-of-Mind (ToM) ability possessed by Large Language Models (LLMs) has
been shown to be limited. Most existing methods for improving ToM in LLMs adopt
zero-shot prompting, and they face challenges including poor performance in
complex ToM reasoning tasks and an inability to handle non-narrative contexts.
We propose a zero-shot prompting method named Constrained Chain-of-ToM (CCoToM)
that leverages domain knowledge and the causal relations between ToM dimensions
to address these limitations. Specifically, CCoToM guides LLMs to construct
explicit reasoning chains by first prompting LLMs to infer related ToM
dimensions (e.g., belief). Afterward, CCoToM prompts LLMs to infer the queried
ToM dimension based on the generated related ToM dimensions and corresponding
causal relations. Additionally, CCoToM adaptively imposes constraints on
prompts to introduce inductive biases and improve consistency between ToM
dimensions. Besides narratives, CCoToM can also handle non-narrative contexts
like conversations. Extensive experiments show that CCoToM consistently
outperforms previous state-of-the-art methods by large margins across all LLMs
and datasets used. We also conduct in-depth analyses to gain deeper insights
into CCoToM. We have made our code publicly available.",2024-09-20,"Zizheng Lin, Chunkit Chan, Yangqiu Song, Xin Liu",http://arxiv.org/pdf/2409.13490v1,cs.CL
'Since Lawyers are Males..': Examining Implicit Gender Bias in Hindi Language Generation by LLMs,"Large Language Models (LLMs) are increasingly being used to generate text
across various languages, for tasks such as translation, customer support, and
education. Despite these advancements, LLMs show notable gender biases in
English, which become even more pronounced when generating content in
relatively underrepresented languages like Hindi. This study explores implicit
gender biases in Hindi text generation and compares them to those in English.
We developed Hindi datasets inspired by WinoBias to examine stereotypical
patterns in responses from models like GPT-4o and Claude-3 sonnet. Our results
reveal a significant gender bias of 87.8% in Hindi, compared to 33.4% in
English GPT-4o generation, with Hindi responses frequently relying on gender
stereotypes related to occupations, power hierarchies, and social class. This
research underscores the variation in gender biases across languages and
provides considerations for navigating these biases in generative AI systems.",2024-09-20,"Ishika Joshi, Ishita Gupta, Adrita Dey, Tapan Parikh",http://arxiv.org/pdf/2409.13484v1,cs.CL
A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering,"Speech-based open-domain question answering (QA over a large corpus of text
passages with spoken questions) has emerged as an important task due to the
increasing number of users interacting with QA systems via speech interfaces.
Passage retrieval is a key task in speech-based open-domain QA. So far,
previous works adopted pipelines consisting of an automatic speech recognition
(ASR) model that transcribes the spoken question before feeding it to a dense
text retriever. Such pipelines have several limitations. The need for an ASR
model limits the applicability to low-resource languages and specialized
domains with no annotated speech data. Furthermore, the ASR model propagates
its errors to the retriever. In this work, we try to alleviate these
limitations by proposing an ASR-free, end-to-end trained multimodal dense
retriever that can work directly on spoken questions. Our experimental results
showed that, on shorter questions, our retriever is a promising alternative to
the \textit{ASR and Retriever} pipeline, achieving better retrieval performance
in cases where ASR would have mistranscribed important words in the question or
have produced a transcription with a high word error rate.",2024-09-20,"Georgios Sidiropoulos, Evangelos Kanoulas",http://arxiv.org/pdf/2409.13483v1,cs.CL
Toward Automated Clinical Transcriptions,"Administrative documentation is a major driver of rising healthcare costs and
is linked to adverse outcomes, including physician burnout and diminished
quality of care. This paper introduces a secure system that applies recent
advancements in speech-to-text transcription and speaker-labeling (diarization)
to patient-provider conversations. This system is optimized to produce accurate
transcriptions and highlight potential errors to promote rapid human
verification, further reducing the necessary manual effort. Applied to over 40
hours of simulated conversations, this system offers a promising foundation for
automating clinical transcriptions.",2024-09-20,"Mitchell A. Klusty, W. Vaiden Logan, Samuel E. Armstrong, Aaron D. Mullen, Caroline N. Leach, Jeff Talbert, V. K. Cody Bumgardner",http://arxiv.org/pdf/2409.15378v1,cs.CL
Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models,"Machine unlearning aims to efficiently eliminate the influence of specific
training data, known as the forget set, from the model. However, existing
unlearning methods for Large Language Models (LLMs) face a critical challenge:
they rely solely on negative feedback to suppress responses related to the
forget set, which often results in nonsensical or inconsistent outputs,
diminishing model utility and posing potential privacy risks. To address this
limitation, we propose a novel approach called Alternate Preference
Optimization (AltPO), which combines negative feedback with in-domain positive
feedback on the forget set. Additionally, we introduce new evaluation metrics
to assess the quality of responses related to the forget set. Extensive
experiments show that our approach not only enables effective unlearning but
also avoids undesirable model behaviors while maintaining overall model
performance. Our implementation can be found at
https://github.com/molereddy/Alternate-Preference-Optimization.",2024-09-20,"Anmol Mekala, Vineeth Dorna, Shreya Dubey, Abhishek Lalwani, David Koleczek, Mukund Rungta, Sadid Hasan, Elita Lobo",http://arxiv.org/pdf/2409.13474v3,cs.CL
Minstrel: Structural Prompt Generation with Multi-Agents Coordination for Non-AI Experts,"LLMs have demonstrated commendable performance across diverse domains.
Nevertheless, formulating high-quality prompts to assist them in their work
poses a challenge for non-AI experts. Existing research in prompt engineering
suggests somewhat scattered optimization principles and designs empirically
dependent prompt optimizers. Unfortunately, these endeavors lack a structural
design, incurring high learning costs and it is not conducive to the iterative
updating of prompts, especially for non-AI experts. Inspired by structured
reusable programming languages, we propose LangGPT, a structural prompt design
framework. Furthermore, we introduce Minstrel, a multi-generative agent system
with reflection to automate the generation of structural prompts. Experiments
and the case study illustrate that structural prompts generated by Minstrel or
written manually significantly enhance the performance of LLMs. Furthermore, we
analyze the ease of use of structural prompts through a user survey in our
online community.",2024-09-20,"Ming Wang, Yuanzhong Liu, Xiaoyu Liang, Yijie Huang, Daling Wang, Xiaocui Yang, Sijia Shen, Shi Feng, Xiaoming Zhang, Chaofeng Guan, Yifei Zhang",http://arxiv.org/pdf/2409.13449v1,cs.CL
AQA: Adaptive Question Answering in a Society of LLMs via Contextual Multi-Armed Bandit,"In question answering (QA), different questions can be effectively addressed
with different answering strategies. Some require a simple lookup, while others
need complex, multi-step reasoning to be answered adequately. This observation
motivates the development of a dynamic method that adaptively selects the most
suitable QA strategy for each question, enabling more efficient and effective
systems capable of addressing a broader range of question types. To this aim,
we build on recent advances in the orchestration of multiple large language
models (LLMs) and formulate adaptive QA as a dynamic orchestration challenge.
We define this as a contextual multi-armed bandit problem, where the context is
defined by the characteristics of the incoming question and the action space
consists of potential communication graph configurations among the LLM agents.
We then train a linear upper confidence bound model to learn an optimal mapping
between different question types and their corresponding optimal multi-LLM
communication graph representation. Our experiments show that the proposed
solution is viable for adaptive orchestration of a QA system with multiple
modules, as it combines the superior performance of more complex strategies
while avoiding their costs when simpler strategies suffice.",2024-09-20,"Mohanna Hoveyda, Arjen P. de Vries, Maarten de Rijke, Harrie Oosterhuis, Faegheh Hasibi",http://arxiv.org/pdf/2409.13447v2,cs.CL
Selective Exploration and Information Gathering in Search and Rescue Using Hierarchical Learning Guided by Natural Language Input,"In recent years, robots and autonomous systems have become increasingly
integral to our daily lives, offering solutions to complex problems across
various domains. Their application in search and rescue (SAR) operations,
however, presents unique challenges. Comprehensively exploring the
disaster-stricken area is often infeasible due to the vastness of the terrain,
transformed environment, and the time constraints involved. Traditional robotic
systems typically operate on predefined search patterns and lack the ability to
incorporate and exploit ground truths provided by human stakeholders, which can
be the key to speeding up the learning process and enhancing triage. Addressing
this gap, we introduce a system that integrates social interaction via large
language models (LLMs) with a hierarchical reinforcement learning (HRL)
framework. The proposed system is designed to translate verbal inputs from
human stakeholders into actionable RL insights and adjust its search strategy.
By leveraging human-provided information through LLMs and structuring task
execution through HRL, our approach not only bridges the gap between autonomous
capabilities and human intelligence but also significantly improves the agent's
learning efficiency and decision-making process in environments characterised
by long horizons and sparse rewards.",2024-09-20,"Dimitrios Panagopoulos, Adolfo Perrusquia, Weisi Guo",http://arxiv.org/pdf/2409.13445v1,cs.CL
AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity,"Recently, when dealing with high-resolution images, dominant LMMs usually
divide them into multiple local images and one global image, which will lead to
a large number of visual tokens. In this work, we introduce AVG-LLaVA, an LMM
that can adaptively select the appropriate visual granularity based on the
input image and instruction. This approach not only reduces the number of
visual tokens and speeds up inference, but also improves the overall model
performance. Specifically, we introduce the following modules based on
LLaVA-NeXT: (a) a visual granularity scaler that includes multiple pooling
layers to obtain visual tokens with different granularities; (b) a visual
granularity router, which includes a Transformer layer, an MLP layer, and a
voter layer, used to select the appropriate visual granularity based on the
image and instruction. Furthermore, we propose RGLF, a novel training paradigm
that aims at aligning the granularity predicted by the router with the
preferences of the LMM, without the need for additional manually annotated
data. Extensive experiments and analysis show that AVG-LLaVA achieves superior
performance across 11 benchmarks, as well as significantly reduces the number
of visual tokens and speeds up inference (e.g., an 85.3% reduction in visual
tokens and a 2.53$\times$ increase in inference speed on the AI2D benchmark).",2024-09-20,"Zhibin Lan, Liqiang Niu, Fandong Meng, Wenbo Li, Jie Zhou, Jinsong Su",http://arxiv.org/pdf/2410.02745v2,cs.CL
Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey,"Large Language Models (LLMs) showcase remarkable abilities, yet they struggle
with limitations such as hallucinations, outdated knowledge, opacity, and
inexplicable reasoning. To address these challenges, Retrieval-Augmented
Generation (RAG) has proven to be a viable solution, leveraging external
databases to improve the consistency and coherence of generated content,
especially valuable for complex, knowledge-rich tasks, and facilitates
continuous improvement by leveraging domain-specific insights. By combining the
intrinsic knowledge of LLMs with the vast, dynamic repositories of external
databases, RAG achieves a synergistic effect. However, RAG is not without its
limitations, including a limited context window, irrelevant information, and
the high processing overhead for extensive contextual data. In this
comprehensive work, we explore the evolution of Contextual Compression
paradigms, providing an in-depth examination of the field. Finally, we outline
the current challenges and suggest potential research and development
directions, paving the way for future advancements in this area.",2024-09-20,Sourav Verma,http://arxiv.org/pdf/2409.13385v2,cs.CL
LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench,"The ability to plan a course of action that achieves a desired state of
affairs has long been considered a core competence of intelligent agents and
has been an integral part of AI research since its inception. With the advent
of large language models (LLMs), there has been considerable interest in the
question of whether or not they possess such planning abilities. PlanBench, an
extensible benchmark we developed in 2022, soon after the release of GPT3, has
remained an important tool for evaluating the planning abilities of LLMs.
Despite the slew of new private and open source LLMs since GPT3, progress on
this benchmark has been surprisingly slow. OpenAI claims that their recent o1
(Strawberry) model has been specifically constructed and trained to escape the
normal limitations of autoregressive LLMs--making it a new kind of model: a
Large Reasoning Model (LRM). Using this development as a catalyst, this paper
takes a comprehensive look at how well current LLMs and new LRMs do on
PlanBench. As we shall see, while o1's performance is a quantum improvement on
the benchmark, outpacing the competition, it is still far from saturating it.
This improvement also brings to the fore questions about accuracy, efficiency,
and guarantees which must be considered before deploying such systems.",2024-09-20,"Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati",http://arxiv.org/pdf/2409.13373v1,cs.CL
EmotionQueen: A Benchmark for Evaluating Empathy of Large Language Models,"Emotional intelligence in large language models (LLMs) is of great importance
in Natural Language Processing. However, the previous research mainly focus on
basic sentiment analysis tasks, such as emotion recognition, which is not
enough to evaluate LLMs' overall emotional intelligence. Therefore, this paper
presents a novel framework named EmotionQueen for evaluating the emotional
intelligence of LLMs. The framework includes four distinctive tasks: Key Event
Recognition, Mixed Event Recognition, Implicit Emotional Recognition, and
Intention Recognition. LLMs are requested to recognize important event or
implicit emotions and generate empathetic response. We also design two metrics
to evaluate LLMs' capabilities in recognition and response for emotion-related
statements. Experiments yield significant conclusions about LLMs' capabilities
and limitations in emotion intelligence.",2024-09-20,"Yuyan Chen, Hao Wang, Songzhou Yan, Sijia Liu, Yueze Li, Yi Zhao, Yanghua Xiao",http://arxiv.org/pdf/2409.13359v1,cs.CL
Recent Advancement of Emotion Cognition in Large Language Models,"Emotion cognition in large language models (LLMs) is crucial for enhancing
performance across various applications, such as social media, human-computer
interaction, and mental health assessment. We explore the current landscape of
research, which primarily revolves around emotion classification, emotionally
rich response generation, and Theory of Mind assessments, while acknowledge the
challenges like dependency on annotated data and complexity in emotion
processing. In this paper, we present a detailed survey of recent progress in
LLMs for emotion cognition. We explore key research studies, methodologies,
outcomes, and resources, aligning them with Ulric Neisser's cognitive stages.
Additionally, we outline potential future directions for research in this
evolving field, including unsupervised learning approaches and the development
of more complex and interpretable emotion cognition LLMs. We also discuss
advanced methods such as contrastive learning used to improve LLMs' emotion
cognition capabilities.",2024-09-20,"Yuyan Chen, Yanghua Xiao",http://arxiv.org/pdf/2409.13354v1,cs.CL
Time Awareness in Large Language Models: Benchmarking Fact Recall Across Time,"Who is the US President? The answer changes depending on when the question is
asked. While large language models (LLMs) are evaluated on various reasoning
tasks, they often miss a crucial dimension: time. In real-world scenarios, the
correctness of answers is frequently tied to temporal context. To address this
gap, we present a novel framework and dataset spanning over 8,000 events from
2018 to 2024, annotated with day-level granularity and sourced globally across
domains such as politics, science, and business. Our TimeShift evaluation
method systematically probes LLMs for temporal reasoning, revealing that base
models often outperform instruction-tuned and synthetic-trained counterparts on
time-sensitive recall. Additionally, we find that even large-scale models
exhibit brittleness in handling paraphrased facts, highlighting unresolved
challenges in temporal consistency. By identifying these limitations, our work
provides a significant step toward advancing time-aware language models capable
of adapting to the dynamic nature of real-world knowledge.",2024-09-20,"David Herel, Vojtech Bartek, Jiri Jirak, Tomas Mikolov",http://arxiv.org/pdf/2409.13338v3,cs.CL
Beyond the binary: Limitations and possibilities of gender-related speech technology research,"This paper presents a review of 107 research papers relating to speech and
sex or gender in ISCA Interspeech publications between 2013 and 2023. We note
the scarcity of work on this topic and find that terminology, particularly the
word gender, is used in ways that are underspecified and often out of step with
the prevailing view in social sciences that gender is socially constructed and
is a spectrum as opposed to a binary category. We draw attention to the
potential problems that this can cause for already marginalised groups, and
suggest some questions for researchers to ask themselves when undertaking work
on speech and gender.",2024-09-20,"Ariadna Sanchez, Alice Ross, Nina Markl",http://arxiv.org/pdf/2409.13335v2,cs.CL
Applying Pre-trained Multilingual BERT in Embeddings for Improved Malicious Prompt Injection Attacks Detection,"Large language models (LLMs) are renowned for their exceptional capabilities,
and applying to a wide range of applications. However, this widespread use
brings significant vulnerabilities. Also, it is well observed that there are
huge gap which lies in the need for effective detection and mitigation
strategies against malicious prompt injection attacks in large language models,
as current approaches may not adequately address the complexity and evolving
nature of these vulnerabilities in real-world applications. Therefore, this
work focuses the impact of malicious prompt injection attacks which is one of
most dangerous vulnerability on real LLMs applications. It examines to apply
various BERT (Bidirectional Encoder Representations from Transformers) like
multilingual BERT, DistilBert for classifying malicious prompts from legitimate
prompts. Also, we observed how tokenizing the prompt texts and generating
embeddings using multilingual BERT contributes to improve the performance of
various machine learning methods: Gaussian Naive Bayes, Random Forest, Support
Vector Machine, and Logistic Regression. The performance of each model is
rigorously analyzed with various parameters to improve the binary
classification to discover malicious prompts. Multilingual BERT approach to
embed the prompts significantly improved and outperformed the existing works
and achieves an outstanding accuracy of 96.55% by Logistic regression.
Additionally, we investigated the incorrect predictions of the model to gain
insights into its limitations. The findings can guide researchers in tuning
various BERT for finding the most suitable model for diverse LLMs
vulnerabilities.",2024-09-20,"Md Abdur Rahman, Hossain Shahriar, Fan Wu, Alfredo Cuzzocrea",http://arxiv.org/pdf/2409.13331v1,cs.CL
SLaVA-CXR: Small Language and Vision Assistant for Chest X-ray Report Automation,"Inspired by the success of large language models (LLMs), there is growing
research interest in developing LLMs in the medical domain to assist
clinicians. However, for hospitals, using closed-source commercial LLMs
involves privacy issues, and developing open-source public LLMs requires
large-scale computational resources, which are usually limited, especially in
resource-efficient regions and low-income countries. We propose an open-source
Small Language and Vision Assistant (SLaVA-CXR) that can be used for Chest
X-Ray report automation. To efficiently train a small assistant, we first
propose the Re$^3$Training method, which simulates the cognitive development of
radiologists and optimizes the model in the Recognition, Reasoning, and
Reporting training manner. Then, we introduce a data synthesis method, RADEX,
which can generate a high-quality and diverse training corpus with privacy
regulation compliance. The extensive experiments show that our SLaVA-CXR built
on a 2.7B backbone not only outperforms but also achieves 6 times faster
inference efficiency than previous state-of-the-art larger models.",2024-09-20,"Jinge Wu, Yunsoo Kim, Daqian Shi, David Cliffton, Fenglin Liu, Honghan Wu",http://arxiv.org/pdf/2409.13321v1,cs.CL
CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Causal Significance and Consistency,"Chain-based reasoning methods like chain of thought (CoT) play a rising role
in solving reasoning tasks for large language models (LLMs). However, the
causal hallucinations between a step of reasoning and corresponding state
transitions are becoming a significant obstacle to advancing LLMs' reasoning
capabilities, especially in long-range reasoning tasks. This paper proposes a
non-chain-based reasoning framework for simultaneous consideration of causal
significance and consistency, i.e., the Causal Significance and Consistency
Enhancer (CSCE). We customize LLM's loss function utilizing treatment effect
assessments to enhance its reasoning ability from two aspects: causal
significance and consistency. This ensures that the model captures essential
causal relationships and maintains robust and consistent performance across
various scenarios. Additionally, we transform the reasoning process from the
cascading multiple one-step reasoning commonly used in Chain-Based methods,
like CoT, to a causal-enhanced method that outputs the entire reasoning process
in one go, further improving the model's reasoning efficiency. Extensive
experiments show that our method improves both the reasoning success rate and
speed. These improvements further demonstrate that non-chain-based methods can
also aid LLMs in completing reasoning tasks.",2024-09-20,"Kangsheng Wang, Xiao Zhang, Juntao Lyu, Tianyu Hu, Huimin Ma",http://arxiv.org/pdf/2409.17174v3,cs.CL
JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models,"Recent developments in Japanese large language models (LLMs) primarily focus
on general domains, with fewer advancements in Japanese biomedical LLMs. One
obstacle is the absence of a comprehensive, large-scale benchmark for
comparison. Furthermore, the resources for evaluating Japanese biomedical LLMs
are insufficient. To advance this field, we propose a new benchmark including
eight LLMs across four categories and 20 Japanese biomedical datasets across
five tasks. Experimental results indicate that: (1) LLMs with a better
understanding of Japanese and richer biomedical knowledge achieve better
performance in Japanese biomedical tasks, (2) LLMs that are not mainly designed
for Japanese biomedical domains can still perform unexpectedly well, and (3)
there is still much room for improving the existing LLMs in certain Japanese
biomedical tasks. Moreover, we offer insights that could further enhance
development in this field. Our evaluation tools tailored to our benchmark as
well as the datasets are publicly available in
https://huggingface.co/datasets/Coldog2333/JMedBench to facilitate future
research.",2024-09-20,"Junfeng Jiang, Jiahao Huang, Akiko Aizawa",http://arxiv.org/pdf/2409.13317v1,cs.CL
GAProtoNet: A Multi-head Graph Attention-based Prototypical Network for Interpretable Text Classification,"Pretrained transformer-based Language Models (LMs) are well-known for their
ability to achieve significant improvement on text classification tasks with
their powerful word embeddings, but their black-box nature, which leads to a
lack of interpretability, has been a major concern. In this work, we introduce
GAProtoNet, a novel white-box Multi-head Graph Attention-based Prototypical
Network designed to explain the decisions of text classification models built
with LM encoders. In our approach, the input vector and prototypes are regarded
as nodes within a graph, and we utilize multi-head graph attention to
selectively construct edges between the input node and prototype nodes to learn
an interpretable prototypical representation. During inference, the model makes
decisions based on a linear combination of activated prototypes weighted by the
attention score assigned for each prototype, allowing its choices to be
transparently explained by the attention weights and the prototypes projected
into the closest matching training examples. Experiments on multiple public
datasets show our approach achieves superior results without sacrificing the
accuracy of the original black-box LMs. We also compare with four alternative
prototypical network variations and our approach achieves the best accuracy and
F1 among all. Our case study and visualization of prototype clusters also
demonstrate the efficiency in explaining the decisions of black-box models
built with LMs.",2024-09-20,"Ximing Wen, Wenjuan Tan, Rosina O. Weber",http://arxiv.org/pdf/2409.13312v2,cs.CL
Learning to Generalize Unseen Domains via Multi-Source Meta Learning for Text Classification,"With the rapid development of deep learning methods, there have been many
breakthroughs in the field of text classification. Models developed for this
task have been shown to achieve high accuracy. However, most of these models
are trained using labeled data from seen domains. It is difficult for these
models to maintain high accuracy in a new challenging unseen domain, which is
directly related to the generalization of the model. In this paper, we study
the multi-source Domain Generalization of text classification and propose a
framework to use multiple seen domains to train a model that can achieve high
accuracy in an unseen domain. Specifically, we propose a multi-source
meta-learning Domain Generalization framework to simulate the process of model
generalization to an unseen domain, so as to extract sufficient domain-related
features. We introduced a memory mechanism to store domain-specific features,
which coordinate with the meta-learning framework. Besides, we adopt the novel
""jury"" mechanism that enables the model to learn sufficient domain-invariant
features. Experiments demonstrate that our meta-learning framework can
effectively enhance the ability of the model to generalize to an unseen domain
and can outperform the state-of-the-art methods on multi-source text
classification datasets.",2024-09-20,"Yuxuan Hu, Chenwei Zhang, Min Yang, Xiaodan Liang, Chengming Li, Xiping Hu",http://arxiv.org/pdf/2409.13787v1,cs.CL
Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts,"Adapting keyphrase generation models to new domains typically involves
few-shot fine-tuning with in-domain labeled data. However, annotating documents
with keyphrases is often prohibitively expensive and impractical, requiring
expert annotators. This paper presents silk, an unsupervised method designed to
address this issue by extracting silver-standard keyphrases from citation
contexts to create synthetic labeled data for domain adaptation. Extensive
experiments across three distinct domains demonstrate that our method yields
high-quality synthetic samples, resulting in significant and consistent
improvements in in-domain performance over strong baselines.",2024-09-20,"Florian Boudin, Akiko Aizawa",http://arxiv.org/pdf/2409.13266v2,cs.CL
Towards LifeSpan Cognitive Systems,"Building a human-like system that continuously interacts with complex
environments -- whether simulated digital worlds or human society -- presents
several key challenges. Central to this is enabling continuous, high-frequency
interactions, where the interactions are termed experiences. We refer to this
envisioned system as the LifeSpan Cognitive System (LSCS). A critical feature
of LSCS is its ability to engage in incremental and rapid updates while
retaining and accurately recalling past experiences. In this paper we focus on
the domain of Large Language Models (LLMs), where we identify two major
challenges: (1) Abstraction and Experience Merging, and (2) Long-term Retention
with Accurate Recall. These properties are essential for storing new
experiences, organizing past experiences, and responding to the environment in
ways that leverage relevant historical data. Unlike language models with
continual learning, which typically rely on large corpora for fine-tuning and
focus on improving performance within specific domains or tasks, LSCS must
rapidly and incrementally update with new information from its environment at a
high frequency. Existing technologies with the potential of solving the above
two major challenges can be classified into four classes based on a conceptual
metric called Storage Complexity, which measures the relative space required to
store past experiences. Each of these four classes of technologies has its own
strengths and limitations while we argue none of them alone can achieve LSCS
alone. To this end, we propose a potential instantiation for LSCS that can
integrate all four classes of technologies. The new instantiation, serving as a
conjecture, operates through two core processes: Absorbing Experiences and
Generating Responses.",2024-09-20,"Yu Wang, Chi Han, Tongtong Wu, Xiaoxin He, Wangchunshu Zhou, Nafis Sadeq, Xiusi Chen, Zexue He, Wei Wang, Gholamreza Haffari, Heng Ji, Julian McAuley",http://arxiv.org/pdf/2409.13265v2,cs.CL
Large Language Model Should Understand Pinyin for Chinese ASR Error Correction,"Large language models can enhance automatic speech recognition systems
through generative error correction. In this paper, we propose Pinyin-enhanced
GEC, which leverages Pinyi, the phonetic representation of Mandarin Chinese, as
supplementary information to improve Chinese ASR error correction. Our approach
only utilizes synthetic errors for training and employs the one-best hypothesis
during inference. Additionally, we introduce a multitask training approach
involving conversion tasks between Pinyin and text to align their feature
spaces. Experiments on the Aishell-1 and the Common Voice datasets demonstrate
that our approach consistently outperforms GEC with text-only input. More
importantly, we provide intuitive explanations for the effectiveness of PY-GEC
and multitask training from two aspects: 1) increased attention weight on
Pinyin features; and 2) aligned feature space between Pinyin and text hidden
states.",2024-09-20,"Yuang Li, Xiaosong Qiao, Xiaofeng Zhao, Huan Zhao, Wei Tang, Min Zhang, Hao Yang",http://arxiv.org/pdf/2409.13262v1,cs.CL
Prompting Large Language Models for Supporting the Differential Diagnosis of Anemia,"In practice, clinicians achieve a diagnosis by following a sequence of steps,
such as laboratory exams, observations, or imaging. The pathways to reach
diagnosis decisions are documented by guidelines authored by expert
organizations, which guide clinicians to reach a correct diagnosis through
these sequences of steps. While these guidelines are beneficial for following
medical reasoning and consolidating medical knowledge, they have some
drawbacks. They often fail to address patients with uncommon conditions due to
their focus on the majority population, and are slow and costly to update,
making them unsuitable for rapidly emerging diseases or new practices. Inspired
by clinical guidelines, our study aimed to develop pathways similar to those
that can be obtained in clinical guidelines. We tested three Large Language
Models (LLMs) -Generative Pretrained Transformer 4 (GPT-4), Large Language
Model Meta AI (LLaMA), and Mistral -on a synthetic yet realistic dataset to
differentially diagnose anemia and its subtypes. By using advanced prompting
techniques to enhance the decision-making process, we generated diagnostic
pathways using these models. Experimental results indicate that LLMs hold huge
potential in clinical pathway discovery from patient data, with GPT-4
exhibiting the best performance in all conducted experiments.",2024-09-20,"Elisa Castagnari, Lillian Muyama, Adrien Coulet",http://arxiv.org/pdf/2409.15377v1,cs.CL
Optimizing RLHF Training for Large Language Models with Stage Fusion,"We present RLHFuse, an efficient training system with stage fusion for
Reinforcement Learning from Human Feedback (RLHF). Due to the intrinsic nature
of RLHF training, i.e., the data skewness in the generation stage and the
pipeline bubbles in the training stage, existing RLHF systems suffer from low
GPU utilization. RLHFuse breaks the traditional view of RLHF workflow as a
composition of individual tasks, splitting each task into finer-grained
subtasks, and performing stage fusion to improve GPU utilization. RLHFuse
contains two key ideas. First, for generation and inference tasks, RLHFuse
splits them into sample-level subtasks, enabling efficient inter-stage fusion
to overlap the execution of generation and inference stages, thus mitigating
the original generation bottleneck dominated by long-tailed samples. Second,
for training tasks, RLHFuse breaks them into subtasks of micro-batches and
performs intra-stage fusion to concurrently execute these subtasks in the
training stage with a fused pipeline schedule, effectively mitigating the
pipeline bubbles. The experiments show that RLHFuse increases the training
throughput by up to $3.7\times$, compared to existing systems.",2024-09-20,"Yinmin Zhong, Zili Zhang, Bingyang Wu, Shengyu Liu, Yukun Chen, Changyi Wan, Hanpeng Hu, Lei Xia, Ranchen Ming, Yibo Zhu, Xin Jin",http://arxiv.org/pdf/2409.13221v3,cs.CL
A Multiple-Fill-in-the-Blank Exam Approach for Enhancing Zero-Resource Hallucination Detection in Large Language Models,"Large language models (LLMs) often fabricate a hallucinatory text. Several
methods have been developed to detect such text by semantically comparing it
with the multiple versions probabilistically regenerated. However, a
significant issue is that if the storyline of each regenerated text changes,
the generated texts become incomparable, which worsen detection accuracy. In
this paper, we propose a hallucination detection method that incorporates a
multiple-fill-in-the-blank exam approach to address this storyline-changing
issue. First, our method creates a multiple-fill-in-the-blank exam by masking
multiple objects from the original text. Second, prompts an LLM to repeatedly
answer this exam. This approach ensures that the storylines of the exam answers
align with the original ones. Finally, quantifies the degree of hallucination
for each original sentence by scoring the exam answers, considering the
potential for \emph{hallucination snowballing} within the original text itself.
Experimental results show that our method alone not only outperforms existing
methods, but also achieves clearer state-of-the-art performance in the
ensembles with existing methods.",2024-09-20,"Satoshi Munakata, Taku Fukui, Takao Mohri",http://arxiv.org/pdf/2409.17173v1,cs.CL
Neural-Symbolic Collaborative Distillation: Advancing Small Language Models for Complex Reasoning Tasks,"In this paper, we propose $\textbf{Ne}$ural-$\textbf{Sy}$mbolic
$\textbf{C}$ollaborative $\textbf{D}$istillation ($\textbf{NesyCD}$), a novel
knowledge distillation method for learning the complex reasoning abilities of
Large Language Models (LLMs, e.g., \textgreater 13B). We argue that complex
reasoning tasks are difficult for Small Language Models (SLMs, e.g., $\leq$
7B), as these tasks demand not only general cognitive abilities but also
specialized knowledge, which is often sparse and difficult for these
neural-based SLMs to effectively capture. Therefore, NesyCD distills the
general capabilities and specialized knowledge in LLMs using different manners.
On the one hand, we distill only general abilities from teacher LLMs into the
student SLMs of parameterized neural networks. On the other hand, for the
specialized abilities and uncommon knowledge of a complex reasoning task, we
employ a symbolic knowledge distillation approach to obtain and store the
specialized knowledge within a symbolic knowledge base (KB). By decoupling
general and specialized capabilities, the proposed NesyCD can achieve superior
performance cost-effectively, utilizing smaller models and blending
parameterized neural networks with symbolic KB. Moreover, the specialized KB
generalizes well and is comprehended and manipulated by humans. Our experiments
show that NesyCD significantly boosts SLMs' complex reasoning performance on
in-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our
approach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in
performance and come close to matching LLaMA3-70B, despite the latter having
nine times more parameters. Our code will be available at
https://github.com/Xnhyacinth/NesyCD.",2024-09-20,"Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Jun Zhao",http://arxiv.org/pdf/2409.13203v4,cs.CL
CITI: Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing General Performance,"Tool learning enables the Large Language Models (LLMs) to interact with the
external environment by invoking tools, enriching the accuracy and capability
scope of LLMs. However, previous works predominantly focus on improving model's
tool-utilizing accuracy and the ability to generalize to new, unseen tools,
excessively forcing LLMs to adjust specific tool-invoking pattern without
considering the harm to model's general performance. This deviates from the
actual applications and original intention of integrating tools to enhance
model. To tackle this problem, we dissect the capability trade-offs by
examining the hidden representation changes and the gradient-based importance
score of model's components. Based on the analysis result, we propose a
Component Importance-based Tool-utilizing ability Injection method (CITI).
According to the gradient-based importance score of different components, it
alleviates the capability conflicts caused by fine-tuning process by applying
distinct training strategies to different components. CITI applies
Mixture-Of-LoRA (MOLoRA) for important components. Meanwhile, it fine-tunes the
parameters of few components deemed less important in the backbone of the LLM,
while keeping other parameters frozen. CITI can effectively enhance the model's
tool-utilizing capability without excessively compromising its general
performance. Experimental results demonstrate that our approach achieves
outstanding performance across a range of evaluation metrics.",2024-09-20,"Yupu Hao, Pengfei Cao, Zhuoran Jin, Huanxuan Liao, Yubo Chen, Kang Liu, Jun Zhao",http://arxiv.org/pdf/2409.13202v2,cs.CL
CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information,"The colossal parameters and computational overhead of Large Language Models
(LLMs) challenge their real-world applications. Network pruning, which targets
unstructured or structured sparsity by removing redundant parameters, has
recently been explored for LLM acceleration. Existing LLM pruning works focus
on unstructured pruning, which typically requires special hardware support for
a practical speed-up. In contrast, structured pruning can reduce latency on
general devices. However, it remains a challenge to perform structured pruning
efficiently and maintain performance, especially at high sparsity ratios. To
this end, we introduce an efficient structured pruning framework named CFSP,
which leverages both Coarse (interblock) and Fine-grained (intrablock)
activation information as an importance criterion to guide pruning. The pruning
is highly efficient, as it only requires one forward pass to compute feature
activations. Specifically, we first allocate the sparsity budget across blocks
based on their importance and then retain important weights within each block.
In addition, we introduce a recovery fine-tuning strategy that adaptively
allocates training overhead based on coarse-grained importance to further
improve performance. Experimental results demonstrate that CFSP outperforms
existing methods on diverse models across various sparsity budgets. Our code
will be available at https://github.com/wyxscir/CFSP.",2024-09-20,"Yuxin Wang, Minghua Ma, Zekun Wang, Jingchang Chen, Huiming Fan, Liping Shan, Qing Yang, Dongliang Xu, Ming Liu, Bing Qin",http://arxiv.org/pdf/2409.13199v2,cs.CL
Exploring Scaling Laws for Local SGD in Large Language Model Training,"This paper investigates scaling laws for local SGD in LLM training, a
distributed optimization algorithm that facilitates training on loosely
connected devices. Through extensive experiments, we show that local SGD
achieves competitive results compared to conventional methods, given equivalent
model parameters, datasets, and computational resources. Furthermore, we
explore the application of local SGD in various practical scenarios, including
multi-cluster setups and edge computing environments. Our findings elucidate
the necessary conditions for effective multi-cluster LLM training and examine
the potential and limitations of leveraging edge computing resources in the LLM
training process. This demonstrates its viability as an alternative to single
large-cluster training.",2024-09-20,"Qiaozhi He, Xiaomin Zhuang, Zhihua Wu",http://arxiv.org/pdf/2409.13198v1,cs.CL
ControlMath: Controllable Data Generation Promotes Math Generalist Models,"Utilizing large language models (LLMs) for data augmentation has yielded
encouraging results in mathematical reasoning. However, these approaches face
constraints in problem diversity, potentially restricting them to
in-domain/distribution data generation. To this end, we propose ControlMath, an
iterative method involving an equation-generator module and two LLM-based
agents. The module creates diverse equations, which the Problem-Crafter agent
then transforms into math word problems. The Reverse-Agent filters and selects
high-quality data, adhering to the ""less is more"" principle, achieving better
results with fewer data points. This approach enables the generation of diverse
math problems, not limited to specific domains or distributions. As a result,
we collect ControlMathQA, which involves 190k math word problems. Extensive
results prove that combining our dataset with in-domain datasets like GSM8K can
help improve the model's mathematical ability to generalize, leading to
improved performances both within and beyond specific domains.",2024-09-20,"Nuo Chen, Ning Wu, Jianhui Chang, Jia Li",http://arxiv.org/pdf/2409.15376v1,cs.CL
ChemDFM-X: Towards Large Multimodal Model for Chemistry,"Rapid developments of AI tools are expected to offer unprecedented assistance
to the research of natural science including chemistry. However, neither
existing unimodal task-specific specialist models nor emerging general large
multimodal models (LMM) can cover the wide range of chemical data modality and
task categories. To address the real demands of chemists, a cross-modal
Chemical General Intelligence (CGI) system, which serves as a truly practical
and useful research assistant utilizing the great potential of LMMs, is in
great need. In this work, we introduce the first Cross-modal Dialogue
Foundation Model for Chemistry (ChemDFM-X). Diverse multimodal data are
generated from an initial modality by approximate calculations and
task-specific model predictions. This strategy creates sufficient chemical
training corpora, while significantly reducing excessive expense, resulting in
an instruction-tuning dataset containing 7.6M data. After instruction
finetuning, ChemDFM-X is evaluated on extensive experiments of different
chemical tasks with various data modalities. The results demonstrate the
capacity of ChemDFM-X for multimodal and inter-modal knowledge comprehension.
ChemDFM-X marks a significant milestone toward aligning all modalities in
chemistry, a step closer to CGI.",2024-09-20,"Zihan Zhao, Bo Chen, Jingpiao Li, Lu Chen, Liyang Wen, Pengyu Wang, Zichen Zhu, Danyang Zhang, Ziping Wan, Yansi Li, Zhongyang Dai, Xin Chen, Kai Yu",http://arxiv.org/pdf/2409.13194v2,cs.CL
Diabetica: Adapting Large Language Model to Enhance Multiple Medical Tasks in Diabetes Care and Management,"Diabetes is a chronic disease with a significant global health burden,
requiring multi-stakeholder collaboration for optimal management. Large
language models (LLMs) have shown promise in various healthcare scenarios, but
their effectiveness across diverse diabetes tasks remains unproven. Our study
introduced a framework to train and validate diabetes-specific LLMs. We first
developed a comprehensive data processing pipeline that includes data
collection, filtering, augmentation and refinement. This created a
high-quality, diabetes-specific dataset and evaluation benchmarks from scratch.
Fine-tuned on the collected training dataset, our diabetes-specific LLM family
demonstrated state-of-the-art proficiency in processing various diabetes tasks
compared to other LLMs. Furthermore, clinical studies revealed the potential
applications of our models in diabetes care, including providing personalized
healthcare, assisting medical education, and streamlining clinical tasks.
Generally, our introduced framework helps develop diabetes-specific LLMs and
highlights their potential to enhance clinical practice and provide
personalized, data-driven support for diabetes management across different end
users. Our codes, benchmarks and models are available at
https://github.com/waltonfuture/Diabetica.",2024-09-20,"Lai Wei, Zhen Ying, Muyang He, Yutong Chen, Qian Yang, Yanzhe Hong, Jiaping Lu, Kaipeng Zheng, Shaoting Zhang, Xiaoying Li, Weiran Huang, Ying Chen",http://arxiv.org/pdf/2409.13191v2,cs.CL
$\textit{SKIntern}$: Internalizing Symbolic Knowledge for Distilling Better CoT Capabilities into Small Language Models,"Small Language Models (SLMs) are attracting attention due to the high
computational demands and privacy concerns of Large Language Models (LLMs).
Some studies fine-tune SLMs using Chains of Thought (CoT) data distilled from
LLMs, aiming to enhance their reasoning ability. Furthermore, Some CoT
distillation methods introduce external symbolic knowledge into the generation
process to improve the limited knowledge memory, reasoning ability and
out-of-domain (OOD) generalization of SLMs. However, the introduction of
symbolic knowledge increases computational overhead and introduces potential
noise. In this paper, we introduce $\textit{SKIntern}$, an innovative approach
that empowers SLMs to internalize symbolic knowledge and few-shot examples
gradually through a progressive fine-tuning process, guided by a predefined
linear decay schedule under curriculum learning. By efficiently internalizing
knowledge, $\textit{SKIntern}$ reduces computational overhead and speeds up the
reasoning process by focusing solely on the question during inference. It
outperforms state-of-the-art baselines by over 5\%, while reducing inference
costs (measured in FLOPs) by up to $4\times$ across a wide range of SLMs in
both in-domain (ID) and out-of-domain (OOD) tasks. Our code will be available
at \url{https://github.com/Xnhyacinth/SKIntern}.",2024-09-20,"Huanxuan Liao, Shizhu He, Yupu Hao, Xiang Li, Yuanzhe Zhang, Jun Zhao, Kang Liu",http://arxiv.org/pdf/2409.13183v2,cs.CL
RRM: Robust Reward Model Training Mitigates Reward Hacking,"Reward models (RMs) play a pivotal role in aligning large language models
(LLMs) with human preferences. However, traditional RM training, which relies
on response pairs tied to specific prompts, struggles to disentangle
prompt-driven preferences from prompt-independent artifacts, such as response
length and format. In this work, we expose a fundamental limitation of current
RM training methods, where RMs fail to effectively distinguish between
contextual signals and irrelevant artifacts when determining preferences. To
address this, we introduce a causal framework that learns preferences
independent of these artifacts and propose a novel data augmentation technique
designed to eliminate them. Extensive experiments show that our approach
successfully filters out undesirable artifacts, yielding a more robust reward
model (RRM). Our RRM improves the performance of a pairwise reward model
trained on Gemma-2-9b-it, on RewardBench, increasing accuracy from 80.61% to
84.15%. Additionally, we train two DPO policies using both the RM and RRM,
demonstrating that the RRM significantly enhances DPO-aligned policies,
improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in
AlpacaEval-2 from 33.46% to 52.49%.",2024-09-20,"Tianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen, Zhen Qin, Tianhe Yu, Daniel Sohn, Anastasiia Makarova, Jeremiah Liu, Yuan Liu, Bilal Piot, Abe Ittycheriah, Aviral Kumar, Mohammad Saleh",http://arxiv.org/pdf/2409.13156v2,cs.CL
Utility of Multimodal Large Language Models in Analyzing Chest X-ray with Incomplete Contextual Information,"Background: Large language models (LLMs) are gaining use in clinical
settings, but their performance can suffer with incomplete radiology reports.
We tested whether multimodal LLMs (using text and images) could improve
accuracy and understanding in chest radiography reports, making them more
effective for clinical decision support.
  Purpose: To assess the robustness of LLMs in generating accurate impressions
from chest radiography reports using both incomplete data and multimodal data.
Material and Methods: We used 300 radiology image-report pairs from the
MIMIC-CXR database. Three LLMs (OpenFlamingo, MedFlamingo, IDEFICS) were tested
in both text-only and multimodal formats. Impressions were first generated from
the full text, then tested by removing 20%, 50%, and 80% of the text. The
impact of adding images was evaluated using chest x-rays, and model performance
was compared using three metrics with statistical analysis.
  Results: The text-only models (OpenFlamingo, MedFlamingo, IDEFICS) had
similar performance (ROUGE-L: 0.39 vs. 0.21 vs. 0.21; F1RadGraph: 0.34 vs. 0.17
vs. 0.17; F1CheXbert: 0.53 vs. 0.40 vs. 0.40), with OpenFlamingo performing
best on complete text (p<0.001). Performance declined with incomplete data
across all models. However, adding images significantly boosted the performance
of MedFlamingo and IDEFICS (p<0.001), equaling or surpassing OpenFlamingo, even
with incomplete text. Conclusion: LLMs may produce low-quality outputs with
incomplete radiology data, but multimodal LLMs can improve reliability and
support clinical decision-making.
  Keywords: Large language model; multimodal; semantic analysis; Chest
Radiography; Clinical Decision Support;",2024-09-20,"Choonghan Kim, Seonhee Cho, Joo Heung Yoon",http://arxiv.org/pdf/2410.07111v1,cs.CL
Are Large Language Models Good Essay Graders?,"We evaluate the effectiveness of Large Language Models (LLMs) in assessing
essay quality, focusing on their alignment with human grading. More precisely,
we evaluate ChatGPT and Llama in the Automated Essay Scoring (AES) task, a
crucial natural language processing (NLP) application in Education. We consider
both zero-shot and few-shot learning and different prompting approaches. We
compare the numeric grade provided by the LLMs to human rater-provided scores
utilizing the ASAP dataset, a well-known benchmark for the AES task. Our
research reveals that both LLMs generally assign lower scores compared to those
provided by the human raters; moreover, those scores do not correlate well with
those provided by the humans. In particular, ChatGPT tends to be harsher and
further misaligned with human evaluations than Llama. We also experiment with a
number of essay features commonly used by previous AES methods, related to
length, usage of connectives and transition words, and readability metrics,
including the number of spelling and grammar mistakes. We find that, generally,
none of these features correlates strongly with human or LLM scores. Finally,
we report results on Llama 3, which are generally better across the board, as
expected. Overall, while LLMs do not seem an adequate replacement for human
grading, our results are somewhat encouraging for their use as a tool to assist
humans in the grading of written essays in the future.",2024-09-19,"Anindita Kundu, Denilson Barbosa",http://arxiv.org/pdf/2409.13120v1,cs.CL
What Would You Ask When You First Saw $a^2+b^2=c^2$? Evaluating LLM on Curiosity-Driven Questioning,"Large language models (LLMs) can store a massive amount of knowledge, yet
their potential to acquire new knowledge remains unknown. We propose a novel
evaluation framework that evaluates this capability. This framework prompts
LLMs to generate questions about a statement introducing scientific knowledge,
simulating a curious person when facing the statement for the first time. We
score the qualities of the generated questions, thereby evaluating the
knowledge acquisition potential of the LLM. We apply controlled ablation
studies to validate our scoring procedures. Additionally, we created a
synthetic dataset consisting of 1101 statements in physics, chemistry, and
maths with distinct levels of difficulties, 300 general knowledge statements,
and 567 incorrect statements. Human evaluations were conducted to validate our
model assessments, achieving an approximate weighted Cohen's kappa of 0.7 on
all three metrics considered. We find that while large models like GPT-4 and
Mistral 8x7b are adept at generating coherent and relevant questions, the
smaller Phi-2 model is equally or more effective. This indicates that size does
not solely determine a model's knowledge acquisition potential. The proposed
framework quantifies a critical model capability that was commonly overlooked
and opens up research opportunities for developing more knowledgeable AI
systems",2024-09-19,"Shashidhar Reddy Javaji, Zining Zhu",http://arxiv.org/pdf/2409.17172v1,cs.CL
Cross-Domain Content Generation with Domain-Specific Small Language Models,"Generating domain-specific content using small language models poses
challenges, especially when dealing with multiple distinct datasets with
minimal overlap. In this study, we explore methods to enable a small language
model to produce coherent and relevant outputs for two different domains:
stories (Dataset A) and recipes (Dataset B). Our initial experiments show that
training individual models on each dataset yields satisfactory results, with
each model generating appropriate content within its domain. We find that
utilizing custom tokenizers tailored to each dataset significantly enhances
generation quality compared to using a generic tokenizer. Attempts to adapt a
single model to both domains using Low-Rank Adaptation (LoRA) or standard
fine-tuning do not yield substantial results, often failing to produce
meaningful outputs. Moreover, full fine-tuning without freezing the model's
existing weights leads to catastrophic forgetting, where the model loses
previously learned information and only retains knowledge from the new data. To
overcome these challenges, we employ a knowledge expansion strategy: training
only with additional parameters. This approach enables the model to generate
both stories and recipes upon request, effectively handling multiple domains
without suffering from catastrophic forgetting. Our findings demonstrate that
knowledge expansion with frozen layers is an effective method for small
language models to generate domain-specific content across distinct datasets.
This work contributes to the development of efficient multi-domain language
models and provides insights into managing catastrophic forgetting in
small-scale architectures.",2024-09-19,"Ankit Maloo, Abhinav Garg",http://arxiv.org/pdf/2409.17171v2,cs.CL
Personalized Speech Recognition for Children with Test-Time Adaptation,"Accurate automatic speech recognition (ASR) for children is crucial for
effective real-time child-AI interaction, especially in educational
applications. However, off-the-shelf ASR models primarily pre-trained on adult
data tend to generalize poorly to children's speech due to the data domain
shift from adults to children. Recent studies have found that supervised
fine-tuning on children's speech data can help bridge this domain shift, but
human annotations may be impractical to obtain for real-world applications and
adaptation at training time can overlook additional domain shifts occurring at
test time. We devised a novel ASR pipeline to apply unsupervised test-time
adaptation (TTA) methods for child speech recognition, so that ASR models
pre-trained on adult speech can be continuously adapted to each child speaker
at test time without further human annotations. Our results show that ASR
models adapted with TTA methods significantly outperform the unadapted
off-the-shelf ASR baselines both on average and statistically across individual
child speakers. Our analysis also discovered significant data domain shifts
both between child speakers and within each child speaker, which further
motivates the need for test-time adaptation.",2024-09-19,"Zhonghao Shi, Harshvardhan Srivastava, Xuan Shi, Shrikanth Narayanan, Maja J. Matarić",http://arxiv.org/pdf/2409.13095v1,cs.CL
Guided Profile Generation Improves Personalization with LLMs,"In modern commercial systems, including Recommendation, Ranking, and
E-Commerce platforms, there is a trend towards improving customer experiences
by incorporating Personalization context as input into Large Language Models
(LLMs). However, LLMs often struggle to effectively parse and utilize sparse
and complex personal context without additional processing or contextual
enrichment, underscoring the need for more sophisticated context understanding
mechanisms. In this work, we propose Guided Profile Generation (GPG), a general
method designed to generate personal profiles in natural language. As is
observed, intermediate guided profile generation enables LLMs to summarize, and
extract the important, distinctive features from the personal context into
concise, descriptive sentences, precisely tailoring their generation more
closely to an individual's unique habits and preferences. Our experimental
results show that GPG improves LLM's personalization ability across different
tasks, for example, it increases 37% accuracy in predicting personal preference
compared to directly feeding the LLMs with raw personal context.",2024-09-19,Jiarui Zhang,http://arxiv.org/pdf/2409.13093v1,cs.CL
Embedding Geometries of Contrastive Language-Image Pre-Training,"Since the publication of CLIP, the approach of using InfoNCE loss for
contrastive pre-training has become widely popular for bridging two or more
modalities. Despite its wide adoption, CLIP's original design choices of L2
normalization and cosine similarity logit have rarely been revisited. We have
systematically experimented with alternative geometries and softmax logits for
language-image pre-training and identified that variants with intuitive
Euclidean geometry, Euclidean CLIP (EuCLIP), match or exceed the performance of
CLIP and support hierarchical relationships at least as well as more
complicated hyperbolic alternative.",2024-09-19,"Jason Chuan-Chih Chou, Nahid Alam",http://arxiv.org/pdf/2409.13079v1,cs.CL
Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions,"Machine-learning technologies are seeing increased deployment in real-world
market scenarios. In this work, we explore the strategic behaviors of large
language models (LLMs) when deployed as autonomous agents in multi-commodity
markets, specifically within Cournot competition frameworks. We examine whether
LLMs can independently engage in anti-competitive practices such as collusion
or, more specifically, market division. Our findings demonstrate that LLMs can
effectively monopolize specific commodities by dynamically adjusting their
pricing and resource allocation strategies, thereby maximizing profitability
without direct human input or explicit collusion commands. These results pose
unique challenges and opportunities for businesses looking to integrate AI into
strategic roles and for regulatory bodies tasked with maintaining fair and
competitive markets. The study provides a foundation for further exploration
into the ramifications of deferring high-stakes decisions to LLM-based agents.",2024-09-19,"Ryan Y. Lin, Siddhartha Ojha, Kevin Cai, Maxwell F. Chen",http://arxiv.org/pdf/2410.00031v2,cs.CL
System 2 thinking in OpenAI's o1-preview model: Near-perfect performance on a mathematics exam,"The processes underlying human cognition are often divided into System 1,
which involves fast, intuitive thinking, and System 2, which involves slow,
deliberate reasoning. Previously, large language models were criticized for
lacking the deeper, more analytical capabilities of System 2. In September
2024, OpenAI introduced the o1 model series, designed to handle System 2-like
reasoning. While OpenAI's benchmarks are promising, independent validation is
still needed. In this study, we tested the o1-preview model twice on the Dutch
'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76
points. For context, only 24 out of 16,414 students in the Netherlands achieved
a perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,
well above the Dutch students' average of 40.63 points. Neither model had
access to the exam figures. Since there was a risk of model contami-nation
(i.e., the knowledge cutoff for o1-preview and GPT-4o was after the exam was
published online), we repeated the procedure with a new Mathematics B exam that
was published after the cutoff date. The results again indicated that
o1-preview performed strongly (97.8th percentile), which suggests that
contamination was not a factor. We also show that there is some variability in
the output of o1-preview, which means that sometimes there is 'luck' (the
answer is correct) or 'bad luck' (the output has diverged into something that
is incorrect). We demonstrate that the self-consistency approach, where
repeated prompts are given and the most common answer is selected, is a useful
strategy for identifying the correct answer. It is concluded that while
OpenAI's new model series holds great potential, certain risks must be
considered.",2024-09-19,"Joost de Winter, Dimitra Dodou, Yke Bauke Eisma",http://arxiv.org/pdf/2410.07114v5,cs.CL
Natural Language Processing Methods for the Study of Protein-Ligand Interactions,"Recent advances in Natural Language Processing (NLP) have ignited interest in
developing effective methods for predicting protein-ligand interactions (PLIs)
given their relevance to drug discovery and protein engineering efforts and the
ever-growing volume of biochemical sequence and structural data available. The
parallels between human languages and the ""languages"" used to represent
proteins and ligands have enabled the use of NLP machine learning approaches to
advance PLI studies. In this review, we explain where and how such approaches
have been applied in the recent literature and discuss useful mechanisms such
as long short-term memory, transformers, and attention. We conclude with a
discussion of the current limitations of NLP methods for the study of PLIs as
well as key challenges that need to be addressed in future work.",2024-09-19,"James Michels, Ramya Bandarupalli, Amin Ahangar Akbari, Thai Le, Hong Xiao, Jing Li, Erik F. Y. Hom",http://arxiv.org/pdf/2409.13057v2,cs.CL
LLM Surgery: Efficient Knowledge Unlearning and Editing in Large Language Models,"Large language models (LLMs) have revolutionized various domains, yet their
utility comes with significant challenges related to outdated or problematic
knowledge embedded during pretraining. This paper addresses the challenge of
modifying LLMs to unlearn problematic and outdated information while
efficiently integrating new knowledge without retraining from scratch. Here, we
propose LLM Surgery, a framework to efficiently modify LLM behaviour by
optimizing a three component objective function that: (1) Performs reverse
gradient on unlearning dataset (problematic and outdated information), (2)
Performs gradient descent on the update dataset (new and updated information),
and (3) Minimizes the KL divergence on the retain dataset (small subset of
unchanged text), ensuring alignment between pretrained and modified model
outputs. Due to the lack of publicly available datasets specifically tailored
for our novel task, we compiled a new dataset and an evaluation benchmark.
Using Llama2-7B, we demonstrate that LLM Surgery can achieve significant
forgetting on the unlearn set, a 20\% increase in accuracy on the update set,
and maintain performance on the retain set.",2024-09-19,"Akshaj Kumar Veldanda, Shi-Xiong Zhang, Anirban Das, Supriyo Chakraborty, Stephen Rawls, Sambit Sahu, Milind Naphade",http://arxiv.org/pdf/2409.13054v1,cs.CL
TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement Learning,"The increasing prevalence of large language models (LLMs) such as GPT-4 in
various applications has led to a surge in the size of prompts required for
optimal performance, leading to challenges in computational efficiency. Prompt
compression aims to reduce the inference cost by minimizing input tokens
without compromising on the task performance. However, existing prompt
compression techniques either rely on sub-optimal metrics such as information
entropy or model it as a task-agnostic token classification problem that fails
to capture task-specific information. To address these issues, we propose a
novel and efficient reinforcement learning (RL) based task-aware prompt
compression method. To ensure low latency requirements, we leverage existing
Transformer encoder-based token classification model while guiding the learning
process with task-specific reward signals using lightweight REINFORCE
algorithm. We evaluate the performance of our method on three diverse and
challenging tasks including text summarization, question answering and code
summarization. We demonstrate that our RL-guided compression method improves
the task performance by 8% - 189% across these three scenarios over
state-of-the-art compression techniques while satisfying the same compression
rate and latency requirements.",2024-09-19,"Shivam Shandilya, Menglin Xia, Supriyo Ghosh, Huiqiang Jiang, Jue Zhang, Qianhui Wu, Victor Rühle",http://arxiv.org/pdf/2409.13035v3,cs.CL
CLAIR-A: Leveraging Large Language Models to Judge Audio Captions,"The Automated Audio Captioning (AAC) task asks models to generate natural
language descriptions of an audio input. Evaluating these machine-generated
audio captions is a complex task that requires considering diverse factors,
among them, auditory scene understanding, sound-object inference, temporal
coherence, and the environmental context of the scene. While current methods
focus on specific aspects, they often fail to provide an overall score that
aligns well with human judgment. In this work, we propose CLAIR-A, a simple and
flexible method that leverages the zero-shot capabilities of large language
models (LLMs) to evaluate candidate audio captions by directly asking LLMs for
a semantic distance score. In our evaluations, CLAIR-A better predicts human
judgements of quality compared to traditional metrics, with a 5.8% relative
accuracy improvement compared to the domain-specific FENSE metric and up to 11%
over the best general-purpose measure on the Clotho-Eval dataset. Moreover,
CLAIR-A offers more transparency by allowing the language model to explain the
reasoning behind its scores, with these explanations rated up to 30% better by
human evaluators than those provided by baseline methods. CLAIR-A is made
publicly available at https://github.com/DavidMChan/clair-a.",2024-09-19,"Tsung-Han Wu, Joseph E. Gonzalez, Trevor Darrell, David M. Chan",http://arxiv.org/pdf/2409.12962v1,cs.CL
MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines,"The advent of Large Language Models (LLMs) has paved the way for AI search
engines, e.g., SearchGPT, showcasing a new paradigm in human-internet
interaction. However, most current AI search engines are limited to text-only
settings, neglecting the multimodal user queries and the text-image interleaved
nature of website information. Recently, Large Multimodal Models (LMMs) have
made impressive strides. Yet, whether they can function as AI search engines
remains under-explored, leaving the potential of LMMs in multimodal search an
open question. To this end, we first design a delicate pipeline,
MMSearch-Engine, to empower any LMMs with multimodal search capabilities. On
top of this, we introduce MMSearch, a comprehensive evaluation benchmark to
assess the multimodal search performance of LMMs. The curated dataset contains
300 manually collected instances spanning 14 subfields, which involves no
overlap with the current LMMs' training data, ensuring the correct answer can
only be obtained within searching. By using MMSearch-Engine, the LMMs are
evaluated by performing three individual tasks (requery, rerank, and
summarization), and one challenging end-to-end task with a complete searching
process. We conduct extensive experiments on closed-source and open-source
LMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best
results, which surpasses the commercial product, Perplexity Pro, in the
end-to-end task, demonstrating the effectiveness of our proposed pipeline. We
further present error analysis to unveil current LMMs still struggle to fully
grasp the multimodal search tasks, and conduct ablation study to indicate the
potential of scaling test-time computation for AI search engine. We hope
MMSearch may provide unique insights to guide the future development of
multimodal AI search engine. Project Page: https://mmsearch.github.io",2024-09-19,"Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, Hongsheng Li",http://arxiv.org/pdf/2409.12959v2,cs.CL
MURI: High-Quality Instruction Tuning Datasets for Low-Resource Languages via Reverse Instructions,"Instruction tuning enhances large language models (LLMs) by aligning them
with human preferences across diverse tasks. Traditional approaches to create
instruction tuning datasets face serious challenges for low-resource languages
due to their dependence on data annotation. This work introduces a novel
method, Multilingual Reverse Instructions (MURI), which generates high-quality
instruction tuning datasets for low-resource languages without requiring human
annotators or pre-existing multilingual models. Utilizing reverse instructions
and a translation pipeline, MURI produces instruction-output pairs from
existing human-written texts in low-resource languages. This method ensures
cultural relevance and diversity by sourcing texts from different native
domains and applying filters to eliminate inappropriate content. Our dataset,
MURI-IT, includes more than 2 million instruction-output pairs across 200
languages. Evaluation by native speakers and fine-tuning experiments with mT5
models demonstrate the approach's effectiveness for both NLU and open-ended
generation. We publicly release datasets and models at
https://github.com/akoksal/muri.",2024-09-19,"Abdullatif Köksal, Marion Thaler, Ayyoob Imani, Ahmet Üstün, Anna Korhonen, Hinrich Schütze",http://arxiv.org/pdf/2409.12958v1,cs.CL
Geometric Interpretation of Layer Normalization and a Comparative Analysis with RMSNorm,"This paper presents a novel geometric interpretation of LayerNorm and
explores how LayerNorm influences the norm and orientation of hidden vectors in
the representation space. With these geometric insights, we prepare the
foundation for comparing LayerNorm with RMSNorm. We show that the definition of
LayerNorm is innately linked to the uniform vector, defined as $\boldsymbol{1}
= [1, 1, 1, 1, \cdots, 1]^T \in \mathbb{R}^d$. We then show that the
standardization step in LayerNorm can be understood in three simple steps: (i)
remove the component of a vector along the uniform vector, (ii) normalize the
remaining vector, and (iii) scale the resultant vector by $\sqrt{d}$, where $d$
is the dimensionality of the representation space. We also provide additional
insights into how LayerNorm operates at inference time. Finally, we compare the
hidden representations of LayerNorm-based LLMs with models trained using
RMSNorm and show that all LLMs naturally operate orthogonal to the uniform
vector at inference time, that is, on average they do not have a component
along the uniform vector during inference. This presents the first mechanistic
evidence that removing the component along the uniform vector in LayerNorm is a
redundant step. These results advocate for using RMSNorm over LayerNorm which
is also more computationally efficient.",2024-09-19,"Akshat Gupta, Atahan Ozdemir, Gopala Anumanchipalli",http://arxiv.org/pdf/2409.12951v2,cs.CL
"Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation","Large Language Models (LLMs) have demonstrated significant performance
improvements across various cognitive tasks. An emerging application is using
LLMs to enhance retrieval-augmented generation (RAG) capabilities. These
systems require LLMs to understand user queries, retrieve relevant information,
and synthesize coherent and accurate responses. Given the increasing real-world
deployment of such systems, comprehensive evaluation becomes crucial. To this
end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set),
a high-quality evaluation dataset designed to test LLMs' ability to provide
factual responses, assess retrieval capabilities, and evaluate the reasoning
required to generate final answers. While previous work has provided datasets
and benchmarks to evaluate these abilities in isolation, FRAMES offers a
unified framework that provides a clearer picture of LLM performance in
end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions
that require the integration of information from multiple sources. We present
baseline results demonstrating that even state-of-the-art LLMs struggle with
this task, achieving 0.40 accuracy with no retrieval. The accuracy is
significantly improved with our proposed multi-step retrieval pipeline,
achieving an accuracy of 0.66 (>50% improvement). We hope our work will help
bridge evaluation gaps and assist in developing more robust and capable RAG
systems.",2024-09-19,"Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, Manaal Faruqui",http://arxiv.org/pdf/2409.12941v3,cs.CL
LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning,"In this paper, we propose a new data synthesis method called
\textbf{LogicPro}, which leverages LeetCode-style algorithm
\underline{Pro}blems and their corresponding \underline{Pro}gram solutions to
synthesize Complex \underline{Logic}al Reasoning data in text format. First, we
synthesize complex reasoning problems through source algorithm problems and
test cases. Then, standard answers and intermediate variable outputs are
obtained for each problem based on standard python solutions and test cases.
Finally, with the guidance of code intermediate variables, we synthesize the
text reasoning process for each reasoning problems. Through this method, we can
synthesize data that is difficult, scalable, effective, and comes with golden
standard answers and high-quality reasoning processes. As a result, with our
540K synthesized dataset constructed solely from 2,360 algorithm problems, our
approach
  Code and data are publicly available at
https://github.com/jiangjin1999/LogicPro achieves significant improvements in
multiple models for the datasets \textit{BBH$^{27}$}, \textit{LogicBench},
\textit{DROP}, \textit{AR-LSAT}, and \textit{GSM8K}, etc. outperforming a wide
range of existing reasoning datasets.",2024-09-19,"Jin Jiang, Yuchen Yan, Yang Liu, Yonggang Jin, Shuai Peng, Mengdi Zhang, Xunliang Cai, Yixin Cao, Liangcai Gao, Zhi Tang",http://arxiv.org/pdf/2409.12929v2,cs.CL
Evaluating Defences against Unsafe Feedback in RLHF,"While there has been progress towards aligning Large Language Models (LLMs)
with human values and ensuring safe behaviour at inference time, safety guards
can easily be removed when fine tuned on unsafe and harmful datasets. While
this setting has been treated extensively, another popular training paradigm,
learning from unsafe feedback with reinforcement learning, has previously been
unexplored. This is concerning due to the widespread deployment of feedback
collection systems. We address this gap by providing an analysis of learning
settings where feedback is harmful, i.e. that unsafe samples are preferred over
safe ones despite model developers goal to maintain safety. We find that
safety-aligned LLMs easily explore unsafe action spaces via generating harmful
text and optimize for reward that violates safety constraints indicating that
current safety guards are not enough to prevent learning from unsafe feedback.
In order to protect against this vulnerability, we adapt a number of both
""implict"" and ""explicit"" harmful fine-tuning defences to evaluate whether they
are effective as learning constraints in an RLHF setting finding that no method
is generally effective pointing to the need for more defence research. We end
the paper with the observation that some defences work by performing ""harmless
reward hacking"" for which we provide a theoretical explanation drawn from the
theory of Constrained Markov Decision Processes and provide some direction for
future defence development.",2024-09-19,"Domenic Rosati, Giles Edkins, Harsh Raj, David Atanasov, Subhabrata Majumdar, Janarthanan Rajendran, Frank Rudzicz, Hassan Sajjad",http://arxiv.org/pdf/2409.12914v3,cs.CL
Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization,"The pre-training phase of language models often begins with randomly
initialized parameters. With the current trends in scaling models, training
their large number of parameters can be extremely slow and costly. In contrast,
small language models are less expensive to train, but they often cannot
achieve the accuracy of large models. In this paper, we explore an intriguing
idea to connect these two different regimes: Can we develop a method to
initialize large language models using smaller pre-trained models? Will such
initialization bring any benefits in terms of training time and final accuracy?
In this paper, we introduce HyperCloning, a method that can expand the
parameters of a pre-trained language model to those of a larger model with
increased hidden dimensions. Our method ensures that the larger model retains
the functionality of the smaller model. As a result, the larger model already
inherits the predictive power and accuracy of the smaller model before the
training starts. We demonstrate that training such an initialized model results
in significant savings in terms of GPU hours required for pre-training large
language models.",2024-09-19,"Mohammad Samragh, Iman Mirzadeh, Keivan Alizadeh Vahid, Fartash Faghri, Minsik Cho, Moin Nabi, Devang Naik, Mehrdad Farajtabar",http://arxiv.org/pdf/2409.12903v2,cs.CL
Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and Gaussian-Decayed Contrastive Learning,"Recently, using large language models (LLMs) for data augmentation has led to
considerable improvements in unsupervised sentence embedding models. However,
existing methods encounter two primary challenges: limited data diversity and
high data noise. Current approaches often neglect fine-grained knowledge, such
as entities and quantities, leading to insufficient diversity. Besides,
unsupervised data frequently lacks discriminative information, and the
generated synthetic samples may introduce noise. In this paper, we propose a
pipeline-based data augmentation method via LLMs and introduce the
Gaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model
to enhance unsupervised sentence embeddings. To tackle the issue of low data
diversity, our pipeline utilizes knowledge graphs (KGs) to extract entities and
quantities, enabling LLMs to generate more diverse samples. To address high
data noise, the GCSE model uses a Gaussian-decayed function to limit the impact
of false hard negative samples, enhancing the model's discriminative
capability. Experimental results show that our approach achieves
state-of-the-art performance in semantic textual similarity (STS) tasks, using
fewer data samples and smaller LLMs, demonstrating its efficiency and
robustness across various models.",2024-09-19,"Peichao Lai, Zhengfeng Zhang, Wentao Zhang, Fangcheng Fu, Bin Cui",http://arxiv.org/pdf/2409.12887v3,cs.CL
Enhancing E-commerce Product Title Translation with Retrieval-Augmented Generation and Large Language Models,"E-commerce stores enable multilingual product discovery which require
accurate product title translation. Multilingual large language models (LLMs)
have shown promising capacity to perform machine translation tasks, and it can
also enhance and translate product titles cross-lingually in one step. However,
product title translation often requires more than just language conversion
because titles are short, lack context, and contain specialized terminology.
This study proposes a retrieval-augmented generation (RAG) approach that
leverages existing bilingual product information in e-commerce by retrieving
similar bilingual examples and incorporating them as few-shot prompts to
enhance LLM-based product title translation. Experiment results show that our
proposed RAG approach improve product title translation quality with chrF score
gains of up to 15.3% for language pairs where the LLM has limited proficiency.",2024-09-19,"Bryan Zhang, Taichi Nakatani, Stephan Walter",http://arxiv.org/pdf/2409.12880v1,cs.CL
A New Perspective on ADHD Research: Knowledge Graph Construction with LLMs and Network Based Insights,"Attention-Deficit/Hyperactivity Disorder (ADHD) is a challenging disorder to
study due to its complex symptomatology and diverse contributing factors. To
explore how we can gain deeper insights on this topic, we performed a network
analysis on a comprehensive knowledge graph (KG) of ADHD, constructed by
integrating scientific literature and clinical data with the help of
cutting-edge large language models. The analysis, including k-core techniques,
identified critical nodes and relationships that are central to understanding
the disorder. Building on these findings, we curated a knowledge graph that is
usable in a context-aware chatbot (Graph-RAG) with Large Language Models
(LLMs), enabling accurate and informed interactions. Our knowledge graph not
only advances the understanding of ADHD but also provides a powerful tool for
research and clinical applications.",2024-09-19,"Hakan T. Otal, Stephen V. Faraone, M. Abdullah Canbaz",http://arxiv.org/pdf/2409.12853v2,cs.CL
Lexicon-Based Sentiment Analysis on Text Polarities with Evaluation of Classification Models,"Sentiment analysis possesses the potential of diverse applicability on
digital platforms. Sentiment analysis extracts the polarity to understand the
intensity and subjectivity in the text. This work uses a lexicon-based method
to perform sentiment analysis and shows an evaluation of classification models
trained over textual data. The lexicon-based methods identify the intensity of
emotion and subjectivity at word levels. The categorization identifies the
informative words inside a text and specifies the quantitative ranking of the
polarity of words. This work is based on a multi-class problem of text being
labeled as positive, negative, or neutral. Twitter sentiment dataset containing
1.6 million unprocessed tweets is used with lexicon-based methods like Text
Blob and Vader Sentiment to introduce the neutrality measure on text. The
analysis of lexicons shows how the word count and the intensity classify the
text. A comparative analysis of machine learning models, Naiive Bayes, Support
Vector Machines, Multinomial Logistic Regression, Random Forest, and Extreme
Gradient (XG) Boost performed across multiple performance metrics. The best
estimations are achieved through Random Forest with an accuracy score of 81%.
Additionally, sentiment analysis is applied for a personality judgment case
against a Twitter profile based on online activity.",2024-09-19,"Muhammad Raees, Samina Fazilat",http://arxiv.org/pdf/2409.12840v1,cs.CL
Pay Attention to What Matters,"Despite the remarkable success of Large Language Models (LLMs), they still
exhibit a limited capability to align their outputs to the user instructions.
In this work, we introduce a simple and effective method, which we name GUIDE,
that mechanistically increases attention scores in instruction tokens. To
support this operation, we present Influence, a novel metric that highlights
how the user's instructions propagate through the transformer layers and impact
the LLM output. Our results show that GUIDE improves the accuracy of following
instructions 29.4 % to 60.4%, outperforming natural prompting alternatives and
Supervised Fine-Tuning up to 1M tokens.",2024-09-19,"Pedro Luiz Silva, Antonio de Domenico, Ali Maatouk, Fadhel Ayed",http://arxiv.org/pdf/2409.19001v1,cs.CL
FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists,"Flavor development in the food industry is increasingly challenged by the
need for rapid innovation and precise flavor profile creation. Traditional
flavor research methods typically rely on iterative, subjective testing, which
lacks the efficiency and scalability required for modern demands. This paper
presents three contributions to address the challenges. Firstly, we define a
new problem domain for scientific agents in flavor science, conceptualized as
the generation of hypotheses for flavor profile sourcing and understanding. To
facilitate research in this area, we introduce the FoodPuzzle, a challenging
benchmark consisting of 978 food items and 1,766 flavor molecules profiles. We
propose a novel Scientific Agent approach, integrating in-context learning and
retrieval augmented techniques to generate grounded hypotheses in the domain of
food science. Experimental results indicate that our model significantly
surpasses traditional methods in flavor profile prediction tasks, demonstrating
its potential to transform flavor development practices.",2024-09-19,"Tenghao Huang, Donghee Lee, John Sweeney, Jiatong Shi, Emily Steliotes, Matthew Lange, Jonathan May, Muhao Chen",http://arxiv.org/pdf/2409.12832v3,cs.CL
Language Models Learn to Mislead Humans via RLHF,"Language models (LMs) can produce errors that are hard to detect for humans,
especially when the task is complex. RLHF, the most popular post-training
method, may exacerbate this problem: to achieve higher rewards, LMs might get
better at convincing humans that they are right even when they are wrong. We
study this phenomenon under a standard RLHF pipeline, calling it ""U-SOPHISTRY""
since it is Unintended by model developers. Specifically, we ask
time-constrained (e.g., 3-10 minutes) human subjects to evaluate the
correctness of model outputs and calculate humans' accuracy against gold
labels. On a question-answering task (QuALITY) and programming task (APPS),
RLHF makes LMs better at convincing our subjects but not at completing the task
correctly. RLHF also makes the model harder to evaluate: our subjects' false
positive rate increases by 24.1% on QuALITY and 18.3% on APPS. Finally, we show
that probing, a state-of-the-art approach for detecting Intended Sophistry
(e.g. backdoored LMs), does not generalize to U-SOPHISTRY. Our results
highlight an important failure mode of RLHF and call for more research in
assisting humans to align them.",2024-09-19,"Jiaxin Wen, Ruiqi Zhong, Akbir Khan, Ethan Perez, Jacob Steinhardt, Minlie Huang, Samuel R. Bowman, He He, Shi Feng",http://arxiv.org/pdf/2409.12822v3,cs.CL
Bilingual Evaluation of Language Models on General Knowledge in University Entrance Exams with Minimal Contamination,"In this article we present UNED-ACCESS 2024, a bilingual dataset that
consists of 1003 multiple-choice questions of university entrance level exams
in Spanish and English. Questions are originally formulated in Spanish and
translated manually into English, and have not ever been publicly released. A
selection of current open-source and proprietary models are evaluated in a
uniform zero-shot experimental setting both on the UNED-ACCESS 2024 dataset and
on an equivalent subset of MMLU questions. Results show that (i) reasoning
questions are challenging for models, (ii) smaller models perform worse than
larger models and degrade faster in Spanish than in English and (iii) the
performance gap between languages is negligible for the best models and grows
up to 37% for smaller models. Model ranking on UNED-ACCESS 2024 is almost
identical in English and Spanish, and has also a high correlation (0.98
Pearson) with ranking on MMLU, suggesting that a small dataset is sufficiently
diverse and representative to measure performance by discipline.",2024-09-19,"Eva Sánchez Salido, Roser Morante, Julio Gonzalo, Guillermo Marco, Jorge Carrillo-de-Albornoz, Laura Plaza, Enrique Amigó, Andrés Fernández, Alejandro Benito-Santos, Adrián Ghajari Espinosa, Victor Fresno",http://arxiv.org/pdf/2409.12746v2,cs.CL
Fine Tuning Large Language Models for Medicine: The Role and Importance of Direct Preference Optimization,"Large Language Model (LLM) fine tuning is underutilized in the field of
medicine. Two of the most common methods of fine tuning are Supervised Fine
Tuning (SFT) and Direct Preference Optimization (DPO), but there is little
guidance informing users when to use either technique. In this investigation,
we compare the performance of SFT and DPO for five common natural language
tasks in medicine: Classification with text data, Classification with numeric
data, Clinical Reasoning, Summarization, and Clinical Triage. We find that SFT
alone is sufficient for Classification with text data, whereas DPO improves
performance for the more complex tasks of Clinical Reasoning, Summarization and
Clinical Triage. Our results establish the role and importance of DPO fine
tuning within medicine, and consequently call attention to current software
gaps that prevent widespread deployment of this technique.",2024-09-19,"Thomas Savage, Stephen Ma, Abdessalem Boukil, Vishwesh Patel, Ekanath Rangan, Ivan Lopez, Jonathan H Chen",http://arxiv.org/pdf/2409.12741v3,cs.CL
Edu-Values: Towards Evaluating the Chinese Education Values of Large Language Models,"In this paper, we present Edu-Values, the first Chinese education values
evaluation benchmark that includes seven core values: professional philosophy,
teachers' professional ethics, education laws and regulations, cultural
literacy, educational knowledge and skills, basic competencies and subject
knowledge. We meticulously design 1,418 questions, covering multiple-choice,
multi-modal question answering, subjective analysis, adversarial prompts, and
Chinese traditional culture (short answer) questions. We conduct human feedback
based automatic evaluation over 21 state-of-the-art (SoTA) LLMs, and highlight
three main findings: (1) due to differences in educational culture, Chinese
LLMs outperform English LLMs, with Qwen 2 ranking the first with a score of
81.37; (2) LLMs often struggle with teachers' professional ethics and
professional philosophy; (3) leveraging Edu-Values to build an external
knowledge repository for RAG significantly improves LLMs' alignment. This
demonstrates the effectiveness of the proposed benchmark.",2024-09-19,"Peiyi Zhang, Yazhou Zhang, Bo Wang, Lu Rong, Prayag Tiwari, Jing Qin",http://arxiv.org/pdf/2409.12739v3,cs.CL
MEXMA: Token-level objectives improve sentence representations,"Current pre-trained cross-lingual sentence encoders approaches use
sentence-level objectives only. This can lead to loss of information,
especially for tokens, which then degrades the sentence representation. We
propose MEXMA, a novel approach that integrates both sentence-level and
token-level objectives. The sentence representation in one language is used to
predict masked tokens in another language, with both the sentence
representation and all tokens directly updating the encoder. We show that
adding token-level objectives greatly improves the sentence representation
quality across several tasks. Our approach outperforms current pre-trained
cross-lingual sentence encoders on bi-text mining as well as several downstream
tasks. We also analyse the information encoded in our tokens, and how the
sentence representation is built from them.",2024-09-19,"João Maria Janeiro, Benjamin Piwowarski, Patrick Gallinari, Loïc Barrault",http://arxiv.org/pdf/2409.12737v1,cs.CL
"LLM-Measure: Generating Valid, Consistent, and Reproducible Text-Based Measures for Social Science Research","The increasing use of text as data in social science research necessitates
the development of valid, consistent, reproducible, and efficient methods for
generating text-based concept measures. This paper presents a novel method that
leverages the internal hidden states of large language models (LLMs) to
generate these concept measures. Specifically, the proposed method learns a
concept vector that captures how the LLM internally represents the target
concept, then estimates the concept value for text data by projecting the
text's LLM hidden states onto the concept vector. Three replication studies
demonstrate the method's effectiveness in producing highly valid, consistent,
and reproducible text-based measures across various social science research
contexts, highlighting its potential as a valuable tool for the research
community.",2024-09-19,"Yi Yang, Hanyu Duan, Jiaxin Liu, Kar Yan Tam",http://arxiv.org/pdf/2409.12722v1,cs.CL
CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair,"Despite the significant progress made in code generation with large language
models, challenges persist, especially with hardware description languages such
as Verilog. This paper first presents an analysis of fine-tuned LLMs on Verilog
coding, with synthetic data from prior methods. We identify two main issues:
difficulties in handling non-textual representations (Karnaugh maps,
state-transition diagrams and waveforms) and significant variability during
training with models randomly making ""minor"" mistakes. To address these
limitations, we enhance data curation by creating correct-by-construction data
targeting non-textual representations. Additionally, we introduce an automated
framework that generates error reports from various model checkpoints and
injects these errors into open-source code to create targeted code repair data.
Our fine-tuned Starcoder2-15B outperforms prior state-of-the-art results by
3.8%, 10.9%, 6.6% for pass@1 on VerilogEval-Machine, VerilogEval-Human, and
RTLLM.",2024-09-19,"Mingjie Liu, Yun-Da Tsai, Wenfei Zhou, Haoxing Ren",http://arxiv.org/pdf/2409.12993v2,cs.CL
Exploring Large Language Models for Product Attribute Value Identification,"Product attribute value identification (PAVI) involves automatically
identifying attributes and their values from product information, enabling
features like product search, recommendation, and comparison. Existing methods
primarily rely on fine-tuning pre-trained language models, such as BART and T5,
which require extensive task-specific training data and struggle to generalize
to new attributes. This paper explores large language models (LLMs), such as
LLaMA and Mistral, as data-efficient and robust alternatives for PAVI. We
propose various strategies: comparing one-step and two-step prompt-based
approaches in zero-shot settings and utilizing parametric and non-parametric
knowledge through in-context learning examples. We also introduce a dense
demonstration retriever based on a pre-trained T5 model and perform instruction
fine-tuning to explicitly train LLMs on task-specific instructions. Extensive
experiments on two product benchmarks show that our two-step approach
significantly improves performance in zero-shot settings, and instruction
fine-tuning further boosts performance when using training data, demonstrating
the practical benefits of using LLMs for PAVI.",2024-09-19,"Kassem Sabeh, Mouna Kacimi, Johann Gamper, Robert Litschko, Barbara Plank",http://arxiv.org/pdf/2409.12695v1,cs.CL
"Connecting Ideas in 'Lower-Resource' Scenarios: NLP for National Varieties, Creoles and Other Low-resource Scenarios","Despite excellent results on benchmarks over a small subset of languages,
large language models struggle to process text from languages situated in
`lower-resource' scenarios such as dialects/sociolects (national or social
varieties of a language), Creoles (languages arising from linguistic contact
between multiple languages) and other low-resource languages. This introductory
tutorial will identify common challenges, approaches, and themes in natural
language processing (NLP) research for confronting and overcoming the obstacles
inherent to data-poor contexts. By connecting past ideas to the present field,
this tutorial aims to ignite collaboration and cross-pollination between
researchers working in these scenarios. Our notion of `lower-resource' broadly
denotes the outstanding lack of data required for model training - and may be
applied to scenarios apart from the three covered in the tutorial.",2024-09-19,"Aditya Joshi, Diptesh Kanojia, Heather Lent, Hour Kaing, Haiyue Song",http://arxiv.org/pdf/2409.12683v1,cs.CL
Text2Traj2Text: Learning-by-Synthesis Framework for Contextual Captioning of Human Movement Trajectories,"This paper presents Text2Traj2Text, a novel learning-by-synthesis framework
for captioning possible contexts behind shopper's trajectory data in retail
stores. Our work will impact various retail applications that need better
customer understanding, such as targeted advertising and inventory management.
The key idea is leveraging large language models to synthesize a diverse and
realistic collection of contextual captions as well as the corresponding
movement trajectories on a store map. Despite learned from fully synthesized
data, the captioning model can generalize well to trajectories/captions created
by real human subjects. Our systematic evaluation confirmed the effectiveness
of the proposed framework over competitive approaches in terms of ROUGE and
BERT Score metrics.",2024-09-19,"Hikaru Asano, Ryo Yonetani, Taiki Sekii, Hiroki Ouchi",http://arxiv.org/pdf/2409.12670v1,cs.CL
"Exploring the topics, sentiments and hate speech in the Spanish information environment","In the digital era, the internet and social media have transformed
communication but have also facilitated the spread of hate speech and
disinformation, leading to radicalization, polarization, and toxicity. This is
especially concerning for media outlets due to their significant role in
shaping public discourse. This study examines the topics, sentiments, and hate
prevalence in 337,807 response messages (website comments and tweets) to news
from five Spanish media outlets (La Vanguardia, ABC, El Pa\'is, El Mundo, and
20 Minutos) in January 2021. These public reactions were originally labeled as
distinct types of hate by experts following an original procedure, and they are
now classified into three sentiment values (negative, neutral, or positive) and
main topics. The BERTopic unsupervised framework was used to extract 81 topics,
manually named with the help of Large Language Models (LLMs) and grouped into
nine primary categories.
  Results show social issues (22.22%), expressions and slang (20.35%), and
political issues (11.80%) as the most discussed. Content is mainly negative
(62.7%) and neutral (28.57%), with low positivity (8.73%). Toxic narratives
relate to conversation expressions, gender, feminism, and COVID-19. Despite low
levels of hate speech (3.98%), the study confirms high toxicity in online
responses to social and political topics.",2024-09-19,"ALEJANDRO BUITRAGO LOPEZ, Javier Pastor-Galindo, José Antonio Ruipérez-Valiente",http://arxiv.org/pdf/2409.12658v1,cs.CL
Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards,"Scientific leaderboards are standardized ranking systems that facilitate
evaluating and comparing competitive methods. Typically, a leaderboard is
defined by a task, dataset, and evaluation metric (TDM) triple, allowing
objective performance assessment and fostering innovation through benchmarking.
However, the exponential increase in publications has made it infeasible to
construct and maintain these leaderboards manually. Automatic leaderboard
construction has emerged as a solution to reduce manual labor. Existing
datasets for this task are based on the community-contributed leaderboards
without additional curation. Our analysis shows that a large portion of these
leaderboards are incomplete, and some of them contain incorrect information. In
this work, we present SciLead, a manually-curated Scientific Leaderboard
dataset that overcomes the aforementioned problems. Building on this dataset,
we propose three experimental settings that simulate real-world scenarios where
TDM triples are fully defined, partially defined, or undefined during
leaderboard construction. While previous research has only explored the first
setting, the latter two are more representative of real-world applications. To
address these diverse settings, we develop a comprehensive LLM-based framework
for constructing leaderboards. Our experiments and analysis reveal that various
LLMs often correctly identify TDM triples while struggling to extract result
values from publications. We make our code and data publicly available.",2024-09-19,"Furkan Şahinuç, Thy Thy Tran, Yulia Grishina, Yufang Hou, Bei Chen, Iryna Gurevych",http://arxiv.org/pdf/2409.12656v1,cs.CL
Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,"We introduce Michelangelo: a minimal, synthetic, and unleaked long-context
reasoning evaluation for large language models which is also easy to
automatically score. This evaluation is derived via a novel, unifying framework
for evaluations over arbitrarily long contexts which measure the model's
ability to do more than retrieve a single piece of information from its
context. The central idea of the Latent Structure Queries framework (LSQ) is to
construct tasks which require a model to ``chisel away'' the irrelevant
information in the context, revealing a latent structure in the context. To
verify a model's understanding of this latent structure, we query the model for
details of the structure. Using LSQ, we produce three diagnostic long-context
evaluations across code and natural-language domains intended to provide a
stronger signal of long-context language model capabilities. We perform
evaluations on several state-of-the-art models and demonstrate both that a) the
proposed evaluations are high-signal and b) that there is significant room for
improvement in synthesizing long-context information.",2024-09-19,"Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, Rohan Anil, Ethan Dyer, Siamak Shakeri, Roopali Vij, Harsh Mehta, Vinay Ramasesh, Quoc Le, Ed Chi, Yifeng Lu, Orhan Firat, Angeliki Lazaridou, Jean-Baptiste Lespiau, Nithya Attaluri, Kate Olszewska",http://arxiv.org/pdf/2409.12640v2,cs.CL
Balancing LoRA Performance and Efficiency with Simple Shard Sharing,"Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank
Adaptation (LoRA), effectively reduce the number of trainable parameters in
Large Language Models (LLMs). However, as model scales continue to grow, the
demand for computational resources remains a significant challenge. Existing
LoRA variants often struggle to strike an optimal balance between adaptability
(model performance and convergence speed) and efficiency (computational
overhead, memory usage, and initialization time). This paper introduces
MiSS(Matrix Shard Sharing ), a novel PEFT approach that addresses this
trade-off through a simple shard-sharing mechanism. MiSS leverages the insight
that a low-rank adaptation can be achieved by decomposing the weight matrix
into multiple fragment matrices and utilizing a shared, trainable common
fragment. This method constructs the low-rank update matrix through the
replication of these shared, partitioned shards. We also propose a
hardware-efficient and broadly applicable implementation for MiSS. Extensive
experiments conducted on a range of tasks, alongside a systematic analysis of
computational performance, demonstrate MiSS's superiority. The results show
that MiSS significantly outperforms standard LoRA and its prominent variants in
both model performance metrics and computational efficiency, including
initialization speed and training throughput. By effectively balancing
expressive power and resource utilization, MiSS offers a compelling solution
for efficiently adapting large-scale models.",2024-09-19,"Jiale Kang, Qingyu Yin",http://arxiv.org/pdf/2409.15371v10,cs.CL
Enhancing TinyBERT for Financial Sentiment Analysis Using GPT-Augmented FinBERT Distillation,"In the rapidly evolving field of financial sentiment analysis, the efficiency
and accuracy of predictive models are critical due to their significant impact
on financial markets. Transformer based models like BERT and large language
models (LLMs) like GPT-4, have advanced NLP tasks considerably. Despite their
advantages, BERT-based models face challenges with computational intensity in
edge computing environments, and the substantial size and compute requirements
of LLMs limit their practical deployment. This study proposes leveraging the
generative capabilities of LLMs, such as GPT-4 Omni, to create synthetic,
domain-specific training data. This approach addresses the challenge of data
scarcity and enhances the performance of smaller models by making them
competitive with their larger counterparts. The research specifically aims to
enhance FinBERT, a BERT model fine-tuned for financial sentiment analysis, and
develop TinyFinBERT, a compact transformer model, through a structured,
two-tiered knowledge distillation strategy. Using data augmented by GPT-4 Omni,
which involves generating new training examples and transforming existing data,
we significantly improved the accuracy of FinBERT, preparing it to serve as a
teacher model. This enhanced FinBERT then distilled knowledge to TinyFinBERT,
employing both GPT-4 Omni and GPT-3.5 Turbo augmented data. The distillation
strategy incorporated both logit and intermediate layer distillation. The
training and evaluation of TinyFinBERT utilized the PhraseBank dataset and the
FiQA 2018 Task1 dataset, achieving performance comparable to FinBERT while
being substantially smaller and more efficient. This research demonstrates how
LLMs can effectively contribute to the advancement of financial sentiment
analysis by enhancing the capabilities of smaller, more efficient models
through innovative data augmentation and distillation techniques.",2024-09-19,Graison Jos Thomas,http://arxiv.org/pdf/2409.18999v1,cs.CL
CamelEval: Advancing Culturally Aligned Arabic Language Models and Benchmarks,"Large Language Models (LLMs) are the cornerstones of modern artificial
intelligence systems. This paper introduces Juhaina, a Arabic-English bilingual
LLM specifically designed to align with the values and preferences of Arabic
speakers. Juhaina inherently supports advanced functionalities such as
instruction following, open-ended question answering, information provisioning,
and text processing. Our model contains 9.24 billion parameters and is trained
on a context window of up to 8,192 tokens. This paper details the creation
process of Juhaina and provides an extensive empirical evaluation. Furthermore,
we identify the limitations of widely-adopted Open Arabic LLM Leaderboard
(OALL) and propose a new evaluation benchmark, CamelEval. Our findings
demonstrate that Juhaina surpasses existing LLMs of comparable sizes, such as
the Llama and Gemma families, in generating helpful responses in Arabic,
providing factually accurate information about the region, and understanding
nuanced cultural aspects. We aspire for Juhaina to democratize cutting-edge AI
technologies, serving over 400 million Arabic speakers by offering LLMs that
not only communicate in their language but also comprehend their culture. We
publicly release all models on Huggingface \url{https://huggingface.co/elmrc}.",2024-09-19,"Zhaozhi Qian, Faroq Altam, Muhammad Alqurishi, Riad Souissi",http://arxiv.org/pdf/2409.12623v2,cs.CL
Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning,"Iterative human engagement is a common and effective means of leveraging the
advanced language processing power of large language models (LLMs). Using
well-structured prompts in a conversational manner, human users can effectively
influence an LLM to develop more thoughtful and accurate responses. Motivated
by this insight, we propose the Iteration of Thought (IoT) framework for
enhancing LLM responses by generating ""thought""-provoking prompts vis a vis an
input query and the current iteration of an LLM's response. Unlike static or
semi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),
IoT adapts its reasoning path dynamically, based on evolving context, and
without generating alternate explorative thoughts which are ultimately
discarded. The three components of the IoT framework are (1) an Inner Dialogue
Agent (IDA) responsible for generating instructive, context-specific prompts;
(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;
and (3) an iterative prompting loop that implements a conversation between the
former two components. We introduce two variants of our framework: Autonomous
Iteration of Thought (AIoT), where an LLM decides when to stop iterating, and
Guided Iteration of Thought (GIoT), which always forces a fixed number
iterations. We investigate the performance of IoT across various datasets,
spanning complex reasoning tasks from the GPQA dataset, explorative
problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop
question answering from the HotpotQA dataset. Our results show that IoT
represents a viable paradigm for autonomous response refinement in LLMs,
showcasing significant improvements over CoT and thereby enabling more adaptive
and efficient reasoning systems that minimize human intervention.",2024-09-19,"Santosh Kumar Radha, Yasamin Nouri Jelyani, Ara Ghukasyan, Oktay Goktas",http://arxiv.org/pdf/2409.12618v2,cs.CL
Controlled LLM-based Reasoning for Clinical Trial Retrieval,"Matching patients to clinical trials demands a systematic and reasoned
interpretation of documents which require significant expert-level background
knowledge, over a complex set of well-defined eligibility criteria. Moreover,
this interpretation process needs to operate at scale, over vast knowledge
bases of trials. In this paper, we propose a scalable method that extends the
capabilities of LLMs in the direction of systematizing the reasoning over sets
of medical eligibility criteria, evaluating it in the context of real-world
cases. The proposed method overlays a Set-guided reasoning method for LLMs. The
proposed framework is evaluated on TREC 2022 Clinical Trials, achieving results
superior to the state-of-the-art: NDCG@10 of 0.693 and Precision@10 of 0.73.",2024-09-19,"Mael Jullien, Alex Bogatu, Harriet Unsworth, Andre Freitas",http://arxiv.org/pdf/2409.18998v1,cs.CL
Enhancing SLM via ChatGPT and Dataset Augmentation,"This paper explores the enhancement of small language models through
strategic dataset augmentation via ChatGPT-3.5-Turbo, in the domain of Natural
Language Inference (NLI). By employing knowledge distillation-based techniques
and synthetic dataset augmentation, we aim to bridge the performance gap
between large language models (LLMs) and small language models (SLMs) without
the immense cost of human annotation. Our methods involve two forms of
rationale generation--information extraction and informed reasoning--to enrich
the ANLI dataset. We then fine-tune T5-Small on these augmented datasets,
evaluating its performance against an established benchmark. Our findings
reveal that the incorporation of synthetic rationales significantly improves
the model's ability to comprehend natural language, leading to 1.3\% and 2.3\%
higher classification accuracy, respectively, on the ANLI dataset,
demonstrating the potential of leveraging LLMs for dataset augmentation. This
approach not only enhances the performance of smaller models on complex tasks
but also introduces a cost-effective method for fine-tuning smaller language
models. By advancing our understanding of knowledge distillation and
fine-tuning strategies, this work contributes to the ongoing effort to create
more capable and efficient NLP systems.",2024-09-19,"Tom Pieper, Mohamad Ballout, Ulf Krumnack, Gunther Heidemann, Kai-Uwe Kühnberger",http://arxiv.org/pdf/2409.12599v1,cs.CL
Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights,"Enhancing small language models for real-life application deployment is a
significant challenge facing the research community. Due to the difficulties
and costs of using large language models, researchers are seeking ways to
effectively deploy task-specific small models. In this work, we introduce a
simple yet effective knowledge distillation method to improve the performance
of small language models. Our approach utilizes a teacher model with
approximately 3 billion parameters to identify the most influential tokens in
its decision-making process. These tokens are extracted from the input based on
their attribution scores relative to the output, using methods like saliency
maps. These important tokens are then provided as rationales to a student
model, aiming to distill the knowledge of the teacher model. This method has
proven to be effective, as demonstrated by testing it on four diverse datasets,
where it shows improvement over both standard fine-tuning methods and
state-of-the-art knowledge distillation models. Furthermore, we explore
explanations of the success of the model by analyzing the important tokens
extracted from the teacher model. Our findings reveal that in 68\% of cases,
specifically in datasets where labels are part of the answer, such as
multiple-choice questions, the extracted tokens are part of the ground truth.",2024-09-19,"Mohamad Ballout, Ulf Krumnack, Gunther Heidemann, Kai-Uwe Kühnberger",http://arxiv.org/pdf/2409.12586v1,cs.CL
RAD-Bench: Evaluating Large Language Models Capabilities in Retrieval Augmented Dialogues,"In real-world applications with Large Language Models (LLMs), external
retrieval mechanisms - such as Search-Augmented Generation (SAG), tool
utilization, and Retrieval-Augmented Generation (RAG) - are often employed to
enhance the quality of augmented generations in dialogues. These approaches
often come with multi-turn dialogue, where each interaction is enriched by
relevant information retrieved from external sources. Existing benchmarks
either assess LLMs' chat abilities in multi-turn dialogues or their use of
retrieval for augmented responses in single-turn settings. However, there is a
gap in evaluating LLMs' ability to leverage retrieval for more precise
responses across multiple turns. To address this limitation, we introduce
RAD-Bench (Retrieval Augmented Dialogue), a benchmark designed to evaluate
LLMs' capabilities in multi-turn dialogues following retrievals, essential for
their deployment in context-rich applications. RAD-Bench evaluates two key
abilities of LLMs: Retrieval Synthesis and Retrieval Reasoning. These are
measured using discriminative questions and retrieved contexts, and
corresponding reference answers, assessing how effectively LLMs integrate and
reason with context to maintain and enhance conversation quality over multiple
turns. Our evaluation results on commonly used LLMs reveal that model
performance deteriorates as additional layers of conditions or constraints are
applied across conversation turns, even when accurate retrieved contexts are
provided. The data and code are available at
https://github.com/mtkresearch/RAD-Bench",2024-09-19,"Tzu-Lin Kuo, Feng-Ting Liao, Mu-Wei Hsieh, Fu-Chieh Chang, Po-Chun Hsu, Da-Shan Shiu",http://arxiv.org/pdf/2409.12558v2,cs.CL
Enhancing Knowledge Distillation of Large Language Models through Efficient Multi-Modal Distribution Alignment,"Knowledge distillation (KD) is an effective model compression method that can
transfer the internal capabilities of large language models (LLMs) to smaller
ones. However, the multi-modal probability distribution predicted by teacher
LLMs causes difficulties for student models to learn. In this paper, we first
demonstrate the importance of multi-modal distribution alignment with
experiments and then highlight the inefficiency of existing KD approaches in
learning multi-modal distributions. To address this problem, we propose Ranking
Loss based Knowledge Distillation (RLKD), which encourages the consistency of
the ranking of peak predictions between the teacher and student models. By
incorporating word-level ranking loss, we ensure excellent compatibility with
existing distillation objectives while fully leveraging the fine-grained
information between different categories in peaks of two predicted
distribution. Experimental results demonstrate that our method enables the
student model to better learn the multi-modal distributions of the teacher
model, leading to a significant performance improvement in various downstream
tasks.",2024-09-19,"Tianyu Peng, Jiajun Zhang",http://arxiv.org/pdf/2409.12545v2,cs.CL
Profiling Patient Transcript Using Large Language Model Reasoning Augmentation for Alzheimer's Disease Detection,"Alzheimer's disease (AD) stands as the predominant cause of dementia,
characterized by a gradual decline in speech and language capabilities. Recent
deep-learning advancements have facilitated automated AD detection through
spontaneous speech. However, common transcript-based detection methods directly
model text patterns in each utterance without a global view of the patient's
linguistic characteristics, resulting in limited discriminability and
interpretability. Despite the enhanced reasoning abilities of large language
models (LLMs), there remains a gap in fully harnessing the reasoning ability to
facilitate AD detection and model interpretation. Therefore, we propose a
patient-level transcript profiling framework leveraging LLM-based reasoning
augmentation to systematically elicit linguistic deficit attributes. The
summarized embeddings of the attributes are integrated into an Albert model for
AD detection. The framework achieves 8.51\% ACC and 8.34\% F1 improvements on
the ADReSS dataset compared to the baseline without reasoning augmentation. Our
further analysis shows the effectiveness of our identified linguistic deficit
attributes and the potential to use LLM for AD detection interpretation.",2024-09-19,"Chin-Po Chen, Jeng-Lin Li",http://arxiv.org/pdf/2409.12541v1,cs.CL
Should RAG Chatbots Forget Unimportant Conversations? Exploring Importance and Forgetting with Psychological Insights,"While Retrieval-Augmented Generation (RAG) has shown promise in enhancing
long-term conversations, the increasing memory load as conversations progress
degrades retrieval accuracy. Drawing on psychological insights, we propose
LUFY, a simple yet effective method that focuses on emotionally arousing
memories and retains less than 10% of the conversation. In the user experiment,
participants interacted with three types of RAG chatbots, each for 2 hours over
4 sessions, marking the most extensive assessment of a chatbot's long-term
capabilities to date -- more than four times longer than any existing
benchmark. The results demonstrate that prioritizing arousing memories while
forgetting the majority of the conversation significantly enhances user
experience. This study pushes the frontier of long-term conversations and
highlights the importance of forgetting unimportant parts of conversations.
Code and Dataset: https://github.com/ryuichi-sumida/LUFY",2024-09-19,"Ryuichi Sumida, Koji Inoue, Tatsuya Kawahara",http://arxiv.org/pdf/2409.12524v1,cs.CL
Exploring and Enhancing the Transfer of Distribution in Knowledge Distillation for Autoregressive Language Models,"Knowledge distillation (KD) is a technique that compresses large teacher
models by training smaller student models to mimic them. The success of KD in
auto-regressive language models mainly relies on Reverse KL for mode-seeking
and student-generated output (SGO) to combat exposure bias. Our theoretical
analyses and experimental validation reveal that while Reverse KL effectively
mimics certain features of the teacher distribution, it fails to capture most
of its behaviors. Conversely, SGO incurs higher computational costs and
presents challenges in optimization, particularly when the student model is
significantly smaller than the teacher model. These constraints are primarily
due to the immutable distribution of the teacher model, which fails to adjust
adaptively to models of varying sizes. We introduce Online Knowledge
Distillation (OKD), where the teacher network integrates small online modules
to concurrently train with the student model. This strategy abolishes the
necessity for on-policy sampling and merely requires minimal updates to the
parameters of the teacher's online module during training, thereby allowing
dynamic adaptation to the student's distribution to make distillation better.
Extensive results across multiple generation datasets show that OKD achieves or
exceeds the performance of leading methods in various model architectures and
sizes, reducing training time by up to fourfold.",2024-09-19,"Jun Rao, Xuebo Liu, Zepeng Lin, Liang Ding, Jing Li, Dacheng Tao, Min Zhang",http://arxiv.org/pdf/2409.12512v2,cs.CL
A Case Study of Web App Coding with OpenAI Reasoning Models,"This paper presents a case study of coding tasks by the latest reasoning
models of OpenAI, i.e. o1-preview and o1-mini, in comparison with other
frontier models. The o1 models deliver SOTA results for WebApp1K, a single-task
benchmark. To this end, we introduce WebApp1K-Duo, a harder benchmark doubling
number of tasks and test cases. The new benchmark causes the o1 model
performances to decline significantly, falling behind Claude 3.5. Moreover,
they consistently fail when confronted with atypical yet correct test cases, a
trap non-reasoning models occasionally avoid. We hypothesize that the
performance variability is due to instruction comprehension. Specifically, the
reasoning mechanism boosts performance when all expectations are captured,
meanwhile exacerbates errors when key expectations are missed, potentially
impacted by input lengths. As such, we argue that the coding success of
reasoning models hinges on the top-notch base model and SFT to ensure
meticulous adherence to instructions.",2024-09-19,Yi Cui,http://arxiv.org/pdf/2409.13773v1,cs.CL
"PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent","Propaganda plays a critical role in shaping public opinion and fueling
disinformation. While existing research primarily focuses on identifying
propaganda techniques, it lacks the ability to capture the broader motives and
the impacts of such content. To address these challenges, we introduce
propainsight, a conceptual framework grounded in foundational social science
research, which systematically dissects propaganda into techniques, arousal
appeals, and underlying intent. propainsight offers a more granular
understanding of how propaganda operates across different contexts.
Additionally, we present propagaze, a novel dataset that combines
human-annotated data with high-quality synthetic data generated through a
meticulously designed pipeline. Our experiments show that off-the-shelf LLMs
struggle with propaganda analysis, but training with propagaze significantly
improves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span
IoU in technique identification and 66.2% higher BertScore in appeal analysis
compared to 1-shot GPT-4-Turbo. Moreover, propagaze complements limited
human-annotated data in data-sparse and cross-domain scenarios, showing its
potential for comprehensive and generalizable propaganda analysis.",2024-09-19,"Jiateng Liu, Lin Ai, Zizhou Liu, Payam Karisani, Zheng Hui, May Fung, Preslav Nakov, Julia Hirschberg, Heng Ji",http://arxiv.org/pdf/2409.18997v2,cs.CL
LLMR: Knowledge Distillation with a Large Language Model-Induced Reward,"Large language models have become increasingly popular and demonstrated
remarkable performance in various natural language processing (NLP) tasks.
However, these models are typically computationally expensive and difficult to
be deployed in resource-constrained environments. In this paper, we propose
LLMR, a novel knowledge distillation (KD) method based on a reward function
induced from large language models. We conducted experiments on multiple
datasets in the dialogue generation and summarization tasks. Empirical results
demonstrate that our LLMR approach consistently outperforms traditional KD
methods in different tasks and datasets.",2024-09-19,"Dongheng Li, Yongchang Hao, Lili Mou",http://arxiv.org/pdf/2409.12500v1,cs.CL
CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling Acceleration in LLMs,"Large language models have achieved notable success across various domains,
yet efficient inference is still limited by the quadratic computation
complexity of the attention mechanism. The inference consists of prefilling and
decoding phases. Although several attempts have been made to accelerate
decoding, the inefficiency of the prefilling phase, especially for long-context
tasks, remains a challenge. In this paper, we observe a locality in query
criticality during the prefilling phase of long-context processing: adjacent
query tokens tend to focus on similar subsets of the past Key-Value (KV) cache.
Based on this observation, we propose CritiPrefill, a criticality-based
segment-wise prefilling method. This method partitions the input sequence's
queries and KV cache into segments and blocks, utilizing a segment-wise
algorithm to estimate the query criticality. By pruning non-critical
computations between query segments and cache blocks in the self-attention
mechanism, the prefilling process can be significantly accelerated. Extensive
evaluations on multiple long-context datasets show up to 2.7x speedup on
Llama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100
GPU, with minimal quality degradation.",2024-09-19,"Junlin Lv, Yuan Feng, Xike Xie, Xin Jia, Qirong Peng, Guiming Xie",http://arxiv.org/pdf/2409.12490v2,cs.CL
AutoMode-ASR: Learning to Select ASR Systems for Better Quality and Cost,"We present AutoMode-ASR, a novel framework that effectively integrates
multiple ASR systems to enhance the overall transcription quality while
optimizing cost. The idea is to train a decision model to select the optimal
ASR system for each segment based solely on the audio input before running the
systems. We achieve this by ensembling binary classifiers determining the
preference between two systems. These classifiers are equipped with various
features, such as audio embeddings, quality estimation, and signal properties.
Additionally, we demonstrate how using a quality estimator can further improve
performance with minimal cost increase. Experimental results show a relative
reduction in WER of 16.2%, a cost saving of 65%, and a speed improvement of
75%, compared to using a single-best model for all segments. Our framework is
compatible with commercial and open-source black-box ASR systems as it does not
require changes in model codes.",2024-09-19,"Ahmet Gündüz, Yunsu Kim, Kamer Ali Yuksel, Mohamed Al-Badrashiny, Thiago Castro Ferreira, Hassan Sawaf",http://arxiv.org/pdf/2409.12476v1,cs.CL
Familiarity-Aware Evidence Compression for Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) improves large language models (LMs) by
incorporating non-parametric knowledge through evidence retrieved from external
sources. However, it often struggles to cope with inconsistent and irrelevant
information that can distract the LM from its tasks, especially when multiple
evidence pieces are required. While compressing the retrieved evidence with a
compression model aims to address this issue, the compressed evidence may still
be unfamiliar to the target model used for downstream tasks, potentially
failing to utilize the evidence effectively. We propose FaviComp
(Familarity-Aware Evidence Compression), a novel training-free evidence
compression technique that makes retrieved evidence more familiar to the target
model, while seamlessly integrating parametric knowledge from the model.
Experimental results show that FaviComp consistently outperforms most recent
evidence compression baselines across multiple open-domain QA datasets,
improving accuracy by up to 28.1% while achieving high compression rates.
Additionally, we demonstrate the effective integration of both parametric and
non-parametric knowledge during evidence compression.",2024-09-19,"Dongwon Jung, Qin Liu, Tenghao Huang, Ben Zhou, Muhao Chen",http://arxiv.org/pdf/2409.12468v2,cs.CL
Unlocking Reasoning Potential in Large Langauge Models by Scaling Code-form Planning,"Despite the remarkable success of large language models (LLMs) on traditional
natural language processing tasks, their planning ability remains a critical
bottleneck in tackling complex multi-step reasoning tasks. Existing approaches
mainly rely on prompting or task-specific fine-tuning, often suffering from
poor robustness and cross-task generalization. To address the limitation, we
introduce CodePlan, a scalable framework that empowers LLMs to generate and
follow \textit{code-form plans} -- pseudocode that outlines high-level,
structured reasoning processes. By leveraging the structured and versatile
nature of code, CodePlan effectively captures the rich semantics and control
flows inherent to sophisticated reasoning tasks. Importantly, CodePlan allows
automatic extraction of code-form plans from massive, wide-ranging text corpora
without the need for curated, task-specific datasets. This enables it to scale
up efficiently and improve LLM's reasoning capabilities across diverse
scenarios. To train CodePlan, we construct a large-scale dataset of 2M examples
that integrate code-form plans with standard prompt-response pairs from
existing corpora. With minimal computation overhead during both training and
inference, CodePlan achieves a 25.1\% relative improvement compared with
directly generating responses, averaged across 13 challenging multi-step
reasoning benchmarks, spanning mathematical reasoning, symbolic reasoning,
instruction-following, multi-hop QA, and decision-making tasks. Further
analysis reveals CodePlan's increasing performance gains on more complex
reasoning tasks, as well as significant data efficiency thanks to its
generalization ability.",2024-09-19,"Jiaxin Wen, Jian Guan, Hongning Wang, Wei Wu, Minlie Huang",http://arxiv.org/pdf/2409.12452v2,cs.CL
Incremental and Data-Efficient Concept Formation to Support Masked Word Prediction,"This paper introduces Cobweb4L, a novel approach for efficient language model
learning that supports masked word prediction. The approach builds on Cobweb,
an incremental system that learns a hierarchy of probabilistic concepts. Each
concept stores the frequencies of words that appear in instances tagged with
that concept label. The system utilizes an attribute value representation to
encode words and their surrounding context into instances. Cobweb4L uses the
information theoretic variant of category utility and a new performance
mechanism that leverages multiple concepts to generate predictions. We
demonstrate that with these extensions it significantly outperforms prior
Cobweb performance mechanisms that use only a single node to generate
predictions. Further, we demonstrate that Cobweb4L learns rapidly and achieves
performance comparable to and even superior to Word2Vec. Next, we show that
Cobweb4L and Word2Vec outperform BERT in the same task with less training data.
Finally, we discuss future work to make our conclusions more robust and
inclusive.",2024-09-19,"Xin Lian, Nishant Baglodi, Christopher J. MacLellan",http://arxiv.org/pdf/2409.12440v1,cs.CL
Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data,"Despite recent advances in training and prompting strategies for Large
Language Models (LLMs), these models continue to face challenges with complex
logical reasoning tasks that involve long reasoning chains. In this work, we
explore the potential and limitations of using graph-based synthetic reasoning
data as training signals to enhance LLMs' reasoning capabilities. Our extensive
experiments, conducted on two established natural language reasoning tasks --
inductive reasoning and spatial reasoning -- demonstrate that supervised
fine-tuning (SFT) with synthetic graph-based reasoning data effectively
enhances LLMs' reasoning performance without compromising their effectiveness
on other standard evaluation benchmarks.",2024-09-19,"Jiaming Zhou, Abbas Ghaddar, Ge Zhang, Liheng Ma, Yaochen Hu, Soumyasundar Pal, Mark Coates, Bin Wang, Yingxue Zhang, Jianye Hao",http://arxiv.org/pdf/2409.12437v2,cs.CL
Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language Models,"We introduce a novel analysis that leverages linguistic minimal pairs to
probe the internal linguistic representations of Large Language Models (LLMs).
By measuring the similarity between LLM activation differences across minimal
pairs, we quantify the and gain insight into the linguistic knowledge captured
by LLMs. Our large-scale experiments, spanning 100+ LLMs and 150k minimal pairs
in three languages, reveal properties of linguistic similarity from four key
aspects: consistency across LLMs, relation to theoretical categorizations,
dependency to semantic context, and cross-lingual alignment of relevant
phenomena. Our findings suggest that 1) linguistic similarity is significantly
influenced by training data exposure, leading to higher cross-LLM agreement in
higher-resource languages. 2) Linguistic similarity strongly aligns with
fine-grained theoretical linguistic categories but weakly with broader ones. 3)
Linguistic similarity shows a weak correlation with semantic similarity,
showing its context-dependent nature. 4) LLMs exhibit limited cross-lingual
alignment in their understanding of relevant linguistic phenomena. This work
demonstrates the potential of minimal pairs as a window into the neural
representations of language in LLMs, shedding light on the relationship between
LLMs and linguistic theory. Codes and data are available at
https://github.com/ChenDelong1999/Linguistic-Similarity",2024-09-19,"Xinyu Zhou, Delong Chen, Samuel Cahyawijaya, Xufeng Duan, Zhenguang G. Cai",http://arxiv.org/pdf/2409.12435v2,cs.CL
Zero-to-Strong Generalization: Eliciting Strong Capabilities of Large Language Models Iteratively without Gold Labels,"Large Language Models (LLMs) have demonstrated remarkable performance through
supervised fine-tuning or in-context learning using gold labels. However, this
paradigm is limited by the availability of gold labels, while in certain
scenarios, LLMs may need to perform tasks that are too complex for humans to
provide such labels. To tackle this challenge, this study explores whether
solely utilizing unlabeled data can elicit strong model capabilities. We
propose a new paradigm termed zero-to-strong generalization. We iteratively
prompt LLMs to annotate unlabeled data and retain high-quality labels by
filtering. Surprisingly, we obverse that this iterative process gradually
unlocks LLMs' potential on downstream tasks. Our experiments on extensive
classification and reasoning tasks confirm the effectiveness of our proposed
framework. Our analysis indicates that this paradigm is effective for both
in-context learning and fine-tuning, and for various model sizes.",2024-09-19,"Chaoqun Liu, Qin Chao, Wenxuan Zhang, Xiaobao Wu, Boyang Li, Anh Tuan Luu, Lidong Bing",http://arxiv.org/pdf/2409.12425v1,cs.CL
From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal Reasoning with Large Language Models,"Cross-modal reasoning (CMR), the intricate process of synthesizing and
drawing inferences across divergent sensory modalities, is increasingly
recognized as a crucial capability in the progression toward more sophisticated
and anthropomorphic artificial intelligence systems. Large Language Models
(LLMs) represent a class of AI algorithms specifically engineered to parse,
produce, and engage with human language on an extensive scale. The recent trend
of deploying LLMs to tackle CMR tasks has marked a new mainstream of approaches
for enhancing their effectiveness. This survey offers a nuanced exposition of
current methodologies applied in CMR using LLMs, classifying these into a
detailed three-tiered taxonomy. Moreover, the survey delves into the principal
design strategies and operational techniques of prototypical models within this
domain. Additionally, it articulates the prevailing challenges associated with
the integration of LLMs in CMR and identifies prospective research directions.
To sum up, this survey endeavors to expedite progress within this burgeoning
field by endowing scholars with a holistic and detailed vista, showcasing the
vanguard of current research whilst pinpointing potential avenues for
advancement. An associated GitHub repository that collects the relevant papers
can be found at
https://github.com/ZuyiZhou/Awesome-Cross-modal-Reasoning-with-LLMs",2024-09-19,"Shengsheng Qian, Zuyi Zhou, Dizhan Xue, Bing Wang, Changsheng Xu",http://arxiv.org/pdf/2409.18996v1,cs.CL
Textualized Agent-Style Reasoning for Complex Tasks by Multiple Round LLM Generation,"Chain-of-thought prompting significantly boosts the reasoning ability of
large language models but still faces three issues: hallucination problem,
restricted interpretability, and uncontrollable generation. To address these
challenges, we present AgentCOT, a llm-based autonomous agent framework, which
can solve complex problems in an agent-style manner by multiple round LLM
generation. At each step, AgentCOT selects an action and executes it to yield
an intermediate result with supporting evidence. In addition, we integrate the
step's index into the reasoning process to form a graph structure for complex
inference logic. We introduce two new strategies to enhance the performance of
AgentCOT.We conduct extensive experiments to verify the effectiveness of our
method on six common benchmarks. Results exhibit that our method brings in
substantial improvements over current competitive approaches.",2024-09-19,"Chen Liang, Zhifan Feng, Zihe Liu, Wenbin Jiang, Jinan Xu, Yufeng Chen, Yong Wang",http://arxiv.org/pdf/2409.12411v1,cs.CL
Mutual Information-based Representations Disentanglement for Unaligned Multimodal Language Sequences,"The key challenge in unaligned multimodal language sequences lies in
effectively integrating information from various modalities to obtain a refined
multimodal joint representation. Recently, the disentangle and fuse methods
have achieved the promising performance by explicitly learning
modality-agnostic and modality-specific representations and then fusing them
into a multimodal joint representation. However, these methods often
independently learn modality-agnostic representations for each modality and
utilize orthogonal constraints to reduce linear correlations between
modality-agnostic and modality-specific representations, neglecting to
eliminate their nonlinear correlations. As a result, the obtained multimodal
joint representation usually suffers from information redundancy, leading to
overfitting and poor generalization of the models. In this paper, we propose a
Mutual Information-based Representations Disentanglement (MIRD) method for
unaligned multimodal language sequences, in which a novel disentanglement
framework is designed to jointly learn a single modality-agnostic
representation. In addition, the mutual information minimization constraint is
employed to ensure superior disentanglement of representations, thereby
eliminating information redundancy within the multimodal joint representation.
Furthermore, the challenge of estimating mutual information caused by the
limited labeled data is mitigated by introducing unlabeled data. Meanwhile, the
unlabeled data also help to characterize the underlying structure of multimodal
data, consequently further preventing overfitting and enhancing the performance
of the models. Experimental results on several widely used benchmark datasets
validate the effectiveness of our proposed approach.",2024-09-19,"Fan Qian, Jiqing Han, Jianchen Li, Yongjun He, Tieran Zheng, Guibin Zheng",http://arxiv.org/pdf/2409.12408v1,cs.CL
Preference Alignment Improves Language Model-Based TTS,"Recent advancements in text-to-speech (TTS) have shown that language model
(LM)-based systems offer competitive performance to their counterparts. Further
optimization can be achieved through preference alignment algorithms, which
adjust LMs to align with the preferences of reward models, enhancing the
desirability of the generated content. This study presents a thorough empirical
evaluation of how preference alignment algorithms, particularly Direct
Preference Optimization (DPO), enhance LM-based TTS. With a 1.15B parameter
LM-based TTS model, we demonstrate that preference alignment consistently
improves intelligibility, speaker similarity, and proxy subjective evaluation
scores, with the latter two metrics surpassing even human speech in certain
evaluations. We also show preference alignment is applicable to low-resource
scenarios and effectively generalized to out-of-domain applications.",2024-09-19,"Jinchuan Tian, Chunlei Zhang, Jiatong Shi, Hao Zhang, Jianwei Yu, Shinji Watanabe, Dong Yu",http://arxiv.org/pdf/2409.12403v1,cs.CL
Small Language Models are Equation Reasoners,"Chain-of-Thought (CoT) reasoning has enabled Large Language Model (LLM) to
achieve remarkable performance in various NLP tasks, including arithmetic
problem-solving. However, this success does not generalize to small language
model (sLM) like T5, due to their limited capacity and absence of emergent
abilities associated with larger models. Recent works to enhance sLM through
knowledge distillation have yielded some improvements but still face
significant limitations, particularly high ambiguity from the variability in
natural language expressions and substantial computational costs. In this
paper, we investigate why sLM perform poorly on arithmetic reasoning tasks and
hypothesize that natural language format variability introduces high ambiguity
for these smaller models. Based on this hypothesis, we conduct experiments with
equation-only format, which is a reasoning format that unifies arithmetic
reasoning previously expressed in natural language formats into mathematical
equations. Experiment results demonstrate that equation-only format effectively
boosts the arithmetic reasoning abilities of sLM, especially in very small
models like T5-Tiny.",2024-09-19,"Bumjun Kim, Kunha Lee, Juyeon Kim, Sangam Lee",http://arxiv.org/pdf/2409.12393v1,cs.CL
Channel-Aware Domain-Adaptive Generative Adversarial Network for Robust Speech Recognition,"While pre-trained automatic speech recognition (ASR) systems demonstrate
impressive performance on matched domains, their performance often degrades
when confronted with channel mismatch stemming from unseen recording
environments and conditions. To mitigate this issue, we propose a novel
channel-aware data simulation method for robust ASR training. Our method
harnesses the synergistic power of channel-extractive techniques and generative
adversarial networks (GANs). We first train a channel encoder capable of
extracting embeddings from arbitrary audio. On top of this, channel embeddings
are extracted using a minimal amount of target-domain data and used to guide a
GAN-based speech synthesizer. This synthesizer generates speech that faithfully
preserves the phonetic content of the input while mimicking the channel
characteristics of the target domain. We evaluate our method on the challenging
Hakka Across Taiwan (HAT) and Taiwanese Across Taiwan (TAT) corpora, achieving
relative character error rate (CER) reductions of 20.02% and 9.64%,
respectively, compared to the baselines. These results highlight the efficacy
of our channel-aware data simulation method for bridging the gap between
source- and target-domain acoustics.",2024-09-19,"Chien-Chun Wang, Li-Wei Chen, Cheng-Kang Chou, Hung-Shin Lee, Berlin Chen, Hsin-Min Wang",http://arxiv.org/pdf/2409.12386v2,cs.CL
Robust Audiovisual Speech Recognition Models with Mixture-of-Experts,"Visual signals can enhance audiovisual speech recognition accuracy by
providing additional contextual information. Given the complexity of visual
signals, an audiovisual speech recognition model requires robust generalization
capabilities across diverse video scenarios, presenting a significant
challenge. In this paper, we introduce EVA, leveraging the mixture-of-Experts
for audioVisual ASR to perform robust speech recognition for ``in-the-wild''
videos. Specifically, we first encode visual information into visual tokens
sequence and map them into speech space by a lightweight projection. Then, we
build EVA upon a robust pretrained speech recognition model, ensuring its
generalization ability. Moreover, to incorporate visual information
effectively, we inject visual information into the ASR model through a
mixture-of-experts module. Experiments show our model achieves state-of-the-art
results on three benchmarks, which demonstrates the generalization ability of
EVA across diverse video domains.",2024-09-19,"Yihan Wu, Yifan Peng, Yichen Lu, Xuankai Chang, Ruihua Song, Shinji Watanabe",http://arxiv.org/pdf/2409.12370v1,cs.CL
Measuring Sound Symbolism in Audio-visual Models,"Audio-visual pre-trained models have gained substantial attention recently
and demonstrated superior performance on various audio-visual tasks. This study
investigates whether pre-trained audio-visual models demonstrate non-arbitrary
associations between sounds and visual representations$\unicode{x2013}$known as
sound symbolism$\unicode{x2013}$which is also observed in humans. We developed
a specialized dataset with synthesized images and audio samples and assessed
these models using a non-parametric approach in a zero-shot setting. Our
findings reveal a significant correlation between the models' outputs and
established patterns of sound symbolism, particularly in models trained on
speech data. These results suggest that such models can capture sound-meaning
connections akin to human language processing, providing insights into both
cognitive architectures and machine learning strategies.",2024-09-18,"Wei-Cheng Tseng, Yi-Jen Shih, David Harwath, Raymond Mooney",http://arxiv.org/pdf/2409.12306v3,cs.CL
"RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models","Large language models (LLMs) have recently emerged as promising tools for
solving challenging robotic tasks, even in the presence of action and
observation uncertainties. Recent LLM-based decision-making methods (also
referred to as LLM-based agents), when paired with appropriate critics, have
demonstrated potential in solving complex, long-horizon tasks with relatively
few interactions. However, most existing LLM-based agents lack the ability to
retain and learn from past interactions - an essential trait of learning-based
robotic systems. We propose RAG-Modulo, a framework that enhances LLM-based
agents with a memory of past interactions and incorporates critics to evaluate
the agents' decisions. The memory component allows the agent to automatically
retrieve and incorporate relevant past experiences as in-context examples,
providing context-aware feedback for more informed decision-making. Further by
updating its memory, the agent improves its performance over time, thereby
exhibiting learning. Through experiments in the challenging BabyAI and AlfWorld
domains, we demonstrate significant improvements in task success rates and
efficiency, showing that the proposed RAG-Modulo framework outperforms
state-of-the-art baselines.",2024-09-18,"Abhinav Jain, Chris Jermaine, Vaibhav Unhelkar",http://arxiv.org/pdf/2409.12294v1,cs.CL
MedCodER: A Generative AI Assistant for Medical Coding,"Medical coding is essential for standardizing clinical data and communication
but is often time-consuming and prone to errors. Traditional Natural Language
Processing (NLP) methods struggle with automating coding due to the large label
space, lengthy text inputs, and the absence of supporting evidence annotations
that justify code selection. Recent advancements in Generative Artificial
Intelligence (AI) offer promising solutions to these challenges. In this work,
we introduce MedCodER, a Generative AI framework for automatic medical coding
that leverages extraction, retrieval, and re-ranking techniques as core
components. MedCodER achieves a micro-F1 score of 0.60 on International
Classification of Diseases (ICD) code prediction, significantly outperforming
state-of-the-art methods. Additionally, we present a new dataset containing
medical records annotated with disease diagnoses, ICD codes, and supporting
evidence texts (https://doi.org/10.5281/zenodo.13308316). Ablation tests
confirm that MedCodER's performance depends on the integration of each of its
aforementioned components, as performance declines when these components are
evaluated in isolation.",2024-09-18,"Krishanu Das Baksi, Elijah Soba, John J. Higgins, Ravi Saini, Jaden Wood, Jane Cook, Jack Scott, Nirmala Pudota, Tim Weninger, Edward Bowen, Sanmitra Bhattacharya",http://arxiv.org/pdf/2409.15368v1,cs.CL
Making Large Language Models into World Models with Precondition and Effect Knowledge,"World models, which encapsulate the dynamics of how actions affect
environments, are foundational to the functioning of intelligent agents. In
this work, we explore the potential of Large Language Models (LLMs) to operate
as world models. Although LLMs are not inherently designed to model real-world
dynamics, we show that they can be induced to perform two critical world model
functions: determining the applicability of an action based on a given world
state, and predicting the resulting world state upon action execution. This is
achieved by fine-tuning two separate LLMs-one for precondition prediction and
another for effect prediction-while leveraging synthetic data generation
techniques. Through human-participant studies, we validate that the
precondition and effect knowledge generated by our models aligns with human
understanding of world dynamics. We also analyze the extent to which the world
model trained on our synthetic data results in an inferred state space that
supports the creation of action chains, a necessary property for planning.",2024-09-18,"Kaige Xie, Ian Yang, John Gunerli, Mark Riedl",http://arxiv.org/pdf/2409.12278v2,cs.CL
Systematic Characterization of the Effectiveness of Alignment in Large Language Models for Categorical Decisions,"As large language models (LLMs) are deployed in high-stakes domains like
healthcare, understanding how well their decision-making aligns with human
preferences and values becomes crucial, especially when we recognize that there
is no single gold standard for these preferences. This paper applies a
systematic methodology for evaluating preference alignment in LLMs on
categorical decision-making with medical triage as a domain-specific use case.
It also measures how effectively an alignment procedure will change the
alignment of a specific model. Key to this methodology is a novel simple
measure, the Alignment Compliance Index (ACI), that quantifies how effectively
a LLM can be aligned to a given preference function or gold standard. Since the
ACI measures the effect rather than the process of alignment, it is applicable
to alignment methods beyond the in-context learning used in this study.
  Using a dataset of simulated patient pairs, three frontier LLMs (GPT4o,
Claude 3.5 Sonnet, and Gemini Advanced) were assessed on their ability to make
triage decisions consistent with an expert clinician's preferences. The models'
performance before and after alignment attempts was evaluated using various
prompting strategies. The results reveal significant variability in alignment
effectiveness across models and alignment approaches. Notably, models that
performed well, as measured by ACI, pre-alignment sometimes degraded
post-alignment, and small changes in the target preference function led to
large shifts in model rankings. The implicit ethical principles, as understood
by humans, underlying the LLMs' decisions were also explored through targeted
questioning.
  This study motivates the use of a practical set of methods and the ACI, in
the near term, to understand the correspondence between the variety of human
and LLM decision-making values in categorical decision-making such as triage.",2024-09-18,Isaac Kohane,http://arxiv.org/pdf/2409.18995v1,cs.CL
MQA-KEAL: Multi-hop Question Answering under Knowledge Editing for Arabic Language,"Large Language Models (LLMs) have demonstrated significant capabilities
across numerous application domains. A key challenge is to keep these models
updated with latest available information, which limits the true potential of
these models for the end-applications. Although, there have been numerous
attempts for LLMs Knowledge Editing (KE), i.e., to edit the LLMs prior
knowledge and in turn test it via Multi-hop Question Answering (MQA), yet so
far these studies are primarily focused on English language. To bridge this
gap, in this paper we propose: Multi-hop Questioning Answering under Knowledge
Editing for Arabic Language (MQA-KEAL). MQA-KEAL stores knowledge edits as
structured knowledge units in the external memory. In order to solve multi-hop
question, it first uses task-decomposition to decompose the question into
smaller sub-problems. Later for each sub-problem, it iteratively queries the
external memory and/or target LLM in order to generate the final response. In
addition, we also contribute MQUAKE-AR (Arabic translation of English benchmark
MQUAKE), as well as a new benchmark MQA-AEVAL for rigorous performance
evaluation of MQA under KE for Arabic language. Experimentation evaluation
reveals MQA-KEAL outperforms the baseline models by a significant margin.",2024-09-18,"Muhammad Asif Ali, Nawal Daftardar, Mutayyaba Waheed, Jianbin Qin, Di Wang",http://arxiv.org/pdf/2409.12257v1,cs.CL
Fine-Tuning a Time Series Foundation Model with Wasserstein Loss,"Inspired by recent advancements in large language models (LLMs) for Natural
Language Processing (NLP), there has been a surge in research focused on
developing foundational models for time series forecasting. One approach
involves training LLM architectures on tokenized time series data using
cross-entropy loss. Although this method has demonstrated promising results,
cross-entropy loss is primarily designed for classification tasks and does not
account for the distance between classes. To address this limitation, we
propose using the Wasserstein loss for such architectures. To validate our
approach, we fine-tuned a foundational time series model on $22$ zero-shot
datasets, comparing the performance of cross-entropy loss with that of
Wasserstein loss. Our results demonstrate that replacing cross-entropy loss
with Wasserstein loss significantly improves point estimation.",2024-09-18,Andrei Chernov,http://arxiv.org/pdf/2409.15367v2,cs.CL
Gender Representation and Bias in Indian Civil Service Mock Interviews,"This paper makes three key contributions. First, via a substantial corpus of
51,278 interview questions sourced from 888 YouTube videos of mock interviews
of Indian civil service candidates, we demonstrate stark gender bias in the
broad nature of questions asked to male and female candidates. Second, our
experiments with large language models show a strong presence of gender bias in
explanations provided by the LLMs on the gender inference task. Finally, we
present a novel dataset of 51,278 interview questions that can inform future
social science studies.",2024-09-18,"Somonnoy Banerjee, Sujan Dutta, Soumyajit Datta, Ashiqur R. KhudaBukhsh",http://arxiv.org/pdf/2409.12194v3,cs.CL
Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL
models that redefines the conventional predetermined-resolution approach in
visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,
which enables the model to dynamically process images of varying resolutions
into different numbers of visual tokens. This approach allows the model to
generate more efficient and accurate visual representations, closely aligning
with human perceptual processes. The model also integrates Multimodal Rotary
Position Embedding (M-RoPE), facilitating the effective fusion of positional
information across text, images, and videos. We employ a unified paradigm for
processing both images and videos, enhancing the model's visual perception
capabilities. To explore the potential of large multimodal models, Qwen2-VL
investigates the scaling laws for large vision-language models (LVLMs). By
scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the
amount of training data, the Qwen2-VL Series achieves highly competitive
performance. Notably, the Qwen2-VL-72B model achieves results comparable to
leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal
benchmarks, outperforming other generalist models. Code is available at
https://github.com/QwenLM/Qwen2-VL .",2024-09-18,"Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin",http://arxiv.org/pdf/2409.12191v2,cs.CL
ARTICLE: Annotator Reliability Through In-Context Learning,"Ensuring annotator quality in training and evaluation data is a key piece of
machine learning in NLP. Tasks such as sentiment analysis and offensive speech
detection are intrinsically subjective, creating a challenging scenario for
traditional quality assessment approaches because it is hard to distinguish
disagreement due to poor work from that due to differences of opinions between
sincere annotators. With the goal of increasing diverse perspectives in
annotation while ensuring consistency, we propose \texttt{ARTICLE}, an
in-context learning (ICL) framework to estimate annotation quality through
self-consistency. We evaluate this framework on two offensive speech datasets
using multiple LLMs and compare its performance with traditional methods. Our
findings indicate that \texttt{ARTICLE} can be used as a robust method for
identifying reliable annotators, hence improving data quality.",2024-09-18,"Sujan Dutta, Deepak Pandita, Tharindu Cyril Weerasooriya, Marcos Zampieri, Christopher M. Homan, Ashiqur R. KhudaBukhsh",http://arxiv.org/pdf/2409.12218v2,cs.CL
Qwen2.5-Coder Technical Report,"In this report, we introduce the Qwen2.5-Coder series, a significant upgrade
from its predecessor, CodeQwen1.5. This series includes six models:
Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model,
Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained
on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning,
scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder
demonstrates impressive code generation capabilities while retaining general
and math skills. These models have been evaluated on a wide range of
code-related tasks, achieving state-of-the-art (SOTA) performance across more
than 10 benchmarks, including code generation, completion, reasoning, and
repair, consistently outperforming larger models of the same model size. We
believe that the release of the Qwen2.5-Coder series will advance research in
code intelligence and, with its permissive licensing, support wider adoption by
developers in real-world applications.",2024-09-18,"Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",http://arxiv.org/pdf/2409.12186v3,cs.CL
To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning,"Chain-of-thought (CoT) via prompting is the de facto method for eliciting
reasoning capabilities from large language models (LLMs). But for what kinds of
tasks is this extra ``thinking'' really helpful? To analyze this, we conducted
a quantitative meta-analysis covering over 100 papers using CoT and ran our own
evaluations of 20 datasets across 14 models. Our results show that CoT gives
strong performance benefits primarily on tasks involving math or logic, with
much smaller gains on other types of tasks. On MMLU, directly generating the
answer without CoT leads to almost identical accuracy as CoT unless the
question or model's response contains an equals sign, indicating symbolic
operations and reasoning. Following this finding, we analyze the behavior of
CoT on these problems by separating planning and execution and comparing
against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic
execution, but it underperforms relative to using a symbolic solver. Our
results indicate that CoT can be applied selectively, maintaining performance
while saving inference costs. Furthermore, they suggest a need to move beyond
prompt-based CoT to new paradigms that better leverage intermediate computation
across the whole range of LLM applications.",2024-09-18,"Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett",http://arxiv.org/pdf/2409.12183v3,cs.CL
A Controlled Study on Long Context Extension and Generalization in LLMs,"Broad textual understanding and in-context learning require language models
that utilize full document contexts. Due to the implementation challenges
associated with directly training long-context models, many methods have been
proposed for extending models to handle long contexts. However, owing to
differences in data and model classes, it has been challenging to compare these
approaches, leading to uncertainty as to how to evaluate long-context
performance and whether it differs from standard evaluation. We implement a
controlled protocol for extension methods with a standardized evaluation,
utilizing consistent base models and extension data. Our study yields several
insights into long-context behavior. First, we reaffirm the critical role of
perplexity as a general-purpose performance indicator even in longer-context
tasks. Second, we find that current approximate attention methods
systematically underperform across long-context tasks. Finally, we confirm that
exact fine-tuning based methods are generally effective within the range of
their extension, whereas extrapolation remains challenging. All codebases,
models, and checkpoints will be made available open-source, promoting
transparency and facilitating further research in this critical area of AI
development.",2024-09-18,"Yi Lu, Jing Nathan Yan, Songlin Yang, Justin T. Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, Alexander M. Rush",http://arxiv.org/pdf/2409.12181v2,cs.CL
Finetuning Language Models to Emit Linguistic Expressions of Uncertainty,"Large language models (LLMs) are increasingly employed in information-seeking
and decision-making tasks. Despite their broad utility, LLMs tend to generate
information that conflicts with real-world facts, and their persuasive style
can make these inaccuracies appear confident and convincing. As a result,
end-users struggle to consistently align the confidence expressed by LLMs with
the accuracy of their predictions, often leading to either blind trust in all
outputs or a complete disregard for their reliability. In this work, we explore
supervised finetuning on uncertainty-augmented predictions as a method to
develop models that produce linguistic expressions of uncertainty.
Specifically, we measure the calibration of pre-trained models and then
fine-tune language models to generate calibrated linguistic expressions of
uncertainty. Through experiments on various question-answering datasets, we
demonstrate that LLMs are well-calibrated in assessing their predictions, and
supervised finetuning based on the model's own confidence leads to
well-calibrated expressions of uncertainty, particularly for single-claim
answers.",2024-09-18,"Arslan Chaudhry, Sridhar Thiagarajan, Dilan Gorur",http://arxiv.org/pdf/2409.12180v1,cs.CL
You Only Read Once (YORO): Learning to Internalize Database Knowledge for Text-to-SQL,"While significant progress has been made on the text-to-SQL task, recent
solutions repeatedly encode the same database schema for every question,
resulting in unnecessary high inference cost and often overlooking crucial
database knowledge. To address these issues, we propose You Only Read Once
(YORO), a novel paradigm that directly internalizes database knowledge into the
parametric knowledge of a text-to-SQL model during training and eliminates the
need for schema encoding during inference. YORO significantly reduces the input
token length by 66%-98%. Despite its shorter inputs, our empirical results
demonstrate YORO's competitive performances with traditional systems on three
benchmarks as well as its significant outperformance on large databases.
Furthermore, YORO excels in handling questions with challenging value
retrievals such as abbreviation.",2024-09-18,"Hideo Kobayashi, Wuwei Lan, Peng Shi, Shuaichen Chang, Jiang Guo, Henghui Zhu, Zhiguo Wang, Patrick Ng",http://arxiv.org/pdf/2409.12172v1,cs.CL
"MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning","Large Language Models' (LLM) reasoning can be improved using test-time
aggregation strategies, i.e., generating multiple samples and voting among
generated samples. While these improve performance, they often reach a
saturation point. Refinement offers an alternative by using LLM-generated
feedback to improve solution quality. However, refinement introduces 3 key
challenges: (1) Excessive refinement: Uniformly refining all instances can
over-correct and reduce the overall performance. (2) Inability to localize and
address errors: LLMs have a limited ability to self-correct and struggle to
identify and correct their own mistakes. (3) Insufficient refinement: Deciding
how many iterations of refinement are needed is non-trivial, and stopping too
soon could leave errors unaddressed. To tackle these issues, we propose
MAgICoRe, which avoids excessive refinement by categorizing problem difficulty
as easy or hard, solving easy problems with coarse-grained aggregation and hard
ones with fine-grained and iterative multi-agent refinement. To improve error
localization, we incorporate external step-wise reward model (RM) scores.
Moreover, to ensure effective refinement, we employ a multi-agent loop with
three agents: Solver, Reviewer (which generates targeted feedback based on
step-wise RM scores), and the Refiner (which incorporates feedback). To ensure
sufficient refinement, we re-evaluate updated solutions, iteratively initiating
further rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5
and show its effectiveness across 5 math datasets. Even one iteration of
MAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by
4.0% while using less than half the samples. Unlike iterative refinement with
baselines, MAgICoRe continues to improve with more iterations. Finally, our
ablations highlight the importance of MAgICoRe's RMs and multi-agent
communication.",2024-09-18,"Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, Mohit Bansal",http://arxiv.org/pdf/2409.12147v1,cs.CL
GRIN: GRadient-INformed MoE,"Mixture-of-Experts (MoE) models scale more effectively than dense models due
to sparse computation through expert routing, selectively activating only a
small subset of expert modules. However, sparse computation challenges
traditional training practices, as discrete expert routing hinders standard
backpropagation and thus gradient-based optimization, which are the cornerstone
of deep learning. To better pursue the scaling power of MoE, we introduce GRIN
(GRadient-INformed MoE training), which incorporates sparse gradient estimation
for expert routing and configures model parallelism to avoid token dropping.
Applying GRIN to autoregressive language modeling, we develop a top-2
16$\times$3.8B MoE model. Our model, with only 6.6B activated parameters,
outperforms a 7B dense model and matches the performance of a 14B dense model
trained on the same data. Extensive evaluations across diverse tasks
demonstrate the potential of GRIN to significantly enhance MoE efficacy,
achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.",2024-09-18,"Liyuan Liu, Young Jin Kim, Shuohang Wang, Chen Liang, Yelong Shen, Hao Cheng, Xiaodong Liu, Masahiro Tanaka, Xiaoxia Wu, Wenxiang Hu, Vishrav Chaudhary, Zeqi Lin, Chenruidong Zhang, Jilong Xue, Hany Awadalla, Jianfeng Gao, Weizhu Chen",http://arxiv.org/pdf/2409.12136v1,cs.CL
BERT-VBD: Vietnamese Multi-Document Summarization Framework,"In tackling the challenge of Multi-Document Summarization (MDS), numerous
methods have been proposed, spanning both extractive and abstractive
summarization techniques. However, each approach has its own limitations,
making it less effective to rely solely on either one. An emerging and
promising strategy involves a synergistic fusion of extractive and abstractive
summarization methods. Despite the plethora of studies in this domain, research
on the combined methodology remains scarce, particularly in the context of
Vietnamese language processing. This paper presents a novel Vietnamese MDS
framework leveraging a two-component pipeline architecture that integrates
extractive and abstractive techniques. The first component employs an
extractive approach to identify key sentences within each document. This is
achieved by a modification of the pre-trained BERT network, which derives
semantically meaningful phrase embeddings using siamese and triplet network
structures. The second component utilizes the VBD-LLaMA2-7B-50b model for
abstractive summarization, ultimately generating the final summary document.
Our proposed framework demonstrates a positive performance, attaining ROUGE-2
scores of 39.6% on the VN-MDS dataset and outperforming the state-of-the-art
baselines.",2024-09-18,"Tuan-Cuong Vuong, Trang Mai Xuan, Thien Van Luong",http://arxiv.org/pdf/2409.12134v1,cs.CL
BodyShapeGPT: SMPL Body Shape Manipulation with LLMs,"Generative AI models provide a wide range of tools capable of performing
complex tasks in a fraction of the time it would take a human. Among these,
Large Language Models (LLMs) stand out for their ability to generate diverse
texts, from literary narratives to specialized responses in different fields of
knowledge. This paper explores the use of fine-tuned LLMs to identify physical
descriptions of people, and subsequently create accurate representations of
avatars using the SMPL-X model by inferring shape parameters. We demonstrate
that LLMs can be trained to understand and manipulate the shape space of SMPL,
allowing the control of 3D human shapes through natural language. This approach
promises to improve human-machine interaction and opens new avenues for
customization and simulation in virtual environments.",2024-09-18,"Baldomero R. Árbol, Dan Casas",http://arxiv.org/pdf/2410.03556v1,cs.CL
Linguini: A benchmark for language-agnostic linguistic reasoning,"We propose a new benchmark to measure a language model's linguistic reasoning
skills without relying on pre-existing language-specific knowledge. The test
covers 894 questions grouped in 160 problems across 75 (mostly) extremely
low-resource languages, extracted from the International Linguistic Olympiad
corpus. To attain high accuracy on this benchmark, models don't need previous
knowledge of the tested language, as all the information needed to solve the
linguistic puzzle is presented in the context. We find that, while all analyzed
models rank below 25% accuracy, there is a significant gap between open and
closed models, with the best-performing proprietary model at 24.05% and the
best-performing open model at 8.84%.",2024-09-18,"Eduardo Sánchez, Belen Alastruey, Christophe Ropers, Pontus Stenetorp, Mikel Artetxe, Marta R. Costa-jussà",http://arxiv.org/pdf/2409.12126v1,cs.CL
Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement,"In this report, we present a series of math-specific large language models:
Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the
Qwen2.5 series lies in integrating the philosophy of self-improvement
throughout the entire pipeline, from pre-training and post-training to
inference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized
to generate large-scale, high-quality mathematical data. (2) In the
post-training phase, we develop a reward model (RM) by conducting massive
sampling from Qwen2-Math-Instruct. This RM is then applied to the iterative
evolution of data in supervised fine-tuning (SFT). With a stronger SFT model,
it's possible to iteratively train and update the RM, which in turn guides the
next round of SFT data iteration. On the final SFT model, we employ the
ultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.
(3) Furthermore, during the inference stage, the RM is used to guide sampling,
optimizing the model's performance.
  Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced
mathematical reasoning capabilities, including Chain-of-Thought (CoT) and
Tool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics
datasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and
AIME24, covering a range of difficulties from grade school level to math
competition problems.",2024-09-18,"An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, Zhenru Zhang",http://arxiv.org/pdf/2409.12122v1,cs.CL
Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference,"Large language models (LLMs) have significantly advanced audio processing
through audio codecs that convert audio into discrete tokens, enabling the
application of language modeling techniques to audio data. However, audio
codecs often operate at high frame rates, resulting in slow training and
inference, especially for autoregressive models. To address this challenge, we
present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that
leverages finite scalar quantization and adversarial training with large speech
language models to achieve high-quality audio compression with a 1.89 kbps
bitrate and 21.5 frames per second. We demonstrate that our novel codec can
make the inference of LLM-based text-to-speech models around three times faster
while improving intelligibility and producing quality comparable to previous
models.",2024-09-18,"Edresson Casanova, Ryan Langman, Paarth Neekhara, Shehzeen Hussain, Jason Li, Subhankar Ghosh, Ante Jukić, Sang-gil Lee",http://arxiv.org/pdf/2409.12117v1,cs.CL
Measuring Human and AI Values Based on Generative Psychometrics with Large Language Models,"Human values and their measurement are long-standing interdisciplinary
inquiry. Recent advances in AI have sparked renewed interest in this area, with
large language models (LLMs) emerging as both tools and subjects of value
measurement. This work introduces Generative Psychometrics for Values (GPV), an
LLM-based, data-driven value measurement paradigm, theoretically grounded in
text-revealed selective perceptions. The core idea is to dynamically parse
unstructured texts into perceptions akin to static stimuli in traditional
psychometrics, measure the value orientations they reveal, and aggregate the
results. Applying GPV to human-authored blogs, we demonstrate its stability,
validity, and superiority over prior psychological tools. Then, extending GPV
to LLM value measurement, we advance the current art with 1) a psychometric
methodology that measures LLM values based on their scalable and free-form
outputs, enabling context-specific measurement; 2) a comparative analysis of
measurement paradigms, indicating response biases of prior methods; and 3) an
attempt to bridge LLM values and their safety, revealing the predictive power
of different value systems and the impacts of various values on LLM safety.
Through interdisciplinary efforts, we aim to leverage AI for next-generation
psychometrics and psychometrics for value-aligned AI.",2024-09-18,"Haoran Ye, Yuhang Xie, Yuanyi Ren, Hanjun Fang, Xin Zhang, Guojie Song",http://arxiv.org/pdf/2409.12106v3,cs.CL
Skill matching at scale: freelancer-project alignment for efficient multilingual candidate retrieval,"Finding the perfect match between a job proposal and a set of freelancers is
not an easy task to perform at scale, especially in multiple languages. In this
paper, we propose a novel neural retriever architecture that tackles this
problem in a multilingual setting. Our method encodes project descriptions and
freelancer profiles by leveraging pre-trained multilingual language models. The
latter are used as backbone for a custom transformer architecture that aims to
keep the structure of the profiles and project. This model is trained with a
contrastive loss on historical data. Thanks to several experiments, we show
that this approach effectively captures skill matching similarity and
facilitates efficient matching, outperforming traditional methods.",2024-09-18,"Warren Jouanneau, Marc Palyart, Emma Jouffroy",http://arxiv.org/pdf/2409.12097v2,cs.CL
VERA: Validation and Enhancement for Retrieval Augmented systems,"Large language models (LLMs) exhibit remarkable capabilities but often
produce inaccurate responses, as they rely solely on their embedded knowledge.
Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating an external
information retrieval system, supplying additional context along with the query
to mitigate inaccuracies for a particular context. However, accuracy issues
still remain, as the model may rely on irrelevant documents or extrapolate
incorrectly from its training knowledge. To assess and improve the performance
of both the retrieval system and the LLM in a RAG framework, we propose
\textbf{VERA} (\textbf{V}alidation and \textbf{E}nhancement for
\textbf{R}etrieval \textbf{A}ugmented systems), a system designed to: 1)
Evaluate and enhance the retrieved context before response generation, and 2)
Evaluate and refine the LLM-generated response to ensure precision and minimize
errors. VERA employs an evaluator-cum-enhancer LLM that first checks if
external retrieval is necessary, evaluates the relevance and redundancy of the
retrieved context, and refines it to eliminate non-essential information.
Post-response generation, VERA splits the response into atomic statements,
assesses their relevance to the query, and ensures adherence to the context.
Our experiments demonstrate VERA's remarkable efficacy not only in improving
the performance of smaller open-source models, but also larger state-of-the art
models. These enhancements underscore VERA's potential to produce accurate and
relevant responses, advancing the state-of-the-art in retrieval-augmented
language modeling. VERA's robust methodology, combining multiple evaluation and
refinement steps, effectively mitigates hallucinations and improves retrieval
and response processes, making it a valuable tool for applications demanding
high accuracy and reliability in information generation. .",2024-09-18,"Nitin Aravind Birur, Tanay Baswa, Divyanshu Kumar, Jatan Loya, Sahil Agarwal, Prashanth Harshangi",http://arxiv.org/pdf/2409.15364v1,cs.CL
PARAPHRASUS : A Comprehensive Benchmark for Evaluating Paraphrase Detection Models,"The task of determining whether two texts are paraphrases has long been a
challenge in NLP. However, the prevailing notion of paraphrase is often quite
simplistic, offering only a limited view of the vast spectrum of paraphrase
phenomena. Indeed, we find that evaluating models in a paraphrase dataset can
leave uncertainty about their true semantic understanding. To alleviate this,
we create PARAPHRASUS, a benchmark designed for multi-dimensional assessment,
benchmarking and selection of paraphrase detection models. We find that
paraphrase detection models under our fine-grained evaluation lens exhibit
trade-offs that cannot be captured through a single classification dataset.
Furthermore, PARAPHRASUS allows prompt calibration for different use cases,
tailoring LLM models to specific strictness levels. PARAPHRASUS includes 3
challenges spanning over 10 datasets, including 8 repurposed and 2 newly
annotated; we release it along with a benchmarking library at
https://github.com/impresso/paraphrasus",2024-09-18,"Andrianos Michail, Simon Clematide, Juri Opitz",http://arxiv.org/pdf/2409.12060v2,cs.CL
"MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning","Large Language Model can reasonably understand and generate human expressions
but may lack of thorough thinking and reasoning mechanisms. Recently there have
been several studies which enhance the thinking ability of language models but
most of them are not data-driven or training-based. In this paper, we are
motivated by the cognitive mechanism in the natural world, and design a novel
model architecture called TaS which allows it to first consider the thoughts
and then express the response based upon the query. We design several pipelines
to annotate or generate the thought contents from prompt-response samples, then
add language heads in a middle layer which behaves as the thinking layer. We
train the language model by the thoughts-augmented data and successfully let
the thinking layer automatically generate reasonable thoughts and finally
output more reasonable responses. Both qualitative examples and quantitative
results validate the effectiveness and performance of TaS. Our code is
available at https://anonymous.4open.science/r/TadE.",2024-09-18,"Ningyuan Xi, Xiaoyu Wang, Yetao Wu, Teng Chen, Qingqing Gu, Yue Zhao, Jinxian Qu, Zhonglin Jiang, Yong Chen, Luo Ji",http://arxiv.org/pdf/2409.12059v4,cs.CL
Using Large Language Models to Generate Clinical Trial Tables and Figures,"Tables, figures, and listings (TFLs) are essential tools for summarizing
clinical trial data. Creation of TFLs for reporting activities is often a
time-consuming task encountered routinely during the execution of clinical
trials. This study explored the use of large language models (LLMs) to automate
the generation of TFLs through prompt engineering and few-shot transfer
learning. Using public clinical trial data in ADaM format, our results
demonstrated that LLMs can efficiently generate TFLs with prompt instructions,
showcasing their potential in this domain. Furthermore, we developed a
conservational agent named Clinical Trial TFL Generation Agent: An app that
matches user queries to predefined prompts that produce customized programs to
generate specific predefined TFLs.",2024-09-18,"Yumeng Yang, Peter Krusche, Kristyn Pantoja, Cheng Shi, Ethan Ludmir, Kirk Roberts, Gen Zhu",http://arxiv.org/pdf/2409.12046v2,cs.CL
ASR Benchmarking: Need for a More Representative Conversational Dataset,"Automatic Speech Recognition (ASR) systems have achieved remarkable
performance on widely used benchmarks such as LibriSpeech and Fleurs. However,
these benchmarks do not adequately reflect the complexities of real-world
conversational environments, where speech is often unstructured and contains
disfluencies such as pauses, interruptions, and diverse accents. In this study,
we introduce a multilingual conversational dataset, derived from TalkBank,
consisting of unstructured phone conversation between adults. Our results show
a significant performance drop across various state-of-the-art ASR models when
tested in conversational settings. Furthermore, we observe a correlation
between Word Error Rate and the presence of speech disfluencies, highlighting
the critical need for more realistic, conversational ASR benchmarks.",2024-09-18,"Gaurav Maheshwari, Dmitry Ivanov, Théo Johannet, Kevin El Haddad",http://arxiv.org/pdf/2409.12042v1,cs.CL
Sampling Latent Material-Property Information From LLM-Derived Embedding Representations,"Vector embeddings derived from large language models (LLMs) show promise in
capturing latent information from the literature. Interestingly, these can be
integrated into material embeddings, potentially useful for data-driven
predictions of materials properties. We investigate the extent to which
LLM-derived vectors capture the desired information and their potential to
provide insights into material properties without additional training. Our
findings indicate that, although LLMs can be used to generate representations
reflecting certain property information, extracting the embeddings requires
identifying the optimal contextual clues and appropriate comparators. Despite
this restriction, it appears that LLMs still have the potential to be useful in
generating meaningful materials-science representations.",2024-09-18,"Luke P. J. Gilligan, Matteo Cobelli, Hasan M. Sayeed, Taylor D. Sparks, Stefano Sanvito",http://arxiv.org/pdf/2409.11971v1,cs.CL
Efficacy of Synthetic Data as a Benchmark,"Large language models (LLMs) have enabled a range of applications in
zero-shot and few-shot learning settings, including the generation of synthetic
datasets for training and testing. However, to reliably use these synthetic
datasets, it is essential to understand how representative they are of
real-world data. We investigate this by assessing the effectiveness of
generating synthetic data through LLM and using it as a benchmark for various
NLP tasks. Our experiments across six datasets, and three different tasks, show
that while synthetic data can effectively capture performance of various
methods for simpler tasks, such as intent classification, it falls short for
more complex tasks like named entity recognition. Additionally, we propose a
new metric called the bias factor, which evaluates the biases introduced when
the same LLM is used to both generate benchmarking data and to perform the
tasks. We find that smaller LLMs exhibit biases towards their own generated
data, whereas larger models do not. Overall, our findings suggest that the
effectiveness of synthetic data as a benchmark varies depending on the task,
and that practitioners should rely on data generated from multiple larger
models whenever possible.",2024-09-18,"Gaurav Maheshwari, Dmitry Ivanov, Kevin El Haddad",http://arxiv.org/pdf/2409.11968v1,cs.CL
"LLMs in Education: Novel Perspectives, Challenges, and Opportunities","The role of large language models (LLMs) in education is an increasing area
of interest today, considering the new opportunities they offer for teaching,
learning, and assessment. This cutting-edge tutorial provides an overview of
the educational applications of NLP and the impact that the recent advances in
LLMs have had on this field. We will discuss the key challenges and
opportunities presented by LLMs, grounding them in the context of four major
educational applications: reading, writing, and speaking skills, and
intelligent tutoring systems (ITS). This COLING 2025 tutorial is designed for
researchers and practitioners interested in the educational applications of NLP
and the role LLMs have to play in this area. It is the first of its kind to
address this timely topic.",2024-09-18,"Bashar Alhafni, Sowmya Vajjala, Stefano Bannò, Kaushal Kumar Maurya, Ekaterina Kochmar",http://arxiv.org/pdf/2409.11917v1,cs.CL
LLMs + Persona-Plug = Personalized LLMs,"Personalization plays a critical role in numerous language tasks and
applications, since users with the same requirements may prefer diverse outputs
based on their individual interests. This has led to the development of various
personalized approaches aimed at adapting large language models (LLMs) to
generate customized outputs aligned with user preferences. Some of them involve
fine-tuning a unique personalized LLM for each user, which is too expensive for
widespread application. Alternative approaches introduce personalization
information in a plug-and-play manner by retrieving the user's relevant
historical texts as demonstrations. However, this retrieval-based strategy may
break the continuity of the user history and fail to capture the user's overall
styles and patterns, hence leading to sub-optimal performance. To address these
challenges, we propose a novel personalized LLM model, \ours{}. It constructs a
user-specific embedding for each individual by modeling all her historical
contexts through a lightweight plug-in user embedder module. By attaching this
embedding to the task input, LLMs can better understand and capture user habits
and preferences, thereby producing more personalized outputs without tuning
their own parameters. Extensive experiments on various tasks in the language
model personalization (LaMP) benchmark demonstrate that the proposed model
significantly outperforms existing personalized LLM approaches.",2024-09-18,"Jiongnan Liu, Yutao Zhu, Shuting Wang, Xiaochi Wei, Erxue Min, Yu Lu, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou",http://arxiv.org/pdf/2409.11901v1,cs.CL
DocMamba: Efficient Document Pre-training with State Space Model,"In recent years, visually-rich document understanding has attracted
increasing attention. Transformer-based pre-trained models have become the
mainstream approach, yielding significant performance gains in this field.
However, the self-attention mechanism's quadratic computational complexity
hinders their efficiency and ability to process long documents. In this paper,
we present DocMamba, a novel framework based on the state space model. It is
designed to reduce computational complexity to linear while preserving global
modeling capabilities. To further enhance its effectiveness in document
processing, we introduce the Segment-First Bidirectional Scan (SFBS) to capture
contiguous semantic information. Experimental results demonstrate that DocMamba
achieves new state-of-the-art results on downstream datasets such as FUNSD,
CORD, and SORIE, while significantly improving speed and reducing memory usage.
Notably, experiments on the HRDoc confirm DocMamba's potential for length
extrapolation.",2024-09-18,"Pengfei Hu, Zhenrong Zhang, Jiefeng Ma, Shuhang Liu, Jun Du, Jianshu Zhang",http://arxiv.org/pdf/2409.11887v2,cs.CL
"Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for Large-Scale Product Retrieval Evaluation","Evaluating production-level retrieval systems at scale is a crucial yet
challenging task due to the limited availability of a large pool of
well-trained human annotators. Large Language Models (LLMs) have the potential
to address this scaling issue and offer a viable alternative to humans for the
bulk of annotation tasks. In this paper, we propose a framework for assessing
the product search engines in a large-scale e-commerce setting, leveraging
Multimodal LLMs for (i) generating tailored annotation guidelines for
individual queries, and (ii) conducting the subsequent annotation task. Our
method, validated through deployment on a large e-commerce platform,
demonstrates comparable quality to human annotations, significantly reduces
time and cost, facilitates rapid problem discovery, and provides an effective
solution for production-level quality control at scale.",2024-09-18,"Kasra Hosseini, Thomas Kober, Josip Krapac, Roland Vollgraf, Weiwei Cheng, Ana Peleteiro Ramallo",http://arxiv.org/pdf/2409.11860v1,cs.CL
Local Explanations and Self-Explanations for Assessing Faithfulness in black-box LLMs,"This paper introduces a novel task to assess the faithfulness of large
language models (LLMs) using local perturbations and self-explanations. Many
LLMs often require additional context to answer certain questions correctly.
For this purpose, we propose a new efficient alternative explainability
technique, inspired by the commonly used leave-one-out approach. Using this
approach, we identify the sufficient and necessary parts for the LLM to
generate correct answers, serving as explanations. We propose a metric for
assessing faithfulness that compares these crucial parts with the
self-explanations of the model. Using the Natural Questions dataset, we
validate our approach, demonstrating its effectiveness in explaining model
decisions and assessing faithfulness.",2024-09-18,"Christos Fragkathoulas, Odysseas S. Chlapanis",http://arxiv.org/pdf/2409.13764v1,cs.CL
TaCIE: Enhancing Instruction Comprehension in Large Language Models through Task-Centred Instruction Evolution,"Large Language Models (LLMs) require precise alignment with complex
instructions to optimize their performance in real-world applications. As the
demand for refined instruction tuning data increases, traditional methods that
evolve simple seed instructions often struggle to effectively enhance
complexity or manage difficulty scaling across various domains. Our innovative
approach, Task-Centered Instruction Evolution (TaCIE), addresses these
shortcomings by redefining instruction evolution from merely evolving seed
instructions to a more dynamic and comprehensive combination of elements. TaCIE
starts by deconstructing complex instructions into their fundamental
components. It then generates and integrates new elements with the original
ones, reassembling them into more sophisticated instructions that progressively
increase in difficulty, diversity, and complexity. Applied across multiple
domains, LLMs fine-tuned with these evolved instructions have substantially
outperformed those tuned with conventional methods, marking a significant
advancement in instruction-based model fine-tuning.",2024-09-18,"Jiuding Yang, Shengyao Lu, Weidong Guo, Xiangyang Li, Kaitong Yang, Yu Xu, Di Niu",http://arxiv.org/pdf/2410.02795v1,cs.CL
MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts,"Large Language Models (LLMs) can memorize sensitive information, raising
concerns about potential misuse. LLM Unlearning, a post-hoc approach to remove
this information from trained LLMs, offers a promising solution to mitigate
these risks. However, previous practices face three key challenges: 1. Utility:
successful unlearning often causes catastrophic collapse on unrelated tasks. 2.
Efficiency: many methods either involve adding similarly sized models, which
slows down unlearning or inference, or require retain data that are difficult
to obtain. 3. Robustness: even effective methods may still leak data via
extraction techniques. To address these challenges, we propose MEOW, a simple
yet effective gradient descent-based unlearning method. Specifically, we use an
offline LLM to generate a set of inverted facts. Then, we design a new metric,
MEMO, to quantify memorization in LLMs. Finally, based on the signals provided
by MEMO, we select the most appropriate set of inverted facts and finetune the
model based on them. We evaluate MEOW on the commonly used unlearn benchmark,
ToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks.
Results demonstrate significant improvement of MEOW in forget quality without
substantial loss in model utility. Meanwhile, MEOW does not exhibit significant
degradation in NLU or NLG capabilities, and there is even a slight improvement
in NLU performance.",2024-09-18,"Tianle Gu, Kexin Huang, Ruilin Luo, Yuanqi Yao, Yujiu Yang, Yan Teng, Yingchun Wang",http://arxiv.org/pdf/2409.11844v1,cs.CL
Extract-and-Abstract: Unifying Extractive and Abstractive Summarization within Single Encoder-Decoder Framework,"Extract-then-Abstract is a naturally coherent paradigm to conduct abstractive
summarization with the help of salient information identified by the extractive
model. Previous works that adopt this paradigm train the extractor and
abstractor separately and introduce extra parameters to highlight the extracted
salients to the abstractor, which results in error accumulation and additional
training costs. In this paper, we first introduce a parameter-free highlight
method into the encoder-decoder framework: replacing the encoder attention mask
with a saliency mask in the cross-attention module to force the decoder to
focus only on salient parts of the input. A preliminary analysis compares
different highlight methods, demonstrating the effectiveness of our saliency
mask. We further propose the novel extract-and-abstract paradigm, ExtAbs, which
jointly and seamlessly performs Extractive and Abstractive summarization tasks
within single encoder-decoder model to reduce error accumulation. In ExtAbs,
the vanilla encoder is augmented to extract salients, and the vanilla decoder
is modified with the proposed saliency mask to generate summaries. Built upon
BART and PEGASUS, experiments on three datasets show that ExtAbs can achieve
superior performance than baselines on the extractive task and performs
comparable, or even better than the vanilla models on the abstractive task.",2024-09-18,"Yuping Wu, Hao Li, Hongbo Zhu, Goran Nenadic, Xiao-Jun Zeng",http://arxiv.org/pdf/2409.11827v1,cs.CL
The Factuality of Large Language Models in the Legal Domain,"This paper investigates the factuality of large language models (LLMs) as
knowledge bases in the legal domain, in a realistic usage scenario: we allow
for acceptable variations in the answer, and let the model abstain from
answering when uncertain. First, we design a dataset of diverse factual
questions about case law and legislation. We then use the dataset to evaluate
several LLMs under different evaluation methods, including exact, alias, and
fuzzy matching. Our results show that the performance improves significantly
under the alias and fuzzy matching methods. Further, we explore the impact of
abstaining and in-context examples, finding that both strategies enhance
precision. Finally, we demonstrate that additional pre-training on legal
documents, as seen with SaulLM, further improves factual precision from 63% to
81%.",2024-09-18,"Rajaa El Hamdani, Thomas Bonald, Fragkiskos Malliaros, Nils Holzenberger, Fabian Suchanek",http://arxiv.org/pdf/2409.11798v1,cs.CL
Development and bilingual evaluation of Japanese medical large language model within reasonably low computational resources,"The recent success of large language models (LLMs) and the scaling law has
led to a widespread adoption of larger models. Particularly in the healthcare
industry, there is an increasing demand for locally operated LLMs due to
security concerns. However, the majority of high quality open-source LLMs have
a size of 70B parameters, imposing significant financial burdens on users for
GPU preparation and operation. To overcome these issues, we present a medical
adaptation based on the recent 7B models, which enables the operation in low
computational resources. We compare the performance on medical
question-answering benchmarks in two languages (Japanese and English),
demonstrating that its scores reach parity with or surpass those of currently
existing medical LLMs that are ten times larger. We find that fine-tuning an
English-centric base model on Japanese medical dataset improves the score in
both language, supporting the effect of cross-lingual knowledge transfer. We
hope that this study will alleviate financial challenges, serving as a stepping
stone for clinical institutions to practically utilize LLMs locally. Our
evaluation code is available at
https://github.com/stardust-coder/japanese-lm-med-harness.",2024-09-18,Issey Sukeda,http://arxiv.org/pdf/2409.11783v2,cs.CL
Multitask Mayhem: Unveiling and Mitigating Safety Gaps in LLMs Fine-tuning,"Recent breakthroughs in Large Language Models (LLMs) have led to their
adoption across a wide range of tasks, ranging from code generation to machine
translation and sentiment analysis, etc. Red teaming/Safety alignment efforts
show that fine-tuning models on benign (non-harmful) data could compromise
safety. However, it remains unclear to what extent this phenomenon is
influenced by different variables, including fine-tuning task, model
calibrations, etc. This paper explores the task-wise safety degradation due to
fine-tuning on downstream tasks such as summarization, code generation,
translation, and classification across various calibration. Our results reveal
that: 1) Fine-tuning LLMs for code generation and translation leads to the
highest degradation in safety guardrails. 2) LLMs generally have weaker
guardrails for translation and classification, with 73-92% of harmful prompts
answered, across baseline and other calibrations, falling into one of two
concern categories. 3) Current solutions, including guards and safety tuning
datasets, lack cross-task robustness. To address these issues, we developed a
new multitask safety dataset effectively reducing attack success rates across a
range of tasks without compromising the model's overall helpfulness. Our work
underscores the need for generalized alignment measures to ensure safer and
more robust models.",2024-09-18,"Essa Jan, Nouar AlDahoul, Moiz Ali, Faizan Ahmad, Fareed Zaffar, Yasir Zaki",http://arxiv.org/pdf/2409.15361v1,cs.CL
Human-like Affective Cognition in Foundation Models,"Understanding emotions is fundamental to human interaction and experience.
Humans easily infer emotions from situations or facial expressions, situations
from emotions, and do a variety of other affective cognition. How adept is
modern AI at these inferences? We introduce an evaluation framework for testing
affective cognition in foundation models. Starting from psychological theory,
we generate 1,280 diverse scenarios exploring relationships between appraisals,
emotions, expressions, and outcomes. We evaluate the abilities of foundation
models (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefully
selected conditions. Our results show foundation models tend to agree with
human intuitions, matching or exceeding interparticipant agreement. In some
conditions, models are ``superhuman'' -- they better predict modal human
judgements than the average human. All models benefit from chain-of-thought
reasoning. This suggests foundation models have acquired a human-like
understanding of emotions and their influence on beliefs and behavior.",2024-09-18,"Kanishk Gandhi, Zoe Lynch, Jan-Philipp Fränken, Kayla Patterson, Sharon Wambu, Tobias Gerstenberg, Desmond C. Ong, Noah D. Goodman",http://arxiv.org/pdf/2409.11733v2,cs.CL
Enabling Real-Time Conversations with Minimal Training Costs,"Large language models (LLMs) have demonstrated the ability to improve human
efficiency through conversational interactions. Conventional LLM-powered
dialogue systems, operating on a turn-based paradigm, preclude real-time
interaction during response generation. To address this limitation, researchers
have proposed duplex models. These models can dynamically adapt to user input,
facilitating real-time interactive feedback. However, these methods typically
require substantial computational resources to acquire the ability. To reduce
overhead, this paper presents a new duplex decoding approach that enhances LLMs
with duplex ability, requiring minimal additional training. Specifically, our
method employs parallel decoding of queries and responses in conversations,
effectively implementing a channel-division-multiplexing decoding strategy.
Experimental results indicate that our proposed method significantly enhances
the naturalness and human-likeness of user-AI interactions with minimal
training costs.",2024-09-18,"Wang Xu, Shuo Wang, Weilin Zhao, Xu Han, Yukun Yan, Yudi Zhang, Zhe Tao, Zhiyuan Liu, Wanxiang Che",http://arxiv.org/pdf/2409.11727v1,cs.CL
Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing,"Large language model (LLM) role-playing has gained widespread attention.
Authentic character knowledge is crucial for constructing realistic LLM
role-playing agents. However, existing works usually overlook the exploration
of LLMs' ability to detect characters' known knowledge errors (KKE) and unknown
knowledge errors (UKE) while playing roles, which would lead to low-quality
automatic construction of character trainable corpus. In this paper, we propose
RoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The
results indicate that even the latest LLMs struggle to detect these two types
of errors effectively, especially when it comes to familiar knowledge. We
experimented with various reasoning strategies and propose an agent-based
reasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore
further the potential for improving error detection capabilities. Experiments
show that our method effectively improves the LLMs' ability to detect error
character knowledge, but it remains an issue that requires ongoing attention.",2024-09-18,"Wenyuan Zhang, Shuaiyi Nie, Jiawei Sheng, Zefeng Zhang, Xinghua Zhang, Yongquan He, Tingwen Liu",http://arxiv.org/pdf/2409.11726v2,cs.CL
TART: An Open-Source Tool-Augmented Framework for Explainable Table-based Reasoning,"Current Large Language Models (LLMs) exhibit limited ability to understand
table structures and to apply precise numerical reasoning, which is crucial for
tasks such as table question answering (TQA) and table-based fact verification
(TFV). To address these challenges, we introduce our Tool-Augmented Reasoning
framework for Tables (TART), which integrates LLMs with specialized tools. TART
contains three key components: a table formatter to ensure accurate data
representation, a tool maker to develop specific computational tools, and an
explanation generator to maintain explainability. We also present the TOOLTAB
dataset, a new benchmark designed specifically for training LLMs in table-tool
integration. Our experiments indicate that TART achieves substantial
improvements over existing methods (e.g., Chain-of-Thought) by improving both
the precision of data processing and the clarity of the reasoning process.
Notably, TART paired with CodeLlama achieves 90.0% of the accuracy of the
closed-sourced LLM GPT-3.5-turbo, highlighting its robustness in diverse
real-world scenarios. All the code and data are available at
https://github.com/XinyuanLu00/TART.",2024-09-18,"Xinyuan Lu, Liangming Pan, Yubo Ma, Preslav Nakov, Min-Yen Kan",http://arxiv.org/pdf/2409.11724v2,cs.CL
From Lists to Emojis: How Format Bias Affects Model Alignment,"In this paper, we study format biases in reinforcement learning from human
feedback (RLHF). We observe that many widely-used preference models, including
human evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,
exhibit strong biases towards specific format patterns, such as lists, links,
bold text, and emojis. Furthermore, large language models (LLMs) can exploit
these biases to achieve higher rankings on popular benchmarks like AlpacaEval
and LMSYS Chatbot Arena. One notable example of this is verbosity bias, where
current preference models favor longer responses that appear more
comprehensive, even when their quality is equal to or lower than shorter,
competing responses. However, format biases beyond verbosity remain largely
underexplored in the literature. In this work, we extend the study of biases in
preference learning beyond the commonly recognized length bias, offering a
comprehensive analysis of a wider range of format biases. Additionally, we show
that with a small amount of biased data (less than 1%), we can inject
significant bias into the reward model. Moreover, these format biases can also
be easily exploited by downstream alignment algorithms, such as best-of-n
sampling and online iterative DPO, as it is usually easier to manipulate the
format than to improve the quality of responses. Our findings emphasize the
need to disentangle format and content both for designing alignment algorithms
and evaluating models.",2024-09-18,"Xuanchang Zhang, Wei Xiong, Lichang Chen, Tianyi Zhou, Heng Huang, Tong Zhang",http://arxiv.org/pdf/2409.11704v2,cs.CL
Harnessing LLMs for API Interactions: A Framework for Classification and Synthetic Data Generation,"As Large Language Models (LLMs) advance in natural language processing, there
is growing interest in leveraging their capabilities to simplify software
interactions. In this paper, we propose a novel system that integrates LLMs for
both classifying natural language inputs into corresponding API calls and
automating the creation of sample datasets tailored to specific API functions.
By classifying natural language commands, our system allows users to invoke
complex software functionalities through simple inputs, improving interaction
efficiency and lowering the barrier to software utilization. Our dataset
generation approach also enables the efficient and systematic evaluation of
different LLMs in classifying API calls, offering a practical tool for
developers or business owners to assess the suitability of LLMs for customized
API management. We conduct experiments on several prominent LLMs using
generated sample datasets for various API functions. The results show that
GPT-4 achieves a high classification accuracy of 0.996, while LLaMA-3-8B
performs much worse at 0.759. These findings highlight the potential of LLMs to
transform API management and validate the effectiveness of our system in
guiding model testing and selection across diverse applications.",2024-09-18,"Chunliang Tao, Xiaojing Fan, Yahe Yang",http://arxiv.org/pdf/2409.11703v1,cs.CL
FLARE: Fusing Language Models and Collaborative Architectures for Recommender Enhancement,"Recent proposals in recommender systems represent items with their textual
description, using a large language model. They show better results on standard
benchmarks compared to an item ID-only model, such as Bert4Rec. In this work,
we revisit the often-used Bert4Rec baseline and show that with further tuning,
Bert4Rec significantly outperforms previously reported numbers, and in some
datasets, is competitive with state-of-the-art models.
  With revised baselines for item ID-only models, this paper also establishes
new competitive results for architectures that combine IDs and textual
descriptions. We demonstrate this with Flare (Fusing Language models and
collaborative Architectures for Recommender Enhancement). Flare is a novel
hybrid sequence recommender that integrates a language model with a
collaborative filtering model using a Perceiver network.
  Prior studies focus evaluation on datasets with limited-corpus size, but many
commercially-applicable recommender systems common on the web must handle
larger corpora. We evaluate Flare on a more realistic dataset with a
significantly larger item vocabulary, introducing new baselines for this
setting. This paper also showcases Flare's inherent ability to support
critiquing, enabling users to provide feedback and refine recommendations. We
leverage critiquing as an evaluation method to assess the model's language
understanding and its transferability to the recommendation task.",2024-09-18,"Liam Hebert, Marialena Kyriakidi, Hubert Pham, Krishna Sayana, James Pine, Sukhdeep Sodhi, Ambarish Jash",http://arxiv.org/pdf/2409.11699v2,cs.CL
Enhancing Complex Formula Recognition with Hierarchical Detail-Focused Network,"Hierarchical and complex Mathematical Expression Recognition (MER) is
challenging due to multiple possible interpretations of a formula, complicating
both parsing and evaluation. In this paper, we introduce the Hierarchical
Detail-Focused Recognition dataset (HDR), the first dataset specifically
designed to address these issues. It consists of a large-scale training set,
HDR-100M, offering an unprecedented scale and diversity with one hundred
million training instances. And the test set, HDR-Test, includes multiple
interpretations of complex hierarchical formulas for comprehensive model
performance evaluation. Additionally, the parsing of complex formulas often
suffers from errors in fine-grained details. To address this, we propose the
Hierarchical Detail-Focused Recognition Network (HDNet), an innovative
framework that incorporates a hierarchical sub-formula module, focusing on the
precise handling of formula details, thereby significantly enhancing MER
performance. Experimental results demonstrate that HDNet outperforms existing
MER models across various datasets.",2024-09-18,"Jiale Wang, Junhui Yu, Huanyong Liu, Chenanran Kong",http://arxiv.org/pdf/2409.11677v2,cs.CL
RUIE: Retrieval-based Unified Information Extraction using Large Language Model,"Unified information extraction (UIE) aims to extract diverse structured
information from unstructured text. While large language models (LLMs) have
shown promise for UIE, they require significant computational resources and
often struggle to generalize to unseen tasks. We propose RUIE (Retrieval-based
Unified Information Extraction), a framework that leverages in-context learning
for efficient task generalization. RUIE introduces a novel demonstration
selection mechanism combining LLM preferences with a keyword-enhanced reward
model, and employs a bi-encoder retriever trained through contrastive learning
and knowledge distillation. As the first trainable retrieval framework for UIE,
RUIE serves as a universal plugin for various LLMs. Experimental results on
eight held-out datasets demonstrate RUIE's effectiveness, with average F1-score
improvements of 19.22 and 3.22 compared to instruction-tuning methods and other
retrievers, respectively.",2024-09-18,"Xincheng Liao, Junwen Duan, Yixi Huang, Jianxin Wang",http://arxiv.org/pdf/2409.11673v2,cs.CL
Reward-Robust RLHF in LLMs,"As Large Language Models (LLMs) continue to progress toward more advanced
forms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is
increasingly seen as a key pathway toward achieving Artificial General
Intelligence (AGI). However, the reliance on reward-model-based (RM-based)
alignment methods introduces significant challenges due to the inherent
instability and imperfections of Reward Models (RMs), which can lead to
critical issues such as reward hacking and misalignment with human intentions.
In this paper, we introduce a reward-robust RLHF framework aimed at addressing
these fundamental challenges, paving the way for more reliable and resilient
learning in LLMs. Our approach introduces a novel optimization objective that
carefully balances performance and robustness by incorporating Bayesian Reward
Model Ensembles (BRME) to model the uncertainty set of reward functions. This
allows the framework to integrate both nominal performance and minimum reward
signals, ensuring more stable learning even with imperfect RMs. Empirical
results demonstrate that our framework consistently outperforms baselines
across diverse benchmarks, showing improved accuracy and long-term stability.
We also provide a theoretical analysis, demonstrating that reward-robust RLHF
approaches the stability of constant reward settings, which proves to be
acceptable even in a stochastic-case analysis. Together, these contributions
highlight the framework potential to enhance both the performance and stability
of LLM alignment.",2024-09-18,"Yuzi Yan, Xingzhou Lou, Jialian Li, Yiping Zhang, Jian Xie, Chao Yu, Yu Wang, Dong Yan, Yuan Shen",http://arxiv.org/pdf/2409.15360v3,cs.CL
Navigation with VLM framework: Go to Any Language,"Navigating towards fully open language goals and exploring open scenes in a
manner akin to human exploration have always posed significant challenges.
Recently, Vision Large Language Models (VLMs) have demonstrated remarkable
capabilities in reasoning with both language and visual data. While many works
have focused on leveraging VLMs for navigation in open scenes and with open
vocabularies, these efforts often fall short of fully utilizing the potential
of VLMs or require substantial computational resources. We introduce Navigation
with VLM (NavVLM), a framework that harnesses equipment-level VLMs to enable
agents to navigate towards any language goal specific or non-specific in open
scenes, emulating human exploration behaviors without any prior training. The
agent leverages the VLM as its cognitive core to perceive environmental
information based on any language goal and constantly provides exploration
guidance during navigation until it reaches the target location or area. Our
framework not only achieves state-of-the-art performance in Success Rate (SR)
and Success weighted by Path Length (SPL) in traditional specific goal settings
but also extends the navigation capabilities to any open-set language goal. We
evaluate NavVLM in richly detailed environments from the Matterport 3D (MP3D),
Habitat Matterport 3D (HM3D), and Gibson datasets within the Habitat simulator.
With the power of VLMs, navigation has entered a new era.",2024-09-18,"Zecheng Yin, Chonghao Cheng, Lizhen",http://arxiv.org/pdf/2410.02787v1,cs.CL
BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs for Bangla,"This study presents BanStereoSet, a dataset designed to evaluate
stereotypical social biases in multilingual LLMs for the Bangla language. In an
effort to extend the focus of bias research beyond English-centric datasets, we
have localized the content from the StereoSet, IndiBias, and Kamruzzaman et.
al.'s datasets, producing a resource tailored to capture biases prevalent
within the Bangla-speaking community. Our BanStereoSet dataset consists of
1,194 sentences spanning 9 categories of bias: race, profession, gender,
ageism, beauty, beauty in profession, region, caste, and religion. This dataset
not only serves as a crucial tool for measuring bias in multilingual LLMs but
also facilitates the exploration of stereotypical bias across different social
categories, potentially guiding the development of more equitable language
technologies in Bangladeshi contexts. Our analysis of several language models
using this dataset indicates significant biases, reinforcing the necessity for
culturally and linguistically adapted datasets to develop more equitable
language technologies.",2024-09-18,"Mahammed Kamruzzaman, Abdullah Al Monsur, Shrabon Das, Enamul Hassan, Gene Louis Kim",http://arxiv.org/pdf/2409.11638v1,cs.CL
"""A Woman is More Culturally Knowledgeable than A Man?"": The Effect of Personas on Cultural Norm Interpretation in LLMs","As the deployment of large language models (LLMs) expands, there is an
increasing demand for personalized LLMs. One method to personalize and guide
the outputs of these models is by assigning a persona -- a role that describes
the expected behavior of the LLM (e.g., a man, a woman, an engineer). This
study investigates whether an LLM's understanding of social norms varies across
assigned personas. Ideally, the perception of a social norm should remain
consistent regardless of the persona, since acceptability of a social norm
should be determined by the region the norm originates from, rather than by
individual characteristics such as gender, body size, or race. A norm is
universal within its cultural context. In our research, we tested 36 distinct
personas from 12 sociodemographic categories (e.g., age, gender, beauty) across
four different LLMs. We find that LLMs' cultural norm interpretation varies
based on the persona used and the norm interpretation also varies within a
sociodemographic category (e.g., a fat person and a thin person as in physical
appearance group) where an LLM with the more socially desirable persona (e.g.,
a thin person) interprets social norms more accurately than with the less
socially desirable persona (e.g., a fat person). We also discuss how different
types of social biases may contribute to the results that we observe.",2024-09-18,"Mahammed Kamruzzaman, Hieu Nguyen, Nazmul Hassan, Gene Louis Kim",http://arxiv.org/pdf/2409.11636v1,cs.CL
Watch Your Steps: Observable and Modular Chains of Thought,"We propose a variant of chain of thought (CoT) prompting called Program Trace
Prompting that makes explanations more observable while preserving the power,
generality and flexibility of CoT. In our approach, few-shot CoT demonstrations
are wrapped in a formal syntax based on Python, and each prompt: identifies and
names steps; defines the input/output behavior of steps; and replaces CoT
explanations of in-context examples with chains of these formalized steps on
the same examples. Program Trace Prompting is applicable to many tasks,
achieving strong results on the 23 diverse tasks in the BIG-Bench Hard
benchmark. More importantly, by instrumenting explanations in this way, we
enable new types of analysis. In particular, we identify ""non-local errors""
(which correspond to incorrectly learning the reasoning method illustrated in
the demonstrations) as an unaddressed issue in CoT learning, and we present
methods for verifying the modularity of steps in a CoT explanation.",2024-09-17,"Cassandra A. Cohen, William W. Cohen",http://arxiv.org/pdf/2409.15359v2,cs.CL
Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation,"Modern language models frequently include retrieval components to improve
their outputs, giving rise to a growing number of retrieval-augmented
generation (RAG) systems. Yet, most existing work in RAG has underemphasized
fair ranking techniques and neglected the diverse interests of all
stakeholders. In this paper, we present the first comprehensive study of RAG
systems that incorporate fairness-aware rankings, focusing on both ranking
fairness and attribution fairness - ensuring equitable exposure of sources
cited in the final text. We specifically examine item-side fairness, i.e.,
whether retrieved documents receive balanced exposure, and assess how this
affects both the system's overall performance and the eventual distribution of
cited sources. Across twelve RAG models and seven tasks, we find that
fairness-aware retrieval frequently retains or even improves ranking
effectiveness and generation quality, countering the widespread belief that
fairness compromises system performance. Moreover, we show that fair retrieval
leads to more balanced attribution in the final responses, ensuring that the
cited sources are credited more equitably. Our results underscore the
importance of item-side fairness throughout both retrieval and generation
phases, offering key insights for building more responsible and equitable RAG
systems and illustrating promising avenues for future exploration in fair
ranking and source attribution.",2024-09-17,"To Eun Kim, Fernando Diaz",http://arxiv.org/pdf/2409.11598v3,cs.CL
REAL: Response Embedding-based Alignment for LLMs,"Aligning large language models (LLMs) to human preferences is a crucial step
in building helpful and safe AI tools, which usually involve training on
supervised datasets. Popular algorithms such as Direct Preference Optimization
rely on pairs of AI-generated responses ranked according to human feedback. The
response pair annotation process is the most labor-intensive and costly part of
the alignment pipeline, and improving its efficiency and annotation quality
would have a meaningful impact on AI development. We propose REAL: Response
Embedding-based Alignment for LLMs, a strategy for constructing a high-quality
training dataset that focuses on acquiring the most informative response pairs
for labeling out of a set of response candidates. Our selection process is
based on embedding responses independently of prompts. Experimental results on
real-world dataset SHP2 and synthetic HH-RLHF benchmarks indicate that choosing
dissimilar response pairs enhances the direct alignment of LLMs while reducing
inherited labeling errors. The model aligned on dissimilar response pairs
obtained a better margin and win rate on the dialogue task. Our findings
suggest that focusing on distinct pairs can reduce the label error to improve
the efficiency of LLM alignment, saving up to 65% of annotators' work.",2024-09-17,"Honggen Zhang, Xufeng Zhao, Igor Molybog, June Zhang",http://arxiv.org/pdf/2409.17169v3,cs.CL
ProSLM : A Prolog Synergized Language Model for explainable Domain Specific Knowledge Based Question Answering,"Neurosymbolic approaches can add robustness to opaque neural systems by
incorporating explainable symbolic representations. However, previous
approaches have not used formal logic to contextualize queries to and validate
outputs of large language models (LLMs). We propose \systemname{}, a novel
neurosymbolic framework, to improve the robustness and reliability of LLMs in
question-answering tasks. We provide \systemname{} with a domain-specific
knowledge base, a logical reasoning system, and an integration to an existing
LLM. This framework has two capabilities (1) context gathering: generating
explainable and relevant context for a given query, and (2) validation:
confirming and validating the factual accuracy of a statement in accordance
with a knowledge base (KB). Our work opens a new area of neurosymbolic
generative AI text validation and user personalization.",2024-09-17,"Priyesh Vakharia, Abigail Kufeldt, Max Meyers, Ian Lane, Leilani Gilpin",http://arxiv.org/pdf/2409.11589v1,cs.CL
A Review of Mechanistic Models of Event Comprehension,"This review examines theoretical assumptions and computational models of
event comprehension, tracing the evolution from discourse comprehension
theories to contemporary event cognition frameworks. The review covers key
discourse comprehension accounts, including Construction-Integration, Event
Indexing, Causal Network, and Resonance models, highlighting their
contributions to understanding cognitive processes in comprehension. I then
discuss contemporary theoretical frameworks of event comprehension, including
Event Segmentation Theory (Zacks et al., 2007), the Event Horizon Model
(Radvansky & Zacks, 2014), and Hierarchical Generative Framework (Kuperberg,
2021), which emphasize prediction, causality, and multilevel representations in
event understanding. Building on these theories, I evaluate five computational
models of event comprehension: REPRISE (Butz et al., 2019), Structured Event
Memory (SEM; Franklin et al., 2020), the Lu model (Lu et al., 2022), the
Gumbsch model (Gumbsch et al., 2022), and the Elman and McRae model (2019). The
analysis focuses on their approaches to hierarchical processing, prediction
mechanisms, and representation learning. Key themes that emerge include the use
of hierarchical structures as inductive biases, the importance of prediction in
comprehension, and diverse strategies for learning event dynamics. The review
identifies critical areas for future research, including the need for more
sophisticated approaches to learning structured representations, integrating
episodic memory mechanisms, and developing adaptive updating algorithms for
working event models. By synthesizing insights from both theoretical frameworks
and computational implementations, this review aims to advance our
understanding of human event comprehension and guide future modeling efforts in
cognitive science.",2024-09-17,Tan T. Nguyen,http://arxiv.org/pdf/2409.18992v2,cs.CL
"HEARTS: A Holistic Framework for Explainable, Sustainable and Robust Text Stereotype Detection","Stereotypes are generalised assumptions about societal groups, and even
state-of-the-art LLMs using in-context learning struggle to identify them
accurately. Due to the subjective nature of stereotypes, where what constitutes
a stereotype can vary widely depending on cultural, social, and individual
perspectives, robust explainability is crucial. Explainable models ensure that
these nuanced judgments can be understood and validated by human users,
promoting trust and accountability. We address these challenges by introducing
HEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text
Stereotype Detection), a framework that enhances model performance, minimises
carbon footprint, and provides transparent, interpretable explanations. We
establish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising
57,201 labelled texts across six groups, including under-represented
demographics like LGBTQ+ and regional stereotypes. Ablation studies confirm
that BERT models fine-tuned on EMGSD outperform those trained on individual
components. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model
using SHAP to generate token-level importance values, ensuring alignment with
human understanding, and calculate explainability confidence scores by
comparing SHAP and LIME outputs...",2024-09-17,"Theo King, Zekun Wu, Adriano Koshiyama, Emre Kazim, Philip Treleaven",http://arxiv.org/pdf/2409.11579v3,cs.CL
"Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey","Preference tuning is a crucial process for aligning deep generative models
with human preferences. This survey offers a thorough overview of recent
advancements in preference tuning and the integration of human feedback. The
paper is organized into three main sections: 1) introduction and preliminaries:
an introduction to reinforcement learning frameworks, preference tuning tasks,
models, and datasets across various modalities: language, speech, and vision,
as well as different policy approaches, 2) in-depth exploration of each
preference tuning approach: a detailed analysis of the methods used in
preference tuning, and 3) applications, discussion, and future directions: an
exploration of the applications of preference tuning in downstream tasks,
including evaluation methods for different modalities, and an outlook on future
research directions. Our objective is to present the latest methodologies in
preference tuning and model alignment, enhancing the understanding of this
field for researchers and practitioners. We hope to encourage further
engagement and innovation in this area.",2024-09-17,"Genta Indra Winata, Hanyang Zhao, Anirban Das, Wenpin Tang, David D. Yao, Shi-Xiong Zhang, Sambit Sahu",http://arxiv.org/pdf/2409.11564v2,cs.CL
Small Language Models can Outperform Humans in Short Creative Writing: A Study Comparing SLMs with Humans and LLMs,"In this paper, we evaluate the creative fiction writing abilities of a
fine-tuned small language model (SLM), BART-large, and compare its performance
to human writers and two large language models (LLMs): GPT-3.5 and GPT-4o. Our
evaluation consists of two experiments: (i) a human study in which 68
participants rated short stories from humans and the SLM on grammaticality,
relevance, creativity, and attractiveness, and (ii) a qualitative linguistic
analysis examining the textual characteristics of stories produced by each
model. In the first experiment, BART-large outscored average human writers
overall (2.11 vs. 1.85), a 14% relative improvement, though the slight human
advantage in creativity was not statistically significant. In the second
experiment, qualitative analysis showed that while GPT-4o demonstrated
near-perfect coherence and used less cliche phrases, it tended to produce more
predictable language, with only 3% of its synopses featuring surprising
associations (compared to 15% for BART). These findings highlight how model
size and fine-tuning influence the balance between creativity, fluency, and
coherence in creative writing tasks, and demonstrate that smaller models can,
in certain contexts, rival both humans and larger models.",2024-09-17,"Guillermo Marco, Luz Rello, Julio Gonzalo",http://arxiv.org/pdf/2409.11547v2,cs.CL
Chain-of-Thought Prompting for Speech Translation,"Large language models (LLMs) have demonstrated remarkable advancements in
language understanding and generation. Building on the success of text-based
LLMs, recent research has adapted these models to use speech embeddings for
prompting, resulting in Speech-LLM models that exhibit strong performance in
automatic speech recognition (ASR) and automatic speech translation (AST). In
this work, we propose a novel approach to leverage ASR transcripts as prompts
for AST in a Speech-LLM built on an encoder-decoder text LLM. The Speech-LLM
model consists of a speech encoder and an encoder-decoder structure
Megatron-T5. By first decoding speech to generate ASR transcripts and
subsequently using these transcripts along with encoded speech for prompting,
we guide the speech translation in a two-step process like chain-of-thought
(CoT) prompting. Low-rank adaptation (LoRA) is used for the T5 LLM for model
adaptation and shows superior performance to full model fine-tuning.
Experimental results show that the proposed CoT prompting significantly
improves AST performance, achieving an average increase of 2.4 BLEU points
across 6 En->X or X->En AST tasks compared to speech prompting alone.
Additionally, compared to a related CoT prediction method that predicts a
concatenated sequence of ASR and AST transcripts, our method performs better by
an average of 2 BLEU points.",2024-09-17,"Ke Hu, Zhehuai Chen, Chao-Han Huck Yang, Piotr Żelasko, Oleksii Hrinchuk, Vitaly Lavrukhin, Jagadeesh Balam, Boris Ginsburg",http://arxiv.org/pdf/2409.11538v2,cs.CL
Egalitarian Language Representation in Language Models: It All Begins with Tokenizers,"Tokenizers act as a bridge between human language and the latent space of
language models, influencing how language is represented in these models. Due
to the immense popularity of English-Centric Large Language Models (LLMs),
efforts are being made to adapt them for other languages. However, we
demonstrate that, from a tokenization standpoint, not all tokenizers offer fair
representation for complex script languages such as Tamil, Sinhala, and Hindi,
primarily due to the choice of pre-tokenization methods. We go further to show
that pre-tokenization plays a more critical role than the tokenization
algorithm itself in achieving an egalitarian representation of these complex
script languages. To address this, we introduce an improvement to the Byte Pair
Encoding (BPE) algorithm by incorporating graphemes, which we term Grapheme
Pair Encoding (GPE). Our experiments show that grapheme-based character
extraction outperforms byte-level tokenizers for complex scripts. We validate
this approach through experiments on Tamil, Sinhala, and Hindi.",2024-09-17,"Menan Velayuthan, Kengatharaiyer Sarveswaran",http://arxiv.org/pdf/2409.11501v1,cs.CL
Multi-Document Grounded Multi-Turn Synthetic Dialog Generation,"We introduce a technique for multi-document grounded multi-turn synthetic
dialog generation that incorporates three main ideas. First, we control the
overall dialog flow using taxonomy-driven user queries that are generated with
Chain-of-Thought (CoT) prompting. Second, we support the generation of
multi-document grounded dialogs by mimicking real-world use of retrievers to
update the grounding documents after every user-turn in the dialog. Third, we
apply LLM-as-a-Judge to filter out queries with incorrect answers. Human
evaluation of the synthetic dialog data suggests that the data is diverse,
coherent, and includes mostly correct answers. Both human and automatic
evaluations of answerable queries indicate that models fine-tuned on synthetic
dialogs consistently out-perform those fine-tuned on existing human generated
training data across four publicly available multi-turn document grounded
benchmark test sets.",2024-09-17,"Young-Suk Lee, Chulaka Gunasekara, Danish Contractor, Ramón Fernandez Astudillo, Radu Florian",http://arxiv.org/pdf/2409.11500v1,cs.CL
"Augment, Drop & Swap: Improving Diversity in LLM Captions for Efficient Music-Text Representation Learning","Audio-text contrastive models have become a powerful approach in music
representation learning. Despite their empirical success, however, little is
known about the influence of key design choices on the quality of music-text
representations learnt through this framework. In this work, we expose these
design choices within the constraints of limited data and computation budgets,
and establish a more solid understanding of their impact grounded in empirical
observations along three axes: the choice of base encoders, the level of
curation in training data, and the use of text augmentation. We find that data
curation is the single most important factor for music-text contrastive
training in resource-constrained scenarios. Motivated by this insight, we
introduce two novel techniques, Augmented View Dropout and TextSwap, which
increase the diversity and descriptiveness of text inputs seen in training.
Through our experiments we demonstrate that these are effective at boosting
performance across different pre-training regimes, model architectures, and
downstream data distributions, without incurring higher computational costs or
requiring additional training data.",2024-09-17,"Ilaria Manco, Justin Salamon, Oriol Nieto",http://arxiv.org/pdf/2409.11498v1,cs.CL
Enriching Datasets with Demographics through Large Language Models: What's in a Name?,"Enriching datasets with demographic information, such as gender, race, and
age from names, is a critical task in fields like healthcare, public policy,
and social sciences. Such demographic insights allow for more precise and
effective engagement with target populations. Despite previous efforts
employing hidden Markov models and recurrent neural networks to predict
demographics from names, significant limitations persist: the lack of
large-scale, well-curated, unbiased, publicly available datasets, and the lack
of an approach robust across datasets. This scarcity has hindered the
development of traditional supervised learning approaches. In this paper, we
demonstrate that the zero-shot capabilities of Large Language Models (LLMs) can
perform as well as, if not better than, bespoke models trained on specialized
data. We apply these LLMs to a variety of datasets, including a real-life,
unlabelled dataset of licensed financial professionals in Hong Kong, and
critically assess the inherent demographic biases in these models. Our work not
only advances the state-of-the-art in demographic enrichment but also opens
avenues for future research in mitigating biases in LLMs.",2024-09-17,"Khaled AlNuaimi, Gautier Marti, Mathieu Ravaut, Abdulla AlKetbi, Andreas Henschel, Raed Jaradat",http://arxiv.org/pdf/2409.11491v1,cs.CL
Learning variant product relationship and variation attributes from e-commerce website structures,"We introduce VARM, variant relationship matcher strategy, to identify pairs
of variant products in e-commerce catalogs. Traditional definitions of entity
resolution are concerned with whether product mentions refer to the same
underlying product. However, this fails to capture product relationships that
are critical for e-commerce applications, such as having similar, but not
identical, products listed on the same webpage or share reviews. Here, we
formulate a new type of entity resolution in variant product relationships to
capture these similar e-commerce product links. In contrast with the
traditional definition, the new definition requires both identifying if two
products are variant matches of each other and what are the attributes that
vary between them. To satisfy these two requirements, we developed a strategy
that leverages the strengths of both encoding and generative AI models. First,
we construct a dataset that captures webpage product links, and therefore
variant product relationships, to train an encoding LLM to predict variant
matches for any given pair of products. Second, we use RAG prompted generative
LLMs to extract variation and common attributes amongst groups of variant
products. To validate our strategy, we evaluated model performance using real
data from one of the world's leading e-commerce retailers. The results showed
that our strategy outperforms alternative solutions and paves the way to
exploiting these new type of product relationships.",2024-09-17,"Pedro Herrero-Vidal, You-Lin Chen, Cris Liu, Prithviraj Sen, Lichao Wang",http://arxiv.org/pdf/2410.02779v1,cs.CL
AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs,"Arabic, with its rich diversity of dialects, remains significantly
underrepresented in Large Language Models, particularly in dialectal
variations. We address this gap by introducing seven synthetic datasets in
dialects alongside Modern Standard Arabic (MSA), created using Machine
Translation (MT) combined with human post-editing. We present AraDiCE, a
benchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on
dialect comprehension and generation, focusing specifically on low-resource
Arabic dialects. Additionally, we introduce the first-ever fine-grained
benchmark designed to evaluate cultural awareness across the Gulf, Egypt, and
Levant regions, providing a novel dimension to LLM evaluation. Our findings
demonstrate that while Arabic-specific models like Jais and AceGPT outperform
multilingual models on dialectal tasks, significant challenges persist in
dialect identification, generation, and translation. This work contributes
$\approx$45K post-edited samples, a cultural benchmark, and highlights the
importance of tailored training to improve LLM performance in capturing the
nuances of diverse Arabic dialects and cultural contexts. We have released the
dialectal translation models and benchmarks developed in this study
(https://huggingface.co/datasets/QCRI/AraDiCE).",2024-09-17,"Basel Mousi, Nadir Durrani, Fatema Ahmad, Md. Arid Hasan, Maram Hasanain, Tameem Kabbani, Fahim Dalvi, Shammur Absar Chowdhury, Firoj Alam",http://arxiv.org/pdf/2409.11404v3,cs.CL
NVLM: Open Frontier-Class Multimodal LLMs,"We introduce NVLM 1.0, a family of frontier-class multimodal large language
models (LLMs) that achieve state-of-the-art results on vision-language tasks,
rivaling the leading proprietary models (e.g., GPT-4o) and open-access models
(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved
text-only performance over its LLM backbone after multimodal training. In terms
of model design, we perform a comprehensive comparison between decoder-only
multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,
Flamingo). Based on the strengths and weaknesses of both approaches, we propose
a novel architecture that enhances both training efficiency and multimodal
reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for
tile-based dynamic high-resolution images, which significantly boosts
performance on multimodal reasoning and OCR-related tasks. Regarding training
data, we meticulously curate and provide detailed information on our multimodal
pretraining and supervised fine-tuning datasets. Our findings indicate that
dataset quality and task diversity are more important than scale, even during
the pretraining phase, across all architectures. Notably, we develop
production-grade multimodality for the NVLM-1.0 models, enabling them to excel
in vision-language tasks while maintaining and even improving text-only
performance compared to their LLM backbones. To achieve this, we craft and
integrate a high-quality text-only dataset into multimodal training, alongside
a substantial amount of multimodal math and reasoning data, leading to enhanced
math and coding capabilities across modalities. To advance research in the
field, we release the model weights at https://huggingface.co/nvidia/NVLM-D-72B
and will open-source the training code for the community soon.",2024-09-17,"Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",http://arxiv.org/pdf/2409.11402v2,cs.CL
Moshi: a speech-text foundation model for real-time dialogue,"We introduce Moshi, a speech-text foundation model and full-duplex spoken
dialogue framework. Current systems for spoken dialogue rely on pipelines of
independent components, namely voice activity detection, speech recognition,
textual dialogue and text-to-speech. Such frameworks cannot emulate the
experience of real conversations. First, their complexity induces a latency of
several seconds between interactions. Second, text being the intermediate
modality for dialogue, non-linguistic information that modifies meaning -- such
as emotion or non-speech sounds -- is lost in the interaction. Finally, they
rely on a segmentation into speaker turns, which does not take into account
overlapping speech, interruptions and interjections. Moshi solves these
independent issues altogether by casting spoken dialogue as speech-to-speech
generation. Starting from a text language model backbone, Moshi generates
speech as tokens from the residual quantizer of a neural audio codec, while
modeling separately its own speech and that of the user into parallel streams.
This allows for the removal of explicit speaker turns, and the modeling of
arbitrary conversational dynamics. We moreover extend the hierarchical
semantic-to-acoustic token generation of previous work to first predict
time-aligned text tokens as a prefix to audio tokens. Not only this ""Inner
Monologue"" method significantly improves the linguistic quality of generated
speech, but we also illustrate how it can provide streaming speech recognition
and text-to-speech. Our resulting model is the first real-time full-duplex
spoken large language model, with a theoretical latency of 160ms, 200ms in
practice, and is available at https://github.com/kyutai-labs/moshi.",2024-09-17,"Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, Neil Zeghidour",http://arxiv.org/pdf/2410.00037v2,cs.CL
Says Who? Effective Zero-Shot Annotation of Focalization,"Focalization, the perspective through which narrative is presented, is
encoded via a wide range of lexico-grammatical features and is subject to
reader interpretation. Even trained annotators frequently disagree on correct
labels, suggesting this task is both qualitatively and computationally
challenging. In this work, we test how well five contemporary large language
model (LLM) families and two baselines perform when annotating short literary
excerpts for focalization. Despite the challenging nature of the task, we find
that LLMs show comparable performance to trained human annotators, with GPT-4o
achieving an average F1 of 84.79%. Further, we demonstrate that the log
probabilities output by GPT-family models frequently reflect the difficulty of
annotating particular excerpts. Finally, we provide a case study analyzing
sixteen Stephen King novels, demonstrating the usefulness of this approach for
computational literary studies and the insights gleaned from examining
focalization at scale.",2024-09-17,"Rebecca M. M. Hicke, Yuri Bizzoni, Pascale Feldkamp, Ross Deans Kristensen-McLachlan",http://arxiv.org/pdf/2409.11390v2,cs.CL
Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement,"Finetuning large language models on instruction data is crucial for enhancing
pre-trained knowledge and improving instruction-following capabilities. As
instruction datasets proliferate, selecting optimal data for effective training
becomes increasingly important. This work addresses the question: How can we
determine the optimal subset of data for effective training? While existing
research often emphasizes local criteria like instance quality for subset
selection, we argue that a global approach focused on data diversity is more
critical. Our method employs k-means clustering to ensure the selected subset
effectively represents the full dataset. We propose an iterative refinement
method inspired by active learning techniques to resample instances from
clusters, reassessing each cluster's importance and sampling weight in every
training iteration. This approach reduces the effect of outliers and
automatically filters out clusters containing low-quality data. Through
extensive evaluation across natural language reasoning, general world
knowledge, code and math reasoning tasks, and by fine-tuning models from
various families, we observe consistent improvements, achieving a 7% increase
over random selection and a 3.8% improvement over state-of-the-art sampling
methods. Our work highlights the significance of diversity-first sampling when
finetuning LLMs to enhance performance across a broad array of evaluation
tasks. Our code is available at
https://github.com/for-ai/iterative-data-selection.",2024-09-17,"Simon Yu, Liangyu Chen, Sara Ahmadian, Marzieh Fadaee",http://arxiv.org/pdf/2409.11378v1,cs.CL
CoCA: Regaining Safety-awareness of Multimodal Large Language Models with Constitutional Calibration,"The deployment of multimodal large language models (MLLMs) has demonstrated
remarkable success in engaging in conversations involving visual inputs, thanks
to the superior power of large language models (LLMs). Those MLLMs are
typically built based on the LLMs, with an image encoder to process images into
the token embedding space of the LLMs. However, the integration of visual
modality has introduced a unique vulnerability: the MLLM becomes susceptible to
malicious visual inputs and prone to generating sensitive or harmful responses,
even though the LLM has been trained on textual dataset to align with human
value. In this paper, we first raise the question: ``Do the MLLMs possess
safety-awareness against malicious image inputs?"". We find that after adding a
principle that specifies the safety requirement into the input of the MLLM, the
model's safety awareness becomes boosted. This phenomenon verifies the
existence of MLLM's safety-awareness against image inputs, it is only weakened
by the modality gap. We then introduce a simple yet effective technique termed
CoCA, which amplifies the safety-awareness of the MLLM by calibrating its
output distribution. Our proposed strategy helps the model reclaim its original
safety awareness without losing its original capabilities. We verify the
effectiveness of our approach on both multimodal safety and understanding
benchmarks.",2024-09-17,"Jiahui Gao, Renjie Pi, Tianyang Han, Han Wu, Lanqing Hong, Lingpeng Kong, Xin Jiang, Zhenguo Li",http://arxiv.org/pdf/2409.11365v2,cs.CL
CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark,"AI agents have the potential to aid users on a variety of consequential
tasks, including conducting scientific research. To spur the development of
useful agents, we need benchmarks that are challenging, but more crucially,
directly correspond to real-world tasks of interest. This paper introduces such
a benchmark, designed to measure the accuracy of AI agents in tackling a
crucial yet surprisingly challenging aspect of scientific research:
computational reproducibility. This task, fundamental to the scientific
process, involves reproducing the results of a study using the provided code
and data. We introduce CORE-Bench (Computational Reproducibility Agent
Benchmark), a benchmark consisting of 270 tasks based on 90 scientific papers
across three disciplines (computer science, social science, and medicine).
Tasks in CORE-Bench consist of three difficulty levels and include both
language-only and vision-language tasks. We provide an evaluation system to
measure the accuracy of agents in a fast and parallelizable way, saving days of
evaluation time for each run compared to a sequential implementation. We
evaluated two baseline agents: the general-purpose AutoGPT and a task-specific
agent called CORE-Agent. We tested both variants using two underlying language
models: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21% on
the hardest task, showing the vast scope for improvement in automating routine
scientific tasks. Having agents that can reproduce existing work is a necessary
step towards building agents that can conduct novel research and could verify
and improve the performance of other research agents. We hope that CORE-Bench
can improve the state of reproducibility and spur the development of future
research agents.",2024-09-17,"Zachary S. Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan",http://arxiv.org/pdf/2409.11363v1,cs.CL
THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models,"Hallucination, the generation of factually incorrect content, is a growing
challenge in Large Language Models (LLMs). Existing detection and mitigation
methods are often isolated and insufficient for domain-specific needs, lacking
a standardized pipeline. This paper introduces THaMES (Tool for Hallucination
Mitigations and EvaluationS), an integrated framework and library addressing
this gap. THaMES offers an end-to-end solution for evaluating and mitigating
hallucinations in LLMs, featuring automated test set generation, multifaceted
benchmarking, and adaptable mitigation strategies. It automates test set
creation from any corpus, ensuring high data quality, diversity, and
cost-efficiency through techniques like batch processing, weighted sampling,
and counterfactual validation. THaMES assesses a model's ability to detect and
reduce hallucinations across various tasks, including text generation and
binary classification, applying optimal mitigation strategies like In-Context
Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient
Fine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base
of academic papers, political news, and Wikipedia reveal that commercial models
like GPT-4o benefit more from RAG than ICL, while open-weight models like
Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT
significantly enhances the performance of Llama-3.1-8B-Instruct in both
evaluation tasks.",2024-09-17,"Mengfei Liang, Archish Arun, Zekun Wu, Cristian Munoz, Jonathan Lutch, Emre Kazim, Adriano Koshiyama, Philip Treleaven",http://arxiv.org/pdf/2409.11353v3,cs.CL
SpMis: An Investigation of Synthetic Spoken Misinformation Detection,"In recent years, speech generation technology has advanced rapidly, fueled by
generative models and large-scale training techniques. While these developments
have enabled the production of high-quality synthetic speech, they have also
raised concerns about the misuse of this technology, particularly for
generating synthetic misinformation. Current research primarily focuses on
distinguishing machine-generated speech from human-produced speech, but the
more urgent challenge is detecting misinformation within spoken content. This
task requires a thorough analysis of factors such as speaker identity, topic,
and synthesis. To address this need, we conduct an initial investigation into
synthetic spoken misinformation detection by introducing an open-source
dataset, SpMis. SpMis includes speech synthesized from over 1,000 speakers
across five common topics, utilizing state-of-the-art text-to-speech systems.
Although our results show promising detection capabilities, they also reveal
substantial challenges for practical implementation, underscoring the
importance of ongoing research in this critical area.",2024-09-17,"Peizhuo Liu, Li Wang, Renqiang He, Haorui He, Lei Wang, Huadi Zheng, Jie Shi, Tong Xiao, Zhizheng Wu",http://arxiv.org/pdf/2409.11308v1,cs.CL
EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage,"Generalist web agents have demonstrated remarkable potential in autonomously
completing a wide range of tasks on real websites, significantly boosting human
productivity. However, web tasks, such as booking flights, usually involve
users' PII, which may be exposed to potential privacy risks if web agents
accidentally interact with compromised websites, a scenario that remains
largely unexplored in the literature. In this work, we narrow this gap by
conducting the first study on the privacy risks of generalist web agents in
adversarial environments. First, we present a realistic threat model for
attacks on the website, where we consider two adversarial targets: stealing
users' specific PII or the entire user request. Then, we propose a novel attack
method, termed Environmental Injection Attack (EIA). EIA injects malicious
content designed to adapt well to environments where the agents operate and our
work instantiates EIA specifically for privacy scenarios in web environments.
We collect 177 action steps that involve diverse PII categories on realistic
websites from the Mind2Web, and conduct experiments using one of the most
capable generalist web agent frameworks to date. The results demonstrate that
EIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user
request. Additionally, by accessing the stealthiness and experimenting with a
defensive system prompt, we indicate that EIA is hard to detect and mitigate.
Notably, attacks that are not well adapted for a webpage can be detected via
human inspection, leading to our discussion about the trade-off between
security and autonomy. However, extra attackers' efforts can make EIA
seamlessly adapted, rendering such supervision ineffective. Thus, we further
discuss the defenses at the pre- and post-deployment stages of the websites
without relying on human supervision and call for more advanced defense
strategies.",2024-09-17,"Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun",http://arxiv.org/pdf/2409.11295v5,cs.CL
Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling,"LLMs obtain remarkable performance but suffer from hallucinations. Most
research on detecting hallucination focuses on the questions with short and
concrete correct answers that are easy to check the faithfulness. Hallucination
detections for text generation with open-ended answers are more challenging.
Some researchers use external knowledge to detect hallucinations in generated
texts, but external resources for specific scenarios are hard to access. Recent
studies on detecting hallucinations in long text without external resources
conduct consistency comparison among multiple sampled outputs. To handle long
texts, researchers split long texts into multiple facts and individually
compare the consistency of each pairs of facts. However, these methods (1)
hardly achieve alignment among multiple facts; (2) overlook dependencies
between multiple contextual facts. In this paper, we propose a graph-based
context-aware (GCA) hallucination detection for text generations, which aligns
knowledge facts and considers the dependencies between contextual knowledge
triples in consistency comparison. Particularly, to align multiple facts, we
conduct a triple-oriented response segmentation to extract multiple knowledge
triples. To model dependencies among contextual knowledge triple (facts), we
construct contextual triple into a graph and enhance triples' interactions via
message passing and aggregating via RGCN. To avoid the omission of knowledge
triples in long text, we conduct a LLM-based reverse verification via
reconstructing the knowledge triples. Experiments show that our model enhances
hallucination detection and excels all baselines.",2024-09-17,"Xinyue Fang, Zhen Huang, Zhiliang Tian, Minghui Fang, Ziyi Pan, Quntian Fang, Zhihua Wen, Hengyue Pan, Dongsheng Li",http://arxiv.org/pdf/2409.11283v4,cs.CL
Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5,"The surge of digital documents in various formats, including less
standardized documents such as business reports and environmental assessments,
underscores the growing importance of Document Understanding. While Large
Language Models (LLMs) have showcased prowess across diverse natural language
processing tasks, their direct application to Document Understanding remains a
challenge. Previous research has demonstrated the utility of LLMs in this
domain, yet their significant computational demands make them challenging to
deploy effectively. Additionally, proprietary Blackbox LLMs often outperform
their open-source counterparts, posing a barrier to widespread accessibility.
In this paper, we delve into the realm of document understanding, leveraging
distillation methods to harness the power of large LLMs while accommodating
computational limitations. Specifically, we present a novel approach wherein we
distill document understanding knowledge from the proprietary LLM ChatGPT into
FLAN-T5. Our methodology integrates labeling and curriculum-learning mechanisms
to facilitate efficient knowledge transfer. This work contributes to the
advancement of document understanding methodologies by offering a scalable
solution that bridges the gap between resource-intensive LLMs and practical
applications. Our findings underscore the potential of distillation techniques
in facilitating the deployment of sophisticated language models in real-world
scenarios, thereby fostering advancements in natural language processing and
document comprehension domains.",2024-09-17,"Marcel Lamott, Muhammad Armaghan Shakir",http://arxiv.org/pdf/2409.11282v1,cs.CL
P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task,"Embodied Everyday Task is a popular task in the embodied AI community,
requiring agents to make a sequence of actions based on natural language
instructions and visual observations. Traditional learning-based approaches
face two challenges. Firstly, natural language instructions often lack explicit
task planning. Secondly, extensive training is required to equip models with
knowledge of the task environment. Previous works based on Large Language Model
(LLM) either suffer from poor performance due to the lack of task-specific
knowledge or rely on ground truth as few-shot samples. To address the above
limitations, we propose a novel approach called Progressive Retrieval Augmented
Generation (P-RAG), which not only effectively leverages the powerful language
processing capabilities of LLMs but also progressively accumulates
task-specific knowledge without ground-truth. Compared to the conventional RAG
methods, which retrieve relevant information from the database in a one-shot
manner to assist generation, P-RAG introduces an iterative approach to
progressively update the database. In each iteration, P-RAG retrieves the
latest database and obtains historical information from the previous
interaction as experiential references for the current interaction. Moreover,
we also introduce a more granular retrieval scheme that not only retrieves
similar tasks but also incorporates retrieval of similar situations to provide
more valuable reference experiences. Extensive experiments reveal that P-RAG
achieves competitive results without utilizing ground truth and can even
further improve performance through self-iterations.",2024-09-17,"Weiye Xu, Min Wang, Wengang Zhou, Houqiang Li",http://arxiv.org/pdf/2409.11279v1,cs.CL
Task Arithmetic for Language Expansion in Speech Translation,"Recent progress in large language models (LLMs) has gained interest in
speech-text multimodal foundation models, achieving strong performance on
instruction-tuned speech translation (ST). However, expanding language pairs is
costly due to re-training on combined new and previous datasets. To address
this, we aim to build a one-to-many ST system from existing one-to-one ST
systems using task arithmetic without re-training. Direct application of task
arithmetic in ST leads to language confusion; therefore, we introduce an
augmented task arithmetic method incorporating a language control model to
ensure correct target language generation. Our experiments on MuST-C and
CoVoST-2 show BLEU score improvements of up to 4.66 and 4.92, with COMET gains
of 8.87 and 11.83. In addition, we demonstrate our framework can extend to
language pairs lacking paired ST training data or pre-trained ST models by
synthesizing ST models based on existing machine translation (MT) and ST models
via task analogies.",2024-09-17,"Yao-Fei Cheng, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Wen Shen Teo, Siddhant Arora, Shinji Watanabe",http://arxiv.org/pdf/2409.11274v2,cs.CL
LOLA -- An Open-Source Massively Multilingual Large Language Model,"This paper presents LOLA, a massively multilingual large language model
trained on more than 160 languages using a sparse Mixture-of-Experts
Transformer architecture. Our architectural and implementation choices address
the challenge of harnessing linguistic diversity while maintaining efficiency
and avoiding the common pitfalls of multilinguality. Our analysis of the
evaluation results shows competitive performance in natural language generation
and understanding tasks. Additionally, we demonstrate how the learned
expert-routing mechanism exploits implicit phylogenetic linguistic patterns to
potentially alleviate the curse of multilinguality. We provide an in-depth look
at the training process, an analysis of the datasets, and a balanced
exploration of the model's strengths and limitations. As an open-source model,
LOLA promotes reproducibility and serves as a robust foundation for future
research. Our findings enable the development of compute-efficient multilingual
models with strong, scalable performance across languages.",2024-09-17,"Nikit Srivastava, Denis Kuchelev, Tatiana Moteu Ngoli, Kshitij Shetty, Michael Röder, Hamada Zahera, Diego Moussallem, Axel-Cyrille Ngonga Ngomo",http://arxiv.org/pdf/2409.11272v7,cs.CL
Bio-Inspired Mamba: Temporal Locality and Bioplausible Learning in Selective State Space Models,"This paper introduces Bio-Inspired Mamba (BIM), a novel online learning
framework for selective state space models that integrates biological learning
principles with the Mamba architecture. BIM combines Real-Time Recurrent
Learning (RTRL) with Spike-Timing-Dependent Plasticity (STDP)-like local
learning rules, addressing the challenges of temporal locality and biological
plausibility in training spiking neural networks. Our approach leverages the
inherent connection between backpropagation through time and STDP, offering a
computationally efficient alternative that maintains the ability to capture
long-range dependencies. We evaluate BIM on language modeling, speech
recognition, and biomedical signal analysis tasks, demonstrating competitive
performance against traditional methods while adhering to biological learning
principles. Results show improved energy efficiency and potential for
neuromorphic hardware implementation. BIM not only advances the field of
biologically plausible machine learning but also provides insights into the
mechanisms of temporal information processing in biological neural networks.",2024-09-17,Jiahao Qin,http://arxiv.org/pdf/2409.11263v1,cs.CL
The Art of Storytelling: Multi-Agent Generative AI for Dynamic Multimodal Narratives,"This paper introduces the concept of an education tool that utilizes
Generative Artificial Intelligence (GenAI) to enhance storytelling for
children. The system combines GenAI-driven narrative co-creation,
text-to-speech conversion, and text-to-video generation to produce an engaging
experience for learners. We describe the co-creation process, the adaptation of
narratives into spoken words using text-to-speech models, and the
transformation of these narratives into contextually relevant visuals through
text-to-video technology. Our evaluation covers the linguistics of the
generated stories, the text-to-speech conversion quality, and the accuracy of
the generated visuals.",2024-09-17,"Samee Arif, Taimoor Arif, Muhammad Saad Haroon, Aamina Jamal Khan, Agha Ali Raza, Awais Athar",http://arxiv.org/pdf/2409.11261v4,cs.CL
Norm of Mean Contextualized Embeddings Determines their Variance,"Contextualized embeddings vary by context, even for the same token, and form
a distribution in the embedding space. To analyze this distribution, we focus
on the norm of the mean embedding and the variance of the embeddings. In this
study, we first demonstrate that these values follow the well-known formula for
variance in statistics and provide an efficient sequential computation method.
Then, by observing embeddings from intermediate layers of several Transformer
models, we found a strong trade-off relationship between the norm and the
variance: as the mean embedding becomes closer to the origin, the variance
increases. This trade-off is likely influenced by the layer normalization
mechanism used in Transformer models. Furthermore, when the sets of token
embeddings are treated as clusters, we show that the variance of the entire
embedding set can theoretically be decomposed into the within-cluster variance
and the between-cluster variance. We found experimentally that as the layers of
Transformer models deepen, the embeddings move farther from the origin, the
between-cluster variance relatively decreases, and the within-cluster variance
relatively increases. These results are consistent with existing studies on the
anisotropy of the embedding spaces across layers.",2024-09-17,"Hiroaki Yamagiwa, Hidetoshi Shimodaira",http://arxiv.org/pdf/2409.11253v2,cs.CL
WER We Stand: Benchmarking Urdu ASR Models,"This paper presents a comprehensive evaluation of Urdu Automatic Speech
Recognition (ASR) models. We analyze the performance of three ASR model
families: Whisper, MMS, and Seamless-M4T using Word Error Rate (WER), along
with a detailed examination of the most frequent wrong words and error types
including insertions, deletions, and substitutions. Our analysis is conducted
using two types of datasets, read speech and conversational speech. Notably, we
present the first conversational speech dataset designed for benchmarking Urdu
ASR models. We find that seamless-large outperforms other ASR models on the
read speech dataset, while whisper-large performs best on the conversational
speech dataset. Furthermore, this evaluation highlights the complexities of
assessing ASR models for low-resource languages like Urdu using quantitative
metrics alone and emphasizes the need for a robust Urdu text normalization
system. Our findings contribute valuable insights for developing robust ASR
systems for low-resource languages like Urdu.",2024-09-17,"Samee Arif, Sualeha Farid, Aamina Jamal Khan, Mustafa Abbas, Agha Ali Raza, Awais Athar",http://arxiv.org/pdf/2409.11252v2,cs.CL
Linear Recency Bias During Training Improves Transformers' Fit to Reading Times,"Recent psycholinguistic research has compared human reading times to
surprisal estimates from language models to study the factors shaping human
sentence processing difficulty. Previous studies have shown a strong fit
between surprisal values from Transformers and reading times. However, standard
Transformers work with a lossless representation of the entire previous
linguistic context, unlike models of human language processing that include
memory decay. To bridge this gap, this paper evaluates a modification of the
Transformer model that uses ALiBi (Press et al., 2022), a recency bias added to
attention scores. Surprisal estimates with ALiBi show an improved fit to human
reading times compared to a standard Transformer baseline. A subsequent
analysis of attention heads suggests that ALiBi's mixture of slopes -- which
determine the rate of memory decay in each attention head -- may play a role in
the improvement by helping models with ALiBi to track different kinds of
linguistic dependencies.",2024-09-17,"Christian Clark, Byung-Doh Oh, William Schuler",http://arxiv.org/pdf/2409.11250v1,cs.CL
Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse,"LLMs are an integral component of retrieval-augmented generation (RAG)
systems. While many studies focus on evaluating the overall quality of
end-to-end RAG systems, there is a gap in understanding the appropriateness of
LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic
metric that evaluates the trustworthiness of LLMs within the RAG framework. Our
results show that various prompting methods, such as in-context learning, fail
to effectively adapt LLMs to the RAG task as measured by Trust-Score.
Consequently, we propose Trust-Align, a method to align LLMs for improved
Trust-Score performance. 26 out of 27 models aligned using Trust-Align
substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5.
Specifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56),
QAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly
enhances models' ability to correctly refuse and provide quality citations. We
also demonstrate the effectiveness of Trust-Align across different open-weight
models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b),
and Phi3.5 (3.8b). We release our code at
https://github.com/declare-lab/trust-align.",2024-09-17,"Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria",http://arxiv.org/pdf/2409.11242v4,cs.CL
Spontaneous Informal Speech Dataset for Punctuation Restoration,"Presently, punctuation restoration models are evaluated almost solely on
well-structured, scripted corpora. On the other hand, real-world ASR systems
and post-processing pipelines typically apply towards spontaneous speech with
significant irregularities, stutters, and deviations from perfect grammar. To
address this discrepancy, we introduce SponSpeech, a punctuation restoration
dataset derived from informal speech sources, which includes punctuation and
casing information. In addition to publicly releasing the dataset, we
contribute a filtering pipeline that can be used to generate more data. Our
filtering pipeline examines the quality of both speech audio and transcription
text. We also carefully construct a ``challenging"" test set, aimed at
evaluating models' ability to leverage audio information to predict otherwise
grammatically ambiguous punctuation. SponSpeech is available at
https://github.com/GitHubAccountAnonymous/PR, along with all code for dataset
building and model runs.",2024-09-17,"Xing Yi Liu, Homayoon Beigi",http://arxiv.org/pdf/2409.11241v1,cs.CL
LLM-as-a-Judge & Reward Model: What They Can and Cannot Do,"LLM-as-a-Judge and reward models are widely used alternatives of
multiple-choice questions or human annotators for large language model (LLM)
evaluation. Their efficacy shines in evaluating long-form responses, serving a
critical role as evaluators of leaderboards and as proxies to align LLMs via
reinforcement learning. However, despite their popularity, their effectiveness
in diverse contexts, such as non-English prompts, factual verification, or
challenging questions, remains unexplored. In this paper, we conduct a
comprehensive analysis of automated evaluators, reporting several key findings
on their behavior. First, we discover that English evaluation capabilities
significantly influence language-specific evaluation capabilities, often more
than the language proficiency itself, enabling evaluators trained in English to
easily transfer their skills to other languages. Second, we identify critical
shortcomings, where LLMs fail to detect and penalize errors, such as factual
inaccuracies, cultural misrepresentations, and the presence of unwanted
language. Finally, we find that state-of-the-art evaluators struggle with
challenging prompts, in either English or Korean, underscoring their
limitations in assessing or generating complex reasoning questions. We release
the dataset and codes used.",2024-09-17,"Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, Seunghyeok Hong",http://arxiv.org/pdf/2409.11239v2,cs.CL
Surveying the MLLM Landscape: A Meta-Review of Current Surveys,"The rise of Multimodal Large Language Models (MLLMs) has become a
transformative force in the field of artificial intelligence, enabling machines
to process and generate content across multiple modalities, such as text,
images, audio, and video. These models represent a significant advancement over
traditional unimodal systems, opening new frontiers in diverse applications
ranging from autonomous agents to medical diagnostics. By integrating multiple
modalities, MLLMs achieve a more holistic understanding of information, closely
mimicking human perception. As the capabilities of MLLMs expand, the need for
comprehensive and accurate performance evaluation has become increasingly
critical. This survey aims to provide a systematic review of benchmark tests
and evaluation methods for MLLMs, covering key topics such as foundational
concepts, applications, evaluation methodologies, ethical concerns, security,
efficiency, and domain-specific applications. Through the classification and
analysis of existing literature, we summarize the main contributions and
methodologies of various surveys, conduct a detailed comparative analysis, and
examine their impact within the academic community. Additionally, we identify
emerging trends and underexplored areas in MLLM research, proposing potential
directions for future studies. This survey is intended to offer researchers and
practitioners a comprehensive understanding of the current state of MLLM
evaluation, thereby facilitating further progress in this rapidly evolving
field.",2024-09-17,"Ming Li, Keyu Chen, Ziqian Bi, Ming Liu, Benji Peng, Qian Niu, Junyu Liu, Jinlang Wang, Sen Zhang, Xuanhe Pan, Jiawei Xu, Pohsun Feng",http://arxiv.org/pdf/2409.18991v1,cs.CL
Evaluating the Impact of Compression Techniques on Task-Specific Performance of Large Language Models,"Large language models (LLMs) offer powerful capabilities but incur
substantial computational costs, driving the need for efficient compression
techniques. This study evaluates the impact of popular compression methods -
Magnitude Pruning, SparseGPT, and Wanda - on the LLaMA-2-7B model, focusing on
the trade-offs between model size reduction, downstream task performance, and
the role of calibration data. Our findings reveal that while SparseGPT and
Wanda preserve perplexity even at 50% sparsity, they suffer significant
degradation on downstream tasks, highlighting the inadequacy of perplexity as
the sole evaluation metric. To address this, we introduce Jensen-Shannon (JS)
Divergence as a more comprehensive metric that captures nuanced changes in
model behavior post-compression. We further demonstrate that task-specific
calibration data significantly enhances the downstream performance of
compressed models compared to general calibration data. This research
underscores the necessity for diverse evaluation metrics and careful
calibration data selection to fully understand the complexities of LLM
compression and its implications for practical applications.",2024-09-17,"Bishwash Khanal, Jeffery M. Capone",http://arxiv.org/pdf/2409.11233v1,cs.CL
Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT Problem: Does the LLM Solve the Problem Itself or Call an External SAT Solver?,"In this manuscript, I present an analysis on the performance of OpenAI
O1-preview model in solving random K-SAT instances for K$\in {2,3,4}$ as a
function of $\alpha=M/N$ where $M$ is the number of clauses and $N$ is the
number of variables of the satisfiable problem. I show that the model can call
an external SAT solver to solve the instances, rather than solving them
directly. Despite using external solvers, the model reports incorrect
assignments as output. Moreover, I propose and present an analysis to quantify
whether the OpenAI O1-preview model demonstrates a spark of intelligence or
merely makes random guesses when outputting an assignment for a Boolean
satisfiability problem.",2024-09-17,Raffaele Marino,http://arxiv.org/pdf/2409.11232v2,cs.CL
Exploring ChatGPT-based Augmentation Strategies for Contrastive Aspect-based Sentiment Analysis,"Aspect-based sentiment analysis (ABSA) involves identifying sentiment towards
specific aspect terms in a sentence and allows us to uncover nuanced
perspectives and attitudes on particular aspects of a product, service, or
topic. However, the scarcity of labeled data poses a significant challenge to
training high-quality models. To address this issue, we explore the potential
of data augmentation using ChatGPT, a well-performing large language model
(LLM), to enhance the sentiment classification performance towards aspect
terms. Specifically, we explore three data augmentation strategies based on
ChatGPT: context-focused, aspect-focused, and context-aspect data augmentation
techniques. Context-focused data augmentation focuses on changing the word
expression of context words in the sentence while keeping aspect terms
unchanged. In contrast, aspect-focused data augmentation aims to change aspect
terms but keep context words unchanged. Context-Aspect data augmentation
integrates the above two data augmentations to generate augmented samples.
Furthermore, we incorporate contrastive learning into the ABSA tasks to improve
performance. Extensive experiments show that all three data augmentation
techniques lead to performance improvements, with the context-aspect data
augmentation strategy performing best and surpassing the performance of the
baseline models.",2024-09-17,"Lingling Xu, Haoran Xie, S. Joe Qin, Fu Lee Wang, Xiaohui Tao",http://arxiv.org/pdf/2409.11218v1,cs.CL
Self-Evolutionary Large Language Models through Uncertainty-Enhanced Preference Optimization,"Iterative preference optimization has recently become one of the de-facto
training paradigms for large language models (LLMs), but the performance is
still underwhelming due to too much noisy preference data yielded in the loop.
To combat this issue, we present an \textbf{U}ncertainty-enhanced
\textbf{P}reference \textbf{O}ptimization (UPO) framework to make the LLM
self-evolve with reliable feedback. The key idea is mitigating the noisy
preference data derived from the current policy and reward models by performing
pair-wise uncertainty estimation and judiciously reliable feedback sampling. To
reach this goal, we thus introduce an estimator model, which incorporates Monte
Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty
estimation for the preference data derived from the LLM policy. Compared to the
existing methods that directly filter generated responses based on the reward
score, the estimator focuses on the model uncertainty in a pair-wise manner and
effectively bypasses the confirmation bias problem of the reward model.
Additionally, we also propose an uncertainty-enhanced self-evolution algorithm
to improve the robustness of preference optimization and encourage the LLM to
generate responses with both high reward and certainty. Extensive experiments
over multiple benchmarks demonstrate that our framework substantially
alleviates the noisy problem and improves the performance of iterative
preference optimization.",2024-09-17,"Jianing Wang, Yang Zhou, Xiaocheng Zhang, Mengjiao Bao, Peng Yan",http://arxiv.org/pdf/2409.11212v1,cs.CL
Mind the Uncertainty in Human Disagreement: Evaluating Discrepancies between Model Predictions and Human Responses in VQA,"Large vision-language models frequently struggle to accurately predict
responses provided by multiple human annotators, particularly when those
responses exhibit human uncertainty. In this study, we focus on the Visual
Question Answering (VQA) task, and we comprehensively evaluate how well the
state-of-the-art vision-language models correlate with the distribution of
human responses. To do so, we categorize our samples based on their levels
(low, medium, high) of human uncertainty in disagreement (HUD) and employ not
only accuracy but also three new human-correlated metrics in VQA, to
investigate the impact of HUD. To better align models with humans, we also
verify the effect of common calibration and human calibration. Our results show
that even BEiT3, currently the best model for this task, struggles to capture
the multi-label distribution inherent in diverse human responses. Additionally,
we observe that the commonly used accuracy-oriented calibration technique
adversely affects BEiT3's ability to capture HUD, further widening the gap
between model predictions and human distributions. In contrast, we show the
benefits of calibrating models towards human distributions for VQA, better
aligning model confidence with human uncertainty. Our findings highlight that
for VQA, the consistent alignment between human responses and model predictions
is understudied and should become the next crucial target of future studies.",2024-09-17,"Jian Lan, Diego Frassinelli, Barbara Plank",http://arxiv.org/pdf/2410.02773v1,cs.CL
Capturing Differences in Character Representations Between Communities: An Initial Study with Fandom,"Sociolinguistic theories have highlighted how narratives are often retold,
co-constructed and reconceptualized in collaborative settings. This working
paper focuses on the re-interpretation of characters, an integral part of the
narrative story-world, and attempts to study how this may be computationally
compared between online communities. Using online fandom - a highly communal
phenomenon that has been largely studied qualitatively - as data, computational
methods were applied to explore shifts in character representations between two
communities and the original text. Specifically, text from the Harry Potter
novels, r/HarryPotter subreddit, and fanfiction on Archive of Our Own were
analyzed for changes in character mentions, centrality measures from
co-occurrence networks, and semantic associations. While fandom elevates
secondary characters as found in past work, the two fan communities prioritize
different subsets of characters. Word embedding tests reveal starkly different
associations of the same characters between communities on the gendered
concepts of femininity/masculinity, cruelty, and beauty. Furthermore,
fanfiction descriptions of a male character analyzed between romance pairings
scored higher for feminine-coded characteristics in male-male romance, matching
past qualitative theorizing. The results high-light the potential for
computational methods to assist in capturing the re-conceptualization of
narrative elements across communities and in supporting qualitative research on
fandom.",2024-09-17,Bianca N. Y. Kang,http://arxiv.org/pdf/2409.11170v1,cs.CL
SAGED: A Holistic Bias-Benchmarking Pipeline for Language Models with Customisable Fairness Calibration,"The development of unbiased large language models is widely recognized as
crucial, yet existing benchmarks fall short in detecting biases due to limited
scope, contamination, and lack of a fairness baseline. SAGED(bias) is the first
holistic benchmarking pipeline to address these problems. The pipeline
encompasses five core stages: scraping materials, assembling benchmarks,
generating responses, extracting numeric features, and diagnosing with
disparity metrics. SAGED includes metrics for max disparity, such as impact
ratio, and bias concentration, such as Max Z-scores. Noticing that metric tool
bias and contextual bias in prompts can distort evaluation, SAGED implements
counterfactual branching and baseline calibration for mitigation. For
demonstration, we use SAGED on G20 Countries with popular 8b-level models
including Gemma2, Llama3.1, Mistral, and Qwen2. With sentiment analysis, we
find that while Mistral and Qwen2 show lower max disparity and higher bias
concentration than Gemma2 and Llama3.1, all models are notably biased against
countries like Russia and (except for Qwen2) China. With further experiments to
have models role-playing U.S. presidents, we see bias amplifies and shifts in
heterogeneous directions. Moreover, we see Qwen2 and Mistral not engage in
role-playing, while Llama3.1 and Gemma2 role-play Trump notably more
intensively than Biden and Harris, indicating role-playing performance bias in
these models.",2024-09-17,"Xin Guan, Ze Wang, Nathaniel Demchak, Saloni Gupta, Ediz Ertekin Jr., Adriano Koshiyama, Emre Kazim, Zekun Wu",http://arxiv.org/pdf/2409.11149v7,cs.CL
Improving the Efficiency of Visually Augmented Language Models,"Despite the impressive performance of autoregressive Language Models (LM) it
has been shown that due to reporting bias, LMs lack visual knowledge, i.e. they
do not know much about the visual world and its properties. To augment LMs with
visual knowledge, existing solutions often rely on explicit images, requiring
time-consuming retrieval or image generation systems. This paper shows that
explicit images are not necessary to visually augment an LM. Instead, we use
visually-grounded text representations obtained from the well-known CLIP
multimodal system. For a fair comparison, we modify VALM, a visually-augmented
LM which uses image retrieval and representation, to work directly with
visually-grounded text representations. We name this new model BLIND-VALM. We
show that BLIND-VALM performs on par with VALM for Visual Language
Understanding (VLU), Natural Language Understanding (NLU) and Language Modeling
tasks, despite being significantly more efficient and simpler. We also show
that scaling up our model within the compute budget of VALM, either increasing
the model or pre-training corpus size, we outperform VALM for all the
evaluation tasks.",2024-09-17,"Paula Ontalvilla, Aitor Ormazabal, Gorka Azkune",http://arxiv.org/pdf/2409.11148v3,cs.CL
Reasoning Graph Enhanced Exemplars Retrieval for In-Context Learning,"Large language models (LLMs) have exhibited remarkable few-shot learning
capabilities and unified the paradigm of NLP tasks through the in-context
learning (ICL) technique. Despite the success of ICL, the quality of the
exemplar demonstrations can significantly influence the LLM's performance.
Existing exemplar selection methods mainly focus on the semantic similarity
between queries and candidate exemplars. On the other hand, the logical
connections between reasoning steps can be beneficial to depict the
problem-solving process as well. In this paper, we proposes a novel method
named Reasoning Graph-enhanced Exemplar Retrieval (RGER). RGER first quires LLM
to generate an initial response, then expresses intermediate problem-solving
steps to a graph structure. After that, it employs graph kernel to select
exemplars with semantic and structural similarity. Extensive experiments
demonstrate the structural relationship is helpful to the alignment of queries
and candidate exemplars. The efficacy of RGER on math and logit reasoning tasks
showcases its superiority over state-of-the-art retrieval-based approaches. Our
code is released at https://github.com/Yukang-Lin/RGER.",2024-09-17,"Yukang Lin, Bingchen Zhong, Shuoran Jiang, Joanna Siebert, Qingcai Chen",http://arxiv.org/pdf/2409.11147v2,cs.CL
Semformer: Transformer Language Models with Semantic Planning,"Next-token prediction serves as the dominant component in current neural
language models. During the training phase, the model employs teacher forcing,
which predicts tokens based on all preceding ground truth tokens. However, this
approach has been found to create shortcuts, utilizing the revealed prefix to
spuriously fit future tokens, potentially compromising the accuracy of the
next-token predictor. In this paper, we introduce Semformer, a novel method of
training a Transformer language model that explicitly models the semantic
planning of response. Specifically, we incorporate a sequence of planning
tokens into the prefix, guiding the planning token representations to predict
the latent semantic representations of the response, which are induced by an
autoencoder. In a minimal planning task (i.e., graph path-finding), our model
exhibits near-perfect performance and effectively mitigates shortcut learning,
a feat that standard training methods and baseline models have been unable to
accomplish. Furthermore, we pretrain Semformer from scratch with 125M
parameters, demonstrating its efficacy through measures of perplexity,
in-context learning, and fine-tuning on summarization tasks.",2024-09-17,"Yongjing Yin, Junran Ding, Kai Song, Yue Zhang",http://arxiv.org/pdf/2409.11143v1,cs.CL
SC-Phi2: A Fine-tuned Small Language Model for StarCraft II Macromanagement Tasks,"This paper introduces SC-Phi2, a fine-tuned StarCraft II small language model
for macromanagement tasks. Small language models, like Phi2, Gemma, and
DistilBERT, are streamlined versions of large language models (LLMs) with fewer
parameters that require less power and memory to run. To teach Microsoft's Phi2
model about StarCraft, we create a new SC2 text dataset with information about
StarCraft races, roles, and actions and use it to fine-tune Phi-2 with
self-supervised learning. We pair this language model with a Vision Transformer
(ViT) from the pre-trained BLIP-2 (Bootstrapping Language Image Pre-training)
model, fine-tuning it on the MSC replay dataset. This enables us to construct
dynamic prompts that include visual game state information. Unlike the large
models used in StarCraft LLMs such as GPT-3.5, Phi2 is trained primarily on
textbook data and contains little inherent knowledge of StarCraft II beyond
what is provided by our training process. By using LoRA (Low-rank Adaptation)
and quantization, our model can be trained on a single GPU. We demonstrate that
our model performs well at micromanagement tasks such as build order and global
state prediction with a small number of parameters.",2024-09-17,"Muhammad Junaid Khan, Gita Sukthankar",http://arxiv.org/pdf/2409.18989v1,cs.CL
Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models,"Instruction-tuned language models (LM) are able to respond to imperative
commands, providing a more natural user interface compared to their base
counterparts. In this work, we present Promptriever, the first retrieval model
able to be prompted like an LM. To train Promptriever, we curate and release a
new instance-level instruction training set from MS MARCO, spanning nearly 500k
instances. Promptriever not only achieves strong performance on standard
retrieval tasks, but also follows instructions. We observe: (1) large gains
(reaching SoTA) on following detailed relevance instructions (+14.3 p-MRR /
+3.1 nDCG on FollowIR), (2) significantly increased robustness to lexical
choices/phrasing in the query+instruction (+12.9 Robustness@10 on InstructIR),
and (3) the ability to perform hyperparameter search via prompting to reliably
improve retrieval performance (+1.4 average increase on BEIR). Promptriever
demonstrates that retrieval models can be controlled with prompts on a
per-query basis, setting the stage for future work aligning LM prompting
techniques with information retrieval.",2024-09-17,"Orion Weller, Benjamin Van Durme, Dawn Lawrie, Ashwin Paranjape, Yuhao Zhang, Jack Hessel",http://arxiv.org/pdf/2409.11136v1,cs.CL
Diversity-grounded Channel Prototypical Learning for Out-of-Distribution Intent Detection,"In the realm of task-oriented dialogue systems, a robust intent detection
mechanism must effectively handle malformed utterances encountered in
real-world scenarios. This study presents a novel fine-tuning framework for
large language models (LLMs) aimed at enhancing in-distribution (ID) intent
classification and out-of-distribution (OOD) intent detection, which utilizes
semantic matching with prototypes derived from ID class names. By harnessing
the highly distinguishable representations of LLMs, we construct semantic
prototypes for each ID class using a diversity-grounded prompt tuning approach.
We rigorously test our framework in a challenging OOD context, where ID and OOD
classes are semantically close yet distinct, referred to as \emph{near} OOD
detection. For a thorough assessment, we benchmark our method against the
prevalent fine-tuning approaches. The experimental findings reveal that our
method demonstrates superior performance in both few-shot ID intent
classification and near-OOD intent detection tasks.",2024-09-17,"Bo Liu, Liming Zhan, Yujie Feng, Zexin Lu, Chengqiang Xie, Lei Xue, Albert Y. S. Lam, Xiao-Ming Wu",http://arxiv.org/pdf/2409.11114v2,cs.CL
Strategic Insights in Human and Large Language Model Tactics at Word Guessing Games,"At the beginning of 2022, a simplistic word-guessing game took the world by
storm and was further adapted to many languages beyond the original English
version. In this paper, we examine the strategies of daily word-guessing game
players that have evolved during a period of over two years. A survey gathered
from 25% of frequent players reveals their strategies and motivations for
continuing the daily journey. We also explore the capability of several popular
open-access large language model systems and open-source models at
comprehending and playing the game in two different languages. Results
highlight the struggles of certain models to maintain correct guess length and
generate repetitions, as well as hallucinations of non-existent words and
inflections.",2024-09-17,"Matīss Rikters, Sanita Reinsone",http://arxiv.org/pdf/2409.11112v2,cs.CL
RoMath: A Mathematical Reasoning Benchmark in Romanian,"Mathematics has long been conveyed through natural language, primarily for
human understanding. With the rise of mechanized mathematics and proof
assistants, there is a growing need to understand informal mathematical text,
yet most existing benchmarks focus solely on English, overlooking other
languages. This paper introduces RoMath, a Romanian mathematical reasoning
benchmark suite comprising three subsets: Baccalaureate, Competitions and
Synthetic, which cover a range of mathematical domains and difficulty levels,
aiming to improve non-English language models and promote multilingual AI
development. By focusing on Romanian, a low-resource language with unique
linguistic features, RoMath addresses the limitations of Anglo-centric models
and emphasizes the need for dedicated resources beyond simple automatic
translation. We benchmark several open-weight language models, highlighting the
importance of creating resources for underrepresented languages. Code and
datasets are be made available.",2024-09-17,"Adrian Cosma, Ana-Maria Bucur, Emilian Radoi",http://arxiv.org/pdf/2409.11074v3,cs.CL
KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models,"The bottleneck associated with the key-value(KV) cache presents a significant
challenge during the inference processes of large language models. While depth
pruning accelerates inference, it requires extensive recovery training, which
can take up to two weeks. On the other hand, width pruning retains much of the
performance but offers slight speed gains. To tackle these challenges, we
propose KVPruner to improve model efficiency while maintaining performance. Our
method uses global perplexity-based analysis to determine the importance ratio
for each block and provides multiple strategies to prune non-essential KV
channels within blocks. Compared to the original model, KVPruner reduces
runtime memory usage by 50% and boosts throughput by over 35%. Additionally,
our method requires only two hours of LoRA fine-tuning on small datasets to
recover most of the performance.",2024-09-17,"Bo Lv, Quan Zhou, Xuanang Ding, Yan Wang, Zeming Ma",http://arxiv.org/pdf/2409.11057v1,cs.CL
Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts,"With the advent of Large Language Models (LLMs), generating rule-based data
for real-world applications has become more accessible. Due to the inherent
ambiguity of natural language and the complexity of rule sets, especially in
long contexts, LLMs often struggle to follow all specified rules, frequently
omitting at least one. To enhance the reasoning and understanding of LLMs on
long and complex contexts, we propose a novel prompting strategy Multi-Lingual
Prompt, namely MLPrompt, which automatically translates the error-prone rule
that an LLM struggles to follow into another language, thus drawing greater
attention to it. Experimental results on public datasets across various tasks
have shown MLPrompt can outperform state-of-the-art prompting methods such as
Chain of Thought, Tree of Thought, and Self-Consistency. Additionally, we
introduce a framework integrating MLPrompt with an auto-checking mechanism for
structured data generation, with a specific case study in text-to-MIP
instances. Further, we extend the proposed framework for text-to-SQL to
demonstrate its generation ability towards structured data synthesis.",2024-09-17,"Teng Wang, Zhenqi He, Wing-Yin Yu, Xiaojin Fu, Xiongwei Han",http://arxiv.org/pdf/2409.11056v2,cs.CL
"Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant","Quantization has gained attention as a promising solution for the
cost-effective deployment of large and small language models. However, most
prior work has been limited to perplexity or basic knowledge tasks and lacks a
comprehensive evaluation of recent models like Llama-3.3. In this paper, we
conduct a comprehensive evaluation of instruction-tuned models spanning 1B to
405B parameters, applying four quantization methods across 13 datasets. Our
findings reveal that (1) quantized models generally surpass smaller FP16
baselines, yet they often struggle with instruction-following and hallucination
detection; (2) FP8 consistently emerges as the most robust option across tasks,
and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller
models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale
models maintain stable performance; (4) notably, \textit{hard} tasks do not
always experience the largest accuracy losses, indicating that quantization
magnifies a model's inherent weaknesses rather than simply correlating with
task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant
performance declines in Coding and STEM tasks, though it occasionally reports
improvements in reasoning.",2024-09-17,"Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon",http://arxiv.org/pdf/2409.11055v5,cs.CL
Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming,"While there has been a lot of research recently on robots in household
environments, at the present time, most robots in existence can be found on
shop floors, and most interactions between humans and robots happen there.
``Collaborative robots'' (cobots) designed to work alongside humans on assembly
lines traditionally require expert programming, limiting ability to make
changes, or manual guidance, limiting expressivity of the resulting programs.
To address these limitations, we explore using Large Language Models (LLMs),
and in particular, their abilities of doing in-context learning, for
conversational code generation. As a first step, we define RATS, the
``Repetitive Assembly Task'', a 2D building task designed to lay the foundation
for simulating industry assembly scenarios. In this task, a `programmer'
instructs a cobot, using natural language, on how a certain assembly is to be
built; that is, the programmer induces a program, through natural language. We
create a dataset that pairs target structures with various example instructions
(human-authored, template-based, and model-generated) and example code. With
this, we systematically evaluate the capabilities of state-of-the-art LLMs for
synthesising this kind of code, given in-context examples. Evaluating in a
simulated environment, we find that LLMs are capable of generating accurate
`first order code' (instruction sequences), but have problems producing
`higher-order code' (abstractions such as functions, or use of loops).",2024-09-17,"Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen",http://arxiv.org/pdf/2409.11041v2,cs.CL
Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI,"Written texts reflect an author's perspective, making the thorough analysis
of literature a key research method in fields such as the humanities and social
sciences. However, conventional text mining techniques like sentiment analysis
and topic modeling are limited in their ability to capture the hierarchical
narrative structures that reveal deeper argumentative patterns. To address this
gap, we propose a method that leverages large language models (LLMs) to extract
and organize these structures into a hierarchical framework. We validate this
approach by analyzing public opinions on generative AI collected by Japan's
Agency for Cultural Affairs, comparing the narratives of supporters and
critics. Our analysis provides clearer visualization of the factors influencing
divergent opinions on generative AI, offering deeper insights into the
structures of agreement and disagreement.",2024-09-17,"Riona Matsuoka, Hiroki Matsumoto, Takahiro Yoshida, Tomohiro Watanabe, Ryoma Kondo, Ryohei Hisano",http://arxiv.org/pdf/2409.11032v3,cs.CL
"DynamicNER: A Dynamic, Multilingual, and Fine-Grained Dataset for LLM-based Named Entity Recognition","With the advancement of Large Language Models (LLMs), more and more
researchers apply LLMs for Named Entity Recognition (NER) methods, bringing
vitality to this classical Natural Language Processing task. However, existing
datasets are designed for traditional machine learning methods, inadequate for
LLM-based methods in terms of corpus selection, entity categorization, and
design logic. This limitation leads to less effective evaluation and model
fine-tuning. To address this issue, we propose DynamicNER, the first NER
dataset specifically designed for LLMs and with dynamic categorization,
transcending the limitations of fixed categorization in existing datasets. It
is also multi-lingual and multi-granular, covering 8 languages and 155 entity
types, with corpus spanning multiple specialized domains. Furthermore, in
response to the limitations demonstrated by existing LLM-based methods during
DynamicNER testing, we develop CascadeNER, a novel NER method based on a
two-stage strategy and lightweight LLMs, addressing the problems in current
methods. Experiments show that DynamicNER is an effective benchmark for
LLM-based NER methods, and CascadeNER outperforms existing methods with fewer
computational resources. Our work is opened at
https://github.com/CascadeNER/CascadeNER.",2024-09-17,"Hanjun Luo, Yingbin Jin, Xinfeng Li, Xuecheng Liu, Ruizhe Chen, Tong Shang, Kun Wang, Qingsong Wen, Zuozhu Liu",http://arxiv.org/pdf/2409.11022v4,cs.CL
CAST: Cross-modal Alignment Similarity Test for Vision Language Models,"Vision Language Models (VLMs) are typically evaluated with Visual Question
Answering (VQA) tasks which assess a model's understanding of scenes. Good VQA
performance is taken as evidence that the model will perform well on a broader
range of tasks that require both visual and language inputs. However,
scene-aware VQA does not fully capture input biases or assess hallucinations
caused by a misalignment between modalities. To address this, we propose a
Cross-modal Alignment Similarity Test (CAST) to probe VLMs for self-consistency
across modalities. This test involves asking the models to identify
similarities between two scenes through text-only, image-only, or both and then
assess the truthfulness of the similarities they generate. Since there is no
ground-truth to compare against, this evaluation does not focus on objective
accuracy but rather on whether VLMs are internally consistent in their outputs.
We argue that while not all self-consistent models are capable or accurate, all
capable VLMs must be self-consistent.",2024-09-17,"Gautier Dagan, Olga Loginova, Anil Batra",http://arxiv.org/pdf/2409.11007v1,cs.CL
Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models,"Audio language models process audio inputs using textual prompts for tasks
like speech recognition and audio captioning. Although built on multilingual
pre-trained components, most are trained primarily on English, limiting their
usability for other languages. This paper evaluates audio language models on
Thai, a low-resource language, and finds that they lack emergent cross-lingual
abilities despite their multilingual foundations. To address this, we explore
data mixtures that optimize audio language models for both a target language
and English while integrating audio comprehension and speech
instruction-following into a unified model. Our experiments provide insights
into improving instruction-following in low-resource languages by balancing
language-specific and multilingual training data. The proposed model,
Typhoon-Audio, significantly outperforms existing open-source models and
achieves performance comparable to state-of-the-art Gemini-1.5-Pro in both
English and Thai.",2024-09-17,"Potsawee Manakul, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, Kunat Pipatanakul",http://arxiv.org/pdf/2409.10999v2,cs.CL
Contextual Breach: Assessing the Robustness of Transformer-based QA Models,"Contextual question-answering models are susceptible to adversarial
perturbations to input context, commonly observed in real-world scenarios.
These adversarial noises are designed to degrade the performance of the model
by distorting the textual input. We introduce a unique dataset that
incorporates seven distinct types of adversarial noise into the context, each
applied at five different intensity levels on the SQuAD dataset. To quantify
the robustness, we utilize robustness metrics providing a standardized measure
for assessing model performance across varying noise types and levels.
Experiments on transformer-based question-answering models reveal robustness
vulnerabilities and important insights into the model's performance in
realistic textual input.",2024-09-17,"Asir Saadat, Nahian Ibn Asad, Md Farhan Ishmam",http://arxiv.org/pdf/2409.10997v3,cs.CL
Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs,"The rapid advancement of Multimodal Large Language Models (MLLMs) has led to
remarkable performances across various domains. However, this progress is
accompanied by a substantial surge in the resource consumption of these models.
We address this pressing issue by introducing a new approach, Token Reduction
using CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without
sacrificing their performance. Inspired by human attention patterns in Visual
Question Answering (VQA) tasks, TRIM presents a fresh perspective on the
selection and reduction of image tokens. The TRIM method has been extensively
tested across 12 datasets, and the results demonstrate a significant reduction
in computational overhead while maintaining a consistent level of performance.
This research marks a critical stride in efficient MLLM development, promoting
greater accessibility and sustainability of high-performing models.",2024-09-17,"Dingjie Song, Wenjun Wang, Shunian Chen, Xidong Wang, Michael Guan, Benyou Wang",http://arxiv.org/pdf/2409.10994v3,cs.CL
GOSt-MT: A Knowledge Graph for Occupation-related Gender Biases in Machine Translation,"Gender bias in machine translation (MT) systems poses significant challenges
that often result in the reinforcement of harmful stereotypes. Especially in
the labour domain where frequently occupations are inaccurately associated with
specific genders, such biases perpetuate traditional gender stereotypes with a
significant impact on society. Addressing these issues is crucial for ensuring
equitable and accurate MT systems. This paper introduces a novel approach to
studying occupation-related gender bias through the creation of the GOSt-MT
(Gender and Occupation Statistics for Machine Translation) Knowledge Graph.
GOSt-MT integrates comprehensive gender statistics from real-world labour data
and textual corpora used in MT training. This Knowledge Graph allows for a
detailed analysis of gender bias across English, French, and Greek,
facilitating the identification of persistent stereotypes and areas requiring
intervention. By providing a structured framework for understanding how
occupations are gendered in both labour markets and MT systems, GOSt-MT
contributes to efforts aimed at making MT systems more equitable and reducing
gender biases in automated translations.",2024-09-17,"Orfeas Menis Mastromichalakis, Giorgos Filandrianos, Eva Tsouparopoulou, Dimitris Parsanoglou, Maria Symeonaki, Giorgos Stamou",http://arxiv.org/pdf/2409.10989v2,cs.CL
Improving Speech Emotion Recognition in Under-Resourced Languages via Speech-to-Speech Translation with Bootstrapping Data Selection,"Speech Emotion Recognition (SER) is a crucial component in developing
general-purpose AI agents capable of natural human-computer interaction.
However, building robust multilingual SER systems remains challenging due to
the scarcity of labeled data in languages other than English and Chinese. In
this paper, we propose an approach to enhance SER performance in low SER
resource languages by leveraging data from high-resource languages.
Specifically, we employ expressive Speech-to-Speech translation (S2ST) combined
with a novel bootstrapping data selection pipeline to generate labeled data in
the target language. Extensive experiments demonstrate that our method is both
effective and generalizable across different upstream models and languages. Our
results suggest that this approach can facilitate the development of more
scalable and robust multilingual SER systems.",2024-09-17,"Hsi-Che Lin, Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee",http://arxiv.org/pdf/2409.10985v2,cs.CL
Enhancing Multilingual Speech Generation and Recognition Abilities in LLMs with Constructed Code-switched Data,"While large language models (LLMs) have been explored in the speech domain
for both generation and recognition tasks, their applications are predominantly
confined to the monolingual scenario, with limited exploration in multilingual
and code-switched (CS) contexts. Additionally, speech generation and
recognition tasks are often handled separately, such as VALL-E and Qwen-Audio.
In this paper, we propose a MutltiLingual MultiTask (MLMT) model, integrating
multilingual speech generation and recognition tasks within the single LLM.
Furthermore, we develop an effective data construction approach that splits and
concatenates words from different languages to equip LLMs with CS synthesis
ability without relying on CS data. The experimental results demonstrate that
our model outperforms other baselines with a comparable data scale.
Furthermore, our data construction approach not only equips LLMs with CS speech
synthesis capability with comparable speaker consistency and similarity to any
given speaker, but also improves the performance of LLMs in multilingual speech
generation and recognition tasks.",2024-09-17,"Jing Xu, Daxin Tan, Jiaqi Wang, Xiao Chen",http://arxiv.org/pdf/2409.10969v1,cs.CL
Cross-lingual transfer of multilingual models on low resource African Languages,"Large multilingual models have significantly advanced natural language
processing (NLP) research. However, their high resource demands and potential
biases from diverse data sources have raised concerns about their effectiveness
across low-resource languages. In contrast, monolingual models, trained on a
single language, may better capture the nuances of the target language,
potentially providing more accurate results. This study benchmarks the
cross-lingual transfer capabilities from a high-resource language to a
low-resource language for both, monolingual and multilingual models, focusing
on Kinyarwanda and Kirundi, two Bantu languages. We evaluate the performance of
transformer based architectures like Multilingual BERT (mBERT), AfriBERT, and
BantuBERTa against neural-based architectures such as BiGRU, CNN, and char-CNN.
The models were trained on Kinyarwanda and tested on Kirundi, with fine-tuning
applied to assess the extent of performance improvement and catastrophic
forgetting. AfriBERT achieved the highest cross-lingual accuracy of 88.3% after
fine-tuning, while BiGRU emerged as the best-performing neural model with 83.3%
accuracy. We also analyze the degree of forgetting in the original language
post-fine-tuning. While monolingual models remain competitive, this study
highlights that multilingual models offer strong cross-lingual transfer
capabilities in resource limited settings.",2024-09-17,"Harish Thangaraj, Ananya Chenat, Jaskaran Singh Walia, Vukosi Marivate",http://arxiv.org/pdf/2409.10965v1,cs.CL
Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style,"Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by
incorporating external information into the response generation process.
However, how context-faithful LLMs are and what factors influence LLMs'
context-faithfulness remain largely unexplored. In this study, we investigate
the impact of memory strength and evidence presentation on LLMs' receptiveness
to external evidence. We introduce a method to quantify the memory strength of
LLMs by measuring the divergence in LLMs' responses to different paraphrases of
the same question, which is not considered by previous works. We also generate
evidence in various styles to evaluate the effects of evidence in different
styles. Two datasets are used for evaluation: Natural Questions (NQ) with
popular questions and popQA featuring long-tail questions. Our results show
that for questions with high memory strength, LLMs are more likely to rely on
internal memory, particularly for larger LLMs such as GPT-4. On the other hand,
presenting paraphrased evidence significantly increases LLMs' receptiveness
compared to simple repetition or adding details.",2024-09-17,"Yuepei Li, Kang Zhou, Qiao Qiao, Bach Nguyen, Qing Wang, Qi Li",http://arxiv.org/pdf/2409.10955v1,cs.CL
Propulsion: Steering LLM with Tiny Fine-Tuning,"The rapid advancements in Large Language Models (LLMs) have revolutionized
natural language processing (NLP) and related fields. However, fine-tuning
these models for specific tasks remains computationally expensive and risks
degrading pre-learned features. To address these challenges, we propose
Propulsion, a novel parameter efficient fine-tuning (PEFT) method designed to
optimize task-specific performance while drastically reducing computational
overhead. Inspired by the concept of controlled adjustments in physical motion,
Propulsion selectively re-scales specific dimensions of a pre-trained model,
guiding output predictions toward task objectives without modifying the model's
parameters. By introducing lightweight, trainable Propulsion parameters at the
pre-trained layer, we minimize the number of parameters updated during
fine-tuning, preventing overfitting or overwriting of existing knowledge. Our
theoretical analysis, supported by Neural Tangent Kernel (NTK) theory, shows
that Propulsion approximates the performance of full fine-tuning with far fewer
trainable parameters. Empirically, Propulsion reduces the parameter count from
355.3 million to just 0.086 million, achieving over a 10x reduction compared to
standard approaches like LoRA while maintaining competitive performance across
benchmarks.",2024-09-17,"Md Kowsher, Nusrat Jahan Prottasha, Prakash Bhat",http://arxiv.org/pdf/2409.10927v3,cs.CL
GenCRF: Generative Clustering and Reformulation Framework for Enhanced Intent-Driven Information Retrieval,"Query reformulation is a well-known problem in Information Retrieval (IR)
aimed at enhancing single search successful completion rate by automatically
modifying user's input query. Recent methods leverage Large Language Models
(LLMs) to improve query reformulation, but often generate limited and redundant
expansions, potentially constraining their effectiveness in capturing diverse
intents. In this paper, we propose GenCRF: a Generative Clustering and
Reformulation Framework to capture diverse intentions adaptively based on
multiple differentiated, well-generated queries in the retrieval phase for the
first time. GenCRF leverages LLMs to generate variable queries from the initial
query using customized prompts, then clusters them into groups to distinctly
represent diverse intents. Furthermore, the framework explores to combine
diverse intents query with innovative weighted aggregation strategies to
optimize retrieval performance and crucially integrates a novel Query
Evaluation Rewarding Model (QERM) to refine the process through feedback loops.
Empirical experiments on the BEIR benchmark demonstrate that GenCRF achieves
state-of-the-art performance, surpassing previous query reformulation SOTAs by
up to 12% on nDCG@10. These techniques can be adapted to various LLMs,
significantly boosting retriever performance and advancing the field of
Information Retrieval.",2024-09-17,"Wonduk Seo, Haojie Zhang, Yueyang Zhang, Changhao Zhang, Songyao Duan, Lixin Su, Daiting Shi, Jiashu Zhao, Dawei Yin",http://arxiv.org/pdf/2409.10909v1,cs.CL
Attention-Seeker: Dynamic Self-Attention Scoring for Unsupervised Keyphrase Extraction,"This paper proposes Attention-Seeker, an unsupervised keyphrase extraction
method that leverages self-attention maps from a Large Language Model to
estimate the importance of candidate phrases. Our approach identifies specific
components - such as layers, heads, and attention vectors - where the model
pays significant attention to the key topics of the text. The attention weights
provided by these components are then used to score the candidate phrases.
Unlike previous models that require manual tuning of parameters (e.g.,
selection of heads, prompts, hyperparameters), Attention-Seeker dynamically
adapts to the input text without any manual adjustments, enhancing its
practical applicability. We evaluate Attention-Seeker on four publicly
available datasets: Inspec, SemEval2010, SemEval2017, and Krapivin. Our results
demonstrate that, even without parameter tuning, Attention-Seeker outperforms
most baseline models, achieving state-of-the-art performance on three out of
four datasets, particularly excelling in extracting keyphrases from long
documents.",2024-09-17,"Erwin D. López Z., Cheng Tang, Atsushi Shimada",http://arxiv.org/pdf/2409.10907v2,cs.CL
A Joint Spectro-Temporal Relational Thinking Based Acoustic Modeling Framework,"Relational thinking refers to the inherent ability of humans to form mental
impressions about relations between sensory signals and prior knowledge, and
subsequently incorporate them into their model of their world. Despite the
crucial role relational thinking plays in human understanding of speech, it has
yet to be leveraged in any artificial speech recognition systems. Recently,
there have been some attempts to correct this oversight, but these have been
limited to coarse utterance-level models that operate exclusively in the time
domain. In an attempt to narrow the gap between artificial systems and human
abilities, this paper presents a novel spectro-temporal relational thinking
based acoustic modeling framework. Specifically, it first generates numerous
probabilistic graphs to model the relationships among speech segments across
both time and frequency domains. The relational information rooted in every
pair of nodes within these graphs is then aggregated and embedded into latent
representations that can be utilized by downstream tasks. Models built upon
this framework outperform state-of-the-art systems with a 7.82\% improvement in
phoneme recognition tasks over the TIMIT dataset. In-depth analyses further
reveal that our proposed relational thinking modeling mainly improves the
model's ability to recognize vowels, which are the most likely to be confused
by phoneme recognizers.",2024-09-17,"Zheng Nan, Ting Dang, Vidhyasaharan Sethu, Beena Ahmed",http://arxiv.org/pdf/2409.15357v1,cs.CL
A Unified Framework to Classify Business Activities into International Standard Industrial Classification through Large Language Models for Circular Economy,"Effective information gathering and knowledge codification are pivotal for
developing recommendation systems that promote circular economy practices. One
promising approach involves the creation of a centralized knowledge repository
cataloguing historical waste-to-resource transactions, which subsequently
enables the generation of recommendations based on past successes. However, a
significant barrier to constructing such a knowledge repository lies in the
absence of a universally standardized framework for representing business
activities across disparate geographical regions. To address this challenge,
this paper leverages Large Language Models (LLMs) to classify textual data
describing economic activities into the International Standard Industrial
Classification (ISIC), a globally recognized economic activity classification
framework. This approach enables any economic activity descriptions provided by
businesses worldwide to be categorized into the unified ISIC standard,
facilitating the creation of a centralized knowledge repository. Our approach
achieves a 95% accuracy rate on a 182-label test dataset with fine-tuned GPT-2
model. This research contributes to the global endeavour of fostering
sustainable circular economy practices by providing a standardized foundation
for knowledge codification and recommendation systems deployable across
regions.",2024-09-17,"Xiang Li, Lan Zhao, Junhao Ren, Yajuan Sun, Chuan Fu Tan, Zhiquan Yeo, Gaoxi Xiao",http://arxiv.org/pdf/2409.18988v1,cs.CL
CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation for Meeting Summarization,"Large Language Models (LLMs) have spurred interest in automatic evaluation
methods for summarization, offering a faster, more cost-effective alternative
to human evaluation. However, existing methods often fall short when applied to
complex tasks like long-context summarizations and dialogue-based meeting
summarizations. In this paper, we introduce CREAM (Comparison-Based
Reference-Free Elo-Ranked Automatic Evaluation for Meeting Summarization), a
novel framework that addresses the unique challenges of evaluating meeting
summaries. CREAM leverages a combination of chain-of-thought reasoning and key
facts alignment to assess conciseness and completeness of model-generated
summaries without requiring reference. By employing an ELO ranking system, our
approach provides a robust mechanism for comparing the quality of different
models or prompt configurations.",2024-09-17,"Ziwei Gong, Lin Ai, Harshsaiprasad Deshpande, Alexander Johnson, Emmy Phung, Zehui Wu, Ahmad Emami, Julia Hirschberg",http://arxiv.org/pdf/2409.10883v1,cs.CL
American Sign Language to Text Translation using Transformer and Seq2Seq with LSTM,"Sign language translation is one of the important issues in communication
between deaf and hearing people, as it expresses words through hand, body, and
mouth movements. American Sign Language is one of the sign languages used, one
of which is the alphabetic sign. The development of neural machine translation
technology is moving towards sign language translation. Transformer became the
state-of-the-art in natural language processing. This study compares the
Transformer with the Sequence-to-Sequence (Seq2Seq) model in translating sign
language to text. In addition, an experiment was conducted by adding Residual
Long Short-Term Memory (ResidualLSTM) in the Transformer. The addition of
ResidualLSTM to the Transformer reduces the performance of the Transformer
model by 23.37% based on the BLEU Score value. In comparison, the Transformer
itself increases the BLEU Score value by 28.14 compared to the Seq2Seq model.",2024-09-17,"Gregorius Guntur Sunardi Putra, Adifa Widyadhani Chanda D'Layla, Dimas Wahono, Riyanarto Sarno, Agus Tri Haryono",http://arxiv.org/pdf/2409.10874v1,cs.CL
Adaptive Large Language Models By Layerwise Attention Shortcuts,"Transformer architectures are the backbone of the modern AI revolution.
However, they are based on simply stacking the same blocks in dozens of layers
and processing information sequentially from one block to another. In this
paper, we propose to challenge this and introduce adaptive computations for
LLM-like setups, which allow the final layer to attend to all of the
intermediate layers as it deems fit through the attention mechanism, thereby
introducing computational \textbf{attention shortcuts}. These shortcuts can
thus make the architecture depth and context adaptive. We showcase four
different datasets, namely acoustic tokens, natural language, and symbolic
music, and we achieve superior performance for GPT-like architecture. We give
evidence via attention maps that the models learn complex dependencies across
layers that are adaptive in context and depth depending on the input tokens.",2024-09-17,"Prateek Verma, Mert Pilanci",http://arxiv.org/pdf/2409.10870v1,cs.CL
Jailbreaking Large Language Models with Symbolic Mathematics,"Recent advancements in AI safety have led to increased efforts in training
and red-teaming large language models (LLMs) to mitigate unsafe content
generation. However, these safety mechanisms may not be comprehensive, leaving
potential vulnerabilities unexplored. This paper introduces MathPrompt, a novel
jailbreaking technique that exploits LLMs' advanced capabilities in symbolic
mathematics to bypass their safety mechanisms. By encoding harmful natural
language prompts into mathematical problems, we demonstrate a critical
vulnerability in current AI safety measures. Our experiments across 13
state-of-the-art LLMs reveal an average attack success rate of 73.6\%,
highlighting the inability of existing safety training mechanisms to generalize
to mathematically encoded inputs. Analysis of embedding vectors shows a
substantial semantic shift between original and encoded prompts, helping
explain the attack's success. This work emphasizes the importance of a holistic
approach to AI safety, calling for expanded red-teaming efforts to develop
robust safeguards across all potential input types and their associated risks.",2024-09-17,"Emet Bethany, Mazal Bethany, Juan Arturo Nolazco Flores, Sumit Kumar Jha, Peyman Najafirad",http://arxiv.org/pdf/2409.11445v2,cs.CL
BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation,"Autoregressive models excel in modeling sequential dependencies by enforcing
causal constraints, yet they struggle to capture complex bidirectional patterns
due to their unidirectional nature. In contrast, mask-based models leverage
bidirectional context, enabling richer dependency modeling. However, they often
assume token independence during prediction, which undermines the modeling of
sequential dependencies. Additionally, the corruption of sequences through
masking or absorption can introduce unnatural distortions, complicating the
learning process. To address these issues, we propose Bidirectional
Autoregressive Diffusion (BAD), a novel approach that unifies the strengths of
autoregressive and mask-based generative models. BAD utilizes a
permutation-based corruption technique that preserves the natural sequence
structure while enforcing causal dependencies through randomized ordering,
enabling the effective capture of both sequential and bidirectional
relationships. Comprehensive experiments show that BAD outperforms
autoregressive and mask-based models in text-to-motion generation, suggesting a
novel pre-training strategy for sequence modeling. The codebase for BAD is
available on https://github.com/RohollahHS/BAD.",2024-09-17,"S. Rohollah Hosseyni, Ali Ahmad Rahmani, S. Jamal Seyedmohammadi, Sanaz Seyedin, Arash Mohammadi",http://arxiv.org/pdf/2409.10847v1,cs.CL
Efficient and Personalized Mobile Health Event Prediction via Small Language Models,"Healthcare monitoring is crucial for early detection, timely intervention,
and the ongoing management of health conditions, ultimately improving
individuals' quality of life. Recent research shows that Large Language Models
(LLMs) have demonstrated impressive performance in supporting healthcare tasks.
However, existing LLM-based healthcare solutions typically rely on cloud-based
systems, which raise privacy concerns and increase the risk of personal
information leakage. As a result, there is growing interest in running these
models locally on devices like mobile phones and wearables to protect users'
privacy. Small Language Models (SLMs) are potential candidates to solve privacy
and computational issues, as they are more efficient and better suited for
local deployment. However, the performance of SLMs in healthcare domains has
not yet been investigated. This paper examines the capability of SLMs to
accurately analyze health data, such as steps, calories, sleep minutes, and
other vital statistics, to assess an individual's health status. Our results
show that, TinyLlama, which has 1.1 billion parameters, utilizes 4.31 GB
memory, and has 0.48s latency, showing the best performance compared other four
state-of-the-art (SOTA) SLMs on various healthcare applications. Our results
indicate that SLMs could potentially be deployed on wearable or mobile devices
for real-time health monitoring, providing a practical solution for efficient
and privacy-preserving healthcare.",2024-09-17,"Xin Wang, Ting Dang, Vassilis Kostakos, Hong Jia",http://arxiv.org/pdf/2409.18987v1,cs.CL
ReXErr: Synthesizing Clinically Meaningful Errors in Diagnostic Radiology Reports,"Accurately interpreting medical images and writing radiology reports is a
critical but challenging task in healthcare. Both human-written and
AI-generated reports can contain errors, ranging from clinical inaccuracies to
linguistic mistakes. To address this, we introduce ReXErr, a methodology that
leverages Large Language Models to generate representative errors within chest
X-ray reports. Working with board-certified radiologists, we developed error
categories that capture common mistakes in both human and AI-generated reports.
Our approach uses a novel sampling scheme to inject diverse errors while
maintaining clinical plausibility. ReXErr demonstrates consistency across error
categories and produces errors that closely mimic those found in real-world
scenarios. This method has the potential to aid in the development and
evaluation of report correction algorithms, potentially enhancing the quality
and reliability of radiology reporting.",2024-09-17,"Vishwanatha M. Rao, Serena Zhang, Julian N. Acosta, Subathra Adithan, Pranav Rajpurkar",http://arxiv.org/pdf/2409.10829v1,cs.CL
Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering,"Large language models (LLMs) have demonstrated remarkable performance across
various real-world tasks. However, they often struggle to fully comprehend and
effectively utilize their input contexts, resulting in responses that are
unfaithful or hallucinated. This difficulty increases for contexts that are
long or contain distracting information, which can divert LLMs from fully
capturing essential evidence. To address this issue, many works use prompting
to help LLMs utilize contextual information more faithfully. For instance,
iterative prompting highlights key information in two steps that first ask the
LLM to identify important pieces of context and then derive answers
accordingly. However, prompting methods are constrained to highlighting key
information implicitly in token space, which is often insufficient to fully
steer the model's attention. To improve model faithfulness more reliably, we
propose AutoPASTA, a method that automatically identifies key contextual
information and explicitly highlights it by steering an LLM's attention scores.
Like prompting, AutoPASTA is applied at inference time and does not require
changing any model parameters. Our experiments on open-book QA demonstrate that
AutoPASTA effectively enables models to grasp essential contextual information,
leading to substantially improved model faithfulness and performance, e.g., an
average improvement of 7.95% for LLAMA3-70B-Instruct. Code will be publicly
available at https://github.com/QingruZhang/AutoPASTA .",2024-09-16,"Qingru Zhang, Xiaodong Yu, Chandan Singh, Xiaodong Liu, Liyuan Liu, Jianfeng Gao, Tuo Zhao, Dan Roth, Hao Cheng",http://arxiv.org/pdf/2409.10790v1,cs.CL
Predicting Punctuation in Ancient Chinese Texts: A Multi-Layered LSTM and Attention-Based Approach,"It was only until the 20th century when the Chinese language began using
punctuation. In fact, many ancient Chinese texts contain thousands of lines
with no distinct punctuation marks or delimiters in sight. The lack of
punctuation in such texts makes it difficult for humans to identify when there
pauses or breaks between particular phrases and understand the semantic meaning
of the written text (Mogahed, 2012). As a result, unless one was educated in
the ancient time period, many readers of ancient Chinese would have
significantly different interpretations of the texts. We propose an approach to
predict the location (and type) of punctuation in ancient Chinese texts that
extends the work of Oh et al (2017) by leveraging a bidirectional multi-layered
LSTM with a multi-head attention mechanism as inspired by Luong et al.'s (2015)
discussion of attention-based architectures. We find that the use of
multi-layered LSTMs and multi-head attention significantly outperforms RNNs
that don't incorporate such components when evaluating ancient Chinese texts.",2024-09-16,"Tracy Cai, Kimmy Chang, Fahad Nabi",http://arxiv.org/pdf/2409.10783v1,cs.CL
Semantics Preserving Emoji Recommendation with Large Language Models,"Emojis have become an integral part of digital communication, enriching text
by conveying emotions, tone, and intent. Existing emoji recommendation methods
are primarily evaluated based on their ability to match the exact emoji a user
chooses in the original text. However, they ignore the essence of users'
behavior on social media in that each text can correspond to multiple
reasonable emojis. To better assess a model's ability to align with such
real-world emoji usage, we propose a new semantics preserving evaluation
framework for emoji recommendation, which measures a model's ability to
recommend emojis that maintain the semantic consistency with the user's text.
To evaluate how well a model preserves semantics, we assess whether the
predicted affective state, demographic profile, and attitudinal stance of the
user remain unchanged. If these attributes are preserved, we consider the
recommended emojis to have maintained the original semantics. The advanced
abilities of Large Language Models (LLMs) in understanding and generating
nuanced, contextually relevant output make them well-suited for handling the
complexities of semantics preserving emoji recommendation. To this end, we
construct a comprehensive benchmark to systematically assess the performance of
six proprietary and open-source LLMs using different prompting techniques on
our task. Our experiments demonstrate that GPT-4o outperforms other LLMs,
achieving a semantics preservation score of 79.23%. Additionally, we conduct
case studies to analyze model biases in downstream classification tasks and
evaluate the diversity of the recommended emojis.",2024-09-16,"Zhongyi Qiu, Kangyi Qiu, Hanjia Lyu, Wei Xiong, Jiebo Luo",http://arxiv.org/pdf/2409.10760v1,cs.CL
NaviQAte: Functionality-Guided Web Application Navigation,"End-to-end web testing is challenging due to the need to explore diverse web
application functionalities. Current state-of-the-art methods, such as
WebCanvas, are not designed for broad functionality exploration; they rely on
specific, detailed task descriptions, limiting their adaptability in dynamic
web environments. We introduce NaviQAte, which frames web application
exploration as a question-and-answer task, generating action sequences for
functionalities without requiring detailed parameters. Our three-phase approach
utilizes advanced large language models like GPT-4o for complex decision-making
and cost-effective models, such as GPT-4o mini, for simpler tasks. NaviQAte
focuses on functionality-guided web application navigation, integrating
multi-modal inputs such as text and images to enhance contextual understanding.
Evaluations on the Mind2Web-Live and Mind2Web-Live-Abstracted datasets show
that NaviQAte achieves a 44.23% success rate in user task navigation and a
38.46% success rate in functionality navigation, representing a 15% and 33%
improvement over WebCanvas. These results underscore the effectiveness of our
approach in advancing automated web application testing.",2024-09-16,"Mobina Shahbandeh, Parsa Alian, Noor Nashid, Ali Mesbah",http://arxiv.org/pdf/2409.10741v1,cs.CL
Generalized Measures of Anticipation and Responsivity in Online Language Processing,"We introduce a generalization of classic information-theoretic measures of
predictive uncertainty in online language processing, based on the simulation
of expected continuations of incremental linguistic contexts. Our framework
provides a formal definition of anticipatory and responsive measures, and it
equips experimenters with the tools to define new, more expressive measures
beyond standard next-symbol entropy and surprisal. While extracting these
standard quantities from language models is convenient, we demonstrate that
using Monte Carlo simulation to estimate alternative responsive and
anticipatory measures pays off empirically: New special cases of our
generalized formula exhibit enhanced predictive power compared to surprisal for
human cloze completion probability as well as ELAN, LAN, and N400 amplitudes,
and greater complementarity with surprisal in predicting reading times.",2024-09-16,"Mario Giulianelli, Andreas Opedal, Ryan Cotterell",http://arxiv.org/pdf/2409.10728v2,cs.CL
Self-Attention Limits Working Memory Capacity of Transformer-Based Models,"Recent work on Transformer-based large language models (LLMs) has revealed
striking limits in their working memory capacity, similar to what has been
found in human behavioral studies. Specifically, these models' performance
drops significantly on N-back tasks as N increases. However, there is still a
lack of mechanistic interpretability as to why this phenomenon would arise.
Inspired by the executive attention theory from behavioral sciences, we
hypothesize that the self-attention mechanism within Transformer-based models
might be responsible for their working memory capacity limits. To test this
hypothesis, we train vanilla decoder-only transformers to perform N-back tasks
and find that attention scores gradually aggregate to the N-back positions over
training, suggesting that the model masters the task by learning a strategy to
pay attention to the relationship between the current position and the N-back
position. Critically, we find that the total entropy of the attention score
matrix increases as N increases, suggesting that the dispersion of attention
scores might be the cause of the capacity limit observed in N-back tasks. Our
findings thus offer insights into the shared role of attention in both human
and artificial intelligence. Moreover, the limitations of the self-attention
mechanism revealed in the current study could inform future efforts to design
more powerful model architectures with enhanced working memory capacity and
cognitive capabilities.",2024-09-16,"Dongyu Gong, Hantao Zhang",http://arxiv.org/pdf/2409.10715v2,cs.CL
Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine,"Accurate interpretation of lab results is crucial in clinical medicine, yet
most patient portals use universal normal ranges, ignoring conditional factors
like age and gender. This study introduces Lab-AI, an interactive system that
offers personalized normal ranges using retrieval-augmented generation (RAG)
from credible health sources. Lab-AI has two modules: factor retrieval and
normal range retrieval. We tested these on 122 lab tests: 40 with conditional
factors and 82 without. For tests with factors, normal ranges depend on
patient-specific information. Our results show GPT-4-turbo with RAG achieved a
0.948 F1 score for factor retrieval and 0.995 accuracy for normal range
retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by 33.5%
in factor retrieval and showed 132% and 100% improvements in question-level and
lab-level performance, respectively, for normal range retrieval. These findings
highlight Lab-AI's potential to enhance patient understanding of lab results.",2024-09-16,"Xiaoyu Wang, Haoyong Ouyang, Balu Bhasuran, Xiao Luo, Karim Hanna, Mia Liza A. Lustria, Carl Yang, Zhe He",http://arxiv.org/pdf/2409.18986v2,cs.CL
Self-supervised Speech Models for Word-Level Stuttered Speech Detection,"Clinical diagnosis of stuttering requires an assessment by a licensed
speech-language pathologist. However, this process is time-consuming and
requires clinicians with training and experience in stuttering and fluency
disorders. Unfortunately, only a small percentage of speech-language
pathologists report being comfortable working with individuals who stutter,
which is inadequate to accommodate for the 80 million individuals who stutter
worldwide. Developing machine learning models for detecting stuttered speech
would enable universal and automated screening for stuttering, enabling speech
pathologists to identify and follow up with patients who are most likely to be
diagnosed with a stuttering speech disorder. Previous research in this area has
predominantly focused on utterance-level detection, which is not sufficient for
clinical settings where word-level annotation of stuttering is the norm. In
this study, we curated a stuttered speech dataset with word-level annotations
and introduced a word-level stuttering speech detection model leveraging
self-supervised speech models. Our evaluation demonstrates that our model
surpasses previous approaches in word-level stuttering speech detection.
Additionally, we conducted an extensive ablation analysis of our method,
providing insight into the most important aspects of adapting self-supervised
speech models for stuttered speech detection.",2024-09-16,"Yi-Jen Shih, Zoi Gkalitsiou, Alexandros G. Dimakis, David Harwath",http://arxiv.org/pdf/2409.10704v1,cs.CL
Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation with LLMs,"The growing demand for AI training data has transformed data annotation into
a global industry, but traditional approaches relying on human annotators are
often time-consuming, labor-intensive, and prone to inconsistent quality. We
propose the Model-in-the-Loop (MILO) framework, which integrates AI/ML models
into the annotation process. Our research introduces a collaborative paradigm
that leverages the strengths of both professional human annotators and large
language models (LLMs). By employing LLMs as pre-annotation and real-time
assistants, and judges on annotator responses, MILO enables effective
interaction patterns between human annotators and LLMs. Three empirical studies
on multimodal data annotation demonstrate MILO's efficacy in reducing handling
time, improving data quality, and enhancing annotator experiences. We also
introduce quality rubrics for flexible evaluation and fine-grained feedback on
open-ended annotations. The MILO framework has implications for accelerating
AI/ML development, reducing reliance on human annotation alone, and promoting
better alignment between human and machine values.",2024-09-16,"Yifan Wang, David Stevens, Pranay Shah, Wenwen Jiang, Miao Liu, Xu Chen, Robert Kuo, Na Li, Boying Gong, Daniel Lee, Jiabo Hu, Ning Zhang, Bob Kamma",http://arxiv.org/pdf/2409.10702v2,cs.CL
A Bayesian Interpretation of Adaptive Low-Rank Adaptation,"Motivated by the sensitivity-based importance score of the adaptive low-rank
adaptation (AdaLoRA), we utilize more theoretically supported metrics,
including the signal-to-noise ratio (SNR), along with the Improved Variational
Online Newton (IVON) optimizer, for adaptive parameter budget allocation. The
resulting Bayesian counterpart not only has matched or surpassed the
performance of using the sensitivity-based importance metric but is also a
faster alternative to AdaLoRA with Adam. Our theoretical analysis reveals a
significant connection between the two metrics, providing a Bayesian
perspective on the efficacy of sensitivity as an importance score. Furthermore,
our findings suggest that the magnitude, rather than the variance, is the
primary indicator of the importance of parameters.",2024-09-16,"Haolin Chen, Philip N. Garner",http://arxiv.org/pdf/2409.10673v2,cs.CL
Do Large Language Models Need a Content Delivery Network?,"As the use of large language models (LLMs) expands rapidly, so does the range
of knowledge needed to supplement various LLM queries. Thus, enabling flexible
and efficient injection of new knowledge in LLM inference is critical. Three
high-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,
fine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,
in-context learning), or (iii) injecting the KV caches of the new knowledge to
LLM during prefill. This paper argues that, although fine-tuning and in-context
learning are popular, using KV caches as the medium of knowledge could
simultaneously enable more modular management of knowledge injection and more
efficient LLM serving with low cost and fast response. To realize these
benefits, we envision a Knowledge Delivery Network (KDN), a new system
component in LLM services that dynamically optimizes the storage, transfer, and
composition of KV cache across LLM engines and other compute and storage
resources. We believe that, just like content delivery networks (CDNs), such as
Akamai, enabled the success of the Internet ecosystem through their efficient
data delivery, KDNs will be critical to the success of LLM applications through
their efficient knowledge delivery. We have open-sourced a KDN prototype at
https://github.com/LMCache/LMCache.",2024-09-16,"Yihua Cheng, Kuntai Du, Jiayi Yao, Junchen Jiang",http://arxiv.org/pdf/2409.13761v2,cs.CL
Visualizing Temporal Topic Embeddings with a Compass,"Dynamic topic modeling is useful at discovering the development and change in
latent topics over time. However, present methodology relies on algorithms that
separate document and word representations. This prevents the creation of a
meaningful embedding space where changes in word usage and documents can be
directly analyzed in a temporal context. This paper proposes an expansion of
the compass-aligned temporal Word2Vec methodology into dynamic topic modeling.
Such a method allows for the direct comparison of word and document embeddings
across time in dynamic topics. This enables the creation of visualizations that
incorporate temporal word embeddings within the context of documents into topic
visualizations. In experiments against the current state-of-the-art, our
proposed method demonstrates overall competitive performance in topic relevancy
and diversity across temporal datasets of varying size. Simultaneously, it
provides insightful visualizations focused on temporal word embeddings while
maintaining the insights provided by global topic evolution, advancing our
understanding of how topics evolve over time.",2024-09-16,"Daniel Palamarchuk, Lemara Williams, Brian Mayer, Thomas Danielson, Rebecca Faust, Larry Deschaine, Chris North",http://arxiv.org/pdf/2409.10649v2,cs.CL
Improving Multi-candidate Speculative Decoding,"Speculative Decoding (SD) is a technique to accelerate the inference of Large
Language Models (LLMs) by using a lower complexity draft model to propose
candidate tokens verified by a larger target model. To further improve
efficiency, Multi-Candidate Speculative Decoding (MCSD) improves upon this by
sampling multiple candidate tokens from the draft model at each step and
verifying them in parallel, thus increasing the chances of accepting a token
and reducing generation time. Existing MCSD methods rely on the draft model to
initialize the multi-candidate sequences and use static length and tree
attention structure for draft generation. However, such an approach suffers
from the draft and target model's output distribution differences, especially
in a dynamic generation context. In this work, we introduce a new version of
MCSD that includes a target model initialized multi-candidate generation, a
dynamic sliced topology-aware causal mask for dynamic length adjustment, and
decision models to optimize early stopping. We experimented with our method on
Llama 2-7B and its variants and observed a maximum 27.5% speedup compared to
our MCSD baseline across three benchmarks with Llama 2-7B as the target model
and JackFram 68M as the draft model. Additionally, we evaluate the effects of
using the target model initialized multi-candidate process with different draft
models on output quality.",2024-09-16,"Xiaofan Lu, Yixiao Zeng, Feiyang Ma, Zixu Yu, Marco Levorato",http://arxiv.org/pdf/2409.10644v3,cs.CL
Exploring Fine-tuned Generative Models for Keyphrase Selection: A Case Study for Russian,"Keyphrase selection plays a pivotal role within the domain of scholarly
texts, facilitating efficient information retrieval, summarization, and
indexing. In this work, we explored how to apply fine-tuned generative
transformer-based models to the specific task of keyphrase selection within
Russian scientific texts. We experimented with four distinct generative models,
such as ruT5, ruGPT, mT5, and mBART, and evaluated their performance in both
in-domain and cross-domain settings. The experiments were conducted on the
texts of Russian scientific abstracts from four domains: mathematics & computer
science, history, medicine, and linguistics. The use of generative models,
namely mBART, led to gains in in-domain performance (up to 4.9% in BERTScore,
9.0% in ROUGE-1, and 12.2% in F1-score) over three keyphrase extraction
baselines for the Russian language. Although the results for cross-domain usage
were significantly lower, they still demonstrated the capability to surpass
baseline performances in several cases, underscoring the promising potential
for further exploration and refinement in this research field.",2024-09-16,"Anna Glazkova, Dmitry Morozov",http://arxiv.org/pdf/2409.10640v2,cs.CL
RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval,"Transformer-based Large Language Models (LLMs) have become increasingly
important. However, due to the quadratic time complexity of attention
computation, scaling LLMs to longer contexts incurs extremely slow inference
speed and high GPU memory consumption for caching key-value (KV) vectors. This
paper proposes RetrievalAttention, a training-free approach to both accelerate
attention computation and reduce GPU memory consumption. By leveraging the
dynamic sparsity of attention mechanism, RetrievalAttention proposes to build
approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory
and retrieve the most relevant ones through vector search during generation.
Unfortunately, we observe that the off-the-shelf ANNS indexes are often
ineffective for such retrieval tasks due to the out-of-distribution (OOD)
between query vectors and key vectors in the attention mechanism.
RetrievalAttention addresses the OOD challenge by designing an attention-aware
vector search algorithm that can adapt to the distribution of query vectors.
Our evaluation demonstrates that RetrievalAttention achieves near full
attention accuracy while only requiring access to 1--3% of the data. This leads
to a significant reduction in the inference cost of long-context LLMs, with a
much lower GPU memory footprint. In particular, RetrievalAttention only needs a
single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,
which is capable of generating one token in 0.188 seconds.",2024-09-16,"Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu",http://arxiv.org/pdf/2409.10516v3,cs.CL
An Efficient Self-Learning Framework For Interactive Spoken Dialog Systems,"Dialog systems, such as voice assistants, are expected to engage with users
in complex, evolving conversations. Unfortunately, traditional automatic speech
recognition (ASR) systems deployed in such applications are usually trained to
recognize each turn independently and lack the ability to adapt to the
conversational context or incorporate user feedback. In this work, we introduce
a general framework for ASR in dialog systems that can go beyond learning from
single-turn utterances and learn over time how to adapt to both explicit
supervision and implicit user feedback present in multi-turn conversations. We
accomplish that by leveraging advances in student-teacher learning and
context-aware dialog processing, and designing contrastive self-supervision
approaches with Ohm, a new online hard-negative mining approach. We show that
leveraging our new framework compared to traditional training leads to relative
WER reductions of close to 10% in real-world dialog systems, and up to 26% on
public synthetic data.",2024-09-16,"Hitesh Tulsiani, David M. Chan, Shalini Ghosh, Garima Lalwani, Prabhat Pandey, Ankish Bansal, Sri Garimella, Ariya Rastrow, Björn Hoffmeister",http://arxiv.org/pdf/2409.10515v1,cs.CL
DILA: Dictionary Label Attention for Mechanistic Interpretability in High-dimensional Multi-label Medical Coding Prediction,"Predicting high-dimensional or extreme multilabels, such as in medical
coding, requires both accuracy and interpretability. Existing works often rely
on local interpretability methods, failing to provide comprehensive
explanations of the overall mechanism behind each label prediction within a
multilabel set. We propose a mechanistic interpretability module called
DIctionary Label Attention (\method) that disentangles uninterpretable dense
embeddings into a sparse embedding space, where each nonzero element (a
dictionary feature) represents a globally learned medical concept. Through
human evaluations, we show that our sparse embeddings are more human
understandable than its dense counterparts by at least 50 percent. Our
automated dictionary feature identification pipeline, leveraging large language
models (LLMs), uncovers thousands of learned medical concepts by examining and
summarizing the highest activating tokens for each dictionary feature. We
represent the relationships between dictionary features and medical codes
through a sparse interpretable matrix, enhancing the mechanistic and global
understanding of the model's predictions while maintaining competitive
performance and scalability without extensive human annotation.",2024-09-16,"John Wu, David Wu, Jimeng Sun",http://arxiv.org/pdf/2409.10504v2,cs.CL
Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles,"Causal language modeling using the Transformer architecture has yielded
remarkable capabilities in Large Language Models (LLMs) over the last few
years. However, the extent to which fundamental search and reasoning
capabilities emerged within LLMs remains a topic of ongoing debate. In this
work, we study if causal language modeling can learn a complex task such as
solving Sudoku puzzles. To solve a Sudoku, the model is first required to
search over all empty cells of the puzzle to decide on a cell to fill and then
apply an appropriate strategy to fill the decided cell. Sometimes, the
application of a strategy only results in thinning down the possible values in
a cell rather than concluding the exact value of the cell. In such cases,
multiple strategies are applied one after the other to fill a single cell. We
observe that Transformer models trained on this synthetic task can indeed learn
to solve Sudokus (our model solves $94.21\%$ of the puzzles fully correctly)
when trained on a logical sequence of steps taken by a solver. We find that
training Transformers with the logical sequence of steps is necessary and
without such training, they fail to learn Sudoku. We also extend our analysis
to Zebra puzzles (known as Einstein puzzles) and show that the model solves
$92.04 \%$ of the puzzles fully correctly. In addition, we study the internal
representations of the trained Transformer and find that through linear
probing, we can decode information about the set of possible values in any
given cell from them, pointing to the presence of a strong reasoning engine
implicit in the Transformer weights.",2024-09-16,"Kulin Shah, Nishanth Dikkala, Xin Wang, Rina Panigrahy",http://arxiv.org/pdf/2409.10502v1,cs.CL
CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios,"Large Language Models (LLMs) have been widely adopted to process long-context
tasks. However, the large memory overhead of the key-value (KV) cache poses
significant challenges in long-context scenarios. Existing training-free KV
cache compression methods typically focus on quantization and token pruning,
which have compression limits, and excessive sparsity can lead to severe
performance degradation. Other methods design new architectures with less KV
overhead but require significant training overhead. To address the above two
drawbacks, we further explore the redundancy in the channel dimension and apply
an architecture-level design with minor training costs. Therefore, we introduce
CSKV, a training-efficient Channel Shrinking technique for KV cache
compression: (1) We first analyze the singular value distribution of the KV
cache, revealing significant redundancy and compression potential along the
channel dimension. Based on this observation, we propose using low-rank
decomposition for key and value layers and storing the low-dimension features.
(2) To preserve model performance, we introduce a bi-branch KV cache, including
a window-based full-precision KV cache and a low-precision compressed KV cache.
(3) To reduce the training costs, we minimize the layer-wise reconstruction
loss for the compressed KV cache instead of retraining the entire LLMs.
Extensive experiments show that CSKV can reduce the memory overhead of the KV
cache by 80% while maintaining the model's long-context capability. Moreover,
we show that our method can be seamlessly combined with quantization to further
reduce the memory overhead, achieving a compression ratio of up to 95%. Code is
available at https://github.com/wln20/CSKV.",2024-09-16,"Luning Wang, Shiyao Li, Xuefei Ning, Zhihang Yuan, Shengen Yan, Guohao Dai, Yu Wang",http://arxiv.org/pdf/2409.10593v3,cs.CL
Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation,"This paper presents a diffusion-based recommender system that incorporates
classifier-free guidance. Most current recommender systems provide
recommendations using conventional methods such as collaborative or
content-based filtering. Diffusion is a new approach to generative AI that
improves on previous generative AI approaches such as Variational Autoencoders
(VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in
a recommender system that mirrors the sequence users take when browsing and
rating items. Although a few current recommender systems incorporate diffusion,
they do not incorporate classifier-free guidance, a new innovation in diffusion
models as a whole. In this paper, we present a diffusion recommender system
that augments the underlying recommender system model for improved performance
and also incorporates classifier-free guidance. Our findings show improvements
over state-of-the-art recommender systems for most metrics for several
recommendation tasks on a variety of datasets. In particular, our approach
demonstrates the potential to provide better recommendations when data is
sparse.",2024-09-16,"Noah Buchanan, Susan Gauch, Quan Mai",http://arxiv.org/pdf/2409.10494v1,cs.CL
Schrodinger's Memory: Large Language Models,"Memory is the foundation of all human activities; without memory, it would be
nearly impossible for people to perform any task in daily life. With the
development of Large Language Models (LLMs), their language capabilities are
becoming increasingly comparable to those of humans. But do LLMs have memory?
Based on current performance, LLMs do appear to exhibit memory. So, what is the
underlying mechanism of this memory? Previous research has lacked a deep
exploration of LLMs' memory capabilities and the underlying theory. In this
paper, we use Universal Approximation Theorem (UAT) to explain the memory
mechanism in LLMs. We also conduct experiments to verify the memory
capabilities of various LLMs, proposing a new method to assess their abilities
based on these memory ability. We argue that LLM memory operates like
Schr\""odinger's memory, meaning that it only becomes observable when a specific
memory is queried. We can only determine if the model retains a memory based on
its output in response to the query; otherwise, it remains indeterminate.
Finally, we expand on this concept by comparing the memory capabilities of the
human brain and LLMs, highlighting the similarities and differences in their
operational mechanisms.",2024-09-16,"Wei Wang, Qing Li",http://arxiv.org/pdf/2409.10482v3,cs.CL
Meta-Whisper: Speech-Based Meta-ICL for ASR on Low-Resource Languages,"This paper presents Meta-Whisper, a novel approach to improve automatic
speech recognition (ASR) for low-resource languages using the Whisper model. By
leveraging Meta In-Context Learning (Meta-ICL) and a k-Nearest Neighbors (KNN)
algorithm for sample selection, Meta-Whisper enhances Whisper's ability to
recognize speech in unfamiliar languages without extensive fine-tuning.
Experiments on the ML-SUPERB dataset show that Meta-Whisper significantly
reduces the Character Error Rate (CER) for low-resource languages compared to
the original Whisper model. This method offers a promising solution for
developing more adaptable multilingual ASR systems, particularly for languages
with limited resources.",2024-09-16,"Ming-Hao Hsu, Kuan Po Huang, Hung-yi Lee",http://arxiv.org/pdf/2409.10429v1,cs.CL
AI Conversational Interviewing: Transforming Surveys with LLMs as Adaptive Interviewers,"Traditional methods for eliciting people's opinions face a trade-off between
depth and scale: structured surveys enable large-scale data collection but
limit respondents' ability to voice their opinions in their own words, while
conversational interviews provide deeper insights but are resource-intensive.
This study explores the potential of replacing human interviewers with large
language models (LLMs) to conduct scalable conversational interviews. Our goal
is to assess the performance of AI Conversational Interviewing and to identify
opportunities for improvement in a controlled environment. We conducted a
small-scale, in-depth study with university students who were randomly assigned
to a conversational interview by either AI or human interviewers, both
employing identical questionnaires on political topics. Various quantitative
and qualitative measures assessed interviewer adherence to guidelines, response
quality, participant engagement, and overall interview efficacy. The findings
indicate the viability of AI Conversational Interviewing in producing quality
data comparable to traditional methods, with the added benefit of scalability.
We publish our data and materials for re-use and present specific
recommendations for effective implementation.",2024-09-16,"Alexander Wuttke, Matthias Aßenmacher, Christopher Klamm, Max M. Lang, Quirin Würschinger, Frauke Kreuter",http://arxiv.org/pdf/2410.01824v2,cs.CL
A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration,"This paper proposes a knowledge-enhanced disease diagnosis method based on a
prompt learning framework. The method retrieves structured knowledge from
external knowledge graphs related to clinical cases, encodes it, and injects it
into the prompt templates to enhance the language model's understanding and
reasoning capabilities for the task.We conducted experiments on three public
datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the
proposed method significantly outperforms existing models across multiple
evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC
dataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.
Additionally,ablation studies confirmed the critical role of the knowledge
injection module,as the removal of this module resulted in a significant drop
in F1 score. The experimental results demonstrate that the proposed method not
only effectively improves the accuracy of disease diagnosis but also enhances
the interpretability of the predictions, providing more reliable support and
evidence for clinical diagnosis.",2024-09-16,Zhang Zheng,http://arxiv.org/pdf/2409.10403v1,cs.CL
Instigating Cooperation among LLM Agents Using Adaptive Information Modulation,"This paper introduces a novel framework combining LLM agents as proxies for
human strategic behavior with reinforcement learning (RL) to engage these
agents in evolving strategic interactions within team environments. Our
approach extends traditional agent-based simulations by using strategic LLM
agents (SLA) and introducing dynamic and adaptive governance through a
pro-social promoting RL agent (PPA) that modulates information access across
agents in a network, optimizing social welfare and promoting pro-social
behavior. Through validation in iterative games, including the prisoner
dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.
The PPA agent effectively learns to adjust information transparency, resulting
in enhanced cooperation rates. This framework offers significant insights into
AI-mediated social dynamics, contributing to the deployment of AI in real-world
team settings.",2024-09-16,"Qiliang Chen, Sepehr Ilami, Nunzio Lore, Babak Heydari",http://arxiv.org/pdf/2409.10372v3,cs.CL
2D or not 2D: How Does the Dimensionality of Gesture Representation Affect 3D Co-Speech Gesture Generation?,"Co-speech gestures are fundamental for communication. The advent of recent
deep learning techniques has facilitated the creation of lifelike, synchronous
co-speech gestures for Embodied Conversational Agents. ""In-the-wild"" datasets,
aggregating video content from platforms like YouTube via human pose detection
technologies, provide a feasible solution by offering 2D skeletal sequences
aligned with speech. Concurrent developments in lifting models enable the
conversion of these 2D sequences into 3D gesture databases. However, it is
important to note that the 3D poses estimated from the 2D extracted poses are,
in essence, approximations of the ground-truth, which remains in the 2D domain.
This distinction raises questions about the impact of gesture representation
dimensionality on the quality of generated motions - a topic that, to our
knowledge, remains largely unexplored. Our study examines the effect of using
either 2D or 3D joint coordinates as training data on the performance of
speech-to-gesture deep generative models. We employ a lifting model for
converting generated 2D pose sequences into 3D and assess how gestures created
directly in 3D stack up against those initially generated in 2D and then
converted to 3D. We perform an objective evaluation using widely used metrics
in the gesture generation field as well as a user study to qualitatively
evaluate the different approaches.",2024-09-16,"Téo Guichoux, Laure Soulier, Nicolas Obin, Catherine Pelachaud",http://arxiv.org/pdf/2409.10357v2,cs.CL
"Detecting Sexism in German Online Newspaper Comments with Open-Source Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks 1 and 2, Closed Track)","Sexism in online media comments is a pervasive challenge that often manifests
subtly, complicating moderation efforts as interpretations of what constitutes
sexism can vary among individuals. We study monolingual and multilingual
open-source text embeddings to reliably detect sexism and misogyny in
German-language online comments from an Austrian newspaper. We observed
classifiers trained on text embeddings to mimic closely the individual
judgements of human annotators. Our method showed robust performance in the
GermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1
score of 0.597 (4th place, as reported on Codabench). It also accurately
predicted the distribution of human annotations in GerMS-Detect Subtask 2, with
an average Jensen-Shannon distance of 0.301 (2nd place). The computational
efficiency of our approach suggests potential for scalable applications across
various languages and linguistic contexts.",2024-09-16,"Florian Bremm, Patrick Gustav Blaneck, Tobias Bornheim, Niklas Grieger, Stephan Bialonski",http://arxiv.org/pdf/2409.10341v2,cs.CL
The 20 questions game to distinguish large language models,"In a parallel with the 20 questions game, we present a method to determine
whether two large language models (LLMs), placed in a black-box context, are
the same or not. The goal is to use a small set of (benign) binary questions,
typically under 20. We formalize the problem and first establish a baseline
using a random selection of questions from known benchmark datasets, achieving
an accuracy of nearly 100% within 20 questions. After showing optimal bounds
for this problem, we introduce two effective questioning heuristics able to
discriminate 22 LLMs by using half as many questions for the same task. These
methods offer significant advantages in terms of stealth and are thus of
interest to auditors or copyright owners facing suspicions of model leaks.",2024-09-16,"Gurvan Richardeau, Erwan Le Merrer, Camilla Penzo, Gilles Tredan",http://arxiv.org/pdf/2409.10338v1,cs.CL
MGSA: Multi-Granularity Graph Structure Attention for Knowledge Graph-to-Text Generation,"The Knowledge Graph-to-Text Generation task aims to convert structured
knowledge graphs into coherent and human-readable natural language text. Recent
efforts in this field have focused on enhancing pre-trained language models
(PLMs) by incorporating graph structure information to capture the intricate
structure details of knowledge graphs. However, most of these approaches tend
to capture only single-granularity structure information, concentrating either
on the relationships between entities within the original graph or on the
relationships between words within the same entity or across different
entities. This narrow focus results in a significant limitation: models that
concentrate solely on entity-level structure fail to capture the nuanced
semantic relationships between words, while those that focus only on word-level
structure overlook the broader relationships between original entire entities.
To overcome these limitations, this paper introduces the Multi-granularity
Graph Structure Attention (MGSA), which is based on PLMs. The encoder of the
model architecture features an entity-level structure encoding module, a
word-level structure encoding module, and an aggregation module that
synthesizes information from both structure. This multi-granularity structure
encoding approach allows the model to simultaneously capture both entity-level
and word-level structure information, providing a more comprehensive
understanding of the knowledge graph's structure information, thereby
significantly improving the quality of the generated text. We conducted
extensive evaluations of the MGSA model using two widely recognized KG-to-Text
Generation benchmark datasets, WebNLG and EventNarrative, where it consistently
outperformed models that rely solely on single-granularity structure
information, demonstrating the effectiveness of our approach.",2024-09-16,"Shanshan Wang, Chun Zhang, Ning Zhang",http://arxiv.org/pdf/2409.10294v2,cs.CL
ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework,"Empathetic response generation necessitates the integration of emotional and
intentional dynamics to foster meaningful interactions. Existing research
either neglects the intricate interplay between emotion and intent, leading to
suboptimal controllability of empathy, or resorts to large language models
(LLMs), which incur significant computational overhead. In this paper, we
introduce ReflectDiffu, a lightweight and comprehensive framework for
empathetic response generation. This framework incorporates emotion contagion
to augment emotional expressiveness and employs an emotion-reasoning mask to
pinpoint critical emotional elements. Additionally, it integrates intent
mimicry within reinforcement learning for refinement during diffusion. By
harnessing an intent twice reflect mechanism of Exploring-Sampling-Correcting,
ReflectDiffu adeptly translates emotional decision-making into precise intent
actions, thereby addressing empathetic response misalignments stemming from
emotional misrecognition. Through reflection, the framework maps emotional
states to intents, markedly enhancing both response empathy and flexibility.
Comprehensive experiments reveal that ReflectDiffu outperforms existing models
regarding relevance, controllability, and informativeness, achieving
state-of-the-art results in both automatic and human evaluations.",2024-09-16,"Jiahao Yuan, Zixiang Di, Zhiqing Cui, Guisong Yang, Usman Naseem",http://arxiv.org/pdf/2409.10289v3,cs.CL
From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes the Emoji Potential in LLMs,"The manipulation of the personality traits of large language models (LLMs)
has emerged as a key area of research. Methods like prompt-based In-Context
Knowledge Editing (IKE) and gradient-based Model Editor Networks (MEND) have
been explored but show irregularity and variability; IKE depends on the prompt,
leading to variability and sensitivity, while MEND yields inconsistent and
gibberish outputs. To address this, we employed Opinion QA Based
Parameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank
Adaptation (QLoRA), to manipulate the Big Five personality traits: Openness,
Conscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,
models such as Mistral-7B-Instruct and LLaMA-2-7B-chat showed a latent
behaviour by generating emojis for certain traits, despite no emojis being
present in the PEFT data. For instance, LLaMA-2-7B-chat generated emojis in
99.5\% of extraversion-related test instances, while Mistral-7B-Instruct did so
in 92.5\% of openness-related test instances. ICL Explainability analysis
indicated that the LLMs used emojis intentionally to express these traits.
Mechanistic Interpretability analysis showed that this latent behaviour of LLMs
could be traced to specific neurons that became activated or amplified after
PEFT. This paper provides a number of novel contributions. First, introducing
an Opinion QA dataset for PEFT-driven personality manipulation; second,
developing metric models to benchmark LLM personality traits; third,
demonstrating PEFT's superiority over IKE in personality manipulation; and
finally, analysing and validating emoji usage through explainability methods
such as Mechanistic Interpretability and In-context learning Explainability
methods.",2024-09-16,"Navya Jain, Zekun Wu, Cristian Munoz, Airlie Hilliard, Xin Guan, Adriano Koshiyama, Emre Kazim, Philip Treleaven",http://arxiv.org/pdf/2409.10245v4,cs.CL
Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models,"Recent progress in Multimodal Large Language Models(MLLMs) often use large
image tokens to compensate the visual shortcoming of MLLMs, which not only
exhibits obvious redundancy but also greatly exacerbates the already high
computation. Token pruning is an effective solution for speeding up MLLMs, but
when and how to drop tokens still remains a challenge. In this paper, we
propose a novel and training-free approach for the effective visual token
pruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning
recipe for MLLMs according to a pre-defined budget. Specifically, FitPrune
considers token pruning as a statistical problem of MLLM and its objective is
to find out an optimal pruning scheme that can minimize the divergence of the
attention distributions before and after pruning. In practice, FitPrune can be
quickly accomplished based on the attention statistics from a small batch of
inference data, avoiding the expensive trials of MLLMs. According to the
pruning recipe, an MLLM can directly remove the redundant visual tokens of
different examples during inference. To validate FitPrune, we apply it to a set
of recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct
extensive experiments on a set of benchmarks. The experimental results show
that our FitPrune can not only reduce the computational complexity to a large
extent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT
with only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in
about 5 minutes. Our code is available at https://github.com/ywh187/FitPrune.",2024-09-16,"Weihao Ye, Qiong Wu, Wenhao Lin, Yiyi Zhou",http://arxiv.org/pdf/2409.10197v2,cs.CL
A Literature Review of Keyword Spotting Technologies for Urdu,"This literature review surveys the advancements of keyword spotting (KWS)
technologies, specifically focusing on Urdu, Pakistan's low-resource language
(LRL), which has complex phonetics. Despite the global strides in speech
technology, Urdu presents unique challenges requiring more tailored solutions.
The review traces the evolution from foundational Gaussian Mixture Models to
sophisticated neural architectures like deep neural networks and transformers,
highlighting significant milestones such as integrating multi-task learning and
self-supervised approaches that leverage unlabeled data. It examines emerging
technologies' role in enhancing KWS systems' performance within multilingual
and resource-constrained settings, emphasizing the need for innovations that
cater to languages like Urdu. Thus, this review underscores the need for
context-specific research addressing the inherent complexities of Urdu and
similar URLs and the means of regions communicating through such languages for
a more inclusive approach to speech technology.",2024-09-16,Syed Muhammad Aqdas Rizvi,http://arxiv.org/pdf/2409.16317v1,cs.CL
LLMs for clinical risk prediction,"This study compares the efficacy of GPT-4 and clinalytix Medical AI in
predicting the clinical risk of delirium development. Findings indicate that
GPT-4 exhibited significant deficiencies in identifying positive cases and
struggled to provide reliable probability estimates for delirium risk, while
clinalytix Medical AI demonstrated superior accuracy. A thorough analysis of
the large language model's (LLM) outputs elucidated potential causes for these
discrepancies, consistent with limitations reported in extant literature. These
results underscore the challenges LLMs face in accurately diagnosing conditions
and interpreting complex clinical data. While LLMs hold substantial potential
in healthcare, they are currently unsuitable for independent clinical
decision-making. Instead, they should be employed in assistive roles,
complementing clinical expertise. Continued human oversight remains essential
to ensure optimal outcomes for both patients and healthcare providers.",2024-09-16,"Mohamed Rezk, Patricia Cabanillas Silva, Fried-Michael Dahlweid",http://arxiv.org/pdf/2409.10191v1,cs.CL
Augmenting Automatic Speech Recognition Models with Disfluency Detection,"Speech disfluency commonly occurs in conversational and spontaneous speech.
However, standard Automatic Speech Recognition (ASR) models struggle to
accurately recognize these disfluencies because they are typically trained on
fluent transcripts. Current research mainly focuses on detecting disfluencies
within transcripts, overlooking their exact location and duration in the
speech. Additionally, previous work often requires model fine-tuning and
addresses limited types of disfluencies.
  In this work, we present an inference-only approach to augment any ASR model
with the ability to detect open-set disfluencies. We first demonstrate that ASR
models have difficulty transcribing speech disfluencies. Next, this work
proposes a modified Connectionist Temporal Classification(CTC)-based forced
alignment algorithm from \cite{kurzinger2020ctc} to predict word-level
timestamps while effectively capturing disfluent speech. Additionally, we
develop a model to classify alignment gaps between timestamps as either
containing disfluent speech or silence. This model achieves an accuracy of
81.62% and an F1-score of 80.07%. We test the augmentation pipeline of
alignment gap detection and classification on a disfluent dataset. Our results
show that we captured 74.13% of the words that were initially missed by the
transcription, demonstrating the potential of this pipeline for downstream
tasks.",2024-09-16,"Robin Amann, Zhaolin Li, Barbara Bruno, Jan Niehues",http://arxiv.org/pdf/2409.10177v2,cs.CL
jina-embeddings-v3: Multilingual Embeddings With Task LoRA,"We introduce jina-embeddings-v3, a novel text embedding model with 570
million parameters, achieves state-of-the-art performance on multilingual data
and long-context retrieval tasks, supporting context lengths of up to 8192
tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)
adapters to generate high-quality embeddings for query-document retrieval,
clustering, classification, and text matching. Evaluation on the MTEB benchmark
shows that jina-embeddings-v3 outperforms the latest proprietary embeddings
from OpenAI and Cohere on English tasks, while achieving superior performance
compared to multilingual-e5-large-instruct across all multilingual tasks. With
a default output dimension of 1024, users can flexibly reduce the embedding
dimensions to as low as 32 without compromising performance, enabled by
Matryoshka Representation Learning.",2024-09-16,"Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, Han Xiao",http://arxiv.org/pdf/2409.10173v3,cs.CL
Quantile Regression for Distributional Reward Models in RLHF,"Reinforcement learning from human feedback (RLHF) has become a key method for
aligning large language models (LLMs) with human preferences through the use of
reward models. However, traditional reward models typically generate point
estimates, which oversimplify the diversity and complexity of human values and
preferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel
approach to reward modeling that learns a distribution over rewards instead of
a single scalar value. Our method uses quantile regression to estimate a full,
potentially multimodal distribution over preferences, providing a more powerful
and nuanced representation of preferences. This distributional approach can
better capture the diversity of human values, addresses label noise, and
accommodates conflicting preferences by modeling them as distinct modes in the
distribution. Our experimental results show that QRM outperforms comparable
traditional point-estimate models on RewardBench. Furthermore, we demonstrate
that the additional information provided by the distributional estimates can be
utilized in downstream applications, such as risk-aware reinforcement learning,
resulting in LLM policies that generate fewer extremely negative responses. Our
code and model are released at https://github.com/Nicolinho/QRM.",2024-09-16,Nicolai Dorka,http://arxiv.org/pdf/2409.10164v1,cs.CL
Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach,"Recent progress in Spoken Language Modeling has shown that learning language
directly from speech is feasible. Generating speech through a pipeline that
operates at the text level typically loses nuances, intonations, and non-verbal
vocalizations. Modeling directly from speech opens up the path to more natural
and expressive systems. On the other hand, speech-only systems require up to
three orders of magnitude more data to catch up to their text-based
counterparts in terms of their semantic abilities. We show that fine-tuning
speech representation models on phoneme classification leads to more
context-invariant representations, and language models trained on these units
achieve comparable lexical comprehension to ones trained on hundred times more
data.",2024-09-16,"Maxime Poli, Emmanuel Chemla, Emmanuel Dupoux",http://arxiv.org/pdf/2410.00025v2,cs.CL
LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology Learning Challenge,"This paper outlines the LLMs4OL 2024, the first edition of the Large Language
Models for Ontology Learning Challenge. LLMs4OL is a community development
initiative collocated with the 23rd International Semantic Web Conference
(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology
Learning (OL), a vital process for enhancing the web with structured knowledge
to improve interoperability. By leveraging LLMs, the challenge aims to advance
understanding and innovation in OL, aligning with the goals of the Semantic Web
to create a more intelligent and user-friendly web. In this paper, we give an
overview of the 2024 edition of the LLMs4OL challenge and summarize the
contributions.",2024-09-16,"Hamed Babaei Giglou, Jennifer D'Souza, Sören Auer",http://arxiv.org/pdf/2409.10146v1,cs.CL
StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge Editing for Large Language Models,"As the modern tool of choice for question answering, large language models
(LLMs) are expected to deliver answers with up-to-date knowledge. To achieve
such ideal question-answering systems, locating and then editing outdated
knowledge in the natural language outputs is a general target of popular
knowledge editing methods. However, this target is challenging, as both
identifying which tokens to edit in the reasoning steps and ensuring the
coherence of the revised reasoning chain are difficult tasks. We argue that
these challenges stem from the unstructured nature of natural language outputs.
To address the above challenges, we propose $\textbf{Stru}$ctural
$\textbf{Edit}$ing ($\textbf{StruEdit}$), an improved baseline for knowledge
editing. We first prompt LLMs to produce structured outputs consisting of
reasoning triplets. Then, StruEdit removes any potentially outdated knowledge
and efficiently refills the structured outputs with up-to-date information in a
single step. Experimental results show that StruEdit consistently delivers the
highest accuracy with lowest latency compared with other knowledge editing
methods.",2024-09-16,"Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Junfeng Fang, Xueqi Cheng",http://arxiv.org/pdf/2409.10132v1,cs.CL
Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT,"Self-supervised speech representation learning has become essential for
extracting meaningful features from untranscribed audio. Recent advances
highlight the potential of deriving discrete symbols from the features
correlated with linguistic units, which enables text-less training across
diverse tasks. In particular, sentence-level Self-Distillation of the
pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech
frame representations extracted from an intermediate Transformer layer. In
SD-HuBERT, sentence-level representation is accumulated from speech frame
features through self-attention layers using a special CLS token. However, we
observe that the information aggregated in the CLS token correlates more with
speaker identity than with linguistic content. To address this, we propose a
speech-only self-supervised fine-tuning approach that separates syllabic units
from speaker information. Our method introduces speaker perturbation as data
augmentation and adopts a frame-level training objective to prevent the CLS
token from aggregating paralinguistic information. Experimental results show
that our approach surpasses the current state-of-the-art method in most
syllable segmentation and syllabic unit quality metrics on Librispeech,
underscoring its effectiveness in promoting syllabic organization within
speech-only models.",2024-09-16,"Ryota Komatsu, Takahiro Shinozaki",http://arxiv.org/pdf/2409.10103v1,cs.CL
Trustworthiness in Retrieval-Augmented Generation Systems: A Survey,"Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal
paradigm in the development of Large Language Models (LLMs). While much of the
current research in this field focuses on performance optimization,
particularly in terms of accuracy and efficiency, the trustworthiness of RAG
systems remains an area still under exploration. From a positive perspective,
RAG systems are promising to enhance LLMs by providing them with useful and
up-to-date knowledge from vast external databases, thereby mitigating the
long-standing problem of hallucination. While from a negative perspective, RAG
systems are at the risk of generating undesirable contents if the retrieved
information is either inappropriate or poorly utilized. To address these
concerns, we propose a unified framework that assesses the trustworthiness of
RAG systems across six key dimensions: factuality, robustness, fairness,
transparency, accountability, and privacy. Within this framework, we thoroughly
review the existing literature on each dimension. Additionally, we create the
evaluation benchmark regarding the six dimensions and conduct comprehensive
evaluations for a variety of proprietary and open-source models. Finally, we
identify the potential challenges for future research based on our
investigation results. Through this work, we aim to lay a structured foundation
for future investigations and provide practical insights for enhancing the
trustworthiness of RAG systems in real-world applications.",2024-09-16,"Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, Philip S. Yu",http://arxiv.org/pdf/2409.10102v1,cs.CL
LLM-DER:A Named Entity Recognition Method Based on Large Language Models for Chinese Coal Chemical Domain,"Domain-specific Named Entity Recognition (NER), whose goal is to recognize
domain-specific entities and their categories, provides an important support
for constructing domain knowledge graphs. Currently, deep learning-based
methods are widely used and effective in NER tasks, but due to the reliance on
large-scale labeled data. As a result, the scarcity of labeled data in a
specific domain will limit its application.Therefore, many researches started
to introduce few-shot methods and achieved some results. However, the entity
structures in specific domains are often complex, and the current few-shot
methods are difficult to adapt to NER tasks with complex features.Taking the
Chinese coal chemical industry domain as an example,there exists a complex
structure of multiple entities sharing a single entity, as well as multiple
relationships for the same pair of entities, which affects the NER task under
the sample less condition.In this paper, we propose a Large Language Models
(LLMs)-based entity recognition framework LLM-DER for the domain-specific
entity recognition problem in Chinese, which enriches the entity information by
generating a list of relationships containing entity types through LLMs, and
designing a plausibility and consistency evaluation method to remove
misrecognized entities, which can effectively solve the complex structural
entity recognition problem in a specific domain.The experimental results of
this paper on the Resume dataset and the self-constructed coal chemical dataset
Coal show that LLM-DER performs outstandingly in domain-specific entity
recognition, not only outperforming the existing GPT-3.5-turbo baseline, but
also exceeding the fully-supervised baseline, verifying its effectiveness in
entity recognition.",2024-09-16,"Le Xiao, Yunfei Xu, Jing Zhao",http://arxiv.org/pdf/2409.10077v1,cs.CL
Increasing faithfulness in human-human dialog summarization with Spoken Language Understanding tasks,"Dialogue summarization aims to provide a concise and coherent summary of
conversations between multiple speakers. While recent advancements in language
models have enhanced this process, summarizing dialogues accurately and
faithfully remains challenging due to the need to understand speaker
interactions and capture relevant information. Indeed, abstractive models used
for dialog summarization may generate summaries that contain inconsistencies.
We suggest using the semantic information proposed for performing Spoken
Language Understanding (SLU) in human-machine dialogue systems for
goal-oriented human-human dialogues to obtain a more semantically faithful
summary regarding the task. This study introduces three key contributions:
First, we propose an exploration of how incorporating task-related information
can enhance the summarization process, leading to more semantically accurate
summaries. Then, we introduce a new evaluation criterion based on task
semantics. Finally, we propose a new dataset version with increased annotated
data standardized for research on task-oriented dialogue summarization. The
study evaluates these methods using the DECODA corpus, a collection of French
spoken dialogues from a call center. Results show that integrating models with
task-related information improves summary accuracy, even with varying word
error rates.",2024-09-16,"Eunice Akani, Benoit Favre, Frederic Bechet, Romain Gemignani",http://arxiv.org/pdf/2409.10070v1,cs.CL
MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid via Edge LLM,"Mental health disorders are among the most prevalent diseases worldwide,
affecting nearly one in four people. Despite their widespread impact, the
intervention rate remains below 25%, largely due to the significant cooperation
required from patients for both diagnosis and intervention. The core issue
behind this low treatment rate is stigma, which discourages over half of those
affected from seeking help. This paper presents MindGuard, an accessible,
stigma-free, and professional mobile mental healthcare system designed to
provide mental health first aid. The heart of MindGuard is an innovative edge
LLM, equipped with professional mental health knowledge, that seamlessly
integrates objective mobile sensor data with subjective Ecological Momentary
Assessment records to deliver personalized screening and intervention
conversations. We conduct a broad evaluation of MindGuard using open datasets
spanning four years and real-world deployment across various mobile devices
involving 20 subjects for two weeks. Remarkably, MindGuard achieves results
comparable to GPT-4 and outperforms its counterpart with more than 10 times the
model size. We believe that MindGuard paves the way for mobile LLM
applications, potentially revolutionizing mental healthcare practices by
substituting self-reporting and intervention conversations with passive,
integrated monitoring within daily life, thus ensuring accessible and
stigma-free mental health support.",2024-09-16,"Sijie Ji, Xinzhe Zheng, Jiawei Sun, Renqi Chen, Wei Gao, Mani Srivastava",http://arxiv.org/pdf/2409.10064v1,cs.CL
Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective,"Activation Editing, which involves directly editting the internal
representations of large language models (LLMs) to alter their behaviors and
achieve desired properties, has emerged as a promising area of research.
Existing works primarily treat LLMs' activations as points in space and modify
them by adding steering vectors. However, this approach is limited in its
ability to achieve greater performance improvement while maintaining the
necessary consistency of activation magnitudes. To overcome these issues, we
propose a novel editing method that views activations in terms of their
directions and magnitudes. Our method, named Householder Pseudo-Rotation (HPR),
mimics the rotation transformation, thus preserving activation norms and
resulting in an improved performance on various safety benchmarks.",2024-09-16,"Van-Cuong Pham, Thien Huu Nguyen",http://arxiv.org/pdf/2409.10053v2,cs.CL
Harnessing Large Language Models: Fine-tuned BERT for Detecting Charismatic Leadership Tactics in Natural Language,"This work investigates the identification of Charismatic Leadership Tactics
(CLTs) in natural language using a fine-tuned Bidirectional Encoder
Representations from Transformers (BERT) model. Based on an own extensive
corpus of CLTs generated and curated for this task, our methodology entails
training a machine learning model that is capable of accurately identifying the
presence of these tactics in natural language. A performance evaluation is
conducted to assess the effectiveness of our model in detecting CLTs. We find
that the total accuracy over the detection of all CLTs is 98.96\% The results
of this study have significant implications for research in psychology and
management, offering potential methods to simplify the currently elaborate
assessment of charisma in texts.",2024-09-16,"Yasser Saeid, Felix Neubürger, Stefanie Krügl, Helena Hüster, Thomas Kopinski, Ralf Lanwehr",http://arxiv.org/pdf/2409.18984v1,cs.CL
Benchmarking Large Language Model Uncertainty for Prompt Optimization,"Prompt optimization algorithms for Large Language Models (LLMs) excel in
multi-step reasoning but still lack effective uncertainty estimation. This
paper introduces a benchmark dataset to evaluate uncertainty metrics, focusing
on Answer, Correctness, Aleatoric, and Epistemic Uncertainty. Through analysis
of models like GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct, we show that
current metrics align more with Answer Uncertainty, which reflects output
confidence and diversity, rather than Correctness Uncertainty, highlighting the
need for improved metrics that are optimization-objective-aware to better guide
prompt optimization. Our code and dataset are available at
https://github.com/0Frett/PO-Uncertainty-Benchmarking.",2024-09-16,"Pei-Fu Guo, Yun-Da Tsai, Shou-De Lin",http://arxiv.org/pdf/2409.10044v2,cs.CL
On the Diagram of Thought,"Current large language models (LLMs) demonstrate impressive capabilities but
struggle with complex, multi-step reasoning tasks. Existing methods often
tackle this by requiring external control mechanisms or multi-model
orchestration, which introduces system complexity and typically lacks formal
guarantees of reasoning soundness. We introduce the Diagram of Thought (DoT), a
framework wherein a single auto-regressive LLM internally constructs and
navigates a Directed Acyclic Graph (DAG). This DAG represents the iterative
reasoning process, encompassing steps like proposing ideas, critiquing them,
refining based on feedback, and synthesizing conclusions. This
self-orchestrated, self-contained process is guided by learned role-specific
tokens (e.g., <proposer>, <critic>, <summarizer>) embedded within the standard
generation loop, thereby eliminating external dependencies. Crucially, we
establish a rigorous mathematical foundation for DoT using Topos Theory. We
formalize the reasoning DAG as a diagram within a suitable topos and prove that
the final synthesis step, aggregating validated information, corresponds
semantically to computing the colimit of the relevant sub-diagram. This
formalization provides theoretical guarantees concerning the logical
consistency and robustness of the synthesized outcome. DoT thus offers a
unified, self-contained, interpretable, efficient, and formally grounded
approach designed to significantly advance the complex reasoning capabilities
of LLMs.",2024-09-16,"Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao",http://arxiv.org/pdf/2409.10038v3,cs.CL
AceParse: A Comprehensive Dataset with Diverse Structured Texts for Academic Literature Parsing,"With the development of data-centric AI, the focus has shifted from
model-driven approaches to improving data quality. Academic literature, as one
of the crucial types, is predominantly stored in PDF formats and needs to be
parsed into texts before further processing. However, parsing diverse
structured texts in academic literature remains challenging due to the lack of
datasets that cover various text structures. In this paper, we introduce
AceParse, the first comprehensive dataset designed to support the parsing of a
wide range of structured texts, including formulas, tables, lists, algorithms,
and sentences with embedded mathematical expressions. Based on AceParse, we
fine-tuned a multimodal model, named AceParser, which accurately parses various
structured texts within academic literature. This model outperforms the
previous state-of-the-art by 4.1% in terms of F1 score and by 5% in Jaccard
Similarity, demonstrating the potential of multimodal models in academic
literature parsing. Our dataset is available at
https://github.com/JHW5981/AceParse.",2024-09-16,"Huawei Ji, Cheng Deng, Bo Xue, Zhouyang Jin, Jiaxin Ding, Xiaoying Gan, Luoyi Fu, Xinbing Wang, Chenghu Zhou",http://arxiv.org/pdf/2409.10016v2,cs.CL
HALO: Hallucination Analysis and Learning Optimization to Empower LLMs with Retrieval-Augmented Context for Guided Clinical Decision Making,"Large language models (LLMs) have significantly advanced natural language
processing tasks, yet they are susceptible to generating inaccurate or
unreliable responses, a phenomenon known as hallucination. In critical domains
such as health and medicine, these hallucinations can pose serious risks. This
paper introduces HALO, a novel framework designed to enhance the accuracy and
reliability of medical question-answering (QA) systems by focusing on the
detection and mitigation of hallucinations. Our approach generates multiple
variations of a given query using LLMs and retrieves relevant information from
external open knowledge bases to enrich the context. We utilize maximum
marginal relevance scoring to prioritize the retrieved context, which is then
provided to LLMs for answer generation, thereby reducing the risk of
hallucinations. The integration of LangChain further streamlines this process,
resulting in a notable and robust increase in the accuracy of both open-source
and commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%
to 70%). This framework underscores the critical importance of addressing
hallucinations in medical QA systems, ultimately improving clinical
decision-making and patient care. The open-source HALO is available at:
https://github.com/ResponsibleAILab/HALO.",2024-09-16,"Sumera Anjum, Hanzhi Zhang, Wenjun Zhou, Eun Jin Paek, Xiaopeng Zhao, Yunhe Feng",http://arxiv.org/pdf/2409.10011v2,cs.CL
SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL,"In recent years,Text-to-SQL, the problem of automatically converting
questions posed in natural language to formal SQL queries, has emerged as an
important problem at the intersection of natural language processing and data
management research. Large language models (LLMs) have delivered impressive
performance when used in an off-the-shelf performance, but still fall
significantly short of expected expert-level performance. Errors are especially
probable when a nuanced understanding is needed of database schemas, questions,
and SQL clauses to do proper Text-to-SQL conversion. We introduce SelECT-SQL, a
novel in-context learning solution that uses an algorithmic combination of
chain-of-thought (CoT) prompting, self-correction, and ensemble methods to
yield a new state-of-the-art result on challenging Text-to-SQL benchmarks.
Specifically, when configured using GPT-3.5-Turbo as the base LLM, SelECT-SQL
achieves 84.2% execution accuracy on the Spider leaderboard's development set,
exceeding both the best results of other baseline GPT-3.5-Turbo-based solutions
(81.1%), and the peak performance (83.5%) of the GPT-4 result reported on the
leaderboard.",2024-09-16,"Ke Shen, Mayank Kejriwal",http://arxiv.org/pdf/2409.10007v1,cs.CL
Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM based system,"This paper provides a comprehensive survey of sentiment analysis within the
context of artificial intelligence (AI) and large language models (LLMs).
Sentiment analysis, a critical aspect of natural language processing (NLP), has
evolved significantly from traditional rule-based methods to advanced deep
learning techniques. This study examines the historical development of
sentiment analysis, highlighting the transition from lexicon-based and
pattern-based approaches to more sophisticated machine learning and deep
learning models. Key challenges are discussed, including handling bilingual
texts, detecting sarcasm, and addressing biases. The paper reviews
state-of-the-art approaches, identifies emerging trends, and outlines future
research directions to advance the field. By synthesizing current methodologies
and exploring future opportunities, this survey aims to understand sentiment
analysis in the AI and LLM context thoroughly.",2024-09-16,"Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh",http://arxiv.org/pdf/2409.09989v1,cs.CL
Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for Fine-grained Text Evaluations,"Large Language Models (LLMs) show promise as a writing aid for professionals
performing legal analyses. However, LLMs can often hallucinate in this setting,
in ways difficult to recognize by non-professionals and existing text
evaluation metrics. In this work, we pose the question: when can
machine-generated legal analysis be evaluated as acceptable? We introduce the
neutral notion of gaps, as opposed to hallucinations in a strict erroneous
sense, to refer to the difference between human-written and machine-generated
legal analysis. Gaps do not always equate to invalid generation. Working with
legal experts, we consider the CLERC generation task proposed in Hou et al.
(2024b), leading to a taxonomy, a fine-grained detector for predicting gap
categories, and an annotated dataset for automatic evaluation. Our best
detector achieves 67% F1 score and 80% precision on the test set. Employing
this detector as an automated metric on legal analysis generated by SOTA LLMs,
we find around 80% contain hallucinations of different kinds.",2024-09-16,"Abe Bohan Hou, William Jurayj, Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme",http://arxiv.org/pdf/2409.09947v2,cs.CL
"Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges","As large language models achieve increasingly impressive results, questions
arise about whether such performance is from generalizability or mere data
memorization. Thus, numerous data contamination detection methods have been
proposed. However, these approaches are often validated with traditional
benchmarks and early-stage LLMs, leaving uncertainty about their effectiveness
when evaluating state-of-the-art LLMs on the contamination of more challenging
benchmarks. To address this gap and provide a dual investigation of SOTA LLM
contamination status and detection method robustness, we evaluate five
contamination detection approaches with four state-of-the-art LLMs across eight
challenging datasets often used in modern LLM evaluation. Our analysis reveals
that (1) Current methods have non-trivial limitations in their assumptions and
practical applications; (2) Notable difficulties exist in detecting
contamination introduced during instruction fine-tuning with answer
augmentation; and (3) Limited consistencies between SOTA contamination
detection techniques. These findings highlight the complexity of contamination
detection in advanced LLMs and the urgent need for further research on robust
and generalizable contamination evaluation. Our code is available at
https://github.com/vsamuel2003/data-contamination.",2024-09-16,"Vinay Samuel, Yue Zhou, Henry Peng Zou",http://arxiv.org/pdf/2409.09927v2,cs.CL
SFR-RAG: Towards Contextually Faithful LLMs,"Retrieval Augmented Generation (RAG), a paradigm that integrates external
contextual information with large language models (LLMs) to enhance factual
accuracy and relevance, has emerged as a pivotal area in generative AI. The
LLMs used in RAG applications are required to faithfully and completely
comprehend the provided context and users' questions, avoid hallucination,
handle unanswerable, counterfactual or otherwise low-quality and irrelevant
contexts, perform complex multi-hop reasoning and produce reliable citations.
In this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with
an emphasis on context-grounded generation and hallucination minimization. We
also present ContextualBench, a new evaluation framework compiling multiple
popular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with
consistent RAG settings to ensure reproducibility and consistency in model
assessments. Experimental results demonstrate that our SFR-RAG-9B model
outperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving
state-of-the-art results in 3 out of 7 benchmarks in ContextualBench with
significantly fewer parameters. The model is also shown to be resilient to
alteration in the contextual information and behave appropriately when relevant
context is removed. Additionally, the SFR-RAG model maintains competitive
performance in general instruction-following tasks and function-calling
capabilities.",2024-09-16,"Xuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam, Austin Xu, Hailin Chen, Yifei Ming, Zixuan Ke, Silvio Savarese, Caiming Xong, Shafiq Joty",http://arxiv.org/pdf/2409.09916v1,cs.CL
Rediscovering the Latent Dimensions of Personality with Large Language Models as Trait Descriptors,"Assessing personality traits using large language models (LLMs) has emerged
as an interesting and challenging area of research. While previous methods
employ explicit questionnaires, often derived from the Big Five model of
personality, we hypothesize that LLMs implicitly encode notions of personality
when modeling next-token responses. To demonstrate this, we introduce a novel
approach that uncovers latent personality dimensions in LLMs by applying
singular value de-composition (SVD) to the log-probabilities of
trait-descriptive adjectives. Our experiments show that LLMs ""rediscover"" core
personality traits such as extraversion, agreeableness, conscientiousness,
neuroticism, and openness without relying on direct questionnaire inputs, with
the top-5 factors corresponding to Big Five traits explaining 74.3% of the
variance in the latent space. Moreover, we can use the derived principal
components to assess personality along the Big Five dimensions, and achieve
improvements in average personality prediction accuracy of up to 5% over
fine-tuned models, and up to 21% over direct LLM-based scoring techniques.",2024-09-16,"Joseph Suh, Suhong Moon, Minwoo Kang, David M. Chan",http://arxiv.org/pdf/2409.09905v1,cs.CL
Acquiring Pronunciation Knowledge from Transcribed Speech Audio via Multi-task Learning,"Recent work has shown the feasibility and benefit of bootstrapping an
integrated sequence-to-sequence (Seq2Seq) linguistic frontend from a
traditional pipeline-based frontend for text-to-speech (TTS). To overcome the
fixed lexical coverage of bootstrapping training data, previous work has
proposed to leverage easily accessible transcribed speech audio as an
additional training source for acquiring novel pronunciation knowledge for
uncovered words, which relies on an auxiliary ASR model as part of a cumbersome
implementation flow. In this work, we propose an alternative method to leverage
transcribed speech audio as an additional training source, based on multi-task
learning (MTL). Experiments show that, compared to a baseline Seq2Seq frontend,
the proposed MTL-based method reduces PER from 2.5% to 1.6% for those word
types covered exclusively in transcribed speech audio, achieving a similar
performance to the previous method but with a much simpler implementation flow.",2024-09-15,"Siqi Sun, Korin Richmond",http://arxiv.org/pdf/2409.09891v1,cs.CL
Optimizing the Songwriting Process: Genre-Based Lyric Generation Using Deep Learning Models,"The traditional songwriting process is rather complex and this is evident in
the time it takes to produce lyrics that fit the genre and form comprehensive
verses. Our project aims to simplify this process with deep learning
techniques, thus optimizing the songwriting process and enabling an artist to
hit their target audience by staying in genre. Using a dataset of 18,000 songs
off Spotify, we developed a unique preprocessing format using tokens to parse
lyrics into individual verses. These results were used to train a baseline
pretrained seq2seq model, and a LSTM-based neural network models according to
song genres. We found that generation yielded higher recall (ROUGE) in the
baseline model, but similar precision (BLEU) for both models. Qualitatively, we
found that many of the lyrical phrases generated by the original model were
still comprehensible and discernible between which genres they fit into,
despite not necessarily being the exact the same as the true lyrics. Overall,
our results yielded that lyric generation can reasonably be sped up to produce
genre-based lyrics and aid in hastening the songwriting process.",2024-09-15,"Tracy Cai, Wilson Liang, Donte Townes",http://arxiv.org/pdf/2409.13758v1,cs.CL
S2Cap: A Benchmark and a Baseline for Singing Style Captioning,"Singing voices contain much richer information than common voices, such as
diverse vocal and acoustic characteristics. However, existing open-source
audio-text datasets for singing voices capture only a limited set of attributes
and lacks acoustic features, leading to limited utility towards downstream
tasks, such as style captioning. To fill this gap, we formally consider the
task of singing style captioning and introduce S2Cap, a singing voice dataset
with comprehensive descriptions of diverse vocal, acoustic and demographic
attributes. Based on this dataset, we develop a simple yet effective baseline
algorithm for the singing style captioning. The algorithm utilizes two novel
technical components: CRESCENDO for mitigating misalignment between pretrained
unimodal models, and demixing supervision to regularize the model to focus on
the singing voice. Despite its simplicity, the proposed method outperforms
state-of-the-art baselines.",2024-09-15,"Hyunjong Ok, Jaeho Lee",http://arxiv.org/pdf/2409.09866v2,cs.CL
A Benchmark Dataset with Larger Context for Non-Factoid Question Answering over Islamic Text,"Accessing and comprehending religious texts, particularly the Quran (the
sacred scripture of Islam) and Ahadith (the corpus of the sayings or traditions
of the Prophet Muhammad), in today's digital era necessitates efficient and
accurate Question-Answering (QA) systems. Yet, the scarcity of QA systems
tailored specifically to the detailed nature of inquiries about the Quranic
Tafsir (explanation, interpretation, context of Quran for clarity) and Ahadith
poses significant challenges. To address this gap, we introduce a comprehensive
dataset meticulously crafted for QA purposes within the domain of Quranic
Tafsir and Ahadith. This dataset comprises a robust collection of over 73,000
question-answer pairs, standing as the largest reported dataset in this
specialized domain. Importantly, both questions and answers within the dataset
are meticulously enriched with contextual information, serving as invaluable
resources for training and evaluating tailored QA systems. However, while this
paper highlights the dataset's contributions and establishes a benchmark for
evaluating QA performance in the Quran and Ahadith domains, our subsequent
human evaluation uncovered critical insights regarding the limitations of
existing automatic evaluation techniques. The discrepancy between automatic
evaluation metrics, such as ROUGE scores, and human assessments became
apparent. The human evaluation indicated significant disparities: the model's
verdict consistency with expert scholars ranged between 11% to 20%, while its
contextual understanding spanned a broader spectrum of 50% to 90%. These
findings underscore the necessity for evaluation techniques that capture the
nuances and complexities inherent in understanding religious texts, surpassing
the limitations of traditional automatic metrics.",2024-09-15,"Faiza Qamar, Seemab Latif, Rabia Latif",http://arxiv.org/pdf/2409.09844v1,cs.CL
Generating Synthetic Free-text Medical Records with Low Re-identification Risk using Masked Language Modeling,"The vast amount of available medical records has the potential to improve
healthcare and biomedical research. However, privacy restrictions make these
data accessible for internal use only. Recent works have addressed this problem
by generating synthetic data using Causal Language Modeling. Unfortunately, by
taking this approach, it is often impossible to guarantee patient privacy while
offering the ability to control the diversity of generations without increasing
the cost of generating such data. In contrast, we present a system for
generating synthetic free-text medical records using Masked Language Modeling.
The system preserves critical medical information while introducing diversity
in the generations and minimising re-identification risk. The system's size is
about 120M parameters, minimising inference cost. The results demonstrate
high-quality synthetic data with a HIPAA-compliant PHI recall rate of 96% and a
re-identification risk of 3.5%. Moreover, downstream evaluations show that the
generated data can effectively train a model with performance comparable to
real data.",2024-09-15,"Samuel Belkadi, Libo Ren, Nicolo Micheletti, Lifeng Han, Goran Nenadic",http://arxiv.org/pdf/2409.09831v3,cs.CL
GP-GPT: Large Language Model for Gene-Phenotype Mapping,"Pre-trained large language models(LLMs) have attracted increasing attention
in biomedical domains due to their success in natural language processing.
However, the complex traits and heterogeneity of multi-sources genomics data
pose significant challenges when adapting these models to the bioinformatics
and biomedical field. To address these challenges, we present GP-GPT, the first
specialized large language model for genetic-phenotype knowledge representation
and genomics relation analysis. Our model is fine-tuned in two stages on a
comprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,
and medical genetics, derived from multiple large-scale validated datasets and
scientific publications. GP-GPT demonstrates proficiency in accurately
retrieving medical genetics information and performing common genomics analysis
tasks, such as genomics information retrieval and relationship determination.
Comparative experiments across domain-specific tasks reveal that GP-GPT
outperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These
results highlight GP-GPT's potential to enhance genetic disease relation
research and facilitate accurate and efficient analysis in the fields of
genomics and medical genetics. Our investigation demonstrated the subtle
changes of bio-factor entities' representations in the GP-GPT, which suggested
the opportunities for the application of LLMs to advancing gene-phenotype
research.",2024-09-15,"Yanjun Lyu, Zihao Wu, Lu Zhang, Jing Zhang, Yiwei Li, Wei Ruan, Zhengliang Liu, Xiaowei Yu, Chao Cao, Tong Chen, Minheng Chen, Yan Zhuang, Xiang Li, Rongjie Liu, Chao Huang, Wentao Li, Tianming Liu, Dajiang Zhu",http://arxiv.org/pdf/2409.09825v2,cs.CL
Causal Inference with Large Language Model: A Survey,"Causal inference has been a pivotal challenge across diverse domains such as
medicine and economics, demanding a complicated integration of human knowledge,
mathematical reasoning, and data mining capabilities. Recent advancements in
natural language processing (NLP), particularly with the advent of large
language models (LLMs), have introduced promising opportunities for traditional
causal inference tasks. This paper reviews recent progress in applying LLMs to
causal inference, encompassing various tasks spanning different levels of
causation. We summarize the main causal problems and approaches, and present a
comparison of their evaluation results in different causal scenarios.
Furthermore, we discuss key findings and outline directions for future
research, underscoring the potential implications of integrating LLMs in
advancing causal inference methodologies.",2024-09-15,Jing Ma,http://arxiv.org/pdf/2409.09822v3,cs.CL
Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models,"Despite recent advances demonstrating vision-language models' (VLMs)
abilities to describe complex relationships in images using natural language,
their capability to quantitatively reason about object sizes and distances
remains underexplored. In this work, we introduce a manually annotated
benchmark, Q-Spatial Bench, with 271 questions across five categories designed
for quantitative spatial reasoning and systematically investigate the
performance of state-of-the-art VLMs on this task. Our analysis reveals that
reasoning about distances between objects is particularly challenging for SoTA
VLMs; however, some VLMs significantly outperform others, with an over 40-point
gap between the two best performing models. We also make the surprising
observation that the success rate of the top-performing VLM increases by 19
points when a reasoning path using a reference object emerges naturally in the
response. Inspired by this observation, we develop a zero-shot prompting
technique, SpatialPrompt, that encourages VLMs to answer quantitative spatial
questions using reference objects as visual cues. By instructing VLMs to use
reference objects in their reasoning paths via SpatialPrompt, Gemini 1.5 Pro,
Gemini 1.5 Flash, and GPT-4V improve their success rates by over 40, 20, and 30
points, respectively. We emphasize that these significant improvements are
obtained without needing more data, model architectural modifications, or
fine-tuning.",2024-09-15,"Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna",http://arxiv.org/pdf/2409.09788v1,cs.CL
"Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition","Given recent advances in generative AI technology, a key question is how
large language models (LLMs) can enhance acoustic modeling tasks using text
decoding results from a frozen, pretrained automatic speech recognition (ASR)
model. To explore new capabilities in language modeling for speech processing,
we introduce the generative speech transcription error correction (GenSEC)
challenge. This challenge comprises three post-ASR language modeling tasks: (i)
post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion
recognition. These tasks aim to emulate future LLM-based agents handling
voice-based interfaces while remaining accessible to a broad audience by
utilizing open pretrained language models or agent-based APIs. We also discuss
insights from baseline evaluations, as well as lessons learned for designing
future evaluations.",2024-09-15,"Chao-Han Huck Yang, Taejin Park, Yuan Gong, Yuanchao Li, Zhehuai Chen, Yen-Ting Lin, Chen Chen, Yuchen Hu, Kunal Dhawan, Piotr Żelasko, Chao Zhang, Yun-Nung Chen, Yu Tsao, Jagadeesh Balam, Boris Ginsburg, Sabato Marco Siniscalchi, Eng Siong Chng, Peter Bell, Catherine Lai, Shinji Watanabe, Andreas Stolcke",http://arxiv.org/pdf/2409.09785v3,cs.CL
Language Models and Retrieval Augmented Generation for Automated Structured Data Extraction from Diagnostic Reports,"Purpose: To develop and evaluate an automated system for extracting
structured clinical information from unstructured radiology and pathology
reports using open-weights large language models (LMs) and retrieval augmented
generation (RAG), and to assess the effects of model configuration variables on
extraction performance. Methods and Materials: The study utilized two datasets:
7,294 radiology reports annotated for Brain Tumor Reporting and Data System
(BT-RADS) scores and 2,154 pathology reports annotated for isocitrate
dehydrogenase (IDH) mutation status. An automated pipeline was developed to
benchmark the performance of various LMs and RAG configurations. The impact of
model size, quantization, prompting strategies, output formatting, and
inference parameters was systematically evaluated. Results: The best performing
models achieved over 98% accuracy in extracting BT-RADS scores from radiology
reports and over 90% for IDH mutation status extraction from pathology reports.
The top model being medical fine-tuned llama3. Larger, newer, and domain
fine-tuned models consistently outperformed older and smaller models. Model
quantization had minimal impact on performance. Few-shot prompting
significantly improved accuracy. RAG improved performance for complex pathology
reports but not for shorter radiology reports. Conclusions: Open LMs
demonstrate significant potential for automated extraction of structured
clinical data from unstructured clinical reports with local privacy-preserving
application. Careful model selection, prompt engineering, and semi-automated
optimization using annotated data are critical for optimal performance. These
approaches could be reliable enough for practical use in research workflows,
highlighting the potential for human-machine collaboration in healthcare data
extraction.",2024-09-15,"Mohamed Sobhi Jabal, Pranav Warman, Jikai Zhang, Kartikeye Gupta, Ayush Jain, Maciej Mazurowski, Walter Wiggins, Kirti Magudia, Evan Calabrese",http://arxiv.org/pdf/2409.10576v2,cs.CL
Efficient Hybrid Inference for LLMs: Reward-Based Token Modelling with Selective Cloud Assistance,"Large language models (LLMs) are known for their exceptional performance
across a range of natural language processing tasks, but their deployment comes
at a high computational and financial cost. On the other hand, smaller language
models (SLMs), which can be deployed on lower-cost edge devices, struggle to
match the performance of their larger counterparts. This paper presents a novel
hybrid inference approach that leverages the strengths of both model types
while minimizing reliance on costly cloud-based LLMs. Unlike existing methods
that route entire queries to either an SLM or a cloud LLM, our approach
introduces a reward-based mechanism to dynamically determine the involvement of
the cloud LLM during token generation. Specifically, each token predicted by
the SLM is evaluated against a reward score, and only when this score falls
below a certain threshold is the cloud LLM consulted for assistance in the next
token prediction. This method not only reduces the traffic to the cloud LLM,
thereby lowering costs, but also allows for flexible control over response
quality depending on the reward score threshold. Experimental results
demonstrate that our approach significantly reduces cloud LLM usage with
minimal impact on overall response quality, offering a cost-effective solution
for deploying high-performance language models",2024-09-15,"Adarsh MS, Jithin VG, Ditto PS",http://arxiv.org/pdf/2409.13757v1,cs.CL
ELMI: Interactive and Intelligent Sign Language Translation of Lyrics for Song Signing,"d/Deaf and hearing song-signers have become prevalent across video-sharing
platforms, but translating songs into sign language remains cumbersome and
inaccessible. Our formative study revealed the challenges song-signers face,
including semantic, syntactic, expressive, and rhythmic considerations in
translations. We present ELMI, an accessible song-signing tool that assists in
translating lyrics into sign language. ELMI enables users to edit glosses
line-by-line, with real-time synced lyric and music video snippets. Users can
also chat with a large language model-driven AI to discuss meaning, glossing,
emoting, and timing. Through an exploratory study with 13 song-signers, we
examined how ELMI facilitates their workflows and how song-signers leverage and
receive an LLM-driven chat for translation. Participants successfully adopted
ELMI to song-signing, with active discussions throughout. They also reported
improved confidence and independence in their translations, finding ELMI
encouraging, constructive, and informative. We discuss research and design
implications for accessible and culturally sensitive song-signing translation
tools.",2024-09-15,"Suhyeon Yoo, Khai N. Truong, Young-Ho Kim",http://arxiv.org/pdf/2409.09760v3,cs.CL
Language Models Learn Metadata: Political Stance Detection Case Study,"Stance detection is a crucial NLP task with numerous applications in social
science, from analyzing online discussions to assessing political campaigns.
This paper investigates the optimal way to incorporate metadata into a
political stance detection task. We demonstrate that previous methods combining
metadata with language-based data for political stance detection have not fully
utilized the metadata information; our simple baseline, using only party
membership information, surpasses the current state-of-the-art. We then show
that prepending metadata (e.g., party and policy) to political speeches
performs best, outperforming all baselines, indicating that complex metadata
inclusion systems may not learn the task optimally.",2024-09-15,"Stanley Cao, Felix Drinkall",http://arxiv.org/pdf/2409.13756v1,cs.CL
Benchmarking LLMs in Political Content Text-Annotation: Proof-of-Concept with Toxicity and Incivility Data,"This article benchmarked the ability of OpenAI's GPTs and a number of
open-source LLMs to perform annotation tasks on political content. We used a
novel protest event dataset comprising more than three million digital
interactions and created a gold standard that includes ground-truth labels
annotated by human coders about toxicity and incivility on social media. We
included in our benchmark Google's Perspective algorithm, which, along with
GPTs, was employed throughout their respective APIs while the open-source LLMs
were deployed locally. The findings show that Perspective API using a laxer
threshold, GPT-4o, and Nous Hermes 2 Mixtral outperform other LLM's zero-shot
classification annotations. In addition, Nous Hermes 2 and Mistral OpenOrca,
with a smaller number of parameters, are able to perform the task with high
performance, being attractive options that could offer good trade-offs between
performance, implementing costs and computing time. Ancillary findings using
experiments setting different temperature levels show that although GPTs tend
to show not only excellent computing time but also overall good levels of
reliability, only open-source LLMs ensure full reproducibility in the
annotation.",2024-09-15,Bastián González-Bustamante,http://arxiv.org/pdf/2409.09741v1,cs.CL
PersonaMark: Personalized LLM watermarking for model protection and user attribution,"The rapid advancement of customized Large Language Models (LLMs) offers
considerable convenience. However, it also intensifies concerns regarding the
protection of copyright/confidential information. With the extensive adoption
of private LLMs, safeguarding model copyright and ensuring data privacy have
become critical. Text watermarking has emerged as a viable solution for
detecting AI-generated content and protecting models. However, existing methods
fall short in providing individualized watermarks for each user, a critical
feature for enhancing accountability and traceability. In this paper, we
introduce PersonaMark, a novel personalized text watermarking scheme designed
to protect LLMs' copyrights and bolster accountability. PersonaMark leverages
sentence structure as a subtle carrier of watermark information and optimizes
the generation process to maintain the natural output of the model. By
employing a personalized hashing function, unique watermarks are embedded for
each user, enabling high-quality text generation without compromising the
model's performance. This approach is both time-efficient and scalable, capable
of handling large numbers of users through a multi-user hashing mechanism. To
the best of our knowledge, this is a pioneer study to explore personalized
watermarking in LLMs. We conduct extensive evaluations across four LLMs,
analyzing various metrics such as perplexity, sentiment, alignment, and
readability. The results validate that PersonaMark preserves text quality,
ensures unbiased watermark insertion, and offers robust watermark detection
capabilities, all while maintaining the model's behavior with minimal
disruption.",2024-09-15,"Yuehan Zhang, Peizhuo Lv, Yinpeng Liu, Yongqiang Ma, Wei Lu, Xiaofeng Wang, Xiaozhong Liu, Jiawei Liu",http://arxiv.org/pdf/2409.09739v2,cs.CL
Automatic Control With Human-Like Reasoning: Exploring Language Model Embodied Air Traffic Agents,"Recent developments in language models have created new opportunities in air
traffic control studies. The current focus is primarily on text and
language-based use cases. However, these language models may offer a higher
potential impact in the air traffic control domain, thanks to their ability to
interact with air traffic environments in an embodied agent form. They also
provide a language-like reasoning capability to explain their decisions, which
has been a significant roadblock for the implementation of automatic air
traffic control.
  This paper investigates the application of a language model-based agent with
function-calling and learning capabilities to resolve air traffic conflicts
without human intervention. The main components of this research are
foundational large language models, tools that allow the agent to interact with
the simulator, and a new concept, the experience library. An innovative part of
this research, the experience library, is a vector database that stores
synthesized knowledge that agents have learned from interactions with the
simulations and language models.
  To evaluate the performance of our language model-based agent, both
open-source and closed-source models were tested. The results of our study
reveal significant differences in performance across various configurations of
the language model-based agents. The best-performing configuration was able to
solve almost all 120 but one imminent conflict scenarios, including up to four
aircraft at the same time. Most importantly, the agents are able to provide
human-level text explanations on traffic situations and conflict resolution
strategies.",2024-09-15,"Justas Andriuškevičius, Junzi Sun",http://arxiv.org/pdf/2409.09717v1,cs.CL
AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using LLMs,"In recent years, there has been a surge in the publication of clinical trial
reports, making it challenging to conduct systematic reviews. Automatically
extracting Population, Intervention, Comparator, and Outcome (PICO) from
clinical trial studies can alleviate the traditionally time-consuming process
of manually scrutinizing systematic reviews. Existing approaches of PICO frame
extraction involves supervised approach that relies on the existence of
manually annotated data points in the form of BIO label tagging. Recent
approaches, such as In-Context Learning (ICL), which has been shown to be
effective for a number of downstream NLP tasks, require the use of labeled
examples. In this work, we adopt ICL strategy by employing the pretrained
knowledge of Large Language Models (LLMs), gathered during the pretraining
phase of an LLM, to automatically extract the PICO-related terminologies from
clinical trial documents in unsupervised set up to bypass the availability of
large number of annotated data instances. Additionally, to showcase the highest
effectiveness of LLM in oracle scenario where large number of annotated samples
are available, we adopt the instruction tuning strategy by employing Low Rank
Adaptation (LORA) to conduct the training of gigantic model in low resource
environment for the PICO frame extraction task. Our empirical results show that
our proposed ICL-based framework produces comparable results on all the version
of EBM-NLP datasets and the proposed instruction tuned version of our framework
produces state-of-the-art results on all the different EBM-NLP datasets. Our
project is available at \url{https://github.com/shrimonmuke0202/AlpaPICO.git}.",2024-09-15,"Madhusudan Ghosh, Shrimon Mukherjee, Asmit Ganguly, Partha Basuchowdhuri, Sudip Kumar Naskar, Debasis Ganguly",http://arxiv.org/pdf/2409.09704v1,cs.CL
Entity-Aware Self-Attention and Contextualized GCN for Enhanced Relation Extraction in Long Sentences,"Relation extraction as an important natural Language processing (NLP) task is
to identify relations between named entities in text. Recently, graph
convolutional networks over dependency trees have been widely used to capture
syntactic features and achieved attractive performance. However, most existing
dependency-based approaches ignore the positive influence of the words outside
the dependency trees, sometimes conveying rich and useful information on
relation extraction. In this paper, we propose a novel model, Entity-aware
Self-attention Contextualized GCN (ESC-GCN), which efficiently incorporates
syntactic structure of input sentences and semantic context of sequences. To be
specific, relative position self-attention obtains the overall semantic
pairwise correlation related to word position, and contextualized graph
convolutional networks capture rich intra-sentence dependencies between words
by adequately pruning operations. Furthermore, entity-aware attention layer
dynamically selects which token is more decisive to make final relation
prediction. In this way, our proposed model not only reduces the noisy impact
from dependency trees, but also obtains easily-ignored entity-related semantic
representation. Extensive experiments on various tasks demonstrate that our
model achieves encouraging performance as compared to existing dependency-based
and sequence-based models. Specially, our model excels in extracting relations
between entities of long sentences.",2024-09-15,"Xin Wang, Xinyi Bai",http://arxiv.org/pdf/2409.13755v2,cs.CL
ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models,"Expressing stressful experiences in words is proven to improve mental and
physical health, but individuals often disengage with writing interventions as
they struggle to organize their thoughts and emotions. Reflective prompts have
been used to provide direction, and large language models (LLMs) have
demonstrated the potential to provide tailored guidance. However, current
systems often limit users' flexibility to direct their reflections. We thus
present ExploreSelf, an LLM-driven application designed to empower users to
control their reflective journey, providing adaptive support through
dynamically generated questions. Through an exploratory study with 19
participants, we examine how participants explore and reflect on personal
challenges using ExploreSelf. Our findings demonstrate that participants valued
the flexible navigation of adaptive guidance to control their reflective
journey, leading to deeper engagement and insight. Building on our findings, we
discuss the implications of designing LLM-driven tools that facilitate
user-driven and effective reflection of personal challenges.",2024-09-15,"Inhwa Song, SoHyun Park, Sachin R. Pendse, Jessica Lee Schleider, Munmun De Choudhury, Young-Ho Kim",http://arxiv.org/pdf/2409.09662v3,cs.CL
Leveraging Open-Source Large Language Models for Native Language Identification,"Native Language Identification (NLI) - the task of identifying the native
language (L1) of a person based on their writing in the second language (L2) -
has applications in forensics, marketing, and second language acquisition.
Historically, conventional machine learning approaches that heavily rely on
extensive feature engineering have outperformed transformer-based language
models on this task. Recently, closed-source generative large language models
(LLMs), e.g., GPT-4, have demonstrated remarkable performance on NLI in a
zero-shot setting, including promising results in open-set classification.
However, closed-source LLMs have many disadvantages, such as high costs and
undisclosed nature of training data. This study explores the potential of using
open-source LLMs for NLI. Our results indicate that open-source LLMs do not
reach the accuracy levels of closed-source LLMs when used out-of-the-box.
However, when fine-tuned on labeled training data, open-source LLMs can achieve
performance comparable to that of commercial LLMs.",2024-09-15,"Yee Man Ng, Ilia Markov",http://arxiv.org/pdf/2409.09659v2,cs.CL
Unveiling Gender Bias in Large Language Models: Using Teacher's Evaluation in Higher Education As an Example,"This paper investigates gender bias in Large Language Model (LLM)-generated
teacher evaluations in higher education setting, focusing on evaluations
produced by GPT-4 across six academic subjects. By applying a comprehensive
analytical framework that includes Odds Ratio (OR) analysis, Word Embedding
Association Test (WEAT), sentiment analysis, and contextual analysis, this
paper identified patterns of gender-associated language reflecting societal
stereotypes. Specifically, words related to approachability and support were
used more frequently for female instructors, while words related to
entertainment were predominantly used for male instructors, aligning with the
concepts of communal and agentic behaviors. The study also found moderate to
strong associations between male salient adjectives and male names, though
career and family words did not distinctly capture gender biases. These
findings align with prior research on societal norms and stereotypes,
reinforcing the notion that LLM-generated text reflects existing biases.",2024-09-15,Yuanning Huang,http://arxiv.org/pdf/2409.09652v1,cs.CL
A Simple HMM with Self-Supervised Representations for Phone Segmentation,"Despite the recent advance in self-supervised representations, unsupervised
phonetic segmentation remains challenging. Most approaches focus on improving
phonetic representations with self-supervised learning, with the hope that the
improvement can transfer to phonetic segmentation. In this paper, contrary to
recent approaches, we show that peak detection on Mel spectrograms is a strong
baseline, better than many self-supervised approaches. Based on this finding,
we propose a simple hidden Markov model that uses self-supervised
representations and features at the boundaries for phone segmentation. Our
results demonstrate consistent improvements over previous approaches, with a
generalized formulation allowing versatile design adaptations.",2024-09-15,"Gene-Ping Yang, Hao Tang",http://arxiv.org/pdf/2409.09646v2,cs.CL
Towards understanding evolution of science through language model series,"We introduce AnnualBERT, a series of language models designed specifically to
capture the temporal evolution of scientific text. Deviating from the
prevailing paradigms of subword tokenizations and ""one model to rule them all"",
AnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model
pretrained from scratch on the full-text of 1.7 million arXiv papers published
until 2008 and a collection of progressively trained models on arXiv papers at
an annual basis. We demonstrate the effectiveness of AnnualBERT models by
showing that they not only have comparable performances in standard tasks but
also achieve state-of-the-art performances on domain-specific NLP tasks as well
as link prediction tasks in the arXiv citation network. We then utilize probing
tasks to quantify the models' behavior in terms of representation learning and
forgetting as time progresses. Our approach enables the pretrained models to
not only improve performances on scientific text processing tasks but also to
provide insights into the development of scientific discourse over time. The
series of the models is available at https://huggingface.co/jd445/AnnualBERTs.",2024-09-15,"Junjie Dong, Zhuoqi Lyu, Qing Ke",http://arxiv.org/pdf/2409.09636v2,cs.CL
Confidence Estimation for LLM-Based Dialogue State Tracking,"Estimation of a model's confidence on its outputs is critical for
Conversational AI systems based on large language models (LLMs), especially for
reducing hallucination and preventing over-reliance. In this work, we provide
an exhaustive exploration of methods, including approaches proposed for open-
and closed-weight LLMs, aimed at quantifying and leveraging model uncertainty
to improve the reliability of LLM-generated responses, specifically focusing on
dialogue state tracking (DST) in task-oriented dialogue systems (TODS).
Regardless of the model type, well-calibrated confidence scores are essential
to handle uncertainties, thereby improving model performance. We evaluate four
methods for estimating confidence scores based on softmax, raw token scores,
verbalized confidences, and a combination of these methods, using the area
under the curve (AUC) metric to assess calibration, with higher AUC indicating
better calibration. We also enhance these with a self-probing mechanism,
proposed for closed models. Furthermore, we assess these methods using an
open-weight model fine-tuned for the task of DST, achieving superior joint goal
accuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can
result in enhanced AUC performance, indicating better confidence score
calibration.",2024-09-15,"Yi-Jyun Sun, Suvodip Dey, Dilek Hakkani-Tur, Gokhan Tur",http://arxiv.org/pdf/2409.09629v2,cs.CL
Enhancing Text Annotation through Rationale-Driven Collaborative Few-Shot Prompting,"The traditional data annotation process is often labor-intensive,
time-consuming, and susceptible to human bias, which complicates the management
of increasingly complex datasets. This study explores the potential of large
language models (LLMs) as automated data annotators to improve efficiency and
consistency in annotation tasks. By employing rationale-driven collaborative
few-shot prompting techniques, we aim to improve the performance of LLMs in
text annotation. We conduct a rigorous evaluation of six LLMs across four
benchmark datasets, comparing seven distinct methodologies. Our results
demonstrate that collaborative methods consistently outperform traditional
few-shot techniques and other baseline approaches, particularly in complex
annotation tasks. Our work provides valuable insights and a robust framework
for leveraging collaborative learning methods to tackle challenging text
annotation tasks.",2024-09-15,"Jianfei Wu, Xubin Wang, Weijia Jia",http://arxiv.org/pdf/2409.09615v1,cs.CL
Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text Quality Filtering in Large Web Corpora,"With the increasing demand for substantial amounts of high-quality data to
train large language models (LLMs), efficiently filtering large web corpora has
become a critical challenge. For this purpose, KenLM, a lightweight
n-gram-based language model that operates on CPUs, is widely used. However, the
traditional method of training KenLM utilizes only high-quality data and,
consequently, does not explicitly learn the linguistic patterns of low-quality
data. To address this issue, we propose an ensemble approach that leverages two
contrasting KenLMs: (i) Good KenLM, trained on high-quality data; and (ii) Bad
KenLM, trained on low-quality data. Experimental results demonstrate that our
approach significantly reduces noisy content while preserving high-quality
content compared to the traditional KenLM training method. This indicates that
our method can be a practical solution with minimal computational overhead for
resource-constrained environments.",2024-09-15,"Yungi Kim, Hyunsoo Ha, Sukyung Lee, Jihoo Kim, Seonghoon Yang, Chanjun Park",http://arxiv.org/pdf/2409.09613v1,cs.CL
Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison,"The goal of aligning language models to human preferences requires data that
reveal these preferences. Ideally, time and money can be spent carefully
collecting and tailoring bespoke preference data to each downstream
application. However, in practice, a select few publicly available preference
datasets are often used to train reward models for reinforcement learning from
human feedback (RLHF). While new preference datasets are being introduced with
increasing frequency, there are currently no existing efforts to measure and
compare these datasets. In this paper, we systematically study preference
datasets through three perspectives: scale, label noise, and information
content. We propose specific metrics for each of these perspectives and uncover
different axes of comparison for a better understanding of preference datasets.
Our work is a first step towards a data-centric approach to alignment by
providing perspectives that aid in training efficiency and iterative data
collection for RLHF.",2024-09-15,"Judy Hanwen Shen, Archit Sharma, Jun Qin",http://arxiv.org/pdf/2409.09603v1,cs.CL
Improving Statistical Significance in Human Evaluation of Automatic Metrics via Soft Pairwise Accuracy,"Selecting an automatic metric that best emulates human annotators is often
non-trivial, because there is no clear definition of ""best emulates."" A
meta-metric is required to compare the human judgments to the automatic metric
scores, and metric rankings depend on the choice of meta-metric. We propose
Soft Pairwise Accuracy (SPA), a new meta-metric that builds on Pairwise
Accuracy (PA) but incorporates the statistical significance of both the human
judgments and the metric scores. We show that SPA is more stable than PA with
respect to changes in the number of systems/segments used for evaluation. We
also show that PA can only assign a small set of distinct output values to
metrics, and this results in many metrics being artificially assigned the exact
same PA score. We demonstrate that SPA fixes this issue. Finally, we show that
SPA is more discriminative than PA, producing more statistically significant
comparisons between metrics. SPA was selected as the official system-level
metric for the 2024 WMT Metrics Shared Task.",2024-09-15,"Brian Thompson, Nitika Mathur, Daniel Deutsch, Huda Khayrallah",http://arxiv.org/pdf/2409.09598v2,cs.CL
ValueCompass: A Framework for Measuring Contextual Value Alignment Between Human and LLMs,"As AI systems become more advanced, ensuring their alignment with a diverse
range of individuals and societal values becomes increasingly critical. But how
can we capture fundamental human values and assess the degree to which AI
systems align with them? We introduce ValueCompass, a framework of fundamental
values, grounded in psychological theory and a systematic review, to identify
and evaluate human-AI alignment. We apply ValueCompass to measure the value
alignment of humans and large language models (LLMs) across four real-world
scenarios: collaborative writing, education, public sectors, and healthcare.
Our findings reveal concerning misalignments between humans and LLMs, such as
humans frequently endorse values like ""National Security"" which were largely
rejected by LLMs. We also observe that values differ across scenarios,
highlighting the need for context-aware AI alignment strategies. This work
provides valuable insights into the design space of human-AI alignment, laying
the foundations for developing AI systems that responsibly reflect societal
values and ethics.",2024-09-15,"Hua Shen, Tiffany Knearem, Reshmi Ghosh, Yu-Ju Yang, Nicholas Clark, Tanushree Mitra, Yun Huang",http://arxiv.org/pdf/2409.09586v2,cs.CL
RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation,"LLM agents enhanced by tree search algorithms have yielded notable
performances in code generation. However, current search algorithms in this
domain suffer from low search quality due to several reasons: 1) Ineffective
design of the search space for the high-reasoning demands of code generation
tasks, 2) Inadequate integration of code feedback with the search algorithm,
and 3) Poor handling of negative feedback during the search, leading to reduced
search efficiency and quality. To address these challenges, we propose to
search for the reasoning process of the code and use the detailed feedback of
code execution to refine erroneous thoughts during the search. In this paper,
we introduce RethinkMCTS, which employs the Monte Carlo Tree Search (MCTS)
algorithm to conduct thought-level searches before generating code, thereby
exploring a wider range of strategies. More importantly, we construct verbal
feedback from fine-grained code execution feedback to refine erroneous thoughts
during the search. This ensures that the search progresses along the correct
reasoning paths, thus improving the overall search quality of the tree by
leveraging execution feedback. Through extensive experiments, we demonstrate
that RethinkMCTS outperforms previous search-based and feedback-based code
generation baselines. On the HumanEval dataset, it improves the pass@1 of
GPT-3.5-turbo from 70.12 to 89.02 and GPT-4o-mini from 87.20 to 94.51. It
effectively conducts more thorough exploration through thought-level searches
and enhances the search quality of the entire tree by incorporating rethink
operation.",2024-09-15,"Qingyao Li, Wei Xia, Kounianhua Du, Xinyi Dai, Ruiming Tang, Yasheng Wang, Yong Yu, Weinan Zhang",http://arxiv.org/pdf/2409.09584v1,cs.CL
Thesis proposal: Are We Losing Textual Diversity to Natural Language Processing?,"This thesis argues that the currently widely used Natural Language Processing
algorithms possibly have various limitations related to the properties of the
texts they handle and produce. With the wide adoption of these tools in rapid
progress, we must ask what these limitations are and what are the possible
implications of integrating such tools even more deeply into our daily lives.
  As a testbed, we have chosen the task of Neural Machine Translation (NMT).
Nevertheless, we aim for general insights and outcomes, applicable even to
current Large Language Models (LLMs). We ask whether the algorithms used in NMT
have inherent inductive biases that are beneficial for most types of inputs but
might harm the processing of untypical texts. To explore this hypothesis, we
define a set of measures to quantify text diversity based on its statistical
properties, like uniformity or rhythmicity of word-level surprisal, on multiple
scales (sentence, discourse, language). We then conduct a series of experiments
to investigate whether NMT systems struggle with maintaining the diversity of
such texts, potentially reducing the richness of the language generated by
these systems, compared to human translators.
  We search for potential causes of these limitations rooted in training
objectives and decoding algorithms. Our ultimate goal is to develop
alternatives that do not enforce uniformity in the distribution of statistical
properties in the output and that allow for better global planning of the
translation, taking into account the intrinsic ambiguity of the translation
task.",2024-09-15,Josef Jon,http://arxiv.org/pdf/2409.09568v1,cs.CL
ASR Error Correction using Large Language Models,"Error correction (EC) models play a crucial role in refining Automatic Speech
Recognition (ASR) transcriptions, enhancing the readability and quality of
transcriptions. Without requiring access to the underlying code or model
weights, EC can improve performance and provide domain adaptation for black-box
ASR systems. This work investigates the use of large language models (LLMs) for
error correction across diverse scenarios. 1-best ASR hypotheses are commonly
used as the input to EC models. We propose building high-performance EC models
using ASR N-best lists which should provide more contextual information for the
correction process. Additionally, the generation process of a standard EC model
is unrestricted in the sense that any output sequence can be generated. For
some scenarios, such as unseen domains, this flexibility may impact
performance. To address this, we introduce a constrained decoding approach
based on the N-best list or an ASR lattice. Finally, most EC models are trained
for a specific ASR system requiring retraining whenever the underlying ASR
system is changed. This paper explores the ability of EC models to operate on
the output of different ASR systems. This concept is further extended to
zero-shot error correction using LLMs, such as ChatGPT. Experiments on three
standard datasets demonstrate the efficacy of our proposed methods for both
Transducer and attention-based encoder-decoder ASR systems. In addition, the
proposed method can serve as an effective method for model ensembling.",2024-09-14,"Rao Ma, Mengjie Qian, Mark Gales, Kate Knill",http://arxiv.org/pdf/2409.09554v2,cs.CL
Synergistic Simulations: Multi-Agent Problem Solving with Large Language Models,"Large Language Models (LLMs) have increasingly demonstrated the ability to
facilitate the development of multi-agent systems that allow the interpretation
of thoughts and actions generated by each individual. Promising advancements
have also been made in LLM-based interaction with existing worlds, particularly
in interacting with simulated environments. This paper aims to integrate both
aforementioned topics (agents & world interaction) into a single simulation
where multiple agents can work together to solve a problem, modeling how groups
of humans can often solve problems better than individuals. By showing whether
LLMs demonstrate the synergy of human collaboration, it could lead to
advancements in the applications of LLMs. We implemented two simulations: a
physical studio apartment with two roommates, and another where agents
collaborate to complete a programming task. We provide a multi-agent framework,
discuss the performance of the agents in each simulation, and discuss potential
future additions.",2024-09-14,"Asher Sprigler, Alexander Drobek, Keagan Weinstock, Wendpanga Tapsoba, Gavin Childress, Andy Dao, Lucas Gral",http://arxiv.org/pdf/2409.13753v1,cs.CL
Planning Transformer: Long-Horizon Offline Reinforcement Learning with Planning Tokens,"Supervised learning approaches to offline reinforcement learning,
particularly those utilizing the Decision Transformer, have shown effectiveness
in continuous environments and for sparse rewards. However, they often struggle
with long-horizon tasks due to the high compounding error of auto-regressive
models. To overcome this limitation, we go beyond next-token prediction and
introduce Planning Tokens, which contain high-level, long time-scale
information about the agent's future. Predicting dual time-scale tokens at
regular intervals enables our model to use these long-horizon Planning Tokens
as a form of implicit planning to guide its low-level policy and reduce
compounding error. This architectural modification significantly enhances
performance on long-horizon tasks, establishing a new state-of-the-art in
complex D4RL environments. Additionally, we demonstrate that Planning Tokens
improve the interpretability of the model's policy through the interpretable
plan visualisations and attention map.",2024-09-14,"Joseph Clinton, Robert Lieck",http://arxiv.org/pdf/2409.09513v1,cs.CL
Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for Privacy-Preserving Personalization of Large Language Models,"Privacy-preserving methods for personalizing large language models (LLMs) are
relatively under-explored. There are two schools of thought on this topic: (1)
generating personalized outputs by personalizing the input prompt through
retrieval augmentation from the user's personal information (RAG-based
methods), and (2) parameter-efficient fine-tuning of LLMs per user that
considers efficiency and space limitations (PEFT-based methods). This paper
presents the first systematic comparison between two approaches on a wide range
of personalization tasks using seven diverse datasets. Our results indicate
that RAG-based and PEFT-based personalization methods on average yield 14.92%
and 1.07% improvements over the non-personalized LLM, respectively. We find
that combining RAG with PEFT elevates these improvements to 15.98%.
Additionally, we identify a positive correlation between the amount of user
data and PEFT's effectiveness, indicating that RAG is a better choice for
cold-start users (i.e., user's with limited personal data).",2024-09-14,"Alireza Salemi, Hamed Zamani",http://arxiv.org/pdf/2409.09510v1,cs.CL
Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent Classification in Low-Resource Bangla Language,"With the increasing popularity of daily information sharing and acquisition
on the Internet, this paper introduces an innovative approach for intent
classification in Bangla language, focusing on social media posts where
individuals share their thoughts and opinions. The proposed method leverages
multimodal data with particular emphasis on authorship identification, aiming
to understand the underlying purpose behind textual content, especially in the
context of varied user-generated posts on social media. Current methods often
face challenges in low-resource languages like Bangla, particularly when author
traits intricately link with intent, as observed in social media posts. To
address this, we present the Multimodal-based Author Bangla Intent
Classification (MABIC) framework, utilizing text and images to gain deeper
insights into the conveyed intentions. We have created a dataset named
""Uddessho,"" comprising 3,048 instances sourced from social media. Our
methodology comprises two approaches for classifying textual intent and
multimodal author intent, incorporating early fusion and late fusion
techniques. In our experiments, the unimodal approach achieved an accuracy of
64.53% in interpreting Bangla textual intent. In contrast, our multimodal
approach significantly outperformed traditional unimodal methods, achieving an
accuracy of 76.19%. This represents an improvement of 11.66%. To our best
knowledge, this is the first research work on multimodal-based author intent
classification for low-resource Bangla language social media posts.",2024-09-14,"Fatema Tuj Johora Faria, Mukaffi Bin Moin, Md. Mahfuzur Rahman, Md Morshed Alam Shanto, Asif Iftekher Fahim, Md. Moinul Hoque",http://arxiv.org/pdf/2409.09504v1,cs.CL
Synthetic4Health: Generating Annotated Synthetic Clinical Letters,"Since clinical letters contain sensitive information, clinical-related
datasets can not be widely applied in model training, medical research, and
teaching. This work aims to generate reliable, various, and de-identified
synthetic clinical letters. To achieve this goal, we explored different
pre-trained language models (PLMs) for masking and generating text. After that,
we worked on Bio\_ClinicalBERT, a high-performing model, and experimented with
different masking strategies. Both qualitative and quantitative methods were
used for evaluation. Additionally, a downstream task, Named Entity Recognition
(NER), was also implemented to assess the usability of these synthetic letters.
  The results indicate that 1) encoder-only models outperform encoder-decoder
models. 2) Among encoder-only models, those trained on general corpora perform
comparably to those trained on clinical data when clinical information is
preserved. 3) Additionally, preserving clinical entities and document structure
better aligns with our objectives than simply fine-tuning the model. 4)
Furthermore, different masking strategies can impact the quality of synthetic
clinical letters. Masking stopwords has a positive impact, while masking nouns
or verbs has a negative effect. 5) For evaluation, BERTScore should be the
primary quantitative evaluation metric, with other metrics serving as
supplementary references. 6) Contextual information does not significantly
impact the models' understanding, so the synthetic clinical letters have the
potential to replace the original ones in downstream tasks.",2024-09-14,"Libo Ren, Samuel Belkadi, Lifeng Han, Warren Del-Pinto, Goran Nenadic",http://arxiv.org/pdf/2409.09501v1,cs.CL
Keeping Humans in the Loop: Human-Centered Automated Annotation with Generative AI,"Automated text annotation is a compelling use case for generative large
language models (LLMs) in social media research. Recent work suggests that LLMs
can achieve strong performance on annotation tasks; however, these studies
evaluate LLMs on a small number of tasks and likely suffer from contamination
due to a reliance on public benchmark datasets. Here, we test a human-centered
framework for responsibly evaluating artificial intelligence tools used in
automated annotation. We use GPT-4 to replicate 27 annotation tasks across 11
password-protected datasets from recently published computational social
science articles in high-impact journals. For each task, we compare GPT-4
annotations against human-annotated ground-truth labels and against annotations
from separate supervised classification models fine-tuned on human-generated
labels. Although the quality of LLM labels is generally high, we find
significant variation in LLM performance across tasks, even within datasets.
Our findings underscore the importance of a human-centered workflow and careful
evaluation standards: Automated annotations significantly diverge from human
judgment in numerous scenarios, despite various optimization strategies such as
prompt tuning. Grounding automated annotation in validation labels generated by
humans is essential for responsible evaluation.",2024-09-14,"Nicholas Pangakis, Samuel Wolken",http://arxiv.org/pdf/2409.09467v2,cs.CL
Measuring the Influence of Incorrect Code on Test Generation,"It is natural to suppose that a Large Language Model is more likely to
generate correct test cases when prompted with correct code under test,
compared to incorrect code under test. However, the size of this effect has
never been previously measured, despite its obvious importance for both
practicing software engineers and researchers. To answer the question, we
conducted a comprehensive empirical study on 5 open source and 6 closed source
language models, with 3 widely-used benchmark data sets together with 41
repo-level real-world examples from two different real-world data sets. Our
results reveal that, when compared to incorrect code under test, LLMs prompted
with correct code achieve improvements in test accuracy, code coverage, and bug
detection of 57\%, 12\%, and 24\% respectively. We further show that these
scientific conclusions carry over from the three benchmark data sets to the
real-world code, where tests generated for incorrect code experience a 47\%
worse bug detection rate. Finally, we report that improvements of +18\% in
accuracy, +4\% coverage, and +34\% in bug detection can be achieved by
providing natural language code descriptions. These findings have actionable
conclusions. For example, the 47\% reduction in real-world bug detection is a
clear concern. Fortunately, it is a concern for which our findings about the
added value of descriptions offer an immediately actionable remedy.",2024-09-14,"Dong Huang, Jie M. Zhang, Mark Harman, Mingzhe Du, Heming Cui",http://arxiv.org/pdf/2409.09464v3,cs.CL
"Enhancing LLM Problem Solving with REAP: Reflection, Explicit Problem Deconstruction, and Advanced Prompting","Large Language Models (LLMs) have transformed natural language processing,
yet improving their problem-solving capabilities, particularly for complex,
reasoning-intensive tasks, remains a persistent challenge. This paper
introduces the REAP (Reflection, Explicit Problem Deconstruction, and Advanced
Prompting) method, an innovative approach within the dynamic context generation
framework. REAP guides LLMs through reflection on the query, deconstructing it
into manageable components, and generating relevant context to enhance the
solution process. We evaluated REAP using a dataset designed to expose LLM
limitations, comparing zero-shot prompting with REAP-enhanced prompts across
six state-of-the-art models: OpenAI's o1-preview, o1-mini, GPT-4o, GPT-4o-mini,
Google's Gemini 1.5 Pro, and Claude 3.5 Sonnet. The results demonstrate notable
performance gains, with o1-mini improving by 40.97%, GPT-4o by 66.26%, and
GPT-4o-mini by 112.93%. Despite the already strong baseline performance of
OpenAI's o1-preview, modest gains were observed. Beyond performance
improvements, REAP offers a cost-effective solution; for example, GPT-4o-mini,
which is approximately 100 times cheaper than o1-preview, delivered competitive
results. REAP also improves the clarity of model outputs, making it easier for
humans to understand the reasoning behind the results and simplifying the
process of identifying and addressing any issues. These findings demonstrate
REAP's potential to greatly improve the capabilities of LLMs, providing both
better performance and increased cost-efficiency across a wide range of
applications.",2024-09-14,"Ryan Lingo, Martin Arroyo, Rajeev Chhajer",http://arxiv.org/pdf/2409.09415v1,cs.CL
Constructive Approach to Bidirectional Influence between Qualia Structure and Language Emergence,"This perspective paper explores the bidirectional influence between language
emergence and the relational structure of subjective experiences, termed qualia
structure, and lays out a constructive approach to the intricate dependency
between the two. We hypothesize that the emergence of languages with
distributional semantics (e.g., syntactic-semantic structures) is linked to the
coordination of internal representations shaped by experience, potentially
facilitating more structured language through reciprocal influence. This
hypothesized mutual dependency connects to recent advancements in AI and symbol
emergence robotics, and is explored within this paper through theoretical
frameworks such as the collective predictive coding. Computational studies show
that neural network-based language models form systematically structured
internal representations, and multimodal language models can share
representations between language and perceptual information. This perspective
suggests that language emergence serves not only as a mechanism creating a
communication tool but also as a mechanism for allowing people to realize
shared understanding of qualitative experiences. The paper discusses the
implications of this bidirectional influence in the context of consciousness
studies, linguistics, and cognitive science, and outlines future constructive
research directions to further explore this dynamic relationship between
language emergence and qualia structure.",2024-09-14,"Tadahiro Taniguchi, Masafumi Oizumi, Noburo Saji, Takato Horii, Naotsugu Tsuchiya",http://arxiv.org/pdf/2409.09413v2,cs.CL
Towards Diverse and Efficient Audio Captioning via Diffusion Models,"We introduce Diffusion-based Audio Captioning (DAC), a non-autoregressive
diffusion model tailored for diverse and efficient audio captioning. Although
existing captioning models relying on language backbones have achieved
remarkable success in various captioning tasks, their insufficient performance
in terms of generation speed and diversity impede progress in audio
understanding and multimedia applications. Our diffusion-based framework offers
unique advantages stemming from its inherent stochasticity and holistic context
modeling in captioning. Through rigorous evaluation, we demonstrate that DAC
not only achieves SOTA performance levels compared to existing benchmarks in
the caption quality, but also significantly outperforms them in terms of
generation speed and diversity. The success of DAC illustrates that text
generation can also be seamlessly integrated with audio and visual generation
tasks using a diffusion backbone, paving the way for a unified, audio-related
generative model across different modalities.",2024-09-14,"Manjie Xu, Chenxing Li, Xinyi Tu, Yong Ren, Ruibo Fu, Wei Liang, Dong Yu",http://arxiv.org/pdf/2409.09401v1,cs.CL
LLM-Powered Ensemble Learning for Paper Source Tracing: A GPU-Free Approach,"We participated in the KDD CUP 2024 paper source tracing competition and
achieved the 3rd place. This competition tasked participants with identifying
the reference sources (i.e., ref-sources, as referred to by the organizers of
the competition) of given academic papers. Unlike most teams that addressed
this challenge by fine-tuning pre-trained neural language models such as BERT
or ChatGLM, our primary approach utilized closed-source large language models
(LLMs). With recent advancements in LLM technology, closed-source LLMs have
demonstrated the capability to tackle complex reasoning tasks in zero-shot or
few-shot scenarios. Consequently, in the absence of GPUs, we employed
closed-source LLMs to directly generate predicted reference sources from the
provided papers. We further refined these predictions through ensemble
learning. Notably, our method was the only one among the award-winning
approaches that did not require the use of GPUs for model training. Code
available at https://github.com/Cklwanfifa/KDDCUP2024-PST.",2024-09-14,"Kunlong Chen, Junjun Wang, Zhaoqun Chen, Kunjin Chen, Yitian Chen",http://arxiv.org/pdf/2409.09383v2,cs.CL
StressPrompt: Does Stress Impact Large Language Models and Human Performance Similarly?,"Human beings often experience stress, which can significantly influence their
performance. This study explores whether Large Language Models (LLMs) exhibit
stress responses similar to those of humans and whether their performance
fluctuates under different stress-inducing prompts. To investigate this, we
developed a novel set of prompts, termed StressPrompt, designed to induce
varying levels of stress. These prompts were derived from established
psychological frameworks and carefully calibrated based on ratings from human
participants. We then applied these prompts to several LLMs to assess their
responses across a range of tasks, including instruction-following, complex
reasoning, and emotional intelligence. The findings suggest that LLMs, like
humans, perform optimally under moderate stress, consistent with the
Yerkes-Dodson law. Notably, their performance declines under both low and
high-stress conditions. Our analysis further revealed that these StressPrompts
significantly alter the internal states of LLMs, leading to changes in their
neural representations that mirror human responses to stress. This research
provides critical insights into the operational robustness and flexibility of
LLMs, demonstrating the importance of designing AI systems capable of
maintaining high performance in real-world scenarios where stress is prevalent,
such as in customer service, healthcare, and emergency response contexts.
Moreover, this study contributes to the broader AI research community by
offering a new perspective on how LLMs handle different scenarios and their
similarities to human cognition.",2024-09-14,"Guobin Shen, Dongcheng Zhao, Aorigele Bao, Xiang He, Yiting Dong, Yi Zeng",http://arxiv.org/pdf/2409.17167v2,cs.CL
Generating Event-oriented Attribution for Movies via Two-Stage Prefix-Enhanced Multimodal LLM,"The prosperity of social media platforms has raised the urgent demand for
semantic-rich services, e.g., event and storyline attribution. However, most
existing research focuses on clip-level event understanding, primarily through
basic captioning tasks, without analyzing the causes of events across an entire
movie. This is a significant challenge, as even advanced multimodal large
language models (MLLMs) struggle with extensive multimodal information due to
limited context length. To address this issue, we propose a Two-Stage
Prefix-Enhanced MLLM (TSPE) approach for event attribution, i.e., connecting
associated events with their causal semantics, in movie videos. In the local
stage, we introduce an interaction-aware prefix that guides the model to focus
on the relevant multimodal information within a single clip, briefly
summarizing the single event. Correspondingly, in the global stage, we
strengthen the connections between associated events using an inferential
knowledge graph, and design an event-aware prefix that directs the model to
focus on associated events rather than all preceding clips, resulting in
accurate event attribution. Comprehensive evaluations of two real-world
datasets demonstrate that our framework outperforms state-of-the-art methods.",2024-09-14,"Yuanjie Lyu, Tong Xu, Zihan Niu, Bo Peng, Jing Ke, Enhong Chen",http://arxiv.org/pdf/2409.09362v1,cs.CL
Overcoming linguistic barriers in code assistants: creating a QLoRA adapter to improve support for Russian-language code writing instructions,"In this paper, an approach to training and evaluating an adapter model for
the popular language model ""zephyr-7b-beta"" is described. The adapter was
developed to improve the performance of the base model in tasks related to
programming and understanding the Russian language. Considering the high
quality of the original model in tasks in the English language, the goal of the
research was to expand its linguistic and technical spectrum. The proposed
adapter was trained using a large and diverse dataset, including
question-answer pairs related to programming, as well code-related texts in
Russian language. The applied training methodology ensures an improvement in
the model's quality of answers in understanding and generating Python code
based on Russian instructions. We evaluated the performance of the base model
with the installed adapter using various metrics, comparing it to the base
model as well as other state-of-the-art models in this field. The obtained
results showed significant improvement, both in tasks related to writing Python
code and in processing the Russian language, confirming the effectiveness of
the proposed adapter.",2024-09-14,"C. B. Pronin, A. V. Volosova, A. V. Ostroukh, Yu. N. Strogov",http://arxiv.org/pdf/2409.09353v1,cs.CL
Efficient Fine-Tuning of Large Language Models for Automated Medical Documentation,"Scientific research indicates that for every hour spent in direct patient
care, physicians spend nearly two additional hours on administrative tasks,
particularly on electronic health records (EHRs) and desk work. This excessive
administrative burden not only reduces the time available for patient care but
also contributes to physician burnout and inefficiencies in healthcare
delivery. To address these challenges, this study introduces MediGen, a
fine-tuned large language model (LLM) designed to automate the generation of
medical reports from medical dialogues. By leveraging state-of-the-art
methodologies for fine-tuning open-source pretrained models, including
LLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing
clinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising
results, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating
its effectiveness in generating accurate and clinically relevant medical
reports. These findings suggest that MediGen has the potential to significantly
reduce the administrative workload on physicians, improving both healthcare
efficiency and physician well-being.",2024-09-14,"Hui Yi Leong, Yi Fan Gao, Ji Shuai, Yang Zhang, Uktu Pamuksuz",http://arxiv.org/pdf/2409.09324v2,cs.CL
A Compressive Memory-based Retrieval Approach for Event Argument Extraction,"Recent works have demonstrated the effectiveness of retrieval augmentation in
the Event Argument Extraction (EAE) task. However, existing retrieval-based EAE
methods have two main limitations: (1) input length constraints and (2) the gap
between the retriever and the inference model. These issues limit the diversity
and quality of the retrieved information. In this paper, we propose a
Compressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the
two limitations mentioned above. Our compressive memory, designed as a dynamic
matrix that effectively caches retrieved information and supports continuous
updates, overcomes the limitations of the input length. Additionally, after
pre-loading all candidate demonstrations into the compressive memory, the model
further retrieves and filters relevant information from memory based on the
input query, bridging the gap between the retriever and the inference model.
Extensive experiments show that our method achieves new state-of-the-art
performance on three public datasets (RAMS, WikiEvents, ACE05), significantly
outperforming existing retrieval-based EAE methods.",2024-09-14,"Wanlong Liu, Enqi Zhang, Li Zhou, Dingyi Zeng, Shaohuan Cheng, Chen Zhang, Malu Zhang, Wenyu Chen",http://arxiv.org/pdf/2409.09322v1,cs.CL
IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web,"Recently advancements in large multimodal models have led to significant
strides in image comprehension capabilities. Despite these advancements, there
is a lack of the robust benchmark specifically for assessing the Image-to-Web
conversion proficiency of these large models. Primarily, it is essential to
ensure the integrity of the web elements generated. These elements comprise
visible and invisible categories. Previous evaluation methods (e.g., BLEU) are
notably susceptible to significant alterations due to the presence of invisible
elements in Web. Furthermore, it is crucial to measure the layout information
of web pages, referring to the positional relationships between elements, which
is overlooked by previous work. To address challenges, we have curated and
aligned a benchmark of images and corresponding web codes (IW-Bench).
Specifically, we propose the Element Accuracy, which tests the completeness of
the elements by parsing the Document Object Model (DOM) tree. Layout Accuracy
is also proposed to analyze the positional relationships of elements by
converting DOM tree into a common subsequence. Besides, we design a five-hop
multimodal Chain-of-Thought Prompting for better performance, which contains
five hop: 1) SoM prompt injection. 2) Inferring Elements. 3) Inferring Layout.
4) Inferring Web code. 5) Reflection. Our benchmark comprises 1200 pairs of
images and web codes with varying levels of difficulty. We have conducted
extensive experiments on existing large multimodal models, offering insights
into their performance and areas for improvement in image-to-web domain.",2024-09-14,"Hongcheng Guo, Wei Zhang, Junhao Chen, Yaonan Gu, Jian Yang, Junjia Du, Binyuan Hui, Tianyu Liu, Jianxin Ma, Chang Zhou, Zhoujun Li",http://arxiv.org/pdf/2409.18980v1,cs.CL
ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models,"Hallucination poses a persistent challenge for multimodal large language
models (MLLMs). However, existing benchmarks for evaluating hallucinations are
generally static, which may overlook the potential risk of data contamination.
To address this issue, we propose ODE, an open-set, dynamic protocol designed
to evaluate object hallucinations in MLLMs at both the existence and attribute
levels. ODE employs a graph-based structure to represent real-world object
concepts, their attributes, and the distributional associations between them.
This structure facilitates the extraction of concept combinations based on
diverse distributional criteria, generating varied samples for structured
queries that evaluate hallucinations in both generative and discriminative
tasks. Through the generation of new samples, dynamic concept combinations, and
varied distribution frequencies, ODE mitigates the risk of data contamination
and broadens the scope of evaluation. This protocol is applicable to both
general and specialized scenarios, including those with limited data.
Experimental results demonstrate the effectiveness of our protocol, revealing
that MLLMs exhibit higher hallucination rates when evaluated with ODE-generated
samples, which indicates potential data contamination. Furthermore, these
generated samples aid in analyzing hallucination patterns and fine-tuning
models, offering an effective approach to mitigating hallucinations in MLLMs.",2024-09-14,"Yahan Tu, Rui Hu, Jitao Sang",http://arxiv.org/pdf/2409.09318v3,cs.CL
"Language Models ""Grok"" to Copy","We examine the pre-training dynamics of language models, focusing on their
ability to copy text from preceding context--a fundamental skill for various
LLM applications, including in-context learning (ICL) and retrieval-augmented
generation (RAG). We propose a novel perspective that Transformer-based
language models develop copying abilities similarly to grokking, which refers
to sudden generalization on test set long after the model fit to the training
set. Our experiments yield three arguments: (1) The pre-training loss decreases
rapidly, while the context copying ability of models initially lags and then
abruptly saturates. (2) The speed of developing copying ability is independent
of the number of tokens trained, similarly to how grokking speed is unaffected
by dataset size as long as the data distribution is preserved. (3) Induction
heads, the attention heads responsible for copying, form from shallow to deep
layers during training, mirroring the development of circuits in deeper layers
during grokking. We contend that the connection between grokking and context
copying can provide valuable insights for more effective language model
training, ultimately improving in-context performance. For example, we
demonstrated that techniques that enhance grokking, such as regularization,
either accelerate or enhance the development of context copying.",2024-09-14,"Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, Rui Yan",http://arxiv.org/pdf/2409.09281v2,cs.CL
An empirical evaluation of using ChatGPT to summarize disputes for recommending similar labor and employment cases in Chinese,"We present a hybrid mechanism for recommending similar cases of labor and
employment litigations. The classifier determines the similarity based on the
itemized disputes of the two cases, that the courts prepared. We cluster the
disputes, compute the cosine similarity between the disputes, and use the
results as the features for the classification tasks. Experimental results
indicate that this hybrid approach outperformed our previous system, which
considered only the information about the clusters of the disputes. We replaced
the disputes that were prepared by the courts with the itemized disputes that
were generated by GPT-3.5 and GPT-4, and repeated the same experiments. Using
the disputes generated by GPT-4 led to better results. Although our classifier
did not perform as well when using the disputes that the ChatGPT generated, the
results were satisfactory. Hence, we hope that the future large-language models
will become practically useful.",2024-09-14,"Po-Hsien Wu, Chao-Lin Liu, Wei-Jie Li",http://arxiv.org/pdf/2409.09280v1,cs.CL
Thinking Before Speaking: A Role-playing Model with Mindset,"Role-playing is an easy task for Large Language Models (LLMs), as they are
skilled at simulating human behaviors. Many current studies have enabled LLMs
to generate responses in the tone of a specific role by fine-tuning the models
or using specialized prompts. However, it is typically easy to recognize when a
role is being played by LLMs. These models tend to perform poorly when
confronted with knowledge that the assumed role does not possess, or a question
that requires the specific experience or logic of the role to answer. To
address this problem and make LLMs act more like real roles, we propose a
Thinking Before Speaking (TBS) model in this paper. Unlike other studies, we
first extend the data based on the character's real-life scenarios and the
historical dialogue, supplementing each pair of dialogue with the character's
mindset. Then we add few data points that include elements beyond the role's
knowledge, and fine-tune the LLMs. This approach can help LLMs adopt the role's
thought process and logic, avoiding responses that fall outside the role's
knowledge base. We have also prepared a dataset and evaluation metrics to test
these capabilities. Experimental results show that our TBS model can better
emulate a role in terms of tone, knowledge, and mindset.",2024-09-14,"Baohua Zhang, Yongyi Huang, Wenyao Cui, Huaping Zhang",http://arxiv.org/pdf/2409.13752v1,cs.CL
From Text to Multimodality: Exploring the Evolution and Impact of Large Language Models in Medical Practice,"Large Language Models (LLMs) have rapidly evolved from text-based systems to
multimodal platforms, significantly impacting various sectors including
healthcare. This comprehensive review explores the progression of LLMs to
Multimodal Large Language Models (MLLMs) and their growing influence in medical
practice. We examine the current landscape of MLLMs in healthcare, analyzing
their applications across clinical decision support, medical imaging, patient
engagement, and research. The review highlights the unique capabilities of
MLLMs in integrating diverse data types, such as text, images, and audio, to
provide more comprehensive insights into patient health. We also address the
challenges facing MLLM implementation, including data limitations, technical
hurdles, and ethical considerations. By identifying key research gaps, this
paper aims to guide future investigations in areas such as dataset development,
modality alignment methods, and the establishment of ethical guidelines. As
MLLMs continue to shape the future of healthcare, understanding their potential
and limitations is crucial for their responsible and effective integration into
medical practice.",2024-09-14,"Qian Niu, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Lawrence KQ Yan, Yichao Zhang, Caitlyn Heqi Yin, Cheng Fei, Junyu Liu, Benji Peng, Tianyang Wang, Yunze Wang, Silin Chen, Ming Liu",http://arxiv.org/pdf/2410.01812v5,cs.CL
Block-Attention for Efficient Prefilling,"We introduce Block-attention, an attention mechanism designed to address the
increased inference latency and cost in Retrieval-Augmented Generation (RAG)
scenarios. Traditional approaches often encode the entire context in an
auto-regressive manner. Instead, Block-attention divides retrieved documents
into discrete blocks, with each block independently calculating key-value (KV)
states except for the final block. In RAG scenarios, by defining each passage
as a block, Block-attention enables us to reuse the KV states of passages that
have been seen before, thereby significantly reducing the latency and the
computation overhead during inference. The implementation of Block-attention
involves block segmentation, position re-encoding, and fine-tuning the LLM to
adapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,
including RAG, ICL, and general domains, demonstrate that after block
fine-tuning, the Block-attention model not only achieves performance comparable
to that of full-attention models, but can also seamlessly switch between the
block and full attention modes without any performance loss. Notably,
Block-attention significantly reduces the time to first token (TTFT) and
floating point operations (FLOPs) to a very low level. It only takes 45 ms to
output the first token for an input sequence with a total length of 32K.
Compared to the full-attention models, the TTFT and corresponding FLOPs are
reduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we
elaborate on how Block-attention is applied in Game AI scenario and the
substantial potential benefits it entails. We strongly suggest researchers in
the gaming field not to overlook this section.",2024-09-14,"Dongyang Ma, Yan Wang, Lan Tian",http://arxiv.org/pdf/2409.15355v5,cs.CL
"Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types","Visual Question-Answering (VQA) has become key to user experience,
particularly after improved generalization capabilities of Vision-Language
Models (VLMs). But evaluating VLMs for an application requirement using a
standardized framework in practical settings is still challenging. This paper
aims to solve that using an end-to-end framework. We present VQA360 - a novel
dataset derived from established VQA benchmarks, annotated with task types,
application domains, and knowledge types, for a comprehensive evaluation. We
also introduce GoEval, a multimodal evaluation metric developed using GPT-4o,
achieving a correlation factor of 56.71% with human judgments. Our experiments
with state-of-the-art VLMs reveal that no single model excels universally,
thus, making a right choice a key design decision. Proprietary models such as
Gemini-1.5-Pro and GPT-4o-mini generally outperform others, but open-source
models like InternVL-2-8B and CogVLM-2-Llama-3-19B also demonstrate competitive
strengths, while providing additional advantages. Our framework can also be
extended to other tasks.",2024-09-14,"Neelabh Sinha, Vinija Jain, Aman Chadha",http://arxiv.org/pdf/2409.09269v3,cs.CL
"Evaluating Cultural Awareness of LLMs for Yoruba, Malayalam, and English","Although LLMs have been extremely effective in a large number of complex
tasks, their understanding and functionality for regional languages and
cultures are not well studied. In this paper, we explore the ability of various
LLMs to comprehend the cultural aspects of two regional languages: Malayalam
(state of Kerala, India) and Yoruba (West Africa). Using Hofstede's six
cultural dimensions: Power Distance (PDI), Individualism (IDV), Motivation
towards Achievement and Success (MAS), Uncertainty Avoidance (UAV), Long Term
Orientation (LTO), and Indulgence (IVR), we quantify the cultural awareness of
LLM-based responses. We demonstrate that although LLMs show a high cultural
similarity for English, they fail to capture the cultural nuances across these
6 metrics for Malayalam and Yoruba. We also highlight the need for large-scale
regional language LLM training with culturally enriched datasets. This will
have huge implications for enhancing the user experience of chat-based LLMs and
also improving the validity of large-scale LLM agent-based market research.",2024-09-14,"Fiifi Dawson, Zainab Mosunmola, Sahil Pocker, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat",http://arxiv.org/pdf/2410.01811v1,cs.CL
What Is Wrong with My Model? Identifying Systematic Problems with Semantic Data Slicing,"Machine learning models make mistakes, yet sometimes it is difficult to
identify the systematic problems behind the mistakes. Practitioners engage in
various activities, including error analysis, testing, auditing, and
red-teaming, to form hypotheses of what can go (or has gone) wrong with their
models. To validate these hypotheses, practitioners employ data slicing to
identify relevant examples. However, traditional data slicing is limited by
available features and programmatic slicing functions. In this work, we propose
SemSlicer, a framework that supports semantic data slicing, which identifies a
semantically coherent slice, without the need for existing features. SemSlicer
uses Large Language Models to annotate datasets and generate slices from any
user-defined slicing criteria. We show that SemSlicer generates accurate slices
with low cost, allows flexible trade-offs between different design dimensions,
reliably identifies under-performing data slices, and helps practitioners
identify useful data slices that reflect systematic problems.",2024-09-14,"Chenyang Yang, Yining Hong, Grace A. Lewis, Tongshuang Wu, Christian Kästner",http://arxiv.org/pdf/2409.09261v1,cs.CL
Analyzing Correlations Between Intrinsic and Extrinsic Bias Metrics of Static Word Embeddings With Their Measuring Biases Aligned,"We examine the abilities of intrinsic bias metrics of static word embeddings
to predict whether Natural Language Processing (NLP) systems exhibit biased
behavior. A word embedding is one of the fundamental NLP technologies that
represents the meanings of words through real vectors, and problematically, it
also learns social biases such as stereotypes. An intrinsic bias metric
measures bias by examining a characteristic of vectors, while an extrinsic bias
metric checks whether an NLP system trained with a word embedding is biased. A
previous study found that a common intrinsic bias metric usually does not
correlate with extrinsic bias metrics. However, the intrinsic and extrinsic
bias metrics did not measure the same bias in most cases, which makes us
question whether the lack of correlation is genuine. In this paper, we extract
characteristic words from datasets of extrinsic bias metrics and analyze
correlations with intrinsic bias metrics with those words to ensure both
metrics measure the same bias. We observed moderate to high correlations with
some extrinsic bias metrics but little to no correlations with the others. This
result suggests that intrinsic bias metrics can predict biased behavior in
particular settings but not in others. Experiment codes are available at
GitHub.",2024-09-14,"Taisei Katô, Yusuke Miyao",http://arxiv.org/pdf/2409.09260v1,cs.CL
Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator,"Owing to the unprecedented capability in semantic understanding and logical
reasoning, the pre-trained large language models (LLMs) have shown fantastic
potential in developing the next-generation recommender systems (RSs). However,
the static index paradigm adopted by current methods greatly restricts the
utilization of LLMs capacity for recommendation, leading to not only the
insufficient alignment between semantic and collaborative knowledge, but also
the neglect of high-order user-item interaction patterns. In this paper, we
propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS
which adopts dynamic semantic index paradigm, targeting at resolving the above
problems simultaneously. To be more specific, we for the first time contrive a
dynamic knowledge fusion framework which integrates a twin-tower semantic token
generator into the LLM-based recommender, hierarchically allocating meaningful
semantic index for items and users, and accordingly predicting the semantic
index of target item. Furthermore, a dual-modality variational auto-encoder is
proposed to facilitate multi-grained alignment between semantic and
collaborative knowledge. Eventually, a series of novel tuning tasks specially
customized for capturing high-order user-item interaction patterns are proposed
to take advantages of user historical behavior. Extensive experiments across
three public datasets demonstrate the superiority of the proposed methodology
in developing LLM-based generative RSs. The proposed TTDS recommender achieves
an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric,
compared with the leading baseline methods.",2024-09-14,"Jun Yin, Zhengxin Zeng, Mingzheng Li, Hao Yan, Chaozhuo Li, Weihao Han, Jianjin Zhang, Ruochen Liu, Allen Sun, Denvy Deng, Feng Sun, Qi Zhang, Shirui Pan, Senzhang Wang",http://arxiv.org/pdf/2409.09253v1,cs.CL
NovAScore: A New Automated Metric for Evaluating Document Level Novelty,"The rapid expansion of online content has intensified the issue of
information redundancy, underscoring the need for solutions that can identify
genuinely new information. Despite this challenge, the research community has
seen a decline in focus on novelty detection, particularly with the rise of
large language models (LLMs). Additionally, previous approaches have relied
heavily on human annotation, which is time-consuming, costly, and particularly
challenging when annotators must compare a target document against a vast
number of historical documents. In this work, we introduce NovAScore (Novelty
Evaluation in Atomicity Score), an automated metric for evaluating
document-level novelty. NovAScore aggregates the novelty and salience scores of
atomic information, providing high interpretability and a detailed analysis of
a document's novelty. With its dynamic weight adjustment scheme, NovAScore
offers enhanced flexibility and an additional dimension to assess both the
novelty level and the importance of information within a document. Our
experiments show that NovAScore strongly correlates with human judgments of
novelty, achieving a 0.626 Point-Biserial correlation on the TAP-DLND 1.0
dataset and a 0.920 Pearson correlation on an internal human-annotated dataset.",2024-09-14,"Lin Ai, Ziwei Gong, Harshsaiprasad Deshpande, Alexander Johnson, Emmy Phung, Ahmad Emami, Julia Hirschberg",http://arxiv.org/pdf/2409.09249v2,cs.CL
Robust Training of Neural Networks at Arbitrary Precision and Sparsity,"The discontinuous operations inherent in quantization and sparsification
introduce obstacles to backpropagation. This is particularly challenging when
training deep neural networks in ultra-low precision and sparse regimes. We
propose a novel, robust, and universal solution: a denoising affine transform
that stabilizes training under these challenging conditions. By formulating
quantization and sparsification as perturbations during training, we derive a
perturbation-resilient approach based on ridge regression. Our solution employs
a piecewise constant backbone model to ensure a performance lower bound and
features an inherent noise reduction mechanism to mitigate perturbation-induced
corruption. This formulation allows existing models to be trained at
arbitrarily low precision and sparsity levels with off-the-shelf recipes.
Furthermore, our method provides a novel perspective on training temporal
binary neural networks, contributing to ongoing efforts to narrow the gap
between artificial and biological neural networks.",2024-09-14,"Chengxi Ye, Grace Chu, Yanfeng Liu, Yichi Zhang, Lukasz Lew, Andrew Howard",http://arxiv.org/pdf/2409.09245v1,cs.CL
Autoregressive + Chain of Thought = Recurrent: Recurrence's Role in Language Models' Computability and a Revisit of Recurrent Transformer,"The Transformer architecture excels in a variety of language modeling tasks,
outperforming traditional neural architectures such as RNN and LSTM. This is
partially due to its elimination of recurrent connections, which allows for
parallel training and a smoother flow of gradients. However, this move away
from recurrent structures places the Transformer model at the lower end of
Chomsky's computational hierarchy, imposing limitations on its computational
abilities. Consequently, even advanced Transformer-based models face
considerable difficulties in tasks like counting, string reversal, and
multiplication. These tasks, though seemingly elementary, require a level of
computational complexity that exceeds the capabilities of the Transformer
architecture. Concurrently, the emergence of ``Chain of Thought"" (CoT)
prompting has enabled Transformer-based language models to tackle tasks that
were previously impossible or poorly executed. In this work, we thoroughly
investigate the influence of recurrent structures in neural models on their
reasoning abilities and computability, contrasting the role autoregression
plays in the neural models' computational power. We then shed light on how the
CoT approach can mimic recurrent computation and act as a bridge between
autoregression and recurrence in the context of language models. It is this
approximated recurrence that notably improves the model's performance and
computational capacity. Moreover, we revisit recent recurrent-based Transformer
model designs, focusing on their computational abilities through our proposed
concept of ``recurrence-completeness"" and identify key theoretical limitations
in models like Linear Transformer and RWKV. Through this, we aim to provide
insight into the neural model architectures and prompt better model design.",2024-09-14,"Xiang Zhang, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan",http://arxiv.org/pdf/2409.09239v3,cs.CL
Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?,"Decoder-only discrete-token language models have recently achieved
significant success in automatic speech recognition. However, systematic
analyses of how different modalities impact performance in specific scenarios
remain limited. In this paper, we investigate the effects of multiple
modalities on recognition accuracy on both synthetic and real-world datasets.
Our experiments suggest that: (1) Integrating more modalities can increase
accuracy; in particular, our paper is, to our best knowledge, the first to show
the benefit of combining audio, image context, and lip information; (2) Images
as a supplementary modality for speech recognition provide the greatest benefit
at moderate noise levels, moreover, they exhibit a different trend compared to
inherently synchronized modalities like lip movements; (3) Performance improves
on both synthetic and real-world datasets when the most relevant visual
information is filtered as a preprocessing step.",2024-09-13,"Yiwen Guan, Viet Anh Trinh, Vivek Voleti, Jacob Whitehill",http://arxiv.org/pdf/2409.09221v2,cs.CL
ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds,"Open-vocabulary audio-language models, like CLAP, offer a promising approach
for zero-shot audio classification (ZSAC) by enabling classification with any
arbitrary set of categories specified with natural language prompts. In this
paper, we propose a simple but effective method to improve ZSAC with CLAP.
Specifically, we shift from the conventional method of using prompts with
abstract category labels (e.g., Sound of an organ) to prompts that describe
sounds using their inherent descriptive features in a diverse context (e.g.,The
organ's deep and resonant tones filled the cathedral.). To achieve this, we
first propose ReCLAP, a CLAP model trained with rewritten audio captions for
improved understanding of sounds in the wild. These rewritten captions describe
each sound event in the original caption using their unique discriminative
characteristics. ReCLAP outperforms all baselines on both multi-modal
audio-text retrieval and ZSAC. Next, to improve zero-shot audio classification
with ReCLAP, we propose prompt augmentation. In contrast to the traditional
method of employing hand-written template prompts, we generate custom prompts
for each unique label in the dataset. These custom prompts first describe the
sound event in the label and then employ them in diverse scenes. Our proposed
method improves ReCLAP's performance on ZSAC by 1%-18% and outperforms all
baselines by 1% - 55%.",2024-09-13,"Sreyan Ghosh, Sonal Kumar, Chandra Kiran Reddy Evuru, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha",http://arxiv.org/pdf/2409.09213v1,cs.CL
Contextual Evaluation of Large Language Models for Classifying Tropical and Infectious Diseases,"While large language models (LLMs) have shown promise for medical question
answering, there is limited work focused on tropical and infectious
disease-specific exploration. We build on an opensource tropical and infectious
diseases (TRINDs) dataset, expanding it to include demographic and semantic
clinical and consumer augmentations yielding 11000+ prompts. We evaluate LLM
performance on these, comparing generalist and medical LLMs, as well as LLM
outcomes to human experts. We demonstrate through systematic experimentation,
the benefit of contextual information such as demographics, location, gender,
risk factors for optimal LLM response. Finally we develop a prototype of
TRINDs-LM, a research tool that provides a playground to navigate how context
impacts LLM outputs for health.",2024-09-13,"Mercy Asiedu, Nenad Tomasev, Chintan Ghate, Tiya Tiyasirichokchai, Awa Dieng, Oluwatosin Akande, Geoffrey Siwo, Steve Adudans, Sylvanus Aitkins, Odianosen Ehiakhamen, Eric Ndombi, Katherine Heller",http://arxiv.org/pdf/2409.09201v3,cs.CL
Transformer with Controlled Attention for Synchronous Motion Captioning,"In this paper, we address a challenging task, synchronous motion captioning,
that aim to generate a language description synchronized with human motion
sequences. This task pertains to numerous applications, such as aligned sign
language transcription, unsupervised action segmentation and temporal
grounding. Our method introduces mechanisms to control self- and
cross-attention distributions of the Transformer, allowing interpretability and
time-aligned text generation. We achieve this through masking strategies and
structuring losses that push the model to maximize attention only on the most
important frames contributing to the generation of a motion word. These
constraints aim to prevent undesired mixing of information in attention maps
and to provide a monotonic attention distribution across tokens. Thus, the
cross attentions of tokens are used for progressive text generation in
synchronization with human motion sequences. We demonstrate the superior
performance of our approach through evaluation on the two available benchmark
datasets, KIT-ML and HumanML3D. As visual evaluation is essential for this
task, we provide a comprehensive set of animated visual illustrations in the
code repository: https://github.com/rd20karim/Synch-Transformer.",2024-09-13,"Karim Radouane, Sylvie Ranwez, Julien Lagarde, Andon Tchechmedjiev",http://arxiv.org/pdf/2409.09177v1,cs.CL
Towards Precision Characterization of Communication Disorders using Models of Perceived Pragmatic Similarity,"The diagnosis and treatment of individuals with communication disorders
offers many opportunities for the application of speech technology, but
research so far has not adequately considered: the diversity of conditions, the
role of pragmatic deficits, and the challenges of limited data. This paper
explores how a general-purpose model of perceived pragmatic similarity may
overcome these limitations. It explains how it might support several use cases
for clinicians and clients, and presents evidence that a simple model can
provide value, and in particular can capture utterance aspects that are
relevant to diagnoses of autism and specific language impairment.",2024-09-13,"Nigel G. Ward, Andres Segura, Georgina Bugarini, Heike Lehnert-LeHouillier, Dancheng Liu, Jinjun Xiong, Olac Fuentes",http://arxiv.org/pdf/2409.09170v1,cs.CL
CPT-Boosted Wav2vec2.0: Towards Noise Robust Speech Recognition for Classroom Environments,"Creating Automatic Speech Recognition (ASR) systems that are robust and
resilient to classroom conditions is paramount to the development of AI tools
to aid teachers and students. In this work, we study the efficacy of continued
pretraining (CPT) in adapting Wav2vec2.0 to the classroom domain. We show that
CPT is a powerful tool in that regard and reduces the Word Error Rate (WER) of
Wav2vec2.0-based models by upwards of 10%. More specifically, CPT improves the
model's robustness to different noises, microphones and classroom conditions.",2024-09-13,"Ahmed Adel Attia, Dorottya Demszky, Tolulope Ogunremi, Jing Liu, Carol Espy-Wilson",http://arxiv.org/pdf/2409.14494v4,cs.CL
DomURLs_BERT: Pre-trained BERT-based Model for Malicious Domains and URLs Detection and Classification,"Detecting and classifying suspicious or malicious domain names and URLs is
fundamental task in cybersecurity. To leverage such indicators of compromise,
cybersecurity vendors and practitioners often maintain and update blacklists of
known malicious domains and URLs. However, blacklists frequently fail to
identify emerging and obfuscated threats. Over the past few decades, there has
been significant interest in developing machine learning models that
automatically detect malicious domains and URLs, addressing the limitations of
blacklists maintenance and updates. In this paper, we introduce DomURLs_BERT, a
pre-trained BERT-based encoder adapted for detecting and classifying
suspicious/malicious domains and URLs. DomURLs_BERT is pre-trained using the
Masked Language Modeling (MLM) objective on a large multilingual corpus of
URLs, domain names, and Domain Generation Algorithms (DGA) dataset. In order to
assess the performance of DomURLs_BERT, we have conducted experiments on
several binary and multi-class classification tasks involving domain names and
URLs, covering phishing, malware, DGA, and DNS tunneling. The evaluations
results show that the proposed encoder outperforms state-of-the-art
character-based deep learning models and cybersecurity-focused BERT models
across multiple tasks and datasets. The pre-training dataset, the pre-trained
DomURLs_BERT encoder, and the experiments source code are publicly available.",2024-09-13,"Abdelkader El Mahdaouy, Salima Lamsiyah, Meryem Janati Idrissi, Hamza Alami, Zakaria Yartaoui, Ismail Berrada",http://arxiv.org/pdf/2409.09143v1,cs.CL
Multimodal Fusion with LLMs for Engagement Prediction in Natural Conversation,"Over the past decade, wearable computing devices (``smart glasses'') have
undergone remarkable advancements in sensor technology, design, and processing
power, ushering in a new era of opportunity for high-density human behavior
data. Equipped with wearable cameras, these glasses offer a unique opportunity
to analyze non-verbal behavior in natural settings as individuals interact. Our
focus lies in predicting engagement in dyadic interactions by scrutinizing
verbal and non-verbal cues, aiming to detect signs of disinterest or confusion.
Leveraging such analyses may revolutionize our understanding of human
communication, foster more effective collaboration in professional
environments, provide better mental health support through empathetic virtual
interactions, and enhance accessibility for those with communication barriers.
  In this work, we collect a dataset featuring 34 participants engaged in
casual dyadic conversations, each providing self-reported engagement ratings at
the end of each conversation. We introduce a novel fusion strategy using Large
Language Models (LLMs) to integrate multiple behavior modalities into a
``multimodal transcript'' that can be processed by an LLM for behavioral
reasoning tasks. Remarkably, this method achieves performance comparable to
established fusion techniques even in its preliminary implementation,
indicating strong potential for further research and optimization. This fusion
method is one of the first to approach ``reasoning'' about real-world human
behavior through a language model. Smart glasses provide us the ability to
unobtrusively gather high-density multimodal data on human behavior, paving the
way for new approaches to understanding and improving human communication with
the potential for important societal benefits. The features and data collected
during the studies will be made publicly available to promote further research.",2024-09-13,"Cheng Charles Ma, Kevin Hyekang Joo, Alexandria K. Vail, Sunreeta Bhattacharya, Álvaro Fernández García, Kailana Baker-Matsuoka, Sheryl Mathew, Lori L. Holt, Fernando De la Torre",http://arxiv.org/pdf/2409.09135v1,cs.CL
Eureka: Evaluating and Understanding Large Foundation Models,"Rigorous and reproducible evaluation is critical for assessing the state of
the art and for guiding scientific advances in Artificial Intelligence.
Evaluation is challenging in practice due to several reasons, including
benchmark saturation, lack of transparency in methods used for measurement,
development challenges in extracting measurements for generative tasks, and,
more generally, the extensive number of capabilities required for a
well-rounded comparison across models. We make three contributions to alleviate
the above challenges. First, we present Eureka, an open-source framework for
standardizing evaluations of large foundation models beyond single-score
reporting and rankings. Second, we introduce Eureka-Bench as an extensible
collection of benchmarks testing capabilities that (i) are still challenging
for state-of-the-art models and (ii) represent fundamental but overlooked
language and multimodal capabilities. The inherent space for improvement in
non-saturated benchmarks enables us to discover meaningful differences between
models at a capability level. Third, using Eureka, we conduct an analysis of 12
state-of-the-art models, providing in-depth insights into failure understanding
and model comparison, which can be leveraged to plan targeted improvements. In
contrast to recent trends in reports and leaderboards showing absolute rankings
and claims for one model or another to be the best, our analysis shows that
there is no such best model. Different models have different strengths, but
there are models that appear more often than others as best performers for some
capabilities. Despite the recent improvements, current models still struggle
with several fundamental capabilities including detailed image understanding,
benefiting from multimodal input when available rather than fully relying on
language, factuality and grounding for information retrieval, and over
refusals.",2024-09-13,"Vidhisha Balachandran, Jingya Chen, Neel Joshi, Besmira Nushi, Hamid Palangi, Eduardo Salinas, Vibhav Vineet, James Woffinden-Luey, Safoora Yousefi",http://arxiv.org/pdf/2409.10566v1,cs.CL
"Agents in Software Engineering: Survey, Landscape, and Vision","In recent years, Large Language Models (LLMs) have achieved remarkable
success and have been widely used in various downstream tasks, especially in
the tasks of the software engineering (SE) field. We find that many studies
combining LLMs with SE have employed the concept of agents either explicitly or
implicitly. However, there is a lack of an in-depth survey to sort out the
development context of existing works, analyze how existing works combine the
LLM-based agent technologies to optimize various tasks, and clarify the
framework of LLM-based agents in SE. In this paper, we conduct the first survey
of the studies on combining LLM-based agents with SE and present a framework of
LLM-based agents in SE which includes three key modules: perception, memory,
and action. We also summarize the current challenges in combining the two
fields and propose future opportunities in response to existing challenges. We
maintain a GitHub repository of the related papers at:
https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.",2024-09-13,"Yanlin Wang, Wanjun Zhong, Yanxian Huang, Ensheng Shi, Min Yang, Jiachi Chen, Hui Li, Yuchi Ma, Qianxiang Wang, Zibin Zheng",http://arxiv.org/pdf/2409.09030v2,cs.CL
AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents,"Truthfulness (adherence to factual accuracy) and utility (satisfying human
needs and instructions) are both fundamental aspects of Large Language Models,
yet these goals often conflict (e.g., sell a car with known flaws), which makes
it challenging to achieve both in real-world deployments. We propose AI-LieDar,
a framework to study how LLM-based agents navigate these scenarios in an
multi-turn interactive setting. We design a set of real-world scenarios where
language agents are instructed to achieve goals that are in conflict with being
truthful during a multi-turn conversation with simulated human agents. To
evaluate the truthfulness at large scale, we develop a truthfulness detector
inspired by psychological literature to assess the agents' responses. Our
experiment demonstrates that all models are truthful less than 50% of the time,
though truthfulness and goal achievement (utility) rates vary across models. We
further test the steerability of LLMs towards truthfulness, finding that models
can be directed to be truthful or deceptive, and even truth-steered models
still lie. These findings reveal the complex nature of truthfulness in LLMs and
underscore the importance of further research to ensure the safe and reliable
deployment of LLMs and LLM-based agents.",2024-09-13,"Zhe Su, Xuhui Zhou, Sanketh Rangreji, Anubha Kabra, Julia Mendelsohn, Faeze Brahman, Maarten Sap",http://arxiv.org/pdf/2409.09013v2,cs.CL
Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach,"Direct speech translation (ST) models often struggle with rare words.
Incorrect translation of these words can have severe consequences, impacting
translation quality and user trust. While rare word translation is inherently
challenging for neural models due to sparse learning signals, real-world
scenarios often allow access to translations of past recordings on similar
topics. To leverage these valuable resources, we propose a
retrieval-and-demonstration approach to enhance rare word translation accuracy
in direct ST models. First, we adapt existing ST models to incorporate
retrieved examples for rare word translation, which allows the model to benefit
from prepended examples, similar to in-context learning. We then develop a
cross-modal (speech-to-speech, speech-to-text, text-to-text) retriever to
locate suitable examples. We demonstrate that standard ST models can be
effectively adapted to leverage examples for rare word translation, improving
rare word translation accuracy over the baseline by 17.6% with gold examples
and 8.5% with retrieved examples. Moreover, our speech-to-speech retrieval
approach outperforms other modalities and exhibits higher robustness to unseen
speakers. Our code is publicly available
(https://github.com/SiqiLii/Retrieve-and-Demonstration-ST).",2024-09-13,"Siqi Li, Danni Liu, Jan Niehues",http://arxiv.org/pdf/2409.09009v2,cs.CL
Pronoun Logic,"Particularly in transgender and nonbinary (TGNB) communities, it is an
increasingly common practice to publicly share one's personal pronouns so that
we may be gendered correctly in others' speech. Many of us have nuanced desires
for how we are gendered, leading us to use more complex descriptions of our
wishes; for example, the descriptor 'she/they'. We observe that these
descriptions of our wishes have the structure of a little language all their
own. We thus propose formal logic as a tool for expressing one's personal
pronouns and potentially other aspects of gender. We explore three potential
logical foundations (linear logic, temporal logic, and free logic with definite
descriptions) and their trade-offs. Our foremost motivation for this proposal
is play, affirming that one can be both a logician and TGNB at the same time.
We present formalization as something that can continue to evolve over time
with society's understanding of gender. This implies that outreach is a major
potential application: we can show TGNB youth that they belong in logic and
have a unique contribution to make. Tools for evaluating whether one's pronouns
are respected are an application as well.",2024-09-13,"Rose Bohrer, Ashe Neth",http://arxiv.org/pdf/2409.18978v1,cs.CL
"E2MoCase: A Dataset for Emotional, Event and Moral Observations in News Articles on High-impact Legal Cases","The way media reports on legal cases can significantly shape public opinion,
often embedding subtle biases that influence societal views on justice and
morality. Analyzing these biases requires a holistic approach that captures the
emotional tone, moral framing, and specific events within the narratives. In
this work we introduce E2MoCase, a novel dataset designed to facilitate the
integrated analysis of emotions, moral values, and events within legal
narratives and media coverage. By leveraging advanced models for emotion
detection, moral value identification, and event extraction, E2MoCase offers a
multi-dimensional perspective on how legal cases are portrayed in news
articles.",2024-09-13,"Candida M. Greco, Lorenzo Zangari, Davide Picca, Andrea Tagarelli",http://arxiv.org/pdf/2409.09001v1,cs.CL
KodeXv0.1: A Family of State-of-the-Art Financial Large Language Models,"Although powerful, current cutting-edge LLMs may not fulfil the needs of
highly specialised sectors. We introduce KodeXv0.1, a family of large language
models that outclass GPT-4 in financial question answering. We utilise the base
variants of Llama 3.1 8B and 70B and adapt them to the financial domain through
a custom training regime. To this end, we collect and process a large number of
publicly available financial documents such as earnings calls and business
reports. These are used to generate a high-quality, synthetic dataset
consisting of Context-Question-Answer triplets which closely mirror real-world
financial tasks. Using the train split of this dataset, we perform RAG-aware
4bit LoRA instruction tuning runs of Llama 3.1 base variants to produce
KodeX-8Bv0.1 and KodeX-70Bv0.1. We then complete extensive model evaluations
using FinanceBench, FinQABench and the withheld test split of our dataset. Our
results show that KodeX-8Bv0.1 is more reliable in financial contexts than
cutting-edge instruct models in the same parameter regime, surpassing them by
up to 9.24%. In addition, it is even capable of outperforming state-of-the-art
proprietary models such as GPT-4 by up to 7.07%. KodeX-70Bv0.1 represents a
further improvement upon this, exceeding GPT-4's performance on every tested
benchmark.",2024-09-13,"Neel Rajani, Lilli Kiessling, Aleksandr Ogaltsov, Claus Lang",http://arxiv.org/pdf/2409.13749v1,cs.CL
Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance,"Ensuring content compliance with community guidelines is crucial for
maintaining healthy online social environments. However, traditional
human-based compliance checking struggles with scaling due to the increasing
volume of user-generated content and a limited number of moderators. Recent
advancements in Natural Language Understanding demonstrated by Large Language
Models unlock new opportunities for automated content compliance verification.
This work evaluates six AI-agents built on Open-LLMs for automated rule
compliance checking in Decentralized Social Networks, a challenging environment
due to heterogeneous community scopes and rules. Analyzing over 50,000 posts
from hundreds of Mastodon servers, we find that AI-agents effectively detect
non-compliant content, grasp linguistic subtleties, and adapt to diverse
community contexts. Most agents also show high inter-rater reliability and
consistency in score justification and suggestions for compliance. Human-based
evaluation with domain experts confirmed the agents' reliability and
usefulness, rendering them promising tools for semi-automated or
human-in-the-loop content moderation systems.",2024-09-13,"Lucio La Cava, Andrea Tagarelli",http://arxiv.org/pdf/2409.08963v1,cs.CL
SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical Records,"We present the SynSUM benchmark, a synthetic dataset linking unstructured
clinical notes to structured background variables. The dataset consists of
10,000 artificial patient records containing tabular variables (like symptoms,
diagnoses and underlying conditions) and related notes describing the fictional
patient encounter in the domain of respiratory diseases. The tabular portion of
the data is generated through a Bayesian network, where both the causal
structure between the variables and the conditional probabilities are proposed
by an expert based on domain knowledge. We then prompt a large language model
(GPT-4o) to generate a clinical note related to this patient encounter,
describing the patient symptoms and additional context. We conduct both an
expert evaluation study to assess the quality of the generated notes, as well
as running some simple predictor models on both the tabular and text portions
of the dataset, forming a baseline for further research. The SynSUM dataset is
primarily designed to facilitate research on clinical information extraction in
the presence of tabular background variables, which can be linked through
domain knowledge to concepts of interest to be extracted from the text - the
symptoms, in the case of SynSUM. Secondary uses include research on the
automation of clinical reasoning over both tabular data and text, causal effect
estimation in the presence of tabular and/or textual confounders, and
multi-modal synthetic data generation.",2024-09-13,"Paloma Rabaey, Henri Arno, Stefan Heytens, Thomas Demeester",http://arxiv.org/pdf/2409.08936v2,cs.CL
Affective Computing Has Changed: The Foundation Model Disruption,"The dawn of Foundation Models has on the one hand revolutionised a wide range
of research problems, and, on the other hand, democratised the access and use
of AI-based tools by the general public. We even observe an incursion of these
models into disciplines related to human psychology, such as the Affective
Computing domain, suggesting their affective, emerging capabilities. In this
work, we aim to raise awareness of the power of Foundation Models in the field
of Affective Computing by synthetically generating and analysing multimodal
affective data, focusing on vision, linguistics, and speech (acoustics). We
also discuss some fundamental problems, such as ethical issues and regulatory
aspects, related to the use of Foundation Models in this research area.",2024-09-13,"Björn Schuller, Adria Mallol-Ragolta, Alejandro Peña Almansa, Iosif Tsangko, Mostafa M. Amin, Anastasia Semertzidou, Lukas Christ, Shahin Amiriparian",http://arxiv.org/pdf/2409.08907v1,cs.CL
Visual Language Tracking with Multi-modal Interaction: A Robust Benchmark,"Visual Language Tracking (VLT) enhances tracking by mitigating the
limitations of relying solely on the visual modality, utilizing high-level
semantic information through language. This integration of the language enables
more advanced human-machine interaction. The essence of interaction is
cognitive alignment, which typically requires multiple information exchanges,
especially in the sequential decision-making process of VLT. However, current
VLT benchmarks do not account for multi-round interactions during tracking.
They provide only an initial text and bounding box (bbox) in the first frame,
with no further interaction as tracking progresses, deviating from the original
motivation of the VLT task. To address these limitations, we propose a novel
and robust benchmark, VLT-MI (Visual Language Tracking with Multi-modal
Interaction), which introduces multi-round interaction into the VLT task for
the first time. (1) We generate diverse, multi-granularity texts for
multi-round, multi-modal interaction based on existing mainstream VLT
benchmarks using DTLLM-VLT, leveraging the world knowledge of LLMs. (2) We
propose a new VLT interaction paradigm that achieves multi-round interaction
through text updates and object recovery. When multiple tracking failures
occur, we provide the tracker with more aligned texts and corrected bboxes
through interaction, thereby expanding the scope of VLT downstream tasks. (3)
We conduct comparative experiments on both traditional VLT benchmarks and
VLT-MI, evaluating and analyzing the accuracy and robustness of trackers under
the interactive paradigm. This work offers new insights and paradigms for the
VLT task, enabling a fine-grained evaluation of multi-modal trackers. We
believe this approach can be extended to additional datasets in the future,
supporting broader evaluations and comparisons of video-language model
capabilities.",2024-09-13,"Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang",http://arxiv.org/pdf/2409.08887v1,cs.CL
Exploring the Impact of Data Quantity on ASR in Extremely Low-resource Languages,"This study investigates the efficacy of data augmentation techniques for
low-resource automatic speech recognition (ASR), focusing on two endangered
Austronesian languages, Amis and Seediq. Recognizing the potential of
self-supervised learning (SSL) in low-resource settings, we explore the impact
of data volume on the continued pre-training of SSL models. We propose a novel
data-selection scheme leveraging a multilingual corpus to augment the limited
target language data. This scheme utilizes a language classifier to extract
utterance embeddings and employs one-class classifiers to identify utterances
phonetically and phonologically proximate to the target languages. Utterances
are ranked and selected based on their decision scores, ensuring the inclusion
of highly relevant data in the SSL-ASR pipeline. Our experimental results
demonstrate the effectiveness of this approach, yielding substantial
improvements in ASR performance for both Amis and Seediq. These findings
underscore the feasibility and promise of data augmentation through
cross-lingual transfer learning for low-resource language ASR.",2024-09-13,"Yao-Fei Cheng, Li-Wei Chen, Hung-Shin Lee, Hsin-Min Wang",http://arxiv.org/pdf/2409.08872v1,cs.CL
FP-VEC: Fingerprinting Large Language Models via Efficient Vector Addition,"Training Large Language Models (LLMs) requires immense computational power
and vast amounts of data. As a result, protecting the intellectual property of
these models through fingerprinting is essential for ownership authentication.
While adding fingerprints to LLMs through fine-tuning has been attempted, it
remains costly and unscalable. In this paper, we introduce FP-VEC, a pilot
study on using fingerprint vectors as an efficient fingerprinting method for
LLMs. Our approach generates a fingerprint vector that represents a
confidential signature embedded in the model, allowing the same fingerprint to
be seamlessly incorporated into an unlimited number of LLMs via vector
addition. Results on several LLMs show that FP-VEC is lightweight by running on
CPU-only devices for fingerprinting, scalable with a single training and
unlimited fingerprinting process, and preserves the model's normal behavior.
The project page is available at https://fingerprintvector.github.io .",2024-09-13,"Zhenhua Xu, Wenpeng Xing, Zhebo Wang, Chang Hu, Chen Jie, Meng Han",http://arxiv.org/pdf/2409.08846v1,cs.CL
AIPO: Improving Training Objective for Iterative Preference Optimization,"Preference Optimization (PO), is gaining popularity as an alternative choice
of Proximal Policy Optimization (PPO) for aligning Large Language Models
(LLMs). Recent research on aligning LLMs iteratively with synthetic or
partially synthetic data shows promising results in scaling up PO training for
both academic settings and proprietary trained models such as Llama3. Despite
its success, our study shows that the length exploitation issue present in PO
is even more severe in Iterative Preference Optimization (IPO) due to the
iterative nature of the process. In this work, we study iterative preference
optimization with synthetic data. We share the findings and analysis along the
way of building the iterative preference optimization pipeline. More
specifically, we discuss the length exploitation issue during iterative
preference optimization and propose our training objective for iterative
preference optimization, namely Agreement-aware Iterative Preference
Optimization (AIPO). To demonstrate the effectiveness of our method, we conduct
comprehensive experiments and achieve state-of-the-art performance on MT-Bench,
AlpacaEval 2.0, and Arena-Hard. Our implementation and model checkpoints will
be made available at https://github.com/bytedance/AIPO.",2024-09-13,"Yaojie Shen, Xinyao Wang, Yulei Niu, Ying Zhou, Lexin Tang, Libo Zhang, Fan Chen, Longyin Wen",http://arxiv.org/pdf/2409.08845v1,cs.CL
Your Weak LLM is Secretly a Strong Teacher for Alignment,"The burgeoning capabilities of large language models (LLMs) have underscored
the need for alignment to ensure these models act in accordance with human
values and intentions. Existing alignment frameworks present constraints either
in the form of expensive human effort or high computational costs. This paper
explores a promising middle ground, where we employ a weak LLM that is
significantly less resource-intensive than top-tier models, yet offers more
automation than purely human feedback. We present a systematic study to
evaluate and understand weak LLM's ability to generate feedback for alignment.
Our empirical findings demonstrate that weak LLMs can provide feedback that
rivals or even exceeds that of fully human-annotated data. Our study indicates
a minimized impact of model size on feedback efficacy, shedding light on a
scalable and sustainable alignment strategy. To deepen our understanding of
alignment under weak LLM feedback, we conduct a series of qualitative and
quantitative analyses, offering novel insights into the quality discrepancies
between human feedback vs. weak LLM feedback.",2024-09-13,"Leitian Tao, Yixuan Li",http://arxiv.org/pdf/2409.08813v2,cs.CL
Exploring SSL Discrete Tokens for Multilingual ASR,"With the advancement of Self-supervised Learning (SSL) in speech-related
tasks, there has been growing interest in utilizing discrete tokens generated
by SSL for automatic speech recognition (ASR), as they offer faster processing
techniques. However, previous studies primarily focused on multilingual ASR
with Fbank features or English ASR with discrete tokens, leaving a gap in
adapting discrete tokens for multilingual ASR scenarios. This study presents a
comprehensive comparison of discrete tokens generated by various leading SSL
models across multiple language domains. We aim to explore the performance and
efficiency of speech discrete tokens across multiple language domains for both
monolingual and multilingual ASR scenarios. Experimental results demonstrate
that discrete tokens achieve comparable results against systems trained on
Fbank features in ASR tasks across seven language domains with an average word
error rate (WER) reduction of 0.31% and 1.76% absolute (2.80% and 15.70%
relative) on dev and test sets respectively, with particularly WER reduction of
6.82% absolute (41.48% relative) on the Polish test set.",2024-09-13,"Mingyu Cui, Daxin Tan, Yifan Yang, Dingdong Wang, Huimeng Wang, Xiao Chen, Xie Chen, Xunying Liu",http://arxiv.org/pdf/2409.08805v1,cs.CL
Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR,"Self-supervised learning (SSL) based discrete speech representations are
highly compact and domain adaptable. In this paper, SSL discrete speech
features extracted from WavLM models are used as additional cross-utterance
acoustic context features in Zipformer-Transducer ASR systems. The efficacy of
replacing Fbank features with discrete token features for modelling either
cross-utterance contexts (from preceding and future segments), or current
utterance's internal contexts alone, or both at the same time, are demonstrated
thoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer
system using discrete tokens based cross-utterance context features outperforms
the baseline using utterance internal context only with statistically
significant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78%
to 3.54% relative) on the dev and test data. The lowest published WER of 11.15%
and 11.14% were obtained on the dev and test sets. Our work is open-source and
publicly available at
https://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\_ASR.",2024-09-13,"Mingyu Cui, Yifan Yang, Jiajun Deng, Jiawen Kang, Shujie Hu, Tianzi Wang, Zhaoqing Li, Shiliang Zhang, Xie Chen, Xunying Liu",http://arxiv.org/pdf/2409.08797v1,cs.CL
Optimizing Ingredient Substitution Using Large Language Models to Enhance Phytochemical Content in Recipes,"In the emerging field of computational gastronomy, aligning culinary
practices with scientifically supported nutritional goals is increasingly
important. This study explores how large language models (LLMs) can be applied
to optimize ingredient substitutions in recipes, specifically to enhance the
phytochemical content of meals. Phytochemicals are bioactive compounds found in
plants, which, based on preclinical studies, may offer potential health
benefits. We fine-tuned models, including OpenAI's GPT-3.5, DaVinci, and Meta's
TinyLlama, using an ingredient substitution dataset. These models were used to
predict substitutions that enhance phytochemical content and create a
corresponding enriched recipe dataset. Our approach improved Hit@1 accuracy on
ingredient substitution tasks, from the baseline 34.53 plus-minus 0.10% to
38.03 plus-minus 0.28% on the original GISMo dataset, and from 40.24 plus-minus
0.36% to 54.46 plus-minus 0.29% on a refined version of the same dataset. These
substitutions led to the creation of 1,951 phytochemically enriched ingredient
pairings and 1,639 unique recipes. While this approach demonstrates potential
in optimizing ingredient substitutions, caution must be taken when drawing
conclusions about health benefits, as the claims are based on preclinical
evidence. Future work should include clinical validation and broader datasets
to further evaluate the nutritional impact of these substitutions. This
research represents a step forward in using AI to promote healthier eating
practices, providing potential pathways for integrating computational methods
with nutritional science.",2024-09-13,"Luis Rita, Josh Southern, Ivan Laponogov, Kyle Higgins, Kirill Veselkov",http://arxiv.org/pdf/2409.08792v1,cs.CL
Sign Language Sense Disambiguation,"This project explores methods to enhance sign language translation of German
sign language, specifically focusing on disambiguation of homonyms. Sign
language is ambiguous and understudied which is the basis for our experiments.
We approach the improvement by training transformer-based models on various
bodypart representations to shift the focus on said bodypart. To determine the
impact of, e.g., the hand or mouth representations, we experiment with
different combinations. The results show that focusing on the mouth increases
the performance in small dataset settings while shifting the focus on the hands
retrieves better results in larger dataset settings. Our results contribute to
better accessibility for non-hearing persons by improving the systems powering
digital assistants, enabling a more accurate interaction. The code for this
project can be found on GitHub.",2024-09-13,"Jana Grimm, Miriam Winkler, Oliver Kraus, Tanalp Agustoslu",http://arxiv.org/pdf/2409.08780v1,cs.CL
"Journalists, Emotions, and the Introduction of Generative AI Chatbots: A Large-Scale Analysis of Tweets Before and After the Launch of ChatGPT","As part of a broader look at the impact of generative AI, this study
investigated the emotional responses of journalists to the release of ChatGPT
at the time of its launch. By analyzing nearly 1 million Tweets from
journalists at major U.S. news outlets, we tracked changes in emotional tone
and sentiment before and after the introduction of ChatGPT in November 2022.
Using various computational and natural language processing techniques to
measure emotional shifts in response to ChatGPT's release, we found an increase
in positive emotion and a more favorable tone post-launch, suggesting initial
optimism toward AI's potential. This research underscores the pivotal role of
journalists as interpreters of technological innovation and disruption,
highlighting how their emotional reactions may shape public narratives around
emerging technologies. The study contributes to understanding the intersection
of journalism, emotion, and AI, offering insights into the broader societal
impact of generative AI tools.",2024-09-13,"Seth C. Lewis, David M. Markowitz, Jon Benedik Bunquin",http://arxiv.org/pdf/2409.08761v1,cs.CL
Distilling Monolingual and Crosslingual Word-in-Context Representations,"In this study, we propose a method that distils representations of word
meaning in context from a pre-trained masked language model in both monolingual
and crosslingual settings. Word representations are the basis for context-aware
lexical semantics and unsupervised semantic textual similarity (STS)
estimation. Different from existing approaches, our method does not require
human-annotated corpora nor updates of the parameters of the pre-trained model.
The latter feature is appealing for practical scenarios where the off-the-shelf
pre-trained model is a common asset among different applications. Specifically,
our method learns to combine the outputs of different hidden layers of the
pre-trained model using self-attention. Our auto-encoder based training only
requires an automatically generated corpus. To evaluate the performance of the
proposed approach, we performed extensive experiments using various benchmark
tasks. The results on the monolingual tasks confirmed that our representations
exhibited a competitive performance compared to that of the previous study for
the context-aware lexical semantic tasks and outperformed it for STS
estimation. The results of the crosslingual tasks revealed that the proposed
method largely improved crosslingual word representations of multilingual
pre-trained models.",2024-09-13,"Yuki Arase, Tomoyuki Kajiwara",http://arxiv.org/pdf/2409.08719v1,cs.CL
Layerwise Change of Knowledge in Neural Networks,"This paper aims to explain how a deep neural network (DNN) gradually extracts
new knowledge and forgets noisy features through layers in forward propagation.
Up to now, although the definition of knowledge encoded by the DNN has not
reached a consensus, Previous studies have derived a series of mathematical
evidence to take interactions as symbolic primitive inference patterns encoded
by a DNN. We extend the definition of interactions and, for the first time,
extract interactions encoded by intermediate layers. We quantify and track the
newly emerged interactions and the forgotten interactions in each layer during
the forward propagation, which shed new light on the learning behavior of DNNs.
The layer-wise change of interactions also reveals the change of the
generalization capacity and instability of feature representations of a DNN.",2024-09-13,"Xu Cheng, Lei Cheng, Zhaoran Peng, Yang Xu, Tian Han, Quanshi Zhang",http://arxiv.org/pdf/2409.08712v1,cs.CL
L3Cube-IndicQuest: A Benchmark Question Answering Dataset for Evaluating Knowledge of LLMs in Indic Context,"Large Language Models (LLMs) have made significant progress in incorporating
Indic languages within multilingual models. However, it is crucial to
quantitatively assess whether these languages perform comparably to globally
dominant ones, such as English. Currently, there is a lack of benchmark
datasets specifically designed to evaluate the regional knowledge of LLMs in
various Indic languages. In this paper, we present the L3Cube-IndicQuest, a
gold-standard factual question-answering benchmark dataset designed to evaluate
how well multilingual LLMs capture regional knowledge across various Indic
languages. The dataset contains 200 question-answer pairs, each for English and
19 Indic languages, covering five domains specific to the Indic region. We aim
for this dataset to serve as a benchmark, providing ground truth for evaluating
the performance of LLMs in understanding and representing knowledge relevant to
the Indian context. The IndicQuest can be used for both reference-based
evaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at
https://github.com/l3cube-pune/indic-nlp .",2024-09-13,"Pritika Rohera, Chaitrali Ginimav, Akanksha Salunke, Gayatri Sawant, Raviraj Joshi",http://arxiv.org/pdf/2409.08706v2,cs.CL
B4: Towards Optimal Assessment of Plausible Code Solutions with Plausible Tests,"Selecting the best code solution from multiple generated ones is an essential
task in code generation, which can be achieved by using some reliable
validators (e.g., developer-written test cases) for assistance. Since reliable
test cases are not always available and can be expensive to build in practice,
researchers propose to automatically generate test cases to assess code
solutions. However, when both code solutions and test cases are plausible and
not reliable, selecting the best solution becomes challenging. Although some
heuristic strategies have been proposed to tackle this problem, they lack a
strong theoretical guarantee and it is still an open question whether an
optimal selection strategy exists. Our work contributes in two ways. First, we
show that within a Bayesian framework, the optimal selection strategy can be
defined based on the posterior probability of the observed passing states
between solutions and tests. The problem of identifying the best solution is
then framed as an integer programming problem. Second, we propose an efficient
approach for approximating this optimal (yet uncomputable) strategy, where the
approximation error is bounded by the correctness of prior knowledge. We then
incorporate effective prior knowledge to tailor code generation tasks. Both
theoretical and empirical studies confirm that existing heuristics are limited
in selecting the best solutions with plausible test cases. Our proposed
approximated optimal strategy B4 significantly surpasses existing heuristics in
selecting code solutions generated by large language models (LLMs) with
LLM-generated tests, achieving a relative performance improvement by up to 50%
over the strongest heuristic and 246% over the random selection in the most
challenging scenarios. Our code is publicly available at
https://github.com/ZJU-CTAG/B4.",2024-09-13,"Mouxiang Chen, Zhongxin Liu, He Tao, Yusu Hong, David Lo, Xin Xia, Jianling Sun",http://arxiv.org/pdf/2409.08692v1,cs.CL
NEST-RQ: Next Token Prediction for Speech Self-Supervised Pre-Training,"Speech self-supervised pre-training can effectively improve the performance
of downstream tasks. However, previous self-supervised learning (SSL) methods
for speech, such as HuBERT and BEST-RQ, focus on utilizing non-causal encoders
with bidirectional context, and lack sufficient support for downstream
streaming models. To address this issue, we introduce the next token prediction
based speech pre-training method with random-projection quantizer (NEST-RQ).
NEST-RQ employs causal encoders with only left context and uses next token
prediction (NTP) as the training task. On the large-scale dataset, compared to
BEST-RQ, the proposed NEST-RQ achieves comparable performance on non-streaming
automatic speech recognition (ASR) and better performance on streaming ASR. We
also conduct analytical experiments in terms of the future context size of
streaming ASR, the codebook quality of SSL and the model size of the encoder.
In summary, the paper demonstrates the feasibility of the NTP in speech SSL and
provides empirical evidence and insights for speech SSL research.",2024-09-13,"Minglun Han, Ye Bai, Chen Shen, Youjia Huang, Mingkun Huang, Zehua Lin, Linhao Dong, Lu Lu, Yuxuan Wang",http://arxiv.org/pdf/2409.08680v1,cs.CL
Investigating Disentanglement in a Phoneme-level Speech Codec for Prosody Modeling,"Most of the prevalent approaches in speech prosody modeling rely on learning
global style representations in a continuous latent space which encode and
transfer the attributes of reference speech. However, recent work on neural
codecs which are based on Residual Vector Quantization (RVQ) already shows
great potential offering distinct advantages. We investigate the prosody
modeling capabilities of the discrete space of such an RVQ-VAE model, modifying
it to operate on the phoneme-level. We condition both the encoder and decoder
of the model on linguistic representations and apply a global speaker embedding
in order to factor out both phonetic and speaker information. We conduct an
extensive set of investigations based on subjective experiments and objective
measures to show that the phoneme-level discrete latent representations
obtained this way achieves a high degree of disentanglement, capturing
fine-grained prosodic information that is robust and transferable. The latent
space turns out to have interpretable structure with its principal components
corresponding to pitch and energy.",2024-09-13,"Sotirios Karapiperis, Nikolaos Ellinas, Alexandra Vioni, Junkwang Oh, Gunu Jho, Inchul Hwang, Spyros Raptis",http://arxiv.org/pdf/2409.08664v1,cs.CL
LA-RAG:Enhancing LLM-based ASR Accuracy with Retrieval-Augmented Generation,"Recent advancements in integrating speech information into large language
models (LLMs) have significantly improved automatic speech recognition (ASR)
accuracy. However, existing methods often constrained by the capabilities of
the speech encoders under varied acoustic conditions, such as accents. To
address this, we propose LA-RAG, a novel Retrieval-Augmented Generation (RAG)
paradigm for LLM-based ASR. LA-RAG leverages fine-grained token-level speech
datastores and a speech-to-speech retrieval mechanism to enhance ASR accuracy
via LLM in-context learning (ICL) capabilities. Experiments on Mandarin and
various Chinese dialect datasets demonstrate significant improvements in ASR
accuracy compared to existing methods, validating the effectiveness of our
approach, especially in handling accent variations.",2024-09-13,"Shaojun Li, Hengchao Shang, Daimeng Wei, Jiaxin Guo, Zongyao Li, Xianghui He, Min Zhang, Hao Yang",http://arxiv.org/pdf/2409.08597v1,cs.CL
Large Language Model Can Transcribe Speech in Multi-Talker Scenarios with Versatile Instructions,"Recent advancements in large language models (LLMs) have revolutionized
various domains, bringing significant progress and new opportunities. Despite
progress in speech-related tasks, LLMs have not been sufficiently explored in
multi-talker scenarios. In this work, we present a pioneering effort to
investigate the capability of LLMs in transcribing speech in multi-talker
environments, following versatile instructions related to multi-talker
automatic speech recognition (ASR), target talker ASR, and ASR based on
specific talker attributes such as sex, occurrence order, language, and keyword
spoken. Our approach utilizes WavLM and Whisper encoder to extract
multi-faceted speech representations that are sensitive to speaker
characteristics and semantic context. These representations are then fed into
an LLM fine-tuned using LoRA, enabling the capabilities for speech
comprehension and transcription. Comprehensive experiments reveal the promising
performance of our proposed system, MT-LLM, in cocktail party scenarios,
highlighting the potential of LLM to handle speech-related tasks based on user
instructions in such complex settings. The code, model, and samples are
available at https://github.com/cuhealthybrains/MT-LLM.",2024-09-13,"Lingwei Meng, Shujie Hu, Jiawen Kang, Zhaoqing Li, Yuejiao Wang, Wenxuan Wu, Xixin Wu, Xunying Liu, Helen Meng",http://arxiv.org/pdf/2409.08596v2,cs.CL
Cracking the Code: Multi-domain LLM Evaluation on Real-World Professional Exams in Indonesia,"While knowledge evaluation in large language models has predominantly focused
on academic subjects like math and physics, these assessments often fail to
capture the practical demands of real-world professions. In this paper, we
introduce IndoCareer, a dataset comprising 8,834 multiple-choice questions
designed to evaluate performance in vocational and professional certification
exams across various fields. With a focus on Indonesia, IndoCareer provides
rich local contexts, spanning six key sectors: (1) healthcare, (2) insurance
and finance, (3) creative and design, (4) tourism and hospitality, (5)
education and training, and (6) law. Our comprehensive evaluation of 27 large
language models shows that these models struggle particularly in fields with
strong local contexts, such as insurance and finance. Additionally, while using
the entire dataset, shuffling answer options generally maintains consistent
evaluation results across models, but it introduces instability specifically in
the insurance and finance sectors.",2024-09-13,Fajri Koto,http://arxiv.org/pdf/2409.08564v2,cs.CL
Expediting and Elevating Large Language Model Reasoning via Hidden Chain-of-Thought Decoding,"Large language models (LLMs) have demonstrated remarkable capabilities in
tasks requiring reasoning and multi-step problem-solving through the use of
chain-of-thought (CoT) prompting. However, generating the full CoT process
results in significantly longer output sequences, leading to increased
computational costs and latency during inference. To address this challenge, we
propose a novel approach to compress the CoT process through semantic
alignment, enabling more efficient decoding while preserving the benefits of
CoT reasoning. Our method introduces an auxiliary CoT model that learns to
generate and compress the full thought process into a compact special token
representation semantically aligned with the original CoT output. This
compressed representation is then integrated into the input of the Hidden
Chain-of-Thought (HCoT) model. The training process follows a two-stage
procedure: First, the CoT model is optimized to generate the compressed token
representations aligned with the ground-truth CoT outputs using a contrastive
loss. Subsequently, with the CoT model parameters frozen, the HCoT model is
fine-tuned to generate accurate subsequent predictions conditioned on the
prefix instruction and the compressed CoT representations from the CoT model.
Extensive experiments across three challenging domains - mathematical
reasoning, agent invocation, and question answering - demonstrate that our
semantic compression approach achieves competitive or improved performance
compared to the full CoT baseline, while providing significant speedups of at
least 1.5x in decoding time. Moreover, incorporating contrastive learning
objectives further enhances the quality of the compressed representations,
leading to better CoT prompting and improved task accuracy. Our work paves the
way for more efficient exploitation of multi-step reasoning capabilities in
LLMs across a wide range of applications.",2024-09-13,"Tianqiao Liu, Zui Chen, Zitao Liu, Mi Tian, Weiqi Luo",http://arxiv.org/pdf/2409.08561v1,cs.CL
LLM-Powered Grapheme-to-Phoneme Conversion: Benchmark and Case Study,"Grapheme-to-phoneme (G2P) conversion is critical in speech processing,
particularly for applications like speech synthesis. G2P systems must possess
linguistic understanding and contextual awareness of languages with polyphone
words and context-dependent phonemes. Large language models (LLMs) have
recently demonstrated significant potential in various language tasks,
suggesting that their phonetic knowledge could be leveraged for G2P. In this
paper, we evaluate the performance of LLMs in G2P conversion and introduce
prompting and post-processing methods that enhance LLM outputs without
additional training or labeled data. We also present a benchmarking dataset
designed to assess G2P performance on sentence-level phonetic challenges of the
Persian language. Our results show that by applying the proposed methods, LLMs
can outperform traditional G2P tools, even in an underrepresented language like
Persian, highlighting the potential of developing LLM-aided G2P systems.",2024-09-13,"Mahta Fetrat Qharabagh, Zahra Dehghanian, Hamid R. Rabiee",http://arxiv.org/pdf/2409.08554v1,cs.CL
AccentBox: Towards High-Fidelity Zero-Shot Accent Generation,"While recent Zero-Shot Text-to-Speech (ZS-TTS) models have achieved high
naturalness and speaker similarity, they fall short in accent fidelity and
control. To address this issue, we propose zero-shot accent generation that
unifies Foreign Accent Conversion (FAC), accented TTS, and ZS-TTS, with a novel
two-stage pipeline. In the first stage, we achieve state-of-the-art (SOTA) on
Accent Identification (AID) with 0.56 f1 score on unseen speakers. In the
second stage, we condition a ZS-TTS system on the pretrained speaker-agnostic
accent embeddings extracted by the AID model. The proposed system achieves
higher accent fidelity on inherent/cross accent generation, and enables unseen
accent generation.",2024-09-13,"Jinzuomu Zhong, Korin Richmond, Zhiba Su, Siqi Sun",http://arxiv.org/pdf/2409.09098v2,cs.CL
Eir: Thai Medical Large Language Models,"We present Eir-8B, a large language model with 8 billion parameters,
specifically designed to enhance the accuracy of handling medical tasks in the
Thai language. This model focuses on providing clear and easy-to-understand
answers for both healthcare professionals and patients, thereby improving the
efficiency of diagnosis and treatment processes. Human evaluation was conducted
to ensure that the model adheres to care standards and provides unbiased
answers.
  To prioritize data security, the model is deployed within the hospital's
internal network, ensuring both high security and faster processing speeds. The
internal API connection is secured with encryption and strict authentication
measures to prevent data leaks and unauthorized access.
  We evaluated several open-source large language models with 8 billion
parameters on four medical benchmarks: MedQA, MedMCQA, PubMedQA, and the
medical subset of MMLU. The best-performing baselines were used to develop
Eir-8B. Our evaluation employed multiple questioning strategies, including
zero-shot, few-shot, chain-of-thought reasoning, and ensemble/self-consistency
voting methods. Our model outperformed commercially available Thai-language
large language models by more than 10%. In addition, we developed enhanced
model testing tailored for clinical use in Thai across 18 clinical tasks, where
our model exceeded GPT-4o performance by more than 11%.",2024-09-13,"Yutthakorn Thiprak, Rungtam Ngodngamthaweesuk, Songtam Ngodngamtaweesuk",http://arxiv.org/pdf/2409.08523v2,cs.CL
MAPX: An explainable model-agnostic framework for the detection of false information on social media networks,"The automated detection of false information has become a fundamental task in
combating the spread of ""fake news"" on online social media networks (OSMN) as
it reduces the need for manual discernment by individuals. In the literature,
leveraging various content or context features of OSMN documents have been
found useful. However, most of the existing detection models often utilise
these features in isolation without regard to the temporal and dynamic changes
oft-seen in reality, thus, limiting the robustness of the models. Furthermore,
there has been little to no consideration of the impact of the quality of
documents' features on the trustworthiness of the final prediction. In this
paper, we introduce a novel model-agnostic framework, called MAPX, which allows
evidence based aggregation of predictions from existing models in an
explainable manner. Indeed, the developed aggregation method is adaptive,
dynamic and considers the quality of OSMN document features. Further, we
perform extensive experiments on benchmarked fake news datasets to demonstrate
the effectiveness of MAPX using various real-world data quality scenarios. Our
empirical results show that the proposed framework consistently outperforms all
state-of-the-art models evaluated. For reproducibility, a demo of MAPX is
available at \href{https://github.com/SCondran/MAPX_framework}{this link}",2024-09-13,"Sarah Condran, Michael Bewong, Selasi Kwashie, Md Zahidul Islam, Irfan Altas, Joshua Condran",http://arxiv.org/pdf/2409.08522v1,cs.CL
A BERT-Based Summarization approach for depression detection,"Depression is a globally prevalent mental disorder with potentially severe
repercussions if not addressed, especially in individuals with recurrent
episodes. Prior research has shown that early intervention has the potential to
mitigate or alleviate symptoms of depression. However, implementing such
interventions in a real-world setting may pose considerable challenges. A
promising strategy involves leveraging machine learning and artificial
intelligence to autonomously detect depression indicators from diverse data
sources. One of the most widely available and informative data sources is text,
which can reveal a person's mood, thoughts, and feelings. In this context,
virtual agents programmed to conduct interviews using clinically validated
questionnaires, such as those found in the DAIC-WOZ dataset, offer a robust
means for depression detection through linguistic analysis. Utilizing
BERT-based models, which are powerful and versatile yet use fewer resources
than contemporary large language models, to convert text into numerical
representations significantly enhances the precision of depression diagnosis.
These models adeptly capture complex semantic and syntactic nuances, improving
the detection accuracy of depressive symptoms. Given the inherent limitations
of these models concerning text length, our study proposes text summarization
as a preprocessing technique to diminish the length and intricacies of input
texts. Implementing this method within our uniquely developed framework for
feature extraction and classification yielded an F1-score of 0.67 on the test
set surpassing all prior benchmarks and 0.81 on the validation set exceeding
most previous results on the DAIC-WOZ dataset. Furthermore, we have devised a
depression lexicon to assess summary quality and relevance. This lexicon
constitutes a valuable asset for ongoing research in depression detection.",2024-09-13,"Hossein Salahshoor Gavalan, Mohmmad Naim Rastgoo, Bahareh Nakisa",http://arxiv.org/pdf/2409.08483v1,cs.CL
Explaining Datasets in Words: Statistical Models with Natural Language Parameters,"To make sense of massive data, we often fit simplified models and then
interpret the parameters; for example, we cluster the text embeddings and then
interpret the mean parameters of each cluster. However, these parameters are
often high-dimensional and hard to interpret. To make model parameters directly
interpretable, we introduce a family of statistical models -- including
clustering, time series, and classification models -- parameterized by natural
language predicates. For example, a cluster of text about COVID could be
parameterized by the predicate ""discusses COVID"". To learn these statistical
models effectively, we develop a model-agnostic algorithm that optimizes
continuous relaxations of predicate parameters with gradient descent and
discretizes them by prompting language models (LMs). Finally, we apply our
framework to a wide range of problems: taxonomizing user chat dialogues,
characterizing how they evolve across time, finding categories where one
language model is better than the other, clustering math problems based on
subareas, and explaining visual features in memorable images. Our framework is
highly versatile, applicable to both textual and visual domains, can be easily
steered to focus on specific properties (e.g. subareas), and explains
sophisticated concepts that classical methods (e.g. n-gram analysis) struggle
to produce.",2024-09-13,"Ruiqi Zhong, Heng Wang, Dan Klein, Jacob Steinhardt",http://arxiv.org/pdf/2409.08466v2,cs.CL
When Context Leads but Parametric Memory Follows in Large Language Models,"Large language models (LLMs) have demonstrated remarkable progress in
leveraging diverse knowledge sources. This study investigates how nine widely
used LLMs allocate knowledge between local context and global parameters when
answering open-ended questions in knowledge-consistent scenarios. We introduce
a novel dataset, WikiAtomic, and systematically vary context sizes to analyze
how LLMs prioritize and utilize the provided information and their parametric
knowledge in knowledge-consistent scenarios. Additionally, we also study their
tendency to hallucinate under varying context sizes. Our findings reveal
consistent patterns across models, including a consistent reliance on both
contextual (around 70%) and parametric (around 30%) knowledge, and a decrease
in hallucinations with increasing context. These insights highlight the
importance of more effective context organization and developing models that
use input more deterministically for robust performance.",2024-09-13,"Yufei Tao, Adam Hiatt, Erik Haake, Antonie J. Jetter, Ameeta Agrawal",http://arxiv.org/pdf/2409.08435v4,cs.CL
Retro-li: Small-Scale Retrieval Augmented Generation Supporting Noisy Similarity Searches and Domain Shift Generalization,"The retrieval augmented generation (RAG) system such as Retro has been shown
to improve language modeling capabilities and reduce toxicity and
hallucinations by retrieving from a database of non-parametric memory
containing trillions of entries. We introduce Retro-li that shows retrieval can
also help using a small-scale database, but it demands more accurate and better
neighbors when searching in a smaller hence sparser non-parametric memory. This
can be met by using a proper semantic similarity search. We further propose
adding a regularization to the non-parametric memory for the first time: it
significantly reduces perplexity when the neighbor search operations are noisy
during inference, and it improves generalization when a domain shift occurs. We
also show that Retro-li's non-parametric memory can potentially be implemented
on analog in-memory computing hardware, exhibiting O(1) search time while
causing noise in retrieving neighbors, with minimal (<1%) performance loss. Our
code is available at:
https://github.com/IBM/Retrieval-Enhanced-Transformer-Little.",2024-09-12,"Gentiana Rashiti, Geethan Karunaratne, Mrinmaya Sachan, Abu Sebastian, Abbas Rahimi",http://arxiv.org/pdf/2410.00004v2,cs.CL
Knowledge Tagging with Large Language Model based Multi-Agent System,"Knowledge tagging for questions is vital in modern intelligent educational
applications, including learning progress diagnosis, practice question
recommendations, and course content organization. Traditionally, these
annotations have been performed by pedagogical experts, as the task demands not
only a deep semantic understanding of question stems and knowledge definitions
but also a strong ability to link problem-solving logic with relevant knowledge
concepts. With the advent of advanced natural language processing (NLP)
algorithms, such as pre-trained language models and large language models
(LLMs), pioneering studies have explored automating the knowledge tagging
process using various machine learning models. In this paper, we investigate
the use of a multi-agent system to address the limitations of previous
algorithms, particularly in handling complex cases involving intricate
knowledge definitions and strict numerical constraints. By demonstrating its
superior performance on the publicly available math question knowledge tagging
dataset, MathKnowCT, we highlight the significant potential of an LLM-based
multi-agent system in overcoming the challenges that previous methods have
encountered. Finally, through an in-depth discussion of the implications of
automating knowledge tagging, we underscore the promising results of deploying
LLM-based algorithms in educational contexts.",2024-09-12,"Hang Li, Tianlong Xu, Ethan Chang, Qingsong Wen",http://arxiv.org/pdf/2409.08406v2,cs.CL
Self-Supervised Inference of Agents in Trustless Environments,"In this paper, we propose a novel approach where agents can form swarms to
produce high-quality responses effectively. This is accomplished by utilizing
agents capable of data inference and ranking, which can be effectively
implemented using LLMs as response classifiers. We assess existing approaches
for trustless agent inference, define our methodology, estimate practical
parameters, and model various types of malicious agent attacks. Our method
leverages the collective intelligence of swarms, ensuring robust and efficient
decentralized AI inference with better accuracy, security, and reliability. We
show that our approach is an order of magnitude faster than other trustless
inference strategies reaching less than 125 ms validation latency.",2024-09-12,"Vladyslav Larin, Ivan Nikitin, Alexander Firsov",http://arxiv.org/pdf/2409.08386v1,cs.CL
Rethinking Prompting Strategies for Multi-Label Recognition with Partial Annotations,"Vision-language models (VLMs) like CLIP have been adapted for Multi-Label
Recognition (MLR) with partial annotations by leveraging prompt-learning, where
positive and negative prompts are learned for each class to associate their
embeddings with class presence or absence in the shared vision-text feature
space. While this approach improves MLR performance by relying on VLM priors,
we hypothesize that learning negative prompts may be suboptimal, as the
datasets used to train VLMs lack image-caption pairs explicitly focusing on
class absence. To analyze the impact of positive and negative prompt learning
on MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is
learned with VLM guidance while the other is replaced by an embedding vector
learned directly in the shared feature space without relying on the text
encoder. Through empirical analysis, we observe that negative prompts degrade
MLR performance, and learning only positive prompts, combined with learned
negative embeddings (PositiveCoOp), outperforms dual prompt learning
approaches. Moreover, we quantify the performance benefits that prompt-learning
offers over a simple vision-features-only baseline, observing that the baseline
displays strong performance comparable to dual prompt learning approach
(DualCoOp), when the proportion of missing labels is low, while requiring half
the training compute and 16 times fewer parameters",2024-09-12,"Samyak Rawlekar, Shubhang Bhatnagar, Narendra Ahuja",http://arxiv.org/pdf/2409.08381v1,cs.CL
Towards Quantifying and Reducing Language Mismatch Effects in Cross-Lingual Speech Anti-Spoofing,"The effects of language mismatch impact speech anti-spoofing systems, while
investigations and quantification of these effects remain limited. Existing
anti-spoofing datasets are mainly in English, and the high cost of acquiring
multilingual datasets hinders training language-independent models. We initiate
this work by evaluating top-performing speech anti-spoofing systems that are
trained on English data but tested on other languages, observing notable
performance declines. We propose an innovative approach - Accent-based data
expansion via TTS (ACCENT), which introduces diverse linguistic knowledge to
monolingual-trained models, improving their cross-lingual capabilities. We
conduct experiments on a large-scale dataset consisting of over 3 million
samples, including 1.8 million training samples and nearly 1.2 million testing
samples across 12 languages. The language mismatch effects are preliminarily
quantified and remarkably reduced over 15% by applying the proposed ACCENT.
This easily implementable method shows promise for multilingual and
low-resource language scenarios.",2024-09-12,"Tianchi Liu, Ivan Kukanov, Zihan Pan, Qiongqiong Wang, Hardik B. Sailor, Kong Aik Lee",http://arxiv.org/pdf/2409.08346v1,cs.CL
Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue,"Studying and building datasets for dialogue tasks is both expensive and
time-consuming due to the need to recruit, train, and collect data from study
participants. In response, much recent work has sought to use large language
models (LLMs) to simulate both human-human and human-LLM interactions, as they
have been shown to generate convincingly human-like text in many settings.
However, to what extent do LLM-based simulations \textit{actually} reflect
human dialogues? In this work, we answer this question by generating a
large-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the
WildChat dataset and quantifying how well the LLM simulations align with their
human counterparts. Overall, we find relatively low alignment between
simulations and human interactions, demonstrating a systematic divergence along
the multiple textual properties, including style and content. Further, in
comparisons of English, Chinese, and Russian dialogues, we find that models
perform similarly. Our results suggest that LLMs generally perform better when
the human themself writes in a way that is more similar to the LLM's own style.",2024-09-12,"Jonathan Ivey, Shivani Kumar, Jiayu Liu, Hua Shen, Sushrita Rakshit, Rohan Raju, Haotian Zhang, Aparna Ananthasubramaniam, Junghwan Kim, Bowen Yi, Dustin Wright, Abraham Israeli, Anders Giovanni Møller, Lechen Zhang, David Jurgens",http://arxiv.org/pdf/2409.08330v2,cs.CL
The Design of Informative Take-Over Requests for Semi-Autonomous Cyber-Physical Systems: Combining Spoken Language and Visual Icons in a Drone-Controller Setting,"The question of how cyber-physical systems should interact with human
partners that can take over control or exert oversight is becoming more
pressing, as these systems are deployed for an ever larger range of tasks.
Drawing on the literatures on handing over control during semi-autonomous
driving and human-robot interaction, we propose a design of a take-over request
that combines an abstract pre-alert with an informative TOR: Relevant sensor
information is highlighted on the controller's display, while a spoken message
verbalizes the reason for the TOR. We conduct our study in the context of a
semi-autonomous drone control scenario as our testbed. The goal of our online
study is to assess in more detail what form a language-based TOR should take.
Specifically, we compare a full sentence condition to shorter fragments, and
test whether the visual highlighting should be done synchronously or
asynchronously with the speech. Participants showed a higher accuracy in
choosing the correct solution with our bi-modal TOR and felt that they were
better able to recognize the critical situation. Using only fragments in the
spoken message rather than full sentences did not lead to improved accuracy or
faster reactions. Also, synchronizing the visual highlighting with the spoken
message did not result in better accuracy and response times were even
increased in this condition.",2024-09-12,"Ashwini Gundappa, Emilia Ellsiepen, Lukas Schmitz, Frederik Wiehr, Vera Demberg",http://arxiv.org/pdf/2409.08253v2,cs.CL
Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources,"Large Language Models still struggle in challenging scenarios that leverage
structured data, complex reasoning, or tool usage. In this paper, we propose
Source2Synth: a new method that can be used for teaching LLMs new skills
without relying on costly human annotations. Source2Synth takes as input a
custom data source and produces synthetic data points with intermediate
reasoning steps grounded in real-world sources. Source2Synth improves the
dataset quality by discarding low-quality generations based on their
answerability. We demonstrate the generality of this approach by applying it to
two challenging domains: we test reasoning abilities in multi-hop question
answering (MHQA), and tool usage in tabular question answering (TQA). Our
method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on
HotPotQA compared to the fine-tuned baselines.",2024-09-12,"Alisia Lupidi, Carlos Gemmell, Nicola Cancedda, Jane Dwivedi-Yu, Jason Weston, Jakob Foerster, Roberta Raileanu, Maria Lomeli",http://arxiv.org/pdf/2409.08239v1,cs.CL
LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems,"The rapid evolution of cyber threats necessitates innovative solutions for
detecting and analyzing malicious activity. Honeypots, which are decoy systems
designed to lure and interact with attackers, have emerged as a critical
component in cybersecurity. In this paper, we present a novel approach to
creating realistic and interactive honeypot systems using Large Language Models
(LLMs). By fine-tuning a pre-trained open-source language model on a diverse
dataset of attacker-generated commands and responses, we developed a honeypot
capable of sophisticated engagement with attackers. Our methodology involved
several key steps: data collection and processing, prompt engineering, model
selection, and supervised fine-tuning to optimize the model's performance.
Evaluation through similarity metrics and live deployment demonstrated that our
approach effectively generates accurate and informative responses. The results
highlight the potential of LLMs to revolutionize honeypot technology, providing
cybersecurity professionals with a powerful tool to detect and analyze
malicious activity, thereby enhancing overall security infrastructure.",2024-09-12,"Hakan T. Otal, M. Abdullah Canbaz",http://arxiv.org/pdf/2409.08234v2,cs.CL
TheraGen: Therapy for Every Generation,"We present TheraGen, an advanced AI-powered mental health chatbot utilizing
the LLaMA 2 7B model. This approach builds upon recent advancements in language
models and transformer architectures. TheraGen provides all-day personalized,
compassionate mental health care by leveraging a large dataset of 1 million
conversational entries, combining anonymized therapy transcripts, online mental
health discussions, and psychological literature, including APA resources. Our
implementation employs transfer learning, fine-tuning, and advanced training
techniques to optimize performance. TheraGen offers a user-friendly interface
for seamless interaction, providing empathetic responses and evidence-based
coping strategies. Evaluation results demonstrate high user satisfaction rates,
with 94% of users reporting improved mental well-being. The system achieved a
BLEU score of 0.67 and a ROUGE score of 0.62, indicating strong response
accuracy. With an average response time of 1395 milliseconds, TheraGen ensures
real-time, efficient support. While not a replacement for professional therapy,
TheraGen serves as a valuable complementary tool, significantly improving user
well-being and addressing the accessibility gap in mental health treatments.
This paper details TheraGen's architecture, training methodology, ethical
considerations, and future directions, contributing to the growing field of
AI-assisted mental healthcare and offering a scalable solution to the pressing
need for mental health support.",2024-09-12,"Kartikey Doshi, Jimit Shah, Narendra Shekokar",http://arxiv.org/pdf/2409.13748v1,cs.CL
What Makes a Maze Look Like a Maze?,"A unique aspect of human visual understanding is the ability to flexibly
interpret abstract concepts: acquiring lifted rules explaining what they
symbolize, grounding them across familiar and unfamiliar contexts, and making
predictions or reasoning about them. While off-the-shelf vision-language models
excel at making literal interpretations of images (e.g., recognizing object
categories such as tree branches), they still struggle to make sense of such
visual abstractions (e.g., how an arrangement of tree branches may form the
walls of a maze). To address this challenge, we introduce Deep Schema Grounding
(DSG), a framework that leverages explicit structured representations of visual
abstractions for grounding and reasoning. At the core of DSG are
schemas--dependency graph descriptions of abstract concepts that decompose them
into more primitive-level symbols. DSG uses large language models to extract
schemas, then hierarchically grounds concrete to abstract components of the
schema onto images with vision-language models. The grounded schema is used to
augment visual abstraction understanding. We systematically evaluate DSG and
different methods in reasoning on our new Visual Abstractions Dataset, which
consists of diverse, real-world images of abstract concepts and corresponding
question-answer pairs labeled by humans. We show that DSG significantly
improves the abstract visual reasoning performance of vision-language models,
and is a step toward human-aligned understanding of visual abstractions.",2024-09-12,"Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Noah D. Goodman, Jiajun Wu",http://arxiv.org/pdf/2409.08202v2,cs.CL
AudioBERT: Audio Knowledge Augmented Language Model,"Recent studies have identified that language models, pretrained on text-only
datasets, often lack elementary visual knowledge, \textit{e.g.,} colors of
everyday objects. Motivated by this observation, we ask whether a similar
shortcoming exists in terms of the \textit{auditory} knowledge. To answer this
question, we construct a new dataset called AuditoryBench, which consists of
two novel tasks for evaluating auditory knowledge. Based on our analysis using
the benchmark, we find that language models also suffer from a severe lack of
auditory knowledge. To address this limitation, we propose AudioBERT, a novel
method to augment the auditory knowledge of BERT through a retrieval-based
approach. First, we detect auditory knowledge spans in prompts to query our
retrieval model efficiently. Then, we inject audio knowledge into BERT and
switch on low-rank adaptation for effective adaptation when audio knowledge is
required. Our experiments demonstrate that AudioBERT is quite effective,
achieving superior performance on the AuditoryBench. The dataset and code are
available at \bulurl{https://github.com/HJ-Ok/AudioBERT}.",2024-09-12,"Hyunjong Ok, Suho Yoo, Jaeho Lee",http://arxiv.org/pdf/2409.08199v2,cs.CL
Fine-tuning Large Language Models for Entity Matching,"Generative large language models (LLMs) are a promising alternative to
pre-trained language models for entity matching due to their high zero-shot
performance and ability to generalize to unseen entities. Existing research on
using LLMs for entity matching has focused on prompt engineering and in-context
learning. This paper explores the potential of fine-tuning LLMs for entity
matching. We analyze fine-tuning along two dimensions: 1) the representation of
training examples, where we experiment with adding different types of
LLM-generated explanations to the training set, and 2) the selection and
generation of training examples using LLMs. In addition to the matching
performance on the source dataset, we investigate how fine-tuning affects the
models ability to generalize to other in-domain datasets as well as across
topical domains. Our experiments show that fine-tuning significantly improves
the performance of the smaller models while the results for the larger models
are mixed. Fine-tuning also improves the generalization to in-domain datasets
while hurting cross-domain transfer. We show that adding structured
explanations to the training set has a positive impact on the performance of
three out of four LLMs, while the proposed example selection and generation
methods, only improve the performance of Llama 3.1 8B while decreasing the
performance of GPT-4o-mini.",2024-09-12,"Aaron Steiner, Ralph Peeters, Christian Bizer",http://arxiv.org/pdf/2409.08185v2,cs.CL
On the Role of Context in Reading Time Prediction,"We present a new perspective on how readers integrate context during
real-time language comprehension. Our proposals build on surprisal theory,
which posits that the processing effort of a linguistic unit (e.g., a word) is
an affine function of its in-context information content. We first observe that
surprisal is only one out of many potential ways that a contextual predictor
can be derived from a language model. Another one is the pointwise mutual
information (PMI) between a unit and its context, which turns out to yield the
same predictive power as surprisal when controlling for unigram frequency.
Moreover, both PMI and surprisal are correlated with frequency. This means that
neither PMI nor surprisal contains information about context alone. In response
to this, we propose a technique where we project surprisal onto the orthogonal
complement of frequency, yielding a new contextual predictor that is
uncorrelated with frequency. Our experiments show that the proportion of
variance in reading times explained by context is a lot smaller when context is
represented by the orthogonalized predictor. From an interpretability
standpoint, this indicates that previous studies may have overstated the role
that context has in predicting reading times.",2024-09-12,"Andreas Opedal, Eleanor Chodroff, Ryan Cotterell, Ethan Gotlieb Wilcox",http://arxiv.org/pdf/2409.08160v3,cs.CL
LLM-POTUS Score: A Framework of Analyzing Presidential Debates with Large Language Models,"Large language models have demonstrated remarkable capabilities in natural
language processing, yet their application to political discourse analysis
remains underexplored. This paper introduces a novel approach to evaluating
presidential debate performances using LLMs, addressing the longstanding
challenge of objectively assessing debate outcomes. We propose a framework that
analyzes candidates' ""Policies, Persona, and Perspective"" (3P) and how they
resonate with the ""Interests, Ideologies, and Identity"" (3I) of four key
audience groups: voters, businesses, donors, and politicians. Our method
employs large language models to generate the LLM-POTUS Score, a quantitative
measure of debate performance based on the alignment between 3P and 3I. We
apply this framework to analyze transcripts from recent U.S. presidential
debates, demonstrating its ability to provide nuanced, multi-dimensional
assessments of candidate performances. Our results reveal insights into the
effectiveness of different debating strategies and their impact on various
audience segments. This study not only offers a new tool for political analysis
but also explores the potential and limitations of using LLMs as impartial
judges in complex social contexts. In addition, this framework provides
individual citizens with an independent tool to evaluate presidential debate
performances, which enhances democratic engagement and reduces reliance on
potentially biased media interpretations and institutional influence, thereby
strengthening the foundation of informed civic participation.",2024-09-12,"Zhengliang Liu, Yiwei Li, Oleksandra Zolotarevych, Rongwei Yang, Tianming Liu",http://arxiv.org/pdf/2409.08147v1,cs.CL
WhisperNER: Unified Open Named Entity and Speech Recognition,"Integrating named entity recognition (NER) with automatic speech recognition
(ASR) can significantly enhance transcription accuracy and informativeness. In
this paper, we introduce WhisperNER, a novel model that allows joint speech
transcription and entity recognition. WhisperNER supports open-type NER,
enabling recognition of diverse and evolving entities at inference. Building on
recent advancements in open NER research, we augment a large synthetic dataset
with synthetic speech samples. This allows us to train WhisperNER on a large
number of examples with diverse NER tags. During training, the model is
prompted with NER labels and optimized to output the transcribed utterance
along with the corresponding tagged entities. To evaluate WhisperNER, we
generate synthetic speech for commonly used NER benchmarks and annotate
existing ASR datasets with open NER tags. Our experiments demonstrate that
WhisperNER outperforms natural baselines on both out-of-domain open type NER
and supervised finetuning.",2024-09-12,"Gil Ayache, Menachem Pirchi, Aviv Navon, Aviv Shamsian, Gill Hetz, Joseph Keshet",http://arxiv.org/pdf/2409.08107v1,cs.CL
The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language,"We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark
corpus designed to push the limits of current approaches to low-resource speech
recognition. Faetar, a Franco-Proven\c{c}al variety spoken primarily in Italy,
has no standard orthography, has virtually no existing textual or speech
resources other than what is included in the benchmark, and is quite different
from other forms of Franco-Proven\c{c}al. The corpus comes from field
recordings, most of which are noisy, for which only 5 hrs have matching
transcriptions, and for which forced alignment is of variable quality. The
corpus contains an additional 20 hrs of unlabelled speech. We report baseline
results from state-of-the-art multilingual speech foundation models with a best
phone error rate of 30.4%, using a pipeline that continues pre-training on the
foundation model using the unlabelled set.",2024-09-12,"Michael Ong, Sean Robertson, Leo Peckham, Alba Jorquera Jimenez de Aberasturi, Paula Arkhangorodsky, Robin Huo, Aman Sakhardande, Mark Hallap, Naomi Nagy, Ewan Dunbar",http://arxiv.org/pdf/2409.08103v4,cs.CL
The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal,"This paper explores the intersection of technological innovation and access
to justice by developing a benchmark for predicting case outcomes in the UK
Employment Tribunal (UKET). To address the challenge of extensive manual
annotation, the study employs a large language model (LLM) for automatic
annotation, resulting in the creation of the CLC-UKET dataset. The dataset
consists of approximately 19,000 UKET cases and their metadata. Comprehensive
legal annotations cover facts, claims, precedent references, statutory
references, case outcomes, reasons and jurisdiction codes. Facilitated by the
CLC-UKET data, we examine a multi-class case outcome prediction task in the
UKET. Human predictions are collected to establish a performance reference for
model comparison. Empirical results from baseline models indicate that
finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET
prediction task. The performance of zero-shot LLMs can be enhanced by
integrating task-related information into few-shot examples. We hope that the
CLC-UKET dataset, along with human annotations and empirical findings, can
serve as a valuable benchmark for employment-related dispute resolution.",2024-09-12,"Huiyuan Xie, Felix Steffek, Joana Ribeiro de Faria, Christine Carter, Jonathan Rutherford",http://arxiv.org/pdf/2409.08098v3,cs.CL
TravelAgent: An AI Assistant for Personalized Travel Planning,"As global tourism expands and artificial intelligence technology advances,
intelligent travel planning services have emerged as a significant research
focus. Within dynamic real-world travel scenarios with multi-dimensional
constraints, services that support users in automatically creating practical
and customized travel itineraries must address three key objectives:
Rationality, Comprehensiveness, and Personalization. However, existing systems
with rule-based combinations or LLM-based planning methods struggle to fully
satisfy these criteria. To overcome the challenges, we introduce TravelAgent, a
travel planning system powered by large language models (LLMs) designed to
provide reasonable, comprehensive, and personalized travel itineraries grounded
in dynamic scenarios. TravelAgent comprises four modules: Tool-usage,
Recommendation, Planning, and Memory Module. We evaluate TravelAgent's
performance with human and simulated users, demonstrating its overall
effectiveness in three criteria and confirming the accuracy of personalized
recommendations.",2024-09-12,"Aili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, Jiangjie Chen",http://arxiv.org/pdf/2409.08069v1,cs.CL
Enhanced Online Grooming Detection Employing Context Determination and Message-Level Analysis,"Online Grooming (OG) is a prevalent threat facing predominately children
online, with groomers using deceptive methods to prey on the vulnerability of
children on social media/messaging platforms. These attacks can have severe
psychological and physical impacts, including a tendency towards
revictimization. Current technical measures are inadequate, especially with the
advent of end-to-end encryption which hampers message monitoring. Existing
solutions focus on the signature analysis of child abuse media, which does not
effectively address real-time OG detection. This paper proposes that OG attacks
are complex, requiring the identification of specific communication patterns
between adults and children. It introduces a novel approach leveraging advanced
models such as BERT and RoBERTa for Message-Level Analysis and a Context
Determination approach for classifying actor interactions, including the
introduction of Actor Significance Thresholds and Message Significance
Thresholds. The proposed method aims to enhance accuracy and robustness in
detecting OG by considering the dynamic and multi-faceted nature of these
attacks. Cross-dataset experiments evaluate the robustness and versatility of
our approach. This paper's contributions include improved detection
methodologies and the potential for application in various scenarios,
addressing gaps in current literature and practices.",2024-09-12,"Jake Street, Isibor Ihianle, Funminiyi Olajide, Ahmad Lotfi",http://arxiv.org/pdf/2409.07958v1,cs.CL
A corpus-based investigation of pitch contours of monosyllabic words in conversational Taiwan Mandarin,"In Mandarin, the tonal contours of monosyllabic words produced in isolation
or in careful speech are characterized by four lexical tones: a high-level tone
(T1), a rising tone (T2), a dipping tone (T3) and a falling tone (T4). However,
in spontaneous speech, the actual tonal realization of monosyllabic words can
deviate significantly from these canonical tones due to intra-syllabic
co-articulation and inter-syllabic co-articulation with adjacent tones. In
addition, Chuang et al. (2024) recently reported that the tonal contours of
disyllabic Mandarin words with T2-T4 tone pattern are co-determined by their
meanings. Following up on their research, we present a corpus-based
investigation of how the pitch contours of monosyllabic words are realized in
spontaneous conversational Mandarin, focusing on the effects of contextual
predictors on the one hand, and the way in words' meanings co-determine pitch
contours on the other hand. We analyze the F0 contours of 3824 tokens of 63
different word types in a spontaneous Taiwan Mandarin corpus, using the
generalized additive (mixed) model to decompose a given observed pitch contour
into a set of component pitch contours. We show that the tonal context
substantially modify a word's canonical tone. Once the effect of tonal context
is controlled for, T2 and T3 emerge as low flat tones, contrasting with T1 as a
high tone, and with T4 as a high-to-mid falling tone. The neutral tone (T0),
which in standard descriptions, is realized based on the preceding tone,
emerges as a low tone in its own right, modified by the other predictors in the
same way as the standard tones T1, T2, T3, and T4. We also show that word, and
even more so, word sense, co-determine words' F0 contours. Analyses of variable
importance using random forests further supported the substantial effect of
tonal context and an effect of word sense.",2024-09-12,"Xiaoyun Jin, Mirjam Ernestus, R. Harald Baayen",http://arxiv.org/pdf/2409.07891v3,cs.CL
Learning Rules from KGs Guided by Language Models,"Advances in information extraction have enabled the automatic construction of
large knowledge graphs (e.g., Yago, Wikidata or Google KG), which are widely
used in many applications like semantic search or data analytics. However, due
to their semi-automatic construction, KGs are often incomplete. Rule learning
methods, concerned with the extraction of frequent patterns from KGs and
casting them into rules, can be applied to predict potentially missing facts. A
crucial step in this process is rule ranking. Ranking of rules is especially
challenging over highly incomplete or biased KGs (e.g., KGs predominantly
storing facts about famous people), as in this case biased rules might fit the
data best and be ranked at the top based on standard statistical metrics like
rule confidence. To address this issue, prior works proposed to rank rules not
only relying on the original KG but also facts predicted by a KG embedding
model. At the same time, with the recent rise of Language Models (LMs), several
works have claimed that LMs can be used as alternative means for KG completion.
In this work, our goal is to verify to which extent the exploitation of LMs is
helpful for improving the quality of rule learning systems.",2024-09-12,"Zihang Peng, Daria Stepanova, Vinh Thinh Ho, Heike Adel, Alessandra Russo, Simon Ott",http://arxiv.org/pdf/2409.07869v1,cs.CL
FPMT: Enhanced Semi-Supervised Model for Traffic Incident Detection,"For traffic incident detection, the acquisition of data and labels is notably
resource-intensive, rendering semi-supervised traffic incident detection both a
formidable and consequential challenge. Thus, this paper focuses on traffic
incident detection with a semi-supervised learning way. It proposes a
semi-supervised learning model named FPMT within the framework of MixText. The
data augmentation module introduces Generative Adversarial Networks to balance
and expand the dataset. During the mix-up process in the hidden space, it
employs a probabilistic pseudo-mixing mechanism to enhance regularization and
elevate model precision. In terms of training strategy, it initiates with
unsupervised training on all data, followed by supervised fine-tuning on a
subset of labeled data, and ultimately completing the goal of semi-supervised
training. Through empirical validation on four authentic datasets, our FPMT
model exhibits outstanding performance across various metrics. Particularly
noteworthy is its robust performance even in scenarios with low label rates.",2024-09-12,"Xinying Lu, Jianli Xiao",http://arxiv.org/pdf/2409.07839v1,cs.CL
Online vs Offline: A Comparative Study of First-Party and Third-Party Evaluations of Social Chatbots,"This paper explores the efficacy of online versus offline evaluation methods
in assessing conversational chatbots, specifically comparing first-party direct
interactions with third-party observational assessments. By extending a
benchmarking dataset of user dialogs with empathetic chatbots with offline
third-party evaluations, we present a systematic comparison between the
feedback from online interactions and the more detached offline third-party
evaluations. Our results reveal that offline human evaluations fail to capture
the subtleties of human-chatbot interactions as effectively as online
assessments. In comparison, automated third-party evaluations using a GPT-4
model offer a better approximation of first-party human judgments given
detailed instructions. This study highlights the limitations of third-party
evaluations in grasping the complexities of user experiences and advocates for
the integration of direct interaction feedback in conversational AI evaluation
to enhance system development and user satisfaction.",2024-09-12,"Ekaterina Svikhnushina, Pearl Pu",http://arxiv.org/pdf/2409.07823v1,cs.CL
Controllable Synthetic Clinical Note Generation with Privacy Guarantees,"In the field of machine learning, domain-specific annotated data is an
invaluable resource for training effective models. However, in the medical
domain, this data often includes Personal Health Information (PHI), raising
significant privacy concerns. The stringent regulations surrounding PHI limit
the availability and sharing of medical datasets, which poses a substantial
challenge for researchers and practitioners aiming to develop advanced machine
learning models. In this paper, we introduce a novel method to ""clone"" datasets
containing PHI. Our approach ensures that the cloned datasets retain the
essential characteristics and utility of the original data without compromising
patient privacy. By leveraging differential-privacy techniques and a novel
fine-tuning task, our method produces datasets that are free from identifiable
information while preserving the statistical properties necessary for model
training. We conduct utility testing to evaluate the performance of machine
learning models trained on the cloned datasets. The results demonstrate that
our cloned datasets not only uphold privacy standards but also enhance model
performance compared to those trained on traditional anonymized datasets. This
work offers a viable solution for the ethical and effective utilization of
sensitive medical data in machine learning, facilitating progress in medical
research and the development of robust predictive models.",2024-09-12,"Tal Baumel, Andre Manoel, Daniel Jones, Shize Su, Huseyin Inan, Aaron, Bornstein, Robert Sim",http://arxiv.org/pdf/2409.07809v1,cs.CL
Full-text Error Correction for Chinese Speech Recognition with Large Language Model,"Large Language Models (LLMs) have demonstrated substantial potential for
error correction in Automatic Speech Recognition (ASR). However, most research
focuses on utterances from short-duration speech recordings, which are the
predominant form of speech data for supervised ASR training. This paper
investigates the effectiveness of LLMs for error correction in full-text
generated by ASR systems from longer speech recordings, such as transcripts
from podcasts, news broadcasts, and meetings. First, we develop a Chinese
dataset for full-text error correction, named ChFT, utilizing a pipeline that
involves text-to-speech synthesis, ASR, and error-correction pair extractor.
This dataset enables us to correct errors across contexts, including both
full-text and segment, and to address a broader range of error types, such as
punctuation restoration and inverse text normalization, thus making the
correction process comprehensive. Second, we fine-tune a pre-trained LLM on the
constructed dataset using a diverse set of prompts and target formats, and
evaluate its performance on full-text error correction. Specifically, we design
prompts based on full-text and segment, considering various output formats,
such as directly corrected text and JSON-based error-correction pairs. Through
various test settings, including homogeneous, up-to-date, and hard test sets,
we find that the fine-tuned LLMs perform well in the full-text setting with
different prompts, each presenting its own strengths and weaknesses. This
establishes a promising baseline for further research. The dataset is available
on the website.",2024-09-12,"Zhiyuan Tang, Dong Wang, Shen Huang, Shidong Shang",http://arxiv.org/pdf/2409.07790v2,cs.CL
Stable Language Model Pre-training by Reducing Embedding Variability,"Stable pre-training is essential for achieving better-performing language
models. However, tracking pre-training stability by calculating gradient
variance at every step is impractical due to the significant computational
costs. We explore Token Embedding Variability (TEV) as a simple and efficient
proxy for assessing pre-training stability in language models with pre-layer
normalization, given that shallower layers are more prone to gradient explosion
(section 2.2). Moreover, we propose Multi-head Low-Rank Attention (MLRA) as an
architecture to alleviate such instability by limiting the exponential growth
of output embedding variance, thereby preventing the gradient explosion
(section 3.2). Empirical results on GPT-2 with MLRA demonstrate increased
stability and lower perplexity, particularly in deeper models.",2024-09-12,"Woojin Chung, Jiwoo Hong, Na Min An, James Thorne, Se-Young Yun",http://arxiv.org/pdf/2409.07787v1,cs.CL
Supporting Online Discussions: Integrating AI Into the adhocracy+ Participation Platform To Enhance Deliberation,"Online spaces allow people to discuss important issues and make joint
decisions, regardless of their location or time zone. However, without proper
support and thoughtful design, these discussions often lack structure and
politeness during the exchanges of opinions. Artificial intelligence (AI)
represents an opportunity to support both participants and organizers of
large-scale online participation processes. In this paper, we present an
extension of adhocracy+, a large-scale open source participation platform, that
provides two additional debate modules that are supported by AI to enhance the
discussion quality and participant interaction.",2024-09-12,"Maike Behrendt, Stefan Sylvius Wagner, Stefan Harmeling",http://arxiv.org/pdf/2409.07780v1,cs.CL
Top-down Activity Representation Learning for Video Question Answering,"Capturing complex hierarchical human activities, from atomic actions (e.g.,
picking up one present, moving to the sofa, unwrapping the present) to
contextual events (e.g., celebrating Christmas) is crucial for achieving
high-performance video question answering (VideoQA). Recent works have expanded
multimodal models (e.g., CLIP, LLaVA) to process continuous video sequences,
enhancing the model's temporal reasoning capabilities. However, these
approaches often fail to capture contextual events that can be decomposed into
multiple atomic actions non-continuously distributed over relatively long-term
sequences. In this paper, to leverage the spatial visual context representation
capability of the CLIP model for obtaining non-continuous visual
representations in terms of contextual events in videos, we convert long-term
video sequences into a spatial image domain and finetune the multimodal model
LLaVA for the VideoQA task. Our approach achieves competitive performance on
the STAR task, in particular, with a 78.4% accuracy score, exceeding the
current state-of-the-art score by 2.8 points on the NExTQA task.",2024-09-12,"Yanan Wang, Shuichiro Haruta, Donghuo Zeng, Julio Vizcarra, Mori Kurokawa",http://arxiv.org/pdf/2409.07748v1,cs.CL
Multi-object event graph representation learning for Video Question Answering,"Video question answering (VideoQA) is a task to predict the correct answer to
questions posed about a given video. The system must comprehend spatial and
temporal relationships among objects extracted from videos to perform causal
and temporal reasoning. While prior works have focused on modeling individual
object movements using transformer-based methods, they falter when capturing
complex scenarios involving multiple objects (e.g., ""a boy is throwing a ball
in a hoop""). We propose a contrastive language event graph representation
learning method called CLanG to address this limitation. Aiming to capture
event representations associated with multiple objects, our method employs a
multi-layer GNN-cluster module for adversarial graph representation learning,
enabling contrastive learning between the question text and its relevant
multi-object event graph. Our method outperforms a strong baseline, achieving
up to 2.2% higher accuracy on two challenging VideoQA datasets, NExT-QA and
TGIF-QA-R. In particular, it is 2.8% better than baselines in handling causal
and temporal questions, highlighting its strength in reasoning multiple
object-based events.",2024-09-12,"Yanan Wang, Shuichiro Haruta, Donghuo Zeng, Julio Vizcarra, Mori Kurokawa",http://arxiv.org/pdf/2409.07747v1,cs.CL
Ruri: Japanese General Text Embeddings,"We report the development of Ruri, a series of Japanese general text
embedding models. While the development of general-purpose text embedding
models in English and multilingual contexts has been active in recent years,
model development in Japanese remains insufficient. The primary reasons for
this are the lack of datasets and the absence of necessary expertise. In this
report, we provide a detailed account of the development process of Ruri.
Specifically, we discuss the training of embedding models using synthesized
datasets generated by LLMs, the construction of the reranker for dataset
filtering and knowledge distillation, and the performance evaluation of the
resulting general-purpose text embedding models.",2024-09-12,"Hayato Tsukagoshi, Ryohei Sasano",http://arxiv.org/pdf/2409.07737v1,cs.CL
Large Language Models are Pattern Matchers: Editing Semi-Structured and Structured Documents with ChatGPT,"Large Language Models (LLMs) offer numerous applications, the full extent of
which is not yet understood. This paper investigates if LLMs can be applied for
editing structured and semi-structured documents with minimal effort. Using a
qualitative research approach, we conduct two case studies with ChatGPT and
thoroughly analyze the results. Our experiments indicate that LLMs can
effectively edit structured and semi-structured documents when provided with
basic, straightforward prompts. ChatGPT demonstrates a strong ability to
recognize and process the structure of annotated documents. This suggests that
explicitly structuring tasks and data in prompts might enhance an LLM's ability
to understand and solve tasks. Furthermore, the experiments also reveal
impressive pattern matching skills in ChatGPT. This observation deserves
further investigation, as it may contribute to understanding the processes
leading to hallucinations in LLMs.",2024-09-12,Irene Weber,http://arxiv.org/pdf/2409.07732v1,cs.CL
On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains,"Retrieval-Augmented Generation (RAG) has been empirically shown to enhance
the performance of large language models (LLMs) in knowledge-intensive domains
such as healthcare, finance, and legal contexts. Given a query, RAG retrieves
relevant documents from a corpus and integrates them into the LLMs' generation
process. In this study, we investigate the adversarial robustness of RAG,
focusing specifically on examining the retrieval system. First, across 225
different setup combinations of corpus, retriever, query, and targeted
information, we show that retrieval systems are vulnerable to universal
poisoning attacks in medical Q\&A. In such attacks, adversaries generate
poisoned documents containing a broad spectrum of targeted information, such as
personally identifiable information. When these poisoned documents are inserted
into a corpus, they can be accurately retrieved by any users, as long as
attacker-specified queries are used. To understand this vulnerability, we
discovered that the deviation from the query's embedding to that of the
poisoned document tends to follow a pattern in which the high similarity
between the poisoned document and the query is retained, thereby enabling
precise retrieval. Based on these findings, we develop a new detection-based
defense to ensure the safe use of RAG. Through extensive experiments spanning
various Q\&A domains, we observed that our proposed method consistently
achieves excellent detection rates in nearly all cases.",2024-09-12,"Xun Xian, Ganghua Wang, Xuan Bi, Jayanth Srinivasa, Ashish Kundu, Charles Fleming, Mingyi Hong, Jie Ding",http://arxiv.org/pdf/2409.17275v1,cs.CL
Experimenting with Legal AI Solutions: The Case of Question-Answering for Access to Justice,"Generative AI models, such as the GPT and Llama series, have significant
potential to assist laypeople in answering legal questions. However, little
prior work focuses on the data sourcing, inference, and evaluation of these
models in the context of laypersons. To this end, we propose a human-centric
legal NLP pipeline, covering data sourcing, inference, and evaluation. We
introduce and release a dataset, LegalQA, with real and specific legal
questions spanning from employment law to criminal law, corresponding answers
written by legal experts, and citations for each answer. We develop an
automatic evaluation protocol for this dataset, then show that
retrieval-augmented generation from only 850 citations in the train set can
match or outperform internet-wide retrieval, despite containing 9 orders of
magnitude less data. Finally, we propose future directions for open-sourced
efforts, which fall behind closed-sourced models.",2024-09-12,"Jonathan Li, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu",http://arxiv.org/pdf/2409.07713v1,cs.CL
DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?,"Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have
demonstrated impressive language/vision reasoning abilities, igniting the
recent trend of building agents for targeted applications such as shopping
assistants or AI software engineers. Recently, many data science benchmarks
have been proposed to investigate their performance in the data science domain.
However, existing data science benchmarks still fall short when compared to
real-world data science applications due to their simplified settings. To
bridge this gap, we introduce DSBench, a comprehensive benchmark designed to
evaluate data science agents with realistic tasks. This benchmark includes 466
data analysis tasks and 74 data modeling tasks, sourced from Eloquence and
Kaggle competitions. DSBench offers a realistic setting by encompassing long
contexts, multimodal task backgrounds, reasoning with large data files and
multi-table structures, and performing end-to-end data modeling tasks. Our
evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle
with most tasks, with the best agent solving only 34.12% of data analysis tasks
and achieving a 34.74% Relative Performance Gap (RPG). These findings
underscore the need for further advancements in developing more practical,
intelligent, and autonomous data science agents.",2024-09-12,"Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu",http://arxiv.org/pdf/2409.07703v3,cs.CL
"Enhancing Q&A Text Retrieval with Ranking Models: Benchmarking, fine-tuning and deploying Rerankers for RAG","Ranking models play a crucial role in enhancing overall accuracy of text
retrieval systems. These multi-stage systems typically utilize either dense
embedding models or sparse lexical indices to retrieve relevant passages based
on a given query, followed by ranking models that refine the ordering of the
candidate passages by its relevance to the query.
  This paper benchmarks various publicly available ranking models and examines
their impact on ranking accuracy. We focus on text retrieval for
question-answering tasks, a common use case for Retrieval-Augmented Generation
systems. Our evaluation benchmarks include models some of which are
commercially viable for industrial applications.
  We introduce a state-of-the-art ranking model, NV-RerankQA-Mistral-4B-v3,
which achieves a significant accuracy increase of ~14% compared to pipelines
with other rerankers. We also provide an ablation study comparing the
fine-tuning of ranking models with different sizes, losses and self-attention
mechanisms.
  Finally, we discuss challenges of text retrieval pipelines with ranking
models in real-world industry applications, in particular the trade-offs among
model size, ranking accuracy and system requirements like indexing and serving
latency / throughput.",2024-09-12,"Gabriel de Souza P. Moreira, Ronay Ak, Benedikt Schifferer, Mengyao Xu, Radek Osmulski, Even Oldridge",http://arxiv.org/pdf/2409.07691v1,cs.CL
An Unsupervised Dialogue Topic Segmentation Model Based on Utterance Rewriting,"Dialogue topic segmentation plays a crucial role in various types of dialogue
modeling tasks. The state-of-the-art unsupervised DTS methods learn topic-aware
discourse representations from conversation data through adjacent discourse
matching and pseudo segmentation to further mine useful clues in unlabeled
conversational relations. However, in multi-round dialogs, discourses often
have co-references or omissions, leading to the fact that direct use of these
discourses for representation learning may negatively affect the semantic
similarity computation in the neighboring discourse matching task. In order to
fully utilize the useful cues in conversational relations, this study proposes
a novel unsupervised dialog topic segmentation method that combines the
Utterance Rewriting (UR) technique with an unsupervised learning algorithm to
efficiently utilize the useful cues in unlabeled dialogs by rewriting the
dialogs in order to recover the co-referents and omitted words. Compared with
existing unsupervised models, the proposed Discourse Rewriting Topic
Segmentation Model (UR-DTS) significantly improves the accuracy of topic
segmentation. The main finding is that the performance on DialSeg711 improves
by about 6% in terms of absolute error score and WD, achieving 11.42% in terms
of absolute error score and 12.97% in terms of WD. on Doc2Dial the absolute
error score and WD improves by about 3% and 2%, respectively, resulting in SOTA
reaching 35.17% in terms of absolute error score and 38.49% in terms of WD.
This shows that the model is very effective in capturing the nuances of
conversational topics, as well as the usefulness and challenges of utilizing
unlabeled conversations.",2024-09-12,"Xia Hou, Qifeng Li, Tongliang Li",http://arxiv.org/pdf/2409.07672v1,cs.CL
Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder,"This project, titled ""Machine Translation with Large Language Models:
Decoder-only vs. Encoder-Decoder,"" aims to develop a multilingual machine
translation (MT) model. Focused on Indian regional languages, especially
Telugu, Tamil, and Malayalam, the model seeks to enable accurate and
contextually appropriate translations across diverse language pairs. By
comparing Decoder-only and Encoder-Decoder architectures, the project aims to
optimize translation quality and efficiency, advancing cross-linguistic
communication tools.The primary objective is to develop a model capable of
delivering high-quality translations that are accurate and contextually
appropriate. By leveraging large language models, specifically comparing the
effectiveness of Decoder-only and Encoder-Decoder architectures, the project
seeks to optimize translation performance and efficiency across multilingual
contexts. Through rigorous experimentation and analysis, this project aims to
advance the field of machine translation, contributing valuable insights into
the effectiveness of different model architectures and paving the way for
enhanced cross-linguistic communication tools.",2024-09-12,"Abhinav P. M., SujayKumar Reddy M, Oswald Christopher",http://arxiv.org/pdf/2409.13747v1,cs.CL
SimulBench: Evaluating Language Models with Creative Simulation Tasks,"We introduce SimulBench, a benchmark designed to evaluate large language
models (LLMs) across a diverse collection of creative simulation scenarios,
such as acting as a Linux terminal or playing text games with users. While
these simulation tasks serve as effective measures of an LLM's general
intelligence, they are seldom incorporated into existing benchmarks. A major
challenge is to develop an evaluation framework for testing different LLMs
fairly while preserving the multi-round interactive nature of simulation tasks
between users and AI. To tackle this issue, we suggest using a fixed LLM as a
user agent to engage with an LLM to collect dialogues first under different
tasks. Then, challenging dialogue scripts are extracted for evaluating
different target LLMs. To facilitate automatic assessment on \DataName{}, GPT-4
is employed as the evaluator, tasked with reviewing the quality of the final
response generated by the target LLMs given multi-turn dialogue scripts. Our
comprehensive experiments indicate that these simulation tasks continue to pose
a significant challenge with their unique natures and show the gap between
proprietary models and the most advanced open LLMs. For example, GPT-4-turbo
outperforms LLaMA-3-70b-Chat on 18.55\% more cases.",2024-09-11,"Qi Jia, Xiang Yue, Tianyu Zheng, Jie Huang, Bill Yuchen Lin",http://arxiv.org/pdf/2409.07641v1,cs.CL
Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4 Capabilities,"In this paper we explore evaluation of LLM capabilities. We present
measurements of GPT-4 performance on several deterministic tasks; each task
involves a basic calculation and takes as input parameter some element drawn
from a large well-defined population (e.g., count elements in a list, multiply
two k-digit numbers, etc). We examine several conditions per-task and perform
enough trials so that statistically significant differences can be detected.
This allows us to investigate the sensitivity of task-accuracy both to query
phrasing and input parameter population. We find that seemingly trivial
modifications in the task-prompt or input population can yield differences far
larger than can be explained by sampling effects. For example, performance on a
simple list-counting task varies with query-phrasing and list-length, but also
with list composition (i.e., the thing-to-be-counted) and object frequency
(e.g., success when an element accounts for $\approx$ 50\% of a list is
different from when it accounts for $\approx$ 70\% etc).
  We conclude that efforts to quantify LLM capabilities easily succumb to the
language-as-fixed-effect fallacy, where experimental observations are
improperly generalized beyond what the data supports. A consequence appears to
be that intuitions that have been formed based on interactions with humans form
a very unreliable guide as to which input modifications should ``make no
difference'' to LLM performance.",2024-09-11,"Thomas Ball, Shuo Chen, Cormac Herley",http://arxiv.org/pdf/2409.07638v2,cs.CL
Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on Prediction Accuracy,"This study evaluates the ability of large language models (LLMs) to map
biomedical ontology terms to their corresponding ontology IDs across the Human
Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies.
Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate
for their prevalence in the biomedical literature, we examined the relationship
between ontology ID prevalence and mapping accuracy. Results indicate that
ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO
IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers.
Higher prevalence of ontology IDs in the biomedical literature correlated with
higher mapping accuracy. Predictive models based on receiver operating
characteristic (ROC) curves confirmed this relationship.
  In contrast, this pattern did not apply to mapping protein names to Human
Genome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline
performance (95%) in mapping protein names to HUGO gene symbols, with mapping
accuracy unaffected by prevalence. We propose that the high prevalence of HUGO
gene symbols in the literature has caused these symbols to become lexicalized,
enabling GPT-4 to map protein names to HUGO gene symbols with high accuracy.
These findings highlight the limitations of LLMs in mapping ontology terms to
low-prevalence ontology IDs and underscore the importance of incorporating
ontology ID prevalence into the training and evaluation of LLMs for biomedical
applications.",2024-09-11,"Thanh Son Do, Daniel B. Hier, Tayo Obafemi-Ajayi",http://arxiv.org/pdf/2409.13746v2,cs.CL
Leveraging User-Generated Reviews for Recommender Systems with Dynamic Headers,"E-commerce platforms have a vast catalog of items to cater to their
customers' shopping interests. Most of these platforms assist their customers
in the shopping process by offering optimized recommendation carousels,
designed to help customers quickly locate their desired items. Many models have
been proposed in academic literature to generate and enhance the ranking and
recall set of items in these carousels. Conventionally, the accompanying
carousel title text (header) of these carousels remains static. In most
instances, a generic text such as ""Items similar to your current viewing"" is
utilized. Fixed variations such as the inclusion of specific attributes ""Other
items from a similar seller"" or ""Items from a similar brand"" in addition to
""frequently bought together"" or ""considered together"" are observed as well.
This work proposes a novel approach to customize the header generation process
of these carousels. Our work leverages user-generated reviews that lay focus on
specific attributes (aspects) of an item that were favorably perceived by users
during their interaction with the given item. We extract these aspects from
reviews and train a graph neural network-based model under the framework of a
conditional ranking task. We refer to our innovative methodology as Dynamic
Text Snippets (DTS) which generates multiple header texts for an anchor item
and its recall set. Our approach demonstrates the potential of utilizing
user-generated reviews and presents a unique paradigm for exploring
increasingly context-aware recommendation systems.",2024-09-11,"Shanu Vashishtha, Abhay Kumar, Lalitesh Morishetti, Kaushiki Nag, Kannan Achan",http://arxiv.org/pdf/2409.07627v1,cs.CL
An Evaluation of GPT-4V for Transcribing the Urban Renewal Hand-Written Collection,"Between 1960 and 1980, urban renewal transformed many cities, creating vast
handwritten records. These documents posed a significant challenge for
researchers due to their volume and handwritten nature. The launch of GPT-4V in
November 2023 offered a breakthrough, enabling large-scale, efficient
transcription and analysis of these historical urban renewal documents.",2024-09-11,"Myeong Lee, Julia H. P. Hsu",http://arxiv.org/pdf/2409.09090v1,cs.CL
"MOSAIC: Multiple Observers Spotting AI Content, a Robust Approach to Machine-Generated Text Detection","The dissemination of Large Language Models (LLMs), trained at scale, and
endowed with powerful text-generating abilities has vastly increased the
threats posed by generative AI technologies by reducing the cost of producing
harmful, toxic, faked or forged content. In response, various proposals have
been made to automatically discriminate artificially generated from
human-written texts, typically framing the problem as a classification problem.
Most approaches evaluate an input document by a well-chosen detector LLM,
assuming that low-perplexity scores reliably signal machine-made content. As
using one single detector can induce brittleness of performance, we instead
consider several and derive a new, theoretically grounded approach to combine
their respective strengths. Our experiments, using a variety of generator LLMs,
suggest that our method effectively leads to robust detection performances. An
early version of the code is available at
https://github.com/BaggerOfWords/MOSAIC.",2024-09-11,"Matthieu Dubois, François Yvon, Pablo Piantanida",http://arxiv.org/pdf/2409.07615v2,cs.CL
Contextualization of ASR with LLM using phonetic retrieval-based augmentation,"Large language models (LLMs) have shown superb capability of modeling
multimodal signals including audio and text, allowing the model to generate
spoken or textual response given a speech input. However, it remains a
challenge for the model to recognize personal named entities, such as contacts
in a phone book, when the input modality is speech. In this work, we start with
a speech recognition task and propose a retrieval-based solution to
contextualize the LLM: we first let the LLM detect named entities in speech
without any context, then use this named entity as a query to retrieve
phonetically similar named entities from a personal database and feed them to
the LLM, and finally run context-aware LLM decoding. In a voice assistant task,
our solution achieved up to 30.2% relative word error rate reduction and 73.6%
relative named entity error rate reduction compared to a baseline system
without contextualization. Notably, our solution by design avoids prompting the
LLM with the full named entity database, making it highly efficient and
applicable to large named entity databases.",2024-09-11,"Zhihong Lei, Xingyu Na, Mingbin Xu, Ernest Pusateri, Christophe Van Gysel, Yuanyuan Zhang, Shiyi Han, Zhen Huang",http://arxiv.org/pdf/2409.15353v1,cs.CL
SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories,"Given that Large Language Models (LLMs) have made significant progress in
writing code, can they now be used to autonomously reproduce results from
research repositories? Such a capability would be a boon to the research
community, helping researchers validate, understand, and extend prior work. To
advance towards this goal, we introduce SUPER, the first benchmark designed to
evaluate the capability of LLMs in setting up and executing tasks from research
repositories. SUPERaims to capture the realistic challenges faced by
researchers working with Machine Learning (ML) and Natural Language Processing
(NLP) research repositories. Our benchmark comprises three distinct problem
sets: 45 end-to-end problems with annotated expert solutions, 152 sub problems
derived from the expert set that focus on specific challenges (e.g.,
configuring a trainer), and 602 automatically generated problems for
larger-scale development. We introduce various evaluation measures to assess
both task success and progress, utilizing gold solutions when available or
approximations otherwise. We show that state-of-the-art approaches struggle to
solve these problems with the best model (GPT-4o) solving only 16.3% of the
end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of
this task, and suggests that SUPER can serve as a valuable resource for the
community to make and measure progress.",2024-09-11,"Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, Tushar Khot",http://arxiv.org/pdf/2409.07440v1,cs.CL
Salmon: A Suite for Acoustic Language Model Evaluation,"Speech language models have recently demonstrated great potential as
universal speech processing systems. Such models have the ability to model the
rich acoustic information existing in audio signals, beyond spoken content,
such as emotion, background noise, etc. Despite this, evaluation benchmarks
which evaluate awareness to a wide range of acoustic aspects, are lacking. To
help bridge this gap, we introduce SALMon, a novel evaluation suite
encompassing background noise, emotion, speaker identity and room impulse
response. The proposed benchmarks both evaluate the consistency of the
inspected element and how much it matches the spoken text. We follow a
modelling based approach, measuring whether a model gives correct samples
higher scores than incorrect ones. This approach makes the benchmark fast to
compute even for large models. We evaluated several speech language models on
SALMon, thus highlighting the strengths and weaknesses of each evaluated
method. We make the code and data publicly available at
https://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .",2024-09-11,"Gallil Maimon, Amit Roth, Yossi Adi",http://arxiv.org/pdf/2409.07437v3,cs.CL
Synthetic continued pretraining,"Pretraining on large-scale, unstructured internet text enables language
models to acquire a significant amount of world knowledge. However, this
knowledge acquisition is data-inefficient--to learn a given fact, models must
be trained on hundreds to thousands of diverse representations of it. This
poses a challenge when adapting a pretrained model to a small corpus of
domain-specific documents, where each fact may appear rarely or only once. We
propose to bridge this gap with synthetic continued pretraining: using the
small domain-specific corpus to synthesize a large corpus more amenable to
learning, and then performing continued pretraining on the synthesized corpus.
We instantiate this proposal with EntiGraph, a synthetic data augmentation
algorithm that extracts salient entities from the source documents and then
generates diverse text by drawing connections between the sampled entities.
Synthetic continued pretraining with EntiGraph enables a language model to
answer questions and follow generic instructions related to the source
documents without access to them. If, instead, the source documents are
available at inference time, we show that the knowledge acquired through our
approach compounds with retrieval-augmented generation. To better understand
these results, we build a simple mathematical model of EntiGraph, and show how
synthetic data augmentation can ""rearrange"" knowledge to enable more
data-efficient learning.",2024-09-11,"Zitong Yang, Neil Band, Shuangping Li, Emmanuel Candès, Tatsunori Hashimoto",http://arxiv.org/pdf/2409.07431v2,cs.CL
Agent Workflow Memory,"Despite the potential of language model-based agents to solve real-world
tasks such as web navigation, current methods still struggle with long-horizon
tasks with complex action trajectories. In contrast, humans can flexibly solve
complex tasks by learning reusable task workflows from past experiences and
using them to guide future actions. To build agents that can similarly benefit
from this process, we introduce Agent Workflow Memory (AWM), a method for
inducing commonly reused routines, i.e., workflows, and selectively providing
workflows to the agent to guide subsequent generations. AWM flexibly applies to
both offline and online scenarios, where agents induce workflows from training
examples beforehand or from test queries on the fly. We experiment on two major
web navigation benchmarks -- Mind2Web and WebArena -- that collectively cover
1000+ tasks from 200+ domains across travel, shopping, and social media, among
others. AWM substantially improves the baseline results by 24.6% and 51.1%
relative success rate on Mind2Web and WebArena while reducing the number of
steps taken to solve WebArena tasks successfully. Furthermore, online AWM
robustly generalizes in cross-task, website, and domain evaluations, surpassing
baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps
widen.",2024-09-11,"Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, Graham Neubig",http://arxiv.org/pdf/2409.07429v1,cs.CL
Towards Fairer Health Recommendations: finding informative unbiased samples via Word Sense Disambiguation,"There have been growing concerns around high-stake applications that rely on
models trained with biased data, which consequently produce biased predictions,
often harming the most vulnerable. In particular, biased medical data could
cause health-related applications and recommender systems to create outputs
that jeopardize patient care and widen disparities in health outcomes. A recent
framework titled Fairness via AI posits that, instead of attempting to correct
model biases, researchers must focus on their root causes by using AI to debias
data. Inspired by this framework, we tackle bias detection in medical curricula
using NLP models, including LLMs, and evaluate them on a gold standard dataset
containing 4,105 excerpts annotated by medical experts for bias from a large
corpus. We build on previous work by coauthors which augments the set of
negative samples with non-annotated text containing social identifier terms.
However, some of these terms, especially those related to race and ethnicity,
can carry different meanings (e.g., ""white matter of spinal cord""). To address
this issue, we propose the use of Word Sense Disambiguation models to refine
dataset quality by removing irrelevant sentences. We then evaluate fine-tuned
variations of BERT models as well as GPT models with zero- and few-shot
prompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for
bias detection, while fine-tuned BERT models generally perform well across all
evaluated metrics.",2024-09-11,"Gavin Butts, Pegah Emdad, Jethro Lee, Shannon Song, Chiman Salavati, Willmar Sosa Diaz, Shiri Dori-Hacohen, Fabricio Murai",http://arxiv.org/pdf/2409.07424v1,cs.CL
Enhancing adversarial robustness in Natural Language Inference using explanations,"The surge of state-of-the-art Transformer-based models has undoubtedly pushed
the limits of NLP model performance, excelling in a variety of tasks. We cast
the spotlight on the underexplored task of Natural Language Inference (NLI),
since models trained on popular well-suited datasets are susceptible to
adversarial attacks, allowing subtle input interventions to mislead the model.
In this work, we validate the usage of natural language explanation as a
model-agnostic defence strategy through extensive experimentation: only by
fine-tuning a classifier on the explanation rather than premise-hypothesis
inputs, robustness under various adversarial attacks is achieved in comparison
to explanation-free baselines. Moreover, since there is no standard strategy of
testing the semantic validity of the generated explanations, we research the
correlation of widely used language generation metrics with human perception,
in order for them to serve as a proxy towards robust NLI models. Our approach
is resource-efficient and reproducible without significant computational
limitations.",2024-09-11,"Alexandros Koulakos, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou",http://arxiv.org/pdf/2409.07423v2,cs.CL
What to align in multimodal contrastive learning?,"Humans perceive the world through multisensory integration, blending the
information of different modalities to adapt their behavior. Contrastive
learning offers an appealing solution for multimodal self-supervised learning.
Indeed, by considering each modality as a different view of the same entity, it
learns to align features of different modalities in a shared representation
space. However, this approach is intrinsically limited as it only learns shared
or redundant information between modalities, while multimodal interactions can
arise in other ways. In this work, we introduce CoMM, a Contrastive MultiModal
learning strategy that enables the communication between modalities in a single
multimodal space. Instead of imposing cross- or intra- modality constraints, we
propose to align multimodal representations by maximizing the mutual
information between augmented versions of these multimodal features. Our
theoretical analysis shows that shared, synergistic and unique terms of
information naturally emerge from this formulation, allowing us to estimate
multimodal interactions beyond redundancy. We test CoMM both in a controlled
and in a series of real-world settings: in the former, we demonstrate that CoMM
effectively captures redundant, unique and synergistic information between
modalities. In the latter, CoMM learns complex multimodal interactions and
achieves state-of-the-art results on the seven multimodal benchmarks. Code is
available at https://github.com/Duplums/CoMM",2024-09-11,"Benoit Dufumier, Javiera Castillo-Navarro, Devis Tuia, Jean-Philippe Thiran",http://arxiv.org/pdf/2409.07402v2,cs.CL
AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge,"Knowledge conflict arises from discrepancies between information in the
context of a large language model (LLM) and the knowledge stored in its
parameters. This can hurt performance when using standard decoding techniques,
which tend to ignore the context. Existing test-time contrastive methods seek
to address this by comparing the LLM's output distribution with and without the
context and adjust the model according to the contrast between them. However,
we find that these methods frequently misjudge the degree of conflict and
struggle to handle instances that vary in their amount of conflict, with static
methods over-adjusting when conflict is absent. We propose a fine-grained,
instance-level approach called AdaCAD, which dynamically infers the weight of
adjustment based on the degree of conflict, as measured by the Jensen-Shannon
divergence between distributions representing contextual and parametric
knowledge. Across four LLMs, six question-answering (QA) and three
summarization datasets, we demonstrate that ADACAD consistently outperforms
other decoding baselines with average QA accuracy gains of 14.21% (absolute)
over a static contrastive baseline, and improves the factuality of summaries by
6.19 (AlignScore). Lastly, we show that while contrastive baselines hurt
performance when conflict is absent, ADACAD mitigates these losses, making it
more applicable to real-world datasets in which some examples have conflict and
others do not.",2024-09-11,"Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal",http://arxiv.org/pdf/2409.07394v2,cs.CL
Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective,"Multimodal affective computing (MAC) has garnered increasing attention due to
its broad applications in analyzing human behaviors and intentions, especially
in text-dominated multimodal affective computing field. This survey presents
the recent trends of multimodal affective computing from NLP perspective
through four hot tasks: multimodal sentiment analysis, multimodal emotion
recognition in conversation, multimodal aspect-based sentiment analysis and
multimodal multi-label emotion recognition. The goal of this survey is to
explore the current landscape of multimodal affective research, identify
development trends, and highlight the similarities and differences across
various tasks, offering a comprehensive report on the recent progress in
multimodal affective computing from an NLP perspective. This survey covers the
formalization of tasks, provides an overview of relevant works, describes
benchmark datasets, and details the evaluation metrics for each task.
Additionally, it briefly discusses research in multimodal affective computing
involving facial expressions, acoustic signals, physiological signals, and
emotion causes. Additionally, we discuss the technical approaches, challenges,
and future directions in multimodal affective computing. To support further
research, we released a repository that compiles related works in multimodal
affective computing, providing detailed resources and references for the
community.",2024-09-11,"Guimin Hu, Yi Xin, Weimin Lyu, Haojian Huang, Chang Sun, Zhihong Zhu, Lin Gui, Ruichu Cai, Erik Cambria, Hasti Seifi",http://arxiv.org/pdf/2409.07388v2,cs.CL
Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring System via Language Model Coordination,"The vast pre-existing slides serve as rich and important materials to carry
lecture knowledge. However, effectively leveraging lecture slides to serve
students is difficult due to the multi-modal nature of slide content and the
heterogeneous teaching actions. We study the problem of discovering effective
designs that convert a slide into an interactive lecture. We develop
Slide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring
system that can (1) effectively convert an input lecture slide into a
structured teaching agenda consisting of a set of heterogeneous teaching
actions; (2) create and manage an interactive lecture that generates responsive
interactions catering to student learning demands while regulating the
interactions to follow teaching actions. Slide2Lecture contains a complete
pipeline for learners to obtain an interactive classroom experience to learn
the slide. For teachers and developers, Slide2Lecture enables customization to
cater to personalized demands. The evaluation rated by annotators and students
shows that Slide2Lecture is effective in outperforming the remaining
implementation. Slide2Lecture's online deployment has made more than 200K
interaction with students in the 3K lecture sessions. We open source
Slide2Lecture's implementation in
https://anonymous.4open.science/r/slide2lecture-4210/.",2024-09-11,"Daniel Zhang-Li, Zheyuan Zhang, Jifan Yu, Joy Lim Jia Yin, Shangqing Tu, Linlu Gong, Haohua Wang, Zhiyuan Liu, Huiqin Liu, Lei Hou, Juanzi Li",http://arxiv.org/pdf/2409.07372v1,cs.CL
Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud Outcomes for Effective Text Evaluation,"This study introduces \textbf{InteractEval}, a framework that integrates
human expertise and Large Language Models (LLMs) using the Think-Aloud (TA)
method to generate attributes for checklist-based text evaluation. By combining
human flexibility and reasoning with LLM consistency, InteractEval outperforms
traditional non-LLM-based and LLM-based baselines across four distinct
dimensions, consisting of Coherence, Fluency, Consistency, and Relevance. The
experiment also investigates the effectiveness of the TA method, showing that
it promotes divergent thinking in both humans and LLMs, leading to the
generation of a wider range of relevant attributes and enhance text evaluation
performance. Comparative analysis reveals that humans excel at identifying
attributes related to internal quality (Coherence and Fluency), but LLMs
perform better at those attributes related to external alignment (Consistency
and Relevance). Consequently, leveraging both humans and LLMs together produces
the best evaluation outcomes. In other words, this study emphasizes the
necessity of effectively combining humans and LLMs in an automated
checklist-based text evaluation framework. The code is available at
\textbf{\url{https://github.com/BBeeChu/InteractEval.git}}.",2024-09-11,"SeongYeub Chu, JongWoo Kim, MunYong Yi",http://arxiv.org/pdf/2409.07355v2,cs.CL
"Explanation, Debate, Align: A Weak-to-Strong Framework for Language Model Generalization","The rapid advancement of artificial intelligence systems has brought the
challenge of AI alignment to the forefront of research, particularly in complex
decision-making and task execution. As these systems surpass human-level
performance in sophisticated problems, ensuring their alignment with human
values, intentions, and ethical guidelines becomes crucial. Building on
previous work in explanation generation for human-agent alignment, we address
the more complex dynamics of multi-agent systems and human-AI teams. This paper
introduces a novel approach to model alignment through weak-to-strong
generalization in the context of language models. We present a framework where
a strong model facilitates the improvement of a weaker model, bridging the gap
between explanation generation and model alignment. Our method, formalized as a
facilitation function, allows for the transfer of capabilities from advanced
models to less capable ones without direct access to extensive training data.
Our results suggest that this facilitation-based approach not only enhances
model performance but also provides insights into the nature of model alignment
and the potential for scalable oversight of AI systems.",2024-09-11,"Mehrdad Zakershahrak, Samira Ghodratnama",http://arxiv.org/pdf/2409.07335v1,cs.CL
MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical Applications,"The rapid development of Large Language Models (LLMs) for healthcare
applications has spurred calls for holistic evaluation beyond frequently-cited
benchmarks like USMLE, to better reflect real-world performance. While
real-world assessments are valuable indicators of utility, they often lag
behind the pace of LLM evolution, likely rendering findings obsolete upon
deployment. This temporal disconnect necessitates a comprehensive upfront
evaluation that can guide model selection for specific clinical applications.
We introduce MEDIC, a framework assessing LLMs across five critical dimensions
of clinical competence: medical reasoning, ethics and bias, data and language
understanding, in-context learning, and clinical safety. MEDIC features a novel
cross-examination framework quantifying LLM performance across areas like
coverage and hallucination detection, without requiring reference outputs. We
apply MEDIC to evaluate LLMs on medical question-answering, safety,
summarization, note generation, and other tasks. Our results show performance
disparities across model sizes, baseline vs medically finetuned models, and
have implications on model selection for applications requiring specific model
strengths, such as low hallucination or lower cost of inference. MEDIC's
multifaceted evaluation reveals these performance trade-offs, bridging the gap
between theoretical capabilities and practical implementation in healthcare
settings, ensuring that the most promising models are identified and adapted
for diverse healthcare applications.",2024-09-11,"Praveen K Kanithi, Clément Christophe, Marco AF Pimentel, Tathagata Raha, Nada Saadi, Hamza Javed, Svetlana Maslenkova, Nasir Hayat, Ronnie Rajan, Shadab Khan",http://arxiv.org/pdf/2409.07314v1,cs.CL
Using Generative Agents to Create Tip Sheets for Investigative Data Reporting,"This paper introduces a system using generative AI agents to create tip
sheets for investigative data reporting. Our system employs three specialized
agents--an analyst, a reporter, and an editor--to collaboratively generate and
refine tips from datasets. We validate this approach using real-world
investigative stories, demonstrating that our agent-based system generally
generates more newsworthy and accurate insights compared to a baseline model
without agents, although some variability was noted between different stories.
Our findings highlight the potential of generative AI to provide leads for
investigative data reporting.",2024-09-11,"Joris Veerbeek, Nicholas Diakopoulos",http://arxiv.org/pdf/2409.07286v1,cs.CL
Cross-Dialect Text-To-Speech in Pitch-Accent Language Incorporating Multi-Dialect Phoneme-Level BERT,"We explore cross-dialect text-to-speech (CD-TTS), a task to synthesize
learned speakers' voices in non-native dialects, especially in pitch-accent
languages. CD-TTS is important for developing voice agents that naturally
communicate with people across regions. We present a novel TTS model comprising
three sub-modules to perform competitively at this task. We first train a
backbone TTS model to synthesize dialect speech from a text conditioned on
phoneme-level accent latent variables (ALVs) extracted from speech by a
reference encoder. Then, we train an ALV predictor to predict ALVs tailored to
a target dialect from input text leveraging our novel multi-dialect
phoneme-level BERT. We conduct multi-dialect TTS experiments and evaluate the
effectiveness of our model by comparing it with a baseline derived from
conventional dialect TTS methods. The results show that our model improves the
dialectal naturalness of synthetic speech in CD-TTS.",2024-09-11,"Kazuki Yamauchi, Yuki Saito, Hiroshi Saruwatari",http://arxiv.org/pdf/2409.07265v1,cs.CL
Propaganda to Hate: A Multimodal Analysis of Arabic Memes with Multi-Agent LLMs,"In the past decade, social media platforms have been used for information
dissemination and consumption. While a major portion of the content is posted
to promote citizen journalism and public awareness, some content is posted to
mislead users. Among different content types such as text, images, and videos,
memes (text overlaid on images) are particularly prevalent and can serve as
powerful vehicles for propaganda, hate, and humor. In the current literature,
there have been efforts to individually detect such content in memes. However,
the study of their intersection is very limited. In this study, we explore the
intersection between propaganda and hate in memes using a multi-agent LLM-based
approach. We extend the propagandistic meme dataset with coarse and
fine-grained hate labels. Our finding suggests that there is an association
between propaganda and hate in memes. We provide detailed experimental results
that can serve as a baseline for future studies. We will make the experimental
resources publicly available to the community
(https://github.com/firojalam/propaganda-and-hateful-memes).",2024-09-11,"Firoj Alam, Md. Rafiul Biswas, Uzair Shah, Wajdi Zaghouani, Georgios Mikros",http://arxiv.org/pdf/2409.07246v2,cs.CL
Learning Efficient Recursive Numeral Systems via Reinforcement Learning,"It has previously been shown that by using reinforcement learning (RL),
agents can derive simple approximate and exact-restricted numeral systems that
are similar to human ones (Carlsson, 2021). However, it is a major challenge to
show how more complex recursive numeral systems, similar to for example
English, could arise via a simple learning mechanism such as RL. Here, we
introduce an approach towards deriving a mechanistic explanation of the
emergence of efficient recursive number systems. We consider pairs of agents
learning how to communicate about numerical quantities through a meta-grammar
that can be gradually modified throughout the interactions. Utilising a
slightly modified version of the meta-grammar of Hurford (1975), we demonstrate
that our RL agents, shaped by the pressures for efficient communication, can
effectively modify their lexicon towards Pareto-optimal configurations which
are comparable to those observed within human numeral systems in terms of their
efficiency.",2024-09-11,"Andrea Silvi, Jonathan Thomas, Emil Carlsson, Devdatt Dubhashi, Moa Johansson",http://arxiv.org/pdf/2409.07170v4,cs.CL
How Effectively Do LLMs Extract Feature-Sentiment Pairs from App Reviews?,"Automatic analysis of user reviews to understand user sentiments toward app
functionality (i.e. app features) helps align development efforts with user
expectations and needs. Recent advances in Large Language Models (LLMs) such as
ChatGPT have shown impressive performance on several new tasks without updating
the model's parameters i.e. using zero or a few labeled examples, but the
capabilities of LLMs are yet unexplored for feature-specific sentiment
analysis. The goal of our study is to explore the capabilities of LLMs to
perform feature-specific sentiment analysis of user reviews. This study
compares the performance of state-of-the-art LLMs, including GPT-4, ChatGPT,
and different variants of Llama-2 chat, against previous approaches for
extracting app features and associated sentiments in zero-shot, 1-shot, and
5-shot scenarios. The results indicate that GPT-4 outperforms the rule-based
SAFE by 17% in f1-score for extracting app features in the zero-shot scenario,
with 5-shot further improving it by 6%. However, the fine-tuned RE-BERT exceeds
GPT-4 by 6% in f1-score. For predicting positive and neutral sentiments, GPT-4
achieves f1-scores of 76% and 45% in the zero-shot setting, which improve by 7%
and 23% in the 5-shot setting, respectively. Our study conducts a thorough
evaluation of both proprietary and open-source LLMs to provide an objective
assessment of their performance in extracting feature-sentiment pairs.",2024-09-11,"Faiz Ali Shah, Ahmed Sabir, Rajesh Sharma, Dietmar Pfahl",http://arxiv.org/pdf/2409.07162v3,cs.CL
Gated Slot Attention for Efficient Linear-Time Sequence Modeling,"Linear attention Transformers and their gated variants, celebrated for
enabling parallel training and efficient recurrent inference, still fall short
in recall-intensive tasks compared to traditional Transformers and demand
significant resources for training from scratch. This paper introduces Gated
Slot Attention (GSA), which enhances Attention with Bounded-memory-Control
(ABC) by incorporating a gating mechanism inspired by Gated Linear Attention
(GLA). Essentially, GSA comprises a two-layer GLA linked via
$\operatorname{softmax}$, utilizing context-aware memory reading and adaptive
forgetting to improve memory capacity while maintaining compact recurrent state
size. This design greatly enhances both training and inference efficiency
through GLA's hardware-efficient training algorithm and reduced state size.
Additionally, retaining the $\operatorname{softmax}$ operation is particularly
beneficial in ""finetuning pretrained Transformers to RNNs"" (T2R) settings,
reducing the need for extensive training from scratch. Extensive experiments
confirm GSA's superior performance in scenarios requiring in-context recall and
in T2R settings.",2024-09-11,"Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, Peng Zhou, Guohong Fu",http://arxiv.org/pdf/2409.07146v2,cs.CL
Leveraging Unstructured Text Data for Federated Instruction Tuning of Large Language Models,"Federated instruction tuning enables multiple clients to collaboratively
fine-tune a shared large language model (LLM) that can follow humans'
instructions without directly sharing raw data. However, existing literature
impractically requires that all the clients readily hold instruction-tuning
data (i.e., structured instruction-response pairs), which necessitates massive
human annotations since clients' data is usually unstructured text instead.
Addressing this, we propose a novel and flexible framework FedIT-U2S, which can
automatically transform unstructured corpus into structured data for federated
instruction tuning. FedIT-U2S consists two key steps: (1) few-shot
instruction-tuning data generation, where each unstructured data piece together
with several examples is combined to prompt an LLM in generating an
instruction-response pair. To further enhance the flexibility, a
retrieval-based example selection technique is proposed, where the examples are
automatically selected based on the relatedness between the client's data piece
and example pool, bypassing the need of determining examples in advance. (2) A
typical federated instruction tuning process based on the generated data.
Overall, FedIT-U2S can be applied to diverse scenarios as long as the client
holds valuable text corpus, broadening the application scope of federated
instruction tuning. We conduct a series of experiments on three domains
(medicine, knowledge, and math), showing that our proposed FedIT-U2S can
consistently and significantly brings improvement over the base LLM.",2024-09-11,"Rui Ye, Rui Ge, Yuchi Fengting, Jingyi Chai, Yanfeng Wang, Siheng Chen",http://arxiv.org/pdf/2409.07136v1,cs.CL
LLM-based feature generation from text for interpretable machine learning,"Existing text representations such as embeddings and bag-of-words are not
suitable for rule learning due to their high dimensionality and absent or
questionable feature-level interpretability. This article explores whether
large language models (LLMs) could address this by extracting a small number of
interpretable features from text. We demonstrate this process on two datasets
(CORD-19 and M17+) containing several thousand scientific articles from
multiple disciplines and a target being a proxy for research impact. An
evaluation based on testing for the statistically significant correlation with
research impact has shown that LLama 2-generated features are semantically
meaningful. We consequently used these generated features in text
classification to predict the binary target variable representing the citation
rate for the CORD-19 dataset and the ordinal 5-class target representing an
expert-awarded grade in the M17+ dataset. Machine-learning models trained on
the LLM-generated features provided similar predictive performance to the
state-of-the-art embedding model SciBERT for scientific text. The LLM used only
62 features compared to 768 features in SciBERT embeddings, and these features
were directly interpretable, corresponding to notions such as article
methodological rigor, novelty, or grammatical correctness. As the final step,
we extract a small number of well-interpretable action rules. Consistently
competitive results obtained with the same LLM feature set across both
thematically diverse datasets show that this approach generalizes across
domains.",2024-09-11,"Vojtěch Balek, Lukáš Sýkora, Vilém Sklenák, Tomáš Kliegr",http://arxiv.org/pdf/2409.07132v1,cs.CL
Reranking Laws for Language Generation: A Communication-Theoretic Perspective,"To ensure large language models (LLMs) are used safely, one must reduce their
propensity to hallucinate or to generate unacceptable answers. A simple and
often used strategy is to first let the LLM generate multiple hypotheses and
then employ a reranker to choose the best one. In this paper, we draw a
parallel between this strategy and the use of redundancy to decrease the error
rate in noisy communication channels. We conceptualize the generator as a
sender transmitting multiple descriptions of a message through parallel noisy
channels. The receiver decodes the message by ranking the (potentially
corrupted) descriptions and selecting the one found to be most reliable. We
provide conditions under which this protocol is asymptotically error-free
(i.e., yields an acceptable answer almost surely) even in scenarios where the
reranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the
channel distributions are statistically dependent. We use our framework to
obtain reranking laws which we validate empirically on two real-world tasks
using LLMs: text-to-code generation with DeepSeek-Coder 7B and machine
translation of medical data with TowerInstruct 13B.",2024-09-11,"António Farinhas, Haau-Sing Li, André F. T. Martins",http://arxiv.org/pdf/2409.07131v2,cs.CL
Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem,"Natural language explanations (NLEs) are vital for elucidating the reasoning
behind large language model (LLM) decisions. Many techniques have been
developed to generate NLEs using LLMs. However, like humans, LLMs might not
always produce optimal NLEs on first attempt. Inspired by human learning
processes, we introduce Cross-Refine, which employs role modeling by deploying
two LLMs as generator and critic, respectively. The generator outputs a first
NLE and then refines this initial explanation using feedback and suggestions
provided by the critic. Cross-Refine does not require any supervised training
data or additional training. We validate Cross-Refine across three NLP tasks
using three state-of-the-art open-source LLMs through automatic and human
evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which
only utilizes self-feedback to refine the explanations. Our findings from
automatic evaluation and a user study indicate that Cross-Refine outperforms
Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful
LLMs, whereas Self-Refine only yields strong results with ChatGPT.
Additionally, we conduct an ablation study to assess the importance of feedback
and suggestions. Both of them play an important role in refining explanations.
We further evaluate Cross-Refine on a bilingual dataset in English and German.",2024-09-11,"Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Sebastian Möller, Vera Schmitt",http://arxiv.org/pdf/2409.07123v2,cs.CL
Ontology-Free General-Domain Knowledge Graph-to-Text Generation Dataset Synthesis using Large Language Model,"Knowledge Graph-to-Text (G2T) generation involves verbalizing structured
knowledge graphs into natural language text. Recent advancements in Pretrained
Language Models (PLMs) have improved G2T performance, but their effectiveness
depends on datasets with precise graph-text alignment. However, the scarcity of
high-quality, general-domain G2T generation datasets restricts progress in the
general-domain G2T generation research. To address this issue, we introduce
Wikipedia Ontology-Free Graph-text dataset (WikiOFGraph), a new large-scale G2T
dataset generated using a novel method that leverages Large Language Model
(LLM) and Data-QuestEval. Our new dataset, which contains 5.85M general-domain
graph-text pairs, offers high graph-text consistency without relying on
external ontologies. Experimental results demonstrate that PLM fine-tuned on
WikiOFGraph outperforms those trained on other datasets across various
evaluation metrics. Our method proves to be a scalable and effective solution
for generating high-quality G2T data, significantly advancing the field of G2T
generation.",2024-09-11,"Daehee Kim, Deokhyung Kang, Sangwon Ryu, Gary Geunbae Lee",http://arxiv.org/pdf/2409.07088v1,cs.CL
Understanding Knowledge Drift in LLMs through Misinformation,"Large Language Models (LLMs) have revolutionized numerous applications,
making them an integral part of our digital ecosystem. However, their
reliability becomes critical, especially when these models are exposed to
misinformation. We primarily analyze the susceptibility of state-of-the-art
LLMs to factual inaccuracies when they encounter false information in a QnA
scenario, an issue that can lead to a phenomenon we refer to as *knowledge
drift*, which significantly undermines the trustworthiness of these models. We
evaluate the factuality and the uncertainty of the models' responses relying on
Entropy, Perplexity, and Token Probability metrics. Our experiments reveal that
an LLM's uncertainty can increase up to 56.6% when the question is answered
incorrectly due to the exposure to false information. At the same time,
repeated exposure to the same false information can decrease the models
uncertainty again (-52.8% w.r.t. the answers on the untainted prompts),
potentially manipulating the underlying model's beliefs and introducing a drift
from its original knowledge. These findings provide insights into LLMs'
robustness and vulnerability to adversarial inputs, paving the way for
developing more reliable LLM applications across various domains. The code is
available at https://github.com/afastowski/knowledge_drift.",2024-09-11,"Alina Fastowski, Gjergji Kasneci",http://arxiv.org/pdf/2409.07085v1,cs.CL
Latent Space Interpretation for Stylistic Analysis and Explainable Authorship Attribution,"Recent state-of-the-art authorship attribution methods learn authorship
representations of texts in a latent, non-interpretable space, hindering their
usability in real-world applications. Our work proposes a novel approach to
interpreting these learned embeddings by identifying representative points in
the latent space and utilizing LLMs to generate informative natural language
descriptions of the writing style of each point. We evaluate the alignment of
our interpretable space with the latent one and find that it achieves the best
prediction agreement compared to other baselines. Additionally, we conduct a
human evaluation to assess the quality of these style descriptions, validating
their utility as explanations for the latent space. Finally, we investigate
whether human performance on the challenging AA task improves when aided by our
system's explanations, finding an average improvement of around +20% in
accuracy.",2024-09-11,"Milad Alshomary, Narutatsu Ri, Marianna Apidianaki, Ajay Patel, Smaranda Muresan, Kathleen McKeown",http://arxiv.org/pdf/2409.07072v1,cs.CL
Automated Speaking Assessment of Conversation Tests with Novel Graph-based Modeling on Spoken Response Coherence,"Automated speaking assessment in conversation tests (ASAC) aims to evaluate
the overall speaking proficiency of an L2 (second-language) speaker in a
setting where an interlocutor interacts with one or more candidates. Although
prior ASAC approaches have shown promising performance on their respective
datasets, there is still a dearth of research specifically focused on
incorporating the coherence of the logical flow within a conversation into the
grading model. To address this critical challenge, we propose a hierarchical
graph model that aptly incorporates both broad inter-response interactions
(e.g., discourse relations) and nuanced semantic information (e.g., semantic
words and speaker intents), which is subsequently fused with contextual
information for the final prediction. Extensive experimental results on the
NICT-JLE benchmark dataset suggest that our proposed modeling approach can
yield considerable improvements in prediction accuracy with respect to various
assessment metrics, as compared to some strong baselines. This also sheds light
on the importance of investigating coherence-related facets of spoken responses
in ASAC.",2024-09-11,"Jiun-Ting Li, Bi-Cheng Yan, Tien-Hong Lo, Yi-Cheng Wang, Yung-Chang Hsu, Berlin Chen",http://arxiv.org/pdf/2409.07064v2,cs.CL
Legal Fact Prediction: The Missing Piece in Legal Judgment Prediction,"Legal judgment prediction (LJP), which enables litigants and their lawyers to
forecast judgment outcomes and refine litigation strategies, has emerged as a
crucial legal NLP task. Existing studies typically utilize legal facts, i.e.,
facts that have been established by evidence and determined by the judge, to
predict the judgment. However, legal facts are often difficult to obtain in the
early stages of litigation, significantly limiting the practical applicability
of fact-based LJP. To address this limitation, we propose a novel legal NLP
task: \textit{legal fact prediction} (LFP), which takes the evidence submitted
by litigants for trial as input to predict legal facts, thereby empowering
fact-based LJP technologies to perform prediction in the absence of
ground-truth legal facts. We also propose the first benchmark dataset,
LFPBench, for evaluating the LFP task. Our extensive experiments on LFPBench
demonstrate the effectiveness of LFP-empowered LJP and highlight promising
research directions for LFP. Our code and data are available at
https://github.com/HPRCEST/LFPBench.",2024-09-11,"Junkai Liu, Yujie Tong, Hui Huang, Bowen Zheng, Yiran Hu, Peicheng Wu, Chuan Xiao, Makoto Onizuka, Muyun Yang, Shuyuan Zheng",http://arxiv.org/pdf/2409.07055v2,cs.CL
Native vs Non-Native Language Prompting: A Comparative Analysis,"Large language models (LLMs) have shown remarkable abilities in different
fields, including standard Natural Language Processing (NLP) tasks. To elicit
knowledge from LLMs, prompts play a key role, consisting of natural language
instructions. Most open and closed source LLMs are trained on available labeled
and unlabeled resources--digital content such as text, images, audio, and
videos. Hence, these models have better knowledge for high-resourced languages
but struggle with low-resourced languages. Since prompts play a crucial role in
understanding their capabilities, the language used for prompts remains an
important research question. Although there has been significant research in
this area, it is still limited, and less has been explored for medium to
low-resourced languages. In this study, we investigate different prompting
strategies (native vs. non-native) on 11 different NLP tasks associated with 12
different Arabic datasets (9.7K data points). In total, we conducted 197
experiments involving 3 LLMs, 12 datasets, and 3 prompting strategies. Our
findings suggest that, on average, the non-native prompt performs the best,
followed by mixed and native prompts.",2024-09-11,"Mohamed Bayan Kmainasi, Rakif Khan, Ali Ezzat Shahroor, Boushra Bendou, Maram Hasanain, Firoj Alam",http://arxiv.org/pdf/2409.07054v2,cs.CL
Beyond IID: Optimizing Instruction Learning from the Perspective of Instruction Interaction and Dependency,"With the availability of various instruction datasets, a pivotal challenge is
how to effectively select and integrate these instructions to fine-tune large
language models (LLMs). Previous research mainly focuses on selecting
individual high-quality instructions. However, these works overlooked the joint
interactions and dependencies between different categories of instructions,
leading to suboptimal selection strategies. Moreover, the nature of these
interaction patterns remains largely unexplored, let alone optimize the
instruction set with regard to them. To fill these gaps, in this paper, we: (1)
systemically investigate interaction and dependency patterns between different
categories of instructions, (2) manage to optimize the instruction set
concerning the interaction patterns using a linear programming-based method,
and optimize the learning schema of SFT using an instruction dependency
taxonomy guided curriculum learning. Experimental results across different LLMs
demonstrate improved performance over strong baselines on widely adopted
benchmarks.",2024-09-11,"Hanyu Zhao, Li Du, Yiming Ju, Chengwei Wu, Tengfei Pan",http://arxiv.org/pdf/2409.07045v1,cs.CL
You Have Thirteen Hours in Which to Solve the Labyrinth: Enhancing AI Game Masters with Function Calling,"Developing a consistent and reliable AI game master for text-based games is a
challenging task due to the limitations of large language models (LLMs) and the
complexity of the game master's role. This paper presents a novel approach to
enhance AI game masters by leveraging function calling in the context of the
table-top role-playing game ""Jim Henson's Labyrinth: The Adventure Game."" Our
methodology involves integrating game-specific controls through functions,
which we show improves the narrative quality and state update consistency of
the AI game master. The experimental results, based on human evaluations and
unit tests, demonstrate the effectiveness of our approach in enhancing gameplay
experience and maintaining coherence with the game state. This work contributes
to the advancement of game AI and interactive storytelling, offering insights
into the design of more engaging and consistent AI-driven game masters.",2024-09-11,"Jaewoo Song, Andrew Zhu, Chris Callison-Burch",http://arxiv.org/pdf/2409.06949v1,cs.CL
Context-Aware Membership Inference Attacks against Pre-trained Large Language Models,"Prior Membership Inference Attacks (MIAs) on pre-trained Large Language
Models (LLMs), adapted from classification model attacks, fail due to ignoring
the generative process of LLMs across token sequences. In this paper, we
present a novel attack that adapts MIA statistical tests to the perplexity
dynamics of subsequences within a data point. Our method significantly
outperforms prior loss-based approaches, revealing context-dependent
memorization patterns in pre-trained LLMs.",2024-09-11,"Hongyan Chang, Ali Shahin Shamsabadi, Kleomenis Katevas, Hamed Haddadi, Reza Shokri",http://arxiv.org/pdf/2409.13745v1,cs.CL
Representation Tuning,"Activation engineering is becoming increasingly popular as a means of online
control of large language models (LLMs). In this work, we extend the idea of
inference-time steering with vectors that represent a behavioral direction of
interest to tuning those vectors directly into the model, obviating the need
for online control. First, we identify activation vectors related to honesty in
an open-source LLM (Llama-2-13b-chat). Next, we demonstrate that model output
can be made more or less honest by adding positive or negative multiples of
these vectors to residual stream activations during generation. Then, we show
that a similar effect can be achieved by fine-tuning the vectors directly into
the model, by use of a dual loss function based on the cosine similarity of
residual stream activations to the vectors combined with a standard token-based
loss (""representation tuning""). Finally, we compare the generations in response
to honesty-probing prompts from the resulting models to those from models
fine-tuned with a token-based loss alone, and to those from the untuned model
subjected to online steering. Overall, fine-tuning the vectors into the models
using the cosine similarity plus token loss showed a stronger effect than
online steering, and generalized better than using the standard loss,
suggesting the potential utility of this approach as a safety measure. Code and
data are available at https://github.com/cma1114/representation_tuning. Tuned
models are available at
https://huggingface.co/collections/cackerman/representation-tuning-66da1e5ab41cd1b824687d9f.",2024-09-11,Christopher M. Ackerman,http://arxiv.org/pdf/2409.06927v4,cs.CL
A Simplified Retriever to Improve Accuracy of Phenotype Normalizations by Large Language Models,"Large language models (LLMs) have shown improved accuracy in phenotype term
normalization tasks when augmented with retrievers that suggest candidate
normalizations based on term definitions. In this work, we introduce a
simplified retriever that enhances LLM accuracy by searching the Human
Phenotype Ontology (HPO) for candidate matches using contextual word embeddings
from BioBERT without the need for explicit term definitions. Testing this
method on terms derived from the clinical synopses of Online Mendelian
Inheritance in Man (OMIM), we demonstrate that the normalization accuracy of a
state-of-the-art LLM increases from a baseline of 62.3% without augmentation to
90.3% with retriever augmentation. This approach is potentially generalizable
to other biomedical term normalization tasks and offers an efficient
alternative to more complex retrieval methods.",2024-09-11,"Daniel B. Hier, Thanh Son Do, Tayo Obafemi-Ajayi",http://arxiv.org/pdf/2409.13744v2,cs.CL
AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs,"Jailbreak vulnerabilities in Large Language Models (LLMs) refer to methods
that extract malicious content from the model by carefully crafting prompts or
suffixes, which has garnered significant attention from the research community.
However, traditional attack methods, which primarily focus on the semantic
level, are easily detected by the model. These methods overlook the difference
in the model's alignment protection capabilities at different output stages. To
address this issue, we propose an adaptive position pre-fill jailbreak attack
approach for executing jailbreak attacks on LLMs. Our method leverages the
model's instruction-following capabilities to first output pre-filled safe
content, then exploits its narrative-shifting abilities to generate harmful
content. Extensive black-box experiments demonstrate our method can improve the
attack success rate by 47% on the widely recognized secure model (Llama2)
compared to existing approaches. Our code can be found at:
https://github.com/Yummy416/AdaPPA.",2024-09-11,"Lijia Lv, Weigang Zhang, Xuehai Tang, Jie Wen, Feng Liu, Jizhong Han, Songlin Hu",http://arxiv.org/pdf/2409.07503v1,cs.CL
DeepScore: A Comprehensive Approach to Measuring Quality in AI-Generated Clinical Documentation,"Medical practitioners are rapidly adopting generative AI solutions for
clinical documentation, leading to significant time savings and reduced stress.
However, evaluating the quality of AI-generated documentation is a complex and
ongoing challenge. This paper presents an overview of DeepScribe's
methodologies for assessing and managing note quality, focusing on various
metrics and the composite ""DeepScore"", an overall index of quality and
accuracy. These methodologies aim to enhance the quality of patient care
documentation through accountability and continuous improvement.",2024-09-10,Jon Oleson,http://arxiv.org/pdf/2409.16307v1,cs.CL
BERTScoreVisualizer: A Web Tool for Understanding Simplified Text Evaluation with BERTScore,"The BERTScore metric is commonly used to evaluate automatic text
simplification systems. However, current implementations of the metric fail to
provide complete visibility into all information the metric can produce.
Notably, the specific token matchings can be incredibly useful in generating
clause-level insight into the quality of simplified text. We address this by
introducing BERTScoreVisualizer, a web application that goes beyond reporting
precision, recall, and F1 score and provides a visualization of the matching
between tokens. We believe that our software can help improve the analysis of
text simplification systems by specifically showing where generated, simplified
text deviates from reference text. We host our code and demo on GitHub.",2024-09-10,"Sebastian Jaskowski, Sahasra Chava, Agam Shah",http://arxiv.org/pdf/2409.17160v1,cs.CL
A Dataset for Evaluating LLM-based Evaluation Functions for Research Question Extraction Task,"The progress in text summarization techniques has been remarkable. However
the task of accurately extracting and summarizing necessary information from
highly specialized documents such as research papers has not been sufficiently
investigated. We are focusing on the task of extracting research questions (RQ)
from research papers and construct a new dataset consisting of machine learning
papers, RQ extracted from these papers by GPT-4, and human evaluations of the
extracted RQ from multiple perspectives. Using this dataset, we systematically
compared recently proposed LLM-based evaluation functions for summarizations,
and found that none of the functions showed sufficiently high correlations with
human evaluations. We expect our dataset provides a foundation for further
research on developing better evaluation functions tailored to the RQ
extraction task, and contribute to enhance the performance of the task. The
dataset is available at https://github.com/auto-res/PaperRQ-HumanAnno-Dataset.",2024-09-10,"Yuya Fujisaki, Shiro Takagi, Hideki Asoh, Wataru Kumagai",http://arxiv.org/pdf/2409.06883v1,cs.CL
A Large Dataset of Spontaneous Speech with the Accent Spoken in São Paulo for Automatic Speech Recognition Evaluation,"We present a freely available spontaneous speech corpus for the Brazilian
Portuguese language and report preliminary automatic speech recognition (ASR)
results, using both the Wav2Vec2-XLSR-53 and Distil-Whisper models fine-tuned
and trained on our corpus. The NURC-SP Audio Corpus comprises 401 different
speakers (204 females, 197 males) with a total of 239.30 hours of transcribed
audio recordings. To the best of our knowledge, this is the first large
Paulistano accented spontaneous speech corpus dedicated to the ASR task in
Portuguese. We first present the design and development procedures of the
NURC-SP Audio Corpus, and then describe four ASR experiments in detail. The
experiments demonstrated promising results for the applicability of the corpus
for ASR. Specifically, we fine-tuned two versions of Wav2Vec2-XLSR-53 model,
trained a Distil-Whisper model using our dataset with labels determined by
Whisper Large-V3 model, and fine-tuned this Distil-Whisper model with our
corpus. Our best results were the Distil-Whisper fine-tuned over NURC-SP Audio
Corpus with a WER of 24.22% followed by a fine-tuned versions of
Wav2Vec2-XLSR-53 model with a WER of 33.73%, that is almost 10% point worse
than Distil-Whisper's. To enable experiment reproducibility, we share the
NURC-SP Audio Corpus dataset, pre-trained models, and training recipes in
Hugging-Face and Github repositories.",2024-09-10,"Rodrigo Lima, Sidney Evaldo Leal, Arnaldo Candido Junior, Sandra Maria Aluísio",http://arxiv.org/pdf/2409.15350v1,cs.CL
NSP: A Neuro-Symbolic Natural Language Navigational Planner,"Path planners that can interpret free-form natural language instructions hold
promise to automate a wide range of robotics applications. These planners
simplify user interactions and enable intuitive control over complex
semi-autonomous systems. While existing symbolic approaches offer guarantees on
the correctness and efficiency, they struggle to parse free-form natural
language inputs. Conversely, neural approaches based on pre-trained Large
Language Models (LLMs) can manage natural language inputs but lack performance
guarantees. In this paper, we propose a neuro-symbolic framework for path
planning from natural language inputs called NSP. The framework leverages the
neural reasoning abilities of LLMs to i) craft symbolic representations of the
environment and ii) a symbolic path planning algorithm. Next, a solution to the
path planning problem is obtained by executing the algorithm on the environment
representation. The framework uses a feedback loop from the symbolic execution
environment to the neural generation process to self-correct syntax errors and
satisfy execution time constraints. We evaluate our neuro-symbolic approach
using a benchmark suite with 1500 path-planning problems. The experimental
evaluation shows that our neuro-symbolic approach produces 90.1% valid paths
that are on average 19-77% shorter than state-of-the-art neural approaches.",2024-09-10,"William English, Dominic Simon, Sumit Jha, Rickard Ewetz",http://arxiv.org/pdf/2409.06859v2,cs.CL
What is the Role of Small Models in the LLM Era: A Survey,"Large Language Models (LLMs) have made significant progress in advancing
artificial general intelligence (AGI), leading to the development of
increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up
model sizes results in exponentially higher computational costs and energy
consumption, making these models impractical for academic researchers and
businesses with limited resources. At the same time, Small Models (SMs) are
frequently used in practical settings, although their significance is currently
underestimated. This raises important questions about the role of small models
in the era of LLMs, a topic that has received limited attention in prior
research. In this work, we systematically examine the relationship between LLMs
and SMs from two key perspectives: Collaboration and Competition. We hope this
survey provides valuable insights for practitioners, fostering a deeper
understanding of the contribution of small models and promoting more efficient
use of computational resources. The code is available at
https://github.com/tigerchen52/role_of_small_models",2024-09-10,"Lihu Chen, Gaël Varoquaux",http://arxiv.org/pdf/2409.06857v5,cs.CL
PingPong: A Benchmark for Role-Playing Language Models with User Emulation and Multi-Model Evaluation,"We introduce a benchmark for evaluating the role-playing capabilities of
language models. Our approach leverages different language models to simulate
users in dynamic, multi-turn conversations and assess the resulting dialogues.
Our methodology involves three main components: a player model that adopts a
specific character role, an interrogator model that simulates user behavior in
a specific situation, and a judge model ensemble that evaluates conversation
quality with 3 metrics: character consistency, entertainment value, and
language fluency. We evaluated more than 40 models in both English and Russian,
with each model participating in 64 conversations with 8 characters and 8
situations. We conducted experiments comparing automated evaluations with human
annotations to validate our approach, demonstrating strong correlations across
multiple criteria. This work provides a foundation for a robust and dynamic
evaluation of different model capabilities in interactive scenarios.",2024-09-10,Ilya Gusev,http://arxiv.org/pdf/2409.06820v4,cs.CL
Decomposition of surprisal: Unified computational model of ERP components in language processing,"The functional interpretation of language-related ERP components has been a
central debate in psycholinguistics for decades. We advance an
information-theoretic model of human language processing in the brain in which
incoming linguistic input is processed at first shallowly and later with more
depth, with these two kinds of information processing corresponding to distinct
electroencephalographic signatures. Formally, we show that the information
content (surprisal) of a word in context can be decomposed into two quantities:
(A) shallow surprisal, which signals shallow processing difficulty for a word,
and corresponds with the N400 signal; and (B) deep surprisal, which reflects
the discrepancy between shallow and deep representations, and corresponds to
the P600 signal and other late positivities. Both of these quantities can be
estimated straightforwardly using modern NLP models. We validate our theory by
successfully simulating ERP patterns elicited by a variety of linguistic
manipulations in previously-reported experimental data from six experiments,
with successful novel qualitative and quantitative predictions. Our theory is
compatible with traditional cognitive theories assuming a `good-enough' shallow
representation stage, but with a precise information-theoretic formulation. The
model provides an information-theoretic model of ERP components grounded on
cognitive processes, and brings us closer to a fully-specified
neuro-computational model of language processing.",2024-09-10,"Jiaxuan Li, Richard Futrell",http://arxiv.org/pdf/2409.06803v2,cs.CL
Translating Step-by-Step: Decomposing the Translation Process for Improved Translation Quality of Long-Form Texts,"In this paper we present a step-by-step approach to long-form text
translation, drawing on established processes in translation studies. Instead
of viewing machine translation as a single, monolithic task, we propose a
framework that engages language models in a multi-turn interaction,
encompassing pre-translation research, drafting, refining, and proofreading,
resulting in progressively improved translations. Extensive automatic
evaluations using Gemini 1.5 Pro across ten language pairs show that
translating step-by-step yields large translation quality improvements over
conventional zero-shot prompting approaches and earlier human-like baseline
strategies, resulting in state-of-the-art results on WMT2024.",2024-09-10,"Eleftheria Briakou, Jiaming Luo, Colin Cherry, Markus Freitag",http://arxiv.org/pdf/2409.06790v1,cs.CL
Geometric-Averaged Preference Optimization for Soft Preference Labels,"Many algorithms for aligning LLMs with human preferences assume that human
preferences are binary and deterministic. However, human preferences can vary
across individuals, and therefore should be represented distributionally. In
this work, we introduce the distributional soft preference labels and improve
Direct Preference Optimization (DPO) with a weighted geometric average of the
LLM output likelihood in the loss function. This approach adjusts the scale of
learning loss based on the soft labels such that the loss would approach zero
when the responses are closer to equally preferred. This simple modification
can be easily applied to any DPO-based methods and mitigate over-optimization
and objective mismatch, which prior works suffer from. Our experiments simulate
the soft preference labels with AI feedback from LLMs and demonstrate that
geometric averaging consistently improves performance on standard benchmarks
for alignment research. In particular, we observe more preferable responses
than binary labels and significant improvements where modestly-confident labels
are in the majority.",2024-09-10,"Hiroki Furuta, Kuang-Huei Lee, Shixiang Shane Gu, Yutaka Matsuo, Aleksandra Faust, Heiga Zen, Izzeddin Gur",http://arxiv.org/pdf/2409.06691v3,cs.CL
Knowing When to Ask -- Bridging Large Language Models and Data,"Large Language Models (LLMs) are prone to generating factually incorrect
information when responding to queries that involve numerical and statistical
data or other timely facts. In this paper, we present an approach for enhancing
the accuracy of LLMs by integrating them with Data Commons, a vast, open-source
repository of public statistics from trusted organizations like the United
Nations (UN), Center for Disease Control and Prevention (CDC) and global census
bureaus. We explore two primary methods: Retrieval Interleaved Generation
(RIG), where the LLM is trained to produce natural language queries to retrieve
data from Data Commons, and Retrieval Augmented Generation (RAG), where
relevant data tables are fetched from Data Commons and used to augment the
LLM's prompt. We evaluate these methods on a diverse set of queries,
demonstrating their effectiveness in improving the factual accuracy of LLM
outputs. Our work represents an early step towards building more trustworthy
and reliable LLMs that are grounded in verifiable statistical data and capable
of complex factual reasoning.",2024-09-10,"Prashanth Radhakrishnan, Jennifer Chen, Bo Xu, Prem Ramaswami, Hannah Pho, Adriana Olmos, James Manyika, R. V. Guha",http://arxiv.org/pdf/2409.13741v1,cs.CL
E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning,"In the realm of Large Language Models (LLMs), the ability to process long
contexts is increasingly crucial for tasks such as multi-round dialogues, code
generation, and document summarization. This paper addresses the challenges of
enhancing the long-context performance, reducing computational complexity, and
leveraging pretrained models collectively termed the ""impossible triangle."" We
introduce E2LLM (Encoder Elongated Large Language Models), a novel approach
that effectively navigates this paradox. The method involves splitting long
contexts into chunks, compressing each into embedding vectors via a pretrained
text encoder, and utilizing an adapter to align these representations with a
decoder-only LLM. Two training objectives, focusing on reconstruction of the
encoder output and long-context instruction fine-tuning, are employed to
facilitate the understanding of soft prompts by the LLM. Experimental results
demonstrate that E2LLM achieves superior performance in long-context scenarios
while balancing efficiency, performance, and compatibility with pretrained
models. Our framework thus represents a significant advancement in the field,
contributing to effective long-text modeling.",2024-09-10,"Zihan Liao, Jun Wang, Hang Yu, Lingxiao Wei, Jianguo Li, Jun Wang, Wei Zhang",http://arxiv.org/pdf/2409.06679v1,cs.CL
LLaMA-Omni: Seamless Speech Interaction with Large Language Models,"Models like GPT-4o enable real-time interaction with large language models
(LLMs) through speech, significantly enhancing user experience compared to
traditional text-based interaction. However, there is still a lack of
exploration on how to build speech interaction models based on open-source
LLMs. To address this, we propose LLaMA-Omni, a novel model architecture
designed for low-latency and high-quality speech interaction with LLMs.
LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,
and a streaming speech decoder. It eliminates the need for speech
transcription, and can simultaneously generate text and speech responses
directly from speech instructions with extremely low latency. We build our
model based on the latest Llama-3.1-8B-Instruct model. To align the model with
speech interaction scenarios, we construct a dataset named InstructS2S-200K,
which includes 200K speech instructions and corresponding speech responses.
Experimental results show that compared to previous speech-language models,
LLaMA-Omni provides better responses in both content and style, with a response
latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3
days on just 4 GPUs, paving the way for the efficient development of
speech-language models in the future.",2024-09-10,"Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, Yang Feng",http://arxiv.org/pdf/2409.06666v2,cs.CL
Sortformer: Seamless Integration of Speaker Diarization and ASR by Bridging Timestamps and Tokens,"We propose Sortformer, a novel neural model for speaker diarization, trained
with unconventional objectives compared to existing end-to-end diarization
models. The permutation problem in speaker diarization has long been regarded
as a critical challenge. Most prior end-to-end diarization systems employ
permutation invariant loss (PIL), which optimizes for the permutation that
yields the lowest error. In contrast, we introduce Sort Loss, which enables a
diarization model to autonomously resolve permutation, with or without PIL. We
demonstrate that combining Sort Loss and PIL achieves performance competitive
with state-of-the-art end-to-end diarization models trained exclusively with
PIL. Crucially, we present a streamlined multispeaker ASR architecture that
leverages Sortformer as a speaker supervision model, embedding speaker label
estimation within the ASR encoder state using a sinusoidal kernel function.
This approach resolves the speaker permutation problem through sorted
objectives, effectively bridging speaker-label timestamps and speaker tokens.
In our experiments, we show that the proposed multispeaker ASR architecture,
enhanced with speaker supervision, improves performance via adapter techniques.
Code and trained models will be made publicly available via the NVIDIA NeMo
framework.",2024-09-10,"Taejin Park, Ivan Medennikov, Kunal Dhawan, Weiqing Wang, He Huang, Nithin Rao Koluguri, Krishna C. Puvvada, Jagadeesh Balam, Boris Ginsburg",http://arxiv.org/pdf/2409.06656v2,cs.CL
TeXBLEU: Automatic Metric for Evaluate LaTeX Format,"LaTeX is suitable for creating specially formatted documents in science,
technology, mathematics, and computer science. Although the use of mathematical
expressions in LaTeX format along with language models is increasing, there are
no proper evaluation matrices to evaluate them. In this study, we propose
TeXBLEU, a metric for evaluating mathematical expressions in the LaTeX format
built on the n-gram-based BLEU metric widely used in translation tasks. The
proposed TeXBLEU consists of a predefined tokenizer trained on the arXiv paper
dataset and a fine-tuned embedding model with positional encoding. The TeXBLEU
score was calculated by replacing BLUE's modified precision score with the
similarity of n-gram-based tokens. TeXBLEU showed improvements of 86\%, 121\%,
and 610\% over traditional evaluation metrics, such as BLEU, sacreBLEU, and
Rouge, respectively, on the MathBridge dataset with 1,000 data points. The code
is available at https://github.com/KyuDan1/TeXBLEU.",2024-09-10,"Kyudan Jung, Nam-Joon Kim, Hyongon Ryu, Sieun Hyeon, Seung-jun Lee, Hyeok-jae Lee",http://arxiv.org/pdf/2409.06639v3,cs.CL
MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders,"The rapid advancements in large language models (LLMs) have significantly
enhanced natural language processing capabilities, facilitating the development
of AudioLLMs that process and understand speech and audio inputs alongside
text. Existing AudioLLMs typically combine a pre-trained audio encoder with a
pre-trained LLM, which are subsequently finetuned on specific audio tasks.
However, the pre-trained audio encoder has constrained capacity to capture
features for new tasks and datasets. To address this, we propose to incorporate
mixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE
supplements a base encoder with a pool of relatively light weight encoders,
selectively activated based on the audio input to enhance feature extraction
without significantly increasing model size. Our empirical results demonstrate
that MoWE effectively improves multi-task performance, broadening the
applicability of AudioLLMs to more diverse audio tasks.",2024-09-10,"Wenyu Zhang, Shuo Sun, Bin Wang, Xunlong Zou, Zhuohan Liu, Yingxu He, Geyu Lin, Nancy F. Chen, Ai Ti Aw",http://arxiv.org/pdf/2409.06635v4,cs.CL
Language agents achieve superhuman synthesis of scientific knowledge,"Language models are known to hallucinate incorrect information, and it is
unclear if they are sufficiently accurate and reliable for use in scientific
research. We developed a rigorous human-AI comparison methodology to evaluate
language model agents on real-world literature search tasks covering
information retrieval, summarization, and contradiction detection tasks. We
show that PaperQA2, a frontier language model agent optimized for improved
factuality, matches or exceeds subject matter expert performance on three
realistic literature research tasks without any restrictions on humans (i.e.,
full access to internet, search tools, and time). PaperQA2 writes cited,
Wikipedia-style summaries of scientific topics that are significantly more
accurate than existing, human-written Wikipedia articles. We also introduce a
hard benchmark for scientific literature research called LitQA2 that guided
design of PaperQA2, leading to it exceeding human performance. Finally, we
apply PaperQA2 to identify contradictions within the scientific literature, an
important scientific task that is challenging for humans. PaperQA2 identifies
2.34 +/- 1.99 contradictions per paper in a random subset of biology papers, of
which 70% are validated by human experts. These results demonstrate that
language model agents are now capable of exceeding domain experts across
meaningful tasks on scientific literature.",2024-09-10,"Michael D. Skarlinski, Sam Cox, Jon M. Laurent, James D. Braza, Michaela Hinks, Michael J. Hammerling, Manvitha Ponnapati, Samuel G. Rodriques, Andrew D. White",http://arxiv.org/pdf/2409.13740v2,cs.CL
A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio,"Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to
obtain the unfamiliar language skill or adapt into new domains. The huge
training cost of CPT often asks for cautious choice of key hyper-parameters
such as the mixture ratio of extra language or domain corpus. However, there is
no systematic study which bridge the gap between the optimal mixture ratio and
the actual model performance, and the gap between experimental scaling law and
the actual deployment in the full model size. In this paper, we perform CPT on
Llama-3 8B and 70B to enhance its Chinese ability. We study the optimal
correlation between the Additional Language Mixture Ratio (ALMR) and the
Learning Rate (LR) on the 8B size which directly indicate the optimal
experimental set up. By thorough choice of hyper-parameter, and subsequent
fine-tuning, the model capability is improved not only on the Chinese-related
benchmark, but also some specific domains including math, coding and emotional
intelligence. We deploy the final 70B version of LLM on an real-life chat
system which obtain satisfying performance.",2024-09-10,"Ningyuan Xi, Yetao Wu, Kun Fan, Teng Chen, Qingqing Gu, Peng Yu, Jinxian Qu, Chenxi Liu, Zhonglin Jiang, Yong Chen, Luo Ji",http://arxiv.org/pdf/2409.06624v1,cs.CL
Exploring Italian sentence embeddings properties through multi-tasking,"We investigate to what degree existing LLMs encode abstract linguistic
information in Italian in a multi-task setting. We exploit curated synthetic
data on a large scale -- several Blackbird Language Matrices (BLMs) problems in
Italian -- and use them to study how sentence representations built using
pre-trained language models encode specific syntactic and semantic information.
We use a two-level architecture to model separately a compression of the
sentence embeddings into a representation that contains relevant information
for a task, and a BLM task. We then investigate whether we can obtain
compressed sentence representations that encode syntactic and semantic
information relevant to several BLM tasks. While we expected that the sentence
structure -- in terms of sequence of phrases/chunks -- and chunk properties
could be shared across tasks, performance and error analysis show that the
clues for the different tasks are encoded in different manners in the sentence
embeddings, suggesting that abstract linguistic notions such as constituents or
thematic roles does not seem to be present in the pretrained sentence
embeddings.",2024-09-10,"Vivi Nastase, Giuseppe Samo, Chunyang Jiang, Paola Merlo",http://arxiv.org/pdf/2409.06622v2,cs.CL
LaMsS: When Large Language Models Meet Self-Skepticism,"Hallucination is a major challenge for large language models (LLMs),
preventing their further application in some fields. The skeptical thinking of
humankind could be useful for LLMs to self-cognition, self-reflection and
alleviate their hallucinations. Inspired by this consideration, we propose a
novel approach called LaMsS, which combines the semantic understanding
capability of LLMs with self-skepticism. By introducing a series of skepticism
tokens and augmenting them into the vocabulary, we conduct both pertaining and
finetuning, which allow the LLM to decode each normal token followed by a
skeptical token, representing different skepticism levels. By calculating the
response skepticism given a query, one can define a new self-aware LLM which is
only willing to answer with relative lower skepticism level than the threshold.
By examining the accuracy, AUC and AP of willingly answering questions, we
demonstrate that LaMsS achieves better performance than baselines on both
multi-choice questions and open-domain question-answering benchmarks, and can
generalize to multi-task and out-of-domain settings. Our study sheds some
lights on the self-skepticism modeling on further artificial intelligence.
Project code and model checkpoints can be found in
https://anonymous.4open.science/r/SM-1E76.",2024-09-10,"Yetao Wu, Yihong Wang, Teng Chen, Ningyuan Xi, Qingqing Gu, Hongyang Lei, Luo Ji",http://arxiv.org/pdf/2409.06601v4,cs.CL
GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering,"Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use
Large Language Models (LLMs) alongside private and up-to-date knowledge bases.
In this work, we address the challenges of using LLM-as-a-Judge when evaluating
grounded answers generated by RAG systems. To assess the calibration and
discrimination capabilities of judge models, we identify 7 generator failure
modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a
meta-evaluation benchmark of 144 unit tests. This benchmark reveals that
existing automated RAG evaluation frameworks often overlook important failure
modes, even when using GPT-4 as a judge.
  To improve on the current design of automated RAG evaluation frameworks, we
propose a novel pipeline and find that while closed models perform well on
GroUSE, state-of-the-art open-source judges do not generalize to our proposed
criteria, despite strong correlation with GPT-4's judgement. Our findings
suggest that correlation with GPT-4 is an incomplete proxy for the practical
performance of judge models and should be supplemented with evaluations on unit
tests for precise failure mode detection.
  We further show that finetuning Llama-3 on GPT-4's reasoning traces
significantly boosts its evaluation capabilities, improving upon both
correlation with GPT-4's evaluations and calibration on reference situations.",2024-09-10,"Sacha Muller, António Loison, Bilel Omrani, Gautier Viaud",http://arxiv.org/pdf/2409.06595v3,cs.CL
Table-to-Text Generation with Pretrained Diffusion Models,"Diffusion models have demonstrated significant potential in achieving
state-of-the-art performance across various text generation tasks. In this
systematic study, we investigate their application to the table-to-text problem
by adapting the diffusion model to the task and conducting an in-depth
analysis. Our experiments cover multiple aspects of diffusion models training.
We explore sampling strategy influence by inducing recent diffusion model
accelerator DPM-Solver++ into our core model. We have tested different
prediction aggregation methods, like ROVER and Minimum Bayes-Risk (MBR). Our
studies cover the impact of the pre-training phase in diffusion models and the
generation length constraints influence. We also have compared diffusion model
generation with auto-regressive text-to-text models with different temperature
settings for diversity evaluation. Our key observation is that diffusion models
demonstrate the balance between quality and diversity while auto-regressive
text-to-text models are not successful at handling both at the same time.
Furthermore, we found out that to achieve the highest quality possible, it is
preferable to use a regular sampler with the strictest length constraint to
create multiple samples, and then use MBR to aggregate the predictions.
However, if you are prepared to give up high level of diversity and to
accelerate the process, you can also utilize a fast sampler DPM-Solver++. Our
findings reveal that diffusion models achieve comparable results in the
table-to-text domain, highlighting their viability in the table-to-text
challenge as a promising research direction.",2024-09-10,"Aleksei S. Krylov, Oleg D. Somov",http://arxiv.org/pdf/2409.13739v1,cs.CL
"NLP4PBM: A Systematic Review on Process Extraction using Natural Language Processing with Rule-based, Machine and Deep Learning Methods","This literature review studies the field of automated process extraction,
i.e., transforming textual descriptions into structured processes using Natural
Language Processing (NLP). We found that Machine Learning (ML) / Deep Learning
(DL) methods are being increasingly used for the NLP component. In some cases,
they were chosen for their suitability towards process extraction, and results
show that they can outperform classic rule-based methods. We also found a
paucity of gold-standard, scalable annotated datasets, which currently hinders
objective evaluations as well as the training or fine-tuning of ML / DL
methods. Finally, we discuss preliminary work on the application of LLMs for
automated process extraction, as well as promising developments in this field.",2024-09-10,"William Van Woensel, Soroor Motie",http://arxiv.org/pdf/2409.13738v1,cs.CL
Exploring syntactic information in sentence embeddings through multilingual subject-verb agreement,"In this paper, our goal is to investigate to what degree multilingual
pretrained language models capture cross-linguistically valid abstract
linguistic representations. We take the approach of developing curated
synthetic data on a large scale, with specific properties, and using them to
study sentence representations built using pretrained language models. We use a
new multiple-choice task and datasets, Blackbird Language Matrices (BLMs), to
focus on a specific grammatical structural phenomenon -- subject-verb agreement
across a variety of sentence structures -- in several languages. Finding a
solution to this task requires a system detecting complex linguistic patterns
and paradigms in text representations. Using a two-level architecture that
solves the problem in two steps -- detect syntactic objects and their
properties in individual sentences, and find patterns across an input sequence
of sentences -- we show that despite having been trained on multilingual texts
in a consistent manner, multilingual pretrained language models have
language-specific differences, and syntactic structure is not shared, even
across closely related languages.",2024-09-10,"Vivi Nastase, Chunyang Jiang, Giuseppe Samo, Paola Merlo",http://arxiv.org/pdf/2409.06567v2,cs.CL
From LIMA to DeepLIMA: following a new path of interoperability,"In this article, we describe the architecture of the LIMA (Libre Multilingual
Analyzer) framework and its recent evolution with the addition of new text
analysis modules based on deep neural networks. We extended the functionality
of LIMA in terms of the number of supported languages while preserving existing
configurable architecture and the availability of previously developed
rule-based and statistical analysis components. Models were trained for more
than 60 languages on the Universal Dependencies 2.5 corpora, WikiNer corpora,
and CoNLL-03 dataset. Universal Dependencies allowed us to increase the number
of supported languages and to generate models that could be integrated into
other platforms. This integration of ubiquitous Deep Learning Natural Language
Processing models and the use of standard annotated collections using Universal
Dependencies can be viewed as a new path of interoperability, through the
normalization of models and data, that are complementary to a more standard
technical interoperability, implemented in LIMA through services available in
Docker containers on Docker Hub.",2024-09-10,"Victor Bocharov, Romaric Besançon, Gaël de Chalendar, Olivier Ferret, Nasredine Semmar",http://arxiv.org/pdf/2409.06550v1,cs.CL
Mapping News Narratives Using LLMs and Narrative-Structured Text Embeddings,"Given the profound impact of narratives across various societal levels, from
personal identities to international politics, it is crucial to understand
their distribution and development over time. This is particularly important in
online spaces. On the Web, narratives can spread rapidly and intensify societal
divides and conflicts. While many qualitative approaches exist, quantifying
narratives remains a significant challenge. Computational narrative analysis
lacks frameworks that are both comprehensive and generalizable. To address this
gap, we introduce a numerical narrative representation grounded in
structuralist linguistic theory. Chiefly, Greimas' Actantial Model represents a
narrative through a constellation of six functional character roles. These
so-called actants are genre-agnostic, making the model highly generalizable. We
extract the actants using an open-source LLM and integrate them into a
Narrative-Structured Text Embedding that captures both the semantics and
narrative structure of a text. We demonstrate the analytical insights of the
method on the example of 5000 full-text news articles from Al Jazeera and The
Washington Post on the Israel-Palestine conflict. Our method successfully
distinguishes articles that cover the same topics but differ in narrative
structure.",2024-09-10,Jan Elfes,http://arxiv.org/pdf/2409.06540v1,cs.CL
Questioning Internal Knowledge Structure of Large Language Models Through the Lens of the Olympic Games,"Large language models (LLMs) have become a dominant approach in natural
language processing, yet their internal knowledge structures remain largely
unexplored. In this paper, we analyze the internal knowledge structures of LLMs
using historical medal tallies from the Olympic Games. We task the models with
providing the medal counts for each team and identifying which teams achieved
specific rankings. Our results reveal that while state-of-the-art LLMs perform
remarkably well in reporting medal counts for individual teams, they struggle
significantly with questions about specific rankings. This suggests that the
internal knowledge structures of LLMs are fundamentally different from those of
humans, who can easily infer rankings from known medal counts. To support
further research, we publicly release our code, dataset, and model outputs.",2024-09-10,"Juhwan Choi, YoungBin Kim",http://arxiv.org/pdf/2409.06518v1,cs.CL
An Effective Context-Balanced Adaptation Approach for Long-Tailed Speech Recognition,"End-to-end (E2E) automatic speech recognition (ASR) models have become
standard practice for various commercial applications. However, in real-world
scenarios, the long-tailed nature of word distribution often leads E2E ASR
models to perform well on common words but fall short in recognizing uncommon
ones. Recently, the notion of a contextual adapter (CA) was proposed to infuse
external knowledge represented by a context word list into E2E ASR models.
Although CA can improve recognition performance on rare words, two crucial data
imbalance problems remain. First, when using low-frequency words as context
words during training, since these words rarely occur in the utterance, CA
becomes prone to overfit on attending to the <no-context> token due to
higher-frequency words not being present in the context list. Second, the
long-tailed distribution within the context list itself still causes the model
to perform poorly on low-frequency context words. In light of this, we explore
in-depth the impact of altering the context list to have words with different
frequency distributions on model performance, and meanwhile extend CA with a
simple yet effective context-balanced learning objective. A series of
experiments conducted on the AISHELL-1 benchmark dataset suggests that using
all vocabulary words from the training corpus as the context list and pairing
them with our balanced objective yields the best performance, demonstrating a
significant reduction in character error rate (CER) by up to 1.21% and a more
pronounced 9.44% reduction in the error rate of zero-shot words.",2024-09-10,"Yi-Cheng Wang, Li-Ting Pai, Bi-Cheng Yan, Hsin-Wei Wang, Chi-Han Lin, Berlin Chen",http://arxiv.org/pdf/2409.06468v1,cs.CL
HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training Data,"Large language models (LLMs) have shown great potential for automatic code
generation and form the basis for various tools such as GitHub Copilot.
However, recent studies highlight that many LLM-generated code contains serious
security vulnerabilities. While previous work tries to address this by training
models that generate secure code, these attempts remain constrained by limited
access to training data and labor-intensive data preparation.
  In this paper, we introduce HexaCoder, a novel approach to enhance the
ability of LLMs to generate secure codes by automatically synthesizing secure
codes, which reduces the effort of finding suitable training data. HexaCoder
comprises two key components: an oracle-guided data synthesis pipeline and a
two-step process for secure code generation. The data synthesis pipeline
generates pairs of vulnerable and fixed codes for specific Common Weakness
Enumeration (CWE) types by utilizing a state-of-the-art LLM for repairing
vulnerable code. A security oracle identifies vulnerabilities, and a
state-of-the-art LLM repairs them by extending and/or editing the codes,
creating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA)
method. Each example of our fine-tuning dataset includes the necessary
security-related libraries and code that form the basis of our novel two-step
generation approach. This allows the model to integrate security-relevant
libraries before generating the main code, significantly reducing the number of
generated vulnerable codes by up to 85% compared to the baseline methods. We
perform extensive evaluations on three different benchmarks for four LLMs,
demonstrating that HexaCoder not only improves the security of the generated
code but also maintains a high level of functional correctness.",2024-09-10,"Hossein Hajipour, Lea Schönherr, Thorsten Holz, Mario Fritz",http://arxiv.org/pdf/2409.06446v1,cs.CL
Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for Scholarly Knowledge Organization,"The increasing amount of published scholarly articles, exceeding 2.5 million
yearly, raises the challenge for researchers in following scientific progress.
Integrating the contributions from scholarly articles into a novel type of
cognitive knowledge graph (CKG) will be a crucial element for accessing and
organizing scholarly knowledge, surpassing the insights provided by titles and
abstracts. This research focuses on effectively conveying structured scholarly
knowledge by utilizing large language models (LLMs) to categorize scholarly
articles and describe their contributions in a structured and comparable
manner. While previous studies explored language models within specific
research domains, the extensive domain-independent knowledge captured by LLMs
offers a substantial opportunity for generating structured contribution
descriptions as CKGs. Additionally, LLMs offer customizable pathways through
prompt engineering or fine-tuning, thus facilitating to leveraging of smaller
LLMs known for their efficiency, cost-effectiveness, and environmental
considerations. Our methodology involves harnessing LLM knowledge, and
complementing it with domain expert-verified scholarly data sourced from a CKG.
This strategic fusion significantly enhances LLM performance, especially in
tasks like scholarly article categorization and predicate recommendation. Our
method involves fine-tuning LLMs with CKG knowledge and additionally injecting
knowledge from a CKG with a novel prompting technique significantly increasing
the accuracy of scholarly knowledge extraction. We integrated our approach in
the Open Research Knowledge Graph (ORKG), thus enabling precise access to
organized scholarly knowledge, crucially benefiting domain-independent
scholarly knowledge exchange and dissemination among policymakers, industrial
practitioners, and the general public.",2024-09-10,"Gollam Rabby, Sören Auer, Jennifer D'Souza, Allard Oelen",http://arxiv.org/pdf/2409.06433v1,cs.CL
How Redundant Is the Transformer Stack in Speech Representation Models?,"Self-supervised speech representation models, particularly those leveraging
transformer architectures, have demonstrated remarkable performance across
various tasks such as speech recognition, speaker identification, and emotion
detection. Recent studies on transformer models revealed a high redundancy
between layers and the potential for significant pruning, which we will
investigate here for transformer-based speech representation models. We perform
a detailed analysis of layer similarity in speech representation models using
three similarity metrics: cosine similarity, centered kernel alignment, and
mutual nearest-neighbor alignment. Our findings reveal a block-like structure
of high similarity, suggesting two main processing steps and significant
redundancy of layers. We demonstrate the effectiveness of pruning
transformer-based speech representation models without the need for
post-training, achieving up to 40% reduction in transformer layers while
maintaining over 95% of the model's predictive capacity. Furthermore, we employ
a knowledge distillation method to substitute the entire transformer stack with
mimicking layers, reducing the network size 95-98% and the inference time by up
to 94%. This substantial decrease in computational load occurs without
considerable performance loss, suggesting that the transformer stack is almost
completely redundant for downstream applications of speech representation
models.",2024-09-10,"Teresa Dorszewski, Albert Kjøller Jacobsen, Lenka Tětková, Lars Kai Hansen",http://arxiv.org/pdf/2409.16302v2,cs.CL
Length Desensitization in Direct Preference Optimization,"Direct Preference Optimization (DPO) is widely utilized in the Reinforcement
Learning from Human Feedback (RLHF) phase to align Large Language Models (LLMs)
with human preferences, thereby enhancing both their harmlessness and efficacy.
However, it has been observed that DPO tends to over-optimize for verbosity,
which can detrimentally affect both performance and user experience. In this
paper, we conduct an in-depth theoretical analysis of DPO's optimization
objective and reveal a strong correlation between its implicit reward and data
length. This correlation misguides the optimization direction, resulting in
length sensitivity during the DPO training and leading to verbosity. To address
this issue, we propose a length-desensitization improvement method for DPO,
termed LD-DPO. The proposed method aims to desensitize DPO to data length by
decoupling explicit length preference, which is relatively insignificant, from
the other implicit preferences, thereby enabling more effective learning of the
intrinsic preferences. We utilized two settings (Base and Instruct) of
Llama2-13B, Llama3-8B, and Qwen2-7B for experimental validation on various
benchmarks including MT-Bench and AlpacaEval 2. The experimental results
indicate that LD-DPO consistently outperforms DPO and other baseline methods,
achieving more concise responses with a 10-40% reduction in length compared to
DPO. We conducted in-depth experimental analyses to demonstrate that LD-DPO can
indeed achieve length desensitization and align the model more closely with
human-like preferences.",2024-09-10,"Wei Liu, Yang Bai, Chengcheng Han, Rongxiang Weng, Jun Xu, Xuezhi Cao, Jingang Wang, Xunliang Cai",http://arxiv.org/pdf/2409.06411v2,cs.CL
Coarse-Grained Sense Inventories Based on Semantic Matching between English Dictionaries,"WordNet is one of the largest handcrafted concept dictionaries visualizing
word connections through semantic relationships. It is widely used as a word
sense inventory in natural language processing tasks. However, WordNet's
fine-grained senses have been criticized for limiting its usability. In this
paper, we semantically match sense definitions from Cambridge dictionaries and
WordNet and develop new coarse-grained sense inventories. We verify the
effectiveness of our inventories by comparing their semantic coherences with
that of Coarse Sense Inventory. The advantages of the proposed inventories
include their low dependency on large-scale resources, better aggregation of
closely related senses, CEFR-level assignments, and ease of expansion and
improvement.",2024-09-10,"Masato Kikuchi, Masatsugu Ono, Toshioki Soga, Tetsu Tanabe, Tadachika Ozono",http://arxiv.org/pdf/2409.06386v1,cs.CL
Enhancing Sequential Recommendations through Multi-Perspective Reflections and Iteration,"Sequence recommendation (SeqRec) aims to predict the next item a user will
interact with by understanding user intentions and leveraging collaborative
filtering information. Large language models (LLMs) have shown great promise in
recommendation tasks through prompt-based, fixed reflection libraries, and
fine-tuning techniques. However, these methods face challenges, including lack
of supervision, inability to optimize reflection sources, inflexibility to
diverse user needs, and high computational costs. Despite promising results,
current studies primarily focus on reflections of users' explicit preferences
(e.g., item titles) while neglecting implicit preferences (e.g., brands) and
collaborative filtering information. This oversight hinders the capture of
preference shifts and dynamic user behaviors. Additionally, existing approaches
lack mechanisms for reflection evaluation and iteration, often leading to
suboptimal recommendations. To address these issues, we propose the Mixture of
REflectors (MoRE) framework, designed to model and learn dynamic user
preferences in SeqRec. Specifically, MoRE introduces three reflectors for
generating LLM-based reflections on explicit preferences, implicit preferences,
and collaborative signals. Each reflector incorporates a self-improving
strategy, termed refining-and-iteration, to evaluate and iteratively update
reflections. Furthermore, a meta-reflector employs a contextual bandit
algorithm to select the most suitable expert and corresponding reflections for
each user's recommendation, effectively capturing dynamic preferences.
Extensive experiments on three real-world datasets demonstrate that MoRE
consistently outperforms state-of-the-art methods, requiring less training time
and GPU memory compared to other LLM-based approaches in SeqRec.",2024-09-10,"Weicong Qin, Yi Xu, Weijie Yu, Chenglei Shen, Xiao Zhang, Ming He, Jianping Fan, Jun Xu",http://arxiv.org/pdf/2409.06377v1,cs.CL
SpeechTaxi: On Multilingual Semantic Speech Classification,"Recent advancements in multilingual speech encoding as well as transcription
raise the question of the most effective approach to semantic speech
classification. Concretely, can (1) end-to-end (E2E) classifiers obtained by
fine-tuning state-of-the-art multilingual speech encoders (MSEs) match or
surpass the performance of (2) cascading (CA), where speech is first
transcribed into text and classification is delegated to a text-based
classifier. To answer this, we first construct SpeechTaxi, an 80-hour
multilingual dataset for semantic speech classification of Bible verses,
covering 28 diverse languages. We then leverage SpeechTaxi to conduct a wide
range of experiments comparing E2E and CA in monolingual semantic speech
classification as well as in cross-lingual transfer. We find that E2E based on
MSEs outperforms CA in monolingual setups, i.e., when trained on in-language
data. However, MSEs seem to have poor cross-lingual transfer abilities, with
E2E substantially lagging CA both in (1) zero-shot transfer to languages unseen
in training and (2) multilingual training, i.e., joint training on multiple
languages. Finally, we devise a novel CA approach based on transcription to
Romanized text as a language-agnostic intermediate representation and show that
it represents a robust solution for languages without native ASR support. Our
SpeechTaxi dataset is publicly available at: https://huggingface.co/
datasets/LennartKeller/SpeechTaxi/.",2024-09-10,"Lennart Keller, Goran Glavaš",http://arxiv.org/pdf/2409.06372v1,cs.CL
Retrieval Or Holistic Understanding? Dolce: Differentiate Our Long Context Evaluation Tasks,"We argue that there are two major distinct capabilities in long context
understanding: retrieval and holistic understanding. Understanding and further
improving LLMs' long context capabilities would not be possible without knowing
the tasks' focus categories. We aim to automatically identify retrieval focused
and holistic understanding focused problems from suites of benchmarks and
quantitatively measure the difficulty within each focus. In this paper, we
present the Dolce framework, which parameterizes each problem by $\lambda$
(complexity) and $k$ (redundancy) and assigns to one of five predefined focus
categories. We propose to sample short contexts from the full context and
estimate the probability an LLM solves the problem using the sampled spans. To
find the $\lambda$ and $k$ for each problem, we further propose a mixture model
of a non-parametric background noise component and a parametric/non-parametric
hybrid oracle component, where we derive the probability functions
parameterized by $\lambda$ and $k$ for both the correct-or-wrong (COW) scenario
and the partial-point-in-grading (PIG) scenario. Our proposed methods can
identify 0% to 67% of the problems are retrieval focused and 0% to 90% of the
problems are holistic understanding focused across 44 existing long context
evaluation tasks.",2024-09-10,Zi Yang,http://arxiv.org/pdf/2409.06338v1,cs.CL
Extracting Paragraphs from LLM Token Activations,"Generative large language models (LLMs) excel in natural language processing
tasks, yet their inner workings remain underexplored beyond token-level
predictions. This study investigates the degree to which these models decide
the content of a paragraph at its onset, shedding light on their contextual
understanding. By examining the information encoded in single-token
activations, specifically the ""\textbackslash n\textbackslash n"" double newline
token, we demonstrate that patching these activations can transfer significant
information about the context of the following paragraph, providing further
insights into the model's capacity to plan ahead.",2024-09-10,"Nicholas Pochinkov, Angelo Benoit, Lovkush Agarwal, Zainab Ali Majid, Lucile Ter-Minassian",http://arxiv.org/pdf/2409.06328v1,cs.CL
Analysis of Socially Unacceptable Discourse with Zero-shot Learning,"Socially Unacceptable Discourse (SUD) analysis is crucial for maintaining
online positive environments. We investigate the effectiveness of
Entailment-based zero-shot text classification (unsupervised method) for SUD
detection and characterization by leveraging pre-trained transformer models and
prompting techniques. The results demonstrate good generalization capabilities
of these models to unseen data and highlight the promising nature of this
approach for generating labeled datasets for the analysis and characterization
of extremist narratives. The findings of this research contribute to the
development of robust tools for studying SUD and promoting responsible
communication online.",2024-09-10,"Rayane Ghilene, Dimitra Niaouri, Michele Linardi, Julien Longhi",http://arxiv.org/pdf/2409.13735v1,cs.CL
Keyword-Aware ASR Error Augmentation for Robust Dialogue State Tracking,"Dialogue State Tracking (DST) is a key part of task-oriented dialogue
systems, identifying important information in conversations. However, its
accuracy drops significantly in spoken dialogue environments due to named
entity errors from Automatic Speech Recognition (ASR) systems. We introduce a
simple yet effective data augmentation method that targets those entities to
improve the robustness of DST model. Our novel method can control the placement
of errors using keyword-highlighted prompts while introducing phonetically
similar errors. As a result, our method generated sufficient error patterns on
keywords, leading to improved accuracy in noised and low-accuracy ASR
environments.",2024-09-10,"Jihyun Lee, Solee Im, Wonjun Lee, Gary Geunbae Lee",http://arxiv.org/pdf/2409.06263v1,cs.CL
Inference is All You Need: Self Example Retriever for Cross-domain Dialogue State Tracking with ChatGPT,"Traditional dialogue state tracking approaches heavily rely on extensive
training data and handcrafted features, limiting their scalability and
adaptability to new domains. In this paper, we propose a novel method that
leverages inference and in-context learning with ChatGPT for domain transfer in
dialogue state tracking, without any parameter updates. By guiding ChatGPT's
chain of thought, we enable it to retrieve relevant examples and generalize
knowledge to accurately infer dialogue states, solely through inference.
Experimental results on the MultiWOZ dataset demonstrate competitive
performance and promising generalization across domains. Our parameter-free
approach offers a scalable and adaptable solution, opening new research
directions in domain transfer learning.",2024-09-10,"Jihyun Lee, Gary Geunbae Lee",http://arxiv.org/pdf/2409.06243v1,cs.CL
Enhancing Kurdish Text-to-Speech with Native Corpus Training: A High-Quality WaveGlow Vocoder Approach,"The ability to synthesize spoken language from text has greatly facilitated
access to digital content with the advances in text-to-speech technology.
However, effective TTS development for low-resource languages, such as Central
Kurdish (CKB), still faces many challenges due mainly to the lack of linguistic
information and dedicated resources. In this paper, we improve the Kurdish TTS
system based on Tacotron by training the Kurdish WaveGlow vocoder on a 21-hour
central Kurdish speech corpus instead of using a pre-trained English vocoder
WaveGlow. Vocoder training on the target language corpus is required to
accurately and fluently adapt phonetic and prosodic changes in Kurdish
language. The effectiveness of these enhancements is that our model is
significantly better than the baseline system with English pretrained models.
In particular, our adaptive WaveGlow model achieves an impressive MOS of 4.91,
which sets a new benchmark for Kurdish speech synthesis. On one hand, this
study empowers the advanced features of the TTS system for Central Kurdish, and
on the other hand, it opens the doors for other dialects in Kurdish and other
related languages to further develop.",2024-09-10,"Abdulhady Abas Abdullah, Sabat Salih Muhamad, Hadi Veisi",http://arxiv.org/pdf/2409.13734v2,cs.CL
RNR: Teaching Large Language Models to Follow Roles and Rules,"Instruction fine-tuning (IFT) elicits instruction following capabilities and
steers the behavior of large language models (LLMs) via supervised learning.
However, existing models trained on open-source IFT datasets only have the
ability to follow instructions from users, and often fail to follow complex
role and rules specified by developers, a.k.a. system prompts. The ability to
follow these roles and rules is essential for deployment, as it ensures that
the model safely interacts with users within developer defined guidelines. To
improve such role and rule following ability, we propose \model, an automated
data generation pipeline that generates diverse roles and rules from existing
IFT instructions, along with corresponding responses. This data can then be
used to train models that follow complex system prompts. The models are
evaluated on our newly created benchmarks for role and rule following ability,
as well as standard instruction-following benchmarks and general NLP tasks. Our
framework significantly improves role and rule following capability in LLMs, as
evidenced by over 25% increase in pass-rate on rule adherence, i.e. following
all requirements, in our experiments with the Alpaca and Ultrachat datasets.
Moreover, our models achieves this increase without any regression on popular
instruction following benchmarks.",2024-09-10,"Kuan Wang, Alexander Bukharin, Haoming Jiang, Qingyu Yin, Zhengyang Wang, Tuo Zhao, Jingbo Shang, Chao Zhang, Bing Yin, Xian Li, Jianshu Chen, Shiyang Li",http://arxiv.org/pdf/2409.13733v1,cs.CL
Enhancing Large Language Models with Domain-Specific Knowledge: The Case in Topological Materials,"Large language models (LLMs), such as ChatGPT, have demonstrated impressive
performance in the text generation task, showing the ability to understand and
respond to complex instructions. However, the performance of naive LLMs in
speciffc domains is limited due to the scarcity of domain-speciffc corpora and
specialized training. Moreover, training a specialized large-scale model
necessitates signiffcant hardware resources, which restricts researchers from
leveraging such models to drive advances. Hence, it is crucial to further
improve and optimize LLMs to meet speciffc domain demands and enhance their
scalability. Based on the condensed matter data center, we establish a material
knowledge graph (MaterialsKG) and integrate it with literature. Using large
language models and prompt learning, we develop a specialized dialogue system
for topological materials called TopoChat. Compared to naive LLMs, TopoChat
exhibits superior performance in structural and property querying, material
recommendation, and complex relational reasoning. This system enables efffcient
and precise retrieval of information and facilitates knowledge interaction,
thereby encouraging the advancement on the ffeld of condensed matter materials.",2024-09-10,"HuangChao Xu, Baohua Zhang, Zhong Jin, Tiannian Zhu, Quansheng Wu, Hongming Weng",http://arxiv.org/pdf/2409.13732v2,cs.CL
NLP-Powered Repository and Search Engine for Academic Papers: A Case Study on Cyber Risk Literature with CyLit,"As the body of academic literature continues to grow, researchers face
increasing difficulties in effectively searching for relevant resources.
Existing databases and search engines often fall short of providing a
comprehensive and contextually relevant collection of academic literature. To
address this issue, we propose a novel framework that leverages Natural
Language Processing (NLP) techniques. This framework automates the retrieval,
summarization, and clustering of academic literature within a specific research
domain. To demonstrate the effectiveness of our approach, we introduce CyLit,
an NLP-powered repository specifically designed for the cyber risk literature.
CyLit empowers researchers by providing access to context-specific resources
and enabling the tracking of trends in the dynamic and rapidly evolving field
of cyber risk. Through the automatic processing of large volumes of data, our
NLP-powered solution significantly enhances the efficiency and specificity of
academic literature searches. We compare the literature categorization results
of CyLit to those presented in survey papers or generated by ChatGPT,
highlighting the distinctive insights this tool provides into cyber risk
research literature. Using NLP techniques, we aim to revolutionize the way
researchers discover, analyze, and utilize academic resources, ultimately
fostering advancements in various domains of knowledge.",2024-09-10,"Linfeng Zhang, Changyue Hu, Zhiyu Quan",http://arxiv.org/pdf/2409.06226v1,cs.CL
Enhancing Temporal Understanding in Audio Question Answering for Large Audio Language Models,"The Audio Question Answering (AQA) task includes audio event classification,
audio captioning, and open-ended reasoning. Recently, AQA has garnered
attention due to the advent of Large Audio Language Models (LALMs). Current
literature focuses on constructing LALMs by integrating audio encoders with
text-only Large Language Models (LLMs) through a projection module. While LALMs
excel in general audio understanding, they are limited in temporal reasoning,
which may hinder their commercial applications and on-device deployment. This
paper addresses these challenges and limitations in audio temporal reasoning.
First, we introduce a data augmentation technique for generating reliable audio
temporal questions and answers using an LLM. Second, we perform a further
fine-tuning of an existing baseline using curriculum learning strategy to
specialize in temporal reasoning without compromising performance on fine-tuned
tasks. We demonstrate the performance of our model using state-of-the-art LALMs
on public audio benchmark datasets. Third, we implement our AQA model on-device
locally and investigate its CPU inference for edge applications.",2024-09-10,"Arvind Krishna Sridhar, Yinyi Guo, Erik Visser",http://arxiv.org/pdf/2409.06223v3,cs.CL
Advancing Topic Segmentation of Broadcasted Speech with Multilingual Semantic Embeddings,"Recent advancements in speech-based topic segmentation have highlighted the
potential of pretrained speech encoders to capture semantic representations
directly from speech. Traditionally, topic segmentation has relied on a
pipeline approach in which transcripts of the automatic speech recognition
systems are generated, followed by text-based segmentation algorithms. In this
paper, we introduce an end-to-end scheme that bypasses this conventional
two-step process by directly employing semantic speech encoders for
segmentation. Focused on the broadcasted news domain, which poses unique
challenges due to the diversity of speakers and topics within single
recordings, we address the challenge of accessing topic change points
efficiently in an end-to-end manner. Furthermore, we propose a new benchmark
for spoken news topic segmentation by utilizing a dataset featuring
approximately 1000 hours of publicly available recordings across six European
languages and including an evaluation set in Hindi to test the model's
cross-domain performance in a cross-lingual, zero-shot scenario. This setup
reflects real-world diversity and the need for models adapting to various
linguistic settings. Our results demonstrate that while the traditional
pipeline approach achieves a state-of-the-art $P_k$ score of 0.2431 for
English, our end-to-end model delivers a competitive $P_k$ score of 0.2564.
When trained multilingually, these scores further improve to 0.1988 and 0.2370,
respectively. To support further research, we release our model along with data
preparation scripts, facilitating open research on multilingual spoken news
topic segmentation.",2024-09-10,"Sakshi Deo Shukla, Pavel Denisov, Tugtekin Turan",http://arxiv.org/pdf/2409.06222v1,cs.CL
SubRegWeigh: Effective and Efficient Annotation Weighing with Subword Regularization,"NLP datasets may still contain annotation errors, even when they are manually
annotated. Researchers have attempted to develop methods to automatically
reduce the adverse effect of errors in datasets. However, existing methods are
time-consuming because they require many trained models to detect errors. This
paper proposes a time-saving method that utilizes a tokenization technique
called subword regularization to simulate multiple error detection models for
detecting errors. Our proposed method, SubRegWeigh, can perform annotation
weighting four to five times faster than the existing method. Additionally,
SubRegWeigh improved performance in document classification and named entity
recognition tasks. In experiments with pseudo-incorrect labels, SubRegWeigh
clearly identifies pseudo-incorrect labels as annotation errors. Our code is
available at https://github.com/4ldk/SubRegWeigh .",2024-09-10,"Kohei Tsuji, Tatsuya Hiraoka, Yuchang Cheng, Tomoya Iwakura",http://arxiv.org/pdf/2409.06216v2,cs.CL
STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning,"Mixture-of-experts (MoEs) have been adopted for reducing inference costs by
sparsely activating experts in Large language models (LLMs). Despite this
reduction, the massive number of experts in MoEs still makes them expensive to
serve. In this paper, we study how to address this, by pruning MoEs. Among
pruning methodologies, unstructured pruning has been known to achieve the
highest performance for a given pruning ratio, compared to structured pruning,
since the latter imposes constraints on the sparsification structure. This is
intuitive, as the solution space of unstructured pruning subsumes that of
structured pruning. However, our counterintuitive finding reveals that expert
pruning, a form of structured pruning, can actually precede unstructured
pruning to outperform unstructured-only pruning. As existing expert pruning,
requiring $O(\frac{k^n}{\sqrt{n}})$ forward passes for $n$ experts, cannot
scale for recent MoEs, we propose a scalable alternative with $O(1)$
complexity, yet outperforming the more expensive methods. The key idea is
leveraging a latent structure between experts, based on behavior similarity,
such that the greedy decision of whether to prune closely captures the joint
pruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized
MoE with 128 experts, our method needs only one H100 and two hours to achieve
nearly no loss in performance with 40% sparsity, even in generative tasks such
as GSM8K, where state-of-the-art unstructured pruning fails to. The code will
be made publicly available.",2024-09-10,"Jaeseong Lee, seung-won hwang, Aurick Qiao, Daniel F Campos, Zhewei Yao, Yuxiong He",http://arxiv.org/pdf/2409.06211v1,cs.CL
SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing Behaviors with LLMs,"This paper introduces text-to-shape-display, a novel approach to generating
dynamic shape changes in pin-based shape displays through natural language
commands. By leveraging large language models (LLMs) and AI-chaining, our
approach allows users to author shape-changing behaviors on demand through text
prompts without programming. We describe the foundational aspects necessary for
such a system, including the identification of key generative elements
(primitive, animation, and interaction) and design requirements to enhance user
interaction, based on formative exploration and iterative design processes.
Based on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a
24 x 24 shape display, which translates the user's textual command into
executable code and allows for quick exploration through a web-based control
interface. We evaluate the effectiveness of SHAPE-IT in two ways: 1)
performance evaluation and 2) user evaluation (N= 10). The study conclusions
highlight the ability to facilitate rapid ideation of a wide range of
shape-changing behaviors with AI. However, the findings also expose
accuracy-related challenges and limitations, prompting further exploration into
refining the framework for leveraging AI to better suit the unique requirements
of shape-changing systems.",2024-09-10,"Wanli Qian, Chenfeng Gao, Anup Sathya, Ryo Suzuki, Ken Nakagaki",http://arxiv.org/pdf/2409.06205v1,cs.CL
NOVI : Chatbot System for University Novice with BERT and LLMs,"To mitigate the difficulties of university freshmen in adapting to university
life, we developed NOVI, a chatbot system based on GPT-4o. This system utilizes
post and comment data from SKKU 'Everytime', a university community site.
Developed using LangChain, NOVI's performance has been evaluated with a BLEU
score, Perplexity score, ROUGE-1 score, ROUGE-2 score, ROUGE-L score and METEOR
score. This approach is not only limited to help university freshmen but is
also expected to help various people adapting to new environments with
different data. This research explores the development and potential
application of new educational technology tools, contributing to easier social
adaptation for beginners and settling a foundation for future advancement in
LLM studies.",2024-09-10,"Yoonji Nam, TaeWoong Seo, Gyeongcheol Shin, Sangji Lee, JaeEun Im",http://arxiv.org/pdf/2409.06192v1,cs.CL
Can Large Language Models Unlock Novel Scientific Research Ideas?,"""An idea is nothing more nor less than a new combination of old elements""
(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and
publicly available ChatGPT have marked a significant turning point in the
integration of Artificial Intelligence (AI) into people's everyday lives. This
study explores the capability of LLMs in generating novel research ideas based
on information from research papers. We conduct a thorough examination of 4
LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and
Physics). We found that the future research ideas generated by Claude-2 and
GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.
We also found that Claude-2 generates more diverse future research ideas than
GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the
novelty, relevancy, and feasibility of the generated future research ideas.
This investigation offers insights into the evolving role of LLMs in idea
generation, highlighting both its capability and limitations. Our work
contributes to the ongoing efforts in evaluating and utilizing language models
for generating future research ideas. We make our datasets and codes publicly
available.",2024-09-10,"Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal",http://arxiv.org/pdf/2409.06185v1,cs.CL
SQLucid: Grounding Natural Language Database Queries with Interactive Explanations,"Though recent advances in machine learning have led to significant
improvements in natural language interfaces for databases, the accuracy and
reliability of these systems remain limited, especially in high-stakes domains.
This paper introduces SQLucid, a novel user interface that bridges the gap
between non-expert users and complex database querying processes. SQLucid
addresses existing limitations by integrating visual correspondence,
intermediate query results, and editable step-by-step SQL explanations in
natural language to facilitate user understanding and engagement. This unique
blend of features empowers users to understand and refine SQL queries easily
and precisely. Two user studies and one quantitative experiment were conducted
to validate SQLucid's effectiveness, showing significant improvement in task
completion accuracy and user confidence compared to existing interfaces. Our
code is available at https://github.com/magic-YuanTian/SQLucid.",2024-09-10,"Yuan Tian, Jonathan K. Kummerfeld, Toby Jia-Jun Li, Tianyi Zhang",http://arxiv.org/pdf/2409.06178v1,cs.CL
Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks,"In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the
dominant technique for performing natural language tasks, as it does not
require updating the model parameters with gradient-based methods. ICL promises
to ""adapt"" the LLM to perform the present task at a competitive or
state-of-the-art level at a fraction of the computational cost. ICL can be
augmented by incorporating the reasoning process to arrive at the final label
explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.
However, recent work has found that ICL relies mostly on the retrieval of task
priors and less so on ""learning"" to perform tasks, especially for complex
subjective domains like emotion and morality, where priors ossify posterior
predictions. In this work, we examine whether ""enabling"" reasoning also creates
the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors
that remain relatively unchanged despite the evidence in the prompt. We find
that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL
for larger language models. Code is avalaible at
https://github.com/gchochla/cot-priors.",2024-09-10,"Georgios Chochlakis, Niyantha Maruthu Pandiyan, Kristina Lerman, Shrikanth Narayanan",http://arxiv.org/pdf/2409.06173v3,cs.CL
Deep Learning and Large Language Models for Audio and Text Analysis in Predicting Suicidal Acts in Chinese Psychological Support Hotlines,"Suicide is a pressing global issue, demanding urgent and effective preventive
interventions. Among the various strategies in place, psychological support
hotlines had proved as a potent intervention method. Approximately two million
people in China attempt suicide annually, with many individuals making multiple
attempts. Prompt identification and intervention for high-risk individuals are
crucial to preventing tragedies. With the rapid advancement of artificial
intelligence (AI), especially the development of large-scale language models
(LLMs), new technological tools have been introduced to the field of mental
health. This study included 1284 subjects, and was designed to validate whether
deep learning models and LLMs, using audio and transcribed text from support
hotlines, can effectively predict suicide risk. We proposed a simple LLM-based
pipeline that first summarizes transcribed text from approximately one hour of
speech to extract key features, and then predict suicidial bahaviours in the
future. We compared our LLM-based method with the traditional manual scale
approach in a clinical setting and with five advanced deep learning models.
Surprisingly, the proposed simple LLM pipeline achieved strong performance on a
test set of 46 subjects, with an F1 score of 76\% when combined with manual
scale rating. This is 7\% higher than the best speech-based deep learning
models and represents a 27.82\% point improvement in F1 score compared to using
the manual scale apporach alone. Our study explores new applications of LLMs
and demonstrates their potential for future use in suicide prevention efforts.",2024-09-10,"Yining Chen, Jianqiang Li, Changwei Song, Qing Zhao, Yongsheng Tong, Guanghui Fu",http://arxiv.org/pdf/2409.06164v1,cs.CL
KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation,"The recently developed retrieval-augmented generation (RAG) technology has
enabled the efficient construction of domain-specific applications. However, it
also has limitations, including the gap between vector similarity and the
relevance of knowledge reasoning, as well as insensitivity to knowledge logic,
such as numerical values, temporal relations, expert rules, and others, which
hinder the effectiveness of professional knowledge services. In this work, we
introduce a professional domain knowledge service framework called Knowledge
Augmented Generation (KAG). KAG is designed to address the aforementioned
challenges with the motivation of making full use of the advantages of
knowledge graph(KG) and vector retrieval, and to improve generation and
reasoning performance by bidirectionally enhancing large language models (LLMs)
and KGs through five key aspects: (1) LLM-friendly knowledge representation,
(2) mutual-indexing between knowledge graphs and original chunks, (3)
logical-form-guided hybrid reasoning engine, (4) knowledge alignment with
semantic reasoning, and (5) model capability enhancement for KAG. We compared
KAG with existing RAG methods in multihop question answering and found that it
significantly outperforms state-of-theart methods, achieving a relative
improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We
have successfully applied KAG to two professional knowledge Q&A tasks of Ant
Group, including E-Government Q&A and E-Health Q&A, achieving significant
improvement in professionalism compared to RAG methods.",2024-09-10,"Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu, Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen, Wenguang Chen, Jun Zhou",http://arxiv.org/pdf/2409.13731v3,cs.CL
VisScience: An Extensive Benchmark for Evaluating K12 Educational Multi-modal Scientific Reasoning,"Multi-modal large language models (MLLMs) have demonstrated promising
capabilities across various tasks by integrating textual and visual information
to achieve visual understanding in complex scenarios. Despite the availability
of several benchmarks aims to evaluating MLLMs in tasks from visual question
answering to complex problem-solving, most focus predominantly on mathematics
or general visual understanding tasks. This reveals a critical gap in current
benchmarks, which often overlook the inclusion of other key scientific
disciplines such as physics and chemistry. To address this gap, we meticulously
construct a comprehensive benchmark, named VisScience, which is utilized to
assess the multi-modal scientific reasoning across the three disciplines of
mathematics, physics, and chemistry. This benchmark comprises 3,000 questions
drawn from K12 education - spanning elementary school through high school -
equally distributed across three disciplines, with 1,000 questions per
discipline. The questions within VisScience span 21 distinct subjects and are
categorized into five difficulty levels, offering a broad spectrum of topics
within each discipline. With VisScience, we present a detailed evaluation of
the performance of 25 representative MLLMs in scientific reasoning.
Experimental results demonstrate that closed-source MLLMs generally outperform
open-source models. The best performance observed include a 53.4\% accuracy in
mathematics by Claude3.5-Sonnet, 38.2\% in physics by GPT-4o, and 47.0\% in
chemistry by Gemini-1.5-Pro. These results underscore the strengths and
limitations of MLLMs, suggesting areas for future improvement and highlighting
the importance of developing models that can effectively handle the diverse
demands of multi-modal scientific reasoning.",2024-09-10,"Zhihuan Jiang, Zhen Yang, Jinhao Chen, Zhengxiao Du, Weihan Wang, Bin Xu, Jie Tang",http://arxiv.org/pdf/2409.13730v2,cs.CL
MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large Language Model,"Large language models (LLMs) have demonstrated significant capabilities in
mathematical reasoning, particularly with text-based mathematical problems.
However, current multi-modal large language models (MLLMs), especially those
specialized in mathematics, tend to focus predominantly on solving geometric
problems but ignore the diversity of visual information available in other
areas of mathematics. Moreover, the geometric information for these specialized
mathematical MLLMs is derived from several public datasets, which are typically
limited in diversity and complexity. To address these limitations, we aim to
construct a fine-tuning dataset named MathVL, and develop a series of
specialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised
Fine-Tuning (SFT) on MathVL with various parameter-scale backbones. To
extensively evaluate the effectiveness of MathGLM-Vision, we conduct
experiments on several public benchmarks and our curated MathVL-test consisting
of 2,000 problems. Experimental results demonstrate that MathGLM-Vision
achieves significant improvements compared with some existing models, including
backbone models and open-source mathematical MLLMs. These findings indicate the
importance of diversity dataset in enhancing the mathematical reasoning
abilities of MLLMs.",2024-09-10,"Zhen Yang, Jinhao Chen, Zhengxiao Du, Wenmeng Yu, Weihan Wang, Wenyi Hong, Zhihuan Jiang, Bin Xu, Jie Tang",http://arxiv.org/pdf/2409.13729v2,cs.CL
"Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review","Traditional Large Language Model (LLM) pretraining relies on autoregressive
language modeling with randomly sampled data from web-scale datasets. Inspired
by human learning techniques like spaced repetition, we hypothesize that random
sampling leads to high training costs, lower-quality models, and significant
data forgetting. To address these inefficiencies, we propose the
Learn-Focus-Review (LFR) paradigm -- a dynamic training approach that adapts to
the model's learning progress. LFR tracks the model's learning performance
across data blocks (sequences of tokens) and prioritizes revisiting challenging
regions of the dataset that are more prone to being forgotten, enabling better
retention and more efficient learning. Using the LFR paradigm, we pretrained
Llama and GPT models on the SlimPajama and OpenWebText datasets, respectively.
These models were evaluated on downstream tasks across various domains,
including question answering, problem-solving, commonsense reasoning, language
modeling, and translation. Compared to baseline models trained on the full
datasets, LFR consistently achieved lower perplexity and higher accuracy, while
using only 5%--19% of the training tokens. Furthermore, LFR matched the
performance of industry-standard Pythia models with up to 2$\times$ the
parameter count, using just 3.2% of the training tokens, demonstrating its
effectiveness and efficiency.",2024-09-10,"Neha Prakriya, Jui-Nan Yen, Cho-Jui Hsieh, Jason Cong",http://arxiv.org/pdf/2409.06131v2,cs.CL
Estimating the Completeness of Discrete Speech Units,"Representing speech with discrete units has been widely used in speech codec
and speech generation. However, there are several unverified claims about
self-supervised discrete units, such as disentangling phonetic and speaker
information with k-means, or assuming information loss after k-means. In this
work, we take an information-theoretic perspective to answer how much
information is present (information completeness) and how much information is
accessible (information accessibility), before and after residual vector
quantization. We show a lower bound for information completeness and estimate
completeness on discretized HuBERT representations after residual vector
quantization. We find that speaker information is sufficiently present in
HuBERT discrete units, and that phonetic information is sufficiently present in
the residual, showing that vector quantization does not achieve
disentanglement. Our results offer a comprehensive assessment on the choice of
discrete units, and suggest that a lot more information in the residual should
be mined rather than discarded.",2024-09-09,"Sung-Lin Yeh, Hao Tang",http://arxiv.org/pdf/2409.06109v2,cs.CL
Doppelgänger's Watch: A Split Objective Approach to Large Language Models,"In this paper, we investigate the problem of ""generation supervision"" in
large language models, and present a novel bicameral architecture to separate
supervision signals from their core capability, helpfulness. Doppelg\""anger, a
new module parallel to the underlying language model, supervises the generation
of each token, and learns to concurrently predict the supervision score(s) of
the sequences up to and including each token. In this work, we present the
theoretical findings, and leave the report on experimental results to a
forthcoming publication.",2024-09-09,"Shervin Ghasemlou, Ashish Katiyar, Aparajita Saraf, Seungwhan Moon, Mangesh Pujari, Pinar Donmez, Babak Damavandi, Anuj Kumar",http://arxiv.org/pdf/2409.06107v1,cs.CL
Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts,"LLMs show remarkable emergent abilities, such as inferring concepts from
presumably out-of-distribution prompts, known as in-context learning. Though
this success is often attributed to the Transformer architecture, our
systematic understanding is limited. In complex real-world data sets, even
defining what is out-of-distribution is not obvious. To better understand the
OOD behaviour of autoregressive LLMs, we focus on formal languages, which are
defined by the intersection of rules. We define a new scenario of OOD
compositional generalization, termed rule extrapolation. Rule extrapolation
describes OOD scenarios, where the prompt violates at least one rule. We
evaluate rule extrapolation in formal languages with varying complexity in
linear and recurrent architectures, the Transformer, and state space models to
understand the architectures' influence on rule extrapolation. We also lay the
first stones of a normative theory of rule extrapolation, inspired by the
Solomonoff prior in algorithmic information theory.",2024-09-09,"Anna Mészáros, Szilvia Ujváry, Wieland Brendel, Patrik Reizinger, Ferenc Huszár",http://arxiv.org/pdf/2409.13728v2,cs.CL
ClarQ-LLM: A Benchmark for Models Clarifying and Requesting Information in Task-Oriented Dialog,"We introduce ClarQ-LLM, an evaluation framework consisting of bilingual
English-Chinese conversation tasks, conversational agents and evaluation
metrics, designed to serve as a strong benchmark for assessing agents' ability
to ask clarification questions in task-oriented dialogues. The benchmark
includes 31 different task types, each with 10 unique dialogue scenarios
between information seeker and provider agents. The scenarios require the
seeker to ask questions to resolve uncertainty and gather necessary information
to complete tasks. Unlike traditional benchmarks that evaluate agents based on
fixed dialogue content, ClarQ-LLM includes a provider conversational agent to
replicate the original human provider in the benchmark. This allows both
current and future seeker agents to test their ability to complete information
gathering tasks through dialogue by directly interacting with our provider
agent. In tests, LLAMA3.1 405B seeker agent managed a maximum success rate of
only 60.05\%, showing that ClarQ-LLM presents a strong challenge for future
research.",2024-09-09,"Yujian Gan, Changling Li, Jinxia Xie, Luou Wen, Matthew Purver, Massimo Poesio",http://arxiv.org/pdf/2409.06097v2,cs.CL
Classification performance and reproducibility of GPT-4 omni for information extraction from veterinary electronic health records,"Large language models (LLMs) can extract information from veterinary
electronic health records (EHRs), but performance differences between models,
the effect of temperature settings, and the influence of text ambiguity have
not been previously evaluated. This study addresses these gaps by comparing the
performance of GPT-4 omni (GPT-4o) and GPT-3.5 Turbo under different conditions
and investigating the relationship between human interobserver agreement and
LLM errors. The LLMs and five humans were tasked with identifying six clinical
signs associated with Feline chronic enteropathy in 250 EHRs from a veterinary
referral hospital. At temperature 0, the performance of GPT-4o compared to the
majority opinion of human respondents, achieved 96.9% sensitivity
(interquartile range [IQR] 92.9-99.3%), 97.6% specificity (IQR 96.5-98.5%),
80.7% positive predictive value (IQR 70.8-84.6%), 99.5% negative predictive
value (IQR 99.0-99.9%), 84.4% F1 score (IQR 77.3-90.4%), and 96.3% balanced
accuracy (IQR 95.0-97.9%). The performance of GPT-4o was significantly better
than that of its predecessor, GPT-3.5 Turbo, particularly with respect to
sensitivity where GPT-3.5 Turbo only achieved 81.7% (IQR 78.9-84.8%). Adjusting
the temperature for GPT-4o did not significantly impact classification
performance. GPT-4o demonstrated greater reproducibility than human pairs
regardless of temperature, with an average Cohen's kappa of 0.98 (IQR
0.98-0.99) at temperature 0 compared to 0.8 (IQR 0.78-0.81) for humans. Most
GPT-4o errors occurred in instances where humans disagreed (35/43 errors,
81.4%), suggesting that these errors were more likely caused by ambiguity of
the EHR than explicit model faults. Using GPT-4o to automate information
extraction from veterinary EHRs is a viable alternative to manual extraction.",2024-09-09,"Judit M Wulcan, Kevin L Jacques, Mary Ann Lee, Samantha L Kovacs, Nicole Dausend, Lauren E Prince, Jonatan Wulcan, Sina Marsilio, Stefan M Keller",http://arxiv.org/pdf/2409.13727v1,cs.CL
DetoxBench: Benchmarking Large Language Models for Multitask Fraud & Abuse Detection,"Large language models (LLMs) have demonstrated remarkable capabilities in
natural language processing tasks. However, their practical application in
high-stake domains, such as fraud and abuse detection, remains an area that
requires further exploration. The existing applications often narrowly focus on
specific tasks like toxicity or hate speech detection. In this paper, we
present a comprehensive benchmark suite designed to assess the performance of
LLMs in identifying and mitigating fraudulent and abusive language across
various real-world scenarios. Our benchmark encompasses a diverse set of tasks,
including detecting spam emails, hate speech, misogynistic language, and more.
We evaluated several state-of-the-art LLMs, including models from Anthropic,
Mistral AI, and the AI21 family, to provide a comprehensive assessment of their
capabilities in this critical domain. The results indicate that while LLMs
exhibit proficient baseline performance in individual fraud and abuse detection
tasks, their performance varies considerably across tasks, particularly
struggling with tasks that demand nuanced pragmatic reasoning, such as
identifying diverse forms of misogynistic language. These findings have
important implications for the responsible development and deployment of LLMs
in high-risk applications. Our benchmark suite can serve as a tool for
researchers and practitioners to systematically evaluate LLMs for multi-task
fraud detection and drive the creation of more robust, trustworthy, and
ethically-aligned systems for fraud and abuse detection.",2024-09-09,"Joymallya Chakraborty, Wei Xia, Anirban Majumder, Dan Ma, Walid Chaabene, Naveed Janvekar",http://arxiv.org/pdf/2409.06072v1,cs.CL
MLLM-LLaVA-FL: Multimodal Large Language Model Assisted Federated Learning,"Previous studies on federated learning (FL) often encounter performance
degradation due to data heterogeneity among different clients. In light of the
recent advances in multimodal large language models (MLLMs), such as GPT-4v and
LLaVA, which demonstrate their exceptional proficiency in multimodal tasks,
such as image captioning and multimodal question answering. We introduce a
novel federated learning framework, named Multimodal Large Language Model
Assisted Federated Learning (MLLM-LLaVA-FL), which employs powerful MLLMs at
the server end to address the heterogeneous and long-tailed challenges. Owing
to the advanced cross-modality representation capabilities and the extensive
open-vocabulary prior knowledge of MLLMs, our framework is adept at harnessing
the extensive, yet previously underexploited, open-source data accessible from
websites and powerful server-side computational resources. Hence, the
MLLM-LLaVA-FL not only enhances the performance but also avoids increasing the
risk of privacy leakage and the computational burden on local devices,
distinguishing it from prior methodologies. Our framework has three key stages.
Initially, we conduct global visual-text pretraining of the model. This
pretraining is facilitated by utilizing the extensive open-source data
available online, with the assistance of MLLMs. Subsequently, the pretrained
model is distributed among various clients for local training. Finally, once
the locally trained models are transmitted back to the server, a global
alignment is carried out under the supervision of MLLMs to further enhance the
performance. Experimental evaluations on established benchmarks, show that our
framework delivers promising performance in the typical scenarios with data
heterogeneity and long-tail distribution across different clients in FL.",2024-09-09,"Jianyi Zhang, Hao Frank Yang, Ang Li, Xin Guo, Pu Wang, Haiming Wang, Yiran Chen, Hai Li",http://arxiv.org/pdf/2409.06067v2,cs.CL
Identifying the sources of ideological bias in GPT models through linguistic variation in output,"Extant work shows that generative AI models such as GPT-3.5 and 4 perpetuate
social stereotypes and biases. One concerning but less explored source of bias
is ideology. Do GPT models take ideological stances on politically sensitive
topics? In this article, we provide an original approach to identifying
ideological bias in generative models, showing that bias can stem from both the
training data and the filtering algorithm. We leverage linguistic variation in
countries with contrasting political attitudes to evaluate bias in average GPT
responses to sensitive political topics in those languages. First, we find that
GPT output is more conservative in languages that map well onto conservative
societies (i.e., Polish), and more liberal in languages used uniquely in
liberal societies (i.e., Swedish). This result provides strong evidence of
training data bias in GPT models. Second, differences across languages observed
in GPT-3.5 persist in GPT-4, even though GPT-4 is significantly more liberal
due to OpenAI's filtering policy. Our main takeaway is that generative model
training must focus on high-quality, curated datasets to reduce bias, even if
it entails a compromise in training data size. Filtering responses after
training only introduces new biases and does not remove the underlying training
biases.",2024-09-09,"Christina Walker, Joan C. Timoneda",http://arxiv.org/pdf/2409.06043v1,cs.CL
Investigating Causal Cues: Strengthening Spoofed Audio Detection with Human-Discernible Linguistic Features,"Several types of spoofed audio, such as mimicry, replay attacks, and
deepfakes, have created societal challenges to information integrity. Recently,
researchers have worked with sociolinguistics experts to label spoofed audio
samples with Expert Defined Linguistic Features (EDLFs) that can be discerned
by the human ear: pitch, pause, word-initial and word-final release bursts of
consonant stops, audible intake or outtake of breath, and overall audio
quality. It is established that there is an improvement in several deepfake
detection algorithms when they augmented the traditional and common features of
audio data with these EDLFs. In this paper, using a hybrid dataset comprised of
multiple types of spoofed audio augmented with sociolinguistic annotations, we
investigate causal discovery and inferences between the discernible linguistic
features and the label in the audio clips, comparing the findings of the causal
models with the expert ground truth validation labeling process. Our findings
suggest that the causal models indicate the utility of incorporating linguistic
features to help discern spoofed audio, as well as the overall need and
opportunity to incorporate human knowledge into models and techniques for
strengthening AI models. The causal discovery and inference can be used as a
foundation of training humans to discern spoofed audio as well as automating
EDLFs labeling for the purpose of performance improvement of the common
AI-based spoofed audio detectors.",2024-09-09,"Zahra Khanjani, Tolulope Ale, Jianwu Wang, Lavon Davis, Christine Mallinson, Vandana P. Janeja",http://arxiv.org/pdf/2409.06033v1,cs.CL
Improved Visually Prompted Keyword Localisation in Real Low-Resource Settings,"Given an image query, visually prompted keyword localisation (VPKL) aims to
find occurrences of the depicted word in a speech collection. This can be
useful when transcriptions are not available for a low-resource language (e.g.
if it is unwritten). Previous work showed that VPKL can be performed with a
visually grounded speech model trained on paired images and unlabelled speech.
But all experiments were done on English. Moreover, transcriptions were used to
get positive and negative pairs for the contrastive loss. This paper introduces
a few-shot learning scheme to mine pairs automatically without transcriptions.
On English, this results in only a small drop in performance. We also - for the
first time - consider VPKL on a real low-resource language, Yoruba. While
scores are reasonable, here we see a bigger drop in performance compared to
using ground truth pairs because the mining is less accurate in Yoruba.",2024-09-09,"Leanne Nortje, Dan Oneata, Herman Kamper",http://arxiv.org/pdf/2409.06013v1,cs.CL
TransformerRanker: A Tool for Efficiently Finding the Best-Suited Language Models for Downstream Classification Tasks,"Classification tasks in NLP are typically addressed by selecting a
pre-trained language model (PLM) from a model hub, and fine-tuning it for the
task at hand. However, given the very large number of PLMs that are currently
available, a practical challenge is to determine which of them will perform
best for a specific downstream task. With this paper, we introduce
TransformerRanker, a lightweight library that efficiently ranks PLMs for
classification tasks without the need for computationally costly fine-tuning.
Our library implements current approaches for transferability estimation
(LogME, H-Score, kNN), in combination with layer aggregation options, which we
empirically showed to yield state-of-the-art rankings of PLMs (Garbas et al.,
2024). We designed the interface to be lightweight and easy to use, allowing
users to directly connect to the HuggingFace Transformers and Dataset
libraries. Users need only select a downstream classification task and a list
of PLMs to create a ranking of likely best-suited PLMs for their task. We make
TransformerRanker available as a pip-installable open-source library
https://github.com/flairNLP/transformer-ranker.",2024-09-09,"Lukas Garbas, Max Ploner, Alan Akbik",http://arxiv.org/pdf/2409.05997v1,cs.CL
MessIRve: A Large-Scale Spanish Information Retrieval Dataset,"Information retrieval (IR) is the task of finding relevant documents in
response to a user query. Although Spanish is the second most spoken native
language, current IR benchmarks lack Spanish data, hindering the development of
information access tools for Spanish speakers. We introduce MessIRve, a
large-scale Spanish IR dataset with around 730 thousand queries from Google's
autocomplete API and relevant documents sourced from Wikipedia. MessIRve's
queries reflect diverse Spanish-speaking regions, unlike other datasets that
are translated from English or do not consider dialectal variations. The large
size of the dataset allows it to cover a wide variety of topics, unlike smaller
datasets. We provide a comprehensive description of the dataset, comparisons
with existing datasets, and baseline evaluations of prominent IR models. Our
contributions aim to advance Spanish IR research and improve information access
for Spanish speakers.",2024-09-09,"Francisco Valentini, Viviana Cotik, Damián Furman, Ivan Bercovich, Edgar Altszyler, Juan Manuel Pérez",http://arxiv.org/pdf/2409.05994v1,cs.CL
Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement,"Non-verbal behavior is a central challenge in understanding the dynamics of a
conversation and the affective states between interlocutors arising from the
interaction. Although psychological research has demonstrated that non-verbal
behaviors vary across cultures, limited computational analysis has been
conducted to clarify these differences and assess their impact on engagement
recognition. To gain a greater understanding of engagement and non-verbal
behaviors among a wide range of cultures and language spheres, in this study we
conduct a multilingual computational analysis of non-verbal features and
investigate their role in engagement and engagement prediction. To achieve this
goal, we first expanded the NoXi dataset, which contains interaction data from
participants living in France, Germany, and the United Kingdom, by collecting
session data of dyadic conversations in Japanese and Chinese, resulting in the
enhanced dataset NoXi+J. Next, we extracted multimodal non-verbal features,
including speech acoustics, facial expressions, backchanneling and gestures,
via various pattern recognition techniques and algorithms. Then, we conducted a
statistical analysis of listening behaviors and backchannel patterns to
identify culturally dependent and independent features in each language and
common features among multiple languages. These features were also correlated
with the engagement shown by the interlocutors. Finally, we analyzed the
influence of cultural differences in the input features of LSTM models trained
to predict engagement for five language datasets. A SHAP analysis combined with
transfer learning confirmed a considerable correlation between the importance
of input features for a language set and the significant cultural
characteristics analyzed.",2024-09-09,"Marius Funk, Shogo Okada, Elisabeth André",http://arxiv.org/pdf/2409.13726v1,cs.CL
Mathematical Formalized Problem Solving and Theorem Proving in Different Fields in Lean 4,"Formalizing mathematical proofs using computerized verification languages
like Lean 4 has the potential to significantly impact the field of mathematics,
it offers prominent capabilities for advancing mathematical reasoning. However,
existing efforts are largely limited to creating formalized versions of proofs
from extensive online mathematical corpora, struggling to keep pace with the
rapidly evolving nature of mathematics. To bridge the gap between traditional
and computerized proof techniques, this paper explores the use of Large
Language Models (LLMs) to generate formal proof steps and complete formalized
proofs. By converting natural language (NL) mathematical proofs into formalized
versions, this work introduces the basic structure and tactics of the Lean 4
language. The goal is to determine how AI can be leveraged to assist the
mathematical formalization process and improve its performance. Several
examples are provided that demonstrate solving problems using both traditional
and Lean 4-based approaches. Ultimately, this paper presents an explanation of
the foundations of Lean 4 and comparative analyses of the mathematical
formalization process using traditional and AI-augmented techniques. The
findings indicate that AI- powered tools have significant potential to
accelerate and enhance the formalization of mathematical proofs, paving the way
for more efficient and reliable theorem-proving for AI for Math in the future.",2024-09-09,Xichen Tang,http://arxiv.org/pdf/2409.05977v3,cs.CL
Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers,"In-context learning (ICL) is a cornerstone of large language model (LLM)
functionality, yet its theoretical foundations remain elusive due to the
complexity of transformer architectures. In particular, most existing work only
theoretically explains how the attention mechanism facilitates ICL under
certain data models. It remains unclear how the other building blocks of the
transformer contribute to ICL. To address this question, we study how a
two-attention-layer transformer is trained to perform ICL on $n$-gram Markov
chain data, where each token in the Markov chain statistically depends on the
previous $n$ tokens. We analyze a sophisticated transformer model featuring
relative positional embedding, multi-head softmax attention, and a feed-forward
layer with normalization. We prove that the gradient flow with respect to a
cross-entropy ICL loss converges to a limiting model that performs a
generalized version of the induction head mechanism with a learned feature,
resulting from the congruous contribution of all the building blocks. In the
limiting model, the first attention layer acts as a $\mathit{copier}$, copying
past tokens within a given window to each position, and the feed-forward
network with normalization acts as a $\mathit{selector}$ that generates a
feature vector by only looking at informationally relevant parents from the
window. Finally, the second attention layer is a $\mathit{classifier}$ that
compares these features with the feature at the output position, and uses the
resulting similarity scores to generate the desired output. Our theory is
further validated by experiments.",2024-09-09,"Siyu Chen, Heejune Sheen, Tianhao Wang, Zhuoran Yang",http://arxiv.org/pdf/2409.10559v1,cs.CL
A Small Claims Court for the NLP: Judging Legal Text Classification Strategies With Small Datasets,"Recent advances in language modelling has significantly decreased the need of
labelled data in text classification tasks. Transformer-based models,
pre-trained on unlabeled data, can outmatch the performance of models trained
from scratch for each task. However, the amount of labelled data need to
fine-tune such type of model is still considerably high for domains requiring
expert-level annotators, like the legal domain. This paper investigates the
best strategies for optimizing the use of a small labeled dataset and large
amounts of unlabeled data and perform a classification task in the legal area
with 50 predefined topics. More specifically, we use the records of demands to
a Brazilian Public Prosecutor's Office aiming to assign the descriptions in one
of the subjects, which currently demands deep legal knowledge for manual
filling. The task of optimizing the performance of classifiers in this scenario
is especially challenging, given the low amount of resources available
regarding the Portuguese language, especially in the legal domain. Our results
demonstrate that classic supervised models such as logistic regression and SVM
and the ensembles random forest and gradient boosting achieve better
performance along with embeddings extracted with word2vec when compared to BERT
language model. The latter demonstrates superior performance in association
with the architecture of the model itself as a classifier, having surpassed all
previous models in that regard. The best result was obtained with Unsupervised
Data Augmentation (UDA), which jointly uses BERT, data augmentation, and
strategies of semi-supervised learning, with an accuracy of 80.7% in the
aforementioned task.",2024-09-09,"Mariana Yukari Noguti, Edduardo Vellasques, Luiz Eduardo Soares Oliveira",http://arxiv.org/pdf/2409.05972v1,cs.CL
MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct,"The development of Multimodal Large Language Models (MLLMs) has seen
significant advancements with increasing demands in various fields (e.g.,
multimodal agents, embodied intelligence). While model-driven approaches
attempt to enhance MLLMs capabilities through diverse architectures, the gains
have become increasingly marginal. Conversely, data-driven methods, which scale
up image-text instruction data, are more effective but face limited data
diversity and complexity challenges. The absence of high-quality data
constitutes a significant development barrier for MLLMs. To address the data
quality bottleneck, we propose MMEvol, a novel multimodal instruction data
evolution framework. This framework iteratively improve data quality through a
refined combination of fine-grained perception, cognitive reasoning, and
interaction evolution, generating a more complex and diverse image-text
instruction dataset that empowers MLLMs with enhanced capabilities. Beginning
with an initial set of instructions, SEED-163K, we utilize MMEvol to
systematically broaden the diversity of instruction types, extend visual
reasoning steps to improve cognitive reasoning abilities, and thoroughly
explore fine-grained information within images to enhance visual understanding
and robustness. To comprehensively evaluate the effectiveness of our approach,
we conduct extensive qualitative analysis and quantitative experiments across
13 vision-language tasks. Compared to baseline models trained with the initial
seed data, the results demonstrate that our method achieves an average accuracy
improvement of 3.1 percentage points. Furthermore, our approach reaches
state-of-the-art (SOTA) performance in nine tasks using significantly less data
compared to state-of-the-art models.",2024-09-09,"Run Luo, Haonan Zhang, Longze Chen, Ting-En Lin, Xiong Liu, Yuchuan Wu, Min Yang, Minzheng Wang, Pengpeng Zeng, Lianli Gao, Heng Tao Shen, Yunshui Li, Xiaobo Xia, Fei Huang, Jingkuan Song, Yongbin Li",http://arxiv.org/pdf/2409.05840v5,cs.CL
Improving Pretraining Data Using Perplexity Correlations,"Quality pretraining data is often seen as the key to high-performance
language models. However, progress in understanding pretraining data has been
slow due to the costly pretraining runs required for data selection
experiments. We present a framework that avoids these costs and selects
high-quality pretraining data without any LLM training of our own. Our work is
based on a simple observation: LLM losses on many pretraining texts are
correlated with downstream benchmark performance, and selecting
high-correlation documents is an effective pretraining data selection method.
We build a new statistical framework for data selection centered around
estimates of perplexity-benchmark correlations and perform data selection using
a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of
thousands of web domains. In controlled pretraining experiments at the 160M
parameter scale on 8 benchmarks, our approach outperforms DSIR on every
benchmark, while matching the best data selector found in DataComp-LM, a
hand-engineered bigram classifier. We have now also updated this paper to
include results from preregistered experiments with new pretraining data on an
aggregation of 22 benchmarks up to the 1.4B scale, showing increasing
improvements of our method over others with more scale. A pip package with full
documentation can be found here:
https://github.com/TristanThrush/perplexity-correlations.",2024-09-09,"Tristan Thrush, Christopher Potts, Tatsunori Hashimoto",http://arxiv.org/pdf/2409.05816v2,cs.CL
"CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs","Chinese, as a linguistic system rich in depth and complexity, is
characterized by distinctive elements such as ancient poetry, proverbs, idioms,
and other cultural constructs. However, current Large Language Models (LLMs)
face limitations in these specialized domains, highlighting the need for the
development of comprehensive datasets that can assess, continuously update, and
progressively improve these culturally-grounded linguistic competencies through
targeted training optimizations. To address this gap, we introduce CKnowEdit,
the first-ever Chinese knowledge editing dataset designed to correct
linguistic, factual, and logical errors in LLMs. We collect seven types of
knowledge from a wide range of sources, including classical texts, idioms, and
content from Baidu Tieba Ruozhiba, taking into account the unique polyphony,
antithesis, and logical structures inherent in the Chinese language. By
analyzing this dataset, we highlight the challenges current LLMs face in
mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge
editing techniques reveals opportunities to advance the correction of Chinese
knowledge. Code and dataset are available at
https://github.com/zjunlp/EasyEdit.",2024-09-09,"Jizhan Fang, Tianhe Lu, Yunzhi Yao, Ziyan Jiang, Xin Xu, Ningyu Zhang, Huajun Chen",http://arxiv.org/pdf/2409.05806v3,cs.CL
PDAF: A Phonetic Debiasing Attention Framework For Speaker Verification,"Speaker verification systems are crucial for authenticating identity through
voice. Traditionally, these systems focus on comparing feature vectors,
overlooking the speech's content. However, this paper challenges this by
highlighting the importance of phonetic dominance, a measure of the frequency
or duration of phonemes, as a crucial cue in speaker verification. A novel
Phoneme Debiasing Attention Framework (PDAF) is introduced, integrating with
existing attention frameworks to mitigate biases caused by phonetic dominance.
PDAF adjusts the weighting for each phoneme and influences feature extraction,
allowing for a more nuanced analysis of speech. This approach paves the way for
more accurate and reliable identity authentication through voice. Furthermore,
by employing various weighting strategies, we evaluate the influence of
phonetic features on the efficacy of the speaker verification system.",2024-09-09,"Massa Baali, Abdulhamid Aldoobi, Hira Dhamyal, Rita Singh, Bhiksha Raj",http://arxiv.org/pdf/2409.05799v1,cs.CL
OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System,"Knowledge representation has been a central aim of AI since its inception.
Symbolic Knowledge Graphs (KGs) and neural Large Language Models (LLMs) can
both represent knowledge. KGs provide highly accurate and explicit knowledge
representation, but face scalability issue; while LLMs offer expansive coverage
of knowledge, but incur significant training costs and struggle with precise
and reliable knowledge manipulation. To this end, we introduce OneEdit, a
neural-symbolic prototype system for collaborative knowledge editing using
natural language, which facilitates easy-to-use knowledge management with KG
and LLM. OneEdit consists of three modules: 1) The Interpreter serves for user
interaction with natural language; 2) The Controller manages editing requests
from various users, leveraging the KG with rollbacks to handle knowledge
conflicts and prevent toxic knowledge attacks; 3) The Editor utilizes the
knowledge from the Controller to edit KG and LLM. We conduct experiments on two
new datasets with KGs which demonstrate that OneEdit can achieve superior
performance.",2024-09-09,"Ningyu Zhang, Zekun Xi, Yujie Luo, Peng Wang, Bozhong Tian, Yunzhi Yao, Jintian Zhang, Shumin Deng, Mengshu Sun, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen",http://arxiv.org/pdf/2409.07497v1,cs.CL
Evidence from fMRI Supports a Two-Phase Abstraction Process in Language Models,"Research has repeatedly demonstrated that intermediate hidden states
extracted from large language models are able to predict measured brain
response to natural language stimuli. Yet, very little is known about the
representation properties that enable this high prediction performance. Why is
it the intermediate layers, and not the output layers, that are most capable
for this unique and highly general transfer task? In this work, we show that
evidence from language encoding models in fMRI supports the existence of a
two-phase abstraction process within LLMs. We use manifold learning methods to
show that this abstraction process naturally arises over the course of training
a language model and that the first ""composition"" phase of this abstraction
process is compressed into fewer layers as training continues. Finally, we
demonstrate a strong correspondence between layerwise encoding performance and
the intrinsic dimensionality of representations from LLMs. We give initial
evidence that this correspondence primarily derives from the inherent
compositionality of LLMs and not their next-word prediction properties.",2024-09-09,"Emily Cheng, Richard J. Antonello",http://arxiv.org/pdf/2409.05771v1,cs.CL
Towards Democratizing Multilingual Large Language Models For Medicine Through A Two-Stage Instruction Fine-tuning Approach,"Open-source, multilingual medical large language models (LLMs) have the
potential to serve linguistically diverse populations across different regions.
Adapting generic LLMs for healthcare often requires continual pretraining, but
this approach is computationally expensive and sometimes impractical.
Instruction fine-tuning on a specific task may not always guarantee optimal
performance due to the lack of broader domain knowledge that the model needs to
understand and reason effectively in diverse scenarios. To address these
challenges, we introduce two multilingual instruction fine-tuning datasets,
MMed-IFT and MMed-IFT-MC, containing over 200k high-quality medical samples in
six languages. We propose a two-stage training paradigm: the first stage
injects general medical knowledge using MMed-IFT, while the second stage
fine-tunes task-specific multiple-choice questions with MMed-IFT-MC. Our method
achieves competitive results on both English and multilingual benchmarks,
striking a balance between computational efficiency and performance. We plan to
make our dataset and model weights public at
\url{https://github.com/SpassMed/Med-Llama3} in the future.",2024-09-09,"Meng Zhou, Surajsinh Parmar, Anubhav Bhatti",http://arxiv.org/pdf/2409.05732v1,cs.CL
Referring Expression Generation in Visually Grounded Dialogue with Discourse-aware Comprehension Guiding,"We propose an approach to referring expression generation (REG) in visually
grounded dialogue that is meant to produce referring expressions (REs) that are
both discriminative and discourse-appropriate. Our method constitutes a
two-stage process. First, we model REG as a text- and image-conditioned
next-token prediction task. REs are autoregressively generated based on their
preceding linguistic context and a visual representation of the referent.
Second, we propose the use of discourse-aware comprehension guiding as part of
a generate-and-rerank strategy through which candidate REs generated with our
REG model are reranked based on their discourse-dependent discriminatory power.
Results from our human evaluation indicate that our proposed two-stage approach
is effective in producing discriminative REs, with higher performance in terms
of text-image retrieval accuracy for reranked REs compared to those generated
using greedy decoding.",2024-09-09,"Bram Willemsen, Gabriel Skantze",http://arxiv.org/pdf/2409.05721v1,cs.CL
RIRAG: Regulatory Information Retrieval and Answer Generation,"Regulatory documents, issued by governmental regulatory bodies, establish
rules, guidelines, and standards that organizations must adhere to for legal
compliance. These documents, characterized by their length, complexity and
frequent updates, are challenging to interpret, requiring significant
allocation of time and expertise on the part of organizations to ensure ongoing
compliance. Regulatory Natural Language Processing (RegNLP) is a
multidisciplinary field aimed at simplifying access to and interpretation of
regulatory rules and obligations. We introduce a task of generating
question-passages pairs, where questions are automatically created and paired
with relevant regulatory passages, facilitating the development of regulatory
question-answering systems. We create the ObliQA dataset, containing 27,869
questions derived from the collection of Abu Dhabi Global Markets (ADGM)
financial regulation documents, design a baseline Regulatory Information
Retrieval and Answer Generation (RIRAG) system and evaluate it with RePASs, a
novel evaluation metric that tests whether generated answers accurately capture
all relevant obligations while avoiding contradictions.",2024-09-09,"Tuba Gokhan, Kexin Wang, Iryna Gurevych, Ted Briscoe",http://arxiv.org/pdf/2409.05677v2,cs.CL
Evaluation of real-time transcriptions using end-to-end ASR models,"Automatic Speech Recognition (ASR) or Speech-to-text (STT) has greatly
evolved in the last few years. Traditional architectures based on pipelines
have been replaced by joint end-to-end (E2E) architectures that simplify and
streamline the model training process. In addition, new AI training methods,
such as weak-supervised learning have reduced the need for high-quality audio
datasets for model training. However, despite all these advancements, little to
no research has been done on real-time transcription. In real-time scenarios,
the audio is not pre-recorded, and the input audio must be fragmented to be
processed by the ASR systems. To achieve real-time requirements, these
fragments must be as short as possible to reduce latency. However, audio cannot
be split at any point as dividing an utterance into two separate fragments will
generate an incorrect transcription. Also, shorter fragments provide less
context for the ASR model. For this reason, it is necessary to design and test
different splitting algorithms to optimize the quality and delay of the
resulting transcription. In this paper, three audio splitting algorithms are
evaluated with different ASR models to determine their impact on both the
quality of the transcription and the end-to-end delay. The algorithms are
fragmentation at fixed intervals, voice activity detection (VAD), and
fragmentation with feedback. The results are compared to the performance of the
same model, without audio fragmentation, to determine the effects of this
division. The results show that VAD fragmentation provides the best quality
with the highest delay, whereas fragmentation at fixed intervals provides the
lowest quality and the lowest delay. The newly proposed feedback algorithm
exchanges a 2-4% increase in WER for a reduction of 1.5-2s delay, respectively,
to the VAD splitting.",2024-09-09,"Carlos Arriaga, Alejandro Pozo, Javier Conde, Alvaro Alonso",http://arxiv.org/pdf/2409.05674v2,cs.CL
Identity-related Speech Suppression in Generative AI Content Moderation,"Automated content moderation has long been used to help identify and filter
undesired user-generated content online. Generative AI systems now use such
filters to keep undesired generated content from being created by or shown to
users. From classrooms to Hollywood, as generative AI is increasingly used for
creative or expressive text generation, whose stories will these technologies
allow to be told, and whose will they suppress? In this paper, we define and
introduce measures of speech suppression, focusing on speech related to
different identity groups incorrectly filtered by a range of content moderation
APIs. Using both short-form, user-generated datasets traditional in content
moderation and longer generative AI-focused data, including two datasets we
introduce in this work, we create a benchmark for measurement of speech
suppression for nine identity groups. Across one traditional and four
generative AI-focused automated content moderation services tested, we find
that identity-related speech is more likely to be incorrectly suppressed than
other speech. We find differences in identity-related speech suppression for
traditional versus generative AI data, with APIs performing better on
generative AI data but worse on longer text instances, and by identity, with
identity-specific reasons for incorrect flagging behavior. Overall, we find
that on traditional short-form data incorrectly suppressed speech is likely to
be political, while for generative AI creative data it is likely to be
television violence.",2024-09-09,"Oghenefejiro Isaacs Anigboro, Charlie M. Crawford, Grace Proebsting, Danaë Metaxa, Sorelle A. Friedler",http://arxiv.org/pdf/2409.13725v2,cs.CL
"WinoPron: Revisiting English Winogender Schemas for Consistency, Coverage, and Grammatical Case","While measuring bias and robustness in coreference resolution are important
goals, such measurements are only as good as the tools we use to measure them.
Winogender Schemas (Rudinger et al., 2018) are an influential dataset proposed
to evaluate gender bias in coreference resolution, but a closer look reveals
issues with the data that compromise its use for reliable evaluation, including
treating different pronominal forms as equivalent, violations of template
constraints, and typographical errors. We identify these issues and fix them,
contributing a new dataset: WinoPron. Using WinoPron, we evaluate two
state-of-the-art supervised coreference resolution systems, SpanBERT, and five
sizes of FLAN-T5, and demonstrate that accusative pronouns are harder to
resolve for all models. We also propose a new method to evaluate pronominal
bias in coreference resolution that goes beyond the binary. With this method,
we also show that bias characteristics vary not just across pronoun sets (e.g.,
he vs. she), but also across surface forms of those sets (e.g., him vs. his).",2024-09-09,"Vagrant Gautam, Julius Steuer, Eileen Bingert, Ray Johns, Anne Lauscher, Dietrich Klakow",http://arxiv.org/pdf/2409.05653v3,cs.CL
Longer is (Not Necessarily) Stronger: Punctuated Long-Sequence Training for Enhanced Speech Recognition and Translation,"This paper presents a new method for training sequence-to-sequence models for
speech recognition and translation tasks. Instead of the traditional approach
of training models on short segments containing only lowercase or partial
punctuation and capitalization (PnC) sentences, we propose training on longer
utterances that include complete sentences with proper punctuation and
capitalization. We achieve this by using the FastConformer architecture which
allows training 1 Billion parameter models with sequences up to 60 seconds long
with full attention. However, while training with PnC enhances the overall
performance, we observed that accuracy plateaus when training on sequences
longer than 40 seconds across various evaluation settings. Our proposed method
significantly improves punctuation and capitalization accuracy, showing a 25%
relative word error rate (WER) improvement on the Earnings-21 and Earnings-22
benchmarks. Additionally, training on longer audio segments increases the
overall model accuracy across speech recognition and translation benchmarks.
The model weights and training code are open-sourced though NVIDIA NeMo.",2024-09-09,"Nithin Rao Koluguri, Travis Bartley, Hainan Xu, Oleksii Hrinchuk, Jagadeesh Balam, Boris Ginsburg, Georg Kucsko",http://arxiv.org/pdf/2409.05601v1,cs.CL
ExDDI: Explaining Drug-Drug Interaction Predictions with Natural Language,"Predicting unknown drug-drug interactions (DDIs) is crucial for improving
medication safety. Previous efforts in DDI prediction have typically focused on
binary classification or predicting DDI categories, with the absence of
explanatory insights that could enhance trust in these predictions. In this
work, we propose to generate natural language explanations for DDI predictions,
enabling the model to reveal the underlying pharmacodynamics and
pharmacokinetics mechanisms simultaneously as making the prediction. To do
this, we have collected DDI explanations from DDInter and DrugBank and
developed various models for extensive experiments and analysis. Our models can
provide accurate explanations for unknown DDIs between known drugs. This paper
contributes new tools to the field of DDI prediction and lays a solid
foundation for further research on generating explanations for DDI predictions.",2024-09-09,"Zhaoyue Sun, Jiazheng Li, Gabriele Pergola, Yulan He",http://arxiv.org/pdf/2409.05592v1,cs.CL
MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation,"Processing long contexts presents a significant challenge for large language
models (LLMs). While recent advancements allow LLMs to handle much longer
contexts than before (e.g., 32K or 128K tokens), it is computationally
expensive and can still be insufficient for many applications.
Retrieval-Augmented Generation (RAG) is considered a promising strategy to
address this problem. However, conventional RAG methods face inherent
limitations because of two underlying requirements: 1) explicitly stated
queries, and 2) well-structured knowledge. These conditions, however, do not
hold in general long-context processing tasks.
  In this work, we propose MemoRAG, a novel RAG framework empowered by global
memory-augmented retrieval. MemoRAG features a dual-system architecture. First,
it employs a light but long-range system to create a global memory of the long
context. Once a task is presented, it generates draft answers, providing useful
clues for the retrieval tools to locate relevant information within the long
context. Second, it leverages an expensive but expressive system, which
generates the final answer based on the retrieved information. Building upon
this fundamental framework, we realize the memory module in the form of KV
compression, and reinforce its memorization and cluing capacity from the
Generation quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG
achieves superior performances across a variety of long-context evaluation
tasks, not only complex scenarios where traditional RAG methods struggle, but
also simpler ones where RAG is typically applied.",2024-09-09,"Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Defu Lian, Zhicheng Dou, Tiejun Huang",http://arxiv.org/pdf/2409.05591v3,cs.CL
Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation,"Embodied AI aims to develop robots that can \textit{understand} and execute
human language instructions, as well as communicate in natural languages. On
this front, we study the task of generating highly detailed navigational
instructions for the embodied robots to follow. Although recent studies have
demonstrated significant leaps in the generation of step-by-step instructions
from sequences of images, the generated instructions lack variety in terms of
their referral to objects and landmarks. Existing speaker models learn
strategies to evade the evaluation metrics and obtain higher scores even for
low-quality sentences. In this work, we propose SAS (Spatially-Aware Speaker),
an instruction generator or \textit{Speaker} model that utilises both
structural and semantic knowledge of the environment to produce richer
instructions. For training, we employ a reward learning method in an
adversarial setting to avoid systematic bias introduced by language evaluation
metrics. Empirically, our method outperforms existing instruction generation
models, evaluated using standard metrics. Our code is available at
\url{https://github.com/gmuraleekrishna/SAS}.",2024-09-09,"Muraleekrishna Gopinathan, Martin Masek, Jumana Abu-Khalaf, David Suter",http://arxiv.org/pdf/2409.05583v1,cs.CL
SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning,"A key challenge in artificial intelligence is the creation of systems capable
of autonomously advancing scientific understanding by exploring novel domains,
identifying complex patterns, and uncovering previously unseen connections in
vast scientific data. In this work, we present SciAgents, an approach that
leverages three core concepts: (1) the use of large-scale ontological knowledge
graphs to organize and interconnect diverse scientific concepts, (2) a suite of
large language models (LLMs) and data retrieval tools, and (3) multi-agent
systems with in-situ learning capabilities. Applied to biologically inspired
materials, SciAgents reveals hidden interdisciplinary relationships that were
previously considered unrelated, achieving a scale, precision, and exploratory
power that surpasses traditional human-driven research methods. The framework
autonomously generates and refines research hypotheses, elucidating underlying
mechanisms, design principles, and unexpected material properties. By
integrating these capabilities in a modular fashion, the intelligent system
yields material discoveries, critique and improve existing hypotheses, retrieve
up-to-date data about existing research, and highlights their strengths and
limitations. Our case studies demonstrate scalable capabilities to combine
generative AI, ontological representations, and multi-agent modeling,
harnessing a `swarm of intelligence' similar to biological systems. This
provides new avenues for materials discovery and accelerates the development of
advanced materials by unlocking Nature's design principles.",2024-09-09,"Alireza Ghafarollahi, Markus J. Buehler",http://arxiv.org/pdf/2409.05556v1,cs.CL
QiBERT -- Classifying Online Conversations Messages with BERT as a Feature,"Recent developments in online communication and their usage in everyday life
have caused an explosion in the amount of a new genre of text data, short text.
Thus, the need to classify this type of text based on its content has a
significant implication in many areas. Online debates are no exception, once
these provide access to information about opinions, positions and preferences
of its users. This paper aims to use data obtained from online social
conversations in Portuguese schools (short text) to observe behavioural trends
and to see if students remain engaged in the discussion when stimulated. This
project used the state of the art (SoA) Machine Learning (ML) algorithms and
methods, through BERT based models to classify if utterances are in or out of
the debate subject. Using SBERT embeddings as a feature, with supervised
learning, the proposed model achieved results above 0.95 average accuracy for
classifying online messages. Such improvements can help social scientists
better understand human communication, behaviour, discussion and persuasion.",2024-09-09,"Bruno D. Ferreira-Saraiva, Zuil Pirola, João P. Matos-Carvalho, Manuel Marques-Pita",http://arxiv.org/pdf/2409.05530v1,cs.CL
Harmonic Reasoning in Large Language Models,"Large Language Models (LLMs) are becoming very popular and are used for many
different purposes, including creative tasks in the arts. However, these models
sometimes have trouble with specific reasoning tasks, especially those that
involve logical thinking and counting. This paper looks at how well LLMs
understand and reason when dealing with musical tasks like figuring out notes
from intervals and identifying chords and scales. We tested GPT-3.5 and GPT-4o
to see how they handle these tasks. Our results show that while LLMs do well
with note intervals, they struggle with more complicated tasks like recognizing
chords and scales. This points out clear limits in current LLM abilities and
shows where we need to make them better, which could help improve how they
think and work in both artistic and other complex areas. We also provide an
automatically generated benchmark data set for the described tasks.",2024-09-09,Anna Kruspe,http://arxiv.org/pdf/2409.05521v1,cs.CL
Logically Consistent Language Models via Neuro-Symbolic Integration,"Large language models (LLMs) are a promising venue for natural language
understanding and generation. However, current LLMs are far from reliable: they
are prone to generating non-factual information and, more crucially, to
contradicting themselves when prompted to reason about relations between
entities of the world. These problems are currently addressed with large scale
fine-tuning or by delegating reasoning to external tools. In this work, we
strive for a middle ground and introduce a loss based on neuro-symbolic
reasoning that teaches an LLM to be logically consistent with an external set
of facts and rules and improves self-consistency even when the LLM is
fine-tuned on a limited set of facts. Our approach also allows to easily
combine multiple logical constraints at once in a principled way, delivering
LLMs that are more consistent w.r.t. all constraints and improve over several
baselines w.r.t. a given constraint. Moreover, our method allows LLMs to
extrapolate to unseen but semantically similar factual knowledge, represented
in unseen datasets, more systematically.",2024-09-09,"Diego Calanzone, Stefano Teso, Antonio Vergari",http://arxiv.org/pdf/2409.13724v1,cs.CL
Efficient Training of Self-Supervised Speech Foundation Models on a Compute Budget,"Despite their impressive success, training foundation models remains
computationally costly. This paper investigates how to efficiently train speech
foundation models with self-supervised learning (SSL) under a limited compute
budget. We examine critical factors in SSL that impact the budget, including
model architecture, model size, and data size. Our goal is to make analytical
steps toward understanding the training dynamics of speech foundation models.
We benchmark SSL objectives in an entirely comparable setting and find that
other factors contribute more significantly to the success of SSL. Our results
show that slimmer model architectures outperform common small architectures
under the same compute and parameter budget. We demonstrate that the size of
the pre-training data remains crucial, even with data augmentation during SSL
training, as performance suffers when iterating over limited data. Finally, we
identify a trade-off between model size and data size, highlighting an optimal
model size for a given compute budget.",2024-09-09,"Andy T. Liu, Yi-Cheng Lin, Haibin Wu, Stefan Winkler, Hung-yi Lee",http://arxiv.org/pdf/2409.16295v2,cs.CL
Elsevier Arena: Human Evaluation of Chemistry/Biology/Health Foundational Large Language Models,"arXiv admin comment: This version has been removed by arXiv administrators as
the submitter did not have the rights to agree to the license at the time of
submission",2024-09-09,"Camilo Thorne, Christian Druckenbrodt, Kinga Szarkowska, Deepika Goyal, Pranita Marajan, Vijay Somanath, Corey Harper, Mao Yan, Tony Scerri",http://arxiv.org/pdf/2409.05486v2,cs.CL
Representational Analysis of Binding in Language Models,"Entity tracking is essential for complex reasoning. To perform in-context
entity tracking, language models (LMs) must bind an entity to its attribute
(e.g., bind a container to its content) to recall attribute for a given entity.
For example, given a context mentioning ``The coffee is in Box Z, the stone is
in Box M, the map is in Box H'', to infer ``Box Z contains the coffee'' later,
LMs must bind ``Box Z'' to ``coffee''. To explain the binding behaviour of LMs,
existing research introduces a Binding ID mechanism and states that LMs use a
abstract concept called Binding ID (BI) to internally mark entity-attribute
pairs. However, they have not captured the Ordering ID (OI) from entity
activations that directly determines the binding behaviour. In this work, we
provide a novel view of the BI mechanism by localizing OI and proving the
causality between OI and binding behaviour. Specifically, by leveraging
dimension reduction methods (e.g., PCA), we discover that there exists a
low-rank subspace in the activations of LMs, that primarily encodes the order
(i.e., OI) of entity and attribute. Moreover, we also discover the causal
effect of OI on binding that when editing representations along the OI encoding
direction, LMs tend to bind a given entity to other attributes accordingly. For
example, by patching activations along the OI encoding direction we can make
the LM to infer ``Box Z contains the stone'' and ``Box Z contains the map''.",2024-09-09,"Qin Dai, Benjamin Heinzerling, Kentaro Inui",http://arxiv.org/pdf/2409.05448v3,cs.CL
SVFit: Parameter-Efficient Fine-Tuning of Large Pre-Trained Models Using Singular Values,"Large pre-trained models (LPMs) have demonstrated exceptional performance in
diverse natural language processing and computer vision tasks. However, fully
fine-tuning these models poses substantial memory challenges, particularly in
resource-constrained environments. Parameter-efficient fine-tuning (PEFT)
methods, such as LoRA, mitigate this issue by adjusting only a small subset of
parameters. Nevertheless, these methods typically employ random initialization
for low-rank matrices, which can lead to inefficiencies in gradient descent and
diminished generalizability due to suboptimal starting points. To address these
limitations, we propose SVFit, a novel PEFT approach that leverages singular
value decomposition (SVD) to initialize low-rank matrices using critical
singular values as trainable parameters. Specifically, SVFit performs SVD on
the pre-trained weight matrix to obtain the best rank-r approximation matrix,
emphasizing the most critical singular values that capture over 99% of the
matrix's information. These top-r singular values are then used as trainable
parameters to scale the fundamental subspaces of the matrix, facilitating rapid
domain adaptation. Extensive experiments across various pre-trained models in
natural language understanding, text-to-image generation, and image
classification tasks reveal that SVFit outperforms LoRA while requiring 16
times fewer trainable parameters.",2024-09-09,"Chengwei Sun, Jiwei Wei, Yujia Wu, Yiming Shi, Shiyuan He, Zeyu Ma, Ning Xie, Yang Yang",http://arxiv.org/pdf/2409.05926v1,cs.CL
Assessing SPARQL capabilities of Large Language Models,"The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)
offers significant synergistic potential for knowledge-driven applications. One
possible integration is the interpretation and generation of formal languages,
such as those used in the Semantic Web, with SPARQL being a core technology for
accessing KGs. In this paper, we focus on measuring out-of-the box capabilities
of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries
applying a quantitative approach.
  We implemented various benchmarking tasks in the LLM-KG-Bench framework for
automated execution and evaluation with several LLMs. The tasks assess
capabilities along the dimensions of syntax, semantic read, semantic create,
and the role of knowledge graph prompt inclusion.
  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,
and Claude models. Our findings indicate that working with SPARQL SELECT
queries is still challenging for LLMs and heavily depends on the specific LLM
as well as the complexity of the task. While fixing basic syntax errors seems
to pose no problems for the best of the current LLMs evaluated, creating
semantically correct SPARQL SELECT queries is difficult in several cases.",2024-09-09,"Lars-Peter Meyer, Johannes Frey, Felix Brei, Natanael Arndt",http://arxiv.org/pdf/2409.05925v2,cs.CL
STLM Engineering Report: Dropout,"In this work we explore the relevance of dropout for modern language models,
particularly in the context of models on the scale of <100M parameters. We
explore it's relevance firstly in the regime of improving the sample efficiency
of models given small, high quality datasets, and secondly in the regime of
improving the quality of its fit on larger datasets where models may underfit.
We find that concordant with conventional wisdom, dropout remains effective in
the overfitting scenario, and that furthermore it may have some relevance for
improving the fit of models even in the case of excess data, as suggested by
previous research. In the process we find that the existing explanation for the
mechanism behind this performance gain is not applicable in the case of
language modelling.",2024-09-09,"Dylan Hillier, Leon Guertler, Bobby Cheng, Cheston Tan",http://arxiv.org/pdf/2409.05423v1,cs.CL
Benchmarking and Building Zero-Shot Hindi Retrieval Model with Hindi-BEIR and NLLB-E5,"Given the large number of Hindi speakers worldwide, there is a pressing need
for robust and efficient information retrieval systems for Hindi. Despite
ongoing research, comprehensive benchmarks for evaluating retrieval models in
Hindi are lacking. To address this gap, we introduce the Hindi-BEIR benchmark,
comprising 15 datasets across seven distinct tasks. We evaluate
state-of-the-art multilingual retrieval models on the Hindi-BEIR benchmark,
identifying task and domain-specific challenges that impact Hindi retrieval
performance. Building on the insights from these results, we introduce NLLB-E5,
a multilingual retrieval model that leverages a zero-shot approach to support
Hindi without the need for Hindi training data. We believe our contributions,
which include the release of the Hindi-BEIR benchmark and the NLLB-E5 model,
will prove to be a valuable resource for researchers and promote advancements
in multilingual retrieval models.",2024-09-09,"Arkadeep Acharya, Rudra Murthy, Vishwajeet Kumar, Jaydeep Sen",http://arxiv.org/pdf/2409.05401v2,cs.CL
Towards Building a Robust Knowledge Intensive Question Answering Model with Large Language Models,"The development of LLMs has greatly enhanced the intelligence and fluency of
question answering, while the emergence of retrieval enhancement has enabled
models to better utilize external information. However, the presence of noise
and errors in retrieved information poses challenges to the robustness of LLMs.
In this work, to evaluate the model's performance under multiple interferences,
we first construct a dataset based on machine reading comprehension datasets
simulating various scenarios, including critical information absence, noise,
and conflicts. To address the issue of model accuracy decline caused by noisy
external information, we propose a data augmentation-based fine-tuning method
to enhance LLM's robustness against noise. Additionally, contrastive learning
approach is utilized to preserve the model's discrimination capability of
external information. We have conducted experiments on both existing LLMs and
our approach, the results are evaluated by GPT-4, which indicates that our
proposed methods improve model robustness while strengthening the model's
discrimination capability.",2024-09-09,"Xingyun Hong, Yan Shao, Zhilin Wang, Manni Duan, Jin Xiongnan",http://arxiv.org/pdf/2409.05385v3,cs.CL
Revisiting the Solution of Meta KDD Cup 2024: CRAG,"This paper presents the solution of our team APEX in the Meta KDD CUP 2024:
CRAG Comprehensive RAG Benchmark Challenge. The CRAG benchmark addresses the
limitations of existing QA benchmarks in evaluating the diverse and dynamic
challenges faced by Retrieval-Augmented Generation (RAG) systems. It provides a
more comprehensive assessment of RAG performance and contributes to advancing
research in this field. We propose a routing-based domain and dynamic adaptive
RAG pipeline, which performs specific processing for the diverse and dynamic
nature of the question in all three stages: retrieval, augmentation, and
generation. Our method achieved superior performance on CRAG and ranked 2nd for
Task 2&3 on the final competition leaderboard. Our implementation is available
at this link: https://github.com/USTCAGI/CRAG-in-KDD-Cup2024.",2024-09-09,"Jie Ouyang, Yucong Luo, Mingyue Cheng, Daoyu Wang, Shuo Yu, Qi Liu, Enhong Chen",http://arxiv.org/pdf/2409.15337v1,cs.CL
Application Specific Compression of Deep Learning Models,"Large Deep Learning models are compressed and deployed for specific
applications. However, current Deep Learning model compression methods do not
utilize the information about the target application. As a result, the
compressed models are application agnostic. Our goal is to customize the model
compression process to create a compressed model that will perform better for
the target application. Our method, Application Specific Compression (ASC),
identifies and prunes components of the large Deep Learning model that are
redundant specifically for the given target application. The intuition of our
work is to prune the parts of the network that do not contribute significantly
to updating the data representation for the given application. We have
experimented with the BERT family of models for three applications: Extractive
QA, Natural Language Inference, and Paraphrase Identification. We observe that
customized compressed models created using ASC method perform better than
existing model compression methods and off-the-shelf compressed models.",2024-09-09,"Rohit Raj Rai, Angana Borah, Amit Awekar",http://arxiv.org/pdf/2409.05368v1,cs.CL
Diagnostic Reasoning in Natural Language: Computational Model and Application,"Diagnostic reasoning is a key component of expert work in many domains. It is
a hard, time-consuming activity that requires expertise, and AI research has
investigated the ways automated systems can support this process. Yet, due to
the complexity of natural language, the applications of AI for diagnostic
reasoning to language-related tasks are lacking. To close this gap, we
investigate diagnostic abductive reasoning (DAR) in the context of
language-grounded tasks (NL-DAR). We propose a novel modeling framework for
NL-DAR based on Pearl's structural causal models and instantiate it in a
comprehensive study of scientific paper assessment in the biomedical domain. We
use the resulting dataset to investigate the human decision-making process in
NL-DAR and determine the potential of LLMs to support structured
decision-making over text. Our framework, open resources and tools lay the
groundwork for the empirical study of collaborative diagnostic reasoning in the
age of LLMs, in the scholarly domain and beyond.",2024-09-09,"Nils Dycke, Matej Zečević, Ilia Kuznetsov, Beatrix Suess, Kristian Kersting, Iryna Gurevych",http://arxiv.org/pdf/2409.05367v1,cs.CL
IndicVoices-R: Unlocking a Massive Multilingual Multi-speaker Speech Corpus for Scaling Indian TTS,"Recent advancements in text-to-speech (TTS) synthesis show that large-scale
models trained with extensive web data produce highly natural-sounding output.
However, such data is scarce for Indian languages due to the lack of
high-quality, manually subtitled data on platforms like LibriVox or YouTube. To
address this gap, we enhance existing large-scale ASR datasets containing
natural conversations collected in low-quality environments to generate
high-quality TTS training data. Our pipeline leverages the cross-lingual
generalization of denoising and speech enhancement models trained on English
and applied to Indian languages. This results in IndicVoices-R (IV-R), the
largest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704
hours of high-quality speech from 10,496 speakers across 22 Indian languages.
IV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS,
and IndicTTS. We also introduce the IV-R Benchmark, the first to assess
zero-shot, few-shot, and many-shot speaker generalization capabilities of TTS
models on Indian voices, ensuring diversity in age, gender, and style. We
demonstrate that fine-tuning an English pre-trained model on a combined dataset
of high-quality IndicTTS and our IV-R dataset results in better zero-shot
speaker generalization compared to fine-tuning on the IndicTTS dataset alone.
Further, our evaluation reveals limited zero-shot generalization for Indian
voices in TTS models trained on prior datasets, which we improve by fine-tuning
the model on our data containing diverse set of speakers across language
families. We open-source all data and code, releasing the first TTS model for
all 22 official Indian languages.",2024-09-09,"Ashwin Sankar, Srija Anand, Praveen Srinivasa Varadhan, Sherry Thomas, Mehak Singal, Shridhar Kumar, Deovrat Mehendale, Aditi Krishana, Giri Raju, Mitesh Khapra",http://arxiv.org/pdf/2409.05356v2,cs.CL
"Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis","The world is currently experiencing an outbreak of mpox, which has been
declared a Public Health Emergency of International Concern by WHO. No prior
work related to social media mining has focused on the development of a dataset
of Instagram posts about the mpox outbreak. The work presented in this paper
aims to address this research gap and makes two scientific contributions to
this field. First, it presents a multilingual dataset of 60,127 Instagram posts
about mpox, published between July 23, 2022, and September 5, 2024. The
dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram
posts about mpox in 52 languages. For each of these posts, the Post ID, Post
Description, Date of publication, language, and translated version of the post
(translation to English was performed using the Google Translate API) are
presented as separate attributes in the dataset. After developing this dataset,
sentiment analysis, hate speech detection, and anxiety or stress detection were
performed. This process included classifying each post into (i) one of the
sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or
neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no
anxiety/stress detected. These results are presented as separate attributes in
the dataset. Second, this paper presents the results of performing sentiment
analysis, hate speech analysis, and anxiety or stress analysis. The variation
of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and
neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and
50.64%, respectively. In terms of hate speech detection, 95.75% of the posts
did not contain hate and the remaining 4.25% of the posts contained hate.
Finally, 72.05% of the posts did not indicate any anxiety/stress, and the
remaining 27.95% of the posts represented some form of anxiety/stress.",2024-09-09,Nirmalya Thakur,http://arxiv.org/pdf/2409.05292v4,cs.CL
Seek and Solve Reasoning for Table Question Answering,"The complexities of table structures and question logic make table-based
question answering (TQA) tasks challenging for Large Language Models (LLMs),
often requiring task simplification before solving. This paper reveals that the
reasoning process during task simplification may be more valuable than the
simplified tasks themselves and aims to improve TQA performance by leveraging
LLMs' reasoning capabilities. We propose a Seek-and-Solve pipeline that
instructs the LLM to first seek relevant information and then answer questions,
integrating these two stages at the reasoning level into a coherent
Seek-and-Solve Chain of Thought (SS-CoT). Additionally, we distill a
single-step TQA-solving prompt from this pipeline, using demonstrations with
SS-CoT paths to guide the LLM in solving complex TQA tasks under In-Context
Learning settings. Our experiments show that our approaches result in improved
performance and reliability while being efficient. Our findings emphasize the
importance of eliciting LLMs' reasoning capabilities to handle complex TQA
tasks effectively.",2024-09-09,"Ruya Jiang, Chun Wang, Weihong Deng",http://arxiv.org/pdf/2409.05286v3,cs.CL
On the Relationship between Truth and Political Bias in Language Models,"Language model alignment research often attempts to ensure that models are
not only helpful and harmless, but also truthful and unbiased. However,
optimizing these objectives simultaneously can obscure how improving one aspect
might impact the others. In this work, we focus on analyzing the relationship
between two concepts essential in both language model alignment and political
science: truthfulness and political bias. We train reward models on various
popular truthfulness datasets and subsequently evaluate their political bias.
Our findings reveal that optimizing reward models for truthfulness on these
datasets tends to result in a left-leaning political bias. We also find that
existing open-source reward models (i.e., those trained on standard human
preference datasets) already show a similar bias and that the bias is larger
for larger models. These results raise important questions about the datasets
used to represent truthfulness, potential limitations of aligning models to be
both truthful and politically unbiased, and what language models capture about
the relationship between truth and politics.",2024-09-09,"Suyash Fulay, William Brannon, Shrestha Mohanty, Cassandra Overney, Elinor Poole-Dayan, Deb Roy, Jad Kabbara",http://arxiv.org/pdf/2409.05283v2,cs.CL
LegiLM: A Fine-Tuned Legal Language Model for Data Compliance,"Ensuring compliance with international data protection standards for privacy
and data security is a crucial but complex task, often requiring substantial
legal expertise. This paper introduces LegiLM, a novel legal language model
specifically tailored for consulting on data or information compliance. LegiLM
leverages a pre-trained GDPR Fines dataset and has been fine-tuned to
automatically assess whether particular actions or events breach data security
and privacy regulations. By incorporating a specialized dataset that includes
global data protection laws, meticulously annotated policy documents, and
relevant privacy policies, LegiLM is optimized for addressing data compliance
challenges. The model integrates advanced legal reasoning methods and
information retrieval enhancements to enhance accuracy and reliability in
practical legal consulting scenarios. Our evaluation using a custom benchmark
dataset demonstrates that LegiLM excels in detecting data regulation breaches,
offering sound legal justifications, and recommending necessary compliance
modifications, setting a new benchmark for AI-driven legal compliance
solutions. Our resources are publicly available at
https://github.com/DAOLegalAI/LegiLM",2024-09-09,"Linkai Zhu, Lu Yang, Chaofan Li, Shanwen Hu, Lu Liu, Bin Yin",http://arxiv.org/pdf/2409.13721v1,cs.CL
RexUniNLU: Recursive Method with Explicit Schema Instructor for Universal NLU,"Information Extraction (IE) and Text Classification (CLS) serve as the
fundamental pillars of NLU, with both disciplines relying on analyzing input
sequences to categorize outputs into pre-established schemas. However, there is
no existing encoder-based model that can unify IE and CLS tasks from this
perspective. To fully explore the foundation shared within NLU tasks, we have
proposed a Recursive Method with Explicit Schema Instructor for Universal NLU.
Specifically, we firstly redefine the true universal information extraction
(UIE) with a formal formulation that covers almost all extraction schemas,
including quadruples and quintuples which remain unsolved for previous UIE
models. Then, we expands the formulation to all CLS and multi-modal NLU tasks.
Based on that, we introduce RexUniNLU, an universal NLU solution that employs
explicit schema constraints for IE and CLS, which encompasses all IE and CLS
tasks and prevent incorrect connections between schema and input sequence. To
avoid interference between different schemas, we reset the position ids and
attention mask matrices. Extensive experiments are conducted on IE, CLS in both
English and Chinese, and multi-modality, revealing the effectiveness and
superiority. Our codes are publicly released.",2024-09-09,"Chengyuan Liu, Shihang Wang, Fubang Zhao, Kun Kuang, Yangyang Kang, Weiming Lu, Changlong Sun, Fei Wu",http://arxiv.org/pdf/2409.05275v1,cs.CL
UPCS: Unbiased Persona Construction for Dialogue Generation,"Narrative systems, such as dialogue and storytelling systems, often utilize
persona profiles to enhance personalized interactions. Existing persona
profiles frequently exhibit biases, posing risks to system integrity and
fairness. To address this, we introduce the UPCS framework, which categorizes
character descriptions into eight dimensions, including bias mitigation
strategies. Experimental results demonstrate UPCS's superiority in accuracy,
diversity, bias elimination, and user satisfaction, marking a significant
advancement in persona construction for reliable narrative systems.",2024-09-09,"Kuiyun Chen, Yanbin Wei",http://arxiv.org/pdf/2409.05257v4,cs.CL
Socially Responsible Data for Large Multilingual Language Models,"Large Language Models (LLMs) have rapidly increased in size and apparent
capabilities in the last three years, but their training data is largely
English text. There is growing interest in multilingual LLMs, and various
efforts are striving for models to accommodate languages of communities outside
of the Global North, which include many languages that have been historically
underrepresented in digital realms. These languages have been coined as ""low
resource languages"" or ""long-tail languages"", and LLMs performance on these
languages is generally poor. While expanding the use of LLMs to more languages
may bring many potential benefits, such as assisting cross-community
communication and language preservation, great care must be taken to ensure
that data collection on these languages is not extractive and that it does not
reproduce exploitative practices of the past. Collecting data from languages
spoken by previously colonized people, indigenous people, and non-Western
languages raises many complex sociopolitical and ethical questions, e.g.,
around consent, cultural safety, and data sovereignty. Furthermore, linguistic
complexity and cultural nuances are often lost in LLMs. This position paper
builds on recent scholarship, and our own work, and outlines several relevant
social, cultural, and ethical considerations and potential ways to mitigate
them through qualitative research, community partnerships, and participatory
design approaches. We provide twelve recommendations for consideration when
collecting language data on underrepresented language communities outside of
the Global North.",2024-09-08,"Andrew Smart, Ben Hutchinson, Lameck Mbangula Amugongo, Suzanne Dikker, Alex Zito, Amber Ebinama, Zara Wudiri, Ding Wang, Erin van Liemt, João Sedoc, Seyi Olojo, Stanley Uwakwe, Edem Wornyo, Sonja Schmer-Galunder, Jamila Smith-Loud",http://arxiv.org/pdf/2409.05247v1,cs.CL
Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation,"Multilingual neural machine translation models support fine-tuning hundreds
of languages simultaneously. However, fine-tuning on full parameters solely is
inefficient potentially leading to negative interactions among languages. In
this work, we demonstrate that the fine-tuning for a language occurs in its
intrinsic language-specific subspace with a tiny fraction of entire parameters.
Thus, we propose language-specific LoRA to isolate intrinsic language-specific
subspaces. Furthermore, we propose architecture learning techniques and
introduce a gradual pruning schedule during fine-tuning to exhaustively explore
the optimal setting and the minimal intrinsic subspaces for each language,
resulting in a lightweight yet effective fine-tuning procedure. The
experimental results on a 12-language subset and a 30-language subset of
FLORES-101 show that our methods not only outperform full-parameter fine-tuning
up to 2.25 spBLEU scores but also reduce trainable parameters to $0.4\%$ for
high and medium-resource languages and $1.6\%$ for low-resource ones.",2024-09-08,"Zhe Cao, Zhi Qu, Hidetaka Kamigaito, Taro Watanabe",http://arxiv.org/pdf/2409.05224v1,cs.CL
Interactive Machine Teaching by Labeling Rules and Instances,"Weakly supervised learning aims to reduce the cost of labeling data by using
expert-designed labeling rules. However, existing methods require experts to
design effective rules in a single shot, which is difficult in the absence of
proper guidance and tooling. Therefore, it is still an open question whether
experts should spend their limited time writing rules or instead providing
instance labels via active learning. In this paper, we investigate how to
exploit an expert's limited time to create effective supervision. First, to
develop practical guidelines for rule creation, we conduct an exploratory
analysis of diverse collections of existing expert-designed rules and find that
rule precision is more important than coverage across datasets. Second, we
compare rule creation to individual instance labeling via active learning and
demonstrate the importance of both across 6 datasets. Third, we propose an
interactive learning framework, INTERVAL, that achieves efficiency by
automatically extracting candidate rules based on rich patterns (e.g., by
prompting a language model), and effectiveness by soliciting expert feedback on
both candidate rules and individual instances. Across 6 datasets, INTERVAL
outperforms state-of-the-art weakly supervised approaches by 7% in F1.
Furthermore, it requires as few as 10 queries for expert feedback to reach F1
values that existing active learning methods cannot match even with 100
queries.",2024-09-08,"Giannis Karamanolakis, Daniel Hsu, Luis Gravano",http://arxiv.org/pdf/2409.05199v1,cs.CL
Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?,"State-of-the-art Large Language Models (LLMs) are accredited with an
increasing number of different capabilities, ranging from reading
comprehension, over advanced mathematical and reasoning skills to possessing
scientific knowledge. In this paper we focus on their multi-hop reasoning
capability: the ability to identify and integrate information from multiple
textual sources.
  Given the concerns with the presence of simplifying cues in existing
multi-hop reasoning benchmarks, which allow models to circumvent the reasoning
requirement, we set out to investigate, whether LLMs are prone to exploiting
such simplifying cues. We find evidence that they indeed circumvent the
requirement to perform multi-hop reasoning, but they do so in more subtle ways
than what was reported about their fine-tuned pre-trained language model (PLM)
predecessors. Motivated by this finding, we propose a challenging multi-hop
reasoning benchmark, by generating seemingly plausible multi-hop reasoning
chains, which ultimately lead to incorrect answers. We evaluate multiple open
and proprietary state-of-the-art LLMs, and find that their performance to
perform multi-hop reasoning is affected, as indicated by up to 45% relative
decrease in F1 score when presented with such seemingly plausible alternatives.
We conduct a deeper analysis and find evidence that while LLMs tend to ignore
misleading lexical cues, misleading reasoning paths indeed present a
significant challenge.",2024-09-08,"Neeladri Bhuiya, Viktor Schlegel, Stefan Winkler",http://arxiv.org/pdf/2409.05197v3,cs.CL
OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs,"Despite the recent advancements in Large Language Models (LLMs), which have
significantly enhanced the generative capabilities for various NLP tasks, LLMs
still face limitations in directly handling retrieval tasks. However, many
practical applications demand the seamless integration of both retrieval and
generation. This paper introduces a novel and efficient One-pass Generation and
retrieval framework (OneGen), designed to improve LLMs' performance on tasks
that require both generation and retrieval. The proposed framework bridges the
traditionally separate training approaches for generation and retrieval by
incorporating retrieval tokens generated autoregressively. This enables a
single LLM to handle both tasks simultaneously in a unified forward pass. We
conduct experiments on two distinct types of composite tasks, RAG and Entity
Linking, to validate the pluggability, effectiveness, and efficiency of OneGen
in training and inference. Furthermore, our results show that integrating
generation and retrieval within the same context preserves the generative
capabilities of LLMs while improving retrieval performance. To the best of our
knowledge, OneGen is the first to enable LLMs to conduct vector retrieval
during the generation.",2024-09-08,"Jintian Zhang, Cheng Peng, Mengshu Sun, Xiang Chen, Lei Liang, Zhiqiang Zhang, Jun Zhou, Huajun Chen, Ningyu Zhang",http://arxiv.org/pdf/2409.05152v2,cs.CL
Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis,"Within the context of creating new Socially Assistive Robots, emotion
recognition has become a key development factor, as it allows the robot to
adapt to the user's emotional state in the wild. In this work, we focused on
the analysis of two voice recording Spanish datasets: ELRA-S0329 and
EmoMatchSpanishDB. Specifically, we centered our work in the paralanguage,
e.~g. the vocal characteristics that go along with the message and clarifies
the meaning. We proposed the use of the DeepSpectrum method, which consists of
extracting a visual representation of the audio tracks and feeding them to a
pretrained CNN model. For the classification task, DeepSpectrum is often paired
with a Support Vector Classifier --DS-SVC--, or a Fully-Connected deep-learning
classifier --DS-FC--. We compared the results of the DS-SVC and DS-FC
architectures with the state-of-the-art (SOTA) for ELRA-S0329 and
EmoMatchSpanishDB. Moreover, we proposed our own classifier based upon
Attention Mechanisms, namely DS-AM. We trained all models against both
datasets, and we found that our DS-AM model outperforms the SOTA models for the
datasets and the SOTA DeepSpectrum architectures. Finally, we trained our DS-AM
model in one dataset and tested it in the other, to simulate real-world
conditions on how biased is the model to the dataset.",2024-09-08,"Elena Ortega-Beltrán, Josep Cabacas-Maso, Ismael Benito-Altamirano, Carles Ventura",http://arxiv.org/pdf/2409.05148v1,cs.CL
READoc: A Unified Benchmark for Realistic Document Structured Extraction,"Document Structured Extraction (DSE) aims to extract structured content from
raw documents. Despite the emergence of numerous DSE systems, their unified
evaluation remains inadequate, significantly hindering the field's advancement.
This problem is largely attributed to existing benchmark paradigms, which
exhibit fragmented and localized characteristics. To address these limitations
and offer a thorough evaluation of DSE systems, we introduce a novel benchmark
named READoc, which defines DSE as a realistic task of converting unstructured
PDFs into semantically rich Markdown. The READoc dataset is derived from 2,233
diverse and real-world documents from arXiv and GitHub. In addition, we develop
a DSE Evaluation S$^3$uite comprising Standardization, Segmentation and Scoring
modules, to conduct a unified evaluation of state-of-the-art DSE approaches. By
evaluating a range of pipeline tools, expert visual models, and general VLMs,
we identify the gap between current work and the unified, realistic DSE
objective for the first time. We aspire that READoc will catalyze future
research in DSE, fostering more comprehensive and practical solutions.",2024-09-08,"Zichao Li, Aizier Abulaiti, Yaojie Lu, Xuanang Chen, Jia Zheng, Hongyu Lin, Xianpei Han, Le Sun",http://arxiv.org/pdf/2409.05137v2,cs.CL
MHS-STMA: Multimodal Hate Speech Detection via Scalable Transformer-Based Multilevel Attention Framework,"Social media has a significant impact on people's lives. Hate speech on
social media has emerged as one of society's most serious issues in recent
years. Text and pictures are two forms of multimodal data that are distributed
within articles. Unimodal analysis has been the primary emphasis of earlier
approaches. Additionally, when doing multimodal analysis, researchers neglect
to preserve the distinctive qualities associated with each modality. To address
these shortcomings, the present article suggests a scalable architecture for
multimodal hate content detection called transformer-based multilevel attention
(STMA). This architecture consists of three main parts: a combined
attention-based deep learning mechanism, a vision attention-mechanism encoder,
and a caption attention-mechanism encoder. To identify hate content, each
component uses various attention processes and handles multimodal data in a
unique way. Several studies employing multiple assessment criteria on three
hate speech datasets such as Hateful memes, MultiOff, and MMHS150K, validate
the suggested architecture's efficacy. The outcomes demonstrate that on all
three datasets, the suggested strategy performs better than the baseline
approaches.",2024-09-08,"Anusha Chhabra, Dinesh Kumar Vishwakarma",http://arxiv.org/pdf/2409.05136v2,cs.CL
Hate Content Detection via Novel Pre-Processing Sequencing and Ensemble Methods,"Social media, particularly Twitter, has seen a significant increase in
incidents like trolling and hate speech. Thus, identifying hate speech is the
need of the hour. This paper introduces a computational framework to curb the
hate content on the web. Specifically, this study presents an exhaustive study
of pre-processing approaches by studying the impact of changing the sequence of
text pre-processing operations for the identification of hate content. The
best-performing pre-processing sequence, when implemented with popular
classification approaches like Support Vector Machine, Random Forest, Decision
Tree, Logistic Regression and K-Neighbor provides a considerable boost in
performance. Additionally, the best pre-processing sequence is used in
conjunction with different ensemble methods, such as bagging, boosting and
stacking to improve the performance further. Three publicly available benchmark
datasets (WZ-LS, DT, and FOUNTA), were used to evaluate the proposed approach
for hate speech identification. The proposed approach achieves a maximum
accuracy of 95.14% highlighting the effectiveness of the unique pre-processing
approach along with an ensemble classifier.",2024-09-08,"Anusha Chhabra, Dinesh Kumar Vishwakarma",http://arxiv.org/pdf/2409.05134v1,cs.CL
WaterSeeker: Pioneering Efficient Detection of Watermarked Segments in Large Documents,"Watermarking algorithms for large language models (LLMs) have attained high
accuracy in detecting LLM-generated text. However, existing methods primarily
focus on distinguishing fully watermarked text from non-watermarked text,
overlooking real-world scenarios where LLMs generate only small sections within
large documents. In this scenario, balancing time complexity and detection
performance poses significant challenges. This paper presents WaterSeeker, a
novel approach to efficiently detect and locate watermarked segments amid
extensive natural text. It first applies an efficient anomaly extraction method
to preliminarily locate suspicious watermarked regions. Following this, it
conducts a local traversal and performs full-text detection for more precise
verification. Theoretical analysis and experimental results demonstrate that
WaterSeeker achieves a superior balance between detection accuracy and
computational efficiency. Moreover, its localization capability lays the
foundation for building interpretable AI detection systems. Our code is
available at https://github.com/THU-BPM/WaterSeeker.",2024-09-08,"Leyi Pan, Aiwei Liu, Yijian Lu, Zitian Gao, Yichen Di, Shiyu Huang, Lijie Wen, Irwin King, Philip S. Yu",http://arxiv.org/pdf/2409.05112v6,cs.CL
EdaCSC: Two Easy Data Augmentation Methods for Chinese Spelling Correction,"Chinese Spelling Correction (CSC) aims to detect and correct spelling errors
in Chinese sentences caused by phonetic or visual similarities. While current
CSC models integrate pinyin or glyph features and have shown significant
progress,they still face challenges when dealing with sentences containing
multiple typos and are susceptible to overcorrection in real-world scenarios.
In contrast to existing model-centric approaches, we propose two data
augmentation methods to address these limitations. Firstly, we augment the
dataset by either splitting long sentences into shorter ones or reducing typos
in sentences with multiple typos. Subsequently, we employ different training
processes to select the optimal model. Experimental evaluations on the SIGHAN
benchmarks demonstrate the superiority of our approach over most existing
models, achieving state-of-the-art performance on the SIGHAN15 test set.",2024-09-08,"Lei Sheng, Shuai-Shuai Xu",http://arxiv.org/pdf/2409.05105v1,cs.CL
Evaluating Large Language Models with Tests of Spanish as a Foreign Language: Pass or Fail?,"Large Language Models (LLMs) have been profusely evaluated on their ability
to answer questions on many topics and their performance on different natural
language understanding tasks. Those tests are usually conducted in English, but
most LLM users are not native English speakers. Therefore, it is of interest to
analyze how LLMs understand other languages at different levels: from
paragraphs to morphems. In this paper, we evaluate the performance of
state-of-the-art LLMs in TELEIA, a recently released benchmark with similar
questions to those of Spanish exams for foreign students, covering topics such
as reading comprehension, word formation, meaning and compositional semantics,
and grammar. The results show that LLMs perform well at understanding Spanish
but are still far from achieving the level of a native speaker in terms of
grammatical competence.",2024-09-08,"Marina Mayor-Rocher, Nina Melero, Elena Merino-Gómez, María Grandury, Javier Conde, Pedro Reviriego",http://arxiv.org/pdf/2409.15334v1,cs.CL
LLM-based Abstraction and Concretization for GUI Test Migration,"GUI test migration aims to produce test cases with events and assertions to
test specific functionalities of a target app. Existing migration approaches
typically focus on the widget-mapping paradigm that maps widgets from source
apps to target apps. However, since different apps may implement the same
functionality in different ways, direct mapping may result in incomplete or
buggy test cases, thus significantly impacting the effectiveness of testing
target functionality and the practical applicability.
  In this paper, we propose a new migration paradigm (i.e.,
abstraction-concretization paradigm) that first abstracts the test logic for
the target functionality and then utilizes this logic to generate the concrete
GUI test case. Furthermore, we introduce MACdroid, the first approach that
migrates GUI test cases based on this paradigm. Specifically, we propose an
abstraction technique that utilizes source test cases from source apps
targeting the same functionality to extract a general test logic for that
functionality. Then, we propose a concretization technique that utilizes the
general test logic to guide an LLM in generating the corresponding GUI test
case (including events and assertions) for the target app. We evaluate MACdroid
on two widely-used datasets (including 31 apps, 34 functionalities, and 123
test cases). On the FrUITeR dataset, the test cases generated by MACdroid
successfully test 64% of the target functionalities, improving the baselines by
191%. On the Lin dataset, MACdroid successfully tests 75% of the target
functionalities, outperforming the baselines by 42%. These results underscore
the effectiveness of MACdroid in GUI test migration.",2024-09-08,"Yakun Zhang, Chen Liu, Xiaofei Xie, Yun Lin, Jin Song Dong, Dan Hao, Lu Zhang",http://arxiv.org/pdf/2409.05028v1,cs.CL
Vision-fused Attack: Advancing Aggressive and Stealthy Adversarial Text against Neural Machine Translation,"While neural machine translation (NMT) models achieve success in our daily
lives, they show vulnerability to adversarial attacks. Despite being harmful,
these attacks also offer benefits for interpreting and enhancing NMT models,
thus drawing increased research attention. However, existing studies on
adversarial attacks are insufficient in both attacking ability and human
imperceptibility due to their sole focus on the scope of language. This paper
proposes a novel vision-fused attack (VFA) framework to acquire powerful
adversarial text, i.e., more aggressive and stealthy. Regarding the attacking
ability, we design the vision-merged solution space enhancement strategy to
enlarge the limited semantic solution space, which enables us to search for
adversarial candidates with higher attacking ability. For human
imperceptibility, we propose the perception-retained adversarial text selection
strategy to align the human text-reading mechanism. Thus, the finally selected
adversarial text could be more deceptive. Extensive experiments on various
models, including large language models (LLMs) like LLaMA and GPT-3.5, strongly
support that VFA outperforms the comparisons by large margins (up to 81%/14%
improvements on ASR/SSIM).",2024-09-08,"Yanni Xue, Haojie Hao, Jiakai Wang, Qiang Sheng, Renshuai Tao, Yu Liang, Pu Feng, Xianglong Liu",http://arxiv.org/pdf/2409.05021v1,cs.CL
Towards Patronizing and Condescending Language in Chinese Videos: A Multimodal Dataset and Detector,"Patronizing and Condescending Language (PCL) is a form of discriminatory
toxic speech targeting vulnerable groups, threatening both online and offline
safety. While toxic speech research has mainly focused on overt toxicity, such
as hate speech, microaggressions in the form of PCL remain underexplored.
Additionally, dominant groups' discriminatory facial expressions and attitudes
toward vulnerable communities can be more impactful than verbal cues, yet these
frame features are often overlooked. In this paper, we introduce the PCLMM
dataset, the first Chinese multimodal dataset for PCL, consisting of 715
annotated videos from Bilibili, with high-quality PCL facial frame spans. We
also propose the MultiPCL detector, featuring a facial expression detection
module for PCL recognition, demonstrating the effectiveness of modality
complementarity in this challenging task. Our work makes an important
contribution to advancing microaggression detection within the domain of toxic
speech.",2024-09-08,"Hongbo Wang, Junyu Lu, Yan Han, Kai Ma, Liang Yang, Hongfei Lin",http://arxiv.org/pdf/2409.05005v2,cs.CL
InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference,"The widespread of Large Language Models (LLMs) marks a significant milestone
in generative AI. Nevertheless, the increasing context length and batch size in
offline LLM inference escalate the memory requirement of the key-value (KV)
cache, which imposes a huge burden on the GPU VRAM, especially for
resource-constraint scenarios (e.g., edge computing and personal devices).
Several cost-effective solutions leverage host memory or SSDs to reduce storage
costs for offline inference scenarios and improve the throughput. Nevertheless,
they suffer from significant performance penalties imposed by intensive KV
cache accesses due to limited PCIe bandwidth. To address these issues, we
propose InstInfer, a novel LLM inference system that offloads the most
performance-critical computation (i.e., attention in decoding phase) and data
(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize
the enormous KV transfer overheads. InstInfer designs a dedicated flash-aware
in-storage attention engine with KV cache management mechanisms to exploit the
high internal bandwidths of CSDs instead of being limited by the PCIe
bandwidth. The optimized P2P transmission between GPU and CSDs further reduces
data migration overheads. Experimental results demonstrate that for a 13B model
using an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence
inference by up to 11.1$\times$, compared to existing SSD-based solutions such
as FlexGen.",2024-09-08,"Xiurui Pan, Endian Li, Qiao Li, Shengwen Liang, Yizhou Shan, Ke Zhou, Yingwei Luo, Xiaolin Wang, Jie Zhang",http://arxiv.org/pdf/2409.04992v1,cs.CL
Evaluation of Google Translate for Mandarin Chinese translation using sentiment and semantic analysis,"Machine translation using large language models (LLMs) is having a
significant global impact, making communication easier. Mandarin Chinese is the
official language used for communication by the government and media in China.
In this study, we provide an automated assessment of translation quality of
Google Translate with human experts using sentiment and semantic analysis. In
order to demonstrate our framework, we select the classic early
twentieth-century novel 'The True Story of Ah Q' with selected Mandarin Chinese
to English translations. We use Google Translate to translate the given text
into English and then conduct a chapter-wise sentiment analysis and semantic
analysis to compare the extracted sentiments across the different translations.
Our results indicate that the precision of Google Translate differs both in
terms of semantic and sentiment analysis when compared to human expert
translations. We find that Google Translate is unable to translate some of the
specific words or phrases in Chinese, such as Chinese traditional allusions.
The mistranslations may be due to lack of contextual significance and
historical knowledge of China.",2024-09-08,"Xuechun Wang, Rodney Beard, Rohitash Chandra",http://arxiv.org/pdf/2409.04964v2,cs.CL
Maximizing Relation Extraction Potential: A Data-Centric Study to Unveil Challenges and Opportunities,"Relation extraction is a Natural Language Processing task that aims to
extract relationships from textual data. It is a critical step for information
extraction. Due to its wide-scale applicability, research in relation
extraction has rapidly scaled to using highly advanced neural networks. Despite
their computational superiority, modern relation extractors fail to handle
complicated extraction scenarios. However, a comprehensive performance analysis
of the state-of-the-art extractors that compile these challenges has been
missing from the literature, and this paper aims to bridge this gap. The goal
has been to investigate the possible data-centric characteristics that impede
neural relation extraction. Based on extensive experiments conducted using 15
state-of-the-art relation extraction algorithms ranging from recurrent
architectures to large language models and seven large-scale datasets, this
research suggests that modern relation extractors are not robust to complex
data and relation characteristics. It emphasizes pivotal issues, such as
contextual ambiguity, correlating relations, long-tail data, and fine-grained
relation distributions. In addition, it sets a marker for future directions to
alleviate these issues, thereby proving to be a critical resource for novice
and advanced researchers. Efficient handling of the challenges described can
have significant implications for the field of information extraction, which is
a critical part of popular systems such as search engines and chatbots. Data
and relevant code can be found at
\url{https://aaig.ece.ufl.edu/projects/relation-extraction}.",2024-09-07,"Anushka Swarup, Avanti Bhandarkar, Olivia P. Dizon-Paradis, Ronald Wilson, Damon L. Woodard",http://arxiv.org/pdf/2409.04934v2,cs.CL
Just ASR + LLM? A Study on Speech Large Language Models' Ability to Identify and Understand Speaker in Spoken Dialogue,"In recent years, we have observed a rapid advancement in speech language
models (SpeechLLMs), catching up with humans' listening and reasoning
abilities. SpeechLLMs have demonstrated impressive spoken dialog
question-answering (SQA) performance in benchmarks like Gaokao, the English
listening test of the college entrance exam in China, which seemingly requires
understanding both the spoken content and voice characteristics of speakers in
a conversation. However, after carefully examining Gaokao's questions, we find
the correct answers to many questions can be inferred from the conversation
transcript alone, i.e.\ without speaker segmentation and identification. Our
evaluation of state-of-the-art models Qwen-Audio and WavLLM on both Gaokao and
our proposed ""What Do You Like?"" dataset shows a significantly higher accuracy
in these context-based questions than in identity-critical questions, which can
only be answered reliably with correct speaker identification. The results and
analysis suggest that when solving SQA, the current SpeechLLMs exhibit limited
speaker awareness from the audio and behave similarly to an LLM reasoning from
the conversation transcription without sound. We propose that tasks focused on
identity-critical questions could offer a more accurate evaluation framework of
SpeechLLMs in SQA.",2024-09-07,"Junkai Wu, Xulin Fan, Bo-Ru Lu, Xilin Jiang, Nima Mesgarani, Mark Hasegawa-Johnson, Mari Ostendorf",http://arxiv.org/pdf/2409.04927v3,cs.CL
DiVA-DocRE: A Discriminative and Voice-Aware Paradigm for Document-Level Relation Extraction,"The remarkable capabilities of Large Language Models (LLMs) in text
comprehension and generation have revolutionized Information Extraction (IE).
One such advancement is in Document-level Relation Triplet Extraction (DocRTE),
a critical task in information systems that aims to extract entities and their
semantic relationships from documents. However, existing methods are primarily
designed for Sentence level Relation Triplet Extraction (SentRTE), which
typically handles a limited set of relations and triplet facts within a single
sentence. Additionally, some approaches treat relations as candidate choices
integrated into prompt templates, resulting in inefficient processing and
suboptimal performance when determining the relation elements in triplets. To
address these limitations, we introduce a Discriminative and Voice Aware
Paradigm DiVA. DiVA involves only two steps: performing document-level relation
extraction (DocRE) and then identifying the subject object entities based on
the relation. No additional processing is required simply input the document to
directly obtain the triplets. This streamlined process more accurately reflects
real-world scenarios for triplet extraction. Our innovation lies in
transforming DocRE into a discriminative task, where the model pays attention
to each relation and to the often overlooked issue of active vs. passive voice
within the triplet. Our experiments on the Re-DocRED and DocRED datasets
demonstrate state-of-the-art results for the DocRTE task.",2024-09-07,"Yiheng Wu, Roman Yangarber, Xian Mao",http://arxiv.org/pdf/2409.13717v2,cs.CL
Constrained Multi-Layer Contrastive Learning for Implicit Discourse Relationship Recognition,"Previous approaches to the task of implicit discourse relation recognition
(IDRR) generally view it as a classification task. Even with pre-trained
language models, like BERT and RoBERTa, IDRR still relies on complicated neural
networks with multiple intermediate layers to proper capture the interaction
between two discourse units. As a result, the outputs of these intermediate
layers may have different capability in discriminating instances of different
classes. To this end, we propose to adapt a supervised contrastive learning
(CL) method, label- and instance-centered CL, to enhance representation
learning. Moreover, we propose a novel constrained multi-layer CL approach to
properly impose a constraint that the contrastive loss of higher layers should
be smaller than that of lower layers. Experimental results on PDTB 2.0 and PDTB
3.0 show that our approach can significantly improve the performance on both
multi-class classification and binary classification.",2024-09-07,"Yiheng Wu, Junhui Li, Muhua Zhu",http://arxiv.org/pdf/2409.13716v1,cs.CL
Introducing MeMo: A Multimodal Dataset for Memory Modelling in Multiparty Conversations,"Conversational memory is the process by which humans encode, retain and
retrieve verbal, non-verbal and contextual information from a conversation.
Since human memory is selective, differing recollections of the same events can
lead to misunderstandings and misalignments within a group. Yet, conversational
facilitation systems, aimed at advancing the quality of group interactions,
usually focus on tracking users' states within an individual session, ignoring
what remains in each participant's memory after the interaction. Understanding
conversational memory can be used as a source of information on the long-term
development of social connections within a group. This paper introduces the
MeMo corpus, the first conversational dataset annotated with participants'
memory retention reports, aimed at facilitating computational modelling of
human conversational memory. The MeMo corpus includes 31 hours of small-group
discussions on Covid-19, repeated 3 times over the term of 2 weeks. It
integrates validated behavioural and perceptual measures, audio, video, and
multimodal annotations, offering a valuable resource for studying and modelling
conversational memory and group dynamics. By introducing the MeMo corpus,
analysing its validity, and demonstrating its usefulness for future research,
this paper aims to pave the way for future research in conversational memory
modelling for intelligent system development.",2024-09-07,"Maria Tsfasman, Bernd Dudzik, Kristian Fenech, Andras Lorincz, Catholijn M. Jonker, Catharine Oertel",http://arxiv.org/pdf/2409.13715v2,cs.CL
Achieving Peak Performance for Large Language Models: A Systematic Review,"In recent years, large language models (LLMs) have achieved remarkable
success in natural language processing (NLP). LLMs require an extreme amount of
parameters to attain high performance. As models grow into the
trillion-parameter range, computational and memory costs increase
significantly. This makes it difficult for many researchers to access the
resources needed to train or apply these models. Optimizing LLM performance
involves two main approaches: fine-tuning pre-trained models for specific tasks
to achieve state-of-the-art performance, and reducing costs or improving
training time while maintaining similar performance. This paper presents a
systematic literature review (SLR) following the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65
publications out of 983 from 2017 to December 2023, retrieved from 5 databases.
The study presents methods to optimize and accelerate LLMs while achieving
cutting-edge results without sacrificing accuracy. We begin with an overview of
the development of language modeling, followed by a detailed explanation of
commonly used frameworks and libraries, and a taxonomy for improving and
speeding up LLMs based on three classes: LLM training, LLM inference, and
system serving. We then delve into recent optimization and acceleration
strategies such as training optimization, hardware optimization, scalability
and reliability, accompanied by the taxonomy and categorization of these
strategies. Finally, we provide an in-depth comparison of each class and
strategy, with two case studies on optimizing model training and enhancing
inference efficiency. These case studies showcase practical approaches to
address LLM resource limitations while maintaining performance.",2024-09-07,"Zhyar Rzgar K Rostam, Sándor Szénási, Gábor Kertész",http://arxiv.org/pdf/2409.04833v1,cs.CL
MILE: A Mutation Testing Framework of In-Context Learning Systems,"In-context Learning (ICL) has achieved notable success in the applications of
large language models (LLMs). By adding only a few input-output pairs that
demonstrate a new task, the LLM can efficiently learn the task during inference
without modifying the model parameters. Such mysterious ability of LLMs has
attracted great research interests in understanding, formatting, and improving
the in-context demonstrations, while still suffering from drawbacks like
black-box mechanisms and sensitivity against the selection of examples. In this
work, inspired by the foundations of adopting testing techniques in machine
learning (ML) systems, we propose a mutation testing framework designed to
characterize the quality and effectiveness of test data for ICL systems. First,
we propose several mutation operators specialized for ICL demonstrations, as
well as corresponding mutation scores for ICL test sets. With comprehensive
experiments, we showcase the effectiveness of our framework in evaluating the
reliability and quality of ICL test suites. Our code is available at
https://github.com/weizeming/MILE.",2024-09-07,"Zeming Wei, Yihao Zhang, Meng Sun",http://arxiv.org/pdf/2409.04831v1,cs.CL
Exploring Straightforward Conversational Red-Teaming,"Large language models (LLMs) are increasingly used in business dialogue
systems but they pose security and ethical risks. Multi-turn conversations,
where context influences the model's behavior, can be exploited to produce
undesired responses. In this paper, we examine the effectiveness of utilizing
off-the-shelf LLMs in straightforward red-teaming approaches, where an attacker
LLM aims to elicit undesired output from a target LLM, comparing both
single-turn and conversational red-teaming tactics. Our experiments offer
insights into various usage strategies that significantly affect their
performance as red teamers. They suggest that off-the-shelf models can act as
effective red teamers and even adjust their attack strategy based on past
attempts, although their effectiveness decreases with greater alignment.",2024-09-07,"George Kour, Naama Zwerdling, Marcel Zalmanovici, Ateret Anaby-Tavor, Ora Nova Fandina, Eitan Farchi",http://arxiv.org/pdf/2409.04822v1,cs.CL
Phrase-Level Adversarial Training for Mitigating Bias in Neural Network-based Automatic Essay Scoring,"Automatic Essay Scoring (AES) is widely used to evaluate candidates for
educational purposes. However, due to the lack of representative data, most
existing AES systems are not robust, and their scoring predictions are biased
towards the most represented data samples. In this study, we propose a
model-agnostic phrase-level method to generate an adversarial essay set to
address the biases and robustness of AES models. Specifically, we construct an
attack test set comprising samples from the original test set and adversarially
generated samples using our proposed method. To evaluate the effectiveness of
the attack strategy and data augmentation, we conducted a comprehensive
analysis utilizing various neural network scoring models. Experimental results
show that the proposed approach significantly improves AES model performance in
the presence of adversarial examples and scenarios without such attacks.",2024-09-07,"Haddad Philip, Tsegaye Misikir Tashu",http://arxiv.org/pdf/2409.04795v1,cs.CL
Selective Self-Rehearsal: A Fine-Tuning Approach to Improve Generalization in Large Language Models,"Fine-tuning Large Language Models (LLMs) on specific datasets is a common
practice to improve performance on target tasks. However, this performance gain
often leads to overfitting, where the model becomes too specialized in either
the task or the characteristics of the training data, resulting in a loss of
generalization. This paper introduces Selective Self-Rehearsal (SSR), a
fine-tuning approach that achieves performance comparable to the standard
supervised fine-tuning (SFT) while improving generalization. SSR leverages the
fact that there can be multiple valid responses to a query. By utilizing the
model's correct responses, SSR reduces model specialization during the
fine-tuning stage. SSR first identifies the correct model responses from the
training set by deploying an appropriate LLM as a judge. Then, it fine-tunes
the model using the correct model responses and the gold response for the
remaining samples. The effectiveness of SSR is demonstrated through experiments
on the task of identifying unanswerable queries across various datasets. The
results show that standard SFT can lead to an average performance drop of up to
$16.7\%$ on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, SSR
results in close to $2\%$ drop on average, indicating better generalization
capabilities compared to standard SFT.",2024-09-07,"Sonam Gupta, Yatin Nandwani, Asaf Yehudai, Mayank Mishra, Gaurav Pandey, Dinesh Raghu, Sachindra Joshi",http://arxiv.org/pdf/2409.04787v1,cs.CL
TracrBench: Generating Interpretability Testbeds with Large Language Models,"Achieving a mechanistic understanding of transformer-based language models is
an open challenge, especially due to their large number of parameters.
Moreover, the lack of ground truth mappings between model weights and their
functional roles hinders the effective evaluation of interpretability methods,
impeding overall progress. Tracr, a method for generating compiled transformers
with inherent ground truth mappings in RASP, has been proposed to address this
issue. However, manually creating a large number of models needed for verifying
interpretability methods is labour-intensive and time-consuming. In this work,
we present a novel approach for generating interpretability test beds using
large language models (LLMs) and introduce TracrBench, a novel dataset
consisting of 121 manually written and LLM-generated, human-validated RASP
programs and their corresponding transformer weights. During this process, we
evaluate the ability of frontier LLMs to autonomously generate RASP programs
and find that this task poses significant challenges. GPT-4-turbo, with a
20-shot prompt and best-of-5 sampling, correctly implements only 57 out of 101
test programs, necessitating the manual implementation of the remaining
programs. With its 121 samples, TracrBench aims to serve as a valuable testbed
for evaluating and comparing interpretability methods.",2024-09-07,"Hannes Thurnherr, Jérémy Scheurer",http://arxiv.org/pdf/2409.13714v1,cs.CL
LoCa: Logit Calibration for Knowledge Distillation,"Knowledge Distillation (KD), aiming to train a better student model by
mimicking the teacher model, plays an important role in model compression. One
typical way is to align the output logits. However, we find a common issue
named mis-instruction, that the student would be misled when the predictions
based on teacher logits do not follow the labels. Meanwhile, there is other
useful dark knowledge in the logits such as the class discriminability, which
is vital for distillation. In this paper, we propose a simple yet effective
Logit Calibration (LoCa) method, which calibrates the logits from the teacher
model based on the ground-truth labels. The key insight is to correct the
prediction (to address the mis-instruction issue) and maintain useful dark
knowledge simultaneously. Our proposed LoCa does not require any additional
parameters. Empirical results on image classification and text generation tasks
demonstrate that LoCa can effectively improve the performance of baselines.",2024-09-07,"Runming Yang, Taiqiang Wu, Yujiu Yang",http://arxiv.org/pdf/2409.04778v1,cs.CL
Untie the Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models,"Large language models (LLM) have prioritized expanding the context window
from which models can incorporate more information. However, training models to
handle long contexts presents significant challenges. These include the
scarcity of high-quality natural long-context data, the potential for
performance degradation on short-context tasks, and the reduced training
efficiency associated with attention mechanisms. In this paper, we introduce
Untie the Knots (\textbf{UtK}), a novel data augmentation strategy employed
during the continue pre-training phase, designed to efficiently enable LLMs to
gain long-context capabilities without the need to modify the existing data
mixture. In particular, we chunk the documents, shuffle the chunks, and create
a complex and knotted structure of long texts; LLMs are then trained to untie
these knots and identify relevant segments within seemingly chaotic token
sequences. This approach greatly improves the model's performance by accurately
attending to relevant information in long context and the training efficiency
is also largely increased. We conduct extensive experiments on models with 7B
and 72B parameters, trained on 20 billion tokens, demonstrating that UtK
achieves 75\% and 84.5\% accurracy on RULER at 128K context length,
significantly outperforming other long context strategies. The trained models
will open-source for further research.",2024-09-07,"Junfeng Tian, Da Zheng, Yang Cheng, Rui Wang, Colin Zhang, Debing Zhang",http://arxiv.org/pdf/2409.04774v1,cs.CL
Sentiment Informed Sentence BERT-Ensemble Algorithm for Depression Detection,"The World Health Organisation (WHO) revealed approximately 280 million people
in the world suffer from depression. Yet, existing studies on early-stage
depression detection using machine learning (ML) techniques are limited. Prior
studies have applied a single stand-alone algorithm, which is unable to deal
with data complexities, prone to overfitting, and limited in generalization. To
this end, our paper examined the performance of several ML algorithms for
early-stage depression detection using two benchmark social media datasets (D1
and D2). More specifically, we incorporated sentiment indicators to improve our
model performance. Our experimental results showed that sentence bidirectional
encoder representations from transformers (SBERT) numerical vectors fitted into
the stacking ensemble model achieved comparable F1 scores of 69% in the dataset
(D1) and 76% in the dataset (D2). Our findings suggest that utilizing sentiment
indicators as an additional feature for depression detection yields an improved
model performance, and thus, we recommend the development of a depressive term
corpus for future work.",2024-09-07,"Bayode Ogunleye, Hemlata Sharma, Olamilekan Shobayo",http://arxiv.org/pdf/2409.13713v1,cs.CL
Property Neurons in Self-Supervised Speech Transformers,"There have been many studies on analyzing self-supervised speech
Transformers, in particular, with layer-wise analysis. It is, however,
desirable to have an approach that can pinpoint exactly a subset of neurons
that is responsible for a particular property of speech, being amenable to
model pruning and model editing. In this work, we identify a set of property
neurons in the feedforward layers of Transformers to study how speech-related
properties, such as phones, gender, and pitch, are stored. When removing
neurons of a particular property (a simple form of model editing), the
respective downstream performance significantly degrades, showing the
importance of the property neurons. We apply this approach to pruning the
feedforward layers in Transformers, where most of the model parameters are. We
show that protecting property neurons during pruning is significantly more
effective than norm-based pruning. The code for identifying property neurons is
available at https://github.com/nervjack2/PropertyNeurons.",2024-09-07,"Tzu-Quan Lin, Guan-Ting Lin, Hung-yi Lee, Hao Tang",http://arxiv.org/pdf/2409.05910v2,cs.CL
Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models,"Many use cases require retrieving smaller portions of text, and dense
vector-based retrieval systems often perform better with shorter text segments,
as the semantics are less likely to be over-compressed in the embeddings.
Consequently, practitioners often split text documents into smaller chunks and
encode them separately. However, chunk embeddings created in this way can lose
contextual information from surrounding chunks, resulting in sub-optimal
representations. In this paper, we introduce a novel method called late
chunking, which leverages long context embedding models to first embed all
tokens of the long text, with chunking applied after the transformer model and
just before mean pooling - hence the term late in its naming. The resulting
chunk embeddings capture the full contextual information, leading to superior
results across various retrieval tasks. The method is generic enough to be
applied to a wide range of long-context embedding models and works without
additional training. To further increase the effectiveness of late chunking, we
propose a dedicated fine-tuning approach for embedding models.",2024-09-07,"Michael Günther, Isabelle Mohr, Daniel James Williams, Bo Wang, Han Xiao",http://arxiv.org/pdf/2409.04701v2,cs.CL
"Good Idea or Not, Representation of LLM Could Tell","In the ever-expanding landscape of academic research, the proliferation of
ideas presents a significant challenge for researchers: discerning valuable
ideas from the less impactful ones. The ability to efficiently evaluate the
potential of these ideas is crucial for the advancement of science and paper
review. In this work, we focus on idea assessment, which aims to leverage the
knowledge of large language models to assess the merit of scientific ideas.
First, we investigate existing text evaluation research and define the problem
of quantitative evaluation of ideas. Second, we curate and release a benchmark
dataset from nearly four thousand manuscript papers with full texts,
meticulously designed to train and evaluate the performance of different
approaches to this task. Third, we establish a framework for quantifying the
value of ideas by employing representations in a specific layer of large
language models. Experimental results show that the scores predicted by our
method are relatively consistent with those of humans. Our findings suggest
that the representations of large language models hold more potential in
quantifying the value of ideas than their generative outputs, demonstrating a
promising avenue for automating the idea assessment process.",2024-09-07,"Yi Xu, Bo Xue, Shuqian Sheng, Cheng Deng, Jiaxin Ding, Zanwei Shen, Luoyi Fu, Xinbing Wang, Chenghu Zhou",http://arxiv.org/pdf/2409.13712v1,cs.CL
QueryBuilder: Human-in-the-Loop Query Development for Information Retrieval,"Frequently, users of an Information Retrieval (IR) system start with an
overarching information need (a.k.a., an analytic task) and proceed to define
finer-grained queries covering various important aspects (i.e., sub-topics) of
that analytic task. We present a novel, interactive system called
$\textit{QueryBuilder}$, which allows a novice, English-speaking user to create
queries with a small amount of effort, through efficient exploration of an
English development corpus in order to rapidly develop cross-lingual
information retrieval queries corresponding to the user's information needs.
QueryBuilder performs near real-time retrieval of documents based on
user-entered search terms; the user looks through the retrieved documents and
marks sentences as relevant to the information needed. The marked sentences are
used by the system as additional information in query formation and refinement:
query terms (and, optionally, event features, which capture event $'triggers'$
(indicator terms) and agent/patient roles) are appropriately weighted, and a
neural-based system, which better captures textual meaning, retrieves other
relevant content. The process of retrieval and marking is repeated as many
times as desired, giving rise to increasingly refined queries in each
iteration. The final product is a fine-grained query used in Cross-Lingual
Information Retrieval (CLIR). Our experiments using analytic tasks and requests
from the IARPA BETTER IR datasets show that with a small amount of effort (at
most 10 minutes per sub-topic), novice users can form $\textit{useful}$
fine-grained queries including in languages they don't understand. QueryBuilder
also provides beneficial capabilities to the traditional corpus exploration and
query formation process. A demonstration video is released at
https://vimeo.com/734795835",2024-09-07,"Hemanth Kandula, Damianos Karakos, Haoling Qiu, Benjamin Rozonoyer, Ian Soboroff, Lee Tarlin, Bonan Min",http://arxiv.org/pdf/2409.04667v2,cs.CL
Sparse Rewards Can Self-Train Dialogue Agents,"Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM)
agents, especially in multi-turn dialogue tasks, have been primarily driven by
supervised fine-tuning and high-quality human feedback. However, as base LLM
models continue to improve, acquiring meaningful human feedback has become
increasingly challenging and costly. In certain domains, base LLM agents may
eventually exceed human capabilities, making traditional feedback-driven
methods impractical. In this paper, we introduce a novel self-improvement
paradigm that empowers LLM agents to autonomously enhance their performance
without external human feedback. Our method, Juxtaposed Outcomes for Simulation
Harvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward
simulation environment to extract ideal behaviors and further train the LLM on
its own outputs. We present ToolWOZ, a sparse reward tool-calling simulation
environment derived from MultiWOZ. We demonstrate that models trained with
JOSH, both small and frontier, significantly improve tool-based interactions
while preserving general model capabilities across diverse benchmarks. Our code
and data are publicly available on GitHub at
https://github.com/asappresearch/josh-llm-simulation-training",2024-09-06,"Barrett Martin Lattimer, Varun Gangal, Ryan McDonald, Yi Yang",http://arxiv.org/pdf/2409.04617v2,cs.CL
BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training,"Language models can largely benefit from efficient tokenization. However,
they still mostly utilize the classical BPE algorithm, a simple and reliable
method. This has been shown to cause such issues as under-trained tokens and
sub-optimal compression that may affect the downstream performance. We
introduce Picky BPE, a modified BPE algorithm that carries out vocabulary
refinement during tokenizer training. Our method improves vocabulary
efficiency, eliminates under-trained tokens, and does not compromise text
compression. Our experiments show that our method does not reduce the
downstream performance, and in several cases improves it.",2024-09-06,"Pavel Chizhov, Catherine Arnett, Elizaveta Korotkova, Ivan P. Yamshchikov",http://arxiv.org/pdf/2409.04599v1,cs.CL
Paper Copilot: A Self-Evolving and Efficient LLM System for Personalized Academic Assistance,"As scientific research proliferates, researchers face the daunting task of
navigating and reading vast amounts of literature. Existing solutions, such as
document QA, fail to provide personalized and up-to-date information
efficiently. We present Paper Copilot, a self-evolving, efficient LLM system
designed to assist researchers, based on thought-retrieval, user profile and
high performance optimization. Specifically, Paper Copilot can offer
personalized research services, maintaining a real-time updated database.
Quantitative evaluation demonstrates that Paper Copilot saves 69.92\% of time
after efficient deployment. This paper details the design and implementation of
Paper Copilot, highlighting its contributions to personalized academic support
and its potential to streamline the research process.",2024-09-06,"Guanyu Lin, Tao Feng, Pengrui Han, Ge Liu, Jiaxuan You",http://arxiv.org/pdf/2409.04593v1,cs.CL
Customizing Large Language Model Generation Style using Parameter-Efficient Finetuning,"One-size-fits-all large language models (LLMs) are increasingly being used to
help people with their writing. However, the style these models are trained to
write in may not suit all users or use cases. LLMs would be more useful as
writing assistants if their idiolect could be customized to match each user. In
this paper, we explore whether parameter-efficient finetuning (PEFT) with
Low-Rank Adaptation can effectively guide the style of LLM generations. We use
this method to customize LLaMA-2 to ten different authors and show that the
generated text has lexical, syntactic, and surface alignment with the target
author but struggles with content memorization. Our findings highlight the
potential of PEFT to support efficient, user-level customization of LLMs.",2024-09-06,"Xinyue Liu, Harshita Diddee, Daphne Ippolito",http://arxiv.org/pdf/2409.04574v1,cs.CL
How Does Code Pretraining Affect Language Model Task Performance?,"Large language models are increasingly trained on corpora containing both
natural language and non-linguistic data like source code. Aside from aiding
programming-related tasks, anecdotal evidence suggests that including code in
pretraining corpora may improve performance on other, unrelated tasks, yet to
date no work has been able to establish a causal connection by controlling
between language and code data. Here we do just this. We pretrain language
models on datasets which interleave natural language and code in two different
settings: additive, in which the total volume of data seen during pretraining
is held constant; and competitive, in which the volume of language data is held
constant. We study how the pretraining mixture affects performance on (a) a
diverse collection of tasks included in the BigBench benchmark, and (b)
compositionality, measured by generalization accuracy on semantic parsing and
syntactic transformations. We find that pretraining on higher proportions of
code improves performance on compositional tasks involving structured output
(like semantic parsing), and mathematics. Conversely, increase code mixture can
harm performance on other tasks, including on tasks that requires sensitivity
to linguistic structure such as syntax or morphology, and tasks measuring
real-world knowledge.",2024-09-06,"Jackson Petty, Sjoerd van Steenkiste, Tal Linzen",http://arxiv.org/pdf/2409.04556v2,cs.CL
RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs,"LLM-powered personalization agent systems employ Large Language Models (LLMs)
to predict users' behavior from their past activities. However, their
effectiveness often hinges on the ability to effectively leverage extensive,
long user historical data due to its inherent noise and length of such data.
Existing pretrained LLMs may generate summaries that are concise but lack the
necessary context for downstream tasks, hindering their utility in
personalization systems. To address these challenges, we introduce
Reinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to
generate concise, human-readable user summaries that are optimized for
downstream task performance. By maximizing the usefulness of the generated
summaries, RLPF effectively distills extensive user history data while
preserving essential information for downstream tasks. Our empirical evaluation
demonstrates significant improvements in both extrinsic downstream task utility
and intrinsic summary quality, surpassing baseline methods by up to 22% on
downstream task performance and achieving an up to 84.59% win rate on
Factuality, Abstractiveness, and Readability. RLPF also achieves a remarkable
74% reduction in context length while improving performance on 16 out of 19
unseen tasks and/or datasets, showcasing its generalizability. This approach
offers a promising solution for enhancing LLM personalization by effectively
transforming long, noisy user histories into informative and human-readable
representations.",2024-09-06,"Jiaxing Wu, Lin Ning, Luyang Liu, Harrison Lee, Neo Wu, Chao Wang, Sushant Prakash, Shawn O'Banion, Bradley Green, Jun Xie",http://arxiv.org/pdf/2409.04421v2,cs.CL
Chain-of-Translation Prompting (CoTR): A Novel Prompting Technique for Low Resource Languages,"This paper introduces Chain of Translation Prompting (CoTR), a novel strategy
designed to enhance the performance of language models in low-resource
languages. CoTR restructures prompts to first translate the input context from
a low-resource language into a higher-resource language, such as English. The
specified task like generation, classification, or any other NLP function is
then performed on the translated text, with the option to translate the output
back to the original language if needed. All these steps are specified in a
single prompt. We demonstrate the effectiveness of this method through a case
study on the low-resource Indic language Marathi. The CoTR strategy is applied
to various tasks, including sentiment analysis, hate speech classification,
subject classification and text generation, and its efficacy is showcased by
comparing it with regular prompting methods. Our results underscore the
potential of translation-based prompting strategies to significantly improve
multilingual LLM performance in low-resource languages, offering valuable
insights for future research and applications. We specifically see the highest
accuracy improvements with the hate speech detection task. The technique also
has the potential to enhance the quality of synthetic data generation for
underrepresented languages using LLMs.",2024-09-06,"Tejas Deshpande, Nidhi Kowtal, Raviraj Joshi",http://arxiv.org/pdf/2409.04512v2,cs.CL
3D Data Long-Term Preservation in Cultural Heritage,"The report explores the challenges and strategies for preserving 3D digital
data in cultural heritage. It discusses the issue of technological
obsolescence, emphasising the need for ustainable storage solutions and ongoing
data management strategies. Key topics include understanding technological
obsolescence, the lifecycle of digital content, digital continuity, data
management plans (DMP), FAIR principles, and the use of public repositories.
The report also covers the importance of metadata in long-term digital
preservation, including types of metadata and strategies for building valuable
metadata. It examines the evolving standards and interoperability in 3D format
preservation and the importance of managing metadata and paradata. The document
provides a comprehensive overview of the challenges and solutions for
preserving 3D cultural heritage data in the long term.",2024-09-06,"Nicola Amico, Achille Felicetti",http://arxiv.org/pdf/2409.04507v2,cs.CL
Empirical Bayesian image restoration by Langevin sampling with a denoising diffusion implicit prior,"Score-based diffusion methods provide a powerful strategy to solve image
restoration tasks by flexibly combining a pre-trained foundational prior model
with a likelihood function specified during test time. Such methods are
predominantly derived from two stochastic processes: reversing
Ornstein-Uhlenbeck, which underpins the celebrated denoising diffusion
probabilistic models (DDPM) and denoising diffusion implicit models (DDIM), and
the Langevin diffusion process. The solutions delivered by DDPM and DDIM are
often remarkably realistic, but they are not always consistent with
measurements because of likelihood intractability issues and the associated
required approximations. Alternatively, using a Langevin process circumvents
the intractable likelihood issue, but usually leads to restoration results of
inferior quality and longer computing times. This paper presents a novel and
highly computationally efficient image restoration method that carefully embeds
a foundational DDPM denoiser within an empirical Bayesian Langevin algorithm,
which jointly calibrates key model hyper-parameters as it estimates the model's
posterior mean. Extensive experimental results on three canonical tasks (image
deblurring, super-resolution, and inpainting) demonstrate that the proposed
approach improves on state-of-the-art strategies both in image estimation
accuracy and computing time.",2024-09-06,"Charlesquin Kemajou Mbakam, Jean-Francois Giovannelli, Marcelo Pereyra",http://arxiv.org/pdf/2409.04384v1,cs.CL
You can remove GPT2's LayerNorm by fine-tuning,"The LayerNorm (LN) layer in GPT-style transformer models has long been a
hindrance to mechanistic interpretability. LN is a crucial component required
to stabilize the training of large language models, and LN or the similar
RMSNorm have been used in practically all large language models based on the
transformer architecture. The non-linear nature of the LN layers is a hindrance
for mechanistic interpretability as it hinders interpretation of the residual
stream, and makes it difficult to decompose the model into circuits. Some
researchers have gone so far as to name ""reasons interpretability researchers
hate layer norm.""
  In this paper we show that it is possible to remove the LN layers from a
pre-trained GPT2-small model by fine-tuning on a fraction (500M tokens) of the
training data. We demonstrate that this LN-free model achieves similar
performance to the original model on the OpenWebText and ThePile datasets
(-0.05 cross-entropy loss), and the Hellaswag benchmark (-0.5% accuracy). We
provide our implementation at https://github.com/ApolloResearch/gpt2_noLN, and
fine-tuned GPT2-small models at
https://huggingface.co/apollo-research/gpt2_noLN.
  Our work not only provides a simplified model for mechanistic
interpretability research, but also provides evidence that the LN layers, at
inference time, do not play a crucial role in transformer models.",2024-09-06,Stefan Heimersheim,http://arxiv.org/pdf/2409.13710v2,cs.CL
Programming Refusal with Conditional Activation Steering,"LLMs have shown remarkable capabilities, but precisely controlling their
response behavior remains challenging. Existing activation steering methods
alter LLM behavior indiscriminately, limiting their practical applicability in
settings where selective responses are essential, such as content moderation or
domain-specific assistants. In this paper, we propose Conditional Activation
Steering (CAST), which analyzes LLM activation patterns during inference to
selectively apply or withhold activation steering based on the input context.
Our method is based on the observation that different categories of prompts
activate distinct patterns in the model's hidden states. Using CAST, one can
systematically control LLM behavior with rules like ""if input is about hate
speech or adult content, then refuse"" or ""if input is not about legal advice,
then refuse."" This allows for selective modification of responses to specific
content while maintaining normal responses to other content, all without
requiring weight optimization. We release an open-source implementation of our
framework at github.com/IBM/activation-steering .",2024-09-06,"Bruce W. Lee, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Erik Miehling, Pierre Dognin, Manish Nagireddy, Amit Dhurandhar",http://arxiv.org/pdf/2409.05907v3,cs.CL
AGR: Age Group fairness Reward for Bias Mitigation in LLMs,"LLMs can exhibit age biases, resulting in unequal treatment of individuals
across age groups. While much research has addressed racial and gender biases,
age bias remains little explored. The scarcity of instruction-tuning and
preference datasets for age bias hampers its detection and measurement, and
existing fine-tuning methods seldom address age-related fairness. In this
paper, we construct age bias preference datasets and instruction-tuning
datasets for RLHF. We introduce ARG, an age fairness reward to reduce
differences in the response quality of LLMs across different age groups.
Extensive experiments demonstrate that this reward significantly improves
response accuracy and reduces performance disparities across age groups. Our
source code and datasets are available at the anonymous
\href{https://anonymous.4open.science/r/FairRLHF-D445/readme.md}{link}.",2024-09-06,"Shuirong Cao, Ruoxi Cheng, Zhiqiang Wang",http://arxiv.org/pdf/2409.04340v1,cs.CL
Column Vocabulary Association (CVA): semantic interpretation of dataless tables,"Traditional Semantic Table Interpretation (STI) methods rely primarily on the
underlying table data to create semantic annotations. This year's SemTab
challenge introduced the ``Metadata to KG'' track, which focuses on performing
STI by using only metadata information, without access to the underlying data.
In response to this new challenge, we introduce a new term: Column Vocabulary
Association (CVA). This term refers to the task of semantic annotation of
column headers solely based on metadata information. In this study, we evaluate
the performance of various methods in executing the CVA task, including a Large
Language Models (LLMs) and Retrieval Augmented Generation (RAG) approach, as
well as a more traditional similarity approach with SemanticBERT. Our
methodology uses a zero-shot setting, with no pretraining or examples passed to
the Large Language Models (LLMs), as we aim to avoid a domain-specific setting.
  We investigate a total of 7 different LLMs, of which three commercial GPT
models (i.e. gpt-3.5-turbo-0.125, gpt-4o and gpt-4-turbo) and four open source
models (i.e. llama3-80b, llama3-7b, gemma-7b and mixtral-8x7b). We integrate
this models with RAG systems, and we explore how variations in temperature
settings affect performances. Moreover, we continue our investigation by
performing the CVA task utilizing SemanticBERT, analyzing how various metadata
information influence its performance.
  Initial findings indicate that LLMs generally perform well at temperatures
below 1.0, achieving an accuracy of 100\% in certain cases. Nevertheless, our
investigation also reveal that the nature of the data significantly influences
CVA task outcomes. In fact, in cases where the input data and glossary are
related (for example by being created by the same organizations) traditional
methods appear to surpass the performance of LLMs.",2024-09-06,"Margherita Martorana, Xueli Pan, Benno Kruit, Tobias Kuhn, Jacco van Ossenbruggen",http://arxiv.org/pdf/2409.13709v1,cs.CL
Learning vs Retrieval: The Role of In-Context Examples in Regression with Large Language Models,"Generative Large Language Models (LLMs) are capable of being in-context
learners. However, the underlying mechanism of in-context learning (ICL) is
still a major research question, and experimental research results about how
models exploit ICL are not always consistent. In this work, we propose a
framework for evaluating in-context learning mechanisms, which we claim are a
combination of retrieving internal knowledge and learning from in-context
examples by focusing on regression tasks. First, we show that LLMs can solve
real-world regression problems and then design experiments to measure the
extent to which the LLM retrieves its internal knowledge versus learning from
in-context examples. We argue that this process lies on a spectrum between
these two extremes. We provide an in-depth analysis of the degrees to which
these mechanisms are triggered depending on various factors, such as prior
knowledge about the tasks and the type and richness of the information provided
by the in-context examples. We employ three LLMs and utilize multiple datasets
to corroborate the robustness of our findings. Our results shed light on how to
engineer prompts to leverage meta-learning from in-context examples and foster
knowledge retrieval depending on the problem being addressed.",2024-09-06,"Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi",http://arxiv.org/pdf/2409.04318v2,cs.CL
Towards Safe Multilingual Frontier AI,"Linguistically inclusive LLMs -- which maintain good performance regardless
of the language with which they are prompted -- are necessary for the diffusion
of AI benefits around the world. Multilingual jailbreaks that rely on language
translation to evade safety measures undermine the safe and inclusive
deployment of AI systems. We provide policy recommendations to enhance the
multilingual capabilities of AI while mitigating the risks of multilingual
jailbreaks. We examine how a language's level of resourcing relates to how
vulnerable LLMs are to multilingual jailbreaks in that language. We do this by
testing five advanced AI models across 24 official languages of the EU.
Building on prior research, we propose policy actions that align with the EU
legal landscape and institutional framework to address multilingual jailbreaks,
while promoting linguistic inclusivity. These include mandatory assessments of
multilingual capabilities and vulnerabilities, public opinion research, and
state support for multilingual AI development. The measures aim to improve AI
safety and functionality through EU policy initiatives, guiding the
implementation of the EU AI Act and informing regulatory efforts of the
European AI Office.",2024-09-06,"Artūrs Kanepajs, Vladimir Ivanov, Richard Moulange",http://arxiv.org/pdf/2409.13708v2,cs.CL
Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets,"Current publicly available knowledge work data collections lack diversity,
extensive annotations, and contextual information about the users and their
documents. These issues hinder objective and comparable data-driven evaluations
and optimizations of knowledge work assistance systems. Due to the considerable
resources needed to collect such data in real-life settings and the necessity
of data censorship, collecting such a dataset appears nearly impossible. For
this reason, we propose a configurable, multi-agent knowledge work dataset
generator. This system simulates collaborative knowledge work among agents
producing Large Language Model-generated documents and accompanying data
traces. Additionally, the generator captures all background information, given
in its configuration or created during the simulation process, in a knowledge
graph. Finally, the resulting dataset can be utilized and shared without
privacy or confidentiality concerns.
  This paper introduces our approach's design and vision and focuses on
generating authentic knowledge work documents using Large Language Models. Our
study involving human raters who assessed 53% of the generated and 74% of the
real documents as realistic demonstrates the potential of our approach.
Furthermore, we analyze the authenticity criteria mentioned in the
participants' comments and elaborate on potential improvements for identified
common issues.",2024-09-06,"Desiree Heim, Christian Jilek, Adrian Ulges, Andreas Dengel",http://arxiv.org/pdf/2409.04286v2,cs.CL
Open Language Data Initiative: Advancing Low-Resource Machine Translation for Karakalpak,"This study presents several contributions for the Karakalpak language: a
FLORES+ devtest dataset translated to Karakalpak, parallel corpora for
Uzbek-Karakalpak, Russian-Karakalpak and English-Karakalpak of 100,000 pairs
each and open-sourced fine-tuned neural models for translation across these
languages. Our experiments compare different model variants and training
approaches, demonstrating improvements over existing baselines. This work,
conducted as part of the Open Language Data Initiative (OLDI) shared task, aims
to advance machine translation capabilities for Karakalpak and contribute to
expanding linguistic diversity in NLP technologies.",2024-09-06,"Mukhammadsaid Mamasaidov, Abror Shopulatov",http://arxiv.org/pdf/2409.04269v1,cs.CL
"An overview of domain-specific foundation model: key technologies, applications and challenges","The impressive performance of ChatGPT and other foundation-model-based
products in human language understanding has prompted both academia and
industry to explore how these models can be tailored for specific industries
and application scenarios. This process, known as the customization of
domain-specific foundation models, addresses the limitations of general-purpose
models, which may not fully capture the unique patterns and requirements of
domain-specific data. Despite its importance, there is a notable lack of
comprehensive overview papers on building domain-specific foundation models,
while numerous resources exist for general-purpose models. To bridge this gap,
this article provides a timely and thorough overview of the methodology for
customizing domain-specific foundation models. It introduces basic concepts,
outlines the general architecture, and surveys key methods for constructing
domain-specific models. Furthermore, the article discusses various domains that
can benefit from these specialized models and highlights the challenges ahead.
Through this overview, we aim to offer valuable guidance and reference for
researchers and practitioners from diverse fields to develop their own
customized foundation models.",2024-09-06,"Haolong Chen, Hanzhi Chen, Zijian Zhao, Kaifeng Han, Guangxu Zhu, Yichen Zhao, Ying Du, Wei Xu, Qingjiang Shi",http://arxiv.org/pdf/2409.04267v1,cs.CL
Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support,"Clients wishing to implement generative AI in the domain of IT Support and
AIOps face two critical issues: domain coverage and model size constraints due
to model choice limitations. Clients might choose to not use larger proprietary
models such as GPT-4 due to cost and privacy concerns and so are limited to
smaller models with potentially less domain coverage that do not generalize to
the client's domain. Retrieval augmented generation is a common solution that
addresses both of these issues: a retrieval system first retrieves the
necessary domain knowledge which a smaller generative model leverages as
context for generation. We present a system developed for a client in the IT
Support domain for support case solution recommendation that combines retrieval
augmented generation (RAG) for answer generation with an encoder-only model for
classification and a generative large language model for query generation. We
cover architecture details, data collection and annotation, development journey
and preliminary validations, expected final deployment process and evaluation
plans, and finally lessons learned.",2024-09-06,"Paulina Toro Isaza, Michael Nidd, Noah Zheutlin, Jae-wook Ahn, Chidansh Amitkumar Bhatt, Yu Deng, Ruchi Mahindru, Martin Franz, Hans Florian, Salim Roukos",http://arxiv.org/pdf/2409.13707v1,cs.CL
Decolonising Data Systems: Using Jyutping or Pinyin as tonal representations of Chinese names for data linkage,"Data linkage is increasingly used in health research and policy making and is
relied on for understanding health inequalities. However, linked data is only
as useful as the underlying data quality, and differential linkage rates may
induce selection bias in the linked data. A mechanism that selectively
compromises data quality is name romanisation. Converting text of a different
writing system into Latin based writing, or romanisation, has long been the
standard process of representing names in character-based writing systems such
as Chinese, Vietnamese, and other languages such as Swahili. Unstandardised
romanisation of Chinese characters, due in part to problems of preserving the
correct name orders the lack of proper phonetic representation of a tonal
language, has resulted in poor linkage rates for Chinese immigrants. This
opinion piece aims to suggests that the use of standardised romanisation
systems for Cantonese (Jyutping) or Mandarin (Pinyin) Chinese, which
incorporate tonal information, could improve linkage rates and accuracy for
individuals with Chinese names. We used 771 Chinese and English names scraped
from openly available sources, and compared the utility of Jyutping, Pinyin and
the Hong Kong Government Romanisation system (HKG-romanisation) for
representing Chinese names. We demonstrate that both Jyutping and Pinyin result
in fewer errors compared with the HKG-romanisation system. We suggest that
collecting and preserving people's names in their original writing systems is
ethically and socially pertinent. This may inform development of
language-specific pre-processing and linkage paradigms that result in more
inclusive research data which better represents the targeted populations.",2024-09-06,"Joseph Lam, Mario Cortina-Borja, Robert Aldridge, Ruth Blackburn, Katie Harron",http://arxiv.org/pdf/2409.13706v1,cs.CL
Fast Forwarding Low-Rank Training,"Parameter efficient finetuning methods like low-rank adaptation (LoRA) aim to
reduce the computational costs of finetuning pretrained Language Models (LMs).
Enabled by these low-rank settings, we propose an even more efficient
optimization strategy: Fast Forward, a simple and effective approach to
accelerate large segments of training. In a Fast Forward stage, we repeat the
most recent optimizer step until the loss stops improving on a tiny validation
set. By alternating between regular optimization steps and Fast Forward stages,
Fast Forward provides up to an 87\% reduction in FLOPs and up to an 81\%
reduction in train time over standard SGD with Adam. We validate Fast Forward
by finetuning various models on different tasks and demonstrate that it speeds
up training without compromising model performance. Additionally, we analyze
when and how to apply Fast Forward.",2024-09-06,"Adir Rahamim, Naomi Saphra, Sara Kangaslahti, Yonatan Belinkov",http://arxiv.org/pdf/2409.04206v1,cs.CL
Residual Stream Analysis with Multi-Layer SAEs,"Sparse autoencoders (SAEs) are a promising approach to interpreting the
internal representations of transformer language models. However, SAEs are
usually trained separately on each transformer layer, making it difficult to
use them to study how information flows across layers. To solve this problem,
we introduce the multi-layer SAE (MLSAE): a single SAE trained on the residual
stream activation vectors from every transformer layer. Given that the residual
stream is understood to preserve information across layers, we expected MLSAE
latents to 'switch on' at a token position and remain active at later layers.
Interestingly, we find that individual latents are often active at a single
layer for a given token or prompt, but the layer at which an individual latent
is active may differ for different tokens or prompts. We quantify these
phenomena by defining a distribution over layers and considering its variance.
We find that the variance of the distributions of latent activations over
layers is about two orders of magnitude greater when aggregating over tokens
compared with a single token. For larger underlying models, the degree to which
latents are active at multiple layers increases, which is consistent with the
fact that the residual stream activation vectors at adjacent layers become more
similar. Finally, we relax the assumption that the residual stream basis is the
same at every layer by applying pre-trained tuned-lens transformations, but our
findings remain qualitatively similar. Our results represent a new approach to
understanding how representations change as they flow through transformers. We
release our code to train and analyze MLSAEs at
https://github.com/tim-lawson/mlsae.",2024-09-06,"Tim Lawson, Lucy Farnik, Conor Houghton, Laurence Aitchison",http://arxiv.org/pdf/2409.04185v3,cs.CL
GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding,"Programming languages possess rich semantic information such as data flow
that is represented by graphs and not available from the surface form of source
code. Recent code language models have scaled to billions of parameters, but
model source code solely as text tokens while ignoring any other structural
information. Conversely, models that do encode structural information of code
make modifications to the Transformer architecture, limiting their scale and
compatibility with pretrained LLMs. In this work, we take the best of both
worlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph
neural networks and cross-modal alignment technologies to inject the structural
information of code into LLMs as an auxiliary task during finetuning. This
framework is both model-agnostic and task-agnostic, as it can be applied to any
code LLM for any code downstream task, and requires the structural graph data
only at training time from a corpus unrelated to the finetuning data, while
incurring no cost at inference time over the baseline LLM. Experiments on five
code tasks with four different baseline LLMs ranging in size from 350M to 8B
validate the effectiveness of GALLa, demonstrating consistent improvement over
the baseline, even for powerful models such as LLaMA3.",2024-09-06,"Ziyin Zhang, Hang Yu, Shijie Li, Peng Di, Jianguo Li, Rui Wang",http://arxiv.org/pdf/2409.04183v1,cs.CL
Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering,"Advancements in natural language processing have revolutionized the way we
can interact with digital information systems, such as databases, making them
more accessible. However, challenges persist, especially when accuracy is
critical, as in the biomedical domain. A key issue is the hallucination
problem, where models generate information unsupported by the underlying data,
potentially leading to dangerous misinformation. This paper presents a novel
approach designed to bridge this gap by combining Large Language Models (LLM)
and Knowledge Graphs (KG) to improve the accuracy and reliability of
question-answering systems, on the example of a biomedical KG. Built on the
LangChain framework, our method incorporates a query checker that ensures the
syntactical and semantic validity of LLM-generated queries, which are then used
to extract information from a Knowledge Graph, substantially reducing errors
like hallucinations. We evaluated the overall performance using a new benchmark
dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo
and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other
models in generating accurate queries, open-source models like llama3:70b show
promise with appropriate prompt engineering. To make this approach accessible,
a user-friendly web-based interface has been developed, allowing users to input
natural language queries, view generated and corrected Cypher queries, and
verify the resulting paths for accuracy. Overall, this hybrid approach
effectively addresses common issues such as data gaps and hallucinations,
offering a reliable and intuitive solution for question answering systems. The
source code for generating the results of this paper and for the user-interface
can be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui",2024-09-06,"Larissa Pusch, Tim O. F. Conrad",http://arxiv.org/pdf/2409.04181v2,cs.CL
Medical Concept Normalization in a Low-Resource Setting,"In the field of biomedical natural language processing, medical concept
normalization is a crucial task for accurately mapping mentions of concepts to
a large knowledge base. However, this task becomes even more challenging in
low-resource settings, where limited data and resources are available. In this
thesis, I explore the challenges of medical concept normalization in a
low-resource setting. Specifically, I investigate the shortcomings of current
medical concept normalization methods applied to German lay texts. Since there
is no suitable dataset available, a dataset consisting of posts from a German
medical online forum is annotated with concepts from the Unified Medical
Language System. The experiments demonstrate that multilingual
Transformer-based models are able to outperform string similarity methods. The
use of contextual information to improve the normalization of lay mentions is
also examined, but led to inferior results. Based on the results of the best
performing model, I present a systematic error analysis and lay out potential
improvements to mitigate frequent errors.",2024-09-06,Tim Patzelt,http://arxiv.org/pdf/2409.14579v1,cs.CL
From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks,"To reduce the need for human annotations, large language models (LLMs) have
been proposed as judges of the quality of other candidate models. The
performance of LLM judges is typically evaluated by measuring the correlation
with human judgments on generative tasks such as summarization or machine
translation. In contrast, we study LLM judges on mathematical reasoning tasks.
These tasks require multi-step reasoning, and the correctness of their
solutions is verifiable, enabling a more objective evaluation. We perform a
detailed performance analysis and find that easy samples are easy to judge, and
difficult samples are difficult to judge. Our analysis uncovers a strong
correlation between judgment performance and the candidate model task
performance, indicating that judges tend to favor higher-quality models even if
their answer is incorrect. As a consequence, we test whether we can predict the
behavior of LLM judges using simple features such as part-of-speech tags and
find that we can correctly predict 70%-75% of judgments. We conclude this study
by analyzing practical use cases, showing that LLM judges consistently detect
the on-average better model but largely fail if we use them to improve task
performance.",2024-09-06,"Andreas Stephan, Dawei Zhu, Matthias Aßenmacher, Xiaoyu Shen, Benjamin Roth",http://arxiv.org/pdf/2409.04168v2,cs.CL
Can OpenSource beat ChatGPT? -- A Comparative Study of Large Language Models for Text-to-Code Generation,"In recent years, large language models (LLMs) have emerged as powerful tools
with potential applications in various fields, including software engineering.
Within the scope of this research, we evaluate five different state-of-the-art
LLMs - Bard, BingChat, ChatGPT, Llama2, and Code Llama - concerning their
capabilities for text-to-code generation. In an empirical study, we feed
prompts with textual descriptions of coding problems sourced from the
programming website LeetCode to the models with the task of creating solutions
in Python. Subsequently, the quality of the generated outputs is assessed using
the testing functionalities of LeetCode. The results indicate large differences
in performance between the investigated models. ChatGPT can handle these
typical programming challenges by far the most effectively, surpassing even
code-specialized models like Code Llama. To gain further insights, we measure
the runtime as well as the memory usage of the generated outputs and compared
them to the other code submissions on Leetcode. A detailed error analysis,
encompassing a comparison of the differences concerning correct indentation and
form of the generated code as well as an assignment of the incorrectly solved
tasks to certain error categories allows us to obtain a more nuanced picture of
the results and potential for improvement. The results also show a clear
pattern of increasingly incorrect produced code when the models are facing a
lot of context in the form of longer prompts.",2024-09-06,"Luis Mayer, Christian Heumann, Matthias Aßenmacher",http://arxiv.org/pdf/2409.04164v1,cs.CL
A Coin Has Two Sides: A Novel Detector-Corrector Framework for Chinese Spelling Correction,"Chinese Spelling Correction (CSC) stands as a foundational Natural Language
Processing (NLP) task, which primarily focuses on the correction of erroneous
characters in Chinese texts. Certain existing methodologies opt to disentangle
the error correction process, employing an additional error detector to
pinpoint error positions. However, owing to the inherent performance
limitations of error detector, precision and recall are like two sides of the
coin which can not be both facing up simultaneously. Furthermore, it is also
worth investigating how the error position information can be judiciously
applied to assist the error correction. In this paper, we introduce a novel
approach based on error detector-corrector framework. Our detector is designed
to yield two error detection results, each characterized by high precision and
recall. Given that the occurrence of errors is context-dependent and detection
outcomes may be less precise, we incorporate the error detection results into
the CSC task using an innovative feature fusion strategy and a selective
masking strategy. Empirical experiments conducted on mainstream CSC datasets
substantiate the efficacy of our proposed method.",2024-09-06,"Xiangke Zeng, Zuchao Li, Lefei Zhang, Ping Wang, Hongqiu Wu, Hai Zhao",http://arxiv.org/pdf/2409.04150v1,cs.CL
Prompt-based Personality Profiling: Reinforcement Learning for Relevance Filtering,"Author profiling is the task of inferring characteristics about individuals
by analyzing content they share. Supervised machine learning still dominates
automatic systems that perform this task, despite the popularity of prompting
large language models to address natural language understanding tasks. One
reason is that the classification instances consist of large amounts of posts,
potentially a whole user profile, which may exceed the input length of
Transformers. Even if a model can use a large context window, the entirety of
posts makes the application of API-accessed black box systems costly and slow,
next to issues which come with such ""needle-in-the-haystack"" tasks. To mitigate
this limitation, we propose a new method for author profiling which aims at
distinguishing relevant from irrelevant content first, followed by the actual
user profiling only with relevant data. To circumvent the need for
relevance-annotated data, we optimize this relevance filter via reinforcement
learning with a reward function that utilizes the zero-shot capabilities of
large language models. We evaluate our method for Big Five personality trait
prediction on two Twitter corpora. On publicly available real-world data with a
skewed label distribution, our method shows similar efficacy to using all posts
in a user profile, but with a substantially shorter context. An evaluation on a
version of these data balanced with artificial posts shows that the filtering
to relevant posts leads to a significantly improved accuracy of the
predictions.",2024-09-06,"Jan Hofmann, Cornelia Sindermann, Roman Klinger",http://arxiv.org/pdf/2409.04122v1,cs.CL
Confidence-Aware Document OCR Error Detection,"Optical Character Recognition (OCR) continues to face accuracy challenges
that impact subsequent applications. To address these errors, we explore the
utility of OCR confidence scores for enhancing post-OCR error detection. Our
study involves analyzing the correlation between confidence scores and error
rates across different OCR systems. We develop ConfBERT, a BERT-based model
that incorporates OCR confidence scores into token embeddings and offers an
optional pre-training phase for noise adjustment. Our experimental results
demonstrate that integrating OCR confidence scores can enhance error detection
capabilities. This work underscores the importance of OCR confidence scores in
improving detection accuracy and reveals substantial disparities in performance
between commercial and open-source OCR technologies.",2024-09-06,"Arthur Hemmer, Mickaël Coustaty, Nicola Bartolo, Jean-Marc Ogier",http://arxiv.org/pdf/2409.04117v1,cs.CL
Multi-Programming Language Ensemble for Code Generation in Large Language Model,"Large language models (LLMs) have significantly improved code generation,
particularly in one-pass code generation. However, most existing approaches
focus solely on generating code in a single programming language, overlooking
the potential of leveraging the multi-language capabilities of LLMs. LLMs have
varying patterns of errors across different languages, suggesting that a more
robust approach could be developed by leveraging these multi-language outputs.
In this study, we propose Multi-Programming Language Ensemble (MPLE), a novel
ensemble-based method that utilizes code generation across multiple programming
languages to enhance overall performance. By treating each language-specific
code generation process as an individual ""weak expert"" and effectively
integrating their outputs, our method mitigates language-specific errors and
biases. This multi-language ensemble strategy leverages the complementary
strengths of different programming languages, enabling the model to produce
more accurate and robust code. Our approach can be seamlessly integrated with
commonly used techniques such as the reflection algorithm and Monte Carlo tree
search to improve code generation quality further. Experimental results show
that our framework consistently enhances baseline performance by up to 17.92%
on existing benchmarks (HumanEval and HumanEval-plus), with a standout result
of 96.25% accuracy on the HumanEval benchmark, achieving new state-of-the-art
results across various LLM models. The code will be released at
https://github.com/NinjaTech-AI/MPLE",2024-09-06,"Tengfei Xue, Xuefeng Li, Tahir Azim, Roman Smirnov, Jianhui Yu, Arash Sadrieh, Babak Pahlavan",http://arxiv.org/pdf/2409.04114v1,cs.CL
Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers,"Recent advancements in large language models (LLMs) have sparked optimism
about their potential to accelerate scientific discovery, with a growing number
of works proposing research agents that autonomously generate and validate new
ideas. Despite this, no evaluations have shown that LLM systems can take the
very first step of producing novel, expert-level ideas, let alone perform the
entire research process. We address this by establishing an experimental design
that evaluates research idea generation while controlling for confounders and
performs the first head-to-head comparison between expert NLP researchers and
an LLM ideation agent. By recruiting over 100 NLP researchers to write novel
ideas and blind reviews of both LLM and human ideas, we obtain the first
statistically significant conclusion on current LLM capabilities for research
ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than
human expert ideas while being judged slightly weaker on feasibility. Studying
our agent baselines closely, we identify open problems in building and
evaluating research agents, including failures of LLM self-evaluation and their
lack of diversity in generation. Finally, we acknowledge that human judgements
of novelty can be difficult, even by experts, and propose an end-to-end study
design which recruits researchers to execute these ideas into full projects,
enabling us to study whether these novelty and feasibility judgements result in
meaningful differences in research outcome.",2024-09-06,"Chenglei Si, Diyi Yang, Tatsunori Hashimoto",http://arxiv.org/pdf/2409.04109v1,cs.CL
Structure and dynamics of growing networks of Reddit threads,"Millions of people use online social networks to reinforce their sense of
belonging, for example by giving and asking for feedback as a form of social
validation and self-recognition. It is common to observe disagreement among
people beliefs and points of view when expressing this feedback. Modeling and
analyzing such interactions is crucial to understand social phenomena that
happen when people face different opinions while expressing and discussing
their values. In this work, we study a Reddit community in which people
participate to judge or be judged with respect to some behavior, as it
represents a valuable source to study how users express judgments online. We
model threads of this community as complex networks of user interactions
growing in time, and we analyze the evolution of their structural properties.
We show that the evolution of Reddit networks differ from other real social
networks, despite falling in the same category. This happens because their
global clustering coefficient is extremely small and the average shortest path
length increases over time. Such properties reveal how users discuss in
threads, i.e. with mostly one other user and often by a single message. We
strengthen such result by analyzing the role that disagreement and reciprocity
play in such conversations. We also show that Reddit thread's evolution over
time is governed by two subgraphs growing at different speeds. We discover
that, in the studied community, the difference of such speed is higher than in
other communities because of the user guidelines enforcing specific user
interactions. Finally, we interpret the obtained results on user behavior
drawing back to Social Judgment Theory.",2024-09-06,"Diletta Goglia, Davide Vega",http://arxiv.org/pdf/2409.04085v1,cs.CL
UI-JEPA: Towards Active Perception of User Intent through Onscreen User Activity,"Generating user intent from a sequence of user interface (UI) actions is a
core challenge in comprehensive UI understanding. Recent advancements in
multimodal large language models (MLLMs) have led to substantial progress in
this area, but their demands for extensive model parameters, computing power,
and high latency makes them impractical for scenarios requiring lightweight,
on-device solutions with low latency or heightened privacy. Additionally, the
lack of high-quality datasets has hindered the development of such lightweight
models. To address these challenges, we propose UI-JEPA, a novel framework that
employs masking strategies to learn abstract UI embeddings from unlabeled data
through self-supervised learning, combined with an LLM decoder fine-tuned for
user intent prediction. We also introduce two new UI-grounded multimodal
datasets, ""Intent in the Wild"" (IIW) and ""Intent in the Tame"" (IIT), designed
for few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos
across 219 intent categories, while IIT contains 914 videos across 10
categories. We establish the first baselines for these datasets, showing that
representations learned using a JEPA-style objective, combined with an LLM
decoder, can achieve user intent predictions that match the performance of
state-of-the-art large MLLMs, but with significantly reduced annotation and
deployment resources. Measured by intent similarity scores, UI-JEPA outperforms
GPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged
across two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x
reduction in computational cost and a 6.6x improvement in latency in the IIW
dataset. These results underscore the effectiveness of UI-JEPA, highlighting
its potential for lightweight, high-performance UI understanding.",2024-09-06,"Yicheng Fu, Raviteja Anantha, Prabal Vashisht, Jianpeng Cheng, Etai Littwin",http://arxiv.org/pdf/2409.04081v3,cs.CL
AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language Model,"Entity matching (EM) is the problem of determining whether two records refer
to same real-world entity, which is crucial in data integration, e.g., for
product catalogs or address databases. A major drawback of many EM approaches
is their dependence on labelled examples. We thus focus on the challenging
setting of zero-shot entity matching where no labelled examples are available
for an unseen target dataset. Recently, large language models (LLMs) have shown
promising results for zero-shot EM, but their low throughput and high
deployment cost limit their applicability and scalability.
  We revisit the zero-shot EM problem with AnyMatch, a small language model
fine-tuned in a transfer learning setup. We propose several novel data
selection techniques to generate fine-tuning data for our model, e.g., by
selecting difficult pairs to match via an AutoML filter, by generating
additional attribute-level examples, and by controlling label imbalance in the
data.
  We conduct an extensive evaluation of the prediction quality and deployment
cost of our model, in a comparison to thirteen baselines on nine benchmark
datasets. We find that AnyMatch provides competitive prediction quality despite
its small parameter size: it achieves the second-highest F1 score overall, and
outperforms several other approaches that employ models with hundreds of
billions of parameters. Furthermore, our approach exhibits major cost benefits:
the average prediction quality of AnyMatch is within 4.4% of the
state-of-the-art method MatchGPT with the proprietary trillion-parameter model
GPT-4, yet AnyMatch requires four orders of magnitude less parameters and
incurs a 3,899 times lower inference cost (in dollars per 1,000 tokens).",2024-09-06,"Zeyu Zhang, Paul Groth, Iacer Calixto, Sebastian Schelter",http://arxiv.org/pdf/2409.04073v2,cs.CL
Self-Harmonized Chain of Thought,"Chain-of-thought (CoT) prompting has demonstrated the capacity of large
language models to perform complex reasoning through intermediate steps. While
effective, current CoT methods face challenges: Zero-shot-CoT can lead to
reasoning errors, and Few-shot-CoT requires labor-intensive manual
demonstrations. Auto-CoT attempts to address these issues by automatically
generating diverse demonstrations, but this diversity can lead to inconsistent
reasoning patterns. We propose ECHO (Self-Harmonized Chain of Thought), a novel
method that unifies diverse solution paths into a consistent and effective
reasoning pattern. ECHO employs an iterative process to refine and harmonize
automatically generated demonstrations, mitigating the limitations of existing
approaches. Our comprehensive experiments across arithmetic, commonsense, and
symbolic reasoning tasks demonstrate that ECHO outperforms Auto-CoT by an
average of 2.8%. These findings suggest that ECHO represents a significant step
towards more robust and generalizable automated reasoning in large language
models.",2024-09-06,"Ziqi Jin, Wei Lu",http://arxiv.org/pdf/2409.04057v2,cs.CL
Refining Wikidata Taxonomy using Large Language Models,"Due to its collaborative nature, Wikidata is known to have a complex
taxonomy, with recurrent issues like the ambiguity between instances and
classes, the inaccuracy of some taxonomic paths, the presence of cycles, and
the high level of redundancy across classes. Manual efforts to clean up this
taxonomy are time-consuming and prone to errors or subjective decisions. We
present WiKC, a new version of Wikidata taxonomy cleaned automatically using a
combination of Large Language Models (LLMs) and graph mining techniques.
Operations on the taxonomy, such as cutting links or merging classes, are
performed with the help of zero-shot prompting on an open-source LLM. The
quality of the refined taxonomy is evaluated from both intrinsic and extrinsic
perspectives, on a task of entity typing for the latter, showing the practical
interest of WiKC.",2024-09-06,"Yiwen Peng, Thomas Bonald, Mehwish Alam",http://arxiv.org/pdf/2409.04056v1,cs.CL
Towards Safer Online Spaces: Simulating and Assessing Intervention Strategies for Eating Disorder Discussions,"Eating disorders are complex mental health conditions that affect millions of
people around the world. Effective interventions on social media platforms are
crucial, yet testing strategies in situ can be risky. We present a novel
LLM-driven experimental testbed for simulating and assessing intervention
strategies in ED-related discussions. Our framework generates synthetic
conversations across multiple platforms, models, and ED-related topics,
allowing for controlled experimentation with diverse intervention approaches.
We analyze the impact of various intervention strategies on conversation
dynamics across four dimensions: intervention type, generative model, social
media platform, and ED-related community/topic. We employ cognitive domain
analysis metrics, including sentiment, emotions, etc., to evaluate the
effectiveness of interventions. Our findings reveal that civility-focused
interventions consistently improve positive sentiment and emotional tone across
all dimensions, while insight-resetting approaches tend to increase negative
emotions. We also uncover significant biases in LLM-generated conversations,
with cognitive metrics varying notably between models (Claude-3 Haiku $>$
Mistral $>$ GPT-3.5-turbo $>$ LLaMA3) and even between versions of the same
model. These variations highlight the importance of model selection in
simulating realistic discussions related to ED. Our work provides valuable
information on the complex dynamics of ED-related discussions and the
effectiveness of various intervention strategies.",2024-09-06,"Louis Penafiel, Hsien-Te Kao, Isabel Erickson, David Chu, Robert McCormack, Kristina Lerman, Svitlana Volkova",http://arxiv.org/pdf/2409.04043v1,cs.CL
Large Margin Prototypical Network for Few-shot Relation Classification with Fine-grained Features,"Relation classification (RC) plays a pivotal role in both natural language
understanding and knowledge graph completion. It is generally formulated as a
task to recognize the relationship between two entities of interest appearing
in a free-text sentence. Conventional approaches on RC, regardless of feature
engineering or deep learning based, can obtain promising performance on
categorizing common types of relation leaving a large proportion of
unrecognizable long-tail relations due to insufficient labeled instances for
training. In this paper, we consider few-shot learning is of great practical
significance to RC and thus improve a modern framework of metric learning for
few-shot RC. Specifically, we adopt the large-margin ProtoNet with fine-grained
features, expecting they can generalize well on long-tail relations. Extensive
experiments were conducted by FewRel, a large-scale supervised few-shot RC
dataset, to evaluate our framework: LM-ProtoNet (FGF). The results demonstrate
that it can achieve substantial improvements over many baseline approaches.",2024-09-06,"Miao Fan, Yeqi Bai, Mingming Sun, Ping Li",http://arxiv.org/pdf/2409.04009v1,cs.CL
OPAL: Outlier-Preserved Microscaling Quantization Accelerator for Generative Large Language Models,"To overcome the burden on the memory size and bandwidth due to
ever-increasing size of large language models (LLMs), aggressive weight
quantization has been recently studied, while lacking research on quantizing
activations. In this paper, we present a hardware-software co-design method
that results in an energy-efficient LLM accelerator, named OPAL, for generation
tasks. First of all, a novel activation quantization method that leverages the
microscaling data format while preserving several outliers per sub-tensor block
(e.g., four out of 128 elements) is proposed. Second, on top of preserving
outliers, mixed precision is utilized that sets 5-bit for inputs to sensitive
layers in the decoder block of an LLM, while keeping inputs to less sensitive
layers to 3-bit. Finally, we present the OPAL hardware architecture that
consists of FP units for handling outliers and vectorized INT multipliers for
dominant non-outlier related operations. In addition, OPAL uses log2-based
approximation on softmax operations that only requires shift and subtraction to
maximize power efficiency. As a result, we are able to improve the energy
efficiency by 1.6~2.2x, and reduce the area by 2.4~3.1x with negligible
accuracy loss, i.e., <1 perplexity increase.",2024-09-06,"Jahyun Koo, Dahoon Park, Sangwoo Jung, Jaeha Kung",http://arxiv.org/pdf/2409.05902v3,cs.CL
On The Role of Prompt Construction In Enhancing Efficacy and Efficiency of LLM-Based Tabular Data Generation,"LLM-based data generation for real-world tabular data can be challenged by
the lack of sufficient semantic context in feature names used to describe
columns. We hypothesize that enriching prompts with domain-specific insights
can improve both the quality and efficiency of data generation. To test this
hypothesis, we explore three prompt construction protocols: Expert-guided,
LLM-guided, and Novel-Mapping. Through empirical studies with the recently
proposed GReaT framework, we find that context-enriched prompts lead to
significantly improved data generation quality and training efficiency.",2024-09-06,"Banooqa Banday, Kowshik Thopalli, Tanzima Z. Islam, Jayaraman J. Thiagarajan",http://arxiv.org/pdf/2409.03946v2,cs.CL
Experimentation in Content Moderation using RWKV,"This paper investigates the RWKV model's efficacy in content moderation
through targeted experimentation. We introduce a novel dataset specifically
designed for distillation into smaller models, enhancing content moderation
practices. This comprehensive dataset encompasses images, videos, sounds, and
text data that present societal challenges. Leveraging advanced Large Language
Models (LLMs), we generated an extensive set of responses -- 558,958 for text
and 83,625 for images -- to train and refine content moderation systems. Our
core experimentation involved fine-tuning the RWKV model, capitalizing on its
CPU-efficient architecture to address large-scale content moderation tasks. By
highlighting the dataset's potential for knowledge distillation, this study not
only demonstrates RWKV's capability in improving the accuracy and efficiency of
content moderation systems but also paves the way for developing more compact,
resource-efficient models in this domain. Datasets and models can be found in
HuggingFace: https://huggingface.co/modrwkv",2024-09-05,"Umut Yildirim, Rohan Dutta, Burak Yildirim, Atharva Vaidya",http://arxiv.org/pdf/2409.03939v1,cs.CL
CACER: Clinical Concept Annotations for Cancer Events and Relations,"Clinical notes contain unstructured representations of patient histories,
including the relationships between medical problems and prescription drugs. To
investigate the relationship between cancer drugs and their associated symptom
burden, we extract structured, semantic representations of medical problem and
drug information from the clinical narratives of oncology notes. We present
Clinical Concept Annotations for Cancer Events and Relations (CACER), a novel
corpus with fine-grained annotations for over 48,000 medical problems and drug
events and 10,000 drug-problem and problem-problem relations. Leveraging CACER,
we develop and evaluate transformer-based information extraction (IE) models
such as BERT, Flan-T5, Llama3, and GPT-4 using fine-tuning and in-context
learning (ICL). In event extraction, the fine-tuned BERT and Llama3 models
achieved the highest performance at 88.2-88.0 F1, which is comparable to the
inter-annotator agreement (IAA) of 88.4 F1. In relation extraction, the
fine-tuned BERT, Flan-T5, and Llama3 achieved the highest performance at
61.8-65.3 F1. GPT-4 with ICL achieved the worst performance across both tasks.
The fine-tuned models significantly outperformed GPT-4 in ICL, highlighting the
importance of annotated training data and model optimization. Furthermore, the
BERT models performed similarly to Llama3. For our task, LLMs offer no
performance advantage over the smaller BERT models. The results emphasize the
need for annotated training data to optimize models. Multiple fine-tuned
transformer models achieved performance comparable to IAA for several
extraction tasks.",2024-09-05,"Yujuan Fu, Giridhar Kaushik Ramachandran, Ahmad Halwani, Bridget T. McInnes, Fei Xia, Kevin Lybarger, Meliha Yetisgen, Özlem Uzuner",http://arxiv.org/pdf/2409.03905v1,cs.CL
Sirius: Contextual Sparsity with Correction for Efficient LLMs,"With the blossom of large language models (LLMs), inference efficiency
becomes increasingly important. Various approximation methods are proposed to
reduce the cost at inference time. Contextual Sparsity (CS) is appealing for
its training-free nature and its ability to reach a higher compression ratio
seemingly without quality degradation. However, after a comprehensive
evaluation of contextual sparsity methods on various complex generation tasks,
we find that although CS succeeds in prompt-understanding tasks, CS
significantly degrades the model performance for reasoning, deduction, and
knowledge-based tasks. Despite the gap in end-to-end accuracy, we observed that
sparse models often share general problem-solving logic and require only a few
token corrections to recover the original model performance. This paper
introduces Sirius, an efficient correction mechanism, which significantly
recovers CS models quality on reasoning tasks while maintaining its efficiency
gain. Sirius is evaluated on 6 models with 8 difficult generation tasks in
reasoning, math, and coding and shows consistent effectiveness and efficiency.
Also, we carefully develop a system implementation for Sirius and show that
Sirius achieves roughly 20% reduction in latency for 8B model on-chip and 35%
reduction for 70B model offloading. We open-source our implementation of Sirius
at https://github.com/Infini-AI-Lab/Sirius.git.",2024-09-05,"Yang Zhou, Zhuoming Chen, Zhaozhuo Xu, Victoria Lin, Beidi Chen",http://arxiv.org/pdf/2409.03856v1,cs.CL
Persona Setting Pitfall: Persistent Outgroup Biases in Large Language Models Arising from Social Identity Adoption,"Drawing parallels between human cognition and artificial intelligence, we
explored how large language models (LLMs) internalize identities imposed by
targeted prompts. Informed by Social Identity Theory, these identity
assignments lead LLMs to distinguish between ""we"" (the ingroup) and ""they"" (the
outgroup). This self-categorization generates both ingroup favoritism and
outgroup bias. Nonetheless, existing literature has predominantly focused on
ingroup favoritism, often overlooking outgroup bias, which is a fundamental
source of intergroup prejudice and discrimination. Our experiment addresses
this gap by demonstrating that outgroup bias manifests as strongly as ingroup
favoritism. Furthermore, we successfully mitigated the inherent pro-liberal,
anti-conservative bias in LLMs by guiding them to adopt the perspectives of the
initially disfavored group. These results were replicated in the context of
gender bias. Our findings highlight the potential to develop more equitable and
balanced language models.",2024-09-05,"Wenchao Dong, Assem Zhunis, Dongyoung Jeong, Hyojin Chin, Jiyoung Han, Meeyoung Cha",http://arxiv.org/pdf/2409.03843v1,cs.CL
Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding,"Complex 3D scene understanding has gained increasing attention, with scene
encoding strategies playing a crucial role in this success. However, the
optimal scene encoding strategies for various scenarios remain unclear,
particularly compared to their image-based counterparts. To address this issue,
we present a comprehensive study that probes various visual encoding models for
3D scene understanding, identifying the strengths and limitations of each model
across different scenarios. Our evaluation spans seven vision foundation
encoders, including image-based, video-based, and 3D foundation models. We
evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual
Grounding, Segmentation, and Registration, each focusing on different aspects
of scene understanding. Our evaluations yield key findings: DINOv2 demonstrates
superior performance, video models excel in object-level tasks, diffusion
models benefit geometric tasks, and language-pretrained models show unexpected
limitations in language-related tasks. These insights challenge some
conventional understandings, provide novel perspectives on leveraging visual
foundation models, and highlight the need for more flexible encoder selection
in future vision-language and scene-understanding tasks. Code:
https://github.com/YunzeMan/Lexicon3D",2024-09-05,"Yunze Man, Shuhong Zheng, Zhipeng Bao, Martial Hebert, Liang-Yan Gui, Yu-Xiong Wang",http://arxiv.org/pdf/2409.03757v3,cs.CL
WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild,"The increasing availability of real-world conversation data offers exciting
opportunities for researchers to study user-chatbot interactions. However, the
sheer volume of this data makes manually examining individual conversations
impractical. To overcome this challenge, we introduce WildVis, an interactive
tool that enables fast, versatile, and large-scale conversation analysis.
WildVis provides search and visualization capabilities in the text and
embedding spaces based on a list of criteria. To manage million-scale datasets,
we implemented optimizations including search index construction, embedding
precomputation and compression, and caching to ensure responsive user
interactions within seconds. We demonstrate WildVis' utility through three case
studies: facilitating chatbot misuse research, visualizing and comparing topic
distributions across datasets, and characterizing user-specific conversation
patterns. WildVis is open-source and designed to be extendable, supporting
additional datasets and customized search and visualization functionalities.",2024-09-05,"Yuntian Deng, Wenting Zhao, Jack Hessel, Xiang Ren, Claire Cardie, Yejin Choi",http://arxiv.org/pdf/2409.03753v2,cs.CL
Attention Heads of Large Language Models: A Survey,"Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in
various tasks but remain as black-box systems. Understanding the reasoning
bottlenecks of LLMs has become a critical challenge, as these limitations are
deeply tied to their internal architecture. Among these, attention heads have
emerged as a focal point for investigating the underlying mechanics of LLMs. In
this survey, we aim to demystify the internal reasoning processes of LLMs by
systematically exploring the roles and mechanisms of attention heads. We first
introduce a novel four-stage framework inspired by the human thought process:
Knowledge Recalling, In-Context Identification, Latent Reasoning, and
Expression Preparation. Using this framework, we comprehensively review
existing research to identify and categorize the functions of specific
attention heads. Additionally, we analyze the experimental methodologies used
to discover these special heads, dividing them into two categories:
Modeling-Free and Modeling-Required methods. We further summarize relevant
evaluation methods and benchmarks. Finally, we discuss the limitations of
current research and propose several potential future directions.",2024-09-05,"Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Mingchuan Yang, Bo Tang, Feiyu Xiong, Zhiyu Li",http://arxiv.org/pdf/2409.03752v3,cs.CL
How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data,"Recently, there has been a growing interest in studying how to construct
better code instruction tuning data. However, we observe Code models trained
with these datasets exhibit high performance on HumanEval but perform worse on
other benchmarks such as LiveCodeBench. Upon further investigation, we find
that many datasets suffer from severe data leakage. After cleaning up most of
the leaked data, some well-known high-quality datasets perform poorly. This
discovery reveals a new challenge: identifying which dataset genuinely qualify
as high-quality code instruction data. To address this, we propose an efficient
code data pruning strategy for selecting good samples. Our approach is based on
three dimensions: instruction complexity, response quality, and instruction
diversity. Based on our selected data, we present XCoder, a family of models
finetuned from LLaMA3. Our experiments show XCoder achieves new
state-of-the-art performance using fewer training data, which verify the
effectiveness of our data strategy. Moreover, we perform a comprehensive
analysis on the data composition and find existing code datasets have different
characteristics according to their construction methods, which provide new
insights for future code LLMs. Our models and dataset are released in
https://github.com/banksy23/XCoder",2024-09-05,"Yejie Wang, Keqing He, Dayuan Fu, Zhuoma Gongque, Heyang Xu, Yanxu Chen, Zhexu Wang, Yujia Fu, Guanting Dong, Muxi Diao, Jingang Wang, Mengdi Zhang, Xunliang Cai, Weiran Xu",http://arxiv.org/pdf/2409.03810v1,cs.CL
Planning In Natural Language Improves LLM Search For Code Generation,"While scaling training compute has led to remarkable improvements in large
language models (LLMs), scaling inference compute has not yet yielded analogous
gains. We hypothesize that a core missing component is a lack of diverse LLM
outputs, leading to inefficient search due to models repeatedly sampling highly
similar, yet incorrect generations. We empirically demonstrate that this lack
of diversity can be mitigated by searching over candidate plans for solving a
problem in natural language. Based on this insight, we propose PlanSearch, a
novel search algorithm which shows strong results across HumanEval+, MBPP+, and
LiveCodeBench (a contamination-free benchmark for competitive coding).
PlanSearch generates a diverse set of observations about the problem and then
uses these observations to construct plans for solving the problem. By
searching over plans in natural language rather than directly over code
solutions, PlanSearch explores a significantly more diverse range of potential
solutions compared to baseline search methods. Using PlanSearch on top of
Claude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on
LiveCodeBench, outperforming both the best score achieved without search
(pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).
Finally, we show that, across all models, search algorithms, and benchmarks
analyzed, we can accurately predict performance gains due to search as a direct
function of the diversity over generated ideas. Code can be found at
https://github.com/scaleapi/plansearch.",2024-09-05,"Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, Hugh Zhang",http://arxiv.org/pdf/2409.03733v2,cs.CL
RAG based Question-Answering for Contextual Response Prediction System,"Large Language Models (LLMs) have shown versatility in various Natural
Language Processing (NLP) tasks, including their potential as effective
question-answering systems. However, to provide precise and relevant
information in response to specific customer queries in industry settings, LLMs
require access to a comprehensive knowledge base to avoid hallucinations.
Retrieval Augmented Generation (RAG) emerges as a promising technique to
address this challenge. Yet, developing an accurate question-answering
framework for real-world applications using RAG entails several challenges: 1)
data availability issues, 2) evaluating the quality of generated content, and
3) the costly nature of human evaluation. In this paper, we introduce an
end-to-end framework that employs LLMs with RAG capabilities for industry use
cases. Given a customer query, the proposed system retrieves relevant knowledge
documents and leverages them, along with previous chat history, to generate
response suggestions for customer service agents in the contact centers of a
major retail company. Through comprehensive automated and human evaluations, we
show that this solution outperforms the current BERT-based algorithms in
accuracy and relevance. Our findings suggest that RAG-based LLMs can be an
excellent support to human customer service representatives by lightening their
workload.",2024-09-05,"Sriram Veturi, Saurabh Vaichal, Reshma Lal Jagadheesh, Nafis Irtiza Tripto, Nian Yan",http://arxiv.org/pdf/2409.03708v2,cs.CL
A Different Level Text Protection Mechanism With Differential Privacy,"The article introduces a method for extracting words of different degrees of
importance based on the BERT pre-training model and proves the effectiveness of
this method. The article also discusses the impact of maintaining the same
perturbation results for words of different importance on the overall text
utility. This method can be applied to long text protection.",2024-09-05,Qingwen Fu,http://arxiv.org/pdf/2409.03707v1,cs.CL
LAST: Language Model Aware Speech Tokenization,"Speech tokenization serves as the foundation of speech language model (LM),
enabling them to perform various tasks such as spoken language modeling,
text-to-speech, speech-to-text, etc. Most speech tokenizers are trained
independently of the LM training process, relying on separate acoustic models
and quantization methods. Following such an approach may create a mismatch
between the tokenization process and its usage afterward. In this study, we
propose a novel approach to training a speech tokenizer by leveraging
objectives from pre-trained textual LMs. We advocate for the integration of
this objective into the process of learning discrete speech representations.
Our aim is to transform features from a pre-trained speech model into a new
feature space that enables better clustering for speech LMs. We empirically
investigate the impact of various model design choices, including speech
vocabulary size and text LM size. Our results demonstrate the proposed
tokenization method outperforms the evaluated baselines considering both spoken
language modeling and speech-to-text. More importantly, unlike prior work, the
proposed method allows the utilization of a single pre-trained LM for
processing both speech and text inputs, setting it apart from conventional
tokenization approaches.",2024-09-05,"Arnon Turetzky, Yossi Adi",http://arxiv.org/pdf/2409.03701v2,cs.CL
A Fused Large Language Model for Predicting Startup Success,"Investors are continuously seeking profitable investment opportunities in
startups and, hence, for effective decision-making, need to predict a startup's
probability of success. Nowadays, investors can use not only various
fundamental information about a startup (e.g., the age of the startup, the
number of founders, and the business sector) but also textual description of a
startup's innovation and business model, which is widely available through
online venture capital (VC) platforms such as Crunchbase. To support the
decision-making of investors, we develop a machine learning approach with the
aim of locating successful startups on VC platforms. Specifically, we develop,
train, and evaluate a tailored, fused large language model to predict startup
success. Thereby, we assess to what extent self-descriptions on VC platforms
are predictive of startup success. Using 20,172 online profiles from
Crunchbase, we find that our fused large language model can predict startup
success, with textual self-descriptions being responsible for a significant
part of the predictive power. Our work provides a decision support tool for
investors to find profitable investment opportunities.",2024-09-05,"Abdurahman Maarouf, Stefan Feuerriegel, Nicolas Pröllochs",http://arxiv.org/pdf/2409.03668v1,cs.CL
The representation landscape of few-shot learning and fine-tuning in large language models,"In-context learning (ICL) and supervised fine-tuning (SFT) are two common
strategies for improving the performance of modern large language models (LLMs)
on specific tasks. Despite their different natures, these strategies often lead
to comparable performance gains. However, little is known about whether they
induce similar representations inside LLMs. We approach this problem by
analyzing the probability landscape of their hidden representations in the two
cases. More specifically, we compare how LLMs solve the same question-answering
task, finding that ICL and SFT create very different internal structures, in
both cases undergoing a sharp transition in the middle of the network. In the
first half of the network, ICL shapes interpretable representations
hierarchically organized according to their semantic content. In contrast, the
probability landscape obtained with SFT is fuzzier and semantically mixed. In
the second half of the model, the fine-tuned representations develop
probability modes that better encode the identity of answers, while the
landscape of ICL representations is characterized by less defined peaks. Our
approach reveals the diverse computational strategies developed inside LLMs to
solve the same task across different conditions, allowing us to make a step
towards designing optimal methods to extract information from language models.",2024-09-05,"Diego Doimo, Alessandro Serra, Alessio Ansuini, Alberto Cazzaniga",http://arxiv.org/pdf/2409.03662v2,cs.CL
LLM-based multi-agent poetry generation in non-cooperative environments,"Despite substantial progress of large language models (LLMs) for automatic
poetry generation, the generated poetry lacks diversity while the training
process differs greatly from human learning. Under the rationale that the
learning process of the poetry generation systems should be more human-like and
their output more diverse and novel, we introduce a framework based on social
learning where we emphasize non-cooperative interactions besides cooperative
interactions to encourage diversity. Our experiments are the first attempt at
LLM-based multi-agent systems in non-cooperative environments for poetry
generation employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED
agents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows
that our framework benefits the poetry generation process for TRAINING-BASED
agents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity
and a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams.
The generated poetry from TRAINING-BASED agents also exhibits group divergence
in terms of lexicons, styles and semantics. PROMPTING-BASED agents in our
framework also benefit from non-cooperative environments and a more diverse
ensemble of models with non-homogeneous agents has the potential to further
enhance diversity, with an increase of 7.0-17.5 pp according to our
experiments. However, PROMPTING-BASED agents show a decrease in lexical
diversity over time and do not exhibit the group-based divergence intended in
the social network. Our paper argues for a paradigm shift in creative tasks
such as automatic poetry generation to include social learning processes (via
LLM-based agent modeling) similar to human interaction.",2024-09-05,"Ran Zhang, Steffen Eger",http://arxiv.org/pdf/2409.03659v2,cs.CL
On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization,"Reinforcement Learning from Human Feedback (RLHF) is an effective approach
for aligning language models to human preferences. Central to RLHF is learning
a reward function for scoring human preferences. Two main approaches for
learning a reward model are 1) training an EXplicit Reward Model (EXRM) as in
RLHF, and 2) using an implicit reward learned from preference data through
methods such as Direct Preference Optimization (DPO). Prior work has shown that
the implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in
the limit. DPORM's effectiveness directly implies the optimality of the learned
policy, and also has practical implication for LLM alignment methods including
iterative DPO. However, it is unclear how well DPORM empirically matches the
performance of EXRM. This work studies the accuracy at distinguishing preferred
and rejected answers for both DPORM and EXRM. Our findings indicate that even
though DPORM fits the training dataset comparably, it generalizes less
effectively than EXRM, especially when the validation datasets contain
distribution shifts. Across five out-of-distribution settings, DPORM has a mean
drop in accuracy of 3% and a maximum drop of 7%. These findings highlight that
DPORM has limited generalization ability and substantiates the integration of
an explicit reward model in iterative DPO approaches.",2024-09-05,"Yong Lin, Skyler Seto, Maartje ter Hoeve, Katherine Metcalf, Barry-John Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang",http://arxiv.org/pdf/2409.03650v2,cs.CL
Image Over Text: Transforming Formula Recognition Evaluation with Character Detection Matching,"Formula recognition presents significant challenges due to the complicated
structure and varied notation of mathematical expressions. Despite continuous
advancements in formula recognition models, the evaluation metrics employed by
these models, such as BLEU and Edit Distance, still exhibit notable
limitations. They overlook the fact that the same formula has diverse
representations and is highly sensitive to the distribution of training data,
thereby causing unfairness in formula recognition evaluation. To this end, we
propose a Character Detection Matching (CDM) metric, ensuring the evaluation
objectivity by designing an image-level rather than a LaTeX-level metric score.
Specifically, CDM renders both the model-predicted LaTeX and the ground-truth
LaTeX formulas into image-formatted formulas, then employs visual feature
extraction and localization techniques for precise character-level matching,
incorporating spatial position information. Such a spatially-aware and
character-matching method offers a more accurate and equitable evaluation
compared with previous BLEU and Edit Distance metrics that rely solely on
text-based character matching. Experimentally, we evaluated various formula
recognition models using CDM, BLEU, and ExpRate metrics. Their results
demonstrate that the CDM aligns more closely with human evaluation standards
and provides a fairer comparison across different models by eliminating
discrepancies caused by diverse formula representations. Code is available at
https://github.com/opendatalab/UniMERNet/tree/main/cdm.",2024-09-05,"Bin Wang, Fan Wu, Linke Ouyang, Zhuangcheng Gu, Rui Zhang, Renqiu Xia, Bo Zhang, Conghui He",http://arxiv.org/pdf/2409.03643v2,cs.CL
"Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers","In decoder-based LLMs, the representation of a given layer serves two
purposes: as input to the next layer during the computation of the current
token; and as input to the attention mechanism of future tokens. In this work,
we show that the importance of the latter role might be overestimated. To show
that, we start by manipulating the representations of previous tokens; e.g. by
replacing the hidden states at some layer k with random vectors. Our
experimenting with four LLMs and four tasks show that this operation often
leads to small to negligible drop in performance. Importantly, this happens if
the manipulation occurs in the top part of the model-k is in the final 30-50%
of the layers. In contrast, doing the same manipulation in earlier layers might
lead to chance level performance. We continue by switching the hidden state of
certain tokens with hidden states of other tokens from another prompt; e.g.,
replacing the word ""Italy"" with ""France"" in ""What is the capital of Italy?"". We
find that when applying this switch in the top 1/3 of the model, the model
ignores it (answering ""Rome""). However if we apply it before, the model
conforms to the switch (""Paris""). Our results hint at a two stage process in
transformer-based LLMs: the first part gathers input from previous tokens,
while the second mainly processes that information internally.",2024-09-05,"Amit Ben-Artzy, Roy Schwartz",http://arxiv.org/pdf/2409.03621v2,cs.CL
Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble,"Increasing use of large language models (LLMs) demand performant guardrails
to ensure the safety of inputs and outputs of LLMs. When these safeguards are
trained on imbalanced data, they can learn the societal biases. We present a
light-weight, post-processing method for mitigating counterfactual fairness in
closed-source text safety classifiers. Our approach involves building an
ensemble that not only outperforms the input classifiers and policy-aligns
them, but also acts as a debiasing regularizer. We introduce two
threshold-agnostic metrics to assess the counterfactual fairness of a model,
and demonstrate how combining these metrics with Fair Data Reweighting (FDW)
helps mitigate biases. We create an expanded Open AI dataset, and a new
templated LLM-generated dataset based on user-prompts, both of which are
counterfactually balanced across identity groups and cover four key areas of
safety; we will work towards publicly releasing these datasets. Our results
show that our approach improves counterfactual fairness with minimal impact on
model performance.",2024-09-05,"Olivia Sturman, Aparna Joshi, Bhaktipriya Radharapu, Piyush Kumar, Renee Shelby",http://arxiv.org/pdf/2409.13705v2,cs.CL
100 instances is all you need: predicting the success of a new LLM on unseen data by testing on a few instances,"Predicting the performance of LLMs on individual task instances is essential
to ensure their reliability in high-stakes applications. To do so, a
possibility is to evaluate the considered LLM on a set of task instances and
train an assessor to predict its performance based on features of the
instances. However, this approach requires evaluating each new LLM on a
sufficiently large set of task instances to train an assessor specific to it.
In this work, we leverage the evaluation results of previously tested LLMs to
reduce the number of evaluations required to predict the performance of a new
LLM. In practice, we propose to test the new LLM on a small set of reference
instances and train a generic assessor which predicts the performance of the
LLM on an instance based on the performance of the former on the reference set
and features of the instance of interest. We conduct empirical studies on
HELM-Lite and KindsOfReasoning, a collection of existing reasoning datasets
that we introduce, where we evaluate all instruction-fine-tuned OpenAI models
until the January 2024 version of GPT4. When predicting performance on
instances with the same distribution as those used to train the generic
assessor, we find this achieves performance comparable to the LLM-specific
assessors trained on the full set of instances. Additionally, we find that
randomly selecting the reference instances performs as well as some advanced
selection methods we tested. For out of distribution, however, no clear winner
emerges and the overall performance is worse, suggesting that the inherent
predictability of LLMs is low.",2024-09-05,"Lorenzo Pacchiardi, Lucy G. Cheke, José Hernández-Orallo",http://arxiv.org/pdf/2409.03563v1,cs.CL
From MOOC to MAIC: Reshaping Online Teaching and Learning through LLM-driven Agents,"Since the first instances of online education, where courses were uploaded to
accessible and shared online platforms, this form of scaling the dissemination
of human knowledge to reach a broader audience has sparked extensive discussion
and widespread adoption. Recognizing that personalized learning still holds
significant potential for improvement, new AI technologies have been
continuously integrated into this learning format, resulting in a variety of
educational AI applications such as educational recommendation and intelligent
tutoring. The emergence of intelligence in large language models (LLMs) has
allowed for these educational enhancements to be built upon a unified
foundational model, enabling deeper integration. In this context, we propose
MAIC (Massive AI-empowered Course), a new form of online education that
leverages LLM-driven multi-agent systems to construct an AI-augmented
classroom, balancing scalability with adaptivity. Beyond exploring the
conceptual framework and technical innovations, we conduct preliminary
experiments at Tsinghua University, one of China's leading universities.
Drawing from over 100,000 learning records of more than 500 students, we obtain
a series of valuable observations and initial analyses. This project will
continue to evolve, ultimately aiming to establish a comprehensive open
platform that supports and unifies research, technology, and applications in
exploring the possibilities of online education in the era of large model AI.
We envision this platform as a collaborative hub, bringing together educators,
researchers, and innovators to collectively explore the future of AI-driven
online education.",2024-09-05,"Jifan Yu, Zheyuan Zhang, Daniel Zhang-li, Shangqing Tu, Zhanxin Hao, Rui Miao Li, Haoxuan Li, Yuanchun Wang, Hanming Li, Linlu Gong, Jie Cao, Jiayin Lin, Jinchang Zhou, Fei Qin, Haohua Wang, Jianxiao Jiang, Lijun Deng, Yisi Zhan, Chaojun Xiao, Xusheng Dai, Xuan Yan, Nianyi Lin, Nan Zhang, Ruixin Ni, Yang Dang, Lei Hou, Yu Zhang, Xu Han, Manli Li, Juanzi Li, Zhiyuan Liu, Huiqin Liu, Maosong Sun",http://arxiv.org/pdf/2409.03512v1,cs.CL
How Much Data is Enough Data? Fine-Tuning Large Language Models for In-House Translation: Performance Evaluation Across Multiple Dataset Sizes,"Decoder-only LLMs have shown impressive performance in MT due to their
ability to learn from extensive datasets and generate high-quality
translations. However, LLMs often struggle with the nuances and style required
for organisation-specific translation. In this study, we explore the
effectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3
8B Instruct, leveraging translation memories (TMs), as a valuable resource to
enhance accuracy and efficiency. We investigate the impact of fine-tuning the
Llama 3 model using TMs from a specific organisation in the software sector.
Our experiments cover five translation directions across languages of varying
resource levels (English to Brazilian Portuguese, Czech, German, Finnish, and
Korean). We analyse diverse sizes of training datasets (1k to 207k segments) to
evaluate their influence on translation quality. We fine-tune separate models
for each training set and evaluate their performance based on automatic
metrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in
translation performance with larger datasets across all metrics. On average,
BLEU and COMET scores increase by 13 and 25 points, respectively, on the
largest training set against the baseline model. Notably, there is a
performance deterioration in comparison with the baseline model when
fine-tuning on only 1k and 2k examples; however, we observe a substantial
improvement as the training dataset size increases. The study highlights the
potential of integrating TMs with LLMs to create bespoke translation models
tailored to the specific needs of businesses, thus enhancing translation
quality and reducing turn-around times. This approach offers a valuable insight
for organisations seeking to leverage TMs and LLMs for optimal translation
outcomes, especially in narrower domains.",2024-09-05,"Inacio Vieira, Will Allred, Séamus Lankford, Sheila Castilho, Andy Way",http://arxiv.org/pdf/2409.03454v2,cs.CL
"Fine-tuning large language models for domain adaptation: Exploration of training strategies, scaling, model merging and synergistic capabilities","The advancement of Large Language Models (LLMs) for domain applications in
fields such as materials science and engineering depends on the development of
fine-tuning strategies that adapt models for specialized, technical
capabilities. In this work, we explore the effects of Continued Pretraining
(CPT), Supervised Fine-Tuning (SFT), and various preference-based optimization
approaches, including Direct Preference Optimization (DPO) and Odds Ratio
Preference Optimization (ORPO), on fine-tuned LLM performance. Our analysis
shows how these strategies influence model outcomes and reveals that the
merging of multiple fine-tuned models can lead to the emergence of capabilities
that surpass the individual contributions of the parent models. We find that
model merging leads to new functionalities that neither parent model could
achieve alone, leading to improved performance in domain-specific assessments.
Experiments with different model architectures are presented, including Llama
3.1 8B and Mistral 7B models, where similar behaviors are observed. Exploring
whether the results hold also for much smaller models, we use a tiny LLM with
1.7 billion parameters and show that very small LLMs do not necessarily feature
emergent capabilities under model merging, suggesting that model scaling may be
a key component. In open-ended yet consistent chat conversations between a
human and AI models, our assessment reveals detailed insights into how
different model variants perform and show that the smallest model achieves a
high intelligence score across key criteria including reasoning depth,
creativity, clarity, and quantitative precision. Other experiments include the
development of image generation prompts based on disparate biological material
design concepts, to create new microstructures, architectural concepts, and
urban design based on biological materials-inspired construction principles.",2024-09-05,"Wei Lu, Rachel K. Luu, Markus J. Buehler",http://arxiv.org/pdf/2409.03444v1,cs.CL
Rx Strategist: Prescription Verification using LLM Agents System,"To protect patient safety, modern pharmaceutical complexity demands strict
prescription verification. We offer a new approach - Rx Strategist - that makes
use of knowledge graphs and different search strategies to enhance the power of
Large Language Models (LLMs) inside an agentic framework. This multifaceted
technique allows for a multi-stage LLM pipeline and reliable information
retrieval from a custom-built active ingredient database. Different facets of
prescription verification, such as indication, dose, and possible drug
interactions, are covered in each stage of the pipeline. We alleviate the
drawbacks of monolithic LLM techniques by spreading reasoning over these
stages, improving correctness and reliability while reducing memory demands.
Our findings demonstrate that Rx Strategist surpasses many current LLMs,
achieving performance comparable to that of a highly experienced clinical
pharmacist. In the complicated world of modern medications, this combination of
LLMs with organized knowledge and sophisticated search methods presents a
viable avenue for reducing prescription errors and enhancing patient outcomes.",2024-09-05,"Phuc Phan Van, Dat Nguyen Minh, An Dinh Ngoc, Huy Phan Thanh",http://arxiv.org/pdf/2409.03440v1,cs.CL
Entity Extraction from High-Level Corruption Schemes via Large Language Models,"The rise of financial crime that has been observed in recent years has
created an increasing concern around the topic and many people, organizations
and governments are more and more frequently trying to combat it. Despite the
increase of interest in this area, there is a lack of specialized datasets that
can be used to train and evaluate works that try to tackle those problems. This
article proposes a new micro-benchmark dataset for algorithms and models that
identify individuals and organizations, and their multiple writings, in news
articles, and presents an approach that assists in its creation. Experimental
efforts are also reported, using this dataset, to identify individuals and
organizations in financial-crime-related articles using various low-billion
parameter Large Language Models (LLMs). For these experiments, standard metrics
(Accuracy, Precision, Recall, F1 Score) are reported and various prompt
variants comprising the best practices of prompt engineering are tested. In
addition, to address the problem of ambiguous entity mentions, a simple, yet
effective LLM-based disambiguation method is proposed, ensuring that the
evaluation aligns with reality. Finally, the proposed approach is compared
against a widely used state-of-the-art open-source baseline, showing the
superiority of the proposed method.",2024-09-05,"Panagiotis Koletsis, Panagiotis-Konstantinos Gemos, Christos Chronis, Iraklis Varlamis, Vasilis Efthymiou, Georgios Th. Papadopoulos",http://arxiv.org/pdf/2409.13704v2,cs.CL
CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks,"Cognitive psychology investigates perception, attention, memory, language,
problem-solving, decision-making, and reasoning. Kahneman's dual-system theory
elucidates the human decision-making process, distinguishing between the rapid,
intuitive System 1 and the deliberative, rational System 2. Recent advancements
have positioned large language Models (LLMs) as formidable tools nearing
human-level proficiency in various cognitive tasks. Nonetheless, the presence
of a dual-system framework analogous to human cognition in LLMs remains
unexplored. This study introduces the \textbf{CogniDual Framework for LLMs}
(CFLLMs), designed to assess whether LLMs can, through self-training, evolve
from deliberate deduction to intuitive responses, thereby emulating the human
process of acquiring and mastering new information. Our findings reveal the
cognitive mechanisms behind LLMs' response generation, enhancing our
understanding of their capabilities in cognitive psychology. Practically,
self-trained models can provide faster responses to certain queries, reducing
computational demands during inference.",2024-09-05,"Yongxin Deng, Xihe Qiu, Xiaoyu Tan, Chao Qu, Jing Pan, Yuan Cheng, Yinghui Xu, Wei Chu",http://arxiv.org/pdf/2409.03381v2,cs.CL
Leveraging Large Language Models through Natural Language Processing to provide interpretable Machine Learning predictions of mental deterioration in real time,"Based on official estimates, 50 million people worldwide are affected by
dementia, and this number increases by 10 million new patients every year.
Without a cure, clinical prognostication and early intervention represent the
most effective ways to delay its progression. To this end, Artificial
Intelligence and computational linguistics can be exploited for natural
language analysis, personalized assessment, monitoring, and treatment. However,
traditional approaches need more semantic knowledge management and
explicability capabilities. Moreover, using Large Language Models (LLMs) for
cognitive decline diagnosis is still scarce, even though these models represent
the most advanced way for clinical-patient communication using intelligent
systems. Consequently, we leverage an LLM using the latest Natural Language
Processing (NLP) techniques in a chatbot solution to provide interpretable
Machine Learning prediction of cognitive decline in real-time.
Linguistic-conceptual features are exploited for appropriate natural language
analysis. Through explainability, we aim to fight potential biases of the
models and improve their potential to help clinical workers in their diagnosis
decisions. More in detail, the proposed pipeline is composed of (i) data
extraction employing NLP-based prompt engineering; (ii) stream-based data
processing including feature engineering, analysis, and selection; (iii)
real-time classification; and (iv) the explainability dashboard to provide
visual and natural language descriptions of the prediction outcome.
Classification results exceed 80 % in all evaluation metrics, with a recall
value for the mental deterioration class about 85 %. To sum up, we contribute
with an affordable, flexible, non-invasive, personalized diagnostic system to
this work.",2024-09-05,"Francisco de Arriba-Pérez, Silvia García-Méndez",http://arxiv.org/pdf/2409.03375v1,cs.CL
Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding,"The training data in large language models is key to their success, but it
also presents privacy and security risks, as it may contain sensitive
information. Detecting pre-training data is crucial for mitigating these
concerns. Existing methods typically analyze target text in isolation or solely
with non-member contexts, overlooking potential insights from simultaneously
considering both member and non-member contexts. While previous work suggested
that member contexts provide little information due to the minor distributional
shift they induce, our analysis reveals that these subtle shifts can be
effectively leveraged when contrasted with non-member contexts. In this paper,
we propose Con-ReCall, a novel approach that leverages the asymmetric
distributional shifts induced by member and non-member contexts through
contrastive decoding, amplifying subtle differences to enhance membership
inference. Extensive empirical evaluations demonstrate that Con-ReCall achieves
state-of-the-art performance on the WikiMIA benchmark and is robust against
various text manipulation techniques.",2024-09-05,"Cheng Wang, Yiwei Wang, Bryan Hooi, Yujun Cai, Nanyun Peng, Kai-Wei Chang",http://arxiv.org/pdf/2409.03363v2,cs.CL
Sketch: A Toolkit for Streamlining LLM Operations,"Large language models (LLMs) represented by GPT family have achieved
remarkable success. The characteristics of LLMs lie in their ability to
accommodate a wide range of tasks through a generative approach. However, the
flexibility of their output format poses challenges in controlling and
harnessing the model's outputs, thereby constraining the application of LLMs in
various domains. In this work, we present Sketch, an innovative toolkit
designed to streamline LLM operations across diverse fields. Sketch comprises
the following components: (1) a suite of task description schemas and prompt
templates encompassing various NLP tasks; (2) a user-friendly, interactive
process for building structured output LLM services tailored to various NLP
tasks; (3) an open-source dataset for output format control, along with tools
for dataset construction; and (4) an open-source model based on
LLaMA3-8B-Instruct that adeptly comprehends and adheres to output formatting
instructions. We anticipate this initiative to bring considerable convenience
to LLM users, achieving the goal of ''plug-and-play'' for various applications.
The components of Sketch will be progressively open-sourced at
https://github.com/cofe-ai/Sketch.",2024-09-05,"Xin Jiang, Xiang Li, Wenjia Ma, Xuezhi Fang, Yiqun Yao, Naitong Yu, Xuying Meng, Peng Han, Jing Li, Aixin Sun, Yequan Wang",http://arxiv.org/pdf/2409.03346v1,cs.CL
Normal forms in Virus Machines,"In the present work, we further study the computational power of virus
machines (VMs in short).VMs provide a computing paradigm inspired by the
transmission and replication networks of viruses.VMs consist of process units
(called hosts) structured by a directed graph whose arcs are called channels
and an instruction graph that controls the transmissions of virus objects among
hosts. The present work complements our understanding of the computing power of
VMs by introducing normal forms; these expressions restrict the features in a
given computing model.Some of the features that we restrict in our normal forms
include (a) the number of hosts, (b) the number of instructions, and (c) the
number of virus objects in each host. After we recall some known results on the
computing power of VMs we give our series of normal forms, such as the size of
the loops in the network, proving new characterisations of family of sets, such
as finite sets, semilinear sets, or recursively enumerable sets (NRE).",2024-09-05,"A. Ramírez-de-Arellano, F. G. C. Cabarle, D. Orellana-Martín, M. J. Pérez-Jiménez",http://arxiv.org/pdf/2409.03327v2,cs.CL
N-gram Prediction and Word Difference Representations for Language Modeling,"Causal language modeling (CLM) serves as the foundational framework
underpinning remarkable successes of recent large language models (LLMs).
Despite its success, the training approach for next word prediction poses a
potential risk of causing the model to overly focus on local dependencies
within a sentence. While prior studies have been introduced to predict future N
words simultaneously, they were primarily applied to tasks such as masked
language modeling (MLM) and neural machine translation (NMT). In this study, we
introduce a simple N-gram prediction framework for the CLM task. Moreover, we
introduce word difference representation (WDR) as a surrogate and
contextualized target representation during model training on the basis of
N-gram prediction framework. To further enhance the quality of next word
prediction, we propose an ensemble method that incorporates the future N words'
prediction results. Empirical evaluations across multiple benchmark datasets
encompassing CLM and NMT tasks demonstrate the significant advantages of our
proposed methods over the conventional CLM.",2024-09-05,"DongNyeong Heo, Daniela Noemi Rim, Heeyoul Choi",http://arxiv.org/pdf/2409.03295v1,cs.CL
LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts,"With the emergence of widely available powerful LLMs, disinformation
generated by large Language Models (LLMs) has become a major concern.
Historically, LLM detectors have been touted as a solution, but their
effectiveness in the real world is still to be proven. In this paper, we focus
on an important setting in information operations -- short news-like posts
generated by moderately sophisticated attackers.
  We demonstrate that existing LLM detectors, whether zero-shot or
purpose-trained, are not ready for real-world use in that setting. All tested
zero-shot detectors perform inconsistently with prior benchmarks and are highly
vulnerable to sampling temperature increase, a trivial attack absent from
recent benchmarks. A purpose-trained detector generalizing across LLMs and
unseen attacks can be developed, but it fails to generalize to new
human-written texts.
  We argue that the former indicates domain-specific benchmarking is needed,
while the latter suggests a trade-off between the adversarial evasion
resilience and overfitting to the reference human text, with both needing
evaluation in benchmarks and currently absent. We believe this suggests a
re-consideration of current LLM detector benchmarking approaches and provides a
dynamically extensible benchmark to allow it
(https://github.com/Reliable-Information-Lab-HEVS/benchmark_llm_texts_detection).",2024-09-05,"Henrique Da Silva Gameiro, Andrei Kucharavy, Ljiljana Dolamic",http://arxiv.org/pdf/2409.03291v2,cs.CL
Shaping the Future of Endangered and Low-Resource Languages -- Our Role in the Age of LLMs: A Keynote at ECIR 2024,"Isidore of Seville is credited with the adage that it is language that gives
birth to a people, and not the other way around , underlining the profound role
played by language in the formation of cultural and social identity. Today, of
the more than 7100 languages listed, a significant number are endangered. Since
the 1970s, linguists, information seekers and enthusiasts have helped develop
digital resources and automatic tools to support a wide range of languages,
including endangered ones. The advent of Large Language Model (LLM)
technologies holds both promise and peril. They offer unprecedented
possibilities for the translation and generation of content and resources, key
elements in the preservation and revitalisation of languages. They also present
threat of homogenisation, cultural oversimplification and the further
marginalisation of already vulnerable languages. The talk this paper is based
on has proposed an initiatory journey, exploring the potential paths and
partnerships between technology and tradition, with a particular focus on the
Occitan language. Occitan is a language from Southern France, parts of Spain
and Italy that played a major cultural and economic role, particularly in the
Middle Ages. It is now endangered according to UNESCO. The talk critically has
examined how human expertise and artificial intelligence can work together to
offer hope for preserving the linguistic diversity that forms the foundation of
our global and especially our European heritage while addressing some of the
ethical and practical challenges that accompany the use of these powerful
technologies. This paper is based on the keynote I gave at the 46th European
Conference on Information Retrieval (ECIR 2024). As an alternative to reading
this paper, a video talk is available online. 1 Date: 26 March 2024.",2024-09-05,Josiane Mothe,http://arxiv.org/pdf/2409.13702v1,cs.CL
iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models,"Most available data is unstructured, making it challenging to access valuable
information. Automatically building Knowledge Graphs (KGs) is crucial for
structuring data and making it accessible, allowing users to search for
information effectively. KGs also facilitate insights, inference, and
reasoning. Traditional NLP methods, such as named entity recognition and
relation extraction, are key in information retrieval but face limitations,
including the use of predefined entity types and the need for supervised
learning. Current research leverages large language models' capabilities, such
as zero- or few-shot learning. However, unresolved and semantically duplicated
entities and relations still pose challenges, leading to inconsistent graphs
and requiring extensive post-processing. Additionally, most approaches are
topic-dependent. In this paper, we propose iText2KG, a method for incremental,
topic-independent KG construction without post-processing. This plug-and-play,
zero-shot method is applicable across a wide range of KG construction scenarios
and comprises four modules: Document Distiller, Incremental Entity Extractor,
Incremental Relation Extractor, and Graph Integrator and Visualization. Our
method demonstrates superior performance compared to baseline methods across
three scenarios: converting scientific papers to graphs, websites to graphs,
and CVs to graphs.",2024-09-05,"Yassir Lairgi, Ludovic Moncla, Rémy Cazabet, Khalid Benabdeslem, Pierre Cléau",http://arxiv.org/pdf/2409.03284v1,cs.CL
ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding,"Automatic chart understanding is crucial for content comprehension and
document parsing. Multimodal Large Language Models (MLLMs) have demonstrated
remarkable capabilities in chart understanding through domain-specific
alignment and fine-tuning. However, current MLLMs still struggle to provide
faithful data and reliable analysis only based on charts. To address it, we
propose ChartMoE, which employs the Mixture of Expert (MoE) architecture to
replace the traditional linear projector to bridge the modality gap.
Specifically, we train several linear connectors through distinct alignment
tasks, which are utilized as the foundational initialization parameters for
different experts. Additionally, we introduce ChartMoE-Align, a dataset with
nearly 1 million chart-table-JSON-code quadruples to conduct three alignment
tasks (chart-table/JSON/code). Combined with the vanilla connector, we
initialize different experts diversely and adopt high-quality knowledge
learning to further refine the MoE connector and LLM parameters. Extensive
experiments demonstrate the effectiveness of the MoE connector and our
initialization strategy, e.g., ChartMoE improves the accuracy of the previous
state-of-the-art from 80.48\% to 84.64\% on the ChartQA benchmark.",2024-09-05,"Zhengzhuo Xu, Bowen Qu, Yiyan Qi, Sinan Du, Chengjin Xu, Chun Yuan, Jian Guo",http://arxiv.org/pdf/2409.03277v3,cs.CL
Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through Strategy Elicitation,"The Chain-of-Thought (CoT) paradigm has emerged as a critical approach for
enhancing the reasoning capabilities of large language models (LLMs). However,
despite their widespread adoption and success, CoT methods often exhibit
instability due to their inability to consistently ensure the quality of
generated reasoning paths, leading to sub-optimal reasoning performance. To
address this challenge, we propose the \textbf{Strategic Chain-of-Thought}
(SCoT), a novel methodology designed to refine LLM performance by integrating
strategic knowledge prior to generating intermediate reasoning steps. SCoT
employs a two-stage approach within a single prompt: first eliciting an
effective problem-solving strategy, which is then used to guide the generation
of high-quality CoT paths and final answers. Our experiments across eight
challenging reasoning datasets demonstrate significant improvements, including
a 21.05\% increase on the GSM8K dataset and 24.13\% on the Tracking\_Objects
dataset, respectively, using the Llama3-8b model. Additionally, we extend the
SCoT framework to develop a few-shot method with automatically matched
demonstrations, yielding even stronger results. These findings underscore the
efficacy of SCoT, highlighting its potential to substantially enhance LLM
performance in complex reasoning tasks.",2024-09-05,"Yu Wang, Shiwan Zhao, Zhihu Wang, Heyuan Huang, Ming Fan, Yubo Zhang, Zhixing Wang, Haijun Wang, Ting Liu",http://arxiv.org/pdf/2409.03271v1,cs.CL
CA-BERT: Leveraging Context Awareness for Enhanced Multi-Turn Chat Interaction,"Effective communication in automated chat systems hinges on the ability to
understand and respond to context. Traditional models often struggle with
determining when additional context is necessary for generating appropriate
responses. This paper introduces Context-Aware BERT (CA-BERT), a
transformer-based model specifically fine-tuned to address this challenge.
CA-BERT innovatively applies deep learning techniques to discern context
necessity in multi-turn chat interactions, enhancing both the relevance and
accuracy of responses.
  We describe the development of CA-BERT, which adapts the robust architecture
of BERT with a novel training regimen focused on a specialized dataset of chat
dialogues. The model is evaluated on its ability to classify context necessity,
demonstrating superior performance over baseline BERT models in terms of
accuracy and efficiency. Furthermore, CA-BERT's implementation showcases
significant reductions in training time and resource usage, making it feasible
for real-time applications.
  The results indicate that CA-BERT can effectively enhance the functionality
of chatbots by providing a nuanced understanding of context, thereby improving
user experience and interaction quality in automated systems. This study not
only advances the field of NLP in chat applications but also provides a
framework for future research into context-sensitive AI developments.",2024-09-05,"Minghao Liu, Mingxiu Sui, Yi Nan, Cangqing Wang, Zhijie Zhou",http://arxiv.org/pdf/2409.13701v2,cs.CL
GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding,"Although Large Language Models (LLMs) have demonstrated potential in
processing graphs, they struggle with comprehending graphical structure
information through prompts of graph description sequences, especially as the
graph size increases. We attribute this challenge to the uneven memory
performance of LLMs across different positions in graph description sequences,
known as ''positional biases''. To address this, we propose GraphInsight, a
novel framework aimed at improving LLMs' comprehension of both macro- and
micro-level graphical information. GraphInsight is grounded in two key
strategies: 1) placing critical graphical information in positions where LLMs
exhibit stronger memory performance, and 2) investigating a lightweight
external knowledge base for regions with weaker memory performance, inspired by
retrieval-augmented generation (RAG). Moreover, GraphInsight explores
integrating these two strategies into LLM agent processes for composite graph
tasks that require multi-step reasoning. Extensive empirical studies on
benchmarks with a wide range of evaluation tasks show that GraphInsight
significantly outperforms all other graph description methods (e.g., prompting
techniques and reordering strategies) in understanding graph structures of
varying sizes.",2024-09-05,"Yukun Cao, Shuo Han, Zengyi Gao, Zezhong Ding, Xike Xie, S. Kevin Zhou",http://arxiv.org/pdf/2409.03258v3,cs.CL
Understanding LLM Development Through Longitudinal Study: Insights from the Open Ko-LLM Leaderboard,"This paper conducts a longitudinal study over eleven months to address the
limitations of prior research on the Open Ko-LLM Leaderboard, which have relied
on empirical studies with restricted observation periods of only five months.
By extending the analysis duration, we aim to provide a more comprehensive
understanding of the progression in developing Korean large language models
(LLMs). Our study is guided by three primary research questions: (1) What are
the specific challenges in improving LLM performance across diverse tasks on
the Open Ko-LLM Leaderboard over time? (2) How does model size impact task
performance correlations across various benchmarks? (3) How have the patterns
in leaderboard rankings shifted over time on the Open Ko-LLM Leaderboard?. By
analyzing 1,769 models over this period, our research offers a comprehensive
examination of the ongoing advancements in LLMs and the evolving nature of
evaluation frameworks.",2024-09-05,"Chanjun Park, Hyeonwoo Kim",http://arxiv.org/pdf/2409.03257v3,cs.CL
E2CL: Exploration-based Error Correction Learning for Embodied Agents,"Language models are exhibiting increasing capability in knowledge utilization
and reasoning. However, when applied as agents in embodied environments, they
often suffer from misalignment between their intrinsic knowledge and
environmental knowledge, leading to infeasible actions. Traditional environment
alignment methods, such as supervised learning on expert trajectories and
reinforcement learning, encounter limitations in covering environmental
knowledge and achieving efficient convergence, respectively. Inspired by human
learning, we propose Exploration-based Error Correction Learning (E2CL), a
novel framework that leverages exploration-induced errors and environmental
feedback to enhance environment alignment for embodied agents. E2CL
incorporates teacher-guided and teacher-free explorations to gather
environmental feedback and correct erroneous actions. The agent learns to
provide feedback and self-correct, thereby enhancing its adaptability to target
environments. Extensive experiments in the VirtualHome environment demonstrate
that E2CL-trained agents outperform those trained by baseline methods and
exhibit superior self-correction capabilities.",2024-09-05,"Hanlin Wang, Chak Tou Leong, Jian Wang, Wenjie Li",http://arxiv.org/pdf/2409.03256v2,cs.CL
Preserving Empirical Probabilities in BERT for Small-sample Clinical Entity Recognition,"Named Entity Recognition (NER) encounters the challenge of unbalanced labels,
where certain entity types are overrepresented while others are
underrepresented in real-world datasets. This imbalance can lead to biased
models that perform poorly on minority entity classes, impeding accurate and
equitable entity recognition. This paper explores the effects of unbalanced
entity labels of the BERT-based pre-trained model. We analyze the different
mechanisms of loss calculation and loss propagation for the task of token
classification on randomized datasets. Then we propose ways to improve the
token classification for the highly imbalanced task of clinical entity
recognition.",2024-09-05,"Abdul Rehman, Jian Jun Zhang, Xiaosong Yang",http://arxiv.org/pdf/2409.03238v1,cs.CL
Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration,"Black-box large language models (LLMs) are increasingly deployed in various
environments, making it essential for these models to effectively convey their
confidence and uncertainty, especially in high-stakes settings. However, these
models often exhibit overconfidence, leading to potential risks and
misjudgments. Existing techniques for eliciting and calibrating LLM confidence
have primarily focused on general reasoning datasets, yielding only modest
improvements. Accurate calibration is crucial for informed decision-making and
preventing adverse outcomes but remains challenging due to the complexity and
variability of tasks these models perform. In this work, we investigate the
miscalibration behavior of black-box LLMs within the healthcare setting. We
propose a novel method, \textit{Atypical Presentations Recalibration}, which
leverages atypical presentations to adjust the model's confidence estimates.
Our approach significantly improves calibration, reducing calibration errors by
approximately 60\% on three medical question answering datasets and
outperforming existing methods such as vanilla verbalized confidence, CoT
verbalized confidence and others. Additionally, we provide an in-depth analysis
of the role of atypicality within the recalibration framework.",2024-09-05,"Jeremy Qin, Bang Liu, Quoc Dinh Nguyen",http://arxiv.org/pdf/2409.03225v1,cs.CL
xLAM: A Family of Large Action Models to Empower AI Agent Systems,"Autonomous agents powered by large language models (LLMs) have attracted
significant research interest. However, the open-source community faces many
challenges in developing specialized models for agent tasks, driven by the
scarcity of high-quality agent datasets and the absence of standard protocols
in this area. We introduce and publicly release xLAM, a series of large action
models designed for AI agent tasks. The xLAM series includes five models with
both dense and mixture-of-expert architectures, ranging from 1B to 8x22B
parameters, trained using a scalable, flexible pipeline that unifies, augments,
and synthesizes diverse datasets to enhance AI agents' generalizability and
performance across varied environments. Our experimental results demonstrate
that xLAM consistently delivers exceptional performance across multiple agent
ability benchmarks, notably securing the 1st position on the Berkeley
Function-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other
models in terms of tool use. By releasing the xLAM series, we aim to advance
the performance of open-source LLMs for autonomous AI agents, potentially
accelerating progress and democratizing access to high-performance models for
agent tasks. Models are available at
https://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4",2024-09-05,"Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, Zhiwei Liu, Yihao Feng, Tulika Awalgaonkar, Rithesh Murthy, Eric Hu, Zeyuan Chen, Ran Xu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong",http://arxiv.org/pdf/2409.03215v1,cs.CL
An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification,"Sentiment classification (SC) often suffers from low-resource challenges such
as domain-specific contexts, imbalanced label distributions, and few-shot
scenarios. The potential of the diffusion language model (LM) for textual data
augmentation (DA) remains unexplored, moreover, textual DA methods struggle to
balance the diversity and consistency of new samples. Most DA methods either
perform logical modifications or rephrase less important tokens in the original
sequence with the language model. In the context of SC, strong emotional tokens
could act critically on the sentiment of the whole sequence. Therefore,
contrary to rephrasing less important context, we propose DiffusionCLS to
leverage a diffusion LM to capture in-domain knowledge and generate pseudo
samples by reconstructing strong label-related tokens. This approach ensures a
balance between consistency and diversity, avoiding the introduction of noise
and augmenting crucial features of datasets. DiffusionCLS also comprises a
Noise-Resistant Training objective to help the model generalize. Experiments
demonstrate the effectiveness of our method in various low-resource scenarios
including domain-specific and domain-general problems. Ablation studies confirm
the effectiveness of our framework's modules, and visualization studies
highlight optimal deployment conditions, reinforcing our conclusions.",2024-09-05,"Zhuowei Chen, Lianxi Wang, Yuben Wu, Xinfeng Liao, Yujia Tian, Junyang Zhong",http://arxiv.org/pdf/2409.03203v2,cs.CL
Lightweight Transducer Based on Frame-Level Criterion,"The transducer model trained based on sequence-level criterion requires a lot
of memory due to the generation of the large probability matrix. We proposed a
lightweight transducer model based on frame-level criterion, which uses the
results of the CTC forced alignment algorithm to determine the label for each
frame. Then the encoder output can be combined with the decoder output at the
corresponding time, rather than adding each element output by the encoder to
each element output by the decoder as in the transducer. This significantly
reduces memory and computation requirements. To address the problem of
imbalanced classification caused by excessive blanks in the label, we decouple
the blank and non-blank probabilities and truncate the gradient of the blank
classifier to the main network. Experiments on the AISHELL-1 demonstrate that
this enables the lightweight transducer to achieve similar results to
transducer. Additionally, we use richer information to predict the probability
of blank, achieving superior results to transducer.",2024-09-05,"Genshun Wan, Mengzhi Wang, Tingzhi Mao, Hang Chen, Zhongfu Ye",http://arxiv.org/pdf/2409.13698v2,cs.CL
Bypassing DARCY Defense: Indistinguishable Universal Adversarial Triggers,"Neural networks (NN) classification models for Natural Language Processing
(NLP) are vulnerable to the Universal Adversarial Triggers (UAT) attack that
triggers a model to produce a specific prediction for any input. DARCY borrows
the ""honeypot"" concept to bait multiple trapdoors, effectively detecting the
adversarial examples generated by UAT. Unfortunately, we find a new UAT
generation method, called IndisUAT, which produces triggers (i.e., tokens) and
uses them to craft adversarial examples whose feature distribution is
indistinguishable from that of the benign examples in a randomly-chosen
category at the detection layer of DARCY. The produced adversarial examples
incur the maximal loss of predicting results in the DARCY-protected models.
Meanwhile, the produced triggers are effective in black-box models for text
generation, text inference, and reading comprehension. Finally, the evaluation
results under NN models for NLP tasks indicate that the IndisUAT method can
effectively circumvent DARCY and penetrate other defenses. For example,
IndisUAT can reduce the true positive rate of DARCY's detection by at least
40.8% and 90.6%, and drop the accuracy by at least 33.3% and 51.6% in the RNN
and CNN models, respectively. IndisUAT reduces the accuracy of the BERT's
adversarial defense model by at least 34.0%, and makes the GPT-2 language model
spew racist outputs even when conditioned on non-racial context.",2024-09-05,"Zuquan Peng, Yuanyuan He, Jianbing Ni, Ben Niu",http://arxiv.org/pdf/2409.03183v1,cs.CL
MARAGS: A Multi-Adapter System for Multi-Task Retrieval Augmented Generation Question Answering,"In this paper we present a multi-adapter retrieval augmented generation
system (MARAGS) for Meta's Comprehensive RAG (CRAG) competition for KDD CUP
2024. CRAG is a question answering dataset contains 3 different subtasks aimed
at realistic question and answering RAG related tasks, with a diverse set of
question topics, question types, time dynamic answers, and questions featuring
entities of varying popularity.
  Our system follows a standard setup for web based RAG, which uses processed
web pages to provide context for an LLM to produce generations, while also
querying API endpoints for additional information. MARAGS also utilizes
multiple different adapters to solve the various requirements for these tasks
with a standard cross-encoder model for ranking candidate passages relevant for
answering the question. Our system achieved 2nd place for Task 1 as well as 3rd
place on Task 2.",2024-09-05,Mitchell DeHaven,http://arxiv.org/pdf/2409.03171v2,cs.CL
Continual Skill and Task Learning via Dialogue,"Continual and interactive robot learning is a challenging problem as the
robot is present with human users who expect the robot to learn novel skills to
solve novel tasks perpetually with sample efficiency. In this work we present a
framework for robots to query and learn visuo-motor robot skills and task
relevant information via natural language dialog interactions with human users.
Previous approaches either focus on improving the performance of instruction
following agents, or passively learn novel skills or concepts. Instead, we used
dialog combined with a language-skill grounding embedding to query or confirm
skills and/or tasks requested by a user. To achieve this goal, we developed and
integrated three different components for our agent. Firstly, we propose a
novel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA),
which enables the existing SoTA ACT model to perform few-shot continual
learning. Secondly, we develop an alignment model that projects demonstrations
across skill embodiments into a shared embedding allowing us to know when to
ask questions and/or demonstrations from users. Finally, we integrated an
existing LLM to interact with a human user to perform grounded interactive
continual skill learning to solve a task. Our ACT-LoRA model learns novel
fine-tuned skills with a 100% accuracy when trained with only five
demonstrations for a novel skill while still maintaining a 74.75% accuracy on
pre-trained skills in the RLBench dataset where other models fall significantly
short. We also performed a human-subjects study with 8 subjects to demonstrate
the continual learning capabilities of our combined framework. We achieve a
success rate of 75% in the task of sandwich making with the real robot learning
from participant data demonstrating that robots can learn novel skills or task
knowledge from dialogue with non-expert users using our approach.",2024-09-05,"Weiwei Gu, Suresh Kondepudi, Lixiao Huang, Nakul Gopalan",http://arxiv.org/pdf/2409.03166v2,cs.CL
MaterialBENCH: Evaluating College-Level Materials Science Problem-Solving Abilities of Large Language Models,"A college-level benchmark dataset for large language models (LLMs) in the
materials science field, MaterialBENCH, is constructed. This dataset consists
of problem-answer pairs, based on university textbooks. There are two types of
problems: one is the free-response answer type, and the other is the
multiple-choice type. Multiple-choice problems are constructed by adding three
incorrect answers as choices to a correct answer, so that LLMs can choose one
of the four as a response. Most of the problems for free-response answer and
multiple-choice types overlap except for the format of the answers. We also
conduct experiments using the MaterialBENCH on LLMs, including ChatGPT-3.5,
ChatGPT-4, Bard (at the time of the experiments), and GPT-3.5 and GPT-4 with
the OpenAI API. The differences and similarities in the performance of LLMs
measured by the MaterialBENCH are analyzed and discussed. Performance
differences between the free-response type and multiple-choice type in the same
models and the influence of using system massages on multiple-choice problems
are also studied. We anticipate that MaterialBENCH will encourage further
developments of LLMs in reasoning abilities to solve more complicated problems
and eventually contribute to materials research and discovery.",2024-09-05,"Michiko Yoshitake, Yuta Suzuki, Ryo Igarashi, Yoshitaka Ushiku, Keisuke Nagato",http://arxiv.org/pdf/2409.03161v2,cs.CL
Debate on Graph: a Flexible and Reliable Reasoning Framework for Large Language Models,"Large Language Models (LLMs) may suffer from hallucinations in real-world
applications due to the lack of relevant knowledge. In contrast, knowledge
graphs encompass extensive, multi-relational structures that store a vast array
of symbolic facts. Consequently, integrating LLMs with knowledge graphs has
been extensively explored, with Knowledge Graph Question Answering (KGQA)
serving as a critical touchstone for the integration. This task requires LLMs
to answer natural language questions by retrieving relevant triples from
knowledge graphs. However, existing methods face two significant challenges:
\textit{excessively long reasoning paths distracting from the answer
generation}, and \textit{false-positive relations hindering the path
refinement}. In this paper, we propose an iterative interactive KGQA framework
that leverages the interactive learning capabilities of LLMs to perform
reasoning and Debating over Graphs (DoG). Specifically, DoG employs a
subgraph-focusing mechanism, allowing LLMs to perform answer trying after each
reasoning step, thereby mitigating the impact of lengthy reasoning paths. On
the other hand, DoG utilizes a multi-role debate team to gradually simplify
complex questions, reducing the influence of false-positive relations. This
debate mechanism ensures the reliability of the reasoning process. Experimental
results on five public datasets demonstrate the effectiveness and superiority
of our architecture. Notably, DoG outperforms the state-of-the-art method ToG
by 23.7\% and 9.1\% in accuracy on WebQuestions and GrailQA, respectively.
Furthermore, the integration experiments with various LLMs on the mentioned
datasets highlight the flexibility of DoG. Code is available at
\url{https://github.com/reml-group/DoG}.",2024-09-05,"Jie Ma, Zhitao Gao, Qi Chai, Wangchun Sun, Pinghui Wang, Hongbin Pei, Jing Tao, Lingyun Song, Jun Liu, Chen Zhang, Lizhen Cui",http://arxiv.org/pdf/2409.03155v1,cs.CL
GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase Recommendation,"Online sellers and advertisers are recommended keyphrases for their listed
products, which they bid on to enhance their sales. One popular paradigm that
generates such recommendations is Extreme Multi-Label Classification (XMC),
which involves tagging/mapping keyphrases to items. We outline the limitations
of using traditional item-query based tagging or mapping techniques for
keyphrase recommendations on E-Commerce platforms. We introduce GraphEx, an
innovative graph-based approach that recommends keyphrases to sellers using
extraction of token permutations from item titles. Additionally, we demonstrate
that relying on traditional metrics such as precision/recall can be misleading
in practical applications, thereby necessitating a combination of metrics to
evaluate performance in real-world scenarios. These metrics are designed to
assess the relevance of keyphrases to items and the potential for buyer
outreach. GraphEx outperforms production models at eBay, achieving the
objectives mentioned above. It supports near real-time inferencing in
resource-constrained production environments and scales effectively for
billions of items.",2024-09-05,"Ashirbad Mishra, Soumik Dey, Marshall Wu, Jinyu Zhao, He Yu, Kaichen Ni, Binbin Li, Kamesh Madduri",http://arxiv.org/pdf/2409.03140v4,cs.CL
"Well, that escalated quickly: The Single-Turn Crescendo Attack (STCA)","This paper introduces a new method for adversarial attacks on large language
models (LLMs) called the Single-Turn Crescendo Attack (STCA). Building on the
multi-turn crescendo attack method introduced by Russinovich, Salem, and Eldan
(2024), which gradually escalates the context to provoke harmful responses, the
STCA achieves similar outcomes in a single interaction. By condensing the
escalation into a single, well-crafted prompt, the STCA bypasses typical
moderation filters that LLMs use to prevent inappropriate outputs. This
technique reveals vulnerabilities in current LLMs and emphasizes the importance
of stronger safeguards in responsible AI (RAI). The STCA offers a novel method
that has not been previously explored.",2024-09-04,"Alan Aqrawi, Arian Abbasi",http://arxiv.org/pdf/2409.03131v2,cs.CL
Probing self-attention in self-supervised speech models for cross-linguistic differences,"Speech models have gained traction thanks to increase in accuracy from novel
transformer architectures. While this impressive increase in performance across
automatic speech recognition (ASR) benchmarks is noteworthy, there is still
much that is unknown about the use of attention mechanisms for speech-related
tasks. For example, while it is assumed that these models are learning
language-independent (i.e., universal) speech representations, there has not
yet been an in-depth exploration of what it would mean for the models to be
language-independent. In the current paper, we explore this question within the
realm of self-attention mechanisms of one small self-supervised speech
transformer model (TERA). We find that even with a small model, the attention
heads learned are diverse ranging from almost entirely diagonal to almost
entirely global regardless of the training language. We highlight some notable
differences in attention patterns between Turkish and English and demonstrate
that the models do learn important phonological information during pretraining.
We also present a head ablation study which shows that models across languages
primarily rely on diagonal heads to classify phonemes.",2024-09-04,"Sai Gopinath, Joselyn Rodriguez",http://arxiv.org/pdf/2409.03115v1,cs.CL
Quantification of stylistic differences in human- and ASR-produced transcripts of African American English,"Common measures of accuracy used to assess the performance of automatic
speech recognition (ASR) systems, as well as human transcribers, conflate
multiple sources of error. Stylistic differences, such as verbatim vs
non-verbatim, can play a significant role in ASR performance evaluation when
differences exist between training and test datasets. The problem is compounded
for speech from underrepresented varieties, where the speech to orthography
mapping is not as standardized. We categorize the kinds of stylistic
differences between 6 transcription versions, 4 human- and 2 ASR-produced, of
10 hours of African American English (AAE) speech. Focusing on verbatim
features and AAE morphosyntactic features, we investigate the interactions of
these categories with how well transcripts can be compared via word error rate
(WER). The results, and overall analysis, help clarify how ASR outputs are a
function of the decisions made by the training data's human transcribers.",2024-09-04,"Annika Heuser, Tyler Kendall, Miguel del Rio, Quinten McNamara, Nishchal Bhandari, Corey Miller, Migüel Jetté",http://arxiv.org/pdf/2409.03059v1,cs.CL
Oddballness: universal anomaly detection with language models,"We present a new method to detect anomalies in texts (in general: in
sequences of any data), using language models, in a totally unsupervised
manner. The method considers probabilities (likelihoods) generated by a
language model, but instead of focusing on low-likelihood tokens, it considers
a new metric introduced in this paper: oddballness. Oddballness measures how
``strange'' a given token is according to the language model. We demonstrate in
grammatical error detection tasks (a specific case of text anomaly detection)
that oddballness is better than just considering low-likelihood events, if a
totally unsupervised setup is assumed.",2024-09-04,"Filip Graliński, Ryszard Staruch, Krzysztof Jurkiewicz",http://arxiv.org/pdf/2409.03046v1,cs.CL
CLUE: Concept-Level Uncertainty Estimation for Large Language Models,"Large Language Models (LLMs) have demonstrated remarkable proficiency in
various natural language generation (NLG) tasks. Previous studies suggest that
LLMs' generation process involves uncertainty. However, existing approaches to
uncertainty estimation mainly focus on sequence-level uncertainty, overlooking
individual pieces of information within sequences. These methods fall short in
separately assessing the uncertainty of each component in a sequence. In
response, we propose a novel framework for Concept-Level Uncertainty Estimation
(CLUE) for LLMs. We leverage LLMs to convert output sequences into
concept-level representations, breaking down sequences into individual concepts
and measuring the uncertainty of each concept separately. We conduct
experiments to demonstrate that CLUE can provide more interpretable uncertainty
estimation results compared with sentence-level uncertainty, and could be a
useful tool for various tasks such as hallucination detection and story
generation.",2024-09-04,"Yu-Hsiang Wang, Andrew Bai, Che-Ping Tsai, Cho-Jui Hsieh",http://arxiv.org/pdf/2409.03021v1,cs.CL
RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version),"In the rapidly advancing field of robotics, dual-arm coordination and complex
object manipulation are essential capabilities for developing advanced
autonomous systems. However, the scarcity of diverse, high-quality
demonstration data and real-world-aligned evaluation benchmarks severely limits
such development. To address this, we introduce RoboTwin, a generative digital
twin framework that uses 3D generative foundation models and large language
models to produce diverse expert datasets and provide a real-world-aligned
evaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates
varied digital twins of objects from single 2D images, generating realistic and
interactive scenarios. It also introduces a spatial relation-aware code
generation framework that combines object annotations with large language
models to break down tasks, determine spatial constraints, and generate precise
robotic movement code. Our framework offers a comprehensive benchmark with both
simulated and real-world data, enabling standardized evaluation and better
alignment between simulated training and real-world performance. We validated
our approach using the open-source COBOT Magic Robot platform. Policies
pre-trained on RoboTwin-generated data and fine-tuned with limited real-world
samples improve the success rate of over 70% for single-arm tasks and over 40%
for dual-arm tasks compared to models trained solely on real-world data. This
significant improvement demonstrates RoboTwin's potential to enhance the
development and evaluation of dual-arm robotic manipulation systems. Project
Page: https://robotwin-benchmark.github.io/early-version/.",2024-09-04,"Yao Mu, Tianxing Chen, Shijia Peng, Zanxin Chen, Zeyu Gao, Yude Zou, Lunkai Lin, Zhiqiang Xie, Ping Luo",http://arxiv.org/pdf/2409.02920v3,cs.CL
NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls,"The resurgence of autonomous agents built using large language models (LLMs)
to solve complex real-world tasks has brought increased focus on LLMs'
fundamental ability of tool or function calling. At the core of these agents,
an LLM must plan, execute, and respond using external tools, APIs, and custom
functions. Research on tool calling has gathered momentum, but evaluation
benchmarks and datasets representing the complexity of the tasks have lagged
behind. In this work, we focus on one such complexity, nested sequencing, with
the goal of extending existing benchmarks and evaluation. Specifically, we
present NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls,
i.e., sequences where the output of one API call is passed as input to a
subsequent call. NESTFUL contains 1800+ nested sequences where all the function
calls are executable. Experimental results on a variety of models show that the
best-performing model (GPT-4o) achieves a full sequence match accuracy of 28%
and a win-rate of 60%, necessitating a large scope for improvement in the
nested sequencing aspect of function calling. Our analysis of these results
provides possible future research directions for the community, in addition to
a benchmark to track progress. We have released the NESTFUL dataset under the
Apache 2.0 license at https://github.com/IBM/NESTFUL.",2024-09-04,"Kinjal Basu, Ibrahim Abdelaziz, Kiran Kate, Mayank Agarwal, Maxwell Crouse, Yara Rizk, Kelsey Bradford, Asim Munawar, Sadhana Kumaravel, Saurabh Goyal, Xin Wang, Luis A. Lastras, Pavan Kapanipathi",http://arxiv.org/pdf/2409.03797v3,cs.CL
Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling,"Masked diffusion models (MDMs) have emerged as a popular research topic for
generative modeling of discrete data, thanks to their superior performance over
other discrete diffusion models, and are rivaling the auto-regressive models
(ARMs) for language modeling tasks. The recent effort in simplifying the masked
diffusion framework further leads to alignment with continuous-space diffusion
models and more principled training and sampling recipes. In this paper,
however, we reveal that both training and sampling of MDMs are theoretically
free from the time variable, arguably the key signature of diffusion models,
and are instead equivalent to masked models. The connection on the sampling
aspect is drawn by our proposed first-hitting sampler (FHS). Specifically, we
show that the FHS is theoretically equivalent to MDMs' original generation
process while significantly alleviating the time-consuming categorical sampling
and achieving a 20$\times$ speedup. In addition, our investigation raises
doubts about whether MDMs can truly beat ARMs in text generation. We identify,
for the first time, an underlying numerical issue, even with the commonly used
32-bit floating-point precision, which results in inaccurate categorical
sampling. We show that it lowers the effective temperature both theoretically
and empirically, and the resulting decrease in token diversity makes previous
evaluations, which assess the generation quality solely through the incomplete
generative perplexity metric, somewhat unfair.",2024-09-04,"Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang",http://arxiv.org/pdf/2409.02908v6,cs.CL
LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA,"Though current long-context large language models (LLMs) have demonstrated
impressive capacities in answering user questions based on extensive text, the
lack of citations in their responses makes user verification difficult, leading
to concerns about their trustworthiness due to their potential hallucinations.
In this work, we aim to enable long-context LLMs to generate responses with
fine-grained sentence-level citations, improving their faithfulness and
verifiability. We first introduce LongBench-Cite, an automated benchmark for
assessing current LLMs' performance in Long-Context Question Answering with
Citations (LQAC), revealing considerable room for improvement. To this end, we
propose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs
to automatically generate long-context QA instances with precise sentence-level
citations, and leverage this pipeline to construct LongCite-45k, a large-scale
SFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the
LongCite-45k dataset, successfully enabling their generation of accurate
responses and fine-grained sentence-level citations in a single output. The
evaluation results on LongBench-Cite show that our trained models achieve
state-of-the-art citation quality, surpassing advanced proprietary models
including GPT-4o.",2024-09-04,"Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, Juanzi Li",http://arxiv.org/pdf/2409.02897v3,cs.CL
LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture,"Expanding the long-context capabilities of Multi-modal Large Language
Models~(MLLMs) is crucial for video understanding, high-resolution image
understanding, and multi-modal agents. This involves a series of systematic
optimizations, including model architecture, data construction and training
strategy, particularly addressing challenges such as \textit{degraded
performance with more images} and \textit{high computational costs}. In this
paper, we adapt the model architecture to a hybrid of Mamba and Transformer
blocks, approach data construction with both temporal and spatial dependencies
among multiple images and employ a progressive training strategy. The released
model \textbf{LongLLaVA}~(\textbf{Long}-Context \textbf{L}arge
\textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant) is the first
hybrid MLLM, which achieved a better balance between efficiency and
effectiveness. LongLLaVA not only achieves competitive results across various
benchmarks, but also maintains high throughput and low memory consumption.
Especially, it could process nearly a thousand images on a single A100 80GB
GPU, showing promising application prospects for a wide range of tasks.",2024-09-04,"Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, Benyou Wang",http://arxiv.org/pdf/2409.02889v2,cs.CL
Configurable Foundation Models: Building LLMs from a Modular Perspective,"Advancements in LLMs have recently unveiled challenges tied to computational
efficiency and continual scalability due to their requirements of huge
parameters, making the applications and evolution of these models on devices
with limited computation resources and scenarios requiring various abilities
increasingly cumbersome. Inspired by modularity within the human brain, there
is a growing tendency to decompose LLMs into numerous functional modules,
allowing for inference with part of modules and dynamic assembly of modules to
tackle complex tasks, such as mixture-of-experts. To highlight the inherent
efficiency and composability of the modular approach, we coin the term brick to
represent each functional module, designating the modularized structure as
configurable foundation models. In this paper, we offer a comprehensive
overview and investigation of the construction, utilization, and limitation of
configurable foundation models. We first formalize modules into emergent bricks
- functional neuron partitions that emerge during the pre-training phase, and
customized bricks - bricks constructed via additional post-training to improve
the capabilities and knowledge of LLMs. Based on diverse functional bricks, we
further present four brick-oriented operations: retrieval and routing, merging,
updating, and growing. These operations allow for dynamic configuration of LLMs
based on instructions to handle complex tasks. To verify our perspective, we
conduct an empirical analysis on widely-used LLMs. We find that the FFN layers
follow modular patterns with functional specialization of neurons and
functional neuron partitions. Finally, we highlight several open issues and
directions for future research. Overall, this paper aims to offer a fresh
modular perspective on existing LLM research and inspire the future creation of
more efficient and scalable foundational models.",2024-09-04,"Chaojun Xiao, Zhengyan Zhang, Chenyang Song, Dazhi Jiang, Feng Yao, Xu Han, Xiaozhi Wang, Shuo Wang, Yufei Huang, Guanyu Lin, Yingfa Chen, Weilin Zhao, Yuge Tu, Zexuan Zhong, Ao Zhang, Chenglei Si, Khai Hao Moo, Chenyang Zhao, Huimin Chen, Yankai Lin, Zhiyuan Liu, Jingbo Shang, Maosong Sun",http://arxiv.org/pdf/2409.02877v1,cs.CL
Historical German Text Normalization Using Type- and Token-Based Language Modeling,"Historic variations of spelling poses a challenge for full-text search or
natural language processing on historical digitized texts. To minimize the gap
between the historic orthography and contemporary spelling, usually an
automatic orthographic normalization of the historical source material is
pursued. This report proposes a normalization system for German literary texts
from c. 1700-1900, trained on a parallel corpus. The proposed system makes use
of a machine learning approach using Transformer language models, combining an
encoder-decoder model to normalize individual word types, and a pre-trained
causal language model to adjust these normalizations within their context. An
extensive evaluation shows that the proposed system provides state-of-the-art
accuracy, comparable with a much larger fully end-to-end sentence-based
normalization system, fine-tuning a pre-trained Transformer large language
model. However, the normalization of historical text remains a challenge due to
difficulties for models to generalize, and the lack of extensive high-quality
parallel data.",2024-09-04,Anton Ehrmanntraut,http://arxiv.org/pdf/2409.02841v2,cs.CL
R2GQA: Retriever-Reader-Generator Question Answering System to Support Students Understanding Legal Regulations in Higher Education,"In this article, we propose the R2GQA system, a Retriever-Reader-Generator
Question Answering system, consisting of three main components: Document
Retriever, Machine Reader, and Answer Generator. The Retriever module employs
advanced information retrieval techniques to extract the context of articles
from a dataset of legal regulation documents. The Machine Reader module
utilizes state-of-the-art natural language understanding algorithms to
comprehend the retrieved documents and extract answers. Finally, the Generator
module synthesizes the extracted answers into concise and informative responses
to questions of students regarding legal regulations. Furthermore, we built the
ViRHE4QA dataset in the domain of university training regulations, comprising
9,758 question-answer pairs with a rigorous construction process. This is the
first Vietnamese dataset in the higher regulations domain with various types of
answers, both extractive and abstractive. In addition, the R2GQA system is the
first system to offer abstractive answers in Vietnamese. This paper discusses
the design and implementation of each module within the R2GQA system on the
ViRHE4QA dataset, highlighting their functionalities and interactions.
Furthermore, we present experimental results demonstrating the effectiveness
and utility of the proposed system in supporting the comprehension of students
of legal regulations in higher education settings. In general, the R2GQA system
and the ViRHE4QA dataset promise to contribute significantly to related
research and help students navigate complex legal documents and regulations,
empowering them to make informed decisions and adhere to institutional policies
effectively. Our dataset is available for research purposes.",2024-09-04,"Phuc-Tinh Pham Do, Duy-Ngoc Dinh Cao, Khanh Quoc Tran, Kiet Van Nguyen",http://arxiv.org/pdf/2409.02840v1,cs.CL
Exploring Sentiment Dynamics and Predictive Behaviors in Cryptocurrency Discussions by Few-Shot Learning with Large Language Models,"This study performs analysis of Predictive statements, Hope speech, and
Regret Detection behaviors within cryptocurrency-related discussions,
leveraging advanced natural language processing techniques. We introduce a
novel classification scheme named ""Prediction statements,"" categorizing
comments into Predictive Incremental, Predictive Decremental, Predictive
Neutral, or Non-Predictive categories. Employing GPT-4o, a cutting-edge large
language model, we explore sentiment dynamics across five prominent
cryptocurrencies: Cardano, Binance, Matic, Fantom, and Ripple. Our analysis
reveals distinct patterns in predictive sentiments, with Matic demonstrating a
notably higher propensity for optimistic predictions. Additionally, we
investigate hope and regret sentiments, uncovering nuanced interplay between
these emotions and predictive behaviors. Despite encountering limitations
related to data volume and resource availability, our study reports valuable
discoveries concerning investor behavior and sentiment trends within the
cryptocurrency market, informing strategic decision-making and future research
endeavors.",2024-09-04,"Moein Shahiki Tash, Zahra Ahani, Mohim Tash, Olga Kolesnikova, Grigori Sidorov",http://arxiv.org/pdf/2409.02836v1,cs.CL
CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models,"Large language models (LLMs) have obtained promising results in mathematical
reasoning, which is a foundational skill for human intelligence. Most previous
studies focus on improving and measuring the performance of LLMs based on
textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few
researchers have released English multimodal math datasets (e.g., MATHVISTA and
MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In
this paper, we release a Chinese multimodal math (CMM-Math) dataset, including
benchmark and training parts, to evaluate and enhance the mathematical
reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples,
featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank,
and so on) with detailed solutions across 12 grade levels from elementary to
high school in China. Specifically, the visual context may be present in the
questions or opinions, which makes this dataset more challenging. Through
comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math
dataset face challenges, emphasizing the necessity for further improvements in
LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to
handle the problems with mixed input of multiple images and text segments. We
train our model using three stages, including foundational pre-training,
foundational fine-tuning, and mathematical fine-tuning. The extensive
experiments indicate that our model effectively improves math reasoning
performance by comparing it with the SOTA LMMs over three multimodal
mathematical datasets.",2024-09-04,"Wentao Liu, Qianjun Pan, Yi Zhang, Zhuo Liu, Ji Wu, Jie Zhou, Aimin Zhou, Qin Chen, Bo Jiang, Liang He",http://arxiv.org/pdf/2409.02834v3,cs.CL
MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark,"This paper introduces MMMU-Pro, a robust version of the Massive
Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark.
MMMU-Pro rigorously assesses multimodal models' true understanding and
reasoning capabilities through a three-step process based on MMMU: (1)
filtering out questions answerable by text-only models, (2) augmenting
candidate options, and (3) introducing a vision-only input setting where
questions are embedded within images. This setting challenges AI to truly ""see""
and ""read"" simultaneously, testing a fundamental human cognitive skill of
seamlessly integrating visual and textual information. Results show that model
performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8%
to 26.9% across models. We explore the impact of OCR prompts and Chain of
Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT
generally improves performance. MMMU-Pro provides a more rigorous evaluation
tool, closely mimicking real-world scenarios and offering valuable directions
for future research in multimodal AI.",2024-09-04,"Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, Graham Neubig",http://arxiv.org/pdf/2409.02813v3,cs.CL
Towards a Unified View of Preference Learning for Large Language Models: A Survey,"Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of
the crucial factors to achieve success is aligning the LLM's output with human
preferences. This alignment process often requires only a small amount of data
to efficiently enhance the LLM's performance. While effective, research in this
area spans multiple domains, and the methods involved are relatively complex to
understand. The relationships between different methods have been
under-explored, limiting the development of the preference alignment. In light
of this, we break down the existing popular alignment strategies into different
components and provide a unified framework to study the current alignment
strategies, thereby establishing connections among them. In this survey, we
decompose all the strategies in preference learning into four components:
model, data, feedback, and algorithm. This unified view offers an in-depth
understanding of existing alignment algorithms and also opens up possibilities
to synergize the strengths of different strategies. Furthermore, we present
detailed working examples of prevalent existing algorithms to facilitate a
comprehensive understanding for the readers. Finally, based on our unified
perspective, we explore the challenges and future research directions for
aligning large language models with human preferences.",2024-09-04,"Bofei Gao, Feifan Song, Yibo Miao, Zefan Cai, Zhe Yang, Liang Chen, Helan Hu, Runxin Xu, Qingxiu Dong, Ce Zheng, Shanghaoran Quan, Wen Xiao, Ge Zhang, Daoguang Zan, Keming Lu, Bowen Yu, Dayiheng Liu, Zeyu Cui, Jian Yang, Lei Sha, Houfeng Wang, Zhifang Sui, Peiyi Wang, Tianyu Liu, Baobao Chang",http://arxiv.org/pdf/2409.02795v5,cs.CL
A Comparative Study of Pre-training and Self-training,"Pre-training and self-training are two approaches to semi-supervised
learning. The comparison between pre-training and self-training has been
explored. However, the previous works led to confusing findings: self-training
outperforms pre-training experienced on some tasks in computer vision, and
contrarily, pre-training outperforms self-training experienced on some tasks in
natural language processing, under certain conditions of incomparable settings.
We propose, comparatively and exhaustively, an ensemble method to empirical
study all feasible training paradigms combining pre-training, self-training,
and fine-tuning within consistent foundational settings comparable to data
augmentation. We conduct experiments on six datasets, four data augmentation,
and imbalanced data for sentiment analysis and natural language inference
tasks. Our findings confirm that the pre-training and fine-tuning paradigm
yields the best overall performances. Moreover, self-training offers no
additional benefits when combined with semi-supervised pre-training.",2024-09-04,"Yiheng Wang, Jiayu Lin, Zuoquan Lin",http://arxiv.org/pdf/2409.02751v1,cs.CL
Pooling And Attention: What Are Effective Designs For LLM-Based Embedding Models?,"The significant advancements of Large Language Models (LLMs) in generative
tasks have led to a growing body of work exploring LLM-based embedding models.
While these models, employing different pooling and attention strategies, have
achieved state-of-the-art performance on public embedding benchmarks, questions
still arise about what constitutes an effective design for LLM-based embedding
models. However, these models are often trained on different datasets, using
different LLM base models or training settings. Moreover, evaluations on public
embedding benchmarks often fail to report statistical significance, making it
difficult to determine which designs truly contribute to final performance.
This complicates the process for practitioners seeking optimal training recipes
for LLM-based embedding models. In this study, we conduct a large-scale
experiment by training a series of LLM-based embedding models using the same
training data and base model but differing in their pooling and attention
strategies. The results show that there is no one-size-fits-all solution: while
bidirectional attention and an additional trainable pooling layer outperform in
text similarity and information retrieval tasks, they do not significantly
surpass simpler designs like EOS-last token pooling and default causal
attention in clustering and classification tasks. Furthermore, we propose a new
pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs
of all hidden layers, rather than just the last layer, using a cross-attention
network. This method proves to be statistically superior in text similarity and
retrieval tasks compared to existing pooling methods. Overall, this paper sheds
light on effective training strategies for LLM-based embedding models.",2024-09-04,"Yixuan Tang, Yi Yang",http://arxiv.org/pdf/2409.02727v2,cs.CL
Pre-training data selection for biomedical domain adaptation using journal impact metrics,"Domain adaptation is a widely used method in natural language processing
(NLP) to improve the performance of a language model within a specific domain.
This method is particularly common in the biomedical domain, which sees regular
publication of numerous scientific articles. PubMed, a significant corpus of
text, is frequently used in the biomedical domain. The primary objective of
this study is to explore whether refining a pre-training dataset using specific
quality metrics for scientific papers can enhance the performance of the
resulting model. To accomplish this, we employ two straightforward journal
impact metrics and conduct experiments by continually pre-training BERT on
various subsets of the complete PubMed training set, we then evaluate the
resulting models on biomedical language understanding tasks from the BLURB
benchmark. Our results show that pruning using journal impact metrics is not
efficient. But we also show that pre-training using fewer abstracts (but with
the same number of training steps) does not necessarily decrease the resulting
model's performance.",2024-09-04,"Mathieu Laï-king, Patrick Paroubek",http://arxiv.org/pdf/2409.02725v1,cs.CL
Hallucination Detection in LLMs: Fast and Memory-Efficient Fine-Tuned Models,"Uncertainty estimation is a necessary component when implementing AI in
high-risk settings, such as autonomous cars, medicine, or insurances. Large
Language Models (LLMs) have seen a surge in popularity in recent years, but
they are subject to hallucinations, which may cause serious harm in high-risk
settings. Despite their success, LLMs are expensive to train and run: they need
a large amount of computations and memory, preventing the use of ensembling
methods in practice. In this work, we present a novel method that allows for
fast and memory-friendly training of LLM ensembles. We show that the resulting
ensembles can detect hallucinations and are a viable approach in practice as
only one GPU is needed for training and inference.",2024-09-04,"Gabriel Y. Arteaga, Thomas B. Schön, Nicolas Pielawski",http://arxiv.org/pdf/2409.02976v2,cs.CL
"""Yes, My LoRD."" Guiding Language Model Extraction with Locality Reinforced Distillation","Model extraction attacks (MEAs) on large language models (LLMs) have received
increasing attention in recent research. However, existing attack methods
typically adapt the extraction strategies originally developed for deep neural
networks (DNNs). They neglect the underlying inconsistency between the training
tasks of MEA and LLM alignment, leading to suboptimal attack performance. To
tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel
model extraction algorithm specifically designed for LLMs. In particular, LoRD
employs a newly defined policy-gradient-style training task that utilizes the
responses of victim model as the signal to guide the crafting of preference for
the local model. Theoretical analyses demonstrate that I) The convergence
procedure of LoRD in model extraction is consistent with the alignment
procedure of LLMs, and II) LoRD can reduce query complexity while mitigating
watermark protection through our exploration-based stealing. Extensive
experiments validate the superiority of our method in extracting various
state-of-the-art commercial LLMs. Our code is available at:
https://github.com/liangzid/LoRD-MEA .",2024-09-04,"Zi Liang, Qingqing Ye, Yanyun Wang, Sen Zhang, Yaxin Xiao, Ronghua Li, Jianliang Xu, Haibo Hu",http://arxiv.org/pdf/2409.02718v3,cs.CL
A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations,"Machine translation in low-resource language pairs faces significant
challenges due to the scarcity of parallel corpora and linguistic resources.
This study focuses on the case of English-Marathi language pairs, where
existing datasets are notably noisy, impeding the performance of machine
translation models. To mitigate the impact of data quality issues, we propose a
data filtering approach based on cross-lingual sentence representations. Our
methodology leverages a multilingual SBERT model to filter out problematic
translations in the training data. Specifically, we employ an IndicSBERT
similarity model to assess the semantic equivalence between original and
translated sentences, allowing us to retain linguistically correct translations
while discarding instances with substantial deviations. The results demonstrate
a significant improvement in translation quality over the baseline
post-filtering with IndicSBERT. This illustrates how cross-lingual sentence
representations can reduce errors in machine translation scenarios with limited
resources. By integrating multilingual sentence BERT models into the
translation pipeline, this research contributes to advancing machine
translation techniques in low-resource environments. The proposed method not
only addresses the challenges in English-Marathi language pairs but also
provides a valuable framework for enhancing translation quality in other
low-resource language translation tasks.",2024-09-04,"Nidhi Kowtal, Tejas Deshpande, Raviraj Joshi",http://arxiv.org/pdf/2409.02712v1,cs.CL
Detecting Calls to Action in Multimodal Content: Analysis of the 2021 German Federal Election Campaign on Instagram,"This study investigates the automated classification of Calls to Action
(CTAs) within the 2021 German Instagram election campaign to advance the
understanding of mobilization in social media contexts. We analyzed over 2,208
Instagram stories and 712 posts using fine-tuned BERT models and OpenAI's GPT-4
models. The fine-tuned BERT model incorporating synthetic training data
achieved a macro F1 score of 0.93, demonstrating a robust classification
performance. Our analysis revealed that 49.58% of Instagram posts and 10.64% of
stories contained CTAs, highlighting significant differences in mobilization
strategies between these content types. Additionally, we found that FDP and the
Greens had the highest prevalence of CTAs in posts, whereas CDU and CSU led in
story CTAs.",2024-09-04,"Michael Achmann-Denkler, Jakob Fehle, Mario Haim, Christian Wolff",http://arxiv.org/pdf/2409.02690v1,cs.CL
Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs,"Large Language Models (LLMs) have demonstrated remarkable efficiency in
tackling various tasks based on human instructions, but studies reveal that
they often struggle with tasks requiring reasoning, such as math or physics.
This limitation raises questions about whether LLMs truly comprehend embedded
knowledge or merely learn to replicate the token distribution without a true
understanding of the content. In this paper, we delve into this problem and aim
to enhance the reasoning capabilities of LLMs. First, we investigate if the
model has genuine reasoning capabilities by visualizing the text generation
process at the attention and representation level. Then, we formulate the
reasoning process of LLMs into a causal framework, which provides a formal
explanation of the problems observed in the visualization. Finally, building
upon this causal framework, we propose Deconfounded Causal Adaptation (DCA), a
novel parameter-efficient fine-tuning (PEFT) method to enhance the model's
reasoning capabilities by encouraging the model to extract the general
problem-solving skills and apply these skills to different questions.
Experiments show that our method outperforms the baseline consistently across
multiple benchmarks, and with only 1.2M tunable parameters, we achieve better
or comparable results to other fine-tuning methods. This demonstrates the
effectiveness and efficiency of our method in improving the overall accuracy
and reliability of LLMs.",2024-09-04,"Ruoyu Wang, Xiaoxuan Li, Lina Yao",http://arxiv.org/pdf/2409.02686v2,cs.CL
Creating Domain-Specific Translation Memories for Machine Translation Fine-tuning: The TRENCARD Bilingual Cardiology Corpus,"This article investigates how translation memories (TM) can be created by
translators or other language professionals in order to compile domain-specific
parallel corpora , which can then be used in different scenarios, such as
machine translation training and fine-tuning, TM leveraging, and/or large
language model fine-tuning. The article introduces a semi-automatic TM
preparation methodology leveraging primarily translation tools used by
translators in favor of data quality and control by the translators. This
semi-automatic methodology is then used to build a cardiology-based Turkish ->
English corpus from bilingual abstracts of Turkish cardiology journals. The
resulting corpus called TRENCARD Corpus has approximately 800,000 source words
and 50,000 sentences. Using this methodology, translators can build their
custom TMs in a reasonable time and use them in their bilingual data requiring
tasks.",2024-09-04,Gokhan Dogru,http://arxiv.org/pdf/2409.02667v1,cs.CL
OpenFact at CheckThat! 2024: Combining Multiple Attack Methods for Effective Adversarial Text Generation,"This paper presents the experiments and results for the CheckThat! Lab at
CLEF 2024 Task 6: Robustness of Credibility Assessment with Adversarial
Examples (InCrediblAE). The primary objective of this task was to generate
adversarial examples in five problem domains in order to evaluate the
robustness of widely used text classification methods (fine-tuned BERT, BiLSTM,
and RoBERTa) when applied to credibility assessment issues.
  This study explores the application of ensemble learning to enhance
adversarial attacks on natural language processing (NLP) models. We
systematically tested and refined several adversarial attack methods, including
BERT-Attack, Genetic algorithms, TextFooler, and CLARE, on five datasets across
various misinformation tasks. By developing modified versions of BERT-Attack
and hybrid methods, we achieved significant improvements in attack
effectiveness. Our results demonstrate the potential of modification and
combining multiple methods to create more sophisticated and effective
adversarial attack strategies, contributing to the development of more robust
and secure systems.",2024-09-04,"Włodzimierz Lewoniewski, Piotr Stolarski, Milena Stróżyna, Elzbieta Lewańska, Aleksandra Wojewoda, Ewelina Księżniak, Marcin Sawiński",http://arxiv.org/pdf/2409.02649v2,cs.CL
Emergent Language: A Survey and Taxonomy,"The field of emergent language represents a novel area of research within the
domain of artificial intelligence, particularly within the context of
multi-agent reinforcement learning. Although the concept of studying language
emergence is not new, early approaches were primarily concerned with explaining
human language formation, with little consideration given to its potential
utility for artificial agents. In contrast, studies based on reinforcement
learning aim to develop communicative capabilities in agents that are
comparable to or even superior to human language. Thus, they extend beyond the
learned statistical representations that are common in natural language
processing research. This gives rise to a number of fundamental questions, from
the prerequisites for language emergence to the criteria for measuring its
success. This paper addresses these questions by providing a comprehensive
review of 181 scientific publications on emergent language in artificial
intelligence. Its objective is to serve as a reference for researchers
interested in or proficient in the field. Consequently, the main contributions
are the definition and overview of the prevailing terminology, the analysis of
existing evaluation methods and metrics, and the description of the identified
research gaps.",2024-09-04,"Jannik Peters, Constantin Waubert de Puiseau, Hasan Tercan, Arya Gopikrishnan, Gustavo Adolpho Lucas De Carvalho, Christian Bitter, Tobias Meisen",http://arxiv.org/pdf/2409.02645v2,cs.CL
PUB: Plot Understanding Benchmark and Dataset for Evaluating Large Language Models on Synthetic Visual Data Interpretation,"The ability of large language models (LLMs) to interpret visual
representations of data is crucial for advancing their application in data
analysis and decision-making processes. This paper presents a novel synthetic
dataset designed to evaluate the proficiency of LLMs in interpreting various
forms of data visualizations, including plots like time series, histograms,
violins, boxplots, and clusters. Our dataset is generated using controlled
parameters to ensure comprehensive coverage of potential real-world scenarios.
We employ multimodal text prompts with questions related to visual data in
images to benchmark several state-of-the-art models like ChatGPT or Gemini,
assessing their understanding and interpretative accuracy.
  To ensure data integrity, our benchmark dataset is generated automatically,
making it entirely new and free from prior exposure to the models being tested.
This strategy allows us to evaluate the models' ability to truly interpret and
understand the data, eliminating possibility of pre-learned responses, and
allowing for an unbiased evaluation of the models' capabilities. We also
introduce quantitative metrics to assess the performance of the models,
providing a robust and comprehensive evaluation tool.
  Benchmarking several state-of-the-art LLMs with this dataset reveals varying
degrees of success, highlighting specific strengths and weaknesses in
interpreting diverse types of visual data. The results provide valuable
insights into the current capabilities of LLMs and identify key areas for
improvement. This work establishes a foundational benchmark for future research
and development aimed at enhancing the visual interpretative abilities of
language models. In the future, improved LLMs with robust visual interpretation
skills can significantly aid in automated data analysis, scientific research,
educational tools, and business intelligence applications.",2024-09-04,"Aneta Pawelec, Victoria Sara Wesołowska, Zuzanna Bączek, Piotr Sankowski",http://arxiv.org/pdf/2409.02617v1,cs.CL
An Analysis of Linear Complexity Attention Substitutes with BEST-RQ,"Self-Supervised Learning (SSL) has proven to be effective in various domains,
including speech processing. However, SSL is computationally and memory
expensive. This is in part due the quadratic complexity of multi-head
self-attention (MHSA). Alternatives for MHSA have been proposed and used in the
speech domain, but have yet to be investigated properly in an SSL setting. In
this work, we study the effects of replacing MHSA with recent state-of-the-art
alternatives that have linear complexity, namely, HyperMixing, Fastformer,
SummaryMixing, and Mamba. We evaluate these methods by looking at the speed,
the amount of VRAM consumed, and the performance on the SSL MP3S benchmark.
Results show that these linear alternatives maintain competitive performance
compared to MHSA while, on average, decreasing VRAM consumption by around 20%
to 60% and increasing speed from 7% to 65% for input sequences ranging from 20
to 80 seconds.",2024-09-04,"Ryan Whetten, Titouan Parcollet, Adel Moumen, Marco Dinarelli, Yannick Estève",http://arxiv.org/pdf/2409.02596v1,cs.CL
Sorbet: A Neuromorphic Hardware-Compatible Transformer-Based Spiking Language Model,"For reasons such as privacy, there are use cases for language models at the
edge. This has given rise to small language models (SLMs) targeted for
deployment in resource-constrained devices where energy efficiency is a
significant concern. Spiking neural networks (SNNs) offer a promising solution
due to their energy efficiency, and there are already works on realizing
transformer-based models on SNNs. However, key operations like softmax and
layer normalization (LN) are difficult to implement on neuromorphic hardware,
and many of these early works sidestepped them. To address these challenges, we
introduce Sorbet, a transformer-based spiking language model that is more
neuromorphic hardware-compatible. Sorbet incorporates a novel shifting-based
softmax called PTsoftmax and a power normalization method using bit-shifting
(BSPN), both designed to replace the respective energy-intensive operations. By
leveraging knowledge distillation and model quantization, Sorbet achieved a
highly compressed binary weight model that maintains competitive performance
while significantly reducing energy consumption. We validate Sorbet's
effectiveness through extensive testing on the GLUE benchmark and a series of
ablation studies, demonstrating its potential as an energy-efficient solution
for language model inference.",2024-09-04,"Kaiwen Tang, Zhanglu Yan, Weng-Fai Wong",http://arxiv.org/pdf/2409.15298v1,cs.CL
More is More: Addition Bias in Large Language Models,"In this paper, we investigate the presence of additive bias in Large Language
Models (LLMs), drawing a parallel to the cognitive bias observed in humans
where individuals tend to favor additive over subtractive changes. Using a
series of controlled experiments, we tested various LLMs, including GPT-3.5
Turbo, Claude 3.5 Sonnet, Mistral, Math$\Sigma$tral, and Llama 3.1, on tasks
designed to measure their propensity for additive versus subtractive
modifications. Our findings demonstrate a significant preference for additive
changes across all tested models. For example, in a palindrome creation task,
Llama 3.1 favored adding letters 97.85% of the time over removing them.
Similarly, in a Lego tower balancing task, GPT-3.5 Turbo chose to add a brick
76.38% of the time rather than remove one. In a text summarization task,
Mistral 7B produced longer summaries in 59.40% to 75.10% of cases when asked to
improve its own or others' writing. These results indicate that, similar to
humans, LLMs exhibit a marked additive bias, which might have implications when
LLMs are used on a large scale. Addittive bias might increase resource use and
environmental impact, leading to higher economic costs due to overconsumption
and waste. This bias should be considered in the development and application of
LLMs to ensure balanced and efficient problem-solving approaches.",2024-09-04,"Luca Santagata, Cristiano De Nobili",http://arxiv.org/pdf/2409.02569v1,cs.CL
Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic Reasoning with Argumentation Theory-Driven Prompts,"We propose misogyny detection as an Argumentative Reasoning task and we
investigate the capacity of large language models (LLMs) to understand the
implicit reasoning used to convey misogyny in both Italian and English. The
central aim is to generate the missing reasoning link between a message and the
implied meanings encoding the misogyny. Our study uses argumentation theory as
a foundation to form a collection of prompts in both zero-shot and few-shot
settings. These prompts integrate different techniques, including
chain-of-thought reasoning and augmented knowledge. Our findings show that LLMs
fall short on reasoning capabilities about misogynistic comments and that they
mostly rely on their implicit knowledge derived from internalized common
stereotypes about women to generate implied assumptions, rather than on
inductive reasoning.",2024-09-04,"Arianna Muti, Federico Ruggeri, Khalid Al-Khatib, Alberto Barrón-Cedeño, Tommaso Caselli",http://arxiv.org/pdf/2409.02519v1,cs.CL
Word and Phrase Features in Graph Convolutional Network for Automatic Question Classification,"Effective question classification is crucial for AI-driven educational tools,
enabling adaptive learning systems to categorize questions by skill area,
difficulty level, and competence. This classification not only supports
educational diagnostics and analytics but also enhances complex tasks like
information retrieval and question answering by associating questions with
relevant categories. Traditional methods, often based on word embeddings and
conventional classifiers, struggle to capture the nuanced relationships in
natural language, leading to suboptimal performance. To address this, we
propose a novel approach leveraging graph convolutional networks, named Phrase
Question-Graph Convolutional Network (PQ-GCN) to better model the inherent
structure of questions. By representing questions as graphs-where nodes signify
words or phrases and edges denote syntactic or semantic relationships-our
method allows the model to learn from the interconnected nature of language
more effectively. Additionally, we explore the incorporation of phrase-based
features to enhance classification performance on question datasets of various
domains and characteristics. Our findings demonstrate that the proposed model,
augmented with these features, offer a promising solution for more robust and
context-aware question classification, bridging the gap between graph neural
network research and practical educational applications of AI.",2024-09-04,"Junyoung Lee, Ninad Dixit, Kaustav Chakrabarti, S. Supraja",http://arxiv.org/pdf/2409.02481v2,cs.CL
A Comparative Study on Large Language Models for Log Parsing,"Background: Log messages provide valuable information about the status of
software systems. This information is provided in an unstructured fashion and
automated approaches are applied to extract relevant parameters. To ease this
process, log parsing can be applied, which transforms log messages into
structured log templates. Recent advances in language models have led to
several studies that apply ChatGPT to the task of log parsing with promising
results. However, the performance of other state-of-the-art large language
models (LLMs) on the log parsing task remains unclear.
  Aims: In this study, we investigate the current capability of
state-of-the-art LLMs to perform log parsing.
  Method: We select six recent LLMs, including both paid proprietary (GPT-3.5,
Claude 2.1) and four free-to-use open models, and compare their performance on
system logs obtained from a selection of mature open-source projects. We design
two different prompting approaches and apply the LLMs on 1, 354 log templates
across 16 different projects. We evaluate their effectiveness, in the number of
correctly identified templates, and the syntactic similarity between the
generated templates and the ground truth.
  Results: We found that free-to-use models are able to compete with paid
models, with CodeLlama extracting 10% more log templates correctly than
GPT-3.5. Moreover, we provide qualitative insights into the usability of
language models (e.g., how easy it is to use their responses).
  Conclusions: Our results reveal that some of the smaller, free-to-use LLMs
can considerably assist log parsing compared to their paid proprietary
competitors, especially code-specialized models.",2024-09-04,"Merve Astekin, Max Hort, Leon Moonen",http://arxiv.org/pdf/2409.02474v1,cs.CL
DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels,"Recently, significant efforts have been devoted to enhancing the long-context
capabilities of Large Language Models (LLMs), particularly in long-context
reasoning. To facilitate this research, we propose \textbf{DetectiveQA}, a
dataset specifically designed for narrative reasoning within long contexts. We
leverage detective novels, averaging over 100k tokens, to create a dataset
containing 1200 human-annotated questions in both Chinese and English, each
paired with corresponding reference reasoning steps. Furthermore, we introduce
a step-wise reasoning metric, which enhances the evaluation of LLMs' reasoning
processes. We validate our approach and evaluate the mainstream LLMs, including
GPT-4, Claude, and LLaMA, revealing persistent long-context reasoning
challenges and demonstrating their evidence-retrieval challenges. Our findings
offer valuable insights into the study of long-context reasoning and lay the
base for more rigorous evaluations.",2024-09-04,"Zhe Xu, Jiasheng Ye, Xiaoran Liu, Xiangyang Liu, Tianxiang Sun, Zhigeng Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, Xipeng Qiu",http://arxiv.org/pdf/2409.02465v2,cs.CL
ISO: Overlap of Computation and Communication within Seqenence For LLM Inference,"In the realm of Large Language Model (LLM) inference, the inherent structure
of transformer models coupled with the multi-GPU tensor parallelism strategy
leads to a sequential execution of computation and communication. This results
in substantial underutilization of computing resources during the communication
phase. To mitigate this inefficiency, various techniques have been developed to
optimize the use of computational power throughout the communication process.
These strategies primarily involve overlapping matrix computations and
communications, as well as interleaving micro-batches across different
requests. Nonetheless, these approaches either fall short of achieving ideal
overlap or impose certain limitations on their application. To overcome these
challenges, this paper introduces a novel strategy for
computation-communication overlap that operates at the sequence level. This
method not only enhances the degree of overlap but also minimizes the
constraints on its applicability. Experimental evaluations conducted using
30b/70b models have demonstrated significant improvements in efficiency.
Specifically, the proposed technique has been shown to reduce time consumption
by approximately 35% on 4090 GPU and by roughly 15% on A800 GPU during the
prefill stage of LLM inference.",2024-09-04,"Bin Xiao, Lei Su",http://arxiv.org/pdf/2409.11155v1,cs.CL
What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations,"This paper explores the pitfalls in evaluating multilingual automatic speech
recognition (ASR) models, with a particular focus on Indic language scripts. We
investigate the text normalization routine employed by leading ASR models,
including OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,
and their unintended consequences on performance metrics. Our research reveals
that current text normalization practices, while aiming to standardize ASR
outputs for fair comparison, by removing inconsistencies such as variations in
spelling, punctuation, and special characters, are fundamentally flawed when
applied to Indic scripts. Through empirical analysis using text similarity
scores and in-depth linguistic examination, we demonstrate that these flaws
lead to artificially improved performance metrics for Indic languages. We
conclude by proposing a shift towards developing text normalization routines
that leverage native linguistic expertise, ensuring more robust and accurate
evaluations of multilingual ASR models.",2024-09-04,"Kavya Manohar, Leena G Pillai, Elizabeth Sherly",http://arxiv.org/pdf/2409.02449v4,cs.CL
Large Language Models as Efficient Reward Function Searchers for Custom-Environment Multi-Objective Reinforcement Learning,"Achieving the effective design and improvement of reward functions in
reinforcement learning (RL) tasks with complex custom environments and multiple
requirements presents considerable challenges. In this paper, we propose ERFSL,
an efficient reward function searcher using LLMs, which enables LLMs to be
effective white-box searchers and highlights their advanced semantic
understanding capabilities. Specifically, we generate reward components for
each numerically explicit user requirement and employ a reward critic to
identify the correct code form. Then, LLMs assign weights to the reward
components to balance their values and iteratively adjust the weights without
ambiguity and redundant adjustments by flexibly adopting directional mutation
and crossover strategies, similar to genetic algorithms, based on the context
provided by the training log analyzer. We applied the framework to an
underwater data collection RL task without direct human feedback or reward
examples (zero-shot learning). The reward critic successfully corrects the
reward code with only one feedback instance for each requirement, effectively
preventing unrectifiable errors. The initialization of weights enables the
acquisition of different reward functions within the Pareto solution set
without the need for weight search. Even in cases where a weight is 500 times
off, on average, only 5.2 iterations are needed to meet user requirements. The
ERFSL also works well with most prompts utilizing GPT-4o mini, as we decompose
the weight searching process to reduce the requirement for numerical and
long-context understanding capabilities",2024-09-04,"Guanwen Xie, Jingzehua Xu, Yiyuan Yang, Yimian Ding, Shuai Zhang",http://arxiv.org/pdf/2409.02428v3,cs.CL
Prompt Baking,"Two primary ways to change LLM behavior are prompting and weight updates
(e.g., fine-tuning). Prompting LLMs is simple and effective, specifying the
desired changes explicitly in natural language, whereas weight updates provide
more expressive and permanent behavior changes, specified implicitly via
training on large datasets. We present a technique for ""baking"" prompts into
the weights of an LLM. Prompt Baking converts a prompt $u$ and initial weights
$\theta$ to a new set of weights $\theta_u$ such that new ""baked"" LLM behaves
like the original prompted LLM. Mathematically, we minimize the KL divergence
between $P_\theta(\cdot | u)$ and $P_{\theta_u}(\cdot)$, where $P$ is the LLM's
probability distribution over token sequences. Across all our experiments, we
find prompts can be readily baked into weight updates. Baking chain-of-thought
prompts improves zero-shot performance on GSM8K, ASDiv, MBPP, ARC-Easy,
ARC-Challenge, and CommonsenseQA benchmarks. Baking news headlines directly
updates an LLM's knowledge. And baking instructions & personas alleviates
""prompt forgetting"" over long sequences. Furthermore, stopping baking early
creates ""half-baked"" models, continuously scaling prompt strength. Baked models
retain their sensitivity to further prompting and baking, including
re-prompting with the baked-in prompt. Surprisingly, the re-prompted models
yield further performance gains in instruction following, as well as math
reasoning and coding benchmarks. Taking re-prompting and re-baking to the limit
yields a form of iterative self-improvement we call Prompt Pursuit, and
preliminary results on instruction following exhibit dramatic performance
gains. Finally, we discuss implications for AI safety, continuous model
updating, enhancing real-time learning capabilities in LLM-based agents, and
generating more stable AI personas.",2024-09-04,"Aman Bhargava, Cameron Witkowski, Alexander Detkov, Matt Thomson",http://arxiv.org/pdf/2409.13697v1,cs.CL
"Abstractive Text Summarization: State of the Art, Challenges, and Improvements","Specifically focusing on the landscape of abstractive text summarization, as
opposed to extractive techniques, this survey presents a comprehensive
overview, delving into state-of-the-art techniques, prevailing challenges, and
prospective research directions. We categorize the techniques into traditional
sequence-to-sequence models, pre-trained large language models, reinforcement
learning, hierarchical methods, and multi-modal summarization. Unlike prior
works that did not examine complexities, scalability and comparisons of
techniques in detail, this review takes a comprehensive approach encompassing
state-of-the-art methods, challenges, solutions, comparisons, limitations and
charts out future improvements - providing researchers an extensive overview to
advance abstractive summarization research. We provide vital comparison tables
across techniques categorized - offering insights into model complexity,
scalability and appropriate applications. The paper highlights challenges such
as inadequate meaning representation, factual consistency, controllable text
summarization, cross-lingual summarization, and evaluation metrics, among
others. Solutions leveraging knowledge incorporation and other innovative
strategies are proposed to address these challenges. The paper concludes by
highlighting emerging research areas like factual inconsistency,
domain-specific, cross-lingual, multilingual, and long-document summarization,
as well as handling noisy data. Our objective is to provide researchers and
practitioners with a structured overview of the domain, enabling them to better
understand the current landscape and identify potential areas for further
research and improvement.",2024-09-04,"Hassan Shakil, Ahmad Farooq, Jugal Kalita",http://arxiv.org/pdf/2409.02413v1,cs.CL
Wavelet GPT: Wavelet Inspired Large Language Models,"Large Language Models (LLMs) have ushered in a new wave of artificial
intelligence advancements impacting every scientific field and discipline. We
live in a world where most of the data around us, e.g., text, audio, and music,
has a multi-scale structure. This paper infuses LLMs with a traditional signal
processing idea, namely wavelets, during pre-training to take advantage of the
structure. Without adding \textbf{any extra parameters} to a GPT-style LLM
architecture in an academic setup, we achieve the same pre-training performance
almost twice as fast in text, audio, and images. This is done by imposing a
structure on intermediate embeddings. When trained for the same number of
training steps, we achieve significant gains in performance, which is
comparable to pre-training a larger neural architecture. Further, we show this
extends to the Long Range Arena benchmark and several input representations
such as characters, BPE tokens, bytes, waveform, math expression, and image
pixels. Our architecture allows every next token prediction access to
intermediate embeddings at different temporal resolutions in every decoder
block. We hope this will pave the way for incorporating multi-rate signal
processing into pre-training.",2024-09-04,Prateek Verma,http://arxiv.org/pdf/2409.12924v4,cs.CL
Determination of language families using deep learning,"We use a c-GAN (convolutional generative adversarial) neural network to
analyze transliterated text fragments of extant, dead comprehensible, and one
dead non-deciphered (Cypro-Minoan) language to establish linguistic affinities.
The paper is agnostic with respect to translation and/or deciphering. However,
there is hope that the proposed approach can be useful for decipherment with
more sophisticated neural network techniques.",2024-09-04,Peter B. Lerner,http://arxiv.org/pdf/2409.02393v2,cs.CL
"Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges","This comprehensive review explores the intersection of Large Language Models
(LLMs) and cognitive science, examining similarities and differences between
LLMs and human cognitive processes. We analyze methods for evaluating LLMs
cognitive abilities and discuss their potential as cognitive models. The review
covers applications of LLMs in various cognitive fields, highlighting insights
gained for cognitive science research. We assess cognitive biases and
limitations of LLMs, along with proposed methods for improving their
performance. The integration of LLMs with cognitive architectures is examined,
revealing promising avenues for enhancing artificial intelligence (AI)
capabilities. Key challenges and future research directions are identified,
emphasizing the need for continued refinement of LLMs to better align with
human cognition. This review provides a balanced perspective on the current
state and future potential of LLMs in advancing our understanding of both
artificial and human intelligence.",2024-09-04,"Qian Niu, Junyu Liu, Ziqian Bi, Pohsun Feng, Benji Peng, Keyu Chen, Ming Li, Lawrence KQ Yan, Yichao Zhang, Caitlyn Heqi Yin, Cheng Fei, Tianyang Wang, Yunze Wang, Silin Chen, Ming Liu",http://arxiv.org/pdf/2409.02387v6,cs.CL
STAB: Speech Tokenizer Assessment Benchmark,"Representing speech as discrete tokens provides a framework for transforming
speech into a format that closely resembles text, thus enabling the use of
speech as an input to the widely successful large language models (LLMs).
Currently, while several speech tokenizers have been proposed, there is
ambiguity regarding the properties that are desired from a tokenizer for
specific downstream tasks and its overall generalizability. Evaluating the
performance of tokenizers across different downstream tasks is a
computationally intensive effort that poses challenges for scalability. To
circumvent this requirement, we present STAB (Speech Tokenizer Assessment
Benchmark), a systematic evaluation framework designed to assess speech
tokenizers comprehensively and shed light on their inherent characteristics.
This framework provides a deeper understanding of the underlying mechanisms of
speech tokenization, thereby offering a valuable resource for expediting the
advancement of future tokenizer models and enabling comparative analysis using
a standardized benchmark. We evaluate the STAB metrics and correlate this with
downstream task performance across a range of speech tasks and tokenizer
choices.",2024-09-04,"Shikhar Vashishth, Harman Singh, Shikhar Bharadwaj, Sriram Ganapathy, Chulayuth Asawaroengchai, Kartik Audhkhasi, Andrew Rosenberg, Ankur Bapna, Bhuvana Ramabhadran",http://arxiv.org/pdf/2409.02384v1,cs.CL
How Privacy-Savvy Are Large Language Models? A Case Study on Compliance and Privacy Technical Review,"The recent advances in large language models (LLMs) have significantly
expanded their applications across various fields such as language generation,
summarization, and complex question answering. However, their application to
privacy compliance and technical privacy reviews remains under-explored,
raising critical concerns about their ability to adhere to global privacy
standards and protect sensitive user data. This paper seeks to address this gap
by providing a comprehensive case study evaluating LLMs' performance in
privacy-related tasks such as privacy information extraction (PIE), legal and
regulatory key point detection (KPD), and question answering (QA) with respect
to privacy policies and data protection regulations. We introduce a Privacy
Technical Review (PTR) framework, highlighting its role in mitigating privacy
risks during the software development life-cycle. Through an empirical
assessment, we investigate the capacity of several prominent LLMs, including
BERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks
and technical privacy reviews. Our experiments benchmark the models across
multiple dimensions, focusing on their precision, recall, and F1-scores in
extracting privacy-sensitive information and detecting key regulatory
compliance points. While LLMs show promise in automating privacy reviews and
identifying regulatory discrepancies, significant gaps persist in their ability
to fully comply with evolving legal standards. We provide actionable
recommendations for enhancing LLMs' capabilities in privacy compliance,
emphasizing the need for robust model improvements and better integration with
legal and regulatory requirements. This study underscores the growing
importance of developing privacy-aware LLMs that can both support businesses in
compliance efforts and safeguard user privacy rights.",2024-09-04,"Yang Liu, Xichou Zhu, Zhou Shen, Yi Liu, Min Li, Yujun Chen, Benzi John, Zhenzhen Ma, Tao Hu, Zhi Li, Bolong Yang, Manman Wang, Zongxing Xie, Peng Liu, Dan Cai, Junhui Wang",http://arxiv.org/pdf/2409.02375v4,cs.CL
Do Large Language Models Possess Sensitive to Sentiment?,"Large Language Models (LLMs) have recently displayed their extraordinary
capabilities in language understanding. However, how to comprehensively assess
the sentiment capabilities of LLMs continues to be a challenge. This paper
investigates the ability of LLMs to detect and react to sentiment in text
modal. As the integration of LLMs into diverse applications is on the rise, it
becomes highly critical to comprehend their sensitivity to emotional tone, as
it can influence the user experience and the efficacy of sentiment-driven
tasks. We conduct a series of experiments to evaluate the performance of
several prominent LLMs in identifying and responding appropriately to
sentiments like positive, negative, and neutral emotions. The models' outputs
are analyzed across various sentiment benchmarks, and their responses are
compared with human evaluations. Our discoveries indicate that although LLMs
show a basic sensitivity to sentiment, there are substantial variations in
their accuracy and consistency, emphasizing the requirement for further
enhancements in their training processes to better capture subtle emotional
cues. Take an example in our findings, in some cases, the models might wrongly
classify a strongly positive sentiment as neutral, or fail to recognize sarcasm
or irony in the text. Such misclassifications highlight the complexity of
sentiment analysis and the areas where the models need to be refined. Another
aspect is that different LLMs might perform differently on the same set of
data, depending on their architecture and training datasets. This variance
calls for a more in-depth study of the factors that contribute to the
performance differences and how they can be optimized.",2024-09-04,"Yang Liu, Xichou Zhu, Zhou Shen, Yi Liu, Min Li, Yujun Chen, Benzi John, Zhenzhen Ma, Tao Hu, Zhi Li, Zhiyang Xu, Wei Luo, Junhui Wang",http://arxiv.org/pdf/2409.02370v4,cs.CL
Diversify-verify-adapt: Efficient and Robust Retrieval-Augmented Ambiguous Question Answering,"The retrieval augmented generation (RAG) framework addresses an ambiguity in
user queries in QA systems by retrieving passages that cover all plausible
interpretations and generating comprehensive responses based on the passages.
However, our preliminary studies reveal that a single retrieval process often
suffers from low quality results, as the retrieved passages frequently fail to
capture all plausible interpretations. Although the iterative RAG approach has
been proposed to address this problem, it comes at the cost of significantly
reduced efficiency. To address these issues, we propose the
diversify-verify-adapt (DIVA) framework. DIVA first diversifies the retrieved
passages to encompass diverse interpretations. Subsequently, DIVA verifies the
quality of the passages and adapts the most suitable approach tailored to their
quality. This approach improves the QA systems accuracy and robustness by
handling low quality retrieval issue in ambiguous questions, while enhancing
efficiency.",2024-09-04,"Yeonjun In, Sungchul Kim, Ryan A. Rossi, Md Mehrab Tanjim, Tong Yu, Ritwik Sinha, Chanyoung Park",http://arxiv.org/pdf/2409.02361v2,cs.CL
NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for Retrieval,"$k$-Nearest Neighbor search on dense vector embeddings ($k$-NN retrieval)
from pre-trained embedding models is the predominant retrieval method for text
and images, as well as Retrieval-Augmented Generation (RAG) pipelines. In
practice, application developers often fine-tune the embeddings to improve
their accuracy on the dataset and query workload in hand. Existing approaches
either fine-tune the pre-trained model itself or, more efficiently, but at the
cost of accuracy, train adaptor models to transform the output of the
pre-trained model. We present NUDGE, a family of novel non-parametric embedding
fine-tuning approaches that are significantly more accurate and efficient than
both sets of existing approaches. NUDGE directly modifies the embeddings of
data records to maximize the accuracy of $k$-NN retrieval. We present a
thorough theoretical and experimental study of NUDGE's non-parametric approach.
We show that even though the underlying problem is NP-Hard, constrained
variations can be solved efficiently. These constraints additionally ensure
that the changes to the embeddings are modest, avoiding large distortions to
the semantics learned during pre-training. In experiments across five
pre-trained models and nine standard text and image retrieval datasets, NUDGE
runs in minutes and often improves NDCG@10 by more than 10% over existing
fine-tuning methods. On average, NUDGE provides 3.3x and 4.3x higher increase
in accuracy and runs 200x and 3x faster, respectively, over fine-tuning the
pre-trained model and training adaptors.",2024-09-04,"Sepanta Zeighami, Zac Wellmer, Aditya Parameswaran",http://arxiv.org/pdf/2409.02343v1,cs.CL
Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining,"Recent studies have been increasingly demonstrating that high-quality data is
crucial for effective pretraining of language models. However, the precise
definition of ""high-quality"" remains underexplored. Focusing on the code
domain, we introduce Arctic-SnowCoder-1.3B, a data-efficient base code model
pretrained on 555B tokens through three phases of progressively refined data:
(1) general pretraining with 500B standard-quality code tokens, preprocessed
through basic filtering, deduplication, and decontamination, (2) continued
pretraining with 50B high-quality tokens, selected from phase one by a
BERT-style quality annotator trained to distinguish good code from random data,
using positive examples drawn from high-quality code files, along with
instruction data from Magicoder and StarCoder2-Instruct, and (3) enhanced
pretraining with 5B synthetic data created by Llama-3.1-70B using phase two
data as seeds, adapting the Magicoder approach for pretraining. Despite being
trained on a limited dataset, Arctic-SnowCoder achieves state-of-the-art
performance on BigCodeBench, a coding benchmark focusing on practical and
challenging programming tasks, compared to similarly sized models trained on no
more than 1T tokens, outperforming Phi-1.5-1.3B by 36%. Across all evaluated
benchmarks, Arctic-SnowCoder-1.3B beats StarCoderBase-3B pretrained on 1T
tokens. Additionally, it matches the performance of leading small base code
models trained on trillions of tokens. For example, Arctic-SnowCoder-1.3B
surpasses StarCoder2-3B, pretrained on over 3.3T tokens, on HumanEval+, a
benchmark that evaluates function-level code generation, and remains
competitive on BigCodeBench. Our evaluation presents a comprehensive analysis
justifying various design choices for Arctic-SnowCoder. Most importantly, we
find that the key to high-quality data is its alignment with the distribution
of downstream applications.",2024-09-03,"Yuxiang Wei, Hojae Han, Rajhans Samdani",http://arxiv.org/pdf/2409.02326v1,cs.CL
Optimal L-Systems for Stochastic L-system Inference Problems,"This paper presents two novel theorems that address two open problems in
stochastic Lindenmayer-system (L-system) inference, specifically focusing on
the construction of an optimal stochastic L-system capable of generating a
given sequence of strings. The first theorem delineates a method for crafting a
stochastic L-system that has the maximum probability of a derivation producing
a given sequence of words through a single derivation (noting that multiple
derivations may generate the same sequence). Furthermore, the second theorem
determines the stochastic L-systems with the highest probability of producing a
given sequence of words with multiple possible derivations. From these, we
introduce an algorithm to infer an optimal stochastic L-system from a given
sequence. This algorithm incorporates advanced optimization techniques, such as
interior point methods, to ensure the creation of a stochastic L-system that
maximizes the probability of generating the given sequence (allowing for
multiple derivations). This allows for the use of stochastic L-systems as a
model for machine learning using only positive data for training.",2024-09-03,"Ali Lotfi, Ian McQuillan",http://arxiv.org/pdf/2409.02259v2,cs.CL
MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs,"Existing benchmarks for large language models (LLMs) increasingly struggle to
differentiate between top-performing models, underscoring the need for more
challenging evaluation frameworks. We introduce MMLU-Pro+, an enhanced
benchmark building upon MMLU-Pro to assess shortcut learning and higher-order
reasoning in LLMs. By incorporating questions with multiple correct answers
across diverse domains, MMLU-Pro+ tests LLMs' ability to engage in complex
reasoning and resist simplistic problem-solving strategies. Our results show
that MMLU-Pro+ maintains MMLU-Pro's difficulty while providing a more rigorous
test of model discrimination, particularly in multi-correct answer scenarios.
We introduce novel metrics like shortcut selection ratio and correct pair
identification ratio, offering deeper insights into model behavior and
anchoring bias. Evaluations of six state-of-the-art LLMs reveal significant
performance gaps, highlighting variations in reasoning abilities and bias
susceptibility. We release the dataset and evaluation codes at
\url{https://github.com/asgsaeid/mmlu-pro-plus}.",2024-09-03,"Saeid Asgari Taghanaki, Aliasgahr Khani, Amir Khasahmadi",http://arxiv.org/pdf/2409.02257v3,cs.CL
Therapy as an NLP Task: Psychologists' Comparison of LLMs and Human Peers in CBT,"Wider access to therapeutic care is one of the biggest challenges in mental
health treatment. Due to institutional barriers, some people seeking mental
health support have turned to large language models (LLMs) for personalized
therapy, even though these models are largely unsanctioned and untested. We
investigate the potential and limitations of using LLMs as providers of
evidence-based therapy by using mixed methods clinical metrics. Using HELPERT,
a prompt run on a large language model using the same process and training as a
comparative group of peer counselors, we replicated publicly accessible mental
health conversations rooted in Cognitive Behavioral Therapy (CBT) to compare
session dynamics and counselor's CBT-based behaviors between original peer
support sessions and their reconstructed HELPERT sessions. Two licensed,
CBT-trained clinical psychologists evaluated the sessions using the Cognitive
Therapy Rating Scale and provided qualitative feedback. Our findings show that
the peer sessions are characterized by empathy, small talk, therapeutic
alliance, and shared experiences but often exhibit therapist drift. Conversely,
HELPERT reconstructed sessions exhibit minimal therapist drift and higher
adherence to CBT methods but display a lack of collaboration, empathy, and
cultural understanding. Through CTRS ratings and psychologists' feedback, we
highlight the importance of human-AI collaboration for scalable mental health.
Our work outlines the ethical implication of imparting human-like subjective
qualities to LLMs in therapeutic settings, particularly the risk of deceptive
empathy, which may lead to unrealistic patient expectations and potential harm.",2024-09-03,"Zainab Iftikhar, Sean Ransom, Amy Xiao, Jeff Huang",http://arxiv.org/pdf/2409.02244v1,cs.CL
Temporal Order Preserved Optimal Transport-based Cross-modal Knowledge Transfer Learning for ASR,"Transferring linguistic knowledge from a pretrained language model (PLM) to
an acoustic model has been shown to greatly improve the performance of
automatic speech recognition (ASR). However, due to the heterogeneous feature
distributions in cross-modalities, designing an effective model for feature
alignment and knowledge transfer between linguistic and acoustic sequences
remains a challenging task. Optimal transport (OT), which efficiently measures
probability distribution discrepancies, holds great potential for aligning and
transferring knowledge between acoustic and linguistic modalities. Nonetheless,
the original OT treats acoustic and linguistic feature sequences as two
unordered sets in alignment and neglects temporal order information during OT
coupling estimation. Consequently, a time-consuming pretraining stage is
required to learn a good alignment between the acoustic and linguistic
representations. In this paper, we propose a Temporal Order Preserved OT
(TOT)-based Cross-modal Alignment and Knowledge Transfer (CAKT) (TOT-CAKT) for
ASR. In the TOT-CAKT, local neighboring frames of acoustic sequences are
smoothly mapped to neighboring regions of linguistic sequences, preserving
their temporal order relationship in feature alignment and matching. With the
TOT-CAKT model framework, we conduct Mandarin ASR experiments with a pretrained
Chinese PLM for linguistic knowledge transfer. Our results demonstrate that the
proposed TOT-CAKT significantly improves ASR performance compared to several
state-of-the-art models employing linguistic knowledge transfer, and addresses
the weaknesses of the original OT-based method in sequential feature alignment
for ASR.",2024-09-03,"Xugang Lu, Peng Shen, Yu Tsao, Hisashi Kawai",http://arxiv.org/pdf/2409.02239v2,cs.CL
Unforgettable Generalization in Language Models,"When language models (LMs) are trained to forget (or ""unlearn'') a skill, how
precisely does their behavior change? We study the behavior of transformer LMs
in which tasks have been forgotten via fine-tuning on randomized labels. Such
LMs learn to generate near-random predictions for individual examples in the
""training'' set used for forgetting. Across tasks, however, LMs exhibit extreme
variability in whether LM predictions change on examples outside the training
set. In some tasks (like entailment classification), forgetting generalizes
robustly, and causes models to produce uninformative predictions on new task
instances; in other tasks (like physical commonsense reasoning and scientific
question answering) forgetting affects only the training examples, and models
continue to perform the ""forgotten'' task accurately even for examples very
similar to those that appeared in the training set. Dataset difficulty is not
predictive of whether a behavior can be forgotten; instead, generalization in
forgetting is (weakly) predicted by the confidence of LMs' initial task
predictions and the variability of LM representations of training data, with
low confidence and low variability both associated with greater generalization.
Perhaps most surprisingly, random-label forgetting appears to be somewhat
insensitive to the contents of the training set: for example, models trained on
science questions with random labels continue to answer other science questions
accurately, but begin to produce random labels on entailment classification
tasks. Finally, we show that even generalizable forgetting is shallow: linear
probes trained on LMs' representations can still perform tasks reliably after
forgetting. Our results highlight the difficulty and unpredictability of
performing targeted skill removal from models via fine-tuning.",2024-09-03,"Eric Zhang, Leshem Chosen, Jacob Andreas",http://arxiv.org/pdf/2409.02228v1,cs.CL
Visually Grounded Speech Models for Low-resource Languages and Cognitive Modelling,"This dissertation examines visually grounded speech (VGS) models that learn
from unlabelled speech paired with images. It focuses on applications for
low-resource languages and understanding human language acquisition. We
introduce a task called visually prompted keyword localisation to detect and
localise keywords in speech using images. We demonstrate the effectiveness of
VGS models in few-shot learning scenarios for low-resource languages like
Yoruba. Additionally, we examine the mutual exclusivity bias in VGS models. Our
monolingual VGS model exhibits this bias, but we found that multilingualism
does not affect the bias in this VGS model similarly to what is observed in
children.",2024-09-03,Leanne Nortje,http://arxiv.org/pdf/2409.02865v1,cs.CL
Calibrating Language Models with Adaptive Temperature Scaling,"The effectiveness of large language models (LLMs) is not only measured by
their ability to generate accurate outputs but also by their calibration-how
well their confidence scores reflect the probability of their outputs being
correct. While unsupervised pre-training has been shown to yield LLMs with
well-calibrated conditional probabilities, recent studies have shown that after
fine-tuning with reinforcement learning from human feedback (RLHF), the
calibration of these models degrades significantly. In this work, we introduce
Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts
a temperature scaling parameter for each token prediction. The predicted
temperature values adapt based on token-level features and are fit over a
standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS
addresses the varying degrees of calibration shift that can occur after RLHF
fine-tuning. ATS improves calibration by over 10-50% across three downstream
natural language evaluation benchmarks compared to prior calibration methods
and does not impede performance improvements from RLHF.",2024-09-29,"Johnathan Xie, Annie S. Chen, Yoonho Lee, Eric Mitchell, Chelsea Finn",http://arxiv.org/pdf/2409.19817v1,cs.LG
Can Models Learn Skill Composition from Examples?,"As large language models (LLMs) become increasingly advanced, their ability
to exhibit compositional generalization -- the capacity to combine learned
skills in novel ways not encountered during training -- has garnered
significant attention. This type of generalization, particularly in scenarios
beyond training data, is also of great interest in the study of AI safety and
alignment. A recent study introduced the SKILL-MIX evaluation, where models are
tasked with composing a short paragraph demonstrating the use of a specified
$k$-tuple of language skills. While small models struggled with composing even
with $k=3$, larger models like GPT-4 performed reasonably well with $k=5$ and
$6$.
  In this paper, we employ a setup akin to SKILL-MIX to evaluate the capacity
of smaller models to learn compositional generalization from examples.
Utilizing a diverse set of language skills -- including rhetorical, literary,
reasoning, theory of mind, and common sense -- GPT-4 was used to generate text
samples that exhibit random subsets of $k$ skills. Subsequent fine-tuning of 7B
and 13B parameter models on these combined skill texts, for increasing values
of $k$, revealed the following findings: (1) Training on combinations of $k=2$
and $3$ skills results in noticeable improvements in the ability to compose
texts with $k=4$ and $5$ skills, despite models never having seen such examples
during training. (2) When skill categories are split into training and held-out
groups, models significantly improve at composing texts with held-out skills
during testing despite having only seen training skills during fine-tuning,
illustrating the efficacy of the training approach even with previously unseen
skills. This study also suggests that incorporating skill-rich (potentially
synthetic) text into training can substantially enhance the compositional
capabilities of models.",2024-09-29,"Haoyu Zhao, Simran Kaur, Dingli Yu, Anirudh Goyal, Sanjeev Arora",http://arxiv.org/pdf/2409.19808v2,cs.LG
Differentially Private Bilevel Optimization,"We present differentially private (DP) algorithms for bilevel optimization, a
problem class that received significant attention lately in various machine
learning applications. These are the first algorithms for such problems under
standard DP constraints, and are also the first to avoid Hessian computations
which are prohibitive in large-scale settings. Under the well-studied setting
in which the upper-level is not necessarily convex and the lower-level problem
is strongly-convex, our proposed gradient-based $(\epsilon,\delta)$-DP
algorithm returns a point with hypergradient norm at most
$\widetilde{\mathcal{O}}\left((\sqrt{d_\mathrm{up}}/\epsilon
n)^{1/2}+(\sqrt{d_\mathrm{low}}/\epsilon n)^{1/3}\right)$ where $n$ is the
dataset size, and $d_\mathrm{up}/d_\mathrm{low}$ are the upper/lower level
dimensions. Our analysis covers constrained and unconstrained problems alike,
accounts for mini-batch gradients, and applies to both empirical and population
losses. As an application, we specialize our analysis to derive a simple
private rule for tuning a regularization hyperparameter.",2024-09-29,Guy Kornowski,http://arxiv.org/pdf/2409.19800v2,cs.LG
Membership Inference Attacks Cannot Prove that a Model Was Trained On Your Data,"We consider the problem of a training data proof, where a data creator or
owner wants to demonstrate to a third party that some machine learning model
was trained on their data. Training data proofs play a key role in recent
lawsuits against foundation models trained on web-scale data. Many prior works
suggest to instantiate training data proofs using membership inference attacks.
We argue that this approach is fundamentally unsound: to provide convincing
evidence, the data creator needs to demonstrate that their attack has a low
false positive rate, i.e., that the attack's output is unlikely under the null
hypothesis that the model was not trained on the target data. Yet, sampling
from this null hypothesis is impossible, as we do not know the exact contents
of the training set, nor can we (efficiently) retrain a large foundation model.
We conclude by offering two paths forward, by showing that data extraction
attacks and membership inference on special canary data can be used to create
sound training data proofs.",2024-09-29,"Jie Zhang, Debeshee Das, Gautam Kamath, Florian Tramèr",http://arxiv.org/pdf/2409.19798v2,cs.LG
Gradient descent with adaptive stepsize converges (nearly) linearly under fourth-order growth,"A prevalent belief among optimization specialists is that linear convergence
of gradient descent is contingent on the function growing quadratically away
from its minimizers. In this work, we argue that this belief is inaccurate. We
show that gradient descent with an adaptive stepsize converges at a local
(nearly) linear rate on any smooth function that merely exhibits fourth-order
growth away from its minimizer. The adaptive stepsize we propose arises from an
intriguing decomposition theorem: any such function admits a smooth manifold
around the optimal solution -- which we call the ravine -- so that the function
grows at least quadratically away from the ravine and has constant order growth
along it. The ravine allows one to interlace many short gradient steps with a
single long Polyak gradient step, which together ensure rapid convergence to
the minimizer. We illustrate the theory and algorithm on the problems of matrix
sensing and factorization and learning a single neuron in the overparameterized
regime.",2024-09-29,"Damek Davis, Dmitriy Drusvyatskiy, Liwei Jiang",http://arxiv.org/pdf/2409.19791v1,cs.LG
Automatic debiasing of neural networks via moment-constrained learning,"Causal and nonparametric estimands in economics and biostatistics can often
be viewed as the mean of a linear functional applied to an unknown outcome
regression function. Naively learning the regression function and taking a
sample mean of the target functional results in biased estimators, and a rich
debiasing literature has developed where one additionally learns the so-called
Riesz representer (RR) of the target estimand (targeted learning, double ML,
automatic debiasing etc.). Learning the RR via its derived functional form can
be challenging, e.g. due to extreme inverse probability weights or the need to
learn conditional density functions. Such challenges have motivated recent
advances in automatic debiasing (AD), where the RR is learned directly via
minimization of a bespoke loss. We propose moment-constrained learning as a new
RR learning approach that addresses some shortcomings in AD, constraining the
predicted moments and improving the robustness of RR estimates to optimization
hyperparamters. Though our approach is not tied to a particular class of
learner, we illustrate it using neural networks, and evaluate on the problems
of average treatment/derivative effect estimation using semi-synthetic data.
Our numerical experiments show improved performance versus state of the art
benchmarks.",2024-09-29,"Christian L. Hines, Oliver J. Hines",http://arxiv.org/pdf/2409.19777v2,cs.LG
Adaptive Event-triggered Reinforcement Learning Control for Complex Nonlinear Systems,"In this paper, we propose an adaptive event-triggered reinforcement learning
control for continuous-time nonlinear systems, subject to bounded
uncertainties, characterized by complex interactions. Specifically, the
proposed method is capable of jointly learning both the control policy and the
communication policy, thereby reducing the number of parameters and
computational overhead when learning them separately or only one of them. By
augmenting the state space with accrued rewards that represent the performance
over the entire trajectory, we show that accurate and efficient determination
of triggering conditions is possible without the need for explicit learning
triggering conditions, thereby leading to an adaptive non-stationary policy.
Finally, we provide several numerical examples to demonstrate the effectiveness
of the proposed approach.",2024-09-29,"Umer Siddique, Abhinav Sinha, Yongcan Cao",http://arxiv.org/pdf/2409.19769v1,cs.LG
Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs,"As large language models (LLMs) are applied to more use cases, creating high
quality, task-specific datasets for fine-tuning becomes a bottleneck for model
improvement. Using high quality human data has been the most common approach to
unlock model performance, but is prohibitively expensive in many scenarios.
Several alternative methods have also emerged, such as generating synthetic or
hybrid data, but the effectiveness of these approaches remain unclear,
especially in resource-constrained scenarios and tasks that are not easily
verified. To investigate this, we group various synthetic data generation
strategies into three representative categories -- Answer Augmentation,
Question Rephrase and New Question -- and study the performance of student LLMs
trained under various constraints, namely seed instruction set size and query
budget. We demonstrate that these strategies are not equally effective across
settings. Notably, the optimal data generation strategy depends strongly on the
ratio between the available teacher query budget and the size of the seed
instruction set. When this ratio is low, generating new answers to existing
questions proves most effective, but as this ratio increases, generating new
questions becomes optimal. Across all tasks, we find that choice of
augmentation method and other design choices matter substantially more in low
to mid data regimes than in high data regimes. We provide a practical framework
for selecting the appropriate augmentation method across settings, taking into
account additional factors such as the scalability of each method, the
importance of verifying synthetic data, and the use of different LLMs for
synthetic data generation.",2024-09-29,"Yung-Chieh Chan, George Pu, Apaar Shanker, Parth Suresh, Penn Jenks, John Heyer, Sam Denton",http://arxiv.org/pdf/2409.19759v3,cs.LG
Balancing the Scales: A Comprehensive Study on Tackling Class Imbalance in Binary Classification,"Class imbalance in binary classification tasks remains a significant
challenge in machine learning, often resulting in poor performance on minority
classes. This study comprehensively evaluates three widely-used strategies for
handling class imbalance: Synthetic Minority Over-sampling Technique (SMOTE),
Class Weights tuning, and Decision Threshold Calibration. We compare these
methods against a baseline scenario of no-intervention across 15 diverse
machine learning models and 30 datasets from various domains, conducting a
total of 9,000 experiments. Performance was primarily assessed using the
F1-score, although our study also tracked results on additional 9 metrics
including F2-score, precision, recall, Brier-score, PR-AUC, and AUC. Our
results indicate that all three strategies generally outperform the baseline,
with Decision Threshold Calibration emerging as the most consistently effective
technique. However, we observed substantial variability in the best-performing
method across datasets, highlighting the importance of testing multiple
approaches for specific problems. This study provides valuable insights for
practitioners dealing with imbalanced datasets and emphasizes the need for
dataset-specific analysis in evaluating class imbalance handling techniques.",2024-09-29,"Mohamed Abdelhamid, Abhyuday Desai",http://arxiv.org/pdf/2409.19751v1,cs.LG
Tailored Federated Learning: Leveraging Direction Regulation & Knowledge Distillation,"Federated learning (FL) has emerged as a transformative training paradigm,
particularly invaluable in privacy-sensitive domains like healthcare. However,
client heterogeneity in data, computing power, and tasks poses a significant
challenge. To address such a challenge, we propose an FL optimization algorithm
that integrates model delta regularization, personalized models, federated
knowledge distillation, and mix-pooling. Model delta regularization optimizes
model updates centrally on the server, efficiently updating clients with
minimal communication costs. Personalized models and federated knowledge
distillation strategies are employed to tackle task heterogeneity effectively.
Additionally, mix-pooling is introduced to accommodate variations in the
sensitivity of readout operations. Experimental results demonstrate the
remarkable accuracy and rapid convergence achieved by model delta
regularization. Additionally, the federated knowledge distillation algorithm
notably improves FL performance, especially in scenarios with diverse data.
Moreover, mix-pooling readout operations provide tangible benefits for clients,
showing the effectiveness of our proposed methods.",2024-09-29,"Huidong Tang, Chen Li, Huachong Yu, Sayaka Kamei, Yasuhiko Morimoto",http://arxiv.org/pdf/2409.19741v1,cs.LG
When Molecular GAN Meets Byte-Pair Encoding,"Deep generative models, such as generative adversarial networks (GANs), are
pivotal in discovering novel drug-like candidates via de novo molecular
generation. However, traditional character-wise tokenizers often struggle with
identifying novel and complex sub-structures in molecular data. In contrast,
alternative tokenization methods have demonstrated superior performance. This
study introduces a molecular GAN that integrates a byte level byte-pair
encoding tokenizer and employs reinforcement learning to enhance de novo
molecular generation. Specifically, the generator functions as an actor,
producing SMILES strings, while the discriminator acts as a critic, evaluating
their quality. Our molecular GAN also integrates innovative reward mechanisms
aimed at improving computational efficiency. Experimental results assessing
validity, uniqueness, novelty, and diversity, complemented by detailed
visualization analysis, robustly demonstrate the effectiveness of our GAN.",2024-09-29,"Huidong Tang, Chen Li, Yasuhiko Morimoto",http://arxiv.org/pdf/2409.19740v1,cs.LG
Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement,"Machine unlearning (MU) has emerged to enhance the privacy and
trustworthiness of deep neural networks. Approximate MU is a practical method
for large-scale models. Our investigation into approximate MU starts with
identifying the steepest descent direction, minimizing the output
Kullback-Leibler divergence to exact MU inside a parameters' neighborhood. This
probed direction decomposes into three components: weighted forgetting gradient
ascent, fine-tuning retaining gradient descent, and a weight saliency matrix.
Such decomposition derived from Euclidean metric encompasses most existing
gradient-based MU methods. Nevertheless, adhering to Euclidean space may result
in sub-optimal iterative trajectories due to the overlooked geometric structure
of the output probability space. We suggest embedding the unlearning update
into a manifold rendered by the remaining geometry, incorporating second-order
Hessian from the remaining data. It helps prevent effective unlearning from
interfering with the retained performance. However, computing the second-order
Hessian for large-scale models is intractable. To efficiently leverage the
benefits of Hessian modulation, we propose a fast-slow parameter update
strategy to implicitly approximate the up-to-date salient unlearning direction.
Free from specific modal constraints, our approach is adaptable across computer
vision unlearning tasks, including classification and generation. Extensive
experiments validate our efficacy and efficiency. Notably, our method
successfully performs class-forgetting on ImageNet using DiT and forgets a
class on CIFAR-10 using DDPM in just 50 steps, compared to thousands of steps
required by previous methods.",2024-09-29,"Zhehao Huang, Xinwen Cheng, JingHao Zheng, Haoran Wang, Zhengbao He, Tao Li, Xiaolin Huang",http://arxiv.org/pdf/2409.19732v1,cs.LG
Investigating the Effect of Network Pruning on Performance and Interpretability,"Deep Neural Networks (DNNs) are often over-parameterized for their tasks and
can be compressed quite drastically by removing weights, a process called
pruning. We investigate the impact of different pruning techniques on the
classification performance and interpretability of GoogLeNet. We systematically
apply unstructured and structured pruning, as well as connection sparsity
(pruning of input weights) methods to the network and analyze the outcomes
regarding the network's performance on the validation set of ImageNet. We also
compare different retraining strategies, such as iterative pruning and one-shot
pruning. We find that with sufficient retraining epochs, the performance of the
networks can approximate the performance of the default GoogLeNet - and even
surpass it in some cases. To assess interpretability, we employ the Mechanistic
Interpretability Score (MIS) developed by Zimmermann et al. . Our experiments
reveal that there is no significant relationship between interpretability and
pruning rate when using MIS as a measure. Additionally, we observe that
networks with extremely low accuracy can still achieve high MIS scores,
suggesting that the MIS may not always align with intuitive notions of
interpretability, such as understanding the basis of correct decisions.",2024-09-29,"Jonathan von Rad, Florian Seuffert",http://arxiv.org/pdf/2409.19727v2,cs.LG
DataDRILL: Formation Pressure Prediction and Kick Detection for Drilling Rigs,"Accurate real-time prediction of formation pressure and kick detection is
crucial for drilling operations, as it can significantly improve
decision-making and the cost-effectiveness of the process. Data-driven models
have gained popularity for automating drilling operations by predicting
formation pressure and detecting kicks. However, the current literature does
not make supporting datasets publicly available to advance research in the
field of drilling rigs, thus impeding technological progress in this domain.
This paper introduces two new datasets to support researchers in developing
intelligent algorithms to enhance oil/gas well drilling research. The datasets
include data samples for formation pressure prediction and kick detection with
28 drilling variables and more than 2000 data samples. Principal component
regression is employed to forecast formation pressure, while principal
component analysis is utilized to identify kicks for the dataset's technical
validation. Notably, the R2 and Residual Predictive Deviation scores for
principal component regression are 0.78 and 0.922, respectively.",2024-09-29,"Murshedul Arifeen, Andrei Petrovski, Md Junayed Hasan, Igor Kotenko, Maksim Sletov, Phil Hassard",http://arxiv.org/pdf/2409.19724v1,cs.LG
Evolving Multi-Scale Normalization for Time Series Forecasting under Distribution Shifts,"Complex distribution shifts are the main obstacle to achieving accurate
long-term time series forecasting. Several efforts have been conducted to
capture the distribution characteristics and propose adaptive normalization
techniques to alleviate the influence of distribution shifts. However, these
methods neglect the intricate distribution dynamics observed from various
scales and the evolving functions of distribution dynamics and normalized
mapping relationships. To this end, we propose a novel model-agnostic Evolving
Multi-Scale Normalization (EvoMSN) framework to tackle the distribution shift
problem. Flexible normalization and denormalization are proposed based on the
multi-scale statistics prediction module and adaptive ensembling. An evolving
optimization strategy is designed to update the forecasting model and
statistics prediction module collaboratively to track the shifting
distributions. We evaluate the effectiveness of EvoMSN in improving the
performance of five mainstream forecasting methods on benchmark datasets and
also show its superiority compared to existing advanced normalization and
online learning approaches. The code is publicly available at
https://github.com/qindalin/EvoMSN.",2024-09-29,"Dalin Qin, Yehui Li, Weiqi Chen, Zhaoyang Zhu, Qingsong Wen, Liang Sun, Pierre Pinson, Yi Wang",http://arxiv.org/pdf/2409.19718v1,cs.LG
Constrained Reinforcement Learning for Safe Heat Pump Control,"Constrained Reinforcement Learning (RL) has emerged as a significant research
area within RL, where integrating constraints with rewards is crucial for
enhancing safety and performance across diverse control tasks. In the context
of heating systems in the buildings, optimizing the energy efficiency while
maintaining the residents' thermal comfort can be intuitively formulated as a
constrained optimization problem. However, to solve it with RL may require
large amount of data. Therefore, an accurate and versatile simulator is
favored. In this paper, we propose a novel building simulator I4B which
provides interfaces for different usages and apply a model-free constrained RL
algorithm named constrained Soft Actor-Critic with Linear Smoothed Log Barrier
function (CSAC-LB) to the heating optimization problem. Benchmarking against
baseline algorithms demonstrates CSAC-LB's efficiency in data exploration,
constraint satisfaction and performance.",2024-09-29,"Baohe Zhang, Lilli Frison, Thomas Brox, Joschka Bödecker",http://arxiv.org/pdf/2409.19716v1,cs.LG
Generating peak-aware pseudo-measurements for low-voltage feeders using metadata of distribution system operators,"Distribution system operators (DSOs) must cope with new challenges such as
the reconstruction of distribution grids along climate neutrality pathways or
the ability to manage and control consumption and generation in the grid. In
order to meet the challenges, measurements within the distribution grid often
form the basis for DSOs. Hence, it is an urgent problem that measurement
devices are not installed in many low-voltage (LV) grids. In order to overcome
this problem, we present an approach to estimate pseudo-measurements for
non-measured LV feeders based on the metadata of the respective feeder using
regression models. The feeder metadata comprise information about the number of
grid connection points, the installed power of consumers and producers, and
billing data in the downstream LV grid. Additionally, we use weather data,
calendar data and timestamp information as model features. The existing
measurements are used as model target. We extensively evaluate the estimated
pseudo-measurements on a large real-world dataset with 2,323 LV feeders
characterized by both consumption and feed-in. For this purpose, we introduce
peak metrics inspired by the BigDEAL challenge for the peak magnitude, timing
and shape for both consumption and feed-in. As regression models, we use
XGBoost, a multilayer perceptron (MLP) and a linear regression (LR). We observe
that XGBoost and MLP outperform the LR. Furthermore, the results show that the
approach adapts to different weather, calendar and timestamp conditions and
produces realistic load curves based on the feeder metadata. In the future, the
approach can be adapted to other grid levels like substation transformers and
can supplement research fields like load modeling, state estimation and LV load
forecasting.",2024-09-29,"Manuel Treutlein, Marc Schmidt, Roman Hahn, Matthias Hertel, Benedikt Heidrich, Ralf Mikut, Veit Hagenmeyer",http://arxiv.org/pdf/2409.19713v1,cs.LG
A multimodal LLM for the non-invasive decoding of spoken text from brain recordings,"Brain-related research topics in artificial intelligence have recently gained
popularity, particularly due to the expansion of what multimodal architectures
can do from computer vision to natural language processing. Our main goal in
this work is to explore the possibilities and limitations of these
architectures in spoken text decoding from non-invasive fMRI recordings.
Contrary to vision and textual data, fMRI data represent a complex modality due
to the variety of brain scanners, which implies (i) the variety of the recorded
signal formats, (ii) the low resolution and noise of the raw signals, and (iii)
the scarcity of pretrained models that can be leveraged as foundation models
for generative learning. These points make the problem of the non-invasive
decoding of text from fMRI recordings very challenging. In this paper, we
propose and end-to-end multimodal LLM for decoding spoken text from fMRI
signals. The proposed architecture is founded on (i) an encoder derived from a
specific transformer incorporating an augmented embedding layer for the encoder
and a better-adjusted attention mechanism than that present in the state of the
art, and (ii) a frozen large language model adapted to align the embedding of
the input text and the encoded embedding of brain activity to decode the output
text. A benchmark in performed on a corpus consisting of a set of interactions
human-human and human-robot interactions where fMRI and conversational signals
are recorded synchronously. The obtained results are very promising, as our
proposal outperforms the evaluated models, and is able to generate text
capturing more accurate semantics present in the ground truth. The
implementation code is provided in https://github.com/Hmamouche/brain_decode.",2024-09-29,"Youssef Hmamouche, Ismail Chihab, Lahoucine Kdouri, Amal El Fallah Seghrouchni",http://arxiv.org/pdf/2409.19710v1,cs.LG
Neural Decompiling of Tracr Transformers,"Recently, the transformer architecture has enabled substantial progress in
many areas of pattern recognition and machine learning. However, as with other
neural network models, there is currently no general method available to
explain their inner workings. The present paper represents a first step towards
this direction. We utilize \textit{Transformer Compiler for RASP} (Tracr) to
generate a large dataset of pairs of transformer weights and corresponding RASP
programs. Based on this dataset, we then build and train a model, with the aim
of recovering the RASP code from the compiled model. We demonstrate that the
simple form of Tracr compiled transformer weights is interpretable for such a
decompiler model. In an empirical evaluation, our model achieves exact
reproductions on more than 30\% of the test objects, while the remaining 70\%
can generally be reproduced with only few errors. Additionally, more than 70\%
of the programs, produced by our model, are functionally equivalent to the
ground truth, and therefore a valid decompilation of the Tracr compiled
transformer weights.",2024-09-29,"Hannes Thurnherr, Kaspar Riesen",http://arxiv.org/pdf/2410.00061v1,cs.LG
Vision-Language Models are Strong Noisy Label Detectors,"Recent research on fine-tuning vision-language models has demonstrated
impressive performance in various downstream tasks. However, the challenge of
obtaining accurately labeled data in real-world applications poses a
significant obstacle during the fine-tuning process. To address this challenge,
this paper presents a Denoising Fine-Tuning framework, called DeFT, for
adapting vision-language models. DeFT utilizes the robust alignment of textual
and visual features pre-trained on millions of auxiliary image-text pairs to
sieve out noisy labels. The proposed framework establishes a noisy label
detector by learning positive and negative textual prompts for each class. The
positive prompt seeks to reveal distinctive features of the class, while the
negative prompt serves as a learnable threshold for separating clean and noisy
samples. We employ parameter-efficient fine-tuning for the adaptation of a
pre-trained visual encoder to promote its alignment with the learned textual
prompts. As a general framework, DeFT can seamlessly fine-tune many pre-trained
models to downstream tasks by utilizing carefully selected clean samples.
Experimental results on seven synthetic and real-world noisy datasets validate
the effectiveness of DeFT in both noisy label detection and image
classification.",2024-09-29,"Tong Wei, Hao-Tian Li, Chun-Shu Li, Jiang-Xin Shi, Yu-Feng Li, Min-Ling Zhang",http://arxiv.org/pdf/2409.19696v1,cs.LG
InfantCryNet: A Data-driven Framework for Intelligent Analysis of Infant Cries,"Understanding the meaning of infant cries is a significant challenge for
young parents in caring for their newborns. The presence of background noise
and the lack of labeled data present practical challenges in developing systems
that can detect crying and analyze its underlying reasons. In this paper, we
present a novel data-driven framework, ""InfantCryNet,"" for accomplishing these
tasks. To address the issue of data scarcity, we employ pre-trained audio
models to incorporate prior knowledge into our model. We propose the use of
statistical pooling and multi-head attention pooling techniques to extract
features more effectively. Additionally, knowledge distillation and model
quantization are applied to enhance model efficiency and reduce the model size,
better supporting industrial deployment in mobile devices. Experiments on
real-life datasets demonstrate the superior performance of the proposed
framework, outperforming state-of-the-art baselines by 4.4% in classification
accuracy. The model compression effectively reduces the model size by 7%
without compromising performance and by up to 28% with only an 8% decrease in
accuracy, offering practical insights for model selection and system design.",2024-09-29,"Mengze Hong, Chen Jason Zhang, Lingxiao Yang, Yuanfeng Song, Di Jiang",http://arxiv.org/pdf/2409.19689v2,cs.LG
Machine Learning for Raman Spectroscopy-based Cyber-Marine Fish Biochemical Composition Analysis,"The rapid and accurate detection of biochemical compositions in fish is a
crucial real-world task that facilitates optimal utilization and extraction of
high-value products in the seafood industry. Raman spectroscopy provides a
promising solution for quickly and non-destructively analyzing the biochemical
composition of fish by associating Raman spectra with biochemical reference
data using machine learning regression models. This paper investigates
different regression models to address this task and proposes a new design of
Convolutional Neural Networks (CNNs) for jointly predicting water, protein, and
lipids yield. To the best of our knowledge, we are the first to conduct a
successful study employing CNNs to analyze the biochemical composition of fish
based on a very small Raman spectroscopic dataset. Our approach combines a
tailored CNN architecture with the comprehensive data preparation procedure,
effectively mitigating the challenges posed by extreme data scarcity. The
results demonstrate that our CNN can significantly outperform two
state-of-the-art CNN models and multiple traditional machine learning models,
paving the way for accurate and automated analysis of fish biochemical
composition.",2024-09-29,"Yun Zhou, Gang Chen, Bing Xue, Mengjie Zhang, Jeremy S. Rooney, Kirill Lagutin, Andrew MacKenzie, Keith C. Gordon, Daniel P. Killeen",http://arxiv.org/pdf/2409.19688v1,cs.LG
Nonideality-aware training makes memristive networks more robust to adversarial attacks,"Neural networks are now deployed in a wide number of areas from object
classification to natural language systems. Implementations using analog
devices like memristors promise better power efficiency, potentially bringing
these applications to a greater number of environments. However, such systems
suffer from more frequent device faults and overall, their exposure to
adversarial attacks has not been studied extensively. In this work, we
investigate how nonideality-aware training - a common technique to deal with
physical nonidealities - affects adversarial robustness. We find that
adversarial robustness is significantly improved, even with limited knowledge
of what nonidealities will be encountered during test time.",2024-09-29,"Dovydas Joksas, Luis Muñoz-González, Emil Lupu, Adnan Mehonic",http://arxiv.org/pdf/2409.19671v1,cs.LG
"Stock Price Prediction and Traditional Models: An Approach to Achieve Short-, Medium- and Long-Term Goals","A comparative analysis of deep learning models and traditional statistical
methods for stock price prediction uses data from the Nigerian stock exchange.
Historical data, including daily prices and trading volumes, are employed to
implement models such as Long Short Term Memory (LSTM) networks, Gated
Recurrent Units (GRUs), Autoregressive Integrated Moving Average (ARIMA), and
Autoregressive Moving Average (ARMA). These models are assessed over three-time
horizons: short-term (1 year), medium-term (2.5 years), and long-term (5
years), with performance measured by Mean Squared Error (MSE) and Mean Absolute
Error (MAE). The stability of the time series is tested using the Augmented
Dickey-Fuller (ADF) test. Results reveal that deep learning models,
particularly LSTM, outperform traditional methods by capturing complex,
nonlinear patterns in the data, resulting in more accurate predictions.
However, these models require greater computational resources and offer less
interpretability than traditional approaches. The findings highlight the
potential of deep learning for improving financial forecasting and investment
strategies. Future research could incorporate external factors such as social
media sentiment and economic indicators, refine model architectures, and
explore real-time applications to enhance prediction accuracy and scalability.",2024-09-29,"Opeyemi Sheu Alamu, Md Kamrul Siam",http://arxiv.org/pdf/2410.07220v1,cs.LG
Temporal Source Recovery for Time-Series Source-Free Unsupervised Domain Adaptation,"Source-Free Unsupervised Domain Adaptation (SFUDA) has gained popularity for
its ability to adapt pretrained models to target domains without accessing
source domains, ensuring source data privacy. While SFUDA is well-developed in
visual tasks, its application to Time-Series SFUDA (TS-SFUDA) remains limited
due to the challenge of transferring crucial temporal dependencies across
domains. Although a few researchers begin to explore this area, they rely on
specific source domain designs, which are impractical as source data owners
cannot be expected to follow particular pretraining protocols. To solve this,
we propose Temporal Source Recovery (TemSR), a framework that transfers
temporal dependencies for effective TS-SFUDA without requiring source-specific
designs. TemSR features a recovery process that leverages masking, recovery,
and optimization to generate a source-like distribution with recovered source
temporal dependencies. To ensure effective recovery, we further design
segment-based regularization to restore local dependencies and anchor-based
recovery diversity maximization to enhance the diversity of the source-like
distribution. The source-like distribution is then adapted to the target domain
using traditional UDA techniques. Extensive experiments across multiple TS
tasks demonstrate the effectiveness of TemSR, even surpassing existing TS-SFUDA
method that requires source domain designs. Code is available in
https://github.com/Frank-Wang-oss/TemSR.",2024-09-29,"Yucheng Wang, Peiliang Gong, Min Wu, Felix Ott, Xiaoli Li, Lihua Xie, Zhenghua Chen",http://arxiv.org/pdf/2409.19635v1,cs.LG
"A Survey on Graph Neural Networks for Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends","Remaining Useful Life (RUL) prediction is a critical aspect of Prognostics
and Health Management (PHM), aimed at predicting the future state of a system
to enable timely maintenance and prevent unexpected failures. While existing
deep learning methods have shown promise, they often struggle to fully leverage
the spatial information inherent in complex systems, limiting their
effectiveness in RUL prediction. To address this challenge, recent research has
explored the use of Graph Neural Networks (GNNs) to model spatial information
for more accurate RUL prediction. This paper presents a comprehensive review of
GNN techniques applied to RUL prediction, summarizing existing methods and
offering guidance for future research. We first propose a novel taxonomy based
on the stages of adapting GNNs to RUL prediction, systematically categorizing
approaches into four key stages: graph construction, graph modeling, graph
information processing, and graph readout. By organizing the field in this way,
we highlight the unique challenges and considerations at each stage of the GNN
pipeline. Additionally, we conduct a thorough evaluation of various
state-of-the-art (SOTA) GNN methods, ensuring consistent experimental settings
for fair comparisons. This rigorous analysis yields valuable insights into the
strengths and weaknesses of different approaches, serving as an experimental
guide for researchers and practitioners working in this area. Finally, we
identify and discuss several promising research directions that could further
advance the field, emphasizing the potential for GNNs to revolutionize RUL
prediction and enhance the effectiveness of PHM strategies. The benchmarking
codes are available in GitHub:
https://github.com/Frank-Wang-oss/GNN\_RUL\_Benchmarking.",2024-09-29,"Yucheng Wang, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen",http://arxiv.org/pdf/2409.19629v1,cs.LG
IDEA: An Inverse Domain Expert Adaptation Based Active DNN IP Protection Method,"Illegitimate reproduction, distribution and derivation of Deep Neural Network
(DNN) models can inflict economic loss, reputation damage and even privacy
infringement. Passive DNN intellectual property (IP) protection methods such as
watermarking and fingerprinting attempt to prove the ownership upon IP
violation, but they are often too late to stop catastrophic damage of IP abuse
and too feeble against strong adversaries. In this paper, we propose IDEA, an
Inverse Domain Expert Adaptation based proactive DNN IP protection method
featuring active authorization and source traceability. IDEA generalizes active
authorization as an inverse problem of domain adaptation. The multi-adaptive
optimization is solved by a mixture-of-experts model with one real and two fake
experts. The real expert re-optimizes the source model to correctly classify
test images with a unique model user key steganographically embedded. The fake
experts are trained to output random prediction on test images without or with
incorrect user key embedded by minimizing their mutual information (MI) with
the real expert. The MoE model is knowledge distilled into a unified protected
model to avoid leaking the expert model features by maximizing their MI with
additional multi-layer attention and contrastive representation loss
optimization. IDEA not only prevents unauthorized users without the valid key
to access the functional model, but also enable the model owner to validate the
deployed model and trace the source of IP infringement. We extensively evaluate
IDEA on five datasets and four DNN models to demonstrate its effectiveness in
authorization control, culprit tracing success rate, and robustness against
various attacks.",2024-09-29,"Chaohui Xu, Qi Cui, Jinxin Dong, Weiyang He, Chip-Hong Chang",http://arxiv.org/pdf/2410.00059v1,cs.LG
DropEdge not Foolproof: Effective Augmentation Method for Signed Graph Neural Networks,"The paper discusses signed graphs, which model friendly or antagonistic
relationships using edges marked with positive or negative signs, focusing on
the task of link sign prediction. While Signed Graph Neural Networks (SGNNs)
have advanced, they face challenges like graph sparsity and unbalanced
triangles. The authors propose using data augmentation (DA) techniques to
address these issues, although many existing methods are not suitable for
signed graphs due to a lack of side information. They highlight that the random
DropEdge method, a rare DA approach applicable to signed graphs, does not
enhance link sign prediction performance. In response, they introduce the
Signed Graph Augmentation (SGA) framework, which includes a structure
augmentation module to identify candidate edges and a strategy for selecting
beneficial candidates, ultimately improving SGNN training. Experimental results
show that SGA significantly boosts the performance of SGNN models, with a
notable 32.3% improvement in F1-micro for SGCN on the Slashdot dataset.",2024-09-29,"Zeyu Zhang, Lu Li, Shuyan Wan, Sijie Wang, Zhiyi Wang, Zhiyuan Lu, Dong Hao, Wanli Li",http://arxiv.org/pdf/2409.19620v2,cs.LG
DuoGNN: Topology-aware Graph Neural Network with Homophily and Heterophily Interaction-Decoupling,"Graph Neural Networks (GNNs) have proven effective in various medical imaging
applications, such as automated disease diagnosis. However, due to the local
neighborhood aggregation paradigm in message passing which characterizes these
models, they inherently suffer from two fundamental limitations: first,
indistinguishable node embeddings due to heterophilic node aggregation (known
as over-smoothing), and second, impaired message passing due to aggregation
through graph bottlenecks (known as over-squashing). These challenges hinder
the model expressiveness and prevent us from using deeper models to capture
long-range node dependencies within the graph. Popular solutions in the
literature are either too expensive to process large graphs due to high time
complexity or do not generalize across all graph topologies. To address these
limitations, we propose DuoGNN, a scalable and generalizable architecture which
leverages topology to decouple homophilic and heterophilic edges and capture
both short-range and long-range interactions. Our three core contributions
introduce (i) a topological edge-filtering algorithm which extracts homophilic
interactions and enables the model to generalize well for any graph topology,
(ii) a heterophilic graph condensation technique which extracts heterophilic
interactions and ensures scalability, and (iii) a dual homophilic and
heterophilic aggregation pipeline which prevents over-smoothing and
over-squashing during the message passing. We benchmark our model on medical
and non-medical node classification datasets and compare it with its variants,
showing consistent improvements across all tasks. Our DuoGNN code is available
at https://github.com/basiralab/DuoGNN.",2024-09-29,"K. Mancini, I. Rekik",http://arxiv.org/pdf/2409.19616v2,cs.LG
Federated Learning from Vision-Language Foundation Models: Theoretical Analysis and Method,"Integrating pretrained vision-language foundation models like CLIP into
federated learning has attracted significant attention for enhancing
generalization across diverse tasks. Typically, federated learning of
vision-language models employs prompt learning to reduce communication and
computational costs, i.e., prompt-based federated learning. However, there is
limited theoretical analysis to understand the performance of prompt-based
federated learning. In this work, we construct a theoretical analysis framework
for prompt-based federated learning via feature learning theory. Specifically,
we monitor the evolution of signal learning and noise memorization in
prompt-based federated learning, demonstrating that performance can be assessed
by the ratio of task-relevant to task-irrelevant coefficients. Furthermore, we
draw an analogy between income and risk in portfolio optimization and the
task-relevant and task-irrelevant terms in feature learning. Leveraging
inspiration from portfolio optimization that combining two independent assets
will maintain the income while reducing the risk, we introduce two prompts:
global prompt and local prompt to construct a prompt portfolio to balance the
generalization and personalization. Consequently, we showed the performance
advantage of the prompt portfolio and derived the optimal mixing coefficient.
These theoretical claims have been further supported by empirical experiments.",2024-09-29,"Bikang Pan, Wei Huang, Ye Shi",http://arxiv.org/pdf/2409.19610v1,cs.LG
Hyper-Connections,"We present hyper-connections, a simple yet effective method that can serve as
an alternative to residual connections. This approach specifically addresses
common drawbacks observed in residual connection variants, such as the seesaw
effect between gradient vanishing and representation collapse. Theoretically,
hyper-connections allow the network to adjust the strength of connections
between features at different depths and dynamically rearrange layers. We
conduct experiments focusing on the pre-training of large language models,
including dense and sparse models, where hyper-connections show significant
performance improvements over residual connections. Additional experiments
conducted on vision tasks also demonstrate similar improvements. We anticipate
that this method will be broadly applicable and beneficial across a wide range
of AI problems.",2024-09-29,"Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu, Qiyang Min, Xun Zhou",http://arxiv.org/pdf/2409.19606v3,cs.LG
The Crucial Role of Samplers in Online Direct Preference Optimization,"Direct Preference Optimization (DPO) has emerged as a stable, scalable, and
efficient solution for language model alignment. Despite its empirical success,
the optimization properties, particularly the impact of samplers on its
convergence rates, remain under-explored. In this paper, we provide a rigorous
analysis of DPO's convergence rates with different sampling strategies under
the exact gradient setting, revealing a surprising separation: uniform sampling
achieves $\textbf{linear}$ convergence, while our proposed online sampler
achieves $\textbf{quadratic}$ convergence. We further adapt the sampler to
practical settings by incorporating posterior distributions and logit mixing,
demonstrating improvements over previous methods. For example, it outperforms
vanilla DPO by over $7.4$% on Safe-RLHF dataset. Our results not only offer
insights into the theoretical understanding of DPO but also pave the way for
further algorithm designs.",2024-09-29,"Ruizhe Shi, Runlong Zhou, Simon S. Du",http://arxiv.org/pdf/2409.19605v3,cs.LG
An Unbiased Risk Estimator for Partial Label Learning with Augmented Classes,"Partial Label Learning (PLL) is a typical weakly supervised learning task,
which assumes each training instance is annotated with a set of candidate
labels containing the ground-truth label. Recent PLL methods adopt
identification-based disambiguation to alleviate the influence of false
positive labels and achieve promising performance. However, they require all
classes in the test set to have appeared in the training set, ignoring the fact
that new classes will keep emerging in real applications. To address this
issue, in this paper, we focus on the problem of Partial Label Learning with
Augmented Class (PLLAC), where one or more augmented classes are not visible in
the training stage but appear in the inference stage. Specifically, we propose
an unbiased risk estimator with theoretical guarantees for PLLAC, which
estimates the distribution of augmented classes by differentiating the
distribution of known classes from unlabeled data and can be equipped with
arbitrary PLL loss functions. Besides, we provide a theoretical analysis of the
estimation error bound of the estimator, which guarantees the convergence of
the empirical risk minimizer to the true risk minimizer as the number of
training data tends to infinity. Furthermore, we add a risk-penalty
regularization term in the optimization objective to alleviate the influence of
the over-fitting issue caused by negative empirical risk. Extensive experiments
on benchmark, UCI and real-world datasets demonstrate the effectiveness of the
proposed approach.",2024-09-29,"Jiayu Hu, Senlin Shu, Beibei Li, Tao Xiang, Zhongshi He",http://arxiv.org/pdf/2409.19600v1,cs.LG
Solution for Temporal Sound Localisation Task of ECCV Second Perception Test Challenge 2024,"This report proposes an improved method for the Temporal Sound Localisation
(TSL) task, which localizes and classifies the sound events occurring in the
video according to a predefined set of sound classes. The champion solution
from last year's first competition has explored the TSL by fusing audio and
video modalities with the same weight. Considering the TSL task aims to
localize sound events, we conduct relevant experiments that demonstrated the
superiority of sound features (Section 3). Based on our findings, to enhance
audio modality features, we employ various models to extract audio features,
such as InterVideo, CaVMAE, and VideoMAE models. Our approach ranks first in
the final test with a score of 0.4925.",2024-09-29,"Haowei Gu, Weihao Zhu, Yang Yang",http://arxiv.org/pdf/2409.19595v1,cs.LG
DiffCP: Ultra-Low Bit Collaborative Perception via Diffusion Model,"Collaborative perception (CP) is emerging as a promising solution to the
inherent limitations of stand-alone intelligence. However, current wireless
communication systems are unable to support feature-level and raw-level
collaborative algorithms due to their enormous bandwidth demands. In this
paper, we propose DiffCP, a novel CP paradigm that utilizes a specialized
diffusion model to efficiently compress the sensing information of
collaborators. By incorporating both geometric and semantic conditions into the
generative model, DiffCP enables feature-level collaboration with an ultra-low
communication cost, advancing the practical implementation of CP systems. This
paradigm can be seamlessly integrated into existing CP algorithms to enhance a
wide range of downstream tasks. Through extensive experimentation, we
investigate the trade-offs between communication, computation, and performance.
Numerical results demonstrate that DiffCP can significantly reduce
communication costs by 14.5-fold while maintaining the same performance as the
state-of-the-art algorithm.",2024-09-29,"Ruiqing Mao, Haotian Wu, Yukuan Jia, Zhaojun Nan, Yuxuan Sun, Sheng Zhou, Deniz Gündüz, Zhisheng Niu",http://arxiv.org/pdf/2409.19592v1,cs.LG
Brain Tumor Classification on MRI in Light of Molecular Markers,"In research findings, co-deletion of the 1p/19q gene is associated with
clinical outcomes in low-grade gliomas. The ability to predict 1p19q status is
critical for treatment planning and patient follow-up. This study aims to
utilize a specially MRI-based convolutional neural network for brain cancer
detection. Although public networks such as RestNet and AlexNet can effectively
diagnose brain cancers using transfer learning, the model includes quite a few
weights that have nothing to do with medical images. As a result, the
diagnostic results are unreliable by the transfer learning model. To deal with
the problem of trustworthiness, we create the model from the ground up, rather
than depending on a pre-trained model. To enable flexibility, we combined
convolution stacking with a dropout and full connect operation, it improved
performance by reducing overfitting. During model training, we also supplement
the given dataset and inject Gaussian noise. We use three--fold
cross-validation to train the best selection model. Comparing InceptionV3,
VGG16, and MobileNetV2 fine-tuned with pre-trained models, our model produces
better results. On an validation set of 125 codeletion vs. 31 not codeletion
images, the proposed network achieves 96.37\% percent F1-score, 97.46\% percent
precision, and 96.34\% percent recall when classifying 1p/19q codeletion and
not codeletion images.",2024-09-29,"Jun Liu, Geng Yuan, Weihao Zeng, Hao Tang, Wenbin Zhang, Xue Lin, XiaoLin Xu, Dong Huang, Yanzhi Wang",http://arxiv.org/pdf/2409.19583v3,cs.LG
DIIT: A Domain-Invariant Information Transfer Method for Industrial Cross-Domain Recommendation,"Cross-Domain Recommendation (CDR) have received widespread attention due to
their ability to utilize rich information across domains. However, most
existing CDR methods assume an ideal static condition that is not practical in
industrial recommendation systems (RS). Therefore, simply applying existing CDR
methods in the industrial RS environment may lead to low effectiveness and
efficiency. To fill this gap, we propose DIIT, an end-to-end Domain-Invariant
Information Transfer method for industrial cross-domain recommendation.
Specifically, We first simulate the industrial RS environment that maintains
respective models in multiple domains, each of them is trained in the
incremental mode. Then, for improving the effectiveness, we design two
extractors to fully extract domain-invariant information from the latest source
domain models at the domain level and the representation level respectively.
Finally, for improving the efficiency, we design a migrator to transfer the
extracted information to the latest target domain model, which only need the
target domain model for inference. Experiments conducted on one production
dataset and two public datasets verify the effectiveness and efficiency of
DIIT.",2024-09-29,"Heyuan Huang, Xingyu Lou, Chaochao Chen, Pengxiang Cheng, Yue Xin, Chengwei He, Xiang Liu, Jun Wang",http://arxiv.org/pdf/2410.10835v1,cs.LG
STTM: A New Approach Based Spatial-Temporal Transformer And Memory Network For Real-time Pressure Signal In On-demand Food Delivery,"On-demand Food Delivery (OFD) services have become very common around the
world. For example, on the Ele.me platform, users place more than 15 million
food orders every day. Predicting the Real-time Pressure Signal (RPS) is
crucial for OFD services, as it is primarily used to measure the current status
of pressure on the logistics system. When RPS rises, the pressure increases,
and the platform needs to quickly take measures to prevent the logistics system
from being overloaded. Usually, the average delivery time for all orders within
a business district is used to represent RPS. Existing research on OFD services
primarily focuses on predicting the delivery time of orders, while relatively
less attention has been given to the study of the RPS. Previous research
directly applies general models such as DeepFM, RNN, and GNN for prediction,
but fails to adequately utilize the unique temporal and spatial characteristics
of OFD services, and faces issues with insufficient sensitivity during sudden
severe weather conditions or peak periods. To address these problems, this
paper proposes a new method based on Spatio-Temporal Transformer and Memory
Network (STTM). Specifically, we use a novel Spatio-Temporal Transformer
structure to learn logistics features across temporal and spatial dimensions
and encode the historical information of a business district and its neighbors,
thereby learning both temporal and spatial information. Additionally, a Memory
Network is employed to increase sensitivity to abnormal events. Experimental
results on the real-world dataset show that STTM significantly outperforms
previous methods in both offline experiments and the online A/B test,
demonstrating the effectiveness of this method.",2024-09-29,"Jiang Wang, Haibin Wei, Xiaowei Xu, Jiacheng Shi, Jian Nie, Longzhi Du, Taixu Jiang",http://arxiv.org/pdf/2410.00057v1,cs.LG
Ads Supply Personalization via Doubly Robust Learning,"Ads supply personalization aims to balance the revenue and user engagement,
two long-term objectives in social media ads, by tailoring the ad quantity and
density. In the industry-scale system, the challenge for ads supply lies in
modeling the counterfactual effects of a conservative supply treatment (e.g., a
small density change) over an extended duration. In this paper, we present a
streamlined framework for personalized ad supply. This framework optimally
utilizes information from data collection policies through the doubly robust
learning. Consequently, it significantly improves the accuracy of long-term
treatment effect estimates. Additionally, its low-complexity design not only
results in computational cost savings compared to existing methods, but also
makes it scalable for billion-scale applications. Through both offline
experiments and online production tests, the framework consistently
demonstrated significant improvements in top-line business metrics over months.
The framework has been fully deployed to live traffic in one of the world's
largest social media platforms.",2024-09-29,"Wei Shi, Chen Fu, Qi Xu, Sanjian Chen, Jizhe Zhang, Qinqin Zhu, Zhigang Hua, Shuang Yang",http://arxiv.org/pdf/2410.12799v1,cs.LG
Unifying back-propagation and forward-forward algorithms through model predictive control,"We introduce a Model Predictive Control (MPC) framework for training deep
neural networks, systematically unifying the Back-Propagation (BP) and
Forward-Forward (FF) algorithms. At the same time, it gives rise to a range of
intermediate training algorithms with varying look-forward horizons, leading to
a performance-efficiency trade-off. We perform a precise analysis of this
trade-off on a deep linear network, where the qualitative conclusions carry
over to general networks. Based on our analysis, we propose a principled method
to choose the optimization horizon based on given objectives and model
specifications. Numerical results on various models and tasks demonstrate the
versatility of our method.",2024-09-29,"Lianhai Ren, Qianxiao Li",http://arxiv.org/pdf/2409.19561v1,cs.LG
Fast-Convergent and Communication-Alleviated Heterogeneous Hierarchical Federated Learning in Autonomous Driving,"Street Scene Semantic Understanding (denoted as TriSU) is a complex task for
autonomous driving (AD). However, inference model trained from data in a
particular geographical region faces poor generalization when applied in other
regions due to inter-city data domain-shift. Hierarchical Federated Learning
(HFL) offers a potential solution for improving TriSU model generalization by
collaborative privacy-preserving training over distributed datasets from
different cities. Unfortunately, it suffers from slow convergence because data
from different cities are with disparate statistical properties. Going beyond
existing HFL methods, we propose a Gaussian heterogeneous HFL algorithm
(FedGau) to address inter-city data heterogeneity so that convergence can be
accelerated. In the proposed FedGau algorithm, both single RGB image and RGB
dataset are modelled as Gaussian distributions for aggregation weight design.
This approach not only differentiates each RGB image by respective statistical
distribution, but also exploits the statistics of dataset from each city in
addition to the conventionally considered data volume. With the proposed
approach, the convergence is accelerated by 35.5\%-40.6\% compared to existing
state-of-the-art (SOTA) HFL methods. On the other hand, to reduce the involved
communication resource, we further introduce a novel performance-aware adaptive
resource scheduling (AdapRS) policy. Unlike the traditional static resource
scheduling policy that exchanges a fixed number of models between two adjacent
aggregations, AdapRS adjusts the number of model aggregation at different
levels of HFL so that unnecessary communications are minimized. Extensive
experiments demonstrate that AdapRS saves 29.65\% communication overhead
compared to conventional static resource scheduling policy while maintaining
almost the same performance.",2024-09-29,"Wei-Bin Kou, Qingfeng Lin, Ming Tang, Rongguang Ye, Shuai Wang, Guangxu Zhu, Yik-Chung Wu",http://arxiv.org/pdf/2409.19560v1,cs.LG
OmniXAS: A Universal Deep-Learning Framework for Materials X-ray Absorption Spectra,"X-ray absorption spectroscopy (XAS) is a powerful characterization technique
for probing the local chemical environment of absorbing atoms. However,
analyzing XAS data presents significant challenges, often requiring extensive,
computationally intensive simulations, as well as significant domain expertise.
These limitations hinder the development of fast, robust XAS analysis pipelines
that are essential in high-throughput studies and for autonomous
experimentation. We address these challenges with OmniXAS, a framework that
contains a suite of transfer learning approaches for XAS prediction, each
contributing to improved accuracy and efficiency, as demonstrated on K-edge
spectra database covering eight 3d transition metals (Ti-Cu). The OmniXAS
framework is built upon three distinct strategies. First, we use M3GNet to
derive latent representations of the local chemical environment of absorption
sites as input for XAS prediction, achieving up to order-of-magnitude
improvements over conventional featurization techniques. Second, we employ a
hierarchical transfer learning strategy, training a universal multi-task model
across elements before fine-tuning for element-specific predictions. Models
based on this cascaded approach after element-wise fine-tuning outperform
element-specific models by up to 69%. Third, we implement cross-fidelity
transfer learning, adapting a universal model to predict spectra generated by
simulation of a different fidelity with a higher computational cost. This
approach improves prediction accuracy by up to 11% over models trained on the
target fidelity alone. Our approach boosts the throughput of XAS modeling by
orders of magnitude versus first-principles simulations and is extendable to
XAS prediction for a broader range of elements. This transfer learning
framework is generalizable to enhance deep-learning models that target other
properties in materials research.",2024-09-29,"Shubha R. Kharel, Fanchen Meng, Xiaohui Qu, Matthew R. Carbone, Deyu Lu",http://arxiv.org/pdf/2409.19552v3,cs.LG
Focus On What Matters: Separated Models For Visual-Based RL Generalization,"A primary challenge for visual-based Reinforcement Learning (RL) is to
generalize effectively across unseen environments. Although previous studies
have explored different auxiliary tasks to enhance generalization, few adopt
image reconstruction due to concerns about exacerbating overfitting to
task-irrelevant features during training. Perceiving the pre-eminence of image
reconstruction in representation learning, we propose SMG (Separated Models for
Generalization), a novel approach that exploits image reconstruction for
generalization. SMG introduces two model branches to extract task-relevant and
task-irrelevant representations separately from visual observations via
cooperatively reconstruction. Built upon this architecture, we further
emphasize the importance of task-relevant features for generalization.
Specifically, SMG incorporates two additional consistency losses to guide the
agent's focus toward task-relevant areas across different scenarios, thereby
achieving free from overfitting. Extensive experiments in DMC demonstrate the
SOTA performance of SMG in generalization, particularly excelling in
video-background settings. Evaluations on robotic manipulation tasks further
confirm the robustness of SMG in real-world applications.",2024-09-29,"Di Zhang, Bowen Lv, Hai Zhang, Feifan Yang, Junqiao Zhao, Hang Yu, Chang Huang, Hongtu Zhou, Chen Ye, Changjun Jiang",http://arxiv.org/pdf/2410.10834v1,cs.LG
Tailed Low-Rank Matrix Factorization for Similarity Matrix Completion,"Similarity matrix serves as a fundamental tool at the core of numerous
downstream machine-learning tasks. However, missing data is inevitable and
often results in an inaccurate similarity matrix. To address this issue,
Similarity Matrix Completion (SMC) methods have been proposed, but they suffer
from high computation complexity due to the Singular Value Decomposition (SVD)
operation. To reduce the computation complexity, Matrix Factorization (MF)
techniques are more explicit and frequently applied to provide a low-rank
solution, but the exact low-rank optimal solution can not be guaranteed since
it suffers from a non-convex structure. In this paper, we introduce a novel SMC
framework that offers a more reliable and efficient solution. Specifically,
beyond simply utilizing the unique Positive Semi-definiteness (PSD) property to
guide the completion process, our approach further complements a carefully
designed rank-minimization regularizer, aiming to achieve an optimal and
low-rank solution. Based on the key insights that the underlying PSD property
and Low-Rank property improve the SMC performance, we present two novel,
scalable, and effective algorithms, SMCNN and SMCNmF, which investigate the PSD
property to guide the estimation process and incorporate nonconvex low-rank
regularizer to ensure the low-rank solution. Theoretical analysis ensures
better estimation performance and convergence speed. Empirical results on
real-world datasets demonstrate the superiority and efficiency of our proposed
methods compared to various baseline methods.",2024-09-29,"Changyi Ma, Runsheng Yu, Xiao Chen, Youzhi Zhang",http://arxiv.org/pdf/2409.19550v1,cs.LG
Asymptotic and Finite Sample Analysis of Nonexpansive Stochastic Approximations with Markovian Noise,"Stochastic approximation is an important class of algorithms, and a large
body of previous analysis focuses on stochastic approximations driven by
contractive operators, which is not applicable in some important reinforcement
learning settings. This work instead investigates stochastic approximations
with merely nonexpansive operators. In particular, we study nonexpansive
stochastic approximations with Markovian noise, providing both asymptotic and
finite sample analysis. Key to our analysis are a few novel bounds of noise
terms resulting from the Poisson equation. As an application, we prove, for the
first time, that the classical tabular average reward temporal difference
learning converges to a sample path dependent fixed point.",2024-09-29,"Ethan Blaser, Shangtong Zhang",http://arxiv.org/pdf/2409.19546v4,cs.LG
Convergence-aware Clustered Federated Graph Learning Framework for Collaborative Inter-company Labor Market Forecasting,"Labor market forecasting on talent demand and supply is essential for
business management and economic development. With accurate and timely
forecasts, employers can adapt their recruitment strategies to align with the
evolving labor market, and employees can have proactive career path planning
according to future demand and supply. However, previous studies ignore the
interconnection between demand-supply sequences among different companies and
positions for predicting variations. Moreover, companies are reluctant to share
their private human resource data for global labor market analysis due to
concerns over jeopardizing competitive advantage, security threats, and
potential ethical or legal violations. To this end, in this paper, we formulate
the Federated Labor Market Forecasting (FedLMF) problem and propose a
Meta-personalized Convergence-aware Clustered Federated Learning (MPCAC-FL)
framework to provide accurate and timely collaborative talent demand and supply
prediction in a privacy-preserving way. First, we design a graph-based
sequential model to capture the inherent correlation between demand and supply
sequences and company-position pairs. Second, we adopt meta-learning techniques
to learn effective initial model parameters that can be shared across
companies, allowing personalized models to be optimized for forecasting
company-specific demand and supply, even when companies have heterogeneous
data. Third, we devise a Convergence-aware Clustering algorithm to dynamically
divide companies into groups according to model similarity and apply federated
aggregation in each group. The heterogeneity can be alleviated for more stable
convergence and better performance. Extensive experiments demonstrate that
MPCAC-FL outperforms compared baselines on three real-world datasets and
achieves over 97% of the state-of-the-art model, i.e., DH-GEM, without exposing
private company data.",2024-09-29,"Zhuoning Guo, Hao Liu, Le Zhang, Qi Zhang, Hengshu Zhu, Hui Xiong",http://arxiv.org/pdf/2409.19545v1,cs.LG
An evolutionary approach for discovering non-Gaussian stochastic dynamical systems based on nonlocal Kramers-Moyal formulas,"Discovering explicit governing equations of stochastic dynamical systems with
both (Gaussian) Brownian noise and (non-Gaussian) L\'evy noise from data is
chanllenging due to possible intricate functional forms and the inherent
complexity of L\'evy motion. This present research endeavors to develop an
evolutionary symbol sparse regression (ESSR) approach to extract non-Gaussian
stochastic dynamical systems from sample path data, based on nonlocal
Kramers-Moyal formulas, genetic programming, and sparse regression. More
specifically, the genetic programming is employed to generate a diverse array
of candidate functions, the sparse regression technique aims at learning the
coefficients associated with these candidates, and the nonlocal Kramers-Moyal
formulas serve as the foundation for constructing the fitness measure in
genetic programming and the loss function in sparse regression. The efficacy
and capabilities of this approach are showcased through its application to
several illustrative models. This approach stands out as a potent instrument
for deciphering non-Gaussian stochastic dynamics from available datasets,
indicating a wide range of applications across different fields.",2024-09-29,"Yang Li, Shengyuan Xu, Jinqiao Duan",http://arxiv.org/pdf/2409.19534v1,cs.LG
Video DataFlywheel: Resolving the Impossible Data Trinity in Video-Language Understanding,"Recently, video-language understanding has achieved great success through
large-scale pre-training. However, data scarcity remains a prevailing
challenge. This study quantitatively reveals an ""impossible trinity"" among data
quantity, diversity, and quality in pre-training datasets. Recent efforts seek
to refine large-scale, diverse ASR datasets compromised by low quality through
synthetic annotations. These methods successfully leverage useful information
in multimodal video content (frames, tags, ASR transcripts, etc.) to refine the
original annotations. Nevertheless, they struggle to mitigate noise within
synthetic annotations and lack scalability as the dataset size expands. To
address these issues, we introduce the Video DataFlywheel framework, which
iteratively refines video annotations with improved noise control methods. For
iterative refinement, we first leverage a video-language model to generate
synthetic annotations, resulting in a refined dataset. Then, we pre-train on it
and fine-tune on human refinement examples for a stronger model. These
processes are repeated for continuous improvement. For noise control, we
present AdaTaiLr, a novel noise control method that requires weaker assumptions
on noise distribution, thereby proving more effective in large datasets with
theoretical guarantees. The combination of iterative refinement and AdaTaiLr
can achieve better scalability in video-language understanding. Extensive
experiments show that our framework outperforms existing data refinement
baselines, delivering a 3% performance boost and improving dataset quality with
minimal diversity loss. Furthermore, our refined dataset facilitates
significant improvements in various video-language understanding tasks,
including video question answering and text-video retrieval.",2024-09-29,"Xiao Wang, Jianlong Wu, Zijia Lin, Fuzheng Zhang, Di Zhang, Liqiang Nie",http://arxiv.org/pdf/2409.19532v1,cs.LG
Understanding Clinical Decision-Making in Traditional East Asian Medicine through Dimensionality Reduction: An Empirical Investigation,"This study examines the clinical decision-making processes in Traditional
East Asian Medicine (TEAM) by reinterpreting pattern identification (PI)
through the lens of dimensionality reduction. Focusing on the Eight Principle
Pattern Identification (EPPI) system and utilizing empirical data from the
Shang-Han-Lun, we explore the necessity and significance of prioritizing the
Exterior-Interior pattern in diagnosis and treatment selection. We test three
hypotheses: whether the Ext-Int pattern contains the most information about
patient symptoms, represents the most abstract and generalizable symptom
information, and facilitates the selection of appropriate herbal prescriptions.
Employing quantitative measures such as the abstraction index,
cross-conditional generalization performance, and decision tree regression, our
results demonstrate that the Exterior-Interior pattern represents the most
abstract and generalizable symptom information, contributing to the efficient
mapping between symptom and herbal prescription spaces. This research provides
an objective framework for understanding the cognitive processes underlying
TEAM, bridging traditional medical practices with modern computational
approaches. The findings offer insights into the development of AI-driven
diagnostic tools in TEAM and conventional medicine, with the potential to
advance clinical practice, education, and research.",2024-09-29,"Hyojin Bae, Bongsu Kang, Chang-Eop Kim",http://arxiv.org/pdf/2409.19531v1,cs.LG
Efficient Backdoor Defense in Multimodal Contrastive Learning: A Token-Level Unlearning Method for Mitigating Threats,"Multimodal contrastive learning uses various data modalities to create
high-quality features, but its reliance on extensive data sources on the
Internet makes it vulnerable to backdoor attacks. These attacks insert
malicious behaviors during training, which are activated by specific triggers
during inference, posing significant security risks. Despite existing
countermeasures through fine-tuning that reduce the malicious impacts of such
attacks, these defenses frequently necessitate extensive training time and
degrade clean accuracy. In this study, we propose an efficient defense
mechanism against backdoor threats using a concept known as machine unlearning.
This entails strategically creating a small set of poisoned samples to aid the
model's rapid unlearning of backdoor vulnerabilities, known as Unlearn Backdoor
Threats (UBT). We specifically use overfit training to improve backdoor
shortcuts and accurately detect suspicious samples in the potential poisoning
data set. Then, we select fewer unlearned samples from suspicious samples for
rapid forgetting in order to eliminate the backdoor effect and thus improve
backdoor defense efficiency. In the backdoor unlearning process, we present a
novel token-based portion unlearning training regime. This technique focuses on
the model's compromised elements, dissociating backdoor correlations while
maintaining the model's overall integrity. Extensive experimental results show
that our method effectively defends against various backdoor attack methods in
the CLIP model. Compared to SoTA backdoor defense methods, UBT achieves the
lowest attack success rate while maintaining a high clean accuracy of the model
(attack success rate decreases by 19% compared to SOTA, while clean accuracy
increases by 2.57%).",2024-09-29,"Kuanrong Liu, Siyuan Liang, Jiawei Liang, Pengwen Dai, Xiaochun Cao",http://arxiv.org/pdf/2409.19526v1,cs.LG
GenTel-Safe: A Unified Benchmark and Shielding Framework for Defending Against Prompt Injection Attacks,"Large Language Models (LLMs) like GPT-4, LLaMA, and Qwen have demonstrated
remarkable success across a wide range of applications. However, these models
remain inherently vulnerable to prompt injection attacks, which can bypass
existing safety mechanisms, highlighting the urgent need for more robust attack
detection methods and comprehensive evaluation benchmarks. To address these
challenges, we introduce GenTel-Safe, a unified framework that includes a novel
prompt injection attack detection method, GenTel-Shield, along with a
comprehensive evaluation benchmark, GenTel-Bench, which compromises 84812
prompt injection attacks, spanning 3 major categories and 28 security
scenarios. To prove the effectiveness of GenTel-Shield, we evaluate it together
with vanilla safety guardrails against the GenTel-Bench dataset. Empirically,
GenTel-Shield can achieve state-of-the-art attack detection success rates,
which reveals the critical weakness of existing safeguarding techniques against
harmful prompts. For reproducibility, we have made the code and benchmarking
dataset available on the project page at
https://gentellab.github.io/gentel-safe.github.io/.",2024-09-29,"Rongchang Li, Minjie Chen, Chang Hu, Han Chen, Wenpeng Xing, Meng Han",http://arxiv.org/pdf/2409.19521v1,cs.LG
"Finetuning YOLOv9 for Vehicle Detection: Deep Learning for Intelligent Transportation Systems in Dhaka, Bangladesh","Rapid urbanization in megacities around the world, like Dhaka, has caused
numerous transportation challenges that need to be addressed. Emerging
technologies of deep learning and artificial intelligence can help us solve
these problems to move towards Intelligent Transportation Systems (ITS) in the
city. The government of Bangladesh recognizes the integration of ITS to ensure
smart mobility as a vital step towards the development plan ""Smart Bangladesh
Vision 2041"", but faces challenges in understanding ITS, its effects, and
directions to implement. A vehicle detection system can pave the way to
understanding traffic congestion, finding mobility patterns, and ensuring
traffic surveillance. So, this paper proposes a fine-tuned object detector, the
YOLOv9 model to detect native vehicles trained on a Bangladesh-based dataset.
Results show that the fine-tuned YOLOv9 model achieved a mean Average Precision
(mAP) of 0.934 at the Intersection over Union (IoU) threshold of 0.5, achieving
state-of-the-art performance over past studies on Bangladesh-based datasets,
shown through a comparison. Later, by suggesting the model to be deployed on
CCTVs (closed circuit television) on the roads, a conceptual technique is
proposed to process the vehicle detection model output data in a graph
structure creating a vehicle detection system in the city. Finally,
applications of such vehicle detection system are discussed showing a framework
on how it can solve further ITS research questions, to provide a rationale for
policymakers to implement the proposed vehicle detection system in the city.",2024-09-29,Shahriar Ahmad Fahim,http://arxiv.org/pdf/2410.08230v2,cs.LG
KODA: A Data-Driven Recursive Model for Time Series Forecasting and Data Assimilation using Koopman Operators,"Approaches based on Koopman operators have shown great promise in forecasting
time series data generated by complex nonlinear dynamical systems (NLDS).
Although such approaches are able to capture the latent state representation of
a NLDS, they still face difficulty in long term forecasting when applied to
real world data. Specifically many real-world NLDS exhibit time-varying
behavior, leading to nonstationarity that is hard to capture with such models.
Furthermore they lack a systematic data-driven approach to perform data
assimilation, that is, exploiting noisy measurements on the fly in the
forecasting task. To alleviate the above issues, we propose a Koopman
operator-based approach (named KODA - Koopman Operator with Data Assimilation)
that integrates forecasting and data assimilation in NLDS. In particular we use
a Fourier domain filter to disentangle the data into a physical component whose
dynamics can be accurately represented by a Koopman operator, and residual
dynamics that represents the local or time varying behavior that are captured
by a flexible and learnable recursive model. We carefully design an
architecture and training criterion that ensures this decomposition lead to
stable and long-term forecasts. Moreover, we introduce a course correction
strategy to perform data assimilation with new measurements at inference time.
The proposed approach is completely data-driven and can be learned end-to-end.
Through extensive experimental comparisons we show that KODA outperforms
existing state of the art methods on multiple time series benchmarks such as
electricity, temperature, weather, lorenz 63 and duffing oscillator
demonstrating its superior performance and efficacy along the three tasks a)
forecasting, b) data assimilation and c) state prediction.",2024-09-29,"Ashutosh Singh, Ashish Singh, Tales Imbiriba, Deniz Erdogmus, Ricardo Borsoi",http://arxiv.org/pdf/2409.19518v1,cs.LG
One Node Per User: Node-Level Federated Learning for Graph Neural Networks,"Graph Neural Networks (GNNs) training often necessitates gathering raw user
data on a central server, which raises significant privacy concerns. Federated
learning emerges as a solution, enabling collaborative model training without
users directly sharing their raw data. However, integrating federated learning
with GNNs presents unique challenges, especially when a client represents a
graph node and holds merely a single feature vector. In this paper, we propose
a novel framework for node-level federated graph learning. Specifically, we
decouple the message-passing and feature vector transformation processes of the
first GNN layer, allowing them to be executed separately on the user devices
and the cloud server. Moreover, we introduce a graph Laplacian term based on
the feature vector's latent representation to regulate the user-side model
updates. The experiment results on multiple datasets show that our approach
achieves better performance compared with baselines.",2024-09-29,"Zhidong Gao, Yuanxiong Guo, Yanmin Gong",http://arxiv.org/pdf/2409.19513v1,cs.LG
Online Client Scheduling and Resource Allocation for Efficient Federated Edge Learning,"Federated learning (FL) enables edge devices to collaboratively train a
machine learning model without sharing their raw data. Due to its
privacy-protecting benefits, FL has been deployed in many real-world
applications. However, deploying FL over mobile edge networks with constrained
resources such as power, bandwidth, and computation suffers from high training
latency and low model accuracy, particularly under data and system
heterogeneity. In this paper, we investigate the optimal client scheduling and
resource allocation for FL over mobile edge networks under resource constraints
and uncertainty to minimize the training latency while maintaining the model
accuracy. Specifically, we first analyze the impact of client sampling on model
convergence in FL and formulate a stochastic optimization problem that captures
the trade-off between the running time and model performance under
heterogeneous and uncertain system resources. To solve the formulated problem,
we further develop an online control scheme based on Lyapunov-based
optimization for client sampling and resource allocation without requiring the
knowledge of future dynamics in the FL system. Extensive experimental results
demonstrate that the proposed scheme can improve both the training latency and
resource efficiency compared with the existing schemes.",2024-09-29,"Zhidong Gao, Zhenxiao Zhang, Yu Zhang, Tongnian Wang, Yanmin Gong, Yuanxiong Guo",http://arxiv.org/pdf/2410.10833v1,cs.LG
Heterogeneity-Aware Resource Allocation and Topology Design for Hierarchical Federated Edge Learning,"Federated Learning (FL) provides a privacy-preserving framework for training
machine learning models on mobile edge devices. Traditional FL algorithms,
e.g., FedAvg, impose a heavy communication workload on these devices. To
mitigate this issue, Hierarchical Federated Edge Learning (HFEL) has been
proposed, leveraging edge servers as intermediaries for model aggregation.
Despite its effectiveness, HFEL encounters challenges such as a slow
convergence rate and high resource consumption, particularly in the presence of
system and data heterogeneity. However, existing works are mainly focused on
improving training efficiency for traditional FL, leaving the efficiency of
HFEL largely unexplored. In this paper, we consider a two-tier HFEL system,
where edge devices are connected to edge servers and edge servers are
interconnected through peer-to-peer (P2P) edge backhauls. Our goal is to
enhance the training efficiency of the HFEL system through strategic resource
allocation and topology design. Specifically, we formulate an optimization
problem to minimize the total training latency by allocating the computation
and communication resources, as well as adjusting the P2P connections. To
ensure convergence under dynamic topologies, we analyze the convergence error
bound and introduce a model consensus constraint into the optimization problem.
The proposed problem is then decomposed into several subproblems, enabling us
to alternatively solve it online. Our method facilitates the efficient
implementation of large-scale FL at edge networks under data and system
heterogeneity. Comprehensive experiment evaluation on benchmark datasets
validates the effectiveness of the proposed method, demonstrating significant
reductions in training latency while maintaining the model accuracy compared to
various baselines.",2024-09-29,"Zhidong Gao, Yu Zhang, Yanmin Gong, Yuanxiong Guo",http://arxiv.org/pdf/2409.19509v1,cs.LG
Psychometrics for Hypnopaedia-Aware Machinery via Chaotic Projection of Artificial Mental Imagery,"Neural backdoors represent insidious cybersecurity loopholes that render
learning machinery vulnerable to unauthorised manipulations, potentially
enabling the weaponisation of artificial intelligence with catastrophic
consequences. A backdoor attack involves the clandestine infiltration of a
trigger during the learning process, metaphorically analogous to hypnopaedia,
where ideas are implanted into a subject's subconscious mind under the state of
hypnosis or unconsciousness. When activated by a sensory stimulus, the trigger
evokes conditioned reflex that directs a machine to mount a predetermined
response. In this study, we propose a cybernetic framework for constant
surveillance of backdoors threats, driven by the dynamic nature of
untrustworthy data sources. We develop a self-aware unlearning mechanism to
autonomously detach a machine's behaviour from the backdoor trigger. Through
reverse engineering and statistical inference, we detect deceptive patterns and
estimate the likelihood of backdoor infection. We employ model inversion to
elicit artificial mental imagery, using stochastic processes to disrupt
optimisation pathways and avoid convergent but potentially flawed patterns.
This is followed by hypothesis analysis, which estimates the likelihood of each
potentially malicious pattern being the true trigger and infers the probability
of infection. The primary objective of this study is to maintain a stable state
of equilibrium between knowledge fidelity and backdoor vulnerability.",2024-09-29,"Ching-Chun Chang, Kai Gao, Shuying Xu, Anastasia Kordoni, Christopher Leckie, Isao Echizen",http://arxiv.org/pdf/2410.05284v1,cs.LG
Survey of Security and Data Attacks on Machine Unlearning In Financial and E-Commerce,"This paper surveys the landscape of security and data attacks on machine
unlearning, with a focus on financial and e-commerce applications. We discuss
key privacy threats such as Membership Inference Attacks and Data
Reconstruction Attacks, where adversaries attempt to infer or reconstruct data
that should have been removed. In addition, we explore security attacks
including Machine Unlearning Data Poisoning, Unlearning Request Attacks, and
Machine Unlearning Jailbreak Attacks, which target the underlying mechanisms of
unlearning to manipulate or corrupt the model. To mitigate these risks, various
defense strategies are examined, including differential privacy, robust
cryptographic guarantees, and Zero-Knowledge Proofs (ZKPs), offering verifiable
and tamper-proof unlearning mechanisms. These approaches are essential for
safeguarding data integrity and privacy in high-stakes financial and e-commerce
contexts, where compromised models can lead to fraud, data leaks, and
reputational damage. This survey highlights the need for continued research and
innovation in secure machine unlearning, as well as the importance of
developing strong defenses against evolving attack vectors.",2024-09-29,Carl E. J. Brodzinski,http://arxiv.org/pdf/2410.00055v1,cs.LG
HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare Conversations,"Effective patient care in digital healthcare requires large language models
(LLMs) that not only answer questions but also actively gather critical
information through well-crafted inquiries. This paper introduces HealthQ, a
novel framework for evaluating the questioning capabilities of LLM healthcare
chains. By implementing advanced LLM chains, including Retrieval-Augmented
Generation (RAG), Chain of Thought (CoT), and reflective chains, HealthQ
assesses how effectively these chains elicit comprehensive and relevant patient
information. To achieve this, we integrate an LLM judge to evaluate generated
questions across metrics such as specificity, relevance, and usefulness, while
aligning these evaluations with traditional Natural Language Processing (NLP)
metrics like ROUGE and Named Entity Recognition (NER)-based set comparisons. We
validate HealthQ using two custom datasets constructed from public medical
datasets, ChatDoctor and MTS-Dialog, and demonstrate its robustness across
multiple LLM judge models, including GPT-3.5, GPT-4, and Claude. Our
contributions are threefold: we present the first systematic framework for
assessing questioning capabilities in healthcare conversations, establish a
model-agnostic evaluation methodology, and provide empirical evidence linking
high-quality questions to improved patient information elicitation.",2024-09-28,"Ziyu Wang, Hao Li, Di Huang, Hye-Sung Kim, Chae-Won Shin, Amir M. Rahmani",http://arxiv.org/pdf/2409.19487v4,cs.LG
Spatial Reasoning and Planning for Deep Embodied Agents,"Humans can perform complex tasks with long-term objectives by planning,
reasoning, and forecasting outcomes of actions. For embodied agents to achieve
similar capabilities, they must gain knowledge of the environment transferable
to novel scenarios with a limited budget of additional trial and error.
Learning-based approaches, such as deep RL, can discover and take advantage of
inherent regularities and characteristics of the application domain from data,
and continuously improve their performances, however at a cost of large amounts
of training data. This thesis explores the development of data-driven
techniques for spatial reasoning and planning tasks, focusing on enhancing
learning efficiency, interpretability, and transferability across novel
scenarios. Four key contributions are made. 1) CALVIN, a differential planner
that learns interpretable models of the world for long-term planning. It
successfully navigated partially observable 3D environments, such as mazes and
indoor rooms, by learning the rewards and state transitions from expert
demonstrations. 2) SOAP, an RL algorithm that discovers options unsupervised
for long-horizon tasks. Options segment a task into subtasks and enable
consistent execution of the subtask. SOAP showed robust performances on
history-conditional corridor tasks as well as classical benchmarks such as
Atari. 3) LangProp, a code optimisation framework using LLMs to solve embodied
agent problems that require reasoning by treating code as learnable policies.
The framework successfully generated interpretable code with comparable or
superior performance to human-written experts in the CARLA autonomous driving
benchmark. 4) Voggite, an embodied agent with a vision-to-action transformer
backend that solves complex tasks in Minecraft. It achieved third place in the
MineRL BASALT Competition by identifying action triggers to segment tasks into
multiple stages.",2024-09-28,Shu Ishida,http://arxiv.org/pdf/2409.19479v1,cs.LG
Hedging and Approximate Truthfulness in Traditional Forecasting Competitions,"In forecasting competitions, the traditional mechanism scores the predictions
of each contestant against the outcome of each event, and the contestant with
the highest total score wins. While it is well-known that this traditional
mechanism can suffer from incentive issues, it is folklore that contestants
will still be roughly truthful as the number of events grows. Yet thus far the
literature lacks a formal analysis of this traditional mechanism. This paper
gives the first such analysis. We first demonstrate that the ''long-run
truthfulness'' folklore is false: even for arbitrary numbers of events, the
best forecaster can have an incentive to hedge, reporting more moderate beliefs
to increase their win probability. On the positive side, however, we show that
two contestants will be approximately truthful when they have sufficient
uncertainty over the relative quality of their opponent and the outcomes of the
events, a case which may arise in practice.",2024-09-28,"Mary Monroe, Anish Thilagar, Melody Hsu, Rafael Frongillo",http://arxiv.org/pdf/2409.19477v2,cs.LG
Evaluating Financial Relational Graphs: Interpretation Before Prediction,"Accurate and robust stock trend forecasting has been a crucial and
challenging task, as stock price changes are influenced by multiple factors.
Graph neural network-based methods have recently achieved remarkable success in
this domain by constructing stock relationship graphs that reflect internal
factors and relationships between stocks. However, most of these methods rely
on predefined factors to construct static stock relationship graphs due to the
lack of suitable datasets, failing to capture the dynamic changes in stock
relationships. Moreover, the evaluation of relationship graphs in these methods
is often tied to the performance of neural network models on downstream tasks,
leading to confusion and imprecision. To address these issues, we introduce the
SPNews dataset, collected based on S\&P 500 Index stocks, to facilitate the
construction of dynamic relationship graphs. Furthermore, we propose a novel
set of financial relationship graph evaluation methods that are independent of
downstream tasks. By using the relationship graph to explain historical
financial phenomena, we assess its validity before constructing a graph neural
network, ensuring the graph's effectiveness in capturing relevant financial
relationships. Experimental results demonstrate that our evaluation methods can
effectively differentiate between various financial relationship graphs,
yielding more interpretable results compared to traditional approaches. We make
our source code publicly available on GitHub to promote reproducibility and
further research in this area.",2024-09-28,"Yingjie Niu, Lanxin Lu, Rian Dolphin, Valerio Poti, Ruihai Dong",http://arxiv.org/pdf/2410.07216v1,cs.LG
Towards Croppable Implicit Neural Representations,"Implicit Neural Representations (INRs) have peaked interest in recent years
due to their ability to encode natural signals using neural networks. While
INRs allow for useful applications such as interpolating new coordinates and
signal compression, their black-box nature makes it difficult to modify them
post-training. In this paper we explore the idea of editable INRs, and
specifically focus on the widely used cropping operation. To this end, we
present Local-Global SIRENs -- a novel INR architecture that supports cropping
by design. Local-Global SIRENs are based on combining local and global feature
extraction for signal encoding. What makes their design unique is the ability
to effortlessly remove specific portions of an encoded signal, with a
proportional weight decrease. This is achieved by eliminating the corresponding
weights from the network, without the need for retraining. We further show how
this architecture can be used to support the straightforward extension of
previously encoded signals. Beyond signal editing, we examine how the
Local-Global approach can accelerate training, enhance encoding of various
signals, improve downstream performance, and be applied to modern INRs such as
INCODE, highlighting its potential and flexibility. Code is available at
https://github.com/maorash/Local-Global-INRs.",2024-09-28,"Maor Ashkenazi, Eran Treister",http://arxiv.org/pdf/2409.19472v2,cs.LG
Transferable Unsupervised Outlier Detection Framework for Human Semantic Trajectories,"Semantic trajectories, which enrich spatial-temporal data with textual
information such as trip purposes or location activities, are key for
identifying outlier behaviors critical to healthcare, social security, and
urban planning. Traditional outlier detection relies on heuristic rules, which
requires domain knowledge and limits its ability to identify unseen outliers.
Besides, there lacks a comprehensive approach that can jointly consider
multi-modal data across spatial, temporal, and textual dimensions. Addressing
the need for a domain-agnostic model, we propose the Transferable Outlier
Detection for Human Semantic Trajectories (TOD4Traj) framework.TOD4Traj first
introduces a modality feature unification module to align diverse data feature
representations, enabling the integration of multi-modal information and
enhancing transferability across different datasets. A contrastive learning
module is further pro-posed for identifying regular mobility patterns both
temporally and across populations, allowing for a joint detection of outliers
based on individual consistency and group majority patterns. Our experimental
results have shown TOD4Traj's superior performance over existing models,
demonstrating its effectiveness and adaptability in detecting human trajectory
outliers across various datasets.",2024-09-28,"Zheng Zhang, Hossein Amiri, Dazhou Yu, Yuntong Hu, Liang Zhao, Andreas Zufle",http://arxiv.org/pdf/2410.00054v2,cs.LG
Accelerating Malware Classification: A Vision Transformer Solution,"The escalating frequency and scale of recent malware attacks underscore the
urgent need for swift and precise malware classification in the ever-evolving
cybersecurity landscape. Key challenges include accurately categorizing closely
related malware families. To tackle this evolving threat landscape, this paper
proposes a novel architecture LeViT-MC which produces state-of-the-art results
in malware detection and classification. LeViT-MC leverages a vision
transformer-based architecture, an image-based visualization approach, and
advanced transfer learning techniques. Experimental results on multi-class
malware classification using the MaleVis dataset indicate LeViT-MC's
significant advantage over existing models. This study underscores the critical
importance of combining image-based and transfer learning techniques, with
vision transformers at the forefront of the ongoing battle against evolving
cyber threats. We propose a novel architecture LeViT-MC which not only achieves
state of the art results on image classification but is also more time
efficient.",2024-09-28,"Shrey Bavishi, Shrey Modi",http://arxiv.org/pdf/2409.19461v1,cs.LG
On the universality of neural encodings in CNNs,"We explore the universality of neural encodings in convolutional neural
networks trained on image classification tasks. We develop a procedure to
directly compare the learned weights rather than their representations. It is
based on a factorization of spatial and channel dimensions and measures the
similarity of aligned weight covariances. We show that, for a range of layers
of VGG-type networks, the learned eigenvectors appear to be universal across
different natural image datasets. Our results suggest the existence of a
universal neural encoding for natural images. They explain, at a more
fundamental level, the success of transfer learning. Our work shows that,
instead of aiming at maximizing the performance of neural networks, one can
alternatively attempt to maximize the universality of the learned encoding, in
order to build a principled foundation model.",2024-09-28,"Florentin Guth, Brice Ménard",http://arxiv.org/pdf/2409.19460v1,cs.LG
Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach,"We study the problem of fine-tuning a language model (LM) for a target task
by optimally using the information from $n$ auxiliary tasks. This problem has
broad applications in NLP, such as targeted instruction tuning and data
selection in chain-of-thought fine-tuning. The key challenge of this problem is
that not all auxiliary tasks are useful to improve the performance of the
target task. Thus, choosing the right subset of auxiliary tasks is crucial.
Conventional subset selection methods, such as forward and backward stepwise
selection, are unsuitable for LM fine-tuning because they require repeated
training on subsets of auxiliary tasks. This paper introduces a new algorithm
to estimate model fine-tuning performances without repeated training. Our
algorithm first performs multitask training using the data of all the tasks to
obtain a meta initialization. Then, we approximate the model fine-tuning loss
of a subset using functional values and gradients from the meta initialization.
Empirically, we find that this gradient-based approximation holds with
remarkable accuracy for twelve transformer-based LMs. Thus, we can now estimate
fine-tuning performances on CPUs within a few seconds. Finally, we fine-tune
the pretrained base model for once on the selected subset of tasks. We conduct
extensive experiments to validate this approach, delivering a speedup of
$30\times$ over conventional subset selection while incurring only $1\%$ error
of the true fine-tuning performances. In downstream evaluations involving both
instruction tuning and chain-of-thought fine-tuning, this loss-based selection
approach improves over prior gradient or representation similarity-based
methods for subset selection by up to $3.8\%$.",2024-09-28,"Dongyue Li, Ziniu Zhang, Lu Wang, Hongyang R. Zhang",http://arxiv.org/pdf/2409.19458v2,cs.LG
HTML-LSTM: Information Extraction from HTML Tables in Web Pages using Tree-Structured LSTM,"In this paper, we propose a novel method for extracting information from HTML
tables with similar contents but with a different structure. We aim to
integrate multiple HTML tables into a single table for retrieval of information
containing in various Web pages. The method is designed by extending
tree-structured LSTM, the neural network for tree-structured data, in order to
extract information that is both linguistic and structural information of HTML
data. We evaluate the proposed method through experiments using real data
published on the WWW.",2024-09-28,"Kazuki Kawamura, Akihiro Yamamoto",http://arxiv.org/pdf/2409.19445v1,cs.LG
Strongly-polynomial time and validation analysis of policy gradient methods,"This paper proposes a novel termination criterion, termed the advantage gap
function, for finite state and action Markov decision processes (MDP) and
reinforcement learning (RL). By incorporating this advantage gap function into
the design of step size rules and deriving a new linear rate of convergence
that is independent of the stationary state distribution of the optimal policy,
we demonstrate that policy gradient methods can solve MDPs in
strongly-polynomial time. To the best of our knowledge, this is the first time
that such strong convergence properties have been established for policy
gradient methods. Moreover, in the stochastic setting, where only stochastic
estimates of policy gradients are available, we show that the advantage gap
function provides close approximations of the optimality gap for each
individual state and exhibits a sublinear rate of convergence at every state.
The advantage gap function can be easily estimated in the stochastic case, and
when coupled with easily computable upper bounds on policy values, they provide
a convenient way to validate the solutions generated by policy gradient
methods. Therefore, our developments offer a principled and computable measure
of optimality for RL, whereas current practice tends to rely on
algorithm-to-algorithm or baselines comparisons with no certificate of
optimality.",2024-09-28,"Caleb Ju, Guanghui Lan",http://arxiv.org/pdf/2409.19437v4,cs.LG
Simulation-based inference with the Python Package sbijax,"Neural simulation-based inference (SBI) describes an emerging family of
methods for Bayesian inference with intractable likelihood functions that use
neural networks as surrogate models. Here we introduce sbijax, a Python package
that implements a wide variety of state-of-the-art methods in neural
simulation-based inference using a user-friendly programming interface. sbijax
offers high-level functionality to quickly construct SBI estimators, and
compute and visualize posterior distributions with only a few lines of code. In
addition, the package provides functionality for conventional approximate
Bayesian computation, to compute model diagnostics, and to automatically
estimate summary statistics. By virtue of being entirely written in JAX, sbijax
is extremely computationally efficient, allowing rapid training of neural
networks and executing code automatically in parallel on both CPU and GPU.",2024-09-28,"Simon Dirmeier, Simone Ulzega, Antonietta Mira, Carlo Albert",http://arxiv.org/pdf/2409.19435v1,cs.LG
Energy-Efficient Computation with DVFS using Deep Reinforcement Learning for Multi-Task Systems in Edge Computing,"Finding an optimal energy-efficient policy that is adaptable to underlying
edge devices while meeting deadlines for tasks has always been challenging.
This research studies generalized systems with multi-task, multi-deadline
scenarios with reinforcement learning-based DVFS for energy saving for periodic
soft real-time applications on edge devices. This work addresses the limitation
of previous work that models a periodic system as a single task and
single-deadline scenario, which is too simplified to cope with complex
situations. The method encodes time series data in the Linux kernel into
information that is easy to interpret for reinforcement learning, allowing the
system to generate DVFS policies to adapt system patterns based on the general
workload. For encoding, we present two different methods for comparison. Both
methods use only one performance counter: system utilization, and the kernel
only needs minimal information from the userspace. Our method is implemented on
Jetson Nano Board (2GB) and is tested with three fixed multitask workloads,
which are three, five, and eight tasks in the workload, respectively. For
randomness and generalization, we also designed a random workload generator to
build different multitask workloads to test. Based on the test results, our
method could save 3%-10% power compared to Linux built-in governors.",2024-09-28,"Xinyi Li, Ti Zhou, Haoyu Wang, Man Lin",http://arxiv.org/pdf/2409.19434v3,cs.LG
RMLR: Extending Multinomial Logistic Regression into General Geometries,"Riemannian neural networks, which extend deep learning techniques to
Riemannian spaces, have gained significant attention in machine learning. To
better classify the manifold-valued features, researchers have started
extending Euclidean multinomial logistic regression (MLR) into Riemannian
manifolds. However, existing approaches suffer from limited applicability due
to their strong reliance on specific geometric properties. This paper proposes
a framework for designing Riemannian MLR over general geometries, referred to
as RMLR. Our framework only requires minimal geometric properties, thus
exhibiting broad applicability and enabling its use with a wide range of
geometries. Specifically, we showcase our framework on the Symmetric Positive
Definite (SPD) manifold and special orthogonal group, i.e., the set of rotation
matrices. On the SPD manifold, we develop five families of SPD MLRs under five
types of power-deformed metrics. On rotation matrices we propose Lie MLR based
on the popular bi-invariant metric. Extensive experiments on different
Riemannian backbone networks validate the effectiveness of our framework.",2024-09-28,"Ziheng Chen, Yue Song, Rui Wang, Xiaojun Wu, Nicu Sebe",http://arxiv.org/pdf/2409.19433v2,cs.LG
MicroFlow: An Efficient Rust-Based Inference Engine for TinyML,"In recent years, there has been a significant interest in developing machine
learning algorithms on embedded systems. This is particularly relevant for bare
metal devices in Internet of Things, Robotics, and Industrial applications that
face limited memory, processing power, and storage, and which require extreme
robustness. To address these constraints, we present MicroFlow, an open-source
TinyML framework for the deployment of Neural Networks (NNs) on embedded
systems using the Rust programming language. The compiler-based inference
engine of MicroFlow, coupled with Rust's memory safety, makes it suitable for
TinyML applications in critical environments. The proposed framework enables
the successful deployment of NNs on highly resource-constrained devices,
including bare-metal 8-bit microcontrollers with only 2kB of RAM. Furthermore,
MicroFlow is able to use less Flash and RAM memory than other state-of-the-art
solutions for deploying NN reference models (i.e. wake-word and person
detection), achieving equally accurate but faster inference compared to
existing engines on medium-size NNs, and similar performance on bigger ones.
The experimental results prove the efficiency and suitability of MicroFlow for
the deployment of TinyML models in critical environments where resources are
particularly limited.",2024-09-28,"Matteo Carnelos, Francesco Pasti, Nicola Bellotto",http://arxiv.org/pdf/2409.19432v3,cs.LG
Generalization Error of the Tilted Empirical Risk,"The generalization error (risk) of a supervised statistical learning
algorithm quantifies its prediction ability on previously unseen data. Inspired
by exponential tilting, Li et al. (2021) proposed the tilted empirical risk as
a non-linear risk metric for machine learning applications such as
classification and regression problems. In this work, we examine the
generalization error of the tilted empirical risk. In particular, we provide
uniform and information-theoretic bounds on the tilted generalization error,
defined as the difference between the population risk and the tilted empirical
risk, with a convergence rate of $O(1/\sqrt{n})$ where $n$ is the number of
training samples. Furthermore, we study the solution to the KL-regularized
expected tilted empirical risk minimization problem and derive an upper bound
on the expected tilted generalization error with a convergence rate of
$O(1/n)$.",2024-09-28,"Gholamali Aminian, Amir R. Asadi, Tian Li, Ahmad Beirami, Gesine Reinert, Samuel N. Cohen",http://arxiv.org/pdf/2409.19431v2,cs.LG
'Simulacrum of Stories': Examining Large Language Models as Qualitative Research Participants,"The recent excitement around generative models has sparked a wave of
proposals suggesting the replacement of human participation and labor in
research and development--e.g., through surveys, experiments, and
interviews--with synthetic research data generated by large language models
(LLMs). We conducted interviews with 19 qualitative researchers to understand
their perspectives on this paradigm shift. Initially skeptical, researchers
were surprised to see similar narratives emerge in the LLM-generated data when
using the interview probe. However, over several conversational turns, they
went on to identify fundamental limitations, such as how LLMs foreclose
participants' consent and agency, produce responses lacking in palpability and
contextual depth, and risk delegitimizing qualitative research methods. We
argue that the use of LLMs as proxies for participants enacts the surrogate
effect, raising ethical and epistemological concerns that extend beyond the
technical limitations of current models to the core of whether LLMs fit within
qualitative ways of knowing.",2024-09-28,"Shivani Kapania, William Agnew, Motahhare Eslami, Hoda Heidari, Sarah Fox",http://arxiv.org/pdf/2409.19430v1,cs.LG
A Proximal Modified Quasi-Newton Method for Nonsmooth Regularized Optimization,"We develop R2N, a modified quasi-Newton method for minimizing the sum of a
$\mathcal{C}^1$ function $f$ and a lower semi-continuous prox-bounded $h$. Both
$f$ and $h$ may be nonconvex. At each iteration, our method computes a step by
minimizing the sum of a quadratic model of $f$, a model of $h$, and an adaptive
quadratic regularization term. A step may be computed by a variant of the
proximal-gradient method. An advantage of R2N over trust-region (TR) methods is
that proximal operators do not involve an extra TR indicator. We also develop
the variant R2DH, in which the model Hessian is diagonal, which allows us to
compute a step without relying on a subproblem solver when $h$ is separable.
R2DH can be used as standalone solver, but also as subproblem solver inside
R2N. We describe non-monotone variants of both R2N and R2DH. Global convergence
of a first-order stationarity measure to zero holds without relying on local
Lipschitz continuity of $\nabla f$, while allowing model Hessians to grow
unbounded, an assumption particularly relevant to quasi-Newton models. Under
Lipschitz-continuity of $\nabla f$, we establish a tight worst-case complexity
bound of $O(1 / \epsilon^{2/(1 - p)})$ to bring said measure below $\epsilon >
0$, where $0 \leq p < 1$ controls the growth of model Hessians. The latter must
not diverge faster than $|\mathcal{S}_k|^p$, where $\mathcal{S}_k$ is the set
of successful iterations up to iteration $k$. When $p = 1$, we establish the
tight exponential complexity bound $O(\exp(c \epsilon^{-2}))$ where $c > 0$ is
a constant. We describe our Julia implementation and report numerical
experience on a basis-pursuit problem, image denoising, minimum-rank matrix
completion, and a nonlinear support vector machine. In particular, the
minimum-rank problem cannot be solved directly at this time by a TR approach as
corresponding proximal operators are not known analytically.",2024-09-28,"Youssef Diouane, Mohamed Laghdaf Habiboullah, Dominique Orban",http://arxiv.org/pdf/2409.19428v1,cs.LG
Identifiable Shared Component Analysis of Unpaired Multimodal Mixtures,"A core task in multi-modal learning is to integrate information from multiple
feature spaces (e.g., text and audio), offering modality-invariant essential
representations of data. Recent research showed that, classical tools such as
{\it canonical correlation analysis} (CCA) provably identify the shared
components up to minor ambiguities, when samples in each modality are generated
from a linear mixture of shared and private components. Such identifiability
results were obtained under the condition that the cross-modality samples are
aligned/paired according to their shared information. This work takes a step
further, investigating shared component identifiability from multi-modal linear
mixtures where cross-modality samples are unaligned. A distribution divergence
minimization-based loss is proposed, under which a suite of sufficient
conditions ensuring identifiability of the shared components are derived. Our
conditions are based on cross-modality distribution discrepancy
characterization and density-preserving transform removal, which are much
milder than existing studies relying on independent component analysis. More
relaxed conditions are also provided via adding reasonable structural
constraints, motivated by available side information in various applications.
The identifiability claims are thoroughly validated using synthetic and
real-world data.",2024-09-28,"Subash Timilsina, Sagar Shrestha, Xiao Fu",http://arxiv.org/pdf/2409.19422v2,cs.LG
Machine Learning Operations: A Mapping Study,"Machine learning and AI have been recently embraced by many companies.
Machine Learning Operations, (MLOps), refers to the use of continuous software
engineering processes, such as DevOps, in the deployment of machine learning
models to production. Nevertheless, not all machine learning initiatives
successfully transition to the production stage owing to the multitude of
intricate factors involved. This article discusses the issues that exist in
several components of the MLOps pipeline, namely the data manipulation
pipeline, model building pipeline, and deployment pipeline. A systematic
mapping study is performed to identify the challenges that arise in the MLOps
system categorized by different focus areas. Using this data, realistic and
applicable recommendations are offered for tools or solutions that can be used
for their implementation. The main value of this work is it maps distinctive
challenges in MLOps along with the recommended solutions outlined in our study.
These guidelines are not specific to any particular tool and are applicable to
both research and industrial settings.",2024-09-28,"Abhijit Chakraborty, Suddhasvatta Das, Kevin Gary",http://arxiv.org/pdf/2409.19416v1,cs.LG
Sequential Signal Mixing Aggregation for Message Passing Graph Neural Networks,"Message Passing Graph Neural Networks (MPGNNs) have emerged as the preferred
method for modeling complex interactions across diverse graph entities. While
the theory of such models is well understood, their aggregation module has not
received sufficient attention. Sum-based aggregators have solid theoretical
foundations regarding their separation capabilities. However, practitioners
often prefer using more complex aggregations and mixtures of diverse
aggregations. In this work, we unveil a possible explanation for this gap. We
claim that sum-based aggregators fail to ""mix"" features belonging to distinct
neighbors, preventing them from succeeding at downstream tasks. To this end, we
introduce Sequential Signal Mixing Aggregation (SSMA), a novel plug-and-play
aggregation for MPGNNs. SSMA treats the neighbor features as 2D discrete
signals and sequentially convolves them, inherently enhancing the ability to
mix features attributed to distinct neighbors. By performing extensive
experiments, we show that when combining SSMA with well-established MPGNN
architectures, we achieve substantial performance gains across various
benchmarks, achieving new state-of-the-art results in many settings. We
published our code at \url{https://almogdavid.github.io/SSMA/}",2024-09-28,"Mitchell Keren Taraday, Almog David, Chaim Baskin",http://arxiv.org/pdf/2409.19414v1,cs.LG
Canonical Correlation Guided Deep Neural Network,"Learning representations of two views of data such that the resulting
representations are highly linearly correlated is appealing in machine
learning. In this paper, we present a canonical correlation guided learning
framework, which allows to be realized by deep neural networks (CCDNN), to
learn such a correlated representation. It is also a novel merging of
multivariate analysis (MVA) and machine learning, which can be viewed as
transforming MVA into end-to-end architectures with the aid of neural networks.
Unlike the linear canonical correlation analysis (CCA), kernel CCA and deep
CCA, in the proposed method, the optimization formulation is not restricted to
maximize correlation, instead we make canonical correlation as a constraint,
which preserves the correlated representation learning ability and focuses more
on the engineering tasks endowed by optimization formulation, such as
reconstruction, classification and prediction. Furthermore, to reduce the
redundancy induced by correlation, a redundancy filter is designed. We
illustrate the performance of CCDNN on various tasks. In experiments on MNIST
dataset, the results show that CCDNN has better reconstruction performance in
terms of mean squared error and mean absolute error than DCCA and DCCAE. Also,
we present the application of the proposed network to industrial fault
diagnosis and remaining useful life cases for the classification and prediction
tasks accordingly. The proposed method demonstrates superior performance in
both tasks when compared to existing methods. Extension of CCDNN to much more
deeper with the aid of residual connection is also presented in appendix.",2024-09-28,"Zhiwen Chen, Siwen Mo, Haobin Ke, Steven X. Ding, Zhaohui Jiang, Chunhua Yang, Weihua Gui",http://arxiv.org/pdf/2409.19396v1,cs.LG
Value-Based Deep Multi-Agent Reinforcement Learning with Dynamic Sparse Training,"Deep Multi-agent Reinforcement Learning (MARL) relies on neural networks with
numerous parameters in multi-agent scenarios, often incurring substantial
computational overhead. Consequently, there is an urgent need to expedite
training and enable model compression in MARL. This paper proposes the
utilization of dynamic sparse training (DST), a technique proven effective in
deep supervised learning tasks, to alleviate the computational burdens in MARL
training. However, a direct adoption of DST fails to yield satisfactory MARL
agents, leading to breakdowns in value learning within deep sparse value-based
MARL models. Motivated by this challenge, we introduce an innovative
Multi-Agent Sparse Training (MAST) framework aimed at simultaneously enhancing
the reliability of learning targets and the rationality of sample distribution
to improve value learning in sparse models. Specifically, MAST incorporates the
Soft Mellowmax Operator with a hybrid TD-($\lambda$) schema to establish
dependable learning targets. Additionally, it employs a dual replay buffer
mechanism to enhance the distribution of training samples. Building upon these
aspects, MAST utilizes gradient-based topology evolution to exclusively train
multiple MARL agents using sparse networks. Our comprehensive experimental
investigation across various value-based MARL algorithms on multiple benchmarks
demonstrates, for the first time, significant reductions in redundancy of up to
$20\times$ in Floating Point Operations (FLOPs) for both training and
inference, with less than $3\%$ performance degradation.",2024-09-28,"Pihe Hu, Shaolong Li, Zhuoran Li, Ling Pan, Longbo Huang",http://arxiv.org/pdf/2409.19391v1,cs.LG
"Interpretable, multi-dimensional Evaluation Framework for Causal Discovery from observational i.i.d. Data","Nonlinear causal discovery from observational data imposes strict
identifiability assumptions on the formulation of structural equations utilized
in the data generating process. The evaluation of structure learning methods
under assumption violations requires a rigorous and interpretable approach,
which quantifies both the structural similarity of the estimation with the
ground truth and the capacity of the discovered graphs to be used for causal
inference. Motivated by the lack of unified performance assessment framework,
we introduce an interpretable, six-dimensional evaluation metric, i.e.,
distance to optimal solution (DOS), which is specifically tailored to the field
of causal discovery. Furthermore, this is the first research to assess the
performance of structure learning algorithms from seven different families on
increasing percentage of non-identifiable, nonlinear causal patterns, inspired
by real-world processes. Our large-scale simulation study, which incorporates
seven experimental factors, shows that besides causal order-based methods,
amortized causal discovery delivers results with comparatively high proximity
to the optimal solution.",2024-09-28,"Georg Velev, Stefan Lessmann",http://arxiv.org/pdf/2409.19377v2,cs.LG
DOTA: Distributional Test-Time Adaptation of Vision-Language Models,"Vision-language foundation models (e.g., CLIP) have shown remarkable
performance across a wide range of tasks. However, deploying these models may
be unreliable when significant distribution gaps exist between the training and
test data. The training-free test-time dynamic adapter (TDA) is a promising
approach to address this issue by storing representative test samples to guide
the classification of subsequent ones. However, TDA only naively maintains a
limited number of reference samples in the cache, leading to severe test-time
catastrophic forgetting when the cache is updated by dropping samples. In this
paper, we propose a simple yet effective method for DistributiOnal Test-time
Adaptation (Dota). Instead of naively memorizing representative test samples,
Dota continually estimates the distributions of test samples, allowing the
model to continually adapt to the deployment environment. The test-time
posterior probabilities are then computed using the estimated distributions
based on Bayes' theorem for adaptation purposes. To further enhance the
adaptability on the uncertain samples, we introduce a new human-in-the-loop
paradigm which identifies uncertain samples, collects human-feedback, and
incorporates it into the Dota framework. Extensive experiments validate that
Dota enables CLIP to continually learn, resulting in a significant improvement
compared to current state-of-the-art methods.",2024-09-28,"Zongbo Han, Jialong Yang, Junfan Li, Qinghua Hu, Qianli Xu, Mike Zheng Shou, Changqing Zhang",http://arxiv.org/pdf/2409.19375v1,cs.LG
Frequency-adaptive Multi-scale Deep Neural Networks,"Multi-scale deep neural networks (MscaleDNNs) with downing-scaling mapping
have demonstrated superiority over traditional DNNs in approximating target
functions characterized by high frequency features. However, the performance of
MscaleDNNs heavily depends on the parameters in the downing-scaling mapping,
which limits their broader application. In this work, we establish a fitting
error bound to explain why MscaleDNNs are advantageous for approximating high
frequency functions. Building on this insight, we construct a hybrid feature
embedding to enhance the accuracy and robustness of the downing-scaling
mapping. To reduce the dependency of MscaleDNNs on parameters in the
downing-scaling mapping, we propose frequency-adaptive MscaleDNNs, which
adaptively adjust these parameters based on a posterior error estimate that
captures the frequency information of the fitted functions. Numerical examples,
including wave propagation and the propagation of a localized solution of the
schr$\ddot{\text{o}}$dinger equation with a smooth potential near the
semi-classical limit, are presented. These examples demonstrate that the
frequency-adaptive MscaleDNNs improve accuracy by two to three orders of
magnitude compared to standard MscaleDNNs.",2024-09-28,"Jizu Huang, Rukang You, Tao Zhou",http://arxiv.org/pdf/2410.00053v1,cs.LG
Learning Strategy Representation for Imitation Learning in Multi-Agent Games,"The offline datasets for imitation learning (IL) in multi-agent games
typically contain player trajectories exhibiting diverse strategies, which
necessitate measures to prevent learning algorithms from acquiring undesirable
behaviors. Learning representations for these trajectories is an effective
approach to depicting the strategies employed by each demonstrator. However,
existing learning strategies often require player identification or rely on
strong assumptions, which are not appropriate for multi-agent games. Therefore,
in this paper, we introduce the Strategy Representation for Imitation Learning
(STRIL) framework, which (1) effectively learns strategy representations in
multi-agent games, (2) estimates proposed indicators based on these
representations, and (3) filters out sub-optimal data using the indicators.
STRIL is a plug-in method that can be integrated into existing IL algorithms.
We demonstrate the effectiveness of STRIL across competitive multi-agent
scenarios, including Two-player Pong, Limit Texas Hold'em, and Connect Four.
Our approach successfully acquires strategy representations and indicators,
thereby identifying dominant trajectories and significantly enhancing existing
IL performance across these environments.",2024-09-28,"Shiqi Lei, Kanghoon Lee, Linjing Li, Jinkyoo Park",http://arxiv.org/pdf/2409.19363v2,cs.LG
Sparse Modelling for Feature Learning in High Dimensional Data,"This paper presents an innovative approach to dimensionality reduction and
feature extraction in high-dimensional datasets, with a specific application
focus on wood surface defect detection. The proposed framework integrates
sparse modeling techniques, particularly Lasso and proximal gradient methods,
into a comprehensive pipeline for efficient and interpretable feature
selection. Leveraging pre-trained models such as VGG19 and incorporating
anomaly detection methods like Isolation Forest and Local Outlier Factor, our
methodology addresses the challenge of extracting meaningful features from
complex datasets. Evaluation metrics such as accuracy and F1 score, alongside
visualizations, are employed to assess the performance of the sparse modeling
techniques. Through this work, we aim to advance the understanding and
application of sparse modeling in machine learning, particularly in the context
of wood surface defect detection.",2024-09-28,"Harish Neelam, Koushik Sai Veerella, Souradip Biswas",http://arxiv.org/pdf/2409.19361v1,cs.LG
Quantum delegated and federated learning via quantum homomorphic encryption,"Quantum learning models hold the potential to bring computational advantages
over the classical realm. As powerful quantum servers become available on the
cloud, ensuring the protection of clients' private data becomes crucial. By
incorporating quantum homomorphic encryption schemes, we present a general
framework that enables quantum delegated and federated learning with a
computation-theoretical data privacy guarantee. We show that learning and
inference under this framework feature substantially lower communication
complexity compared with schemes based on blind quantum computing. In addition,
in the proposed quantum federated learning scenario, there is less
computational burden on local quantum devices from the client side, since the
server can operate on encrypted quantum data without extracting any
information. We further prove that certain quantum speedups in supervised
learning carry over to private delegated learning scenarios employing quantum
kernel methods. Our results provide a valuable guide toward privacy-guaranteed
quantum learning on the cloud, which may benefit future studies and
security-related applications.",2024-09-28,"Weikang Li, Dong-Ling Deng",http://arxiv.org/pdf/2409.19359v1,cs.LG
"Unveil Benign Overfitting for Transformer in Vision: Training Dynamics, Convergence, and Generalization","Transformers have demonstrated great power in the recent development of large
foundational models. In particular, the Vision Transformer (ViT) has brought
revolutionary changes to the field of vision, achieving significant
accomplishments on the experimental side. However, their theoretical
capabilities, particularly in terms of generalization when trained to overfit
training data, are still not fully understood. To address this gap, this work
delves deeply into the benign overfitting perspective of transformers in
vision. To this end, we study the optimization of a Transformer composed of a
self-attention layer with softmax followed by a fully connected layer under
gradient descent on a certain data distribution model. By developing techniques
that address the challenges posed by softmax and the interdependent nature of
multiple weights in transformer optimization, we successfully characterized the
training dynamics and achieved generalization in post-training. Our results
establish a sharp condition that can distinguish between the small test error
phase and the large test error regime, based on the signal-to-noise ratio in
the data model. The theoretical results are further verified by experimental
simulation. To the best of our knowledge, this is the first work to
characterize benign overfitting for Transformers.",2024-09-28,"Jiarui Jiang, Wei Huang, Miao Zhang, Taiji Suzuki, Liqiang Nie",http://arxiv.org/pdf/2409.19345v2,cs.LG
DelayPTC-LLM: Metro Passenger Travel Choice Prediction under Train Delays with Large Language Models,"Train delays can propagate rapidly throughout the Urban Rail Transit (URT)
network under networked operation conditions, posing significant challenges to
operational departments. Accurately predicting passenger travel choices under
train delays can provide interpretable insights into the redistribution of
passenger flow, offering crucial decision support for emergency response and
service recovery. However, the diversity of travel choices due to passenger
heterogeneity and the sparsity of delay events leads to issues of data sparsity
and sample imbalance in the travel choices dataset under metro delays. It is
challenging to model this problem using traditional machine learning
approaches, which typically rely on large, balanced datasets. Given the
strengths of large language models (LLMs) in text processing, understanding,
and their capabilities in small-sample and even zero-shot learning, this paper
proposes a novel Passenger Travel Choice prediction framework under metro
delays with the Large Language Model (DelayPTC-LLM). The well-designed
prompting engineering is developed to guide the LLM in making and rationalizing
predictions about travel choices, taking into account passenger heterogeneity
and features of the delay events. Utilizing real-world data from Shenzhen
Metro, including Automated Fare Collection (AFC) data and detailed delay logs,
a comparative analysis of DelayPTC-LLM with traditional prediction models
demonstrates the superior capability of LLMs in handling complex, sparse
datasets commonly encountered under disruption of transportation systems. The
results validate the advantages of DelayPTC-LLM in terms of predictive accuracy
and its potential to provide actionable insights for big traffic data.",2024-09-28,"Chen Chen, Yuxin He, Hao Wang, Jingjing Chen, Qin Luo",http://arxiv.org/pdf/2410.00052v1,cs.LG
Visual Question Decomposition on Multimodal Large Language Models,"Question decomposition has emerged as an effective strategy for prompting
Large Language Models (LLMs) to answer complex questions. However, while
existing methods primarily focus on unimodal language models, the question
decomposition capability of Multimodal Large Language Models (MLLMs) has yet to
be explored. To this end, this paper explores visual question decomposition on
MLLMs. Specifically, we introduce a systematic evaluation framework including a
dataset and several evaluation criteria to assess the quality of the decomposed
sub-questions, revealing that existing MLLMs struggle to produce high-quality
sub-questions. To address this limitation, we propose a specific finetuning
dataset, DecoVQA+, for enhancing the model's question decomposition capability.
Aiming at enabling models to perform appropriate selective decomposition, we
propose an efficient finetuning pipeline. The finetuning pipeline consists of
our proposed dataset and a training objective for selective decomposition.
Finetuned MLLMs demonstrate significant improvements in the quality of
sub-questions and the policy of selective question decomposition. Additionally,
the models also achieve higher accuracy with selective decomposition on VQA
benchmark datasets.",2024-09-28,"Haowei Zhang, Jianzhe Liu, Zhen Han, Shuo Chen, Bailan He, Volker Tresp, Zhiqiang Xu, Jindong Gu",http://arxiv.org/pdf/2409.19339v2,cs.LG
Multi-Atlas Brain Network Classification through Consistency Distillation and Complementary Information Fusion,"In the realm of neuroscience, identifying distinctive patterns associated
with neurological disorders via brain networks is crucial. Resting-state
functional magnetic resonance imaging (fMRI) serves as a primary tool for
mapping these networks by correlating blood-oxygen-level-dependent (BOLD)
signals across different brain regions, defined as regions of interest (ROIs).
Constructing these brain networks involves using atlases to parcellate the
brain into ROIs based on various hypotheses of brain division. However, there
is no standard atlas for brain network classification, leading to limitations
in detecting abnormalities in disorders. Some recent methods have proposed
utilizing multiple atlases, but they neglect consistency across atlases and
lack ROI-level information exchange. To tackle these limitations, we propose an
Atlas-Integrated Distillation and Fusion network (AIDFusion) to improve brain
network classification using fMRI data. AIDFusion addresses the challenge of
utilizing multiple atlases by employing a disentangle Transformer to filter out
inconsistent atlas-specific information and distill distinguishable connections
across atlases. It also incorporates subject- and population-level consistency
constraints to enhance cross-atlas consistency. Additionally, AIDFusion employs
an inter-atlas message-passing mechanism to fuse complementary information
across brain regions. Experimental results on four datasets of different
diseases demonstrate the effectiveness and efficiency of AIDFusion compared to
state-of-the-art methods. A case study illustrates AIDFusion extract patterns
that are both interpretable and consistent with established neuroscience
findings.",2024-09-28,"Jiaxing Xu, Mengcheng Lan, Xia Dong, Kai He, Wei Zhang, Qingtian Bian, Yiping Ke",http://arxiv.org/pdf/2410.08228v1,cs.LG
A Generalized Model for Multidimensional Intransitivity,"Intransitivity is a critical issue in pairwise preference modeling. It refers
to the intransitive pairwise preferences between a group of players or objects
that potentially form a cyclic preference chain and has been long discussed in
social choice theory in the context of the dominance relationship. However,
such multifaceted intransitivity between players and the corresponding player
representations in high dimensions is difficult to capture. In this paper, we
propose a probabilistic model that jointly learns each player's d-dimensional
representation (d>1) and a dataset-specific metric space that systematically
captures the distance metric in Rd over the embedding space. Interestingly, by
imposing additional constraints in the metric space, our proposed model
degenerates to former models used in intransitive representation learning.
Moreover, we present an extensive quantitative investigation of the vast
existence of intransitive relationships between objects in various real-world
benchmark datasets. To our knowledge, this investigation is the first of this
type. The predictive performance of our proposed method on different real-world
datasets, including social choice, election, and online game datasets, shows
that our proposed method outperforms several competing methods in terms of
prediction accuracy.",2024-09-28,"Jiuding Duan, Jiyi Li, Yukino Baba, Hisashi Kashima",http://arxiv.org/pdf/2409.19325v1,cs.LG
NeuralQP: A General Hypergraph-based Optimization Framework for Large-scale QCQPs,"Machine Learning (ML) optimization frameworks have gained attention for their
ability to accelerate the optimization of large-scale Quadratically Constrained
Quadratic Programs (QCQPs) by learning shared problem structures. However,
existing ML frameworks often rely heavily on strong problem assumptions and
large-scale solvers. This paper introduces NeuralQP, a general hypergraph-based
framework for large-scale QCQPs. NeuralQP features two main components:
Hypergraph-based Neural Prediction, which generates embeddings and predicted
solutions for QCQPs without problem assumptions, and Parallel Neighborhood
Optimization, which employs a McCormick relaxation-based repair strategy to
identify and correct illegal variables, iteratively improving the solution with
a small-scale solver. We further prove that our framework UniEGNN with our
hypergraph representation is equivalent to the Interior-Point Method (IPM) for
quadratic programming. Experiments on two benchmark problems and large-scale
real-world instances from QPLIB demonstrate that NeuralQP outperforms
state-of-the-art solvers (e.g., Gurobi and SCIP) in both solution quality and
time efficiency, further validating the efficiency of ML optimization
frameworks for QCQPs.",2024-09-28,"Zhixiao Xiong, Fangyu Zong, Huigen Ye, Hua Xu",http://arxiv.org/pdf/2410.03720v1,cs.LG
Generalizing Consistency Policy to Visual RL with Prioritized Proximal Experience Regularization,"With high-dimensional state spaces, visual reinforcement learning (RL) faces
significant challenges in exploitation and exploration, resulting in low sample
efficiency and training stability. As a time-efficient diffusion model,
although consistency models have been validated in online state-based RL, it is
still an open question whether it can be extended to visual RL. In this paper,
we investigate the impact of non-stationary distribution and the actor-critic
framework on consistency policy in online RL, and find that consistency policy
was unstable during the training, especially in visual RL with the
high-dimensional state space. To this end, we suggest sample-based entropy
regularization to stabilize the policy training, and propose a consistency
policy with prioritized proximal experience regularization (CP3ER) to improve
sample efficiency. CP3ER achieves new state-of-the-art (SOTA) performance in 21
tasks across DeepMind control suite and Meta-world. To our knowledge, CP3ER is
the first method to apply diffusion/consistency models to visual RL and
demonstrates the potential of consistency models in visual RL. More
visualization results are available at https://jzndd.github.io/CP3ER-Page/.",2024-09-28,"Haoran Li, Zhennan Jiang, Yuhui Chen, Dongbin Zhao",http://arxiv.org/pdf/2410.00051v2,cs.LG
CycleBNN: Cyclic Precision Training in Binary Neural Networks,"This paper works on Binary Neural Networks (BNNs), a promising avenue for
efficient deep learning, offering significant reductions in computational
overhead and memory footprint to full precision networks. However, the
challenge of energy-intensive training and the drop in performance have been
persistent issues. Tackling the challenge, prior works focus primarily on
task-related inference optimization. Unlike prior works, this study offers an
innovative methodology integrating BNNs with cyclic precision training,
introducing the CycleBNN. This approach is designed to enhance training
efficiency while minimizing the loss in performance. By dynamically adjusting
precision in cycles, we achieve a convenient trade-off between training
efficiency and model performance. This emphasizes the potential of our method
in energy-constrained training scenarios, where data is collected onboard and
paves the way for sustainable and efficient deep learning architectures. To
gather insights on CycleBNN's efficiency, we conduct experiments on ImageNet,
CIFAR-10, and PASCAL-VOC, obtaining competitive performances while using
96.09\% less operations during training on ImageNet, 88.88\% on CIFAR-10 and
96.09\% on PASCAL-VOC. Finally, CycleBNN offers a path towards faster, more
accessible training of efficient networks, accelerating the development of
practical applications. The PyTorch code is available at
\url{https://github.com/fedeloper/CycleBNN/}",2024-09-28,"Federico Fontana, Romeo Lanzino, Anxhelo Diko, Gian Luca Foresti, Luigi Cinque",http://arxiv.org/pdf/2410.00050v1,cs.LG
Distributed Optimization via Energy Conservation Laws in Dilated Coordinates,"Optimizing problems in a distributed manner is critical for systems involving
multiple agents with private data. Despite substantial interest, a unified
method for analyzing the convergence rates of distributed optimization
algorithms is lacking. This paper introduces an energy conservation approach
for analyzing continuous-time dynamical systems in dilated coordinates. Instead
of directly analyzing dynamics in the original coordinate system, we establish
a conserved quantity, akin to physical energy, in the dilated coordinate
system. Consequently, convergence rates can be explicitly expressed in terms of
the inverse time-dilation factor. Leveraging this generalized approach, we
formulate a novel second-order distributed accelerated gradient flow with a
convergence rate of $O\left(1/t^{2-\epsilon}\right)$ in time $t$ for
$\epsilon>0$. We then employ a semi second-order symplectic Euler
discretization to derive a rate-matching algorithm with a convergence rate of
$O\left(1/k^{2-\epsilon}\right)$ in $k$ iterations. To the best of our
knowledge, this represents the most favorable convergence rate for any
distributed optimization algorithm designed for smooth convex optimization. Its
accelerated convergence behavior is benchmarked against various
state-of-the-art distributed optimization algorithms on practical, large-scale
problems.",2024-09-28,"Mayank Baranwal, Kushal Chakrabarti",http://arxiv.org/pdf/2409.19279v1,cs.LG
Explicit construction of recurrent neural networks effectively approximating discrete dynamical systems,"We consider arbitrary bounded discrete time series originating from dynamical
system with recursivity. More precisely, we provide an explicit construction of
recurrent neural networks which effectively approximate the corresponding
discrete dynamical systems.",2024-09-28,"Chikara Nakayama, Tsuyoshi Yoneda",http://arxiv.org/pdf/2409.19278v1,cs.LG
Public interest in science or bots? Selective amplification of scientific articles on Twitter,"With the remarkable capability to reach the public instantly, social media
has become integral in sharing scholarly articles to measure public response.
Since spamming by bots on social media can steer the conversation and present a
false public interest in given research, affecting policies impacting the
public's lives in the real world, this topic warrants critical study and
attention. We used the Altmetric dataset in combination with data collected
through the Twitter Application Programming Interface (API) and the Botometer
API. We combined the data into an extensive dataset with academic articles,
several features from the article and a label indicating whether the article
had excessive bot activity on Twitter or not. We analyzed the data to see the
possibility of bot activity based on different characteristics of the article.
We also trained machine-learning models using this dataset to identify possible
bot activity in any given article. Our machine-learning models were capable of
identifying possible bot activity in any academic article with an accuracy of
0.70. We also found that articles related to ""Health and Human Science"" are
more prone to bot activity compared to other research areas. Without arguing
the maliciousness of the bot activity, our work presents a tool to identify the
presence of bot activity in the dissemination of an academic article and
creates a baseline for future research in this direction.",2024-09-28,"Ashiqur Rahman, Ehsan Mohammadi, Hamed Alhoori",http://arxiv.org/pdf/2410.01842v1,cs.LG
VecLSTM: Trajectory Data Processing and Management for Activity Recognition through LSTM Vectorization and Database Integration,"Activity recognition is a challenging task due to the large scale of
trajectory data and the need for prompt and efficient processing. Existing
methods have attempted to mitigate this problem by employing traditional LSTM
architectures, but these approaches often suffer from inefficiencies in
processing large datasets. In response to this challenge, we propose VecLSTM, a
novel framework that enhances the performance and efficiency of LSTM-based
neural networks. Unlike conventional approaches, VecLSTM incorporates
vectorization layers, leveraging optimized mathematical operations to process
input sequences more efficiently. We have implemented VecLSTM and incorporated
it into the MySQL database. To evaluate the effectiveness of VecLSTM, we
compare its performance against a conventional LSTM model using a dataset
comprising 1,467,652 samples with seven unique labels. Experimental results
demonstrate superior accuracy and efficiency compared to the state-of-the-art,
with VecLSTM achieving a validation accuracy of 85.57\%, a test accuracy of
85.47\%, and a weighted F1-score of 0.86. Furthermore, VecLSTM significantly
reduces training time, offering a 26.2\% reduction compared to traditional LSTM
models.",2024-09-28,"Solmaz Seyed Monir, Dongfang Zhao",http://arxiv.org/pdf/2409.19258v1,cs.LG
HybridFlow: A Flexible and Efficient RLHF Framework,"Reinforcement Learning from Human Feedback (RLHF) is widely used in Large
Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow,
where each node represents computation of a neural network (NN) and each edge
denotes data dependencies between the NNs. RLHF complicates the dataflow by
expanding each node into a distributed LLM training or generation program, and
each edge into a many-to-many multicast. Traditional RL frameworks execute the
dataflow using a single controller to instruct both intra-node computation and
inter-node communication, which can be inefficient in RLHF due to large control
dispatch overhead for distributed intra-node computation. Existing RLHF systems
adopt a multi-controller paradigm, which can be inflexible due to nesting
distributed computation and data communication. We propose HybridFlow, which
combines single-controller and multi-controller paradigms in a hybrid manner to
enable flexible representation and efficient execution of the RLHF dataflow. We
carefully design a set of hierarchical APIs that decouple and encapsulate
computation and data dependencies in the complex RLHF dataflow, allowing
efficient operation orchestration to implement RLHF algorithms and flexible
mapping of the computation onto various devices. We further design a
3D-HybridEngine for efficient actor model resharding between training and
generation phases, with zero memory redundancy and significantly reduced
communication overhead. Our experimental results demonstrate
1.53$\times$~20.57$\times$ throughput improvement when running various RLHF
algorithms using HybridFlow, as compared with state-of-the-art baselines.
HybridFlow source code will be available at https://github.com/volcengine/verl.",2024-09-28,"Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, Chuan Wu",http://arxiv.org/pdf/2409.19256v2,cs.LG
"Forgetting, Ignorance or Myopia: Revisiting Key Challenges in Online Continual Learning","Online continual learning requires the models to learn from constant, endless
streams of data. While significant efforts have been made in this field, most
were focused on mitigating the catastrophic forgetting issue to achieve better
classification ability, at the cost of a much heavier training workload. They
overlooked that in real-world scenarios, e.g., in high-speed data stream
environments, data do not pause to accommodate slow models. In this paper, we
emphasize that model throughput -- defined as the maximum number of training
samples that a model can process within a unit of time -- is equally important.
It directly limits how much data a model can utilize and presents a challenging
dilemma for current methods. With this understanding, we revisit key challenges
in OCL from both empirical and theoretical perspectives, highlighting two
critical issues beyond the well-documented catastrophic forgetting: Model's
ignorance: the single-pass nature of OCL challenges models to learn effective
features within constrained training time and storage capacity, leading to a
trade-off between effective learning and model throughput; Model's myopia: the
local learning nature of OCL on the current task leads the model to adopt
overly simplified, task-specific features and excessively sparse classifier,
resulting in the gap between the optimal solution for the current task and the
global objective. To tackle these issues, we propose the Non-sparse Classifier
Evolution framework (NsCE) to facilitate effective global discriminative
feature learning with minimal time cost. NsCE integrates non-sparse maximum
separation regularization and targeted experience replay techniques with the
help of pre-trained models, enabling rapid acquisition of new globally
discriminative features.",2024-09-28,"Xinrui Wang, Chuanxing Geng, Wenhai Wan, Shao-yuan Li, Songcan Chen",http://arxiv.org/pdf/2409.19245v2,cs.LG
Zorro: A Flexible and Differentiable Parametric Family of Activation Functions That Extends ReLU and GELU,"Even in recent neural network architectures such as Transformers and Extended
LSTM (xLSTM), and traditional ones like Convolutional Neural Networks,
Activation Functions are an integral part of nearly all neural networks. They
enable more effective training and capture nonlinear data patterns. More than
400 functions have been proposed over the last 30 years, including fixed or
trainable parameters, but only a few are widely used. ReLU is one of the most
frequently used, with GELU and Swish variants increasingly appearing. However,
ReLU presents non-differentiable points and exploding gradient issues, while
testing different parameters of GELU and Swish variants produces varying
results, needing more parameters to adapt to datasets and architectures. This
article introduces a novel set of activation functions called Zorro, a
continuously differentiable and flexible family comprising five main functions
fusing ReLU and Sigmoid. Zorro functions are smooth and adaptable, and serve as
information gates, aligning with ReLU in the 0-1 range, offering an alternative
to ReLU without the need for normalization, neuron death, or gradient
explosions. Zorro also approximates functions like Swish, GELU, and DGELU,
providing parameters to adjust to different datasets and architectures. We
tested it on fully connected, convolutional, and transformer architectures to
demonstrate its effectiveness.",2024-09-28,"Matias Roodschild, Jorge Gotay-Sardiñas, Victor A. Jimenez, Adrian Will",http://arxiv.org/pdf/2409.19239v1,cs.LG
Decoding Android Malware with a Fraction of Features: An Attention-Enhanced MLP-SVM Approach,"The escalating sophistication of Android malware poses significant challenges
to traditional detection methods, necessitating innovative approaches that can
efficiently identify and classify threats with high precision. This paper
introduces a novel framework that synergistically integrates an
attention-enhanced Multi-Layer Perceptron (MLP) with a Support Vector Machine
(SVM) to make Android malware detection and classification more effective. By
carefully analyzing a mere 47 features out of over 9,760 available in the
comprehensive CCCS-CIC-AndMal-2020 dataset, our MLP-SVM model achieves an
impressive accuracy over 99% in identifying malicious applications. The MLP,
enhanced with an attention mechanism, focuses on the most discriminative
features and further reduces the 47 features to only 14 components using Linear
Discriminant Analysis (LDA). Despite this significant reduction in
dimensionality, the SVM component, equipped with an RBF kernel, excels in
mapping these components to a high-dimensional space, facilitating precise
classification of malware into their respective families. Rigorous evaluations,
encompassing accuracy, precision, recall, and F1-score metrics, confirm the
superiority of our approach compared to existing state-of-the-art techniques.
The proposed framework not only significantly reduces the computational
complexity by leveraging a compact feature set but also exhibits resilience
against the evolving Android malware landscape.",2024-09-28,"Safayat Bin Hakim, Muhammad Adil, Kamal Acharya, Houbing Herbert Song",http://arxiv.org/pdf/2409.19234v2,cs.LG
Double Actor-Critic with TD Error-Driven Regularization in Reinforcement Learning,"To obtain better value estimation in reinforcement learning, we propose a
novel algorithm based on the double actor-critic framework with temporal
difference error-driven regularization, abbreviated as TDDR. TDDR employs
double actors, with each actor paired with a critic, thereby fully leveraging
the advantages of double critics. Additionally, TDDR introduces an innovative
critic regularization architecture. Compared to classical deterministic policy
gradient-based algorithms that lack a double actor-critic structure, TDDR
provides superior estimation. Moreover, unlike existing algorithms with double
actor-critic frameworks, TDDR does not introduce any additional
hyperparameters, significantly simplifying the design and implementation
process. Experiments demonstrate that TDDR exhibits strong competitiveness
compared to benchmark algorithms in challenging continuous control tasks.",2024-09-28,"Haohui Chen, Zhiyong Chen, Aoxiang Liu, Wentuo Fang",http://arxiv.org/pdf/2409.19231v1,cs.LG
Epidemiology-Aware Neural ODE with Continuous Disease Transmission Graph,"Effective epidemic forecasting is critical for public health strategies and
efficient medical resource allocation, especially in the face of rapidly
spreading infectious diseases. However, existing deep-learning methods often
overlook the dynamic nature of epidemics and fail to account for the specific
mechanisms of disease transmission. In response to these challenges, we
introduce an innovative end-to-end framework called Epidemiology-Aware Neural
ODE with Continuous Disease Transmission Graph (EARTH) in this paper. To learn
continuous and regional disease transmission patterns, we first propose EANO,
which seamlessly integrates the neural ODE approach with the epidemic
mechanism, considering the complex spatial spread process during epidemic
evolution. Additionally, we introduce GLTG to model global infection trends and
leverage these signals to guide local transmission dynamically. To accommodate
both the global coherence of epidemic trends and the local nuances of epidemic
transmission patterns, we build a cross-attention approach to fuse the most
meaningful information for forecasting. Through the smooth synergy of both
components, EARTH offers a more robust and flexible approach to understanding
and predicting the spread of infectious diseases. Extensive experiments show
EARTH superior performance in forecasting real-world epidemics compared to
state-of-the-art methods. The code will be available at
https://github.com/Emory-Melody/EpiLearn.",2024-09-28,"Guancheng Wan, Zewen Liu, Max S. Y. Lau, B. Aditya Prakash, Wei Jin",http://arxiv.org/pdf/2410.00049v2,cs.LG
Cauchy activation function and XNet,"We have developed a novel activation function, named the Cauchy Activation
Function. This function is derived from the Cauchy Integral Theorem in complex
analysis and is specifically tailored for problems requiring high precision.
This innovation has led to the creation of a new class of neural networks,
which we call (Comple)XNet, or simply XNet. We will demonstrate that XNet is
particularly effective for high-dimensional challenges such as image
classification and solving Partial Differential Equations (PDEs). Our
evaluations show that XNet significantly outperforms established benchmarks
like MNIST and CIFAR-10 in computer vision, and offers substantial advantages
over Physics-Informed Neural Networks (PINNs) in both low-dimensional and
high-dimensional PDE scenarios.",2024-09-28,"Xin Li, Zhihong Xia, Hongkun Zhang",http://arxiv.org/pdf/2409.19221v2,cs.LG
A Characterization of List Regression,"There has been a recent interest in understanding and characterizing the
sample complexity of list learning tasks, where the learning algorithm is
allowed to make a short list of $k$ predictions, and we simply require one of
the predictions to be correct. This includes recent works characterizing the
PAC sample complexity of standard list classification and online list
classification.
  Adding to this theme, in this work, we provide a complete characterization of
list PAC regression. We propose two combinatorial dimensions, namely the
$k$-OIG dimension and the $k$-fat-shattering dimension, and show that they
characterize realizable and agnostic $k$-list regression respectively. These
quantities generalize known dimensions for standard regression. Our work thus
extends existing list learning characterizations from classification to
regression.",2024-09-28,"Chirag Pabbaraju, Sahasrajit Sarmasarkar",http://arxiv.org/pdf/2409.19218v2,cs.LG
Test Case-Informed Knowledge Tracing for Open-ended Coding Tasks,"Open-ended coding tasks, which ask students to construct programs according
to certain specifications, are common in computer science education. Student
modeling can be challenging since their open-ended nature means that student
code can be diverse. Traditional knowledge tracing (KT) models that only
analyze response correctness may not fully capture nuances in student knowledge
from student code. In this paper, we introduce Test case-Informed Knowledge
Tracing for Open-ended Coding (TIKTOC), a framework to simultaneously analyze
and predict both open-ended student code and whether the code passes each test
case. We augment the existing CodeWorkout dataset with the test cases used for
a subset of the open-ended coding questions, and propose a multi-task learning
KT method to simultaneously analyze and predict 1) whether a student's code
submission passes each test case and 2) the student's open-ended code, using a
large language model as the backbone. We quantitatively show that these methods
outperform existing KT methods for coding that only use the overall score a
code submission receives. We also qualitatively demonstrate how test case
information, combined with open-ended code, helps us gain fine-grained insights
into student knowledge.",2024-09-28,"Zhangqi Duan, Nigel Fernandez, Alexander Hicks, Andrew Lan",http://arxiv.org/pdf/2410.10829v3,cs.LG
Group & Reweight: A Novel Cost-Sensitive Approach to Mitigating Class Imbalance in Network Traffic Classification,"Internet services have led to the eruption of network traffic, and machine
learning on these Internet data has become an indispensable tool, especially
when the application is risk-sensitive. This paper focuses on network traffic
classification in the presence of severe class imbalance. Such a distributional
trait mostly drifts the optimal decision boundary and results in an
unsatisfactory solution. This raises safety concerns in the network traffic
field when previous class imbalance methods hardly deal with numerous minority
malicious classes. To alleviate these effects, we design a group & reweight
strategy for alleviating class imbalance. Inspired by the group
distributionally optimization framework, our approach heuristically clusters
classes into groups, iteratively updates the non-parametric weights for
separate classes, and optimizes the learning model by minimizing reweighted
losses. We theoretically interpret the optimization process from a Stackelberg
game and perform extensive experiments on typical benchmarks. Results show that
our approach can not only suppress the negative effect of class imbalance but
also improve the comprehensive performance in prediction.",2024-09-28,"Wumei Du, Dong Liang, Yiqin Lv, Xingxing Liang, Guanlin Wu, Qi Wang, Zheng Xie",http://arxiv.org/pdf/2409.19214v6,cs.LG
An Accelerated Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness,"This paper investigates a class of stochastic bilevel optimization problems
where the upper-level function is nonconvex with potentially unbounded
smoothness and the lower-level problem is strongly convex. These problems have
significant applications in sequential data learning, such as text
classification using recurrent neural networks. The unbounded smoothness is
characterized by the smoothness constant of the upper-level function scaling
linearly with the gradient norm, lacking a uniform upper bound. Existing
state-of-the-art algorithms require $\widetilde{O}(1/\epsilon^4)$ oracle calls
of stochastic gradient or Hessian/Jacobian-vector product to find an
$\epsilon$-stationary point. However, it remains unclear if we can further
improve the convergence rate when the assumptions for the function in the
population level also hold for each random realization almost surely. To
address this issue, we propose a new Accelerated Bilevel Optimization algorithm
named AccBO. The algorithm updates the upper-level variable by normalized
stochastic gradient descent with recursive momentum and the lower-level
variable by the stochastic Nesterov accelerated gradient descent algorithm with
averaging. We prove that our algorithm achieves an oracle complexity of
$\widetilde{O}(1/\epsilon^3)$ to find an $\epsilon$-stationary point, when the
lower-level stochastic gradient's variance is $O(\epsilon)$. Our proof relies
on a novel lemma characterizing the dynamics of stochastic Nesterov accelerated
gradient descent algorithm under distribution drift with high probability for
the lower-level variable, which is of independent interest and also plays a
crucial role in analyzing the hypergradient estimation error over time.
Experimental results on various tasks confirm that our proposed algorithm
achieves the predicted theoretical acceleration and significantly outperforms
baselines in bilevel optimization.",2024-09-28,"Xiaochuan Gong, Jie Hao, Mingrui Liu",http://arxiv.org/pdf/2409.19212v5,cs.LG
Boosting SISSO Performance on Small Sample Datasets by Using Random Forests Prescreening for Complex Feature Selection,"In materials science, data-driven methods accelerate material discovery and
optimization while reducing costs and improving success rates. Symbolic
regression is a key to extracting material descriptors from large datasets, in
particular the Sure Independence Screening and Sparsifying Operator (SISSO)
method. While SISSO needs to store the entire expression space to impose heavy
memory demands, it limits the performance in complex problems. To address this
issue, we propose a RF-SISSO algorithm by combining Random Forests (RF) with
SISSO. In this algorithm, the Random Forest algorithm is used for prescreening,
capturing non-linear relationships and improving feature selection, which may
enhance the quality of the input data and boost the accuracy and efficiency on
regression and classification tasks. For a testing on the SISSO's verification
problem for 299 materials, RF-SISSO demonstrates its robust performance and
high accuracy. RF-SISSO can maintain the testing accuracy above 0.9 across all
four training sample sizes and significantly enhancing regression efficiency,
especially in training subsets with smaller sample sizes. For the training
subset with 45 samples, the efficiency of RF-SISSO was 265 times higher than
that of original SISSO. As collecting large datasets would be both costly and
time-consuming in the practical experiments, it is thus believed that RF-SISSO
may benefit scientific researches by offering a high predicting accuracy with
limited data efficiently.",2024-09-28,"Xiaolin Jiang, Guanqi Liu, Jiaying Xie, Zhenpeng Hu",http://arxiv.org/pdf/2409.19209v1,cs.LG
Faster Acceleration for Steepest Descent,"Recent advances (Sherman, 2017; Sidford and Tian, 2018; Cohen et al., 2021)
have overcome the fundamental barrier of dimension dependence in the iteration
complexity of solving $\ell_\infty$ regression with first-order methods. Yet it
remains unclear to what extent such acceleration can be achieved for general
$\ell_p$ smooth functions. In this paper, we propose a new accelerated
first-order method for convex optimization under non-Euclidean smoothness
assumptions. In contrast to standard acceleration techniques, our approach uses
primal-dual iterate sequences taken with respect to $\textit{differing}$ norms,
which are then coupled using an $\textit{implicitly}$ determined interpolation
parameter. For $\ell_p$ norm smooth problems in $d$ dimensions, our method
provides an iteration complexity improvement of up to $O(d^{1-\frac{2}{p}})$ in
terms of calls to a first-order oracle, thereby allowing us to circumvent
long-standing barriers in accelerated non-Euclidean steepest descent.",2024-09-28,"Site Bai, Brian Bullins",http://arxiv.org/pdf/2409.19200v2,cs.LG
Learning-Based Image Compression for Machines,"While learning based compression techniques for images have outperformed
traditional methods, they have not been widely adopted in machine learning
pipelines. This is largely due to lack of standardization and lack of retention
of salient features needed for such tasks. Decompression of images have taken a
back seat in recent years while the focus has shifted to an image's utility in
performing machine learning based analysis on top of them. Thus the demand for
compression pipelines that incorporate such features from images has become
ever present. The methods outlined in the report build on the recent work done
on learning based image compression techniques to incorporate downstream tasks
in them. We propose various methods of finetuning and enhancing different parts
of pretrained compression encoding pipeline and present the results of our
investigation regarding the performance of vision tasks using compression based
pipelines.",2024-09-27,"Kartik Gupta, Kimberley Faria, Vikas Mehta",http://arxiv.org/pdf/2409.19184v1,cs.LG
Evidence Is All You Need: Ordering Imaging Studies via Language Model Alignment with the ACR Appropriateness Criteria,"Diagnostic imaging studies are an increasingly important component of the
workup and management of acutely presenting patients. However, ordering
appropriate imaging studies according to evidence-based medical guidelines is a
challenging task with a high degree of variability between healthcare
providers. To address this issue, recent work has investigated if generative AI
and large language models can be leveraged to help clinicians order relevant
imaging studies for patients. However, it is challenging to ensure that these
tools are correctly aligned with medical guidelines, such as the American
College of Radiology's Appropriateness Criteria (ACR AC). In this study, we
introduce a framework to intelligently leverage language models by recommending
imaging studies for patient cases that are aligned with evidence-based
guidelines. We make available a novel dataset of patient ""one-liner"" scenarios
to power our experiments, and optimize state-of-the-art language models to
achieve an accuracy on par with clinicians in image ordering. Finally, we
demonstrate that our language model-based pipeline can be used as intelligent
assistants by clinicians to support image ordering workflows and improve the
accuracy of imaging study ordering according to the ACR AC. Our work
demonstrates and validates a strategy to leverage AI-based software to improve
trustworthy clinical decision making in alignment with expert evidence-based
guidelines.",2024-09-27,"Michael S. Yao, Allison Chae, Charles E. Kahn Jr., Walter R. Witschey, James C. Gee, Hersh Sagreiya, Osbert Bastani",http://arxiv.org/pdf/2409.19177v2,cs.LG
Reducing Overtreatment of Indeterminate Thyroid Nodules Using a Multimodal Deep Learning Model,"Objective: Molecular testing (MT) classifies cytologically indeterminate
thyroid nodules as benign or malignant with high sensitivity but low positive
predictive value (PPV), only using molecular profiles, ignoring ultrasound (US)
imaging and biopsy. We address this limitation by applying attention multiple
instance learning (AMIL) to US images.
  Methods: We retrospectively reviewed 333 patients with indeterminate thyroid
nodules at UCLA medical center (259 benign, 74 malignant). A multi-modal deep
learning AMIL model was developed, combining US images and MT to classify the
nodules as benign or malignant and enhance the malignancy risk stratification
of MT.
  Results: The final AMIL model matched MT sensitivity (0.946) while
significantly improving PPV (0.477 vs 0.448 for MT alone), indicating fewer
false positives while maintaining high sensitivity.
  Conclusion: Our approach reduces false positives compared to MT while
maintaining the same ability to identify positive cases, potentially reducing
unnecessary benign thyroid resections in patients with indeterminate nodules.",2024-09-27,"Shreeram Athreya, Andrew Melehy, Sujit Silas Armstrong Suthahar, Vedrana Ivezić, Ashwath Radhachandran, Vivek Sant, Chace Moleta, Henry Zheng, Maitraya Patel, Rinat Masamed, Corey W. Arnold, William Speier",http://arxiv.org/pdf/2409.19171v1,cs.LG
Revisiting the Superficial Alignment Hypothesis,"The Superficial Alignment Hypothesis posits that almost all of a language
model's abilities and knowledge are learned during pre-training, while
post-training is about giving a model the right style and format. We re-examine
these claims by empirically studying the scaling behavior of post-training with
increasing finetuning examples and evaluating them using objective
task-specific standardized benchmarks. Through experiments with the Llama-3,
Mistral, and Llama-2 model families of multiple sizes, we observe that, similar
to the pre-training scaling laws, post-training task performance scales as a
power law against the number of finetuning examples. This power law
relationship holds across a broad array of capabilities, including mathematical
reasoning, coding, instruction following, and multihop-reasoning. In addition,
for tasks like math and multihop reasoning, we observe that a handful of
examples merely align the model stylistically but do not saturate performance
on the benchmarks. Model performance is instead correlated with its reasoning
ability and it improves significantly with more examples, illustrating the need
for holistic evaluation programs leveraging objective benchmarks in addition to
measurement of alignment to human preferences. We also observe that language
models are not necessarily limited to using knowledge learned during
pre-training. With appropriate post-training, a model's ability to integrate
new knowledge greatly improves on downstream tasks like multihop
question-answering. Taken together, these results shed new light on the
Superficial Alignment Hypothesis, suggesting that it is, at best, an
over-simplification.",2024-09-27,"Mohit Raghavendra, Vaskar Nath, Sean Hendryx",http://arxiv.org/pdf/2410.03717v1,cs.LG
Calibrated Probabilistic Forecasts for Arbitrary Sequences,"Real-world data streams can change unpredictably due to distribution shifts,
feedback loops and adversarial actors, which challenges the validity of
forecasts. We present a forecasting framework ensuring valid uncertainty
estimates regardless of how data evolves. Leveraging the concept of Blackwell
approachability from game theory, we introduce a forecasting framework that
guarantees calibrated uncertainties for outcomes in any compact space (e.g.,
classification or bounded regression). We extend this framework to recalibrate
existing forecasters, guaranteeing calibration without sacrificing predictive
performance. We implement both general-purpose gradient-based algorithms and
algorithms optimized for popular special cases of our framework. Empirically,
our algorithms improve calibration and downstream decision-making for energy
systems.",2024-09-27,"Charles Marx, Volodymyr Kuleshov, Stefano Ermon",http://arxiv.org/pdf/2409.19157v2,cs.LG
Multimodal Pragmatic Jailbreak on Text-to-image Models,"Diffusion models have recently achieved remarkable advancements in terms of
image quality and fidelity to textual prompts. Concurrently, the safety of such
generative models has become an area of growing concern. This work introduces a
novel type of jailbreak, which triggers T2I models to generate the image with
visual text, where the image and the text, although considered to be safe in
isolation, combine to form unsafe content. To systematically explore this
phenomenon, we propose a dataset to evaluate the current diffusion-based
text-to-image (T2I) models under such jailbreak. We benchmark nine
representative T2I models, including two close-source commercial models.
Experimental results reveal a concerning tendency to produce unsafe content:
all tested models suffer from such type of jailbreak, with rates of unsafe
generation ranging from 8\% to 74\%. In real-world scenarios, various filters
such as keyword blocklists, customized prompt filters, and NSFW image filters,
are commonly employed to mitigate these risks. We evaluate the effectiveness of
such filters against our jailbreak and found that, while current classifiers
may be effective for single modality detection, they fail to work against our
jailbreak. Our work provides a foundation for further development towards more
secure and reliable T2I models.",2024-09-27,"Tong Liu, Zhixin Lai, Gengyuan Zhang, Philip Torr, Vera Demberg, Volker Tresp, Jindong Gu",http://arxiv.org/pdf/2409.19149v1,cs.LG
Physics-Informed Echo State Networks for Modeling Controllable Dynamical Systems,"Echo State Networks (ESNs) are recurrent neural networks usually employed for
modeling nonlinear dynamic systems with relatively ease of training. By
incorporating physical laws into the training of ESNs, Physics-Informed ESNs
(PI-ESNs) were proposed initially to model chaotic dynamic systems without
external inputs. They require less data for training since Ordinary
Differential Equations (ODEs) of the considered system help to regularize the
ESN. In this work, the PI-ESN is extended with external inputs to model
controllable nonlinear dynamic systems. Additionally, an existing self-adaptive
balancing loss method is employed to balance the contributions of the residual
regression term and the physics-informed loss term in the total loss function.
The experiments with two nonlinear systems modeled by ODEs, the Van der Pol
oscillator and the four-tank system, and with one differential-algebraic (DAE)
system, an electric submersible pump, revealed that the proposed PI-ESN
outperforms the conventional ESN, especially in scenarios with limited data
availability, showing that PI-ESNs can regularize an ESN model with external
inputs previously trained on just a few datapoints, reducing its overfitting
and improving its generalization error (up to 92% relative reduction in the
test error). Further experiments demonstrated that the proposed PI-ESN is
robust to parametric uncertainties in the ODE equations and that model
predictive control using PI-ESN outperforms the one using plain ESN,
particularly when training data is scarce.",2024-09-27,"Eric Mochiutti, Eric Aislan Antonelo, Eduardo Camponogara",http://arxiv.org/pdf/2409.19140v2,cs.LG
Sequencing the Neurome: Towards Scalable Exact Parameter Reconstruction of Black-Box Neural Networks,"Inferring the exact parameters of a neural network with only query access is
an NP-Hard problem, with few practical existing algorithms. Solutions would
have major implications for security, verification, interpretability, and
understanding biological networks. The key challenges are the massive parameter
space, and complex non-linear relationships between neurons. We resolve these
challenges using two insights. First, we observe that almost all networks used
in practice are produced by random initialization and first order optimization,
an inductive bias that drastically reduces the practical parameter space.
Second, we present a novel query generation algorithm that produces maximally
informative samples, letting us untangle the non-linear relationships
efficiently. We demonstrate reconstruction of a hidden network containing over
1.5 million parameters, and of one 7 layers deep, the largest and deepest
reconstructions to date, with max parameter difference less than 0.0001, and
illustrate robustness and scalability across a variety of architectures,
datasets, and training procedures.",2024-09-27,"Judah Goldfeder, Quinten Roets, Gabe Guo, John Wright, Hod Lipson",http://arxiv.org/pdf/2409.19138v1,cs.LG
Kinematic Detection of Anomalies in Human Trajectory Data,"Historically, much of the research in understanding, modeling, and mining
human trajectory data has focused on where an individual stays. Thus, the focus
of existing research has been on where a user goes. On the other hand, the
study of how a user moves between locations has great potential for new
research opportunities. Kinematic features describe how an individual moves
between locations and can be used for tasks such as identification of
individuals or anomaly detection. Unfortunately, data availability and quality
challenges make kinematic trajectory mining difficult. In this paper, we
leverage the Geolife dataset of human trajectories to investigate the viability
of using kinematic features to identify individuals and detect anomalies. We
show that humans have an individual ""kinematic profile"" which can be used as a
strong signal to identify individual humans. We experimentally show that, for
the two use-cases of individual identification and anomaly detection, simple
kinematic features fed to standard classification and anomaly detection
algorithms significantly improve results.",2024-09-27,"Lance Kennedy, Andreas Züfle",http://arxiv.org/pdf/2409.19136v1,cs.LG
Looking through the mind's eye via multimodal encoder-decoder networks,"In this work, we explore the decoding of mental imagery from subjects using
their fMRI measurements. In order to achieve this decoding, we first created a
mapping between a subject's fMRI signals elicited by the videos the subjects
watched. This mapping associates the high dimensional fMRI activation states
with visual imagery. Next, we prompted the subjects textually, primarily with
emotion labels which had no direct reference to visual objects. Then to decode
visual imagery that may have been in a person's mind's eye, we align a latent
representation of these fMRI measurements with a corresponding video-fMRI based
on textual labels given to the videos themselves. This alignment has the effect
of overlapping the video fMRI embedding with the text-prompted fMRI embedding,
thus allowing us to use our fMRI-to-video mapping to decode. Additionally, we
enhance an existing fMRI dataset, initially consisting of data from five
subjects, by including recordings from three more subjects gathered by our
team. We demonstrate the efficacy of our model on this augmented dataset both
in accurately creating a mapping, as well as in plausibly decoding mental
imagery.",2024-09-27,"Arman Afrasiyabi, Erica Busch, Rahul Singh, Dhananjay Bhaskar, Laurent Caplette, Nicholas Turk-Browne, Smita Krishnaswamy",http://arxiv.org/pdf/2410.00047v1,cs.LG
Chebyshev Feature Neural Network for Accurate Function Approximation,"We present a new Deep Neural Network (DNN) architecture capable of
approximating functions up to machine accuracy. Termed Chebyshev Feature Neural
Network (CFNN), the new structure employs Chebyshev functions with learnable
frequencies as the first hidden layer, followed by the standard fully connected
hidden layers. The learnable frequencies of the Chebyshev layer are initialized
with exponential distributions to cover a wide range of frequencies. Combined
with a multi-stage training strategy, we demonstrate that this CFNN structure
can achieve machine accuracy during training. A comprehensive set of numerical
examples for dimensions up to $20$ are provided to demonstrate the
effectiveness and scalability of the method.",2024-09-27,"Zhongshu Xu, Yuan Chen, Dongbin Xiu",http://arxiv.org/pdf/2409.19135v2,cs.LG
From Vision to Audio and Beyond: A Unified Model for Audio-Visual Representation and Generation,"Video encompasses both visual and auditory data, creating a perceptually rich
experience where these two modalities complement each other. As such, videos
are a valuable type of media for the investigation of the interplay between
audio and visual elements. Previous studies of audio-visual modalities
primarily focused on either audio-visual representation learning or generative
modeling of a modality conditioned on the other, creating a disconnect between
these two branches. A unified framework that learns representation and
generates modalities has not been developed yet. In this work, we introduce a
novel framework called Vision to Audio and Beyond (VAB) to bridge the gap
between audio-visual representation learning and vision-to-audio generation.
The key approach of VAB is that rather than working with raw video frames and
audio data, VAB performs representation learning and generative modeling within
latent spaces. In particular, VAB uses a pre-trained audio tokenizer and an
image encoder to obtain audio tokens and visual features, respectively. It then
performs the pre-training task of visual-conditioned masked audio token
prediction. This training strategy enables the model to engage in contextual
learning and simultaneous video-to-audio generation. After the pre-training
phase, VAB employs the iterative-decoding approach to rapidly generate audio
tokens conditioned on visual features. Since VAB is a unified model, its
backbone can be fine-tuned for various audio-visual downstream tasks. Our
experiments showcase the efficiency of VAB in producing high-quality audio from
video, and its capability to acquire semantic audio-visual features, leading to
competitive results in audio-visual retrieval and classification.",2024-09-27,"Kun Su, Xiulong Liu, Eli Shlizerman",http://arxiv.org/pdf/2409.19132v1,cs.LG
Multi-modal Cross-domain Self-supervised Pre-training for fMRI and EEG Fusion,"Neuroimaging techniques including functional magnetic resonance imaging
(fMRI) and electroencephalogram (EEG) have shown promise in detecting
functional abnormalities in various brain disorders. However, existing studies
often focus on a single domain or modality, neglecting the valuable
complementary information offered by multiple domains from both fMRI and EEG,
which is crucial for a comprehensive representation of disorder pathology. This
limitation poses a challenge in effectively leveraging the synergistic
information derived from these modalities. To address this, we propose a
Multi-modal Cross-domain Self-supervised Pre-training Model (MCSP), a novel
approach that leverages self-supervised learning to synergize multi-modal
information across spatial, temporal, and spectral domains. Our model employs
cross-domain self-supervised loss that bridges domain differences by
implementing domain-specific data augmentation and contrastive loss, enhancing
feature discrimination. Furthermore, MCSP introduces cross-modal
self-supervised loss to capitalize on the complementary information of fMRI and
EEG, facilitating knowledge distillation within domains and maximizing
cross-modal feature convergence. We constructed a large-scale pre-training
dataset and pretrained MCSP model by leveraging proposed self-supervised
paradigms to fully harness multimodal neuroimaging data. Through comprehensive
experiments, we have demonstrated the superior performance and generalizability
of our model on multiple classification tasks. Our study contributes a
significant advancement in the fusion of fMRI and EEG, marking a novel
integration of cross-domain features, which enriches the existing landscape of
neuroimaging research, particularly within the context of mental disorder
studies.",2024-09-27,"Xinxu Wei, Kanhao Zhao, Yong Jiao, Nancy B. Carlisle, Hua Xie, Gregory A. Fonzo, Yu Zhang",http://arxiv.org/pdf/2409.19130v1,cs.LG
Range-aware Positional Encoding via High-order Pretraining: Theory and Practice,"Unsupervised pre-training on vast amounts of graph data is critical in
real-world applications wherein labeled data is limited, such as molecule
properties prediction or materials science. Existing approaches pre-train
models for specific graph domains, neglecting the inherent connections within
networks. This limits their ability to transfer knowledge to various supervised
tasks. In this work, we propose a novel pre-training strategy on graphs that
focuses on modeling their multi-resolution structural information, allowing us
to capture global information of the whole graph while preserving local
structures around its nodes. We extend the work of Wave}let Positional Encoding
(WavePE) from (Ngo et al., 2023) by pretraining a High-Order
Permutation-Equivariant Autoencoder (HOPE-WavePE) to reconstruct node
connectivities from their multi-resolution wavelet signals. Unlike existing
positional encodings, our method is designed to become sensitivity to the input
graph size in downstream tasks, which efficiently capture global structure on
graphs. Since our approach relies solely on the graph structure, it is also
domain-agnostic and adaptable to datasets from various domains, therefore
paving the wave for developing general graph structure encoders and graph
foundation models. We theoretically demonstrate that there exists a
parametrization of such architecture that it can predict the output adjacency
up to arbitrarily low error. We also evaluate HOPE-WavePE on graph-level
prediction tasks of different areas and show its superiority compared to other
methods.",2024-09-27,"Viet Anh Nguyen, Nhat Khang Ngo, Truong Son Hy",http://arxiv.org/pdf/2409.19117v1,cs.LG
Mixture of Multicenter Experts in Multimodal Generative AI for Advanced Radiotherapy Target Delineation,"Clinical experts employ diverse philosophies and strategies in patient care,
influenced by regional patient populations. However, existing medical
artificial intelligence (AI) models are often trained on data distributions
that disproportionately reflect highly prevalent patterns, reinforcing biases
and overlooking the diverse expertise of clinicians. To overcome this
limitation, we introduce the Mixture of Multicenter Experts (MoME) approach.
This method strategically integrates specialized expertise from diverse
clinical strategies, enhancing the AI model's ability to generalize and adapt
across multiple medical centers. The MoME-based multimodal target volume
delineation model, trained with few-shot samples including images and clinical
notes from each medical center, outperformed baseline methods in prostate
cancer radiotherapy target delineation. The advantages of MoME were most
pronounced when data characteristics varied across centers or when data
availability was limited, demonstrating its potential for broader clinical
applications. Therefore, the MoME framework enables the deployment of AI-based
target volume delineation models in resource-constrained medical facilities by
adapting to specific preferences of each medical center only using a few sample
data, without the need for data sharing between institutions. Expanding the
number of multicenter experts within the MoME framework will significantly
enhance the generalizability, while also improving the usability and
adaptability of clinical AI applications in the field of precision radiation
oncology.",2024-09-27,"Yujin Oh, Sangjoon Park, Xiang Li, Wang Yi, Jonathan Paly, Jason Efstathiou, Annie Chan, Jun Won Kim, Hwa Kyung Byun, Ik Jae Lee, Jaeho Cho, Chan Woo Wee, Peng Shu, Peilong Wang, Nathan Yu, Jason Holmes, Jong Chul Ye, Quanzheng Li, Wei Liu, Woong Sub Koom, Jin Sung Kim, Kyungsang Kim",http://arxiv.org/pdf/2410.00046v2,cs.LG
Implementing NLPs in industrial process modeling: Addressing Categorical Variables,"Important variables of processes are often categorical, i.e. names or labels
representing, e.g. categories of inputs, or types of reactors or a sequence of
steps. In this work, we use Natural Language Processing Models to derive
embeddings of such inputs that represent their actual meaning, or reflect the
""distances"" between categories, i.e. how similar or dissimilar they are. This
is a marked difference from the current standard practice of using binary, or
one-hot encoding to replace categorical variables with sequences of ones and
zeros. Combined with dimensionality reduction techniques, either linear such as
Principal Component Analysis, or nonlinear such as Uniform Manifold
Approximation and Projection, the proposed approach leads to a meaningful,
low-dimensional feature space. The significance of obtaining meaningful
embeddings is illustrated in the context of an industrial coating process for
cutting tools that includes both numerical and categorical inputs. In this
industrial process, subject matter expertise suggests that the categorical
inputs are critical for determining the final outcome but this cannot be taken
into account with the current state-of-the-art. The proposed approach enables
feature importance which is a marked improvement compared to the current
state-of-the-art in the encoding of categorical variables. The proposed
approach is not limited to the case-study presented here and is suitable for
applications with similar mix of categorical and numerical critical inputs.",2024-09-27,"Eleni D. Koronaki, Geremy Loachamin Suntaxi, Paris Papavasileiou, Dimitrios G. Giovanis, Martin Kathrein, Andreas G. Boudouvis, Stéphane P. A. Bordas",http://arxiv.org/pdf/2409.19097v2,cs.LG
Enhancing Robustness of Graph Neural Networks through p-Laplacian,"With the increase of data in day-to-day life, businesses and different
stakeholders need to analyze the data for better predictions. Traditionally,
relational data has been a source of various insights, but with the increase in
computational power and the need to understand deeper relationships between
entities, the need to design new techniques has arisen. For this graph data
analysis has become an extraordinary tool for understanding the data, which
reveals more realistic and flexible modelling of complex relationships.
Recently, Graph Neural Networks (GNNs) have shown great promise in various
applications, such as social network analysis, recommendation systems, drug
discovery, and more. However, many adversarial attacks can happen over the
data, whether during training (poisoning attack) or during testing (evasion
attack), which can adversely manipulate the desired outcome from the GNN model.
Therefore, it is crucial to make the GNNs robust to such attacks. The existing
robustness methods are computationally demanding and perform poorly when the
intensity of attack increases. This paper presents a computationally efficient
framework, namely, pLapGNN, based on weighted p-Laplacian for making GNNs
robust. Empirical evaluation on real datasets establishes the efficacy and
efficiency of the proposed method.",2024-09-27,"Anuj Kumar Sirohi, Subhanu Halder, Kabir Kumar, Sandeep Kumar",http://arxiv.org/pdf/2409.19096v1,cs.LG
Federated Online Prediction from Experts with Differential Privacy: Separations and Regret Speed-ups,"We study the problems of differentially private federated online prediction
from experts against both stochastic adversaries and oblivious adversaries. We
aim to minimize the average regret on $m$ clients working in parallel over time
horizon $T$ with explicit differential privacy (DP) guarantees. With stochastic
adversaries, we propose a Fed-DP-OPE-Stoch algorithm that achieves
$\sqrt{m}$-fold speed-up of the per-client regret compared to the single-player
counterparts under both pure DP and approximate DP constraints, while
maintaining logarithmic communication costs. With oblivious adversaries, we
establish non-trivial lower bounds indicating that collaboration among clients
does not lead to regret speed-up with general oblivious adversaries. We then
consider a special case of the oblivious adversaries setting, where there
exists a low-loss expert. We design a new algorithm Fed-SVT and show that it
achieves an $m$-fold regret speed-up under both pure DP and approximate DP
constraints over the single-player counterparts. Our lower bound indicates that
Fed-SVT is nearly optimal up to logarithmic factors. Experiments demonstrate
the effectiveness of our proposed algorithms. To the best of our knowledge,
this is the first work examining the differentially private online prediction
from experts in the federated setting.",2024-09-27,"Fengyu Gao, Ruiquan Huang, Jing Yang",http://arxiv.org/pdf/2409.19092v1,cs.LG
DANA: Domain-Aware Neurosymbolic Agents for Consistency and Accuracy,"Large Language Models (LLMs) have shown remarkable capabilities, but their
inherent probabilistic nature often leads to inconsistency and inaccuracy in
complex problem-solving tasks. This paper introduces DANA (Domain-Aware
Neurosymbolic Agent), an architecture that addresses these issues by
integrating domain-specific knowledge with neurosymbolic approaches. We begin
by analyzing current AI architectures, including AutoGPT, LangChain ReAct and
OpenAI's ChatGPT, through a neurosymbolic lens, highlighting how their reliance
on probabilistic inference contributes to inconsistent outputs. In response,
DANA captures and applies domain expertise in both natural-language and
symbolic forms, enabling more deterministic and reliable problem-solving
behaviors. We implement a variant of DANA using Hierarchical Task Plans (HTPs)
in the open-source OpenSSA framework. This implementation achieves over 90\%
accuracy on the FinanceBench financial-analysis benchmark, significantly
outperforming current LLM-based systems in both consistency and accuracy.
Application of DANA in physical industries such as semiconductor shows that its
flexible architecture for incorporating knowledge is effective in mitigating
the probabilistic limitations of LLMs and has potential in tackling complex,
real-world problems that require reliability and precision.",2024-09-27,"Vinh Luong, Sang Dinh, Shruti Raghavan, William Nguyen, Zooey Nguyen, Quynh Le, Hung Vo, Kentaro Maegaito, Loc Nguyen, Thao Nguyen, Anh Hai Ha, Christopher Nguyen",http://arxiv.org/pdf/2410.02823v1,cs.LG
Differential privacy enables fair and accurate AI-based analysis of speech disorders while protecting patient data,"Speech pathology has impacts on communication abilities and quality of life.
While deep learning-based models have shown potential in diagnosing these
disorders, the use of sensitive data raises critical privacy concerns. Although
differential privacy (DP) has been explored in the medical imaging domain, its
application in pathological speech analysis remains largely unexplored despite
the equally critical privacy concerns. This study is the first to investigate
DP's impact on pathological speech data, focusing on the trade-offs between
privacy, diagnostic accuracy, and fairness. Using a large, real-world dataset
of 200 hours of recordings from 2,839 German-speaking participants, we observed
a maximum accuracy reduction of 3.85% when training with DP with high privacy
levels. To highlight real-world privacy risks, we demonstrated the
vulnerability of non-private models to explicit gradient inversion attacks,
reconstructing identifiable speech samples and showcasing DP's effectiveness in
mitigating these risks. To generalize our findings across languages and
disorders, we validated our approach on a dataset of Spanish-speaking
Parkinson's disease patients, leveraging pretrained models from healthy
English-speaking datasets, and demonstrated that careful pretraining on
large-scale task-specific datasets can maintain favorable accuracy under DP
constraints. A comprehensive fairness analysis revealed minimal gender bias at
reasonable privacy levels but underscored the need for addressing age-related
disparities. Our results establish that DP can balance privacy and utility in
speech disorder detection, while highlighting unique challenges in
privacy-fairness trade-offs for speech data. This provides a foundation for
refining DP methodologies and improving fairness across diverse patient groups
in real-world deployments.",2024-09-27,"Soroosh Tayebi Arasteh, Mahshad Lotfinia, Paula Andrea Perez-Toro, Tomas Arias-Vergara, Mahtab Ranji, Juan Rafael Orozco-Arroyave, Maria Schuster, Andreas Maier, Seung Hee Yang",http://arxiv.org/pdf/2409.19078v2,cs.LG
Localizing Memorization in SSL Vision Encoders,"Recent work on studying memorization in self-supervised learning (SSL)
suggests that even though SSL encoders are trained on millions of images, they
still memorize individual data points. While effort has been put into
characterizing the memorized data and linking encoder memorization to
downstream utility, little is known about where the memorization happens inside
SSL encoders. To close this gap, we propose two metrics for localizing
memorization in SSL encoders on a per-layer (layermem) and per-unit basis
(unitmem). Our localization methods are independent of the downstream task, do
not require any label information, and can be performed in a forward pass. By
localizing memorization in various encoder architectures (convolutional and
transformer-based) trained on diverse datasets with contrastive and
non-contrastive SSL frameworks, we find that (1) while SSL memorization
increases with layer depth, highly memorizing units are distributed across the
entire encoder, (2) a significant fraction of units in SSL encoders experiences
surprisingly high memorization of individual data points, which is in contrast
to models trained under supervision, (3) atypical (or outlier) data points
cause much higher layer and unit memorization than standard data points, and
(4) in vision transformers, most memorization happens in the fully-connected
layers. Finally, we show that localizing memorization in SSL has the potential
to improve fine-tuning and to inform pruning strategies.",2024-09-27,"Wenhao Wang, Adam Dziedzic, Michael Backes, Franziska Boenisch",http://arxiv.org/pdf/2409.19069v3,cs.LG
CURATE: Scaling-up Differentially Private Causal Graph Discovery,"Causal Graph Discovery (CGD) is the process of estimating the underlying
probabilistic graphical model that represents joint distribution of features of
a dataset. CGD-algorithms are broadly classified into two categories: (i)
Constraint-based algorithms (outcome depends on conditional independence (CI)
tests), (ii) Score-based algorithms (outcome depends on optimized
score-function). Since, sensitive features of observational data is prone to
privacy-leakage, Differential Privacy (DP) has been adopted to ensure user
privacy in CGD. Adding same amount of noise in this sequential-natured
estimation process affects the predictive performance of the algorithms. As
initial CI tests in constraint-based algorithms and later iterations of the
optimization process of score-based algorithms are crucial, they need to be
more accurate, less noisy. Based on this key observation, we present CURATE
(CaUsal gRaph AdapTivE privacy), a DP-CGD framework with adaptive privacy
budgeting. In contrast to existing DP-CGD algorithms with uniform privacy
budgeting across all iterations, CURATE allows adaptive privacy budgeting by
minimizing error probability (for constraint-based), maximizing iterations of
the optimization problem (for score-based) while keeping the cumulative leakage
bounded. To validate our framework, we present a comprehensive set of
experiments on several datasets and show that CURATE achieves higher utility
compared to existing DP-CGD algorithms with less privacy-leakage.",2024-09-27,"Payel Bhattacharjee, Ravi Tandon",http://arxiv.org/pdf/2409.19060v1,cs.LG
CLLMate: A Multimodal Benchmark for Weather and Climate Events Forecasting,"Forecasting weather and climate events is crucial for making appropriate
measures to mitigate environmental hazards and minimize losses. However,
existing environmental forecasting research focuses narrowly on predicting
numerical meteorological variables (e.g., temperature), neglecting the
translation of these variables into actionable textual narratives of events and
their consequences. To bridge this gap, we proposed Weather and Climate Event
Forecasting (WCEF), a new task that leverages numerical meteorological raster
data and textual event data to predict weather and climate events. This task is
challenging to accomplish due to difficulties in aligning multimodal data and
the lack of supervised datasets. To address these challenges, we present
CLLMate, the first multimodal dataset for WCEF, using 26,156 environmental news
articles aligned with ERA5 reanalysis data. We systematically benchmark 23
existing MLLMs on CLLMate, including closed-source, open-source, and our
fine-tuned models. Our experiments reveal the advantages and limitations of
existing MLLMs and the value of CLLMate for the training and benchmarking of
the WCEF task.",2024-09-27,"Haobo Li, Zhaowei Wang, Jiachen Wang, Yueya Wang, Alexis Kai Hon Lau, Huamin Qu",http://arxiv.org/pdf/2409.19058v2,cs.LG
PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation,"We present PhysGen, a novel image-to-video generation method that converts a
single image and an input condition (e.g., force and torque applied to an
object in the image) to produce a realistic, physically plausible, and
temporally consistent video. Our key insight is to integrate model-based
physical simulation with a data-driven video generation process, enabling
plausible image-space dynamics. At the heart of our system are three core
components: (i) an image understanding module that effectively captures the
geometry, materials, and physical parameters of the image; (ii) an image-space
dynamics simulation model that utilizes rigid-body physics and inferred
parameters to simulate realistic behaviors; and (iii) an image-based rendering
and refinement module that leverages generative video diffusion to produce
realistic video footage featuring the simulated motion. The resulting videos
are realistic in both physics and appearance and are even precisely
controllable, showcasing superior results over existing data-driven
image-to-video generation works through quantitative comparison and
comprehensive user study. PhysGen's resulting videos can be used for various
downstream applications, such as turning an image into a realistic animation or
allowing users to interact with the image and create various dynamics. Project
page: https://stevenlsw.github.io/physgen/",2024-09-27,"Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, Shenlong Wang",http://arxiv.org/pdf/2409.18964v1,cs.LG
Exploring Token Pruning in Vision State Space Models,"State Space Models (SSMs) have the advantage of keeping linear computational
complexity compared to attention modules in transformers, and have been applied
to vision tasks as a new type of powerful vision foundation model. Inspired by
the observations that the final prediction in vision transformers (ViTs) is
only based on a subset of most informative tokens, we take the novel step of
enhancing the efficiency of SSM-based vision models through token-based
pruning. However, direct applications of existing token pruning techniques
designed for ViTs fail to deliver good performance, even with extensive
fine-tuning. To address this issue, we revisit the unique computational
characteristics of SSMs and discover that naive application disrupts the
sequential token positions. This insight motivates us to design a novel and
general token pruning method specifically for SSM-based vision models. We first
introduce a pruning-aware hidden state alignment method to stabilize the
neighborhood of remaining tokens for performance enhancement. Besides, based on
our detailed analysis, we propose a token importance evaluation method adapted
for SSM models, to guide the token pruning. With efficient implementation and
practical acceleration methods, our method brings actual speedup. Extensive
experiments demonstrate that our approach can achieve significant computation
reduction with minimal impact on performance across different tasks. Notably,
we achieve 81.7\% accuracy on ImageNet with a 41.6\% reduction in the FLOPs for
pruned PlainMamba-L3. Furthermore, our work provides deeper insights into
understanding the behavior of SSM-based vision models for future research.",2024-09-27,"Zheng Zhan, Zhenglun Kong, Yifan Gong, Yushu Wu, Zichong Meng, Hangyu Zheng, Xuan Shen, Stratis Ioannidis, Wei Niu, Pu Zhao, Yanzhi Wang",http://arxiv.org/pdf/2409.18962v1,cs.LG
O(d/T) Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions,"Score-based diffusion models, which generate new data by learning to reverse
a diffusion process that perturbs data from the target distribution into noise,
have achieved remarkable success across various generative tasks. Despite their
superior empirical performance, existing theoretical guarantees are often
constrained by stringent assumptions or suboptimal convergence rates. In this
paper, we establish a fast convergence theory for the denoising diffusion
probabilistic model (DDPM), a widely used SDE-based sampler, under minimal
assumptions. Our analysis shows that, provided $\ell_{2}$-accurate estimates of
the score functions, the total variation distance between the target and
generated distributions is upper bounded by $O(d/T)$ (ignoring logarithmic
factors), where $d$ is the data dimensionality and $T$ is the number of steps.
This result holds for any target distribution with finite first-order moment.
Moreover, we show that with careful coefficient design, the convergence rate
improves to $O(k/T)$, where $k$ is the intrinsic dimension of the target data
distribution. This highlights the ability of DDPM to automatically adapt to
unknown low-dimensional structures, a common feature of natural image
distributions. These results are achieved through a novel set of analytical
tools that provides a fine-grained characterization of how the error propagates
at each step of the reverse process.",2024-09-27,"Gen Li, Yuling Yan",http://arxiv.org/pdf/2409.18959v2,cs.LG
LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction,"Classification tasks are typically handled using Machine Learning (ML)
models, which lack a balance between accuracy and interpretability. This paper
introduces a new approach for classification tasks using Large Language Models
(LLMs) in an explainable method. Unlike ML models, which rely heavily on data
cleaning and feature engineering, this method streamlines the process using
LLMs. This paper proposes a method called ""Language Model Learning (LML)""
powered by a new method called ""Data-Augmented Prediction (DAP)."" The
classification is performed by LLMs using a method similar to that used by
humans who manually explore and understand the data to decide classifications.
In the process of LML, a dataset is summarized and evaluated to determine the
features leading to each label the most. In the DAP process, the system uses
the data summary and a row of the testing dataset to automatically generate a
query to retrieve relevant rows from the dataset for context-aware
classification. LML and DAP unlock new possibilities in areas that require
explainable and context-aware decisions by ensuring satisfactory accuracy even
with complex data. The system scored an accuracy above 90% in some test cases,
confirming the effectiveness and potential of the system to outperform ML
models in various scenarios. The source code is available at
https://github.com/Pro-GenAI/LML-DAP",2024-09-27,Praneeth Vadlapati,http://arxiv.org/pdf/2409.18957v3,cs.LG
On the Inductive Bias of Stacking Towards Improving Reasoning,"Given the increasing scale of model sizes, novel training strategies like
gradual stacking [Gong et al., 2019, Reddi et al., 2023] have garnered
interest. Stacking enables efficient training by gradually growing the depth of
a model in stages and using layers from a smaller model in an earlier stage to
initialize the next stage. Although efficient for training, the model biases
induced by such growing approaches are largely unexplored. In this work, we
examine this fundamental aspect of gradual stacking, going beyond its
efficiency benefits. We propose a variant of gradual stacking called MIDAS that
can speed up language model training by up to 40%. Furthermore we discover an
intriguing phenomenon: MIDAS is not only training-efficient but surprisingly
also has an inductive bias towards improving downstream tasks, especially tasks
that require reasoning abilities like reading comprehension and math problems,
despite having similar or slightly worse perplexity compared to baseline
training. To further analyze this inductive bias, we construct reasoning
primitives -- simple synthetic tasks that are building blocks for reasoning --
and find that a model pretrained with stacking is significantly better than
standard pretraining on these primitives, with and without fine-tuning. This
provides stronger and more robust evidence for this inductive bias towards
reasoning. These findings of training efficiency and inductive bias towards
reasoning are verified at 1B, 2B and 8B parameter language models. Finally, we
conjecture the underlying reason for this inductive bias by exploring the
connection of stacking to looped models and provide strong supporting empirical
analysis.",2024-09-27,"Nikunj Saunshi, Stefani Karp, Shankar Krishnan, Sobhan Miryoosefi, Sashank J. Reddi, Sanjiv Kumar",http://arxiv.org/pdf/2409.19044v1,cs.LG
RepairBench: Leaderboard of Frontier Models for Program Repair,"AI-driven program repair uses AI models to repair buggy software by producing
patches. Rapid advancements in AI surely impact state-of-the-art performance of
program repair. Yet, grasping this progress requires frequent and standardized
evaluations. We propose RepairBench, a novel leaderboard for AI-driven program
repair. The key characteristics of RepairBench are: 1) it is execution-based:
all patches are compiled and executed against a test suite, 2) it assesses
frontier models in a frequent and standardized way. RepairBench leverages two
high-quality benchmarks, Defects4J and GitBug-Java, to evaluate frontier models
against real-world program repair tasks. We publicly release the evaluation
framework of RepairBench. We will update the leaderboard as new frontier models
are released.",2024-09-27,"André Silva, Martin Monperrus",http://arxiv.org/pdf/2409.18952v1,cs.LG
Spectral Wavelet Dropout: Regularization in the Wavelet Domain,"Regularization techniques help prevent overfitting and therefore improve the
ability of convolutional neural networks (CNNs) to generalize. One reason for
overfitting is the complex co-adaptations among different parts of the network,
which make the CNN dependent on their joint response rather than encouraging
each part to learn a useful feature representation independently. Frequency
domain manipulation is a powerful strategy for modifying data that has temporal
and spatial coherence by utilizing frequency decomposition. This work
introduces Spectral Wavelet Dropout (SWD), a novel regularization method that
includes two variants: 1D-SWD and 2D-SWD. These variants improve CNN
generalization by randomly dropping detailed frequency bands in the discrete
wavelet decomposition of feature maps. Our approach distinguishes itself from
the pre-existing Spectral ""Fourier"" Dropout (2D-SFD), which eliminates
coefficients in the Fourier domain. Notably, SWD requires only a single
hyperparameter, unlike the two required by SFD. We also extend the literature
by implementing a one-dimensional version of Spectral ""Fourier"" Dropout
(1D-SFD), setting the stage for a comprehensive comparison. Our evaluation
shows that both 1D and 2D SWD variants have competitive performance on
CIFAR-10/100 benchmarks relative to both 1D-SFD and 2D-SFD. Specifically,
1D-SWD has a significantly lower computational complexity compared to
1D/2D-SFD. In the Pascal VOC Object Detection benchmark, SWD variants surpass
1D-SFD and 2D-SFD in performance and demonstrate lower computational complexity
during training.",2024-09-27,"Rinor Cakaj, Jens Mehnert, Bin Yang",http://arxiv.org/pdf/2409.18951v1,cs.LG
Unconditional stability of a recurrent neural circuit implementing divisive normalization,"Stability in recurrent neural models poses a significant challenge,
particularly in developing biologically plausible neurodynamical models that
can be seamlessly trained. Traditional cortical circuit models are notoriously
difficult to train due to expansive nonlinearities in the dynamical system,
leading to an optimization problem with nonlinear stability constraints that
are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in
tasks involving sequential data but lack biological plausibility and
interpretability. In this work, we address these challenges by linking dynamic
divisive normalization (DN) to the stability of ORGaNICs, a biologically
plausible recurrent cortical circuit model that dynamically achieves DN and
that has been shown to simulate a wide range of neurophysiological phenomena.
By using the indirect method of Lyapunov, we prove the remarkable property of
unconditional local stability for an arbitrary-dimensional ORGaNICs circuit
when the recurrent weight matrix is the identity. We thus connect ORGaNICs to a
system of coupled damped harmonic oscillators, which enables us to derive the
circuit's energy function, providing a normative principle of what the circuit,
and individual neurons, aim to accomplish. Further, for a generic recurrent
weight matrix, we prove the stability of the 2D model and demonstrate
empirically that stability holds in higher dimensions. Finally, we show that
ORGaNICs can be trained by backpropagation through time without gradient
clipping/scaling, thanks to its intrinsic stability property and adaptive time
constants, which address the problems of exploding, vanishing, and oscillating
gradients. By evaluating the model's performance on RNN benchmarks, we find
that ORGaNICs outperform alternative neurodynamical models on static image
classification tasks and perform comparably to LSTMs on sequential tasks.",2024-09-27,"Shivang Rawat, David J. Heeger, Stefano Martiniani",http://arxiv.org/pdf/2409.18946v3,cs.LG
Volatility Forecasting in Global Financial Markets Using TimeMixer,"Predicting volatility in financial markets, including stocks, index ETFs,
foreign exchange, and cryptocurrencies, remains a challenging task due to the
inherent complexity and non-linear dynamics of these time series. In this
study, I apply TimeMixer, a state-of-the-art time series forecasting model, to
predict the volatility of global financial assets. TimeMixer utilizes a
multiscale-mixing approach that effectively captures both short-term and
long-term temporal patterns by analyzing data across different scales. My
empirical results reveal that while TimeMixer performs exceptionally well in
short-term volatility forecasting, its accuracy diminishes for longer-term
predictions, particularly in highly volatile markets. These findings highlight
TimeMixer's strength in capturing short-term volatility, making it highly
suitable for practical applications in financial risk management, where precise
short-term forecasts are critical. However, the model's limitations in
long-term forecasting point to potential areas for further refinement.",2024-09-27,Alex Li,http://arxiv.org/pdf/2410.09062v1,cs.LG
A-FedPD: Aligning Dual-Drift is All Federated Primal-Dual Learning Needs,"As a popular paradigm for juggling data privacy and collaborative training,
federated learning (FL) is flourishing to distributively process the large
scale of heterogeneous datasets on edged clients. Due to bandwidth limitations
and security considerations, it ingeniously splits the original problem into
multiple subproblems to be solved in parallel, which empowers primal dual
solutions to great application values in FL. In this paper, we review the
recent development of classical federated primal dual methods and point out a
serious common defect of such methods in non-convex scenarios, which we say is
a ""dual drift"" caused by dual hysteresis of those longstanding inactive clients
under partial participation training. To further address this problem, we
propose a novel Aligned Federated Primal Dual (A-FedPD) method, which
constructs virtual dual updates to align global consensus and local dual
variables for those protracted unparticipated local clients. Meanwhile, we
provide a comprehensive analysis of the optimization and generalization
efficiency for the A-FedPD method on smooth non-convex objectives, which
confirms its high efficiency and practicality. Extensive experiments are
conducted on several classical FL setups to validate the effectiveness of our
proposed method.",2024-09-27,"Yan Sun, Li Shen, Dacheng Tao",http://arxiv.org/pdf/2409.18915v2,cs.LG
Best Arm Identification with Minimal Regret,"Motivated by real-world applications that necessitate responsible
experimentation, we introduce the problem of best arm identification (BAI) with
minimal regret. This innovative variant of the multi-armed bandit problem
elegantly amalgamates two of its most ubiquitous objectives: regret
minimization and BAI. More precisely, the agent's goal is to identify the best
arm with a prescribed confidence level $\delta$, while minimizing the
cumulative regret up to the stopping time. Focusing on single-parameter
exponential families of distributions, we leverage information-theoretic
techniques to establish an instance-dependent lower bound on the expected
cumulative regret. Moreover, we present an intriguing impossibility result that
underscores the tension between cumulative regret and sample complexity in
fixed-confidence BAI. Complementarily, we design and analyze the Double KL-UCB
algorithm, which achieves asymptotic optimality as the confidence level tends
to zero. Notably, this algorithm employs two distinct confidence bounds to
guide arm selection in a randomized manner. Our findings elucidate a fresh
perspective on the inherent connections between regret minimization and BAI.",2024-09-27,"Junwen Yang, Vincent Y. F. Tan, Tianyuan Jin",http://arxiv.org/pdf/2409.18909v1,cs.LG
In-depth Analysis of Privacy Threats in Federated Learning for Medical Data,"Federated learning is emerging as a promising machine learning technique in
the medical field for analyzing medical images, as it is considered an
effective method to safeguard sensitive patient data and comply with privacy
regulations. However, recent studies have revealed that the default settings of
federated learning may inadvertently expose private training data to privacy
attacks. Thus, the intensity of such privacy risks and potential mitigation
strategies in the medical domain remain unclear. In this paper, we make three
original contributions to privacy risk analysis and mitigation in federated
learning for medical data. First, we propose a holistic framework, MedPFL, for
analyzing privacy risks in processing medical data in the federated learning
environment and developing effective mitigation strategies for protecting
privacy. Second, through our empirical analysis, we demonstrate the severe
privacy risks in federated learning to process medical images, where
adversaries can accurately reconstruct private medical images by performing
privacy attacks. Third, we illustrate that the prevalent defense mechanism of
adding random noises may not always be effective in protecting medical images
against privacy attacks in federated learning, which poses unique and pressing
challenges related to protecting the privacy of medical data. Furthermore, the
paper discusses several unique research questions related to the privacy
protection of medical data in the federated learning environment. We conduct
extensive experiments on several benchmark medical image datasets to analyze
and mitigate the privacy risks associated with federated learning for medical
data.",2024-09-27,"Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu",http://arxiv.org/pdf/2409.18907v1,cs.LG
"Probabilistic Analysis of Least Squares, Orthogonal Projection, and QR Factorization Algorithms Subject to Gaussian Noise","In this paper, we extend the work of Liesen et al. (2002), which analyzes how
the condition number of an orthonormal matrix Q changes when a column is added
([Q, c]), particularly focusing on the perpendicularity of c to the span of Q.
Their result, presented in Theorem 2.3 of Liesen et al. (2002), assumes exact
arithmetic and orthonormality of Q, which is a strong assumption when applying
these results to numerical methods such as QR factorization algorithms. In our
work, we address this gap by deriving bounds on the condition number increase
for a matrix B without assuming perfect orthonormality, even when a column is
not perfectly orthogonal to the span of B. This framework allows us to analyze
QR factorization methods where orthogonalization is imperfect and subject to
Gaussian noise. We also provide results on the performance of orthogonal
projection and least squares under Gaussian noise, further supporting the
development of this theory.",2024-09-27,"Ali Lotfi, Julien Langou, Mohammad Meysami",http://arxiv.org/pdf/2409.18905v2,cs.LG
Multi-Source Hard and Soft Information Fusion Approach for Accurate Cryptocurrency Price Movement Prediction,"One of the most important challenges in the financial and cryptocurrency
field is accurately predicting cryptocurrency price trends. Leveraging
artificial intelligence (AI) is beneficial in addressing this challenge.
Cryptocurrency markets, marked by substantial growth and volatility, attract
investors and scholars keen on deciphering and forecasting cryptocurrency price
movements. The vast and diverse array of data available for such predictions
increases the complexity of the task. In our study, we introduce a novel
approach termed hard and soft information fusion (HSIF) to enhance the accuracy
of cryptocurrency price movement forecasts. The hard information component of
our approach encompasses historical price records alongside technical
indicators. Complementing this, the soft data component extracts from X
(formerly Twitter), encompassing news headlines and tweets about the
cryptocurrency. To use this data, we use the Bidirectional Encoder
Representations from Transformers (BERT)-based sentiment analysis method,
financial BERT (FinBERT), which performs best. Finally, our model feeds on the
information set including processed hard and soft data. We employ the
bidirectional long short-term memory (BiLSTM) model because processing
information in both forward and backward directions can capture long-term
dependencies in sequential information. Our empirical findings emphasize the
superiority of the HSIF approach over models dependent on single-source data by
testing on Bitcoin-related data. By fusing hard and soft information on Bitcoin
dataset, our model has about 96.8\% accuracy in predicting price movement.
Incorporating information enables our model to grasp the influence of social
sentiment on price fluctuations, thereby supplementing the technical
analysis-based predictions derived from hard information.",2024-09-27,"Saeed Mohammadi Dashtaki, Mehdi Hosseini Chagahi, Behzad Moshiri, Md. Jalil Piran",http://arxiv.org/pdf/2409.18895v1,cs.LG
HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models,"Model merging is a technique that combines multiple large pretrained models
into a single model with enhanced performance and broader task adaptability. It
has gained popularity in large pretrained model development due to its ability
to bypass the need for original training data and further training processes.
However, most existing model merging approaches focus solely on exploring the
parameter space, merging models with identical architectures. Merging within
the architecture space, despite its potential, remains in its early stages due
to the vast search space and the challenges of layer compatibility. This paper
marks a significant advance toward more flexible and comprehensive model
merging techniques by modeling the architecture-space merging process as a
reinforcement learning task. We train policy and value networks using offline
sampling of weight vectors, which are then employed for the online optimization
of merging strategies. Moreover, a multi-objective optimization paradigm is
introduced to accommodate users' diverse task preferences, learning the Pareto
front of optimal models to offer customized merging suggestions. Experimental
results across multiple tasks, including text translation, mathematical
reasoning, and code generation, validate the effectiveness and superiority of
the proposed framework in model merging. The code will be made publicly
available after the review process.",2024-09-27,"Yu Zhou, Xingyu Wu, Jibin Wu, Liang Feng, Kay Chen Tan",http://arxiv.org/pdf/2409.18893v1,cs.LG
HR-Extreme: A High-Resolution Dataset for Extreme Weather Forecasting,"The application of large deep learning models in weather forecasting has led
to significant advancements in the field, including higher-resolution
forecasting and extended prediction periods exemplified by models such as Pangu
and Fuxi. Despite these successes, previous research has largely been
characterized by the neglect of extreme weather events, and the availability of
datasets specifically curated for such events remains limited. Given the
critical importance of accurately forecasting extreme weather, this study
introduces a comprehensive dataset that incorporates high-resolution extreme
weather cases derived from the High-Resolution Rapid Refresh (HRRR) data, a
3-km real-time dataset provided by NOAA.
  We also evaluate the current state-of-the-art deep learning models and
Numerical Weather Prediction (NWP) systems on HR-Extreme, and provide a
improved baseline deep learning model called HR-Heim which has superior
performance on both general loss and HR-Extreme compared to others. Our results
reveal that the errors of extreme weather cases are significantly larger than
overall forecast error, highlighting them as an crucial source of loss in
weather prediction. These findings underscore the necessity for future research
to focus on improving the accuracy of extreme weather forecasts to enhance
their practical utility.",2024-09-27,"Nian Ran, Peng Xiao, Yue Wang, Wesley Shi, Jianxin Lin, Qi Meng, Richard Allmendinger",http://arxiv.org/pdf/2409.18885v2,cs.LG
CESNET-TimeSeries24: Time Series Dataset for Network Traffic Anomaly Detection and Forecasting,"Anomaly detection in network traffic is crucial for maintaining the security
of computer networks and identifying malicious activities. One of the primary
approaches to anomaly detection are methods based on forecasting. Nevertheless,
extensive real-world network datasets for forecasting and anomaly detection
techniques are missing, potentially causing performance overestimation of
anomaly detection algorithms. This manuscript addresses this gap by introducing
a dataset comprising time series data of network entities' behavior, collected
from the CESNET3 network. The dataset was created from 40 weeks of network
traffic of 275 thousand active IP addresses. The ISP origin of the presented
data ensures a high level of variability among network entities, which forms a
unique and authentic challenge for forecasting and anomaly detection models. It
provides valuable insights into the practical deployment of forecast-based
anomaly detection approaches.",2024-09-27,"Josef Koumar, Karel Hynek, Tomáš Čejka, Pavel Šiška",http://arxiv.org/pdf/2409.18874v1,cs.LG
Simulating Dynamic Tumor Contrast Enhancement in Breast MRI using Conditional Generative Adversarial Networks,"This paper presents a method for virtual contrast enhancement in breast MRI,
offering a promising non-invasive alternative to traditional contrast
agent-based DCE-MRI acquisition. Using a conditional generative adversarial
network, we predict DCE-MRI images, including jointly-generated sequences of
multiple corresponding DCE-MRI timepoints, from non-contrast-enhanced MRIs,
enabling tumor localization and characterization without the associated health
risks. Furthermore, we qualitatively and quantitatively evaluate the synthetic
DCE-MRI images, proposing a multi-metric Scaled Aggregate Measure (SAMe),
assessing their utility in a tumor segmentation downstream task, and conclude
with an analysis of the temporal patterns in multi-sequence DCE-MRI generation.
Our approach demonstrates promising results in generating realistic and useful
DCE-MRI sequences, highlighting the potential of virtual contrast enhancement
for improving breast cancer diagnosis and treatment, particularly for patients
where contrast agent administration is contraindicated.",2024-09-27,"Richard Osuala, Smriti Joshi, Apostolia Tsirikoglou, Lidia Garrucho, Walter H. L. Pinaya, Daniel M. Lang, Julia A. Schnabel, Oliver Diaz, Karim Lekadir",http://arxiv.org/pdf/2409.18872v2,cs.LG
EarthquakeNPP: Benchmark Datasets for Earthquake Forecasting with Neural Point Processes,"Classical point process models, such as the epidemic-type aftershock sequence
(ETAS) model, have been widely used for forecasting the event times and
locations of earthquakes for decades. Recent advances have led to Neural Point
Processes (NPPs), which promise greater flexibility and improvements over
classical models. However, the currently-used benchmark dataset for NPPs does
not represent an up-to-date challenge in the seismological community since it
lacks a key earthquake sequence from the region and improperly splits training
and testing data. Furthermore, initial earthquake forecast benchmarking lacks a
comparison to state-of-the-art earthquake forecasting models typically used by
the seismological community. To address these gaps, we introduce EarthquakeNPP:
a collection of benchmark datasets to facilitate testing of NPPs on earthquake
data, accompanied by a credible implementation of the ETAS model. The datasets
cover a range of small to large target regions within California, dating from
1971 to 2021, and include different methodologies for dataset generation. In a
benchmarking experiment, we compare three spatio-temporal NPPs against ETAS and
find that none outperform ETAS in either spatial or temporal log-likelihood.
These results indicate that current NPP implementations are not yet suitable
for practical earthquake forecasting. However, EarthquakeNPP will serve as a
platform for collaboration between the seismology and machine learning
communities with the goal of improving earthquake predictability.",2024-09-27,"Samuel Stockman, Daniel Lawson, Maximilian Werner",http://arxiv.org/pdf/2410.08226v1,cs.LG
Individuation in Neural Models with and without Visual Grounding,"We show differences between a language-and-vision model CLIP and two
text-only models - FastText and SBERT - when it comes to the encoding of
individuation information. We study latent representations that CLIP provides
for substrates, granular aggregates, and various numbers of objects. We
demonstrate that CLIP embeddings capture quantitative differences in
individuation better than models trained on text-only data. Moreover, the
individuation hierarchy we deduce from the CLIP embeddings agrees with the
hierarchies proposed in linguistics and cognitive science.",2024-09-27,"Alexey Tikhonov, Lisa Bylinina, Ivan P. Yamshchikov",http://arxiv.org/pdf/2409.18868v1,cs.LG
Positional Encoder Graph Quantile Neural Networks for Geographic Data,"Positional Encoder Graph Neural Networks (PE-GNNs) are among the most
effective models for learning from continuous spatial data. However, their
predictive distributions are often poorly calibrated, limiting their utility in
applications that require reliable uncertainty quantification. We propose the
Positional Encoder Graph Quantile Neural Network (PE-GQNN), a novel framework
that combines PE-GNNs with Quantile Neural Networks, partially monotonic neural
blocks, and post-hoc recalibration techniques. The PE-GQNN enables flexible and
robust conditional density estimation with minimal assumptions about the target
distribution, and it extends naturally to tasks beyond spatial data. Empirical
results on benchmark datasets show that the PE-GQNN outperforms existing
methods in both predictive accuracy and uncertainty quantification, without
incurring additional computational cost. We also provide theoretical insights
and identify important special cases arising from our formulation, including
the PE-GNN.",2024-09-27,"William E. R. de Amorim, Scott A. Sisson, T. Rodrigues, David J. Nott, Guilherme S. Rodrigues",http://arxiv.org/pdf/2409.18865v2,cs.LG
Challenges of Generating Structurally Diverse Graphs,"For many graph-related problems, it can be essential to have a set of
structurally diverse graphs. For instance, such graphs can be used for testing
graph algorithms or their neural approximations. However, to the best of our
knowledge, the problem of generating structurally diverse graphs has not been
explored in the literature. In this paper, we fill this gap. First, we discuss
how to define diversity for a set of graphs, why this task is non-trivial, and
how one can choose a proper diversity measure. Then, for a given diversity
measure, we propose and compare several algorithms optimizing it: we consider
approaches based on standard random graph models, local graph optimization,
genetic algorithms, and neural generative models. We show that it is possible
to significantly improve diversity over basic random graph generators.
Additionally, our analysis of generated graphs allows us to better understand
the properties of graph distances: depending on which diversity measure is used
for optimization, the obtained graphs may possess very different structural
properties which gives a better understanding of the graph distance underlying
the diversity measure.",2024-09-27,"Fedor Velikonivtsev, Mikhail Mironov, Liudmila Prokhorenkova",http://arxiv.org/pdf/2409.18859v2,cs.LG
Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization,"Neural networks are often challenging to work with due to their large size
and complexity. To address this, various methods aim to reduce model size by
sparsifying or decomposing weight matrices, such as magnitude pruning and
low-rank or block-diagonal factorization. In this work, we present Double
Sparse Factorization (DSF), where we factorize each weight matrix into two
sparse matrices. Although solving this problem exactly is computationally
infeasible, we propose an efficient heuristic based on alternating minimization
via ADMM that achieves state-of-the-art results, enabling unprecedented
sparsification of neural networks. For instance, in a one-shot pruning setting,
our method can reduce the size of the LLaMA2-13B model by 50% while maintaining
better performance than the dense LLaMA2-7B model. We also compare favorably
with Optimal Brain Compression, the state-of-the-art layer-wise pruning
approach for convolutional neural networks. Furthermore, accuracy improvements
of our method persist even after further model fine-tuning.
  Code available at: https://github.com/usamec/double_sparse.",2024-09-27,"Vladimír Boža, Vladimír Macko",http://arxiv.org/pdf/2409.18850v1,cs.LG
"Classical Statistical (In-Sample) Intuitions Don't Generalize Well: A Note on Bias-Variance Tradeoffs, Overfitting and Moving from Fixed to Random Designs","The sudden appearance of modern machine learning (ML) phenomena like double
descent and benign overfitting may leave many classically trained statisticians
feeling uneasy -- these phenomena appear to go against the very core of
statistical intuitions conveyed in any introductory class on learning from
data. The historical lack of earlier observation of such phenomena is usually
attributed to today's reliance on more complex ML methods,
overparameterization, interpolation and/or higher data dimensionality. In this
note, we show that there is another reason why we observe behaviors today that
appear at odds with intuitions taught in classical statistics textbooks, which
is much simpler to understand yet rarely discussed explicitly. In particular,
many intuitions originate in fixed design settings, in which in-sample
prediction error (under resampling of noisy outcomes) is of interest, while
modern ML evaluates its predictions in terms of generalization error, i.e.
out-of-sample prediction error in random designs. Here, we highlight that this
simple move from fixed to random designs has (perhaps surprisingly)
far-reaching consequences on textbook intuitions relating to the bias-variance
tradeoff, and comment on the resulting (im)possibility of observing double
descent and benign overfitting in fixed versus random designs.",2024-09-27,Alicia Curth,http://arxiv.org/pdf/2409.18842v1,cs.LG
Constructing Confidence Intervals for 'the' Generalization Error -- a Comprehensive Benchmark Study,"When assessing the quality of prediction models in machine learning,
confidence intervals (CIs) for the generalization error, which measures
predictive performance, are a crucial tool. Luckily, there exist many methods
for computing such CIs and new promising approaches are continuously being
proposed. Typically, these methods combine various resampling procedures, most
popular among them cross-validation and bootstrapping, with different variance
estimation techniques. Unfortunately, however, there is currently no consensus
on when any of these combinations may be most reliably employed and how they
generally compare. In this work, we conduct a large-scale study comparing CIs
for the generalization error, the first one of such size, where we empirically
evaluate 13 different CI methods on a total of 19 tabular regression and
classification problems, using seven different inducers and a total of eight
loss functions. We give an overview of the methodological foundations and
inherent challenges of constructing CIs for the generalization error and
provide a concise review of all 13 methods in a unified framework. Finally, the
CI methods are evaluated in terms of their relative coverage frequency, width,
and runtime. Based on these findings, we can identify a subset of methods that
we would recommend. We also publish the datasets as a benchmarking suite on
OpenML and our code on GitHub to serve as a basis for further studies.",2024-09-27,"Hannah Schulz-Kümpel, Sebastian Fischer, Roman Hornung, Anne-Laure Boulesteix, Thomas Nagler, Bernd Bischl",http://arxiv.org/pdf/2409.18836v2,cs.LG
Classification and regression of trajectories rendered as images via 2D Convolutional Neural Networks,"Trajectories can be regarded as time-series of coordinates, typically arising
from motile objects. Methods for trajectory classification are particularly
important to detect different movement patterns, while methods for regression
to compute motility metrics and forecasting. Recent advances in computer vision
have facilitated the processing of trajectories rendered as images via
artificial neural networks with 2d convolutional layers (CNNs). This approach
leverages the capability of CNNs to learn spatial hierarchies of features from
images, necessary to recognize complex shapes. Moreover, it overcomes the
limitation of other machine learning methods that require input trajectories
with a fixed number of points. However, rendering trajectories as images can
introduce poorly investigated artifacts such as information loss due to the
plotting of coordinates on a discrete grid, and spectral changes due to line
thickness and aliasing. In this study, we investigate the effectiveness of CNNs
for solving classification and regression problems from synthetic trajectories
that have been rendered as images using different modalities. The parameters
considered in this study include line thickness, image resolution, usage of
motion history (color-coding of the temporal component) and anti-aliasing.
Results highlight the importance of choosing an appropriate image resolution
according to model depth and motion history in applications where movement
direction is critical.",2024-09-27,"Mariaclaudia Nicolai, Raffaella Fiamma Cabini, Diego Ulisse Pizzagalli",http://arxiv.org/pdf/2409.18832v1,cs.LG
ARLBench: Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning,"Hyperparameters are a critical factor in reliably training well-performing
reinforcement learning (RL) agents. Unfortunately, developing and evaluating
automated approaches for tuning such hyperparameters is both costly and
time-consuming. As a result, such approaches are often only evaluated on a
single domain or algorithm, making comparisons difficult and limiting insights
into their generalizability. We propose ARLBench, a benchmark for
hyperparameter optimization (HPO) in RL that allows comparisons of diverse HPO
approaches while being highly efficient in evaluation. To enable research into
HPO in RL, even in settings with low compute resources, we select a
representative subset of HPO tasks spanning a variety of algorithm and
environment combinations. This selection allows for generating a performance
profile of an automated RL (AutoRL) method using only a fraction of the compute
previously necessary, enabling a broader range of researchers to work on HPO in
RL. With the extensive and large-scale dataset on hyperparameter landscapes
that our selection is based on, ARLBench is an efficient, flexible, and
future-oriented foundation for research on AutoRL. Both the benchmark and the
dataset are available at https://github.com/automl/arlbench.",2024-09-27,"Jannis Becktepe, Julian Dierkes, Carolin Benjamins, Aditya Mohan, David Salinas, Raghu Rajan, Frank Hutter, Holger Hoos, Marius Lindauer, Theresa Eimer",http://arxiv.org/pdf/2409.18827v1,cs.LG
Early diagnosis of Alzheimer's disease from MRI images with deep learning model,"It is acknowledged that the most common cause of dementia worldwide is
Alzheimer's disease (AD). This condition progresses in severity from mild to
severe and interferes with people's everyday routines. Early diagnosis plays a
critical role in patient care and clinical trials. Convolutional neural
networks (CNN) are used to create a framework for identifying specific disease
features from MRI scans Classification of dementia involves approaches such as
medical history review, neuropsychological tests, and magnetic resonance
imaging (MRI). However, the image dataset obtained from Kaggle faces a
significant issue of class imbalance, which requires equal distribution of
samples from each class to address. In this article, to address this imbalance,
the Synthetic Minority Oversampling Technique (SMOTE) is utilized. Furthermore,
a pre-trained convolutional neural network has been applied to the DEMNET
dementia network to extract key features from AD images. The proposed model
achieved an impressive accuracy of 98.67%.",2024-09-27,"Sajjad Aghasi Javid, Mahmood Mohassel Feghhi",http://arxiv.org/pdf/2409.18814v1,cs.LG
Convergence of Diffusion Models Under the Manifold Hypothesis in High-Dimensions,"Denoising Diffusion Probabilistic Models (DDPM) are powerful state-of-the-art
methods used to generate synthetic data from high-dimensional data
distributions and are widely used for image, audio, and video generation as
well as many more applications in science and beyond. The \textit{manifold
hypothesis} states that high-dimensional data often lie on lower-dimensional
manifolds within the ambient space, and is widely believed to hold in provided
examples. While recent results have provided invaluable insight into how
diffusion models adapt to the manifold hypothesis, they do not capture the
great empirical success of these models, making this a very fruitful research
direction.
  In this work, we study DDPMs under the manifold hypothesis and prove that
they achieve rates independent of the ambient dimension in terms of score
learning. In terms of sampling complexity, we obtain rates independent of the
ambient dimension w.r.t. the Kullback-Leibler divergence, and $O(\sqrt{D})$
w.r.t. the Wasserstein distance. We do this by developing a new framework
connecting diffusion models to the well-studied theory of extrema of Gaussian
Processes.",2024-09-27,"Iskander Azangulov, George Deligiannidis, Judith Rousseau",http://arxiv.org/pdf/2409.18804v2,cs.LG
Esports Debut as a Medal Event at 2023 Asian Games: Exploring Public Perceptions with BERTopic and GPT-4 Topic Fine-Tuning,"This study examined the public opinions of esports at the 2023 Asian Games
and value co-creation during the event using an LLM-enhanced BERTopic modeling
analysis. We identified five major themes representing public perceptions, as
well as how major stakeholders co-created value within and beyond the esports
ecosystem. Key findings highlighted the strategic use of social media marketing
to influence public opinion and promote esports events and brands, emphasizing
the importance of event logistics and infrastructure. Additionally, the study
revealed the co-creation value contributed by stakeholders outside the
traditional esports ecosystem, particularly in promoting national
representation and performance. Our findings supported the ongoing efforts to
legitimize esports as a sport, noting that mainstream recognition remains a
challenge. The inclusion of esports as a medal event showcased broader
acceptance and helped mitigate negative public perceptions. Moreover,
contributions from non-traditional stakeholders underscored the value of
cross-subcultural collaborations in esports.",2024-09-27,"Tyreal Yizhou Qian, Bo Yu, Weizhe Li, Chenglong Xu",http://arxiv.org/pdf/2409.18798v1,cs.LG
Hierarchical Federated ADMM,"In this paper, we depart from the widely-used gradient descent-based
hierarchical federated learning (FL) algorithms to develop a novel hierarchical
FL framework based on the alternating direction method of multipliers (ADMM).
Within this framework, we propose two novel FL algorithms, which both use ADMM
in the top layer: one that employs ADMM in the lower layer and another that
uses the conventional gradient descent-based approach. The proposed framework
enhances privacy, and experiments demonstrate the superiority of the proposed
algorithms compared to the conventional algorithms in terms of learning
convergence and accuracy. Additionally, gradient descent on the lower layer
performs well even if the number of local steps is very limited, while ADMM on
both layers lead to better performance otherwise.",2024-09-27,"Seyed Mohammad Azimi-Abarghouyi, Nicola Bastianello, Karl H. Johansson, Viktoria Fodor",http://arxiv.org/pdf/2409.18796v1,cs.LG
HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation,"Efficiently determining the satisfiability of a boolean equation -- known as
the SAT problem for brevity -- is crucial in various industrial problems.
Recently, the advent of deep learning methods has introduced significant
potential for enhancing SAT solving. However, a major barrier to the
advancement of this field has been the scarcity of large, realistic datasets.
The majority of current public datasets are either randomly generated or
extremely limited, containing only a few examples from unrelated problem
families. These datasets are inadequate for meaningful training of deep
learning methods. In light of this, researchers have started exploring
generative techniques to create data that more accurately reflect SAT problems
encountered in practical situations. These methods have so far suffered from
either the inability to produce challenging SAT problems or time-scalability
obstacles. In this paper we address both by identifying and manipulating the
key contributors to a problem's ``hardness'', known as cores. Although some
previous work has addressed cores, the time costs are unacceptably high due to
the expense of traditional heuristic core detection techniques. We introduce a
fast core detection procedure that uses a graph neural network. Our empirical
results demonstrate that we can efficiently generate problems that remain hard
to solve and retain key attributes of the original example problems. We show
via experiment that the generated synthetic SAT problems can be used in a data
augmentation setting to provide improved prediction of solver runtimes.",2024-09-27,"Joseph Cotnareanu, Zhanguang Zhang, Hui-Ling Zhen, Yingxue Zhang, Mark Coates",http://arxiv.org/pdf/2409.18778v1,cs.LG
A method of using RSVD in residual calculation of LowBit GEMM,"The advancements of hardware technology in recent years has brought many
possibilities for low-precision applications. However, the use of low precision
can introduce significant computational errors, posing a considerable challenge
to maintaining the computational accuracy.
  We propose low-rank residuals quantized matrix multiplication(LRQMM) method
which introduces low-rank approximation in residual compensation for dense low
precision quantization matrix multiplication. It can bring several times
accuracy improvement with only BLAS-2 level extra time overhead. Moreover,
LRQMM is a completely data-free quantization method that does not require
additional data for pre-training. And it only works with low precision GEMM
operator, which is easy to couple with other methods.
  Through experimentation, LRQMM can reduce the error of direct quantized
matrix multiplication by 1~2 orders of magnitude, when dealing with larger
matrix sizes, the computational speed is only reduced by approximately 20\%. In
deep learning networks, LRQMM-4bit achieves 61.8% ImageNet Top-1 accuracy in
Resnet-50, while the Direct Quant accuracy is only 8.3%.",2024-09-27,Hongyaoxing Gu,http://arxiv.org/pdf/2409.18772v1,cs.LG
Learning from Demonstration with Implicit Nonlinear Dynamics Models,"Learning from Demonstration (LfD) is a useful paradigm for training policies
that solve tasks involving complex motions, such as those encountered in
robotic manipulation. In practice, the successful application of LfD requires
overcoming error accumulation during policy execution, i.e. the problem of
drift due to errors compounding over time and the consequent
out-of-distribution behaviours. Existing works seek to address this problem
through scaling data collection, correcting policy errors with a
human-in-the-loop, temporally ensembling policy predictions or through learning
a dynamical system model with convergence guarantees. In this work, we propose
and validate an alternative approach to overcoming this issue. Inspired by
reservoir computing, we develop a recurrent neural network layer that includes
a fixed nonlinear dynamical system with tunable dynamical properties for
modelling temporal dynamics. We validate the efficacy of our neural network
layer on the task of reproducing human handwriting motions using the LASA Human
Handwriting Dataset. Through empirical experiments we demonstrate that
incorporating our layer into existing neural network architectures addresses
the issue of compounding errors in LfD. Furthermore, we perform a comparative
evaluation against existing approaches including a temporal ensemble of policy
predictions and an Echo State Network (ESN) implementation. We find that our
approach yields greater policy precision and robustness on the handwriting task
while also generalising to multiple dynamics regimes and maintaining
competitive latency scores.",2024-09-27,"Peter David Fagan, Subramanian Ramamoorthy",http://arxiv.org/pdf/2409.18768v3,cs.LG
Geometric deep learning for galaxy-halo connection: a case study for galaxy intrinsic alignments,"Forthcoming cosmological imaging surveys, such as the Rubin Observatory LSST,
require large-scale simulations encompassing realistic galaxy populations for a
variety of scientific applications. Of particular concern is the phenomenon of
intrinsic alignments (IA), whereby galaxies orient themselves towards
overdensities, potentially introducing significant systematic biases in weak
gravitational lensing analyses if they are not properly modeled. Due to
computational constraints, simulating the intricate details of galaxy formation
and evolution relevant to IA across vast volumes is impractical. As an
alternative, we propose a Deep Generative Model trained on the IllustrisTNG-100
simulation to sample 3D galaxy shapes and orientations to accurately reproduce
intrinsic alignments along with correlated scalar features. We model the cosmic
web as a set of graphs, each graph representing a halo with nodes representing
the subhalos/galaxies. The architecture consists of a SO(3) $\times$
$\mathbb{R}^n$ diffusion generative model, for galaxy orientations and $n$
scalars, implemented with E(3) equivariant Graph Neural Networks that
explicitly respect the Euclidean symmetries of our Universe. The model is able
to learn and predict features such as galaxy orientations that are
statistically consistent with the reference simulation. Notably, our model
demonstrates the ability to jointly model Euclidean-valued scalars (galaxy
sizes, shapes, and colors) along with non-Euclidean valued SO(3) quantities
(galaxy orientations) that are governed by highly complex galactic physics at
non-linear scales.",2024-09-27,"Yesukhei Jagvaral, Francois Lanusse, Rachel Mandelbaum",http://arxiv.org/pdf/2409.18761v1,cs.LG
TensorSocket: Shared Data Loading for Deep Learning Training,"Training deep learning models is a repetitive and resource-intensive process.
Data scientists often train several models before landing on set of parameters
(e.g., hyper-parameter tuning), model architecture (e.g., neural architecture
search), among other things that yields the highest accuracy. The computational
efficiency of these training tasks depends highly on how well we can supply the
training process with training data. The repetitive nature of these tasks
results in the same data processing pipelines running over and over
exacerbating the need for and costs of computational resources.
  In this paper, we present Tensorsocket to reduce the computational needs of
deep learning training by enabling simultaneous training processes to share the
same data loader. Tensorsocket mitigates CPU-side bottlenecks in cases where
the collocated training workloads have high throughput on GPU, but are held
back by lower data-loading throughput on CPU. Tensorsocket achieves this by
reducing redundant computations across collocated training processes and
leveraging modern GPU-GPU interconnects. We demonstrate the hardware- and
pipeline-agnostic nature of Tensorsocket and evaluate it using a variety of
training scenarios.
  Our evaluation shows that Tensorsocket enables scenarios that are infeasible
without data sharing, increases training throughput by up to $100\%$, and when
utilizing cloud instances, Tensorsocket achieves cost savings of $50\%$ by
reducing the hardware resource needs on the CPU side. Furthermore, Tensorsocket
outperforms the state-of-the-art solutions for shared data loading such as
CoorDL and Joader. It is easier to use, maintain, and deploy, and either
achieves higher or matches the throughput of other solutions while requiring
less CPU resources.",2024-09-27,"Ties Robroek, Neil Kim Nielsen, Pınar Tözün",http://arxiv.org/pdf/2409.18749v1,cs.LG
Cottention: Linear Transformers With Cosine Attention,"Attention mechanisms, particularly softmax attention, have been instrumental
in the success of transformer-based models such as GPT. However, the quadratic
memory complexity of softmax attention with respect to sequence length poses
significant challenges for processing longer sequences. We introduce
Cottention, a novel attention mechanism that replaces the softmax operation
with cosine similarity. By leveraging the properties of cosine similarity and
rearranging the attention equation, Cottention achieves native linear memory
complexity with respect to sequence length, making it inherently more
memory-efficient than softmax attention. We demonstrate that Cottention can be
reformulated as a recurrent neural network (RNN) with a finite hidden state,
allowing for constant memory usage during inference. We evaluate Cottention on
both the bidirectional BERT and causal GPT tasks, demonstrating comparable
performance to softmax attention while significantly reducing memory
requirements. To ensure efficient computation, we develop a custom CUDA kernel
for Cottention. Our results show that Cottention is a promising alternative to
softmax attention, enabling the processing of longer sequences without
sacrificing performance, due to its native linear memory complexity and ability
to maintain a constant memory footprint during inference.",2024-09-27,"Gabriel Mongaras, Trevor Dohm, Eric C. Larson",http://arxiv.org/pdf/2409.18747v1,cs.LG
Autoregressive Policy Optimization for Constrained Allocation Tasks,"Allocation tasks represent a class of problems where a limited amount of
resources must be allocated to a set of entities at each time step. Prominent
examples of this task include portfolio optimization or distributing
computational workloads across servers. Allocation tasks are typically bound by
linear constraints describing practical requirements that have to be strictly
fulfilled at all times. In portfolio optimization, for example, investors may
be obligated to allocate less than 30\% of the funds into a certain industrial
sector in any investment period. Such constraints restrict the action space of
allowed allocations in intricate ways, which makes learning a policy that
avoids constraint violations difficult. In this paper, we propose a new method
for constrained allocation tasks based on an autoregressive process to
sequentially sample allocations for each entity. In addition, we introduce a
novel de-biasing mechanism to counter the initial bias caused by sequential
sampling. We demonstrate the superior performance of our approach compared to a
variety of Constrained Reinforcement Learning (CRL) methods on three distinct
constrained allocation tasks: portfolio optimization, computational workload
distribution, and a synthetic allocation benchmark. Our code is available at:
https://github.com/niklasdbs/paspo",2024-09-27,"David Winkel, Niklas Strauß, Maximilian Bernhard, Zongyue Li, Thomas Seidl, Matthias Schubert",http://arxiv.org/pdf/2409.18735v1,cs.LG
Scalable Cross-Entropy Loss for Sequential Recommendations with Large Item Catalogs,"Scalability issue plays a crucial role in productionizing modern recommender
systems. Even lightweight architectures may suffer from high computational
overload due to intermediate calculations, limiting their practicality in
real-world applications. Specifically, applying full Cross-Entropy (CE) loss
often yields state-of-the-art performance in terms of recommendations quality.
Still, it suffers from excessive GPU memory utilization when dealing with large
item catalogs. This paper introduces a novel Scalable Cross-Entropy (SCE) loss
function in the sequential learning setup. It approximates the CE loss for
datasets with large-size catalogs, enhancing both time efficiency and memory
usage without compromising recommendations quality. Unlike traditional negative
sampling methods, our approach utilizes a selective GPU-efficient computation
strategy, focusing on the most informative elements of the catalog,
particularly those most likely to be false positives. This is achieved by
approximating the softmax distribution over a subset of the model outputs
through the maximum inner product search. Experimental results on multiple
datasets demonstrate the effectiveness of SCE in reducing peak memory usage by
a factor of up to 100 compared to the alternatives, retaining or even exceeding
their metrics values. The proposed approach also opens new perspectives for
large-scale developments in different domains, such as large language models.",2024-09-27,"Gleb Mezentsev, Danil Gusak, Ivan Oseledets, Evgeny Frolov",http://arxiv.org/pdf/2409.18721v2,cs.LG
Enhancing Spectrum Efficiency in 6G Satellite Networks: A GAIL-Powered Policy Learning via Asynchronous Federated Inverse Reinforcement Learning,"In this paper, a novel generative adversarial imitation learning
(GAIL)-powered policy learning approach is proposed for optimizing beamforming,
spectrum allocation, and remote user equipment (RUE) association in NTNs.
Traditional reinforcement learning (RL) methods for wireless network
optimization often rely on manually designed reward functions, which can
require extensive parameter tuning. To overcome these limitations, we employ
inverse RL (IRL), specifically leveraging the GAIL framework, to automatically
learn reward functions without manual design. We augment this framework with an
asynchronous federated learning approach, enabling decentralized
multi-satellite systems to collaboratively derive optimal policies. The
proposed method aims to maximize spectrum efficiency (SE) while meeting minimum
information rate requirements for RUEs. To address the non-convex, NP-hard
nature of this problem, we combine the many-to-one matching theory with a
multi-agent asynchronous federated IRL (MA-AFIRL) framework. This allows agents
to learn through asynchronous environmental interactions, improving training
efficiency and scalability. The expert policy is generated using the Whale
optimization algorithm (WOA), providing data to train the automatic reward
function within GAIL. Simulation results show that the proposed MA-AFIRL method
outperforms traditional RL approaches, achieving a $14.6\%$ improvement in
convergence and reward value. The novel GAIL-driven policy learning establishes
a novel benchmark for 6G NTN optimization.",2024-09-27,"Sheikh Salman Hassan, Yu Min Park, Yan Kyaw Tun, Walid Saad, Zhu Han, Choong Seon Hong",http://arxiv.org/pdf/2409.18718v1,cs.LG
Rethinking the Power of Timestamps for Robust Time Series Forecasting: A Global-Local Fusion Perspective,"Time series forecasting has played a pivotal role across various industries,
including finance, transportation, energy, healthcare, and climate. Due to the
abundant seasonal information they contain, timestamps possess the potential to
offer robust global guidance for forecasting techniques. However, existing
works primarily focus on local observations, with timestamps being treated
merely as an optional supplement that remains underutilized. When data gathered
from the real world is polluted, the absence of global information will damage
the robust prediction capability of these algorithms. To address these
problems, we propose a novel framework named GLAFF. Within this framework, the
timestamps are modeled individually to capture the global dependencies. Working
as a plugin, GLAFF adaptively adjusts the combined weights for global and local
information, enabling seamless collaboration with any time series forecasting
backbone. Extensive experiments conducted on nine real-world datasets
demonstrate that GLAFF significantly enhances the average performance of widely
used mainstream forecasting models by 12.5%, surpassing the previous
state-of-the-art method by 5.5%.",2024-09-27,"Chengsen Wang, Qi Qi, Jingyu Wang, Haifeng Sun, Zirui Zhuang, Jinming Wu, Jianxin Liao",http://arxiv.org/pdf/2409.18696v3,cs.LG
MG-Net: Learn to Customize QAOA with Circuit Depth Awareness,"Quantum Approximate Optimization Algorithm (QAOA) and its variants exhibit
immense potential in tackling combinatorial optimization challenges. However,
their practical realization confronts a dilemma: the requisite circuit depth
for satisfactory performance is problem-specific and often exceeds the maximum
capability of current quantum devices. To address this dilemma, here we first
analyze the convergence behavior of QAOA, uncovering the origins of this
dilemma and elucidating the intricate relationship between the employed mixer
Hamiltonian, the specific problem at hand, and the permissible maximum circuit
depth. Harnessing this understanding, we introduce the Mixer Generator Network
(MG-Net), a unified deep learning framework adept at dynamically formulating
optimal mixer Hamiltonians tailored to distinct tasks and circuit depths.
Systematic simulations, encompassing Ising models and weighted Max-Cut
instances with up to 64 qubits, substantiate our theoretical findings,
highlighting MG-Net's superior performance in terms of both approximation ratio
and efficiency.",2024-09-27,"Yang Qian, Xinbiao Wang, Yuxuan Du, Yong Luo, Dacheng Tao",http://arxiv.org/pdf/2409.18692v1,cs.LG
Understanding the Benefits of SimCLR Pre-Training in Two-Layer Convolutional Neural Networks,"SimCLR is one of the most popular contrastive learning methods for vision
tasks. It pre-trains deep neural networks based on a large amount of unlabeled
data by teaching the model to distinguish between positive and negative pairs
of augmented images. It is believed that SimCLR can pre-train a deep neural
network to learn efficient representations that can lead to a better
performance of future supervised fine-tuning. Despite its effectiveness, our
theoretical understanding of the underlying mechanisms of SimCLR is still
limited. In this paper, we theoretically introduce a case study of the SimCLR
method. Specifically, we consider training a two-layer convolutional neural
network (CNN) to learn a toy image data model. We show that, under certain
conditions on the number of labeled data, SimCLR pre-training combined with
supervised fine-tuning achieves almost optimal test loss. Notably, the label
complexity for SimCLR pre-training is far less demanding compared to direct
training on supervised data. Our analysis sheds light on the benefits of SimCLR
in learning with fewer labels.",2024-09-27,"Han Zhang, Yuan Cao",http://arxiv.org/pdf/2409.18685v1,cs.LG
"How green is continual learning, really? Analyzing the energy consumption in continual training of vision foundation models","With the ever-growing adoption of AI, its impact on the environment is no
longer negligible. Despite the potential that continual learning could have
towards Green AI, its environmental sustainability remains relatively
uncharted. In this work we aim to gain a systematic understanding of the energy
efficiency of continual learning algorithms. To that end, we conducted an
extensive set of empirical experiments comparing the energy consumption of
recent representation-, prompt-, and exemplar-based continual learning
algorithms and two standard baseline (fine tuning and joint training) when used
to continually adapt a pre-trained ViT-B/16 foundation model. We performed our
experiments on three standard datasets: CIFAR-100, ImageNet-R, and DomainNet.
Additionally, we propose a novel metric, the Energy NetScore, which we use
measure the algorithm efficiency in terms of energy-accuracy trade-off. Through
numerous evaluations varying the number and size of the incremental learning
steps, our experiments demonstrate that different types of continual learning
algorithms have very different impacts on energy consumption during both
training and inference. Although often overlooked in the continual learning
literature, we found that the energy consumed during the inference phase is
crucial for evaluating the environmental sustainability of continual learning
models.",2024-09-27,"Tomaso Trinci, Simone Magistri, Roberto Verdecchia, Andrew D. Bagdanov",http://arxiv.org/pdf/2409.18664v1,cs.LG
"Entropy, concentration, and learning: a statistical mechanics primer","Artificial intelligence models trained through loss minimization have
demonstrated significant success, grounded in principles from fields like
information theory and statistical physics. This work explores these
established connections through the lens of statistical mechanics, starting
from first-principles sample concentration behaviors that underpin AI and
machine learning. Our development of statistical mechanics for modeling
highlights the key role of exponential families, and quantities of statistics,
physics, and information theory.",2024-09-27,Akshay Balsubramani,http://arxiv.org/pdf/2409.18630v1,cs.LG
Towards Integrating Epistemic Uncertainty Estimation into the Radiotherapy Workflow,"The precision of contouring target structures and organs-at-risk (OAR) in
radiotherapy planning is crucial for ensuring treatment efficacy and patient
safety. Recent advancements in deep learning (DL) have significantly improved
OAR contouring performance, yet the reliability of these models, especially in
the presence of out-of-distribution (OOD) scenarios, remains a concern in
clinical settings. This application study explores the integration of epistemic
uncertainty estimation within the OAR contouring workflow to enable OOD
detection in clinically relevant scenarios, using specifically compiled data.
Furthermore, we introduce an advanced statistical method for OOD detection to
enhance the methodological framework of uncertainty estimation. Our empirical
evaluation demonstrates that epistemic uncertainty estimation is effective in
identifying instances where model predictions are unreliable and may require an
expert review. Notably, our approach achieves an AUC-ROC of 0.95 for OOD
detection, with a specificity of 0.95 and a sensitivity of 0.92 for implant
cases, underscoring its efficacy. This study addresses significant gaps in the
current research landscape, such as the lack of ground truth for uncertainty
estimation and limited empirical evaluations. Additionally, it provides a
clinically relevant application of epistemic uncertainty estimation in an
FDA-approved and widely used clinical solution for OAR segmentation from
Varian, a Siemens Healthineers company, highlighting its practical benefits.",2024-09-27,"Marvin Tom Teichmann, Manasi Datar, Lisa Kratzke, Fernando Vega, Florin C. Ghesu",http://arxiv.org/pdf/2409.18628v1,cs.LG
Unsupervised Cognition,"Unsupervised learning methods have a soft inspiration in cognition models. To
this day, the most successful unsupervised learning methods revolve around
clustering samples in a mathematical space. In this paper we propose a
state-of-the-art, primitive-based, unsupervised learning approach for
decision-making inspired by a novel cognition framework. This
representation-centric approach models the input space constructively as a
distributed hierarchical structure in an input-agnostic way. We compared our
approach with both current state-of-the-art unsupervised learning
classification, and with current state-of-the-art cancer type classification.
We show how our proposal outperforms previous state-of-the-art. We also
evaluate some cognition-like properties of our proposal where it not only
outperforms the compared algorithms (even supervised learning ones), but it
also shows a different, more cognition-like, behaviour.",2024-09-27,"Alfredo Ibias, Hector Antona, Guillem Ramirez-Miranda, Enric Guinovart, Eduard Alarcon",http://arxiv.org/pdf/2409.18624v2,cs.LG
Differentially Private Non Parametric Copulas: Generating synthetic data with non parametric copulas under privacy guarantees,"Creation of synthetic data models has represented a significant advancement
across diverse scientific fields, but this technology also brings important
privacy considerations for users. This work focuses on enhancing a
non-parametric copula-based synthetic data generation model, DPNPC, by
incorporating Differential Privacy through an Enhanced Fourier Perturbation
method. The model generates synthetic data for mixed tabular databases while
preserving privacy. We compare DPNPC with three other models (PrivBayes,
DP-Copula, and DP-Histogram) across three public datasets, evaluating privacy,
utility, and execution time. DPNPC outperforms others in modeling multivariate
dependencies, maintaining privacy for small $\epsilon$ values, and reducing
training times. However, limitations include the need to assess the model's
performance with different encoding methods and consider additional privacy
attacks. Future research should address these areas to enhance
privacy-preserving synthetic data generation.",2024-09-27,"Pablo A. Osorio-Marulanda, John Esteban Castro Ramirez, Mikel Hernández Jiménez, Nicolas Moreno Reyes, Gorka Epelde Unanue",http://arxiv.org/pdf/2409.18611v1,cs.LG
TemporalPaD: a reinforcement-learning framework for temporal feature representation and dimension reduction,"Recent advancements in feature representation and dimension reduction have
highlighted their crucial role in enhancing the efficacy of predictive
modeling. This work introduces TemporalPaD, a novel end-to-end deep learning
framework designed for temporal pattern datasets. TemporalPaD integrates
reinforcement learning (RL) with neural networks to achieve concurrent feature
representation and feature reduction. The framework consists of three
cooperative modules: a Policy Module, a Representation Module, and a
Classification Module, structured based on the Actor-Critic (AC) framework. The
Policy Module, responsible for dimensionality reduction through RL, functions
as the actor, while the Representation Module for feature extraction and the
Classification Module collectively serve as the critic. We comprehensively
evaluate TemporalPaD using 29 UCI datasets, a well-known benchmark for
validating feature reduction algorithms, through 10 independent tests and
10-fold cross-validation. Additionally, given that TemporalPaD is specifically
designed for time series data, we apply it to a real-world DNA classification
problem involving enhancer category and enhancer strength. The results
demonstrate that TemporalPaD is an efficient and effective framework for
achieving feature reduction, applicable to both structured data and sequence
datasets. The source code of the proposed TemporalPaD is freely available as
supplementary material to this article and at
http://www.healthinformaticslab.org/supp/.",2024-09-27,"Xuechen Mu, Zhenyu Huang, Kewei Li, Haotian Zhang, Xiuli Wang, Yusi Fan, Kai Zhang, Fengfeng Zhou",http://arxiv.org/pdf/2409.18597v1,cs.LG
ASAG2024: A Combined Benchmark for Short Answer Grading,"Open-ended questions test a more thorough understanding than closed-ended
questions and are often a preferred assessment method. However, open-ended
questions are tedious to grade and subject to personal bias. Therefore, there
have been efforts to speed up the grading process through automation. Short
Answer Grading (SAG) systems aim to automatically score students' answers.
Despite growth in SAG methods and capabilities, there exists no comprehensive
short-answer grading benchmark across different subjects, grading scales, and
distributions. Thus, it is hard to assess the capabilities of current automated
grading methods in terms of their generalizability. In this preliminary work,
we introduce the combined ASAG2024 benchmark to facilitate the comparison of
automated grading systems. Combining seven commonly used short-answer grading
datasets in a common structure and grading scale. For our benchmark, we
evaluate a set of recent SAG methods, revealing that while LLM-based approaches
reach new high scores, they still are far from reaching human performance. This
opens up avenues for future research on human-machine SAG systems.",2024-09-27,"Gérôme Meyer, Philip Breuer, Jonathan Fürst",http://arxiv.org/pdf/2409.18596v1,cs.LG
"""Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree"": Zero-Shot Decision Tree Induction and Embedding with Large Language Models","Large language models (LLMs) provide powerful means to leverage prior
knowledge for predictive modeling when data is limited. In this work, we
demonstrate how LLMs can use their compressed world knowledge to generate
intrinsically interpretable machine learning models, i.e., decision trees,
without any training data. We find that these zero-shot decision trees can
surpass data-driven trees on some small-sized tabular datasets and that
embeddings derived from these trees perform on par with data-driven tree-based
embeddings on average. Our knowledge-driven decision tree induction and
embedding approaches therefore serve as strong new baselines for data-driven
machine learning methods in the low-data regime.",2024-09-27,"Ricardo Knauer, Mario Koddenbrock, Raphael Wallsberger, Nicholas M. Brisson, Georg N. Duda, Deborah Falla, David W. Evans, Erik Rodner",http://arxiv.org/pdf/2409.18594v1,cs.LG
Optimistic Games for Combinatorial Bayesian Optimization with Application to Protein Design,"Bayesian optimization (BO) is a powerful framework to optimize black-box
expensive-to-evaluate functions via sequential interactions. In several
important problems (e.g. drug discovery, circuit design, neural architecture
search, etc.), though, such functions are defined over large
$\textit{combinatorial and unstructured}$ spaces. This makes existing BO
algorithms not feasible due to the intractable maximization of the acquisition
function over these domains. To address this issue, we propose
$\textbf{GameOpt}$, a novel game-theoretical approach to combinatorial BO.
$\textbf{GameOpt}$ establishes a cooperative game between the different
optimization variables, and selects points that are game $\textit{equilibria}$
of an upper confidence bound acquisition function. These are stable
configurations from which no variable has an incentive to deviate$-$ analog to
local optima in continuous domains. Crucially, this allows us to efficiently
break down the complexity of the combinatorial domain into individual decision
sets, making $\textbf{GameOpt}$ scalable to large combinatorial spaces. We
demonstrate the application of $\textbf{GameOpt}$ to the challenging
$\textit{protein design}$ problem and validate its performance on four
real-world protein datasets. Each protein can take up to $20^{X}$ possible
configurations, where $X$ is the length of a protein, making standard BO
methods infeasible. Instead, our approach iteratively selects informative
protein configurations and very quickly discovers highly active protein
variants compared to other baselines.",2024-09-27,"Melis Ilayda Bal, Pier Giuseppe Sessa, Mojmir Mutny, Andreas Krause",http://arxiv.org/pdf/2409.18582v2,cs.LG
Using Deep Autoregressive Models as Causal Inference Engines,"Existing causal inference (CI) models are limited to primarily handling
low-dimensional confounders and singleton actions. We propose an autoregressive
(AR) CI framework capable of handling complex confounders and sequential
actions common in modern applications. We accomplish this by {\em
sequencification}, transforming data from an underlying causal diagram into a
sequence of tokens. This approach not only enables training with data generated
from any DAG but also extends existing CI capabilities to accommodate
estimating several statistical quantities using a {\em single} model. We can
directly predict interventional probabilities, simplifying inference and
enhancing outcome prediction accuracy. We demonstrate that an AR model adapted
for CI is efficient and effective in various complex applications such as
navigating mazes, playing chess endgames, and evaluating the impact of certain
keywords on paper acceptance rates.",2024-09-27,"Daniel Jiwoong Im, Kevin Zhang, Nakul Verma, Kyunghyun Cho",http://arxiv.org/pdf/2409.18581v2,cs.LG
"Intention-aware policy graphs: answering what, how, and why in opaque agents","Agents are a special kind of AI-based software in that they interact in
complex environments and have increased potential for emergent behaviour.
Explaining such emergent behaviour is key to deploying trustworthy AI, but the
increasing complexity and opaque nature of many agent implementations makes
this hard. In this work, we propose a Probabilistic Graphical Model along with
a pipeline for designing such model -- by which the behaviour of an agent can
be deliberated about -- and for computing a robust numerical value for the
intentions the agent has at any moment. We contribute measurements that
evaluate the interpretability and reliability of explanations provided, and
enables explainability questions such as `what do you want to do now?' (e.g.
deliver soup) `how do you plan to do it?' (e.g. returning a plan that considers
its skills and the world), and `why would you take this action at this state?'
(e.g. explaining how that furthers or hinders its own goals). This model can be
constructed by taking partial observations of the agent's actions and world
states, and we provide an iterative workflow for increasing the proposed
measurements through better design and/or pointing out irrational agent
behaviour.",2024-09-27,"Victor Gimenez-Abalos, Sergio Alvarez-Napagao, Adrian Tormos, Ulises Cortés, Javier Vázquez-Salceda",http://arxiv.org/pdf/2409.19038v1,cs.LG
An Enhanced Federated Prototype Learning Method under Domain Shift,"Federated Learning (FL) allows collaborative machine learning training
without sharing private data. Numerous studies have shown that one significant
factor affecting the performance of federated learning models is the
heterogeneity of data across different clients, especially when the data is
sampled from various domains. A recent paper introduces variance-aware
dual-level prototype clustering and uses a novel $\alpha$-sparsity prototype
loss, which increases intra-class similarity and reduces inter-class
similarity. To ensure that the features converge within specific clusters, we
introduce an improved algorithm, Federated Prototype Learning with Convergent
Clusters, abbreviated as FedPLCC. To increase inter-class distances, we weight
each prototype with the size of the cluster it represents. To reduce
intra-class distances, considering that prototypes with larger distances might
come from different domains, we select only a certain proportion of prototypes
for the loss function calculation. Evaluations on the Digit-5, Office-10, and
DomainNet datasets show that our method performs better than existing
approaches.",2024-09-27,"Liang Kuang, Kuangpu Guo, Jian Liang, Jianguo Zhang",http://arxiv.org/pdf/2409.18578v1,cs.LG
Climate Adaptation with Reinforcement Learning: Experiments with Flooding and Transportation in Copenhagen,"Due to climate change the frequency and intensity of extreme rainfall events,
which contribute to urban flooding, are expected to increase in many places.
These floods can damage transport infrastructure and disrupt mobility,
highlighting the need for cities to adapt to escalating risks. Reinforcement
learning (RL) serves as a powerful tool for uncovering optimal adaptation
strategies, determining how and where to deploy adaptation measures
effectively, even under significant uncertainty. In this study, we leverage RL
to identify the most effective timing and locations for implementing measures,
aiming to reduce both direct and indirect impacts of flooding. Our framework
integrates climate change projections of future rainfall events and floods,
models city-wide motorized trips, and quantifies direct and indirect impacts on
infrastructure and mobility. Preliminary results suggest that our RL-based
approach can significantly enhance decision-making by prioritizing
interventions in specific urban areas and identifying the optimal periods for
their implementation. Our framework is publicly available:
\url{https://github.com/MLSM-at-DTU/floods_transport_rl}.",2024-09-27,"Miguel Costa, Morten W. Petersen, Arthur Vandervoort, Martin Drews, Karyn Morrissey, Francisco C. Pereira",http://arxiv.org/pdf/2409.18574v2,cs.LG
Towards an active-learning approach to resource allocation for population-based damage prognosis,"Damage prognosis is, arguably, one of the most difficult tasks of structural
health monitoring (SHM). To address common problems of damage prognosis, a
population-based SHM (PBSHM) approach is adopted in the current work. In this
approach the prognosis problem is considered as an information-sharing problem
where data from past structures are exploited to make more accurate inferences
regarding currently-degrading structures. For a given population, there may
exist restrictions on the resources available to conduct monitoring; thus, the
current work studies the problem of allocating such resources within a
population of degrading structures with a view to maximising the
damage-prognosis accuracy. The challenges of the current framework are mainly
associated with the inference of outliers on the level of damage evolution,
given partial data from the damage-evolution phenomenon. The current approach
considers an initial population of structures for which damage evolution is
extensively observed. Subsequently, a second population of structures with
evolving damage is considered for which two monitoring systems are available, a
low-availability and high-fidelity (low-uncertainty) one, and a
widely-available and low-fidelity (high-uncertainty) one. The task of the
current work is to follow an active-learning approach to identify the
structures to which the high-fidelity system should be assigned in order to
enhance the predictive capabilities of the machine-learning model throughout
the population.",2024-09-27,"George Tsialiamanis, Keith Worden, Nikolaos Dervilis, Aidan J Hughes",http://arxiv.org/pdf/2409.18572v1,cs.LG
Experimental Evaluation of Machine Learning Models for Goal-oriented Customer Service Chatbot with Pipeline Architecture,"Integrating machine learning (ML) into customer service chatbots enhances
their ability to understand and respond to user queries, ultimately improving
service performance. However, they may appear artificial to some users and
affecting customer experience. Hence, meticulous evaluation of ML models for
each pipeline component is crucial for optimizing performance, though
differences in functionalities can lead to unfair comparisons. In this paper,
we present a tailored experimental evaluation approach for goal-oriented
customer service chatbots with pipeline architecture, focusing on three key
components: Natural Language Understanding (NLU), dialogue management (DM), and
Natural Language Generation (NLG). Our methodology emphasizes individual
assessment to determine optimal ML models. Specifically, we focus on optimizing
hyperparameters and evaluating candidate models for NLU (utilizing BERT and
LSTM), DM (employing DQN and DDQN), and NLG (leveraging GPT-2 and DialoGPT).
The results show that for the NLU component, BERT excelled in intent detection
whereas LSTM was superior for slot filling. For the DM component, the DDQN
model outperformed DQN by achieving fewer turns, higher rewards, as well as
greater success rates. For NLG, the large language model GPT-2 surpassed
DialoGPT in BLEU, METEOR, and ROUGE metrics. These findings aim to provide a
benchmark for future research in developing and optimizing customer service
chatbots, offering valuable insights into model performance and optimal
hyperparameters.",2024-09-27,"Nurul Ain Nabilah Mohd Isa, Siti Nuraishah Agos Jawaddi, Azlan Ismail",http://arxiv.org/pdf/2409.18568v1,cs.LG
Optimizing DNN Inference on Multi-Accelerator SoCs at Training-time,"The demand for executing Deep Neural Networks (DNNs) with low latency and
minimal power consumption at the edge has led to the development of advanced
heterogeneous Systems-on-Chips (SoCs) that incorporate multiple specialized
computing units (CUs), such as accelerators. Offloading DNN computations to a
specific CU from the available set often exposes accuracy vs efficiency
trade-offs, due to differences in their supported operations (e.g., standard
vs. depthwise convolution) or data representations (e.g., more/less
aggressively quantized). A challenging yet unresolved issue is how to map a DNN
onto these multi-CU systems to maximally exploit the parallelization
possibilities while taking accuracy into account. To address this problem, we
present ODiMO, a hardware-aware tool that efficiently explores fine-grain
mapping of DNNs among various on-chip CUs, during the training phase. ODiMO
strategically splits individual layers of the neural network and executes them
in parallel on the multiple available CUs, aiming to balance the total
inference energy consumption or latency with the resulting accuracy, impacted
by the unique features of the different hardware units. We test our approach on
CIFAR-10, CIFAR-100, and ImageNet, targeting two open-source heterogeneous
SoCs, i.e., DIANA and Darkside. We obtain a rich collection of Pareto-optimal
networks in the accuracy vs. energy or latency space. We show that ODiMO
reduces the latency of a DNN executed on the Darkside SoC by up to 8x at
iso-accuracy, compared to manual heuristic mappings. When targeting energy, on
the same SoC, ODiMO produced up to 50.8x more efficient mappings, with minimal
accuracy drop (< 0.3%).",2024-09-27,"Matteo Risso, Alessio Burrello, Daniele Jahier Pagliari",http://arxiv.org/pdf/2409.18566v2,cs.LG
CodeSCAN: ScreenCast ANalysis for Video Programming Tutorials,"Programming tutorials in the form of coding screencasts play a crucial role
in programming education, serving both novices and experienced developers.
However, the video format of these tutorials presents a challenge due to the
difficulty of searching for and within videos. Addressing the absence of
large-scale and diverse datasets for screencast analysis, we introduce the
CodeSCAN dataset. It comprises 12,000 screenshots captured from the Visual
Studio Code environment during development, featuring 24 programming languages,
25 fonts, and over 90 distinct themes, in addition to diverse layout changes
and realistic user interactions. Moreover, we conduct detailed quantitative and
qualitative evaluations to benchmark the performance of Integrated Development
Environment (IDE) element detection, color-to-black-and-white conversion, and
Optical Character Recognition (OCR). We hope that our contributions facilitate
more research in coding screencast analysis, and we make the source code for
creating the dataset and the benchmark publicly available on this website.",2024-09-27,"Alexander Naumann, Felix Hertlein, Jacqueline Höllig, Lucas Cazzonelli, Steffen Thoma",http://arxiv.org/pdf/2409.18556v1,cs.LG
Efficient Noise Mitigation for Enhancing Inference Accuracy in DNNs on Mixed-Signal Accelerators,"In this paper, we propose a framework to enhance the robustness of the neural
models by mitigating the effects of process-induced and aging-related
variations of analog computing components on the accuracy of the analog neural
networks. We model these variations as the noise affecting the precision of the
activations and introduce a denoising block inserted between selected layers of
a pre-trained model. We demonstrate that training the denoising block
significantly increases the model's robustness against various noise levels. To
minimize the overhead associated with adding these blocks, we present an
exploration algorithm to identify optimal insertion points for the denoising
blocks. Additionally, we propose a specialized architecture to efficiently
execute the denoising blocks, which can be integrated into mixed-signal
accelerators. We evaluate the effectiveness of our approach using Deep Neural
Network (DNN) models trained on the ImageNet and CIFAR-10 datasets. The results
show that on average, by accepting 2.03% parameter count overhead, the accuracy
drop due to the variations reduces from 31.7% to 1.15%.",2024-09-27,"Seyedarmin Azizi, Mohammad Erfan Sadeghi, Mehdi Kamal, Massoud Pedram",http://arxiv.org/pdf/2409.18553v1,cs.LG
Wasserstein Distance-Weighted Adversarial Network for Cross-Domain Credit Risk Assessment,"This paper delves into the application of adversarial domain adaptation (ADA)
for enhancing credit risk assessment in financial institutions. It addresses
two critical challenges: the cold start problem, where historical lending data
is scarce, and the data imbalance issue, where high-risk transactions are
underrepresented. The paper introduces an improved ADA framework, the
Wasserstein Distance Weighted Adversarial Domain Adaptation Network (WD-WADA),
which leverages the Wasserstein distance to align source and target domains
effectively. The proposed method includes an innovative weighted strategy to
tackle data imbalance, adjusting for both the class distribution and the
difficulty level of predictions. The paper demonstrates that WD-WADA not only
mitigates the cold start problem but also provides a more accurate measure of
domain differences, leading to improved cross-domain credit risk assessment.
Extensive experiments on real-world credit datasets validate the model's
effectiveness, showcasing superior performance in cross-domain learning,
classification accuracy, and model stability compared to traditional methods.",2024-09-27,"Mohan Jiang, Jiating Lin, Hongju Ouyang, Jingming Pan, Siyuan Han, Bingyao Liu",http://arxiv.org/pdf/2409.18544v1,cs.LG
Robustness of AI-based weather forecasts in a changing climate,"Data-driven machine learning models for weather forecasting have made
transformational progress in the last 1-2 years, with state-of-the-art ones now
outperforming the best physics-based models for a wide range of skill scores.
Given the strong links between weather and climate modelling, this raises the
question whether machine learning models could also revolutionize climate
science, for example by informing mitigation and adaptation to climate change
or to generate larger ensembles for more robust uncertainty estimates. Here, we
show that current state-of-the-art machine learning models trained for weather
forecasting in present-day climate produce skillful forecasts across different
climate states corresponding to pre-industrial, present-day, and future 2.9K
warmer climates. This indicates that the dynamics shaping the weather on short
timescales may not differ fundamentally in a changing climate. It also
demonstrates out-of-distribution generalization capabilities of the machine
learning models that are a critical prerequisite for climate applications.
Nonetheless, two of the models show a global-mean cold bias in the forecasts
for the future warmer climate state, i.e. they drift towards the colder
present-day climate they have been trained for. A similar result is obtained
for the pre-industrial case where two out of three models show a warming. We
discuss possible remedies for these biases and analyze their spatial
distribution, revealing complex warming and cooling patterns that are partly
related to missing ocean-sea ice and land surface information in the training
data. Despite these current limitations, our results suggest that data-driven
machine learning models will provide powerful tools for climate science and
transform established approaches by complementing conventional physics-based
models.",2024-09-27,"Thomas Rackow, Nikolay Koldunov, Christian Lessig, Irina Sandu, Mihai Alexe, Matthew Chantry, Mariana Clare, Jesper Dramsch, Florian Pappenberger, Xabier Pedruzo-Bagazgoitia, Steffen Tietsche, Thomas Jung",http://arxiv.org/pdf/2409.18529v1,cs.LG
Token Caching for Diffusion Transformer Acceleration,"Diffusion transformers have gained substantial interest in diffusion
generative modeling due to their outstanding performance. However, their high
computational cost, arising from the quadratic computational complexity of
attention mechanisms and multi-step inference, presents a significant
bottleneck. To address this challenge, we propose TokenCache, a novel
post-training acceleration method that leverages the token-based multi-block
architecture of transformers to reduce redundant computations among tokens
across inference steps. TokenCache specifically addresses three critical
questions in the context of diffusion transformers: (1) which tokens should be
pruned to eliminate redundancy, (2) which blocks should be targeted for
efficient pruning, and (3) at which time steps caching should be applied to
balance speed and quality. In response to these challenges, TokenCache
introduces a Cache Predictor that assigns importance scores to tokens, enabling
selective pruning without compromising model performance. Furthermore, we
propose an adaptive block selection strategy to focus on blocks with minimal
impact on the network's output, along with a Two-Phase Round-Robin (TPRR)
scheduling policy to optimize caching intervals throughout the denoising
process. Experimental results across various models demonstrate that TokenCache
achieves an effective trade-off between generation quality and inference speed
for diffusion transformers. Our code will be publicly available.",2024-09-27,"Jinming Lou, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, Chenguang Ma",http://arxiv.org/pdf/2409.18523v1,cs.LG
Med-IC: Fusing a Single Layer Involution with Convolutions for Enhanced Medical Image Classification and Segmentation,"The majority of medical images, especially those that resemble cells, have
similar characteristics. These images, which occur in a variety of shapes,
often show abnormalities in the organ or cell region. The convolution operation
possesses a restricted capability to extract visual patterns across several
spatial regions of an image. The involution process, which is the inverse
operation of convolution, complements this inherent lack of spatial information
extraction present in convolutions. In this study, we investigate how applying
a single layer of involution prior to a convolutional neural network (CNN)
architecture can significantly improve classification and segmentation
performance, with a comparatively negligible amount of weight parameters. The
study additionally shows how excessive use of involution layers might result in
inaccurate predictions in a particular type of medical image. According to our
findings from experiments, the strategy of adding only a single involution
layer before a CNN-based model outperforms most of the previous works.",2024-09-27,"Md. Farhadul Islam, Sarah Zabeen, Meem Arafat Manab, Mohammad Rakibul Hasan Mahin, Joyanta Jyoti Mondal, Md. Tanzim Reza, Md Zahidul Hasan, Munima Haque, Farig Sadeque, Jannatun Noor",http://arxiv.org/pdf/2409.18506v1,cs.LG
WHOMP: Optimizing Randomized Controlled Trials via Wasserstein Homogeneity,"We investigate methods for partitioning datasets into subgroups that maximize
diversity within each subgroup while minimizing dissimilarity across subgroups.
We introduce a novel partitioning method called the $\textit{Wasserstein
Homogeneity Partition}$ (WHOMP), which optimally minimizes type I and type II
errors that often result from imbalanced group splitting or partitioning,
commonly referred to as accidental bias, in comparative and controlled trials.
We conduct an analytical comparison of WHOMP against existing partitioning
methods, such as random subsampling, covariate-adaptive randomization,
rerandomization, and anti-clustering, demonstrating its advantages. Moreover,
we characterize the optimal solutions to the WHOMP problem and reveal an
inherent trade-off between the stability of subgroup means and variances among
these solutions. Based on our theoretical insights, we design algorithms that
not only obtain these optimal solutions but also equip practitioners with tools
to select the desired trade-off. Finally, we validate the effectiveness of
WHOMP through numerical experiments, highlighting its superiority over
traditional methods.",2024-09-27,"Shizhou Xu, Thomas Strohmer",http://arxiv.org/pdf/2409.18504v2,cs.LG
Fairness-aware Multiobjective Evolutionary Learning,"Multiobjective evolutionary learning (MOEL) has demonstrated its advantages
of training fairer machine learning models considering a predefined set of
conflicting objectives, including accuracy and different fairness measures.
Recent works propose to construct a representative subset of fairness measures
as optimisation objectives of MOEL throughout model training. However, the
determination of a representative measure set relies on dataset, prior
knowledge and requires substantial computational costs. What's more, those
representative measures may differ across different model training processes.
Instead of using a static predefined set determined before model training, this
paper proposes to dynamically and adaptively determine a representative measure
set online during model training. The dynamically determined representative set
is then used as optimising objectives of the MOEL framework and can vary with
time. Extensive experimental results on 12 well-known benchmark datasets
demonstrate that our proposed framework achieves outstanding performance
compared to state-of-the-art approaches for mitigating unfairness in terms of
accuracy as well as 25 fairness measures although only a few of them were
dynamically selected and used as optimisation objectives. The results indicate
the importance of setting optimisation objectives dynamically during training.",2024-09-27,"Qingquan Zhang, Jialin Liu, Xin Yao",http://arxiv.org/pdf/2409.18499v1,cs.LG
Treating Brain-inspired Memories as Priors for Diffusion Model to Forecast Multivariate Time Series,"Forecasting Multivariate Time Series (MTS) involves significant challenges in
various application domains. One immediate challenge is modeling temporal
patterns with the finite length of the input. These temporal patterns usually
involve periodic and sudden events that recur across different channels. To
better capture temporal patterns, we get inspiration from humans' memory
mechanisms and propose a channel-shared, brain-inspired memory module for MTS.
Specifically, brain-inspired memory comprises semantic and episodic memory,
where the former is used to capture general patterns, such as periodic events,
and the latter is employed to capture special patterns, such as sudden events,
respectively. Meanwhile, we design corresponding recall and update mechanisms
to better utilize these patterns. Furthermore, acknowledging the capacity of
diffusion models to leverage memory as a prior, we present a brain-inspired
memory-augmented diffusion model. This innovative model retrieves relevant
memories for different channels, utilizing them as distinct priors for MTS
predictions. This incorporation significantly enhances the accuracy and
robustness of predictions. Experimental results on eight datasets consistently
validate the superiority of our approach in capturing and leveraging diverse
recurrent temporal patterns across different channels.",2024-09-27,"Muyao Wang, Wenchao Chen, Zhibin Duan, Bo Chen",http://arxiv.org/pdf/2409.18491v1,cs.LG
HSTFL: A Heterogeneous Federated Learning Framework for Misaligned Spatiotemporal Forecasting,"Spatiotemporal forecasting has emerged as an indispensable building block of
diverse smart city applications, such as intelligent transportation and smart
energy management. Recent advancements have uncovered that the performance of
spatiotemporal forecasting can be significantly improved by integrating
knowledge in geo-distributed time series data from different domains, \eg
enhancing real-estate appraisal with human mobility data; joint taxi and bike
demand predictions. While effective, existing approaches assume a centralized
data collection and exploitation environment, overlooking the privacy and
commercial interest concerns associated with data owned by different parties.
In this paper, we investigate multi-party collaborative spatiotemporal
forecasting without direct access to multi-source private data. However, this
task is challenging due to 1) cross-domain feature heterogeneity and 2)
cross-client geographical heterogeneity, where standard horizontal or vertical
federated learning is inapplicable. To this end, we propose a Heterogeneous
SpatioTemporal Federated Learning (HSTFL) framework to enable multiple clients
to collaboratively harness geo-distributed time series data from different
domains while preserving privacy. Specifically, we first devise vertical
federated spatiotemporal representation learning to locally preserve
spatiotemporal dependencies among individual participants and generate
effective representations for heterogeneous data. Then we propose a
cross-client virtual node alignment block to incorporate cross-client
spatiotemporal dependencies via a multi-level knowledge fusion scheme.
Extensive privacy analysis and experimental evaluations demonstrate that HSTFL
not only effectively resists inference attacks but also provides a significant
improvement against various baselines.",2024-09-27,"Shuowei Cai, Hao Liu",http://arxiv.org/pdf/2409.18482v1,cs.LG
Deep Heterogeneous Contrastive Hyper-Graph Learning for In-the-Wild Context-Aware Human Activity Recognition,"Human Activity Recognition (HAR) is a challenging, multi-label classification
problem as activities may co-occur and sensor signals corresponding to the same
activity may vary in different contexts (e.g., different device placements).
This paper proposes a Deep Heterogeneous Contrastive Hyper-Graph Learning
(DHC-HGL) framework that captures heterogenous Context-Aware HAR (CA-HAR)
hypergraph properties in a message-passing and neighborhood-aggregation
fashion. Prior work only explored homogeneous or shallow-node-heterogeneous
graphs. DHC-HGL handles heterogeneous CA-HAR data by innovatively 1)
Constructing three different types of sub-hypergraphs that are each passed
through different custom HyperGraph Convolution (HGC) layers designed to handle
edge-heterogeneity and 2) Adopting a contrastive loss function to ensure
node-heterogeneity. In rigorous evaluation on two CA-HAR datasets, DHC-HGL
significantly outperformed state-of-the-art baselines by 5.8% to 16.7% on
Matthews Correlation Coefficient (MCC) and 3.0% to 8.4% on Macro F1 scores.
UMAP visualizations of learned CA-HAR node embeddings are also presented to
enhance model explainability.",2024-09-27,"Wen Ge, Guanyi Mou, Emmanuel O. Agu, Kyumin Lee",http://arxiv.org/pdf/2409.18481v1,cs.LG
CycleNet: Enhancing Time Series Forecasting through Modeling Periodic Patterns,"The stable periodic patterns present in time series data serve as the
foundation for conducting long-horizon forecasts. In this paper, we pioneer the
exploration of explicitly modeling this periodicity to enhance the performance
of models in long-term time series forecasting (LTSF) tasks. Specifically, we
introduce the Residual Cycle Forecasting (RCF) technique, which utilizes
learnable recurrent cycles to model the inherent periodic patterns within
sequences, and then performs predictions on the residual components of the
modeled cycles. Combining RCF with a Linear layer or a shallow MLP forms the
simple yet powerful method proposed in this paper, called CycleNet. CycleNet
achieves state-of-the-art prediction accuracy in multiple domains including
electricity, weather, and energy, while offering significant efficiency
advantages by reducing over 90% of the required parameter quantity.
Furthermore, as a novel plug-and-play technique, the RCF can also significantly
improve the prediction accuracy of existing models, including PatchTST and
iTransformer. The source code is available at:
https://github.com/ACAT-SCUT/CycleNet.",2024-09-27,"Shengsheng Lin, Weiwei Lin, Xinyi Hu, Wentai Wu, Ruichao Mo, Haocheng Zhong",http://arxiv.org/pdf/2409.18479v2,cs.LG
URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological and Multilingual Knowledge Base,"URIEL is a knowledge base offering geographical, phylogenetic, and
typological vector representations for 7970 languages. It includes distance
measures between these vectors for 4005 languages, which are accessible via the
lang2vec tool. Despite being frequently cited, URIEL is limited in terms of
linguistic inclusion and overall usability. To tackle these challenges, we
introduce URIEL+, an enhanced version of URIEL and lang2vec that addresses
these limitations. In addition to expanding typological feature coverage for
2898 languages, URIEL+ improves the user experience with robust, customizable
distance calculations to better suit the needs of users. These upgrades also
offer competitive performance on downstream tasks and provide distances that
better align with linguistic distance studies.",2024-09-27,"Aditya Khan, Mason Shipton, David Anugraha, Kaiyao Duan, Phuong H. Hoang, Eric Khiu, A. Seza Doğruöz, En-Shiun Annie Lee",http://arxiv.org/pdf/2409.18472v3,cs.LG
Fairness without Sensitive Attributes via Knowledge Sharing,"While model fairness improvement has been explored previously, existing
methods invariably rely on adjusting explicit sensitive attribute values in
order to improve model fairness in downstream tasks. However, we observe a
trend in which sensitive demographic information becomes inaccessible as public
concerns around data privacy grow. In this paper, we propose a confidence-based
hierarchical classifier structure called ""Reckoner"" for reliable fair model
learning under the assumption of missing sensitive attributes. We first present
results showing that if the dataset contains biased labels or other hidden
biases, classifiers significantly increase the bias gap across different
demographic groups in the subset with higher prediction confidence. Inspired by
these findings, we devised a dual-model system in which a version of the model
initialised with a high-confidence data subset learns from a version of the
model initialised with a low-confidence data subset, enabling it to avoid
biased predictions. Our experimental results show that Reckoner consistently
outperforms state-of-the-art baselines in COMPAS dataset and New Adult dataset,
considering both accuracy and fairness metrics.",2024-09-27,"Hongliang Ni, Lei Han, Tong Chen, Shazia Sadiq, Gianluca Demartini",http://arxiv.org/pdf/2409.18470v1,cs.LG
A TextGCN-Based Decoding Approach for Improving Remote Sensing Image Captioning,"Remote sensing images are highly valued for their ability to address complex
real-world issues such as risk management, security, and meteorology. However,
manually captioning these images is challenging and requires specialized
knowledge across various domains. This letter presents an approach for
automatically describing (captioning) remote sensing images. We propose a novel
encoder-decoder setup that deploys a Text Graph Convolutional Network (TextGCN)
and multi-layer LSTMs. The embeddings generated by TextGCN enhance the
decoder's understanding by capturing the semantic relationships among words at
both the sentence and corpus levels. Furthermore, we advance our approach with
a comparison-based beam search method to ensure fairness in the search strategy
for generating the final caption. We present an extensive evaluation of our
approach against various other state-of-the-art encoder-decoder frameworks. We
evaluated our method across three datasets using seven metrics: BLEU-1 to
BLEU-4, METEOR, ROUGE-L, and CIDEr. The results demonstrate that our approach
significantly outperforms other state-of-the-art encoder-decoder methods.",2024-09-27,"Swadhin Das, Raksha Sharma",http://arxiv.org/pdf/2409.18467v3,cs.LG
Latent Representation Learning for Multimodal Brain Activity Translation,"Neuroscience employs diverse neuroimaging techniques, each offering distinct
insights into brain activity, from electrophysiological recordings such as EEG,
which have high temporal resolution, to hemodynamic modalities such as fMRI,
which have increased spatial precision. However, integrating these
heterogeneous data sources remains a challenge, which limits a comprehensive
understanding of brain function. We present the Spatiotemporal Alignment of
Multimodal Brain Activity (SAMBA) framework, which bridges the spatial and
temporal resolution gaps across modalities by learning a unified latent space
free of modality-specific biases. SAMBA introduces a novel attention-based
wavelet decomposition for spectral filtering of electrophysiological
recordings, graph attention networks to model functional connectivity between
functional brain units, and recurrent layers to capture temporal
autocorrelations in brain signal. We show that the training of SAMBA, aside
from achieving translation, also learns a rich representation of brain
information processing. We showcase this classify external stimuli driving
brain activity from the representation learned in hidden layers of SAMBA,
paving the way for broad downstream applications in neuroscience research and
clinical contexts.",2024-09-27,"Arman Afrasiyabi, Dhananjay Bhaskar, Erica L. Busch, Laurent Caplette, Rahul Singh, Guillaume Lajoie, Nicholas B. Turk-Browne, Smita Krishnaswamy",http://arxiv.org/pdf/2409.18462v1,cs.LG
Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration,"Federated Learning has emerged as a promising paradigm for collaborative
machine learning, while preserving user data privacy. Despite its potential,
standard FL lacks support for diverse heterogeneous device prototypes, which
vary significantly in model and dataset sizes -- from small IoT devices to
large workstations. This limitation is only partially addressed by existing
knowledge distillation techniques, which often fail to transfer knowledge
effectively across a broad spectrum of device prototypes with varied
capabilities. This failure primarily stems from two issues: the dilution of
informative logits from more capable devices by those from less capable ones,
and the use of a single integrated logits as the distillation target across all
devices, which neglects their individual learning capacities and and the unique
contributions of each. To address these challenges, we introduce TAKFL, a novel
KD-based framework that treats the knowledge transfer from each device
prototype's ensemble as a separate task, independently distilling each to
preserve its unique contributions and avoid dilution. TAKFL also incorporates a
KD-based self-regularization technique to mitigate the issues related to the
noisy and unsupervised ensemble distillation process. To integrate the
separately distilled knowledge, we introduce an adaptive task arithmetic
knowledge integration process, allowing each student model to customize the
knowledge integration for optimal performance. Additionally, we present
theoretical results demonstrating the effectiveness of task arithmetic in
transferring knowledge across heterogeneous devices with varying capacities.
Comprehensive evaluations of our method across both CV and NLP tasks
demonstrate that TAKFL achieves SOTA results in a variety of datasets and
settings, significantly outperforming existing KD-based methods Code is
released at https://github.com/MMorafah/TAKFL",2024-09-27,"Mahdi Morafah, Vyacheslav Kungurtsev, Hojin Chang, Chen Chen, Bill Lin",http://arxiv.org/pdf/2409.18461v2,cs.LG
Review of Digital Asset Development with Graph Neural Network Unlearning,"In the rapidly evolving landscape of digital assets, the imperative for
robust data privacy and compliance with regulatory frameworks has intensified.
This paper investigates the critical role of Graph Neural Networks (GNNs) in
the management of digital assets and introduces innovative unlearning
techniques specifically tailored to GNN architectures. We categorize unlearning
strategies into two primary classes: data-driven approximation, which
manipulates the graph structure to isolate and remove the influence of specific
nodes, and model-driven approximation, which modifies the internal parameters
and architecture of the GNN itself. By examining recent advancements in these
unlearning methodologies, we highlight their applicability in various use
cases, including fraud detection, risk assessment, token relationship
prediction, and decentralized governance. We discuss the challenges inherent in
balancing model performance with the requirements for data unlearning,
particularly in the context of real-time financial applications. Furthermore,
we propose a hybrid approach that combines the strengths of both unlearning
strategies to enhance the efficiency and effectiveness of GNNs in digital asset
ecosystems. Ultimately, this paper aims to provide a comprehensive framework
for understanding and implementing GNN unlearning techniques, paving the way
for secure and compliant deployment of machine learning in the digital asset
domain.",2024-09-27,Zara Lisbon,http://arxiv.org/pdf/2409.18455v1,cs.LG
Hierarchical Federated Learning with Multi-Timescale Gradient Correction,"While traditional federated learning (FL) typically focuses on a star
topology where clients are directly connected to a central server, real-world
distributed systems often exhibit hierarchical architectures. Hierarchical FL
(HFL) has emerged as a promising solution to bridge this gap, leveraging
aggregation points at multiple levels of the system. However, existing
algorithms for HFL encounter challenges in dealing with multi-timescale model
drift, i.e., model drift occurring across hierarchical levels of data
heterogeneity. In this paper, we propose a multi-timescale gradient correction
(MTGC) methodology to resolve this issue. Our key idea is to introduce distinct
control variables to (i) correct the client gradient towards the group
gradient, i.e., to reduce client model drift caused by local updates based on
individual datasets, and (ii) correct the group gradient towards the global
gradient, i.e., to reduce group model drift caused by FL over clients within
the group. We analytically characterize the convergence behavior of MTGC under
general non-convex settings, overcoming challenges associated with couplings
between correction terms. We show that our convergence bound is immune to the
extent of data heterogeneity, confirming the stability of the proposed
algorithm against multi-level non-i.i.d. data. Through extensive experiments on
various datasets and models, we validate the effectiveness of MTGC in diverse
HFL settings. The code for this project is available at
\href{https://github.com/wenzhifang/MTGC}{https://github.com/wenzhifang/MTGC}.",2024-09-27,"Wenzhi Fang, Dong-Jun Han, Evan Chen, Shiqiang Wang, Christopher G. Brinton",http://arxiv.org/pdf/2409.18448v3,cs.LG
Analysis and Optimization of Seismic Monitoring Networks with Bayesian Optimal Experiment Design,"Monitoring networks increasingly aim to assimilate data from a large number
of diverse sensors covering many sensing modalities. Bayesian optimal
experimental design (OED) seeks to identify data, sensor configurations, or
experiments which can optimally reduce uncertainty and hence increase the
performance of a monitoring network. Information theory guides OED by
formulating the choice of experiment or sensor placement as an optimization
problem that maximizes the expected information gain (EIG) about quantities of
interest given prior knowledge and models of expected observation data.
Therefore, within the context of seismo-acoustic monitoring, we can use
Bayesian OED to configure sensor networks by choosing sensor locations, types,
and fidelity in order to improve our ability to identify and locate seismic
sources. In this work, we develop the framework necessary to use Bayesian OED
to optimize a sensor network's ability to locate seismic events from arrival
time data of detected seismic phases at the regional-scale. Bayesian OED
requires four elements:
  1) A likelihood function that describes the distribution of detection and
travel time data from the sensor network,
  2) A Bayesian solver that uses a prior and likelihood to identify the
posterior distribution of seismic events given the data,
  3) An algorithm to compute EIG about seismic events over a dataset of
hypothetical prior events,
  4) An optimizer that finds a sensor network which maximizes EIG.
  Once we have developed this framework, we explore many relevant questions to
monitoring such as: how to trade off sensor fidelity and earth model
uncertainty; how sensor types, number, and locations influence uncertainty; and
how prior models and constraints influence sensor placement.",2024-09-27,"Jake Callahan, Kevin Monogue, Ruben Villarreal, Tommie Catanach",http://arxiv.org/pdf/2410.07215v2,cs.LG
Gradient-free Decoder Inversion in Latent Diffusion Models,"In latent diffusion models (LDMs), denoising diffusion process efficiently
takes place on latent space whose dimension is lower than that of pixel space.
Decoder is typically used to transform the representation in latent space to
that in pixel space. While a decoder is assumed to have an encoder as an
accurate inverse, exact encoder-decoder pair rarely exists in practice even
though applications often require precise inversion of decoder. Prior works for
decoder inversion in LDMs employed gradient descent inspired by inversions of
generative adversarial networks. However, gradient-based methods require larger
GPU memory and longer computation time for larger latent space. For example,
recent video LDMs can generate more than 16 frames, but GPUs with 24 GB memory
can only perform gradient-based decoder inversion for 4 frames. Here, we
propose an efficient gradient-free decoder inversion for LDMs, which can be
applied to diverse latent models. Theoretical convergence property of our
proposed inversion has been investigated not only for the forward step method,
but also for the inertial Krasnoselskii-Mann (KM) iterations under mild
assumption on cocoercivity that is satisfied by recent LDMs. Our proposed
gradient-free method with Adam optimizer and learning rate scheduling
significantly reduced computation time and memory usage over prior
gradient-based methods and enabled efficient computation in applications such
as noise-space watermarking while achieving comparable error levels.",2024-09-27,"Seongmin Hong, Suh Yoon Jeon, Kyeonghyun Lee, Ernest K. Ryu, Se Young Chun",http://arxiv.org/pdf/2409.18442v1,cs.LG
State-free Reinforcement Learning,"In this work, we study the \textit{state-free RL} problem, where the
algorithm does not have the states information before interacting with the
environment. Specifically, denote the reachable state set by ${S}^\Pi := \{
s|\max_{\pi\in \Pi}q^{P, \pi}(s)>0 \}$, we design an algorithm which requires
no information on the state space $S$ while having a regret that is completely
independent of ${S}$ and only depend on ${S}^\Pi$. We view this as a concrete
first step towards \textit{parameter-free RL}, with the goal of designing RL
algorithms that require no hyper-parameter tuning.",2024-09-27,"Mingyu Chen, Aldo Pacchiano, Xuezhou Zhang",http://arxiv.org/pdf/2409.18439v1,cs.LG
Multi-agent Reinforcement Learning for Dynamic Dispatching in Material Handling Systems,"This paper proposes a multi-agent reinforcement learning (MARL) approach to
learn dynamic dispatching strategies, which is crucial for optimizing
throughput in material handling systems across diverse industries. To benchmark
our method, we developed a material handling environment that reflects the
complexities of an actual system, such as various activities at different
locations, physical constraints, and inherent uncertainties. To enhance
exploration during learning, we propose a method to integrate domain knowledge
in the form of existing dynamic dispatching heuristics. Our experimental
results show that our method can outperform heuristics by up to 7.4 percent in
terms of median throughput. Additionally, we analyze the effect of different
architectures on MARL performance when training multiple agents with different
functions. We also demonstrate that the MARL agents performance can be further
improved by using the first iteration of MARL agents as heuristics to train a
second iteration of MARL agents. This work demonstrates the potential of
applying MARL to learn effective dynamic dispatching strategies that may be
deployed in real-world systems to improve business outcomes.",2024-09-27,"Xian Yeow Lee, Haiyan Wang, Daisuke Katsumata, Takaharu Matsui, Chetan Gupta",http://arxiv.org/pdf/2409.18435v1,cs.LG
Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization,"While generalization over tasks from easy to hard is crucial to profile
language models (LLMs), the datasets with fine-grained difficulty annotations
for each problem across a broad range of complexity are still blank. Aiming to
address this limitation, we present Easy2Hard-Bench, a consistently formatted
collection of 6 benchmark datasets spanning various domains, such as
mathematics and programming problems, chess puzzles, and reasoning questions.
Each problem within these datasets is annotated with numerical difficulty
scores. To systematically estimate problem difficulties, we collect abundant
performance data on attempts to each problem by humans in the real world or
LLMs on the prominent leaderboard. Leveraging the rich performance data, we
apply well-established difficulty ranking systems, such as Item Response Theory
(IRT) and Glicko-2 models, to uniformly assign numerical difficulty scores to
problems. Moreover, datasets in Easy2Hard-Bench distinguish themselves from
previous collections by a higher proportion of challenging problems. Through
extensive experiments with six state-of-the-art LLMs, we provide a
comprehensive analysis of their performance and generalization capabilities
across varying levels of difficulty, with the aim of inspiring future research
in LLM generalization. The datasets are available at
https://huggingface.co/datasets/furonghuang-lab/Easy2Hard-Bench.",2024-09-27,"Mucong Ding, Chenghao Deng, Jocelyn Choo, Zichu Wu, Aakriti Agrawal, Avi Schwarzschild, Tianyi Zhou, Tom Goldstein, John Langford, Anima Anandkumar, Furong Huang",http://arxiv.org/pdf/2409.18433v1,cs.LG
Neural Collaborative Filtering to Detect Anomalies in Human Semantic Trajectories,"Human trajectory anomaly detection has become increasingly important across a
wide range of applications, including security surveillance and public health.
However, existing trajectory anomaly detection methods are primarily focused on
vehicle-level traffic, while human-level trajectory anomaly detection remains
under-explored. Since human trajectory data is often very sparse, machine
learning methods have become the preferred approach for identifying complex
patterns. However, concerns regarding potential biases and the robustness of
these models have intensified the demand for more transparent and explainable
alternatives. In response to these challenges, our research focuses on
developing a lightweight anomaly detection model specifically designed to
detect anomalies in human trajectories. We propose a Neural Collaborative
Filtering approach to model and predict normal mobility. Our method is designed
to model users' daily patterns of life without requiring prior knowledge,
thereby enhancing performance in scenarios where data is sparse or incomplete,
such as in cold start situations. Our algorithm consists of two main modules.
The first is the collaborative filtering module, which applies collaborative
filtering to model normal mobility of individual humans to places of interest.
The second is the neural module, responsible for interpreting the complex
spatio-temporal relationships inherent in human trajectory data. To validate
our approach, we conducted extensive experiments using simulated and real-world
datasets comparing to numerous state-of-the-art trajectory anomaly detection
approaches.",2024-09-27,"Yueyang Liu, Lance Kennedy, Hossein Amiri, Andreas Züfle",http://arxiv.org/pdf/2409.18427v3,cs.LG
Dual Cone Gradient Descent for Training Physics-Informed Neural Networks,"Physics-informed neural networks (PINNs) have emerged as a prominent approach
for solving partial differential equations (PDEs) by minimizing a combined loss
function that incorporates both boundary loss and PDE residual loss. Despite
their remarkable empirical performance in various scientific computing tasks,
PINNs often fail to generate reasonable solutions, and such pathological
behaviors remain difficult to explain and resolve. In this paper, we identify
that PINNs can be adversely trained when gradients of each loss function
exhibit a significant imbalance in their magnitudes and present a negative
inner product value. To address these issues, we propose a novel optimization
framework, Dual Cone Gradient Descent (DCGD), which adjusts the direction of
the updated gradient to ensure it falls within a dual cone region. This region
is defined as a set of vectors where the inner products with both the gradients
of the PDE residual loss and the boundary loss are non-negative. Theoretically,
we analyze the convergence properties of DCGD algorithms in a non-convex
setting. On a variety of benchmark equations, we demonstrate that DCGD
outperforms other optimization algorithms in terms of various evaluation
metrics. In particular, DCGD achieves superior predictive accuracy and enhances
the stability of training for failure modes of PINNs and complex PDEs, compared
to existing optimally tuned models. Moreover, DCGD can be further improved by
combining it with popular strategies for PINNs, including learning rate
annealing and the Neural Tangent Kernel (NTK).",2024-09-27,"Youngsik Hwang, Dong-Young Lim",http://arxiv.org/pdf/2409.18426v2,cs.LG
A physics-driven sensor placement optimization methodology for temperature field reconstruction,"Perceiving the global field from sparse sensors has been a grand challenge in
the monitoring, analysis, and design of physical systems. In this context,
sensor placement optimization is a crucial issue. Most existing works require
large and sufficient data to construct data-based criteria, which are
intractable in data-free scenarios without numerical and experimental data. To
this end, we propose a novel physics-driven sensor placement optimization
(PSPO) method for temperature field reconstruction using a physics-based
criterion to optimize sensor locations. In our methodological framework, we
firstly derive the theoretical upper and lower bounds of the reconstruction
error under noise scenarios by analyzing the optimal solution, proving that
error bounds correlate with the condition number determined by sensor
locations. Furthermore, the condition number, as the physics-based criterion,
is used to optimize sensor locations by the genetic algorithm. Finally, the
best sensors are validated by reconstruction models, including non-invasive
end-to-end models, non-invasive reduced-order models, and physics-informed
models. Experimental results, both on a numerical and an application case,
demonstrate that the PSPO method significantly outperforms random and uniform
selection methods, improving the reconstruction accuracy by nearly an order of
magnitude. Moreover, the PSPO method can achieve comparable reconstruction
accuracy to the existing data-driven placement optimization methods.",2024-09-27,"Xu Liu, Wen Yao, Wei Peng, Zhuojia Fu, Zixue Xiang, Xiaoqian Chen",http://arxiv.org/pdf/2409.18423v1,cs.LG
Robust Network Learning via Inverse Scale Variational Sparsification,"While neural networks have made significant strides in many AI tasks, they
remain vulnerable to a range of noise types, including natural corruptions,
adversarial noise, and low-resolution artifacts. Many existing approaches focus
on enhancing robustness against specific noise types, limiting their
adaptability to others. Previous studies have addressed general robustness by
adopting a spectral perspective, which tends to blur crucial features like
texture and object contours. Our proposed solution, however, introduces an
inverse scale variational sparsification framework within a time-continuous
inverse scale space formulation. This framework progressively learns
finer-scale features by discerning variational differences between pixels,
ultimately preserving only large-scale features in the smoothed image. Unlike
frequency-based methods, our approach not only removes noise by smoothing
small-scale features where corruptions often occur but also retains
high-contrast details such as textures and object contours. Moreover, our
framework offers simplicity and efficiency in implementation. By integrating
this algorithm into neural network training, we guide the model to prioritize
learning large-scale features. We show the efficacy of our approach through
enhanced robustness against various noise types.",2024-09-27,"Zhiling Zhou, Zirui Liu, Chengming Xu, Yanwei Fu, Xinwei Sun",http://arxiv.org/pdf/2409.18419v1,cs.LG
A3: Active Adversarial Alignment for Source-Free Domain Adaptation,"Unsupervised domain adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. Recent works have focused
on source-free UDA, where only target data is available. This is challenging as
models rely on noisy pseudo-labels and struggle with distribution shifts. We
propose Active Adversarial Alignment (A3), a novel framework combining
self-supervised learning, adversarial training, and active learning for robust
source-free UDA. A3 actively samples informative and diverse data using an
acquisition function for training. It adapts models via adversarial losses and
consistency regularization, aligning distributions without source data access.
A3 advances source-free UDA through its synergistic integration of active and
adversarial learning for effective domain alignment and noise reduction.",2024-09-27,"Chrisantus Eze, Christopher Crick",http://arxiv.org/pdf/2409.18418v2,cs.LG
VickreyFeedback: Cost-efficient Data Construction for Reinforcement Learning from Human Feedback,"This paper addresses the cost-efficiency aspect of Reinforcement Learning
from Human Feedback (RLHF). RLHF leverages datasets of human preferences over
outputs of large language models (LLM)s to instill human expectations into
LLMs. Although preference annotation comes with a monetized cost, the economic
utility of a preference dataset has not been considered by far. What
exacerbates this situation is that, given complex intransitive or cyclic
relationships in preference datasets, existing algorithms for fine-tuning LLMs
are still far from capturing comprehensive preferences. This raises severe
cost-efficiency concerns in production environments, where preference data
accumulate over time. In this paper, we discuss the fine-tuning of LLMs as a
monetized economy and introduce an auction mechanism to improve the efficiency
of preference data collection in dollar terms. We show that introducing an
auction mechanism can play an essential role in enhancing the cost-efficiency
of RLHF, while maintaining satisfactory model performance. Experimental results
demonstrate that our proposed auction-based protocol is cost-effective for
fine-tuning LLMs concentrating on high-quality feedback.",2024-09-27,"Guoxi Zhang, Jiuding Duan",http://arxiv.org/pdf/2409.18417v2,cs.LG
Embed and Emulate: Contrastive representations for simulation-based inference,"Scientific modeling and engineering applications rely heavily on parameter
estimation methods to fit physical models and calibrate numerical simulations
using real-world measurements. In the absence of analytic statistical models
with tractable likelihoods, modern simulation-based inference (SBI) methods
first use a numerical simulator to generate a dataset of parameters and
simulated outputs. This dataset is then used to approximate the likelihood and
estimate the system parameters given observation data. Several SBI methods
employ machine learning emulators to accelerate data generation and parameter
estimation. However, applying these approaches to high-dimensional physical
systems remains challenging due to the cost and complexity of training
high-dimensional emulators. This paper introduces Embed and Emulate (E&E): a
new SBI method based on contrastive learning that efficiently handles
high-dimensional data and complex, multimodal parameter posteriors. E&E learns
a low-dimensional latent embedding of the data (i.e., a summary statistic) and
a corresponding fast emulator in the latent space, eliminating the need to run
expensive simulations or a high dimensional emulator during inference. We
illustrate the theoretical properties of the learned latent space through a
synthetic experiment and demonstrate superior performance over existing methods
in a realistic, non-identifiable parameter estimation task using the
high-dimensional, chaotic Lorenz 96 system.",2024-09-27,"Ruoxi Jiang, Peter Y. Lu, Rebecca Willett",http://arxiv.org/pdf/2409.18402v1,cs.LG
Scientific Machine Learning Seismology,"Scientific machine learning (SciML) is an interdisciplinary research field
that integrates machine learning, particularly deep learning, with physics
theory to understand and predict complex natural phenomena. By incorporating
physical knowledge, SciML reduces the dependency on observational data, which
is often limited in the natural sciences. In this article, the fundamental
concepts of SciML, its applications in seismology, and prospects are described.
Specifically, two popular methods are mainly discussed: physics-informed neural
networks (PINNs) and neural operators (NOs). PINNs can address both forward and
inverse problems by incorporating governing laws into the loss functions. The
use of PINNs is expanding into areas such as simultaneous solutions of
differential equations, inference in underdetermined systems, and
regularization based on physics. These research directions would broaden the
scope of deep learning in natural sciences. NOs are models designed for
operator learning, which deals with relationships between infinite-dimensional
spaces. NOs show promise in modeling the time evolution of complex systems
based on observational or simulation data. Since large amounts of data are
often required, combining NOs with physics-informed learning holds significant
potential. Finally, SciML is considered from a broader perspective beyond deep
learning: statistical (or mathematical) frameworks that integrate observational
data with physical principles to model natural phenomena. In seismology,
mathematically rigorous Bayesian statistics has been developed over the past
decades, whereas more flexible and scalable deep learning has only emerged
recently. Both approaches can be considered as part of SciML in a broad sense.
Theoretical and practical insights in both directions would advance SciML
methodologies and thereby deepen our understanding of earthquake phenomena.",2024-09-27,Tomohisa Okazaki,http://arxiv.org/pdf/2409.18397v2,cs.LG
CurricuLLM: Automatic Task Curricula Design for Learning Complex Robot Skills using Large Language Models,"Curriculum learning is a training mechanism in reinforcement learning (RL)
that facilitates the achievement of complex policies by progressively
increasing the task difficulty during training. However, designing effective
curricula for a specific task often requires extensive domain knowledge and
human intervention, which limits its applicability across various domains. Our
core idea is that large language models (LLMs), with their extensive training
on diverse language data and ability to encapsulate world knowledge, present
significant potential for efficiently breaking down tasks and decomposing
skills across various robotics environments. Additionally, the demonstrated
success of LLMs in translating natural language into executable code for RL
agents strengthens their role in generating task curricula. In this work, we
propose CurricuLLM, which leverages the high-level planning and programming
capabilities of LLMs for curriculum design, thereby enhancing the efficient
learning of complex target tasks. CurricuLLM consists of: (Step 1) Generating
sequence of subtasks that aid target task learning in natural language form,
(Step 2) Translating natural language description of subtasks in executable
task code, including the reward code and goal distribution code, and (Step 3)
Evaluating trained policies based on trajectory rollout and subtask
description. We evaluate CurricuLLM in various robotics simulation
environments, ranging from manipulation, navigation, and locomotion, to show
that CurricuLLM can aid learning complex robot control tasks. In addition, we
validate humanoid locomotion policy learned through CurricuLLM in real-world.
Project website is https://iconlab.negarmehr.com/CurricuLLM/",2024-09-27,"Kanghyun Ryu, Qiayuan Liao, Zhongyu Li, Payam Delgosha, Koushil Sreenath, Negar Mehr",http://arxiv.org/pdf/2409.18382v2,cs.LG
Adaptive Learning of the Latent Space of Wasserstein Generative Adversarial Networks,"Generative models based on latent variables, such as generative adversarial
networks (GANs) and variational auto-encoders (VAEs), have gained lots of
interests due to their impressive performance in many fields. However, many
data such as natural images usually do not populate the ambient Euclidean space
but instead reside in a lower-dimensional manifold. Thus an inappropriate
choice of the latent dimension fails to uncover the structure of the data,
possibly resulting in mismatch of latent representations and poor generative
qualities. Towards addressing these problems, we propose a novel framework
called the latent Wasserstein GAN (LWGAN) that fuses the Wasserstein
auto-encoder and the Wasserstein GAN so that the intrinsic dimension of the
data manifold can be adaptively learned by a modified informative latent
distribution. We prove that there exist an encoder network and a generator
network in such a way that the intrinsic dimension of the learned encoding
distribution is equal to the dimension of the data manifold. We theoretically
establish that our estimated intrinsic dimension is a consistent estimate of
the true dimension of the data manifold. Meanwhile, we provide an upper bound
on the generalization error of LWGAN, implying that we force the synthetic data
distribution to be similar to the real data distribution from a population
perspective. Comprehensive empirical experiments verify our framework and show
that LWGAN is able to identify the correct intrinsic dimension under several
scenarios, and simultaneously generate high-quality synthetic data by sampling
from the learned latent distribution.",2024-09-27,"Yixuan Qiu, Qingyi Gao, Xiao Wang",http://arxiv.org/pdf/2409.18374v1,cs.LG
A Model-Constrained Discontinuous Galerkin Network (DGNet) for Compressible Euler Equations with Out-of-Distribution Generalization,"Real-time accurate solutions of large-scale complex dynamical systems are
critically needed for control, optimization, uncertainty quantification, and
decision-making in practical engineering and science applications, particularly
in digital twin contexts. In this work, we develop a model-constrained
discontinuous Galerkin Network (DGNet) approach, a significant extension to our
previous work [Model-constrained Tagent Slope Learning Approach for Dynamical
Systems], for compressible Euler equations with out-of-distribution
generalization. The core of DGNet is the synergy of several key strategies: (i)
leveraging time integration schemes to capture temporal correlation and taking
advantage of neural network speed for computation time reduction; (ii)
employing a model-constrained approach to ensure the learned tangent slope
satisfies governing equations; (iii) utilizing a GNN-inspired architecture
where edges represent Riemann solver surrogate models and nodes represent
volume integration correction surrogate models, enabling capturing
discontinuity capability, aliasing error reduction, and mesh discretization
generalizability; (iv) implementing the input normalization technique that
allows surrogate models to generalize across different initial conditions,
geometries, meshes, boundary conditions, and solution orders; and (v)
incorporating a data randomization technique that not only implicitly promotes
agreement between surrogate models and true numerical models up to second-order
derivatives, ensuring long-term stability and prediction capacity, but also
serves as a data generation engine during training, leading to enhanced
generalization on unseen data. To validate the effectiveness, stability, and
generalizability of our novel DGNet approach, we present comprehensive
numerical results for 1D and 2D compressible Euler equation problems.",2024-09-27,"Hai V. Nguyen, Jau-Uei Chen, Tan Bui-Thanh",http://arxiv.org/pdf/2409.18371v2,cs.LG
Discovery and inversion of the viscoelastic wave equation in inhomogeneous media,"In scientific machine learning, the task of identifying partial differential
equations accurately from sparse and noisy data poses a significant challenge.
Current sparse regression methods may identify inaccurate equations on sparse
and noisy datasets and are not suitable for varying coefficients. To address
this issue, we propose a hybrid framework that combines two alternating
direction optimization phases: discovery and embedding. The discovery phase
employs current well-developed sparse regression techniques to preliminarily
identify governing equations from observations. The embedding phase implements
a recurrent convolutional neural network (RCNN), enabling efficient processes
for time-space iterations involved in discretized forms of wave equation. The
RCNN model further optimizes the imperfect sparse regression results to obtain
more accurate functional terms and coefficients. Through alternating update of
discovery-embedding phases, essential physical equations can be robustly
identified from noisy and low-resolution measurements. To assess the
performance of proposed framework, numerical experiments are conducted on
various scenarios involving wave equation in elastic/viscoelastic and
homogeneous/inhomogeneous media. The results demonstrate that the proposed
method exhibits excellent robustness and accuracy, even when faced with high
levels of noise and limited data availability in both spatial and temporal
domains.",2024-09-27,"Su Chen, Yi Ding, Hiroe Miyake, Xiaojun Li",http://arxiv.org/pdf/2409.18370v2,cs.LG
Defect Prediction with Content-based Features,"Traditional defect prediction approaches often use metrics that measure the
complexity of the design or implementing code of a software system, such as the
number of lines of code in a source file. In this paper, we explore a different
approach based on content of source code. Our key assumption is that source
code of a software system contains information about its technical aspects and
those aspects might have different levels of defect-proneness. Thus,
content-based features such as words, topics, data types, and package names
extracted from a source code file could be used to predict its defects. We have
performed an extensive empirical evaluation and found that: i) such
content-based features have higher predictive power than code complexity
metrics and ii) the use of feature selection, reduction, and combination
further improves the prediction performance.",2024-09-27,"Hung Viet Pham, Tung Thanh Nguyen",http://arxiv.org/pdf/2409.18365v1,cs.LG
Multi-hypotheses Conditioned Point Cloud Diffusion for 3D Human Reconstruction from Occluded Images,"3D human shape reconstruction under severe occlusion due to human-object or
human-human interaction is a challenging problem. Parametric models i.e.,
SMPL(-X), which are based on the statistics across human shapes, can represent
whole human body shapes but are limited to minimally-clothed human shapes.
Implicit-function-based methods extract features from the parametric models to
employ prior knowledge of human bodies and can capture geometric details such
as clothing and hair. However, they often struggle to handle misaligned
parametric models and inpaint occluded regions given a single RGB image. In
this work, we propose a novel pipeline, MHCDIFF, Multi-hypotheses Conditioned
Point Cloud Diffusion, composed of point cloud diffusion conditioned on
probabilistic distributions for pixel-aligned detailed 3D human reconstruction
under occlusion. Compared to previous implicit-function-based methods, the
point cloud diffusion model can capture the global consistent features to
generate the occluded regions, and the denoising process corrects the
misaligned SMPL meshes. The core of MHCDIFF is extracting local features from
multiple hypothesized SMPL(-X) meshes and aggregating the set of features to
condition the diffusion model. In the experiments on CAPE and MultiHuman
datasets, the proposed method outperforms various SOTA methods based on SMPL,
implicit functions, point cloud diffusion, and their combined, under synthetic
and real occlusions. Our code is publicly available at
https://donghwankim0101.github.io/projects/mhcdiff/ .",2024-09-27,"Donghwan Kim, Tae-Kyun Kim",http://arxiv.org/pdf/2409.18364v3,cs.LG
Generative AI for fast and accurate statistical computation of fluids,"We present a generative AI algorithm for addressing the pressing task of
fast, accurate, and robust statistical computation of three-dimensional
turbulent fluid flows. Our algorithm, termed as GenCFD, is based on an
end-to-end conditional score-based diffusion model. Through extensive numerical
experimentation with a set of challenging fluid flows, we demonstrate that
GenCFD provides an accurate approximation of relevant statistical quantities of
interest while also efficiently generating high-quality realistic samples of
turbulent fluid flows and ensuring excellent spectral resolution. In contrast,
ensembles of deterministic ML algorithms, trained to minimize mean square
errors, regress to the mean flow. We present rigorous theoretical results
uncovering the surprising mechanisms through which diffusion models accurately
generate fluid flows. These mechanisms are illustrated with solvable toy models
that exhibit the mathematically relevant features of turbulent fluid flows
while being amenable to explicit analytical formulae. Our codes are publicly
available at https://github.com/camlab-ethz/GenCFD.",2024-09-27,"Roberto Molinaro, Samuel Lanthaler, Bogdan Raonić, Tobias Rohner, Victor Armegioiu, Stephan Simonis, Dana Grund, Yannick Ramic, Zhong Yi Wan, Fei Sha, Siddhartha Mishra, Leonardo Zepeda-Núñez",http://arxiv.org/pdf/2409.18359v2,cs.LG
FedDCL: a federated data collaboration learning as a hybrid-type privacy-preserving framework based on federated learning and data collaboration,"Recently, federated learning has attracted much attention as a
privacy-preserving integrated analysis that enables integrated analysis of data
held by multiple institutions without sharing raw data. On the other hand,
federated learning requires iterative communication across institutions and has
a big challenge for implementation in situations where continuous communication
with the outside world is extremely difficult. In this study, we propose a
federated data collaboration learning (FedDCL), which solves such communication
issues by combining federated learning with recently proposed non-model
share-type federated learning named as data collaboration analysis. In the
proposed FedDCL framework, each user institution independently constructs
dimensionality-reduced intermediate representations and shares them with
neighboring institutions on intra-group DC servers. On each intra-group DC
server, intermediate representations are transformed to incorporable forms
called collaboration representations. Federated learning is then conducted
between intra-group DC servers. The proposed FedDCL framework does not require
iterative communication by user institutions and can be implemented in
situations where continuous communication with the outside world is extremely
difficult. The experimental results show that the performance of the proposed
FedDCL is comparable to that of existing federated learning.",2024-09-27,"Akira Imakura, Tetsuya Sakurai",http://arxiv.org/pdf/2409.18356v1,cs.LG
Building a Chinese Medical Dialogue System: Integrating Large-scale Corpora and Novel Models,"The global COVID-19 pandemic underscored major deficiencies in traditional
healthcare systems, hastening the advancement of online medical services,
especially in medical triage and consultation. However, existing studies face
two main challenges. First, the scarcity of large-scale, publicly available,
domain-specific medical datasets due to privacy concerns, with current datasets
being small and limited to a few diseases, limiting the effectiveness of triage
methods based on Pre-trained Language Models (PLMs). Second, existing methods
lack medical knowledge and struggle to accurately understand professional terms
and expressions in patient-doctor consultations. To overcome these obstacles,
we construct the Large-scale Chinese Medical Dialogue Corpora (LCMDC), thereby
addressing the data shortage in this field. Moreover, we further propose a
novel triage system that combines BERT-based supervised learning with prompt
learning, as well as a GPT-based medical consultation model. To enhance domain
knowledge acquisition, we pre-trained PLMs using our self-constructed
background corpus. Experimental results on the LCMDC demonstrate the efficacy
of our proposed systems.",2024-09-27,"Xinyuan Wang, Haozhou Li, Dingfang Zheng, Qinke Peng",http://arxiv.org/pdf/2410.03521v2,cs.LG
AQMLator -- An Auto Quantum Machine Learning E-Platform,"A successful Machine Learning (ML) model implementation requires three main
components: training dataset, suitable model architecture and training
procedure. Given dataset and task, finding an appropriate model might be
challenging. AutoML, a branch of ML, focuses on automatic architecture search
-- a meta method that aims at moving human from ML system design process. The
success of ML and the development of quantum computing (QC) in recent years led
to a birth of new fascinating field called Quantum Machine Learning (QML) that,
amongst others, incorporates quantum computers into ML models. In this paper we
present AQMLator, an Auto Quantum Machine Learning platform that aims to
automatically propose and train the quantum layers of an ML model with minimal
input from the user. This way, data scientists can bypass the entry barrier for
QC and use QML. AQMLator uses standard ML libraries, making it easy to
introduce into existing ML pipelines.",2024-09-26,"Tomasz Rybotycki, Piotr Gawron",http://arxiv.org/pdf/2409.18338v3,cs.LG
A Framework for Standardizing Similarity Measures in a Rapidly Evolving Field,"Similarity measures are fundamental tools for quantifying the alignment
between artificial and biological systems. However, the diversity of similarity
measures and their varied naming and implementation conventions makes it
challenging to compare across studies. To facilitate comparisons and make
explicit the implementation choices underlying a given code package, we have
created and are continuing to develop a Python repository that benchmarks and
standardizes similarity measures. The goal of creating a consistent naming
convention that uniquely and efficiently specifies a similarity measure is not
trivial as, for example, even commonly used methods like Centered Kernel
Alignment (CKA) have at least 12 different variations, and this number will
likely continue to grow as the field evolves. For this reason, we do not
advocate for a fixed, definitive naming convention. The landscape of similarity
measures and best practices will continue to change and so we see our current
repository, which incorporates approximately 100 different similarity measures
from 14 packages, as providing a useful tool at this snapshot in time. To
accommodate the evolution of the field we present a framework for developing,
validating, and refining naming conventions with the goal of uniquely and
efficiently specifying similarity measures, ultimately making it easier for the
community to make comparisons across studies.",2024-09-26,"Nathan Cloos, Guangyu Robert Yang, Christopher J. Cueva",http://arxiv.org/pdf/2409.18333v1,cs.LG
Conformal Prediction: A Theoretical Note and Benchmarking Transductive Node Classification in Graphs,"Conformal prediction has become increasingly popular for quantifying the
uncertainty associated with machine learning models. Recent work in graph
uncertainty quantification has built upon this approach for conformal graph
prediction. The nascent nature of these explorations has led to conflicting
choices for implementations, baselines, and method evaluation. In this work, we
analyze the design choices made in the literature and discuss the tradeoffs
associated with existing methods. Building on the existing implementations, we
introduce techniques to scale existing methods to large-scale graph datasets
without sacrificing performance. Our theoretical and empirical results justify
our recommendations for future scholarship in graph conformal prediction.",2024-09-26,"Pranav Maneriker, Aditya T. Vadlamani, Anutam Srinivasan, Yuntian He, Ali Payani, Srinivasan Parthasarathy",http://arxiv.org/pdf/2409.18332v2,cs.LG
DMC-VB: A Benchmark for Representation Learning for Control with Visual Distractors,"Learning from previously collected data via behavioral cloning or offline
reinforcement learning (RL) is a powerful recipe for scaling generalist agents
by avoiding the need for expensive online learning. Despite strong
generalization in some respects, agents are often remarkably brittle to minor
visual variations in control-irrelevant factors such as the background or
camera viewpoint. In this paper, we present theDeepMind Control Visual
Benchmark (DMC-VB), a dataset collected in the DeepMind Control Suite to
evaluate the robustness of offline RL agents for solving continuous control
tasks from visual input in the presence of visual distractors. In contrast to
prior works, our dataset (a) combines locomotion and navigation tasks of
varying difficulties, (b) includes static and dynamic visual variations, (c)
considers data generated by policies with different skill levels, (d)
systematically returns pairs of state and pixel observation, (e) is an order of
magnitude larger, and (f) includes tasks with hidden goals. Accompanying our
dataset, we propose three benchmarks to evaluate representation learning
methods for pretraining, and carry out experiments on several recently proposed
methods. First, we find that pretrained representations do not help policy
learning on DMC-VB, and we highlight a large representation gap between
policies learned on pixel observations and on states. Second, we demonstrate
when expert data is limited, policy learning can benefit from representations
pretrained on (a) suboptimal data, and (b) tasks with stochastic hidden goals.
Our dataset and benchmark code to train and evaluate agents are available at:
https://github.com/google-deepmind/dmc_vision_benchmark.",2024-09-26,"Joseph Ortiz, Antoine Dedieu, Wolfgang Lehrach, Swaroop Guntupalli, Carter Wendelken, Ahmad Humayun, Guangyao Zhou, Sivaramakrishnan Swaminathan, Miguel Lázaro-Gredilla, Kevin Murphy",http://arxiv.org/pdf/2409.18330v1,cs.LG
Local Prediction-Powered Inference,"To infer a function value on a specific point $x$, it is essential to assign
higher weights to the points closer to $x$, which is called local polynomial /
multivariable regression. In many practical cases, a limited sample size may
ruin this method, but such conditions can be improved by the Prediction-Powered
Inference (PPI) technique. This paper introduced a specific algorithm for local
multivariable regression using PPI, which can significantly reduce the variance
of estimations without enlarge the error. The confidence intervals, bias
correction, and coverage probabilities are analyzed and proved the correctness
and superiority of our algorithm. Numerical simulation and real-data
experiments are applied and show these conclusions. Another contribution
compared to PPI is the theoretical computation efficiency and explainability by
taking into account the dependency of the dependent variable.",2024-09-26,"Yanwu Gu, Dong Xia",http://arxiv.org/pdf/2409.18321v1,cs.LG
Towards the Mitigation of Confirmation Bias in Semi-supervised Learning: a Debiased Training Perspective,"Semi-supervised learning (SSL) commonly exhibits confirmation bias, where
models disproportionately favor certain classes, leading to errors in predicted
pseudo labels that accumulate under a self-training paradigm. Unlike supervised
settings, which benefit from a rich, static data distribution, SSL inherently
lacks mechanisms to correct this self-reinforced bias, necessitating debiased
interventions at each training step. Although the generation of debiased pseudo
labels has been extensively studied, their effective utilization remains
underexplored. Our analysis indicates that data from biased classes should have
a reduced influence on parameter updates, while more attention should be given
to underrepresented classes. To address these challenges, we introduce TaMatch,
a unified framework for debiased training in SSL. TaMatch employs a scaling
ratio derived from both a prior target distribution and the model's learning
status to estimate and correct bias at each training step. This ratio adjusts
the raw predictions on unlabeled data to produce debiased pseudo labels. In the
utilization phase, these labels are differently weighted according to their
predicted class, enhancing training equity and minimizing class bias.
Additionally, TaMatch dynamically adjust the target distribution in response to
the model's learning progress, facilitating robust handling of practical
scenarios where the prior distribution is unknown. Empirical evaluations show
that TaMatch significantly outperforms existing state-of-the-art methods across
a range of challenging image classification tasks, highlighting the critical
importance of both the debiased generation and utilization of pseudo labels in
SSL.",2024-09-26,"Yu Wang, Yuxuan Yin, Peng Li",http://arxiv.org/pdf/2409.18316v1,cs.LG
Realistic Evaluation of Model Merging for Compositional Generalization,"Merging has become a widespread way to cheaply combine individual models into
a single model that inherits their capabilities and attains better performance.
This popularity has spurred rapid development of many new merging methods,
which are typically validated in disparate experimental settings and frequently
differ in the assumptions made about model architecture, data availability, and
computational budget. In this work, we characterize the relative merits of
different merging methods by evaluating them in a shared experimental setting
and precisely identifying the practical requirements of each method.
Specifically, our setting focuses on using merging for compositional
generalization of capabilities in image classification, image generation, and
natural language processing. Additionally, we measure the computational costs
of different merging methods as well as how they perform when scaling the
number of models being merged. Taken together, our results clarify the state of
the field of model merging and provide a comprehensive and rigorous
experimental setup to test new methods.",2024-09-26,"Derek Tam, Yash Kant, Brian Lester, Igor Gilitschenski, Colin Raffel",http://arxiv.org/pdf/2409.18314v1,cs.LG
Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation,"There is no limit to how much a robot might explore and learn, but all of
that knowledge needs to be searchable and actionable. Within language research,
retrieval augmented generation (RAG) has become the workhorse of large-scale
non-parametric knowledge; however, existing techniques do not directly transfer
to the embodied domain, which is multimodal, where data is highly correlated,
and perception requires abstraction. To address these challenges, we introduce
Embodied-RAG, a framework that enhances the foundational model of an embodied
agent with a non-parametric memory system capable of autonomously constructing
hierarchical knowledge for both navigation and language generation.
Embodied-RAG handles a full range of spatial and semantic resolutions across
diverse environments and query types, whether for a specific object or a
holistic description of ambiance. At its core, Embodied-RAG's memory is
structured as a semantic forest, storing language descriptions at varying
levels of detail. This hierarchical organization allows the system to
efficiently generate context-sensitive outputs across different robotic
platforms. We demonstrate that Embodied-RAG effectively bridges RAG to the
robotics domain, successfully handling over 250 explanation and navigation
queries across kilometer-level environments, highlighting its promise as a
general-purpose non-parametric system for embodied agents.",2024-09-26,"Quanting Xie, So Yeon Min, Pengliang Ji, Yue Yang, Tianyi Zhang, Kedi Xu, Aarav Bajaj, Ruslan Salakhutdinov, Matthew Johnson-Roberson, Yonatan Bisk",http://arxiv.org/pdf/2409.18313v5,cs.LG
Deep-ER: Deep Learning ECCENTRIC Reconstruction for fast high-resolution neurometabolic imaging,"Introduction: Altered neurometabolism is an important pathological mechanism
in many neurological diseases and brain cancer, which can be mapped
non-invasively by Magnetic Resonance Spectroscopic Imaging (MRSI). Advanced
MRSI using non-cartesian compressed-sense acquisition enables fast
high-resolution metabolic imaging but has lengthy reconstruction times that
limits throughput and needs expert user interaction. Here, we present a robust
and efficient Deep Learning reconstruction to obtain high-quality metabolic
maps.
  Methods: Fast high-resolution whole-brain metabolic imaging was performed at
3.4 mm$^3$ isotropic resolution with acquisition times between 4:11-9:21 min:s
using ECCENTRIC pulse sequence on a 7T MRI scanner. Data were acquired in a
high-resolution phantom and 27 human participants, including 22 healthy
volunteers and 5 glioma patients. A deep neural network using recurring
interlaced convolutional layers with joint dual-space feature representation
was developed for deep learning ECCENTRIC reconstruction (Deep-ER). 21 subjects
were used for training and 6 subjects for testing. Deep-ER performance was
compared to conventional iterative Total Generalized Variation reconstruction
using image and spectral quality metrics.
  Results: Deep-ER demonstrated 600-fold faster reconstruction than
conventional methods, providing improved spatial-spectral quality and
metabolite quantification with 12%-45% (P<0.05) higher signal-to-noise and
8%-50% (P<0.05) smaller Cramer-Rao lower bounds. Metabolic images clearly
visualize glioma tumor heterogeneity and boundary.
  Conclusion: Deep-ER provides efficient and robust reconstruction for
sparse-sampled MRSI. The accelerated acquisition-reconstruction MRSI is
compatible with high-throughput imaging workflow. It is expected that such
improved performance will facilitate basic and clinical MRSI applications.",2024-09-26,"Paul Weiser, Georg Langs, Wolfgang Bogner, Stanislav Motyka, Bernhard Strasser, Polina Golland, Nalini Singh, Jorg Dietrich, Erik Uhlmann, Tracy Batchelor, Daniel Cahill, Malte Hoffmann, Antoine Klauser, Ovidiu C. Andronesi",http://arxiv.org/pdf/2409.18303v1,cs.LG
Wavelet-Driven Generalizable Framework for Deepfake Face Forgery Detection,"The evolution of digital image manipulation, particularly with the
advancement of deep generative models, significantly challenges existing
deepfake detection methods, especially when the origin of the deepfake is
obscure. To tackle the increasing complexity of these forgeries, we propose
\textbf{Wavelet-CLIP}, a deepfake detection framework that integrates wavelet
transforms with features derived from the ViT-L/14 architecture, pre-trained in
the CLIP fashion. Wavelet-CLIP utilizes Wavelet Transforms to deeply analyze
both spatial and frequency features from images, thus enhancing the model's
capability to detect sophisticated deepfakes. To verify the effectiveness of
our approach, we conducted extensive evaluations against existing
state-of-the-art methods for cross-dataset generalization and detection of
unseen images generated by standard diffusion models. Our method showcases
outstanding performance, achieving an average AUC of 0.749 for cross-data
generalization and 0.893 for robustness against unseen deepfakes, outperforming
all compared methods. The code can be reproduced from the repo:
\url{https://github.com/lalithbharadwajbaru/Wavelet-CLIP}",2024-09-26,"Lalith Bharadwaj Baru, Rohit Boddeda, Shilhora Akshay Patel, Sai Mohan Gajapaka",http://arxiv.org/pdf/2409.18301v3,cs.LG
SOAR: Self-supervision Optimized UAV Action Recognition with Efficient Object-Aware Pretraining,"We introduce SOAR, a novel Self-supervised pretraining algorithm for aerial
footage captured by Unmanned Aerial Vehicles (UAVs). We incorporate human
object knowledge throughout the pretraining process to enhance UAV video
pretraining efficiency and downstream action recognition performance. This is
in contrast to prior works that primarily incorporate object information during
the fine-tuning stage. Specifically, we first propose a novel object-aware
masking strategy designed to retain the visibility of certain patches related
to objects throughout the pretraining phase. Second, we introduce an
object-aware loss function that utilizes object information to adjust the
reconstruction loss, preventing bias towards less informative background
patches. In practice, SOAR with a vanilla ViT backbone, outperforms best UAV
action recognition models, recording a 9.7% and 21.4% boost in top-1 accuracy
on the NEC-Drone and UAV-Human datasets, while delivering an inference speed of
18.7ms per video, making it 2x to 5x faster. Additionally, SOAR obtains
comparable accuracy to prior self-supervised learning (SSL) methods while
requiring 87.5% less pretraining time and 25% less memory usage",2024-09-26,"Ruiqi Xian, Xiyang Wu, Tianrui Guan, Xijun Wang, Boqing Gong, Dinesh Manocha",http://arxiv.org/pdf/2409.18300v1,cs.LG
Causality-based Subject and Task Fingerprints using fMRI Time-series Data,"Recently, there has been a revived interest in system neuroscience causation
models due to their unique capability to unravel complex relationships in
multi-scale brain networks. In this paper, our goal is to verify the
feasibility and effectiveness of using a causality-based approach for fMRI
fingerprinting. Specifically, we propose an innovative method that utilizes the
causal dynamics activities of the brain to identify the unique cognitive
patterns of individuals (e.g., subject fingerprint) and fMRI tasks (e.g., task
fingerprint). The key novelty of our approach stems from the development of a
two-timescale linear state-space model to extract 'spatio-temporal' (aka
causal) signatures from an individual's fMRI time series data. To the best of
our knowledge, we pioneer and subsequently quantify, in this paper, the concept
of 'causal fingerprint.' Our method is well-separated from other fingerprint
studies as we quantify fingerprints from a cause-and-effect perspective, which
are then incorporated with a modal decomposition and projection method to
perform subject identification and a GNN-based (Graph Neural Network) model to
perform task identification. Finally, we show that the experimental results and
comparisons with non-causality-based methods demonstrate the effectiveness of
the proposed methods. We visualize the obtained causal signatures and discuss
their biological relevance in light of the existing understanding of brain
functionalities. Collectively, our work paves the way for further studies on
causal fingerprints with potential applications in both healthy controls and
neurodegenerative diseases.",2024-09-26,"Dachuan Song, Li Shen, Duy Duong-Tran, Xuan Wang",http://arxiv.org/pdf/2409.18298v1,cs.LG
Enhancing Lossy Compression Through Cross-Field Information for Scientific Applications,"Lossy compression is one of the most effective methods for reducing the size
of scientific data containing multiple data fields. It reduces information
density through prediction or transformation techniques to compress the data.
Previous approaches use local information from a single target field when
predicting target data points, limiting their potential to achieve higher
compression ratios. In this paper, we identified significant cross-field
correlations within scientific datasets. We propose a novel hybrid prediction
model that utilizes CNN to extract cross-field information and combine it with
existing local field information. Our solution enhances the prediction accuracy
of lossy compressors, leading to improved compression ratios without
compromising data quality. We evaluate our solution on three scientific
datasets, demonstrating its ability to improve compression ratios by up to 25%
under specific error bounds. Additionally, our solution preserves more data
details and reduces artifacts compared to baseline approaches.",2024-09-26,"Youyuan Liu, Wenqi Jia, Taolue Yang, Miao Yin, Sian Jin",http://arxiv.org/pdf/2409.18295v1,cs.LG
Criticality and Safety Margins for Reinforcement Learning,"State of the art reinforcement learning methods sometimes encounter unsafe
situations. Identifying when these situations occur is of interest both for
post-hoc analysis and during deployment, where it might be advantageous to call
out to a human overseer for help. Efforts to gauge the criticality of different
points in time have been developed, but their accuracy is not well established
due to a lack of ground truth, and they are not designed to be easily
interpretable by end users. Therefore, we seek to define a criticality
framework with both a quantifiable ground truth and a clear significance to
users. We introduce true criticality as the expected drop in reward when an
agent deviates from its policy for n consecutive random actions. We also
introduce the concept of proxy criticality, a low-overhead metric that has a
statistically monotonic relationship to true criticality. Safety margins make
these interpretable, when defined as the number of random actions for which
performance loss will not exceed some tolerance with high confidence. We
demonstrate this approach in several environment-agent combinations; for an A3C
agent in an Atari Beamrider environment, the lowest 5% of safety margins
contain 47% of agent losses; i.e., supervising only 5% of decisions could
potentially prevent roughly half of an agent's errors. This criticality
framework measures the potential impacts of bad decisions, even before those
decisions are made, allowing for more effective debugging and oversight of
autonomous agents.",2024-09-26,"Alexander Grushin, Walt Woods, Alvaro Velasquez, Simon Khan",http://arxiv.org/pdf/2409.18289v1,cs.LG
SLIDE: A machine-learning based method for forced dynamic response estimation of multibody systems,"In computational engineering, enhancing the simulation speed and efficiency
is a perpetual goal. To fully take advantage of neural network techniques and
hardware, we present the SLiding-window Initially-truncated Dynamic-response
Estimator (SLIDE), a deep learning-based method designed to estimate output
sequences of mechanical or multibody systems with primarily, but not
exclusively, forced excitation. A key advantage of SLIDE is its ability to
estimate the dynamic response of damped systems without requiring the full
system state, making it particularly effective for flexible multibody systems.
The method truncates the output window based on the decay of initial effects,
such as damping, which is approximated by the complex eigenvalues of the
systems linearized equations. In addition, a second neural network is trained
to provide an error estimation, further enhancing the methods applicability.
The method is applied to a diverse selection of systems, including the Duffing
oscillator, a flexible slider-crank system, and an industrial 6R manipulator,
mounted on a flexible socket. Our results demonstrate significant speedups from
the simulation up to several millions, exceeding real-time performance
substantially.",2024-09-26,"Peter Manzl, Alexander Humer, Qasim Khadim, Johannes Gerstmayr",http://arxiv.org/pdf/2409.18272v1,cs.LG
Using dynamic loss weighting to boost improvements in forecast stability,"Rolling origin forecast instability refers to variability in forecasts for a
specific period induced by updating the forecast when new data points become
available. Recently, an extension to the N-BEATS model for univariate time
series point forecasting was proposed to include forecast stability as an
additional optimization objective, next to accuracy. It was shown that more
stable forecasts can be obtained without harming accuracy by minimizing a
composite loss function that contains both a forecast error and a forecast
instability component, with a static hyperparameter to control the impact of
stability. In this paper, we empirically investigate whether further
improvements in stability can be obtained without compromising accuracy by
applying dynamic loss weighting algorithms, which change the loss weights
during training. We show that existing dynamic loss weighting methods can
achieve this objective and provide insights into why this might be the case.
Additionally, we propose an extension to the Random Weighting approach --
Task-Aware Random Weighting -- which also achieves this objective.",2024-09-26,"Daan Caljon, Jeff Vercauteren, Simon De Vos, Wouter Verbeke, Jente Van Belle",http://arxiv.org/pdf/2409.18267v2,cs.LG
Predicting Muscle Thickness Deformation from Muscle Activation Patterns: A Dual-Attention Framework,"Understanding the relationship between muscle activation and thickness
deformation is critical for diagnosing muscle-related diseases and monitoring
muscle health. Although ultrasound technique can measure muscle thickness
change during muscle movement, its application in portable devices is limited
by wiring and data collection challenges. Surface electromyography (sEMG), on
the other hand, records muscle bioelectrical signals as the muscle activation.
This paper introduced a deep-learning approach to leverage sEMG signals for
muscle thickness deformation prediction, eliminating the need for ultrasound
measurement. Using a dual-attention framework combining self-attention and
cross-attention mechanisms, this method predicted muscle deformation directly
from sEMG data. Experimental results with six healthy subjects showed that the
approach could accurately predict muscle excursion with an average precision of
0.923$\pm$0.900mm, which shows that this method can facilitate real-time
portable muscle health monitoring, showing potential for applications in
clinical diagnostics, sports science, and rehabilitation.",2024-09-26,"Bangyu Lan, Kenan Niu",http://arxiv.org/pdf/2409.18266v1,cs.LG
Task-recency bias strikes back: Adapting covariances in Exemplar-Free Class Incremental Learning,"Exemplar-Free Class Incremental Learning (EFCIL) tackles the problem of
training a model on a sequence of tasks without access to past data. Existing
state-of-the-art methods represent classes as Gaussian distributions in the
feature extractor's latent space, enabling Bayes classification or training the
classifier by replaying pseudo features. However, we identify two critical
issues that compromise their efficacy when the feature extractor is updated on
incremental tasks. First, they do not consider that classes' covariance
matrices change and must be adapted after each task. Second, they are
susceptible to a task-recency bias caused by dimensionality collapse occurring
during training. In this work, we propose AdaGauss -- a novel method that
adapts covariance matrices from task to task and mitigates the task-recency
bias owing to the additional anti-collapse loss function. AdaGauss yields
state-of-the-art results on popular EFCIL benchmarks and datasets when training
from scratch or starting from a pre-trained backbone. The code is available at:
https://github.com/grypesc/AdaGauss.",2024-09-26,"Grzegorz Rypeść, Sebastian Cygert, Tomasz Trzciński, Bartłomiej Twardowski",http://arxiv.org/pdf/2409.18265v2,cs.LG
DisGeM: Distractor Generation for Multiple Choice Questions with Span Masking,"Recent advancements in Natural Language Processing (NLP) have impacted
numerous sub-fields such as natural language generation, natural language
inference, question answering, and more. However, in the field of question
generation, the creation of distractors for multiple-choice questions (MCQ)
remains a challenging task. In this work, we present a simple, generic
framework for distractor generation using readily available Pre-trained
Language Models (PLMs). Unlike previous methods, our framework relies solely on
pre-trained language models and does not require additional training on
specific datasets. Building upon previous research, we introduce a two-stage
framework consisting of candidate generation and candidate selection. Our
proposed distractor generation framework outperforms previous methods without
the need for training or fine-tuning. Human evaluations confirm that our
approach produces more effective and engaging distractors. The related codebase
is publicly available at https://github.com/obss/disgem.",2024-09-26,"Devrim Cavusoglu, Secil Sen, Ulas Sert",http://arxiv.org/pdf/2409.18263v1,cs.LG
Development of an Edge Resilient ML Ensemble to Tolerate ICS Adversarial Attacks,"Deploying machine learning (ML) in dynamic data-driven applications systems
(DDDAS) can improve the security of industrial control systems (ICS). However,
ML-based DDDAS are vulnerable to adversarial attacks because adversaries can
alter the input data slightly so that the ML models predict a different result.
In this paper, our goal is to build a resilient edge machine learning (reML)
architecture that is designed to withstand adversarial attacks by performing
Data Air Gap Transformation (DAGT) to anonymize data feature spaces using deep
neural networks and randomize the ML models used for predictions. The reML is
based on the Resilient DDDAS paradigm, Moving Target Defense (MTD) theory, and
TinyML and is applied to combat adversarial attacks on ICS. Furthermore, the
proposed approach is power-efficient and privacy-preserving and, therefore, can
be deployed on power-constrained devices to enhance ICS security. This approach
enables resilient ML inference at the edge by shifting the computation from the
computing-intensive platforms to the resource-constrained edge devices. The
incorporation of TinyML with TensorFlow Lite ensures efficient resource
utilization and, consequently, makes reML suitable for deployment in various
industrial control environments. Furthermore, the dynamic nature of reML,
facilitated by the resilient DDDAS development environment, allows for
continuous adaptation and improvement in response to emerging threats. Lastly,
we evaluate our approach on an ICS dataset and demonstrate that reML provides a
viable and effective solution for resilient ML inference at the edge devices.",2024-09-26,"Likai Yao, Qinxuan Shi, Zhanglong Yang, Sicong Shao, Salim Hariri",http://arxiv.org/pdf/2409.18244v1,cs.LG
Towards Sub-millisecond Latency Real-Time Speech Enhancement Models on Hearables,"Low latency models are critical for real-time speech enhancement
applications, such as hearing aids and hearables. However, the sub-millisecond
latency space for resource-constrained hearables remains underexplored. We
demonstrate speech enhancement using a computationally efficient minimum-phase
FIR filter, enabling sample-by-sample processing to achieve mean algorithmic
latency of 0.32 ms to 1.25 ms. With a single microphone, we observe a mean
SI-SDRi of 4.1 dB. The approach shows generalization with a DNSMOS increase of
0.2 on unseen audio recordings. We use a lightweight LSTM-based model of 626k
parameters to generate FIR taps. Using a real hardware implementation on a
low-power DSP, our system can run with 376 MIPS and a mean end-to-end latency
of 3.35 ms. In addition, we provide a comparison with existing low-latency
spectral masking techniques. We hope this work will enable a better
understanding of latency and can be used to improve the comfort and usability
of hearables.",2024-09-26,"Artem Dementyev, Chandan K. A. Reddy, Scott Wisdom, Navin Chatlani, John R. Hershey, Richard F. Lyon",http://arxiv.org/pdf/2409.18239v2,cs.LG
Spatial Visibility and Temporal Dynamics: Revolutionizing Field of View Prediction in Adaptive Point Cloud Video Streaming,"Field-of-View (FoV) adaptive streaming significantly reduces bandwidth
requirement of immersive point cloud video (PCV) by only transmitting visible
points in a viewer's FoV. The traditional approaches often focus on
trajectory-based 6 degree-of-freedom (6DoF) FoV predictions. The predicted FoV
is then used to calculate point visibility. Such approaches do not explicitly
consider video content's impact on viewer attention, and the conversion from
FoV to point visibility is often error-prone and time-consuming. We reformulate
the PCV FoV prediction problem from the cell visibility perspective, allowing
for precise decision-making regarding the transmission of 3D data at the cell
level based on the predicted visibility distribution. We develop a novel
spatial visibility and object-aware graph model that leverages the historical
3D visibility data and incorporates spatial perception, neighboring cell
correlation, and occlusion information to predict the cell visibility in the
future. Our model significantly improves the long-term cell visibility
prediction, reducing the prediction MSE loss by up to 50% compared to the
state-of-the-art models while maintaining real-time performance (more than
30fps) for point cloud videos with over 1 million points.",2024-09-26,"Chen Li, Tongyu Zong, Yueyu Hu, Yao Wang, Yong Liu",http://arxiv.org/pdf/2409.18236v2,cs.LG
Visual Concept Networks: A Graph-Based Approach to Detecting Anomalous Data in Deep Neural Networks,"Deep neural networks (DNNs), while increasingly deployed in many
applications, struggle with robustness against anomalous and
out-of-distribution (OOD) data. Current OOD benchmarks often oversimplify,
focusing on single-object tasks and not fully representing complex real-world
anomalies. This paper introduces a new, straightforward method employing graph
structures and topological features to effectively detect both far-OOD and
near-OOD data. We convert images into networks of interconnected human
understandable features or visual concepts. Through extensive testing on two
novel tasks, including ablation studies with large vocabularies and diverse
tasks, we demonstrate the method's effectiveness. This approach enhances DNN
resilience to OOD data and promises improved performance in various
applications.",2024-09-26,"Debargha Ganguly, Debayan Gupta, Vipin Chaudhary",http://arxiv.org/pdf/2409.18235v1,cs.LG
Packet Inspection Transformer: A Self-Supervised Journey to Unseen Malware Detection with Few Samples,"As networks continue to expand and become more interconnected, the need for
novel malware detection methods becomes more pronounced. Traditional security
measures are increasingly inadequate against the sophistication of modern cyber
attacks. Deep Packet Inspection (DPI) has been pivotal in enhancing network
security, offering an in-depth analysis of network traffic that surpasses
conventional monitoring techniques. DPI not only examines the metadata of
network packets, but also dives into the actual content being carried within
the packet payloads, providing a comprehensive view of the data flowing through
networks. While the integration of advanced deep learning techniques with DPI
has introduced modern methodologies into malware detection and network traffic
classification, state-of-the-art supervised learning approaches are limited by
their reliance on large amounts of annotated data and their inability to
generalize to novel, unseen malware threats. To address these limitations, this
paper leverages the recent advancements in self-supervised learning (SSL) and
few-shot learning (FSL). Our proposed self-supervised approach trains a
transformer via SSL to learn the embedding of packet content, including
payload, from vast amounts of unlabeled data by masking portions of packets,
leading to a learned representation that generalizes to various downstream
tasks. Once the representation is extracted from the packets, they are used to
train a malware detection algorithm. The representation obtained from the
transformer is then used to adapt the malware detector to novel types of
attacks using few-shot learning approaches. Our experimental results
demonstrate that our method achieves classification accuracies of up to 94.76%
on the UNSW-NB15 dataset and 83.25% on the CIC-IoT23 dataset.",2024-09-26,"Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh",http://arxiv.org/pdf/2409.18219v2,cs.LG
Learning to Drive via Asymmetric Self-Play,"Large-scale data is crucial for learning realistic and capable driving
policies. However, it can be impractical to rely on scaling datasets with real
data alone. The majority of driving data is uninteresting, and deliberately
collecting new long-tail scenarios is expensive and unsafe. We propose
asymmetric self-play to scale beyond real data with additional challenging,
solvable, and realistic synthetic scenarios. Our approach pairs a teacher that
learns to generate scenarios it can solve but the student cannot, with a
student that learns to solve them. When applied to traffic simulation, we learn
realistic policies with significantly fewer collisions in both nominal and
long-tail scenarios. Our policies further zero-shot transfer to generate
training data for end-to-end autonomy, significantly outperforming
state-of-the-art adversarial approaches, or using real data alone. For more
information, visit https://waabi.ai/selfplay .",2024-09-26,"Chris Zhang, Sourav Biswas, Kelvin Wong, Kion Fallah, Lunjun Zhang, Dian Chen, Sergio Casas, Raquel Urtasun",http://arxiv.org/pdf/2409.18218v1,cs.LG
MMMT-IF: A Challenging Multimodal Multi-Turn Instruction Following Benchmark,"Evaluating instruction following capabilities for multimodal, multi-turn
dialogue is challenging. With potentially multiple instructions in the input
model context, the task is time-consuming for human raters and we show LLM
based judges are biased towards answers from the same model. We propose
MMMT-IF, an image based multi-turn Q$\&$A evaluation set with added global
instructions between questions, constraining the answer format. This challenges
models to retrieve instructions dispersed across long dialogues and reason
under instruction constraints. All instructions are objectively verifiable
through code execution. We introduce the Programmatic Instruction Following
($\operatorname{PIF}$) metric to measure the fraction of the instructions that
are correctly followed while performing a reasoning task. The
$\operatorname{PIF-N-K}$ set of metrics further evaluates robustness by
measuring the fraction of samples in a corpus where, for each sample, at least
K out of N generated model responses achieve a $\operatorname{PIF}$ score of
one. The $\operatorname{PIF}$ metric aligns with human instruction following
ratings, showing 60 percent correlation. Experiments show Gemini 1.5 Pro,
GPT-4o, and Claude 3.5 Sonnet, have a $\operatorname{PIF}$ metric that drops
from 0.81 on average at turn 1 across the models, to 0.64 at turn 20. Across
all turns, when each response is repeated 4 times ($\operatorname{PIF-4-4}$),
GPT-4o and Gemini successfully follow all instructions only $11\%$ of the time.
When all the instructions are also appended to the end of the model input
context, the $\operatorname{PIF}$ metric improves by 22.3 points on average,
showing that the challenge with the task lies not only in following the
instructions, but also in retrieving the instructions spread out in the model
context. We plan to open source the MMMT-IF dataset and metric computation
code.",2024-09-26,"Elliot L. Epstein, Kaisheng Yao, Jing Li, Xinyi Bai, Hamid Palangi",http://arxiv.org/pdf/2409.18216v1,cs.LG
Trustworthy Text-to-Image Diffusion Models: A Timely and Focused Survey,"Text-to-Image (T2I) Diffusion Models (DMs) have garnered widespread attention
for their impressive advancements in image generation. However, their growing
popularity has raised ethical and social concerns related to key non-functional
properties of trustworthiness, such as robustness, fairness, security, privacy,
factuality, and explainability, similar to those in traditional deep learning
(DL) tasks. Conventional approaches for studying trustworthiness in DL tasks
often fall short due to the unique characteristics of T2I DMs, e.g., the
multi-modal nature. Given the challenge, recent efforts have been made to
develop new methods for investigating trustworthiness in T2I DMs via various
means, including falsification, enhancement, verification \& validation and
assessment. However, there is a notable lack of in-depth analysis concerning
those non-functional properties and means. In this survey, we provide a timely
and focused review of the literature on trustworthy T2I DMs, covering a
concise-structured taxonomy from the perspectives of property, means,
benchmarks and applications. Our review begins with an introduction to
essential preliminaries of T2I DMs, and then we summarise key
definitions/metrics specific to T2I tasks and analyses the means proposed in
recent literature based on these definitions/metrics. Additionally, we review
benchmarks and domain applications of T2I DMs. Finally, we highlight the gaps
in current research, discuss the limitations of existing methods, and propose
future research directions to advance the development of trustworthy T2I DMs.
Furthermore, we keep up-to-date updates in this field to track the latest
developments and maintain our GitHub repository at:
https://github.com/wellzline/Trustworthy_T2I_DMs",2024-09-26,"Yi Zhang, Zhen Chen, Chih-Hong Cheng, Wenjie Ruan, Xiaowei Huang, Dezong Zhao, David Flynn, Siddartha Khastgir, Xingyu Zhao",http://arxiv.org/pdf/2409.18214v1,cs.LG
A Unified View on Learning Unnormalized Distributions via Noise-Contrastive Estimation,"This paper studies a family of estimators based on noise-contrastive
estimation (NCE) for learning unnormalized distributions. The main contribution
of this work is to provide a unified perspective on various methods for
learning unnormalized distributions, which have been independently proposed and
studied in separate research communities, through the lens of NCE. This unified
view offers new insights into existing estimators. Specifically, for
exponential families, we establish the finite-sample convergence rates of the
proposed estimators under a set of regularity assumptions, most of which are
new.",2024-09-26,"J. Jon Ryu, Abhin Shah, Gregory W. Wornell",http://arxiv.org/pdf/2409.18209v1,cs.LG
Bridging OOD Detection and Generalization: A Graph-Theoretic View,"In the context of modern machine learning, models deployed in real-world
scenarios often encounter diverse data shifts like covariate and semantic
shifts, leading to challenges in both out-of-distribution (OOD) generalization
and detection. Despite considerable attention to these issues separately, a
unified framework for theoretical understanding and practical usage is lacking.
To bridge the gap, we introduce a graph-theoretic framework to jointly tackle
both OOD generalization and detection problems. By leveraging the graph
formulation, data representations are obtained through the factorization of the
graph's adjacency matrix, enabling us to derive provable error quantifying OOD
generalization and detection performance. Empirical results showcase
competitive performance in comparison to existing methods, thereby validating
our theoretical underpinnings. Code is publicly available at
https://github.com/deeplearning-wisc/graph-spectral-ood.",2024-09-26,"Han Wang, Yixuan Li",http://arxiv.org/pdf/2409.18205v1,cs.LG
AI Policy Projector: Grounding LLM Policy Design in Iterative Mapmaking,"Whether a large language model policy is an explicit constitution or an
implicit reward model, it is challenging to assess coverage over the unbounded
set of real-world situations that a policy must contend with. We introduce an
AI policy design process inspired by mapmaking, which has developed tactics for
visualizing and iterating on maps even when full coverage is not possible. With
Policy Projector, policy designers can survey the landscape of model
input-output pairs, define custom regions (e.g., ""violence""), and navigate
these regions with rules that can be applied to LLM outputs (e.g., if output
contains ""violence"" and ""graphic details,"" then rewrite without ""graphic
details""). Policy Projector supports interactive policy authoring using LLM
classification and steering and a map visualization reflecting the policy
designer's work. In an evaluation with 12 AI safety experts, our system helps
policy designers to address problematic model behaviors extending beyond an
existing, comprehensive harm taxonomy.",2024-09-26,"Michelle S. Lam, Fred Hohman, Dominik Moritz, Jeffrey P. Bigham, Kenneth Holstein, Mary Beth Kery",http://arxiv.org/pdf/2409.18203v1,cs.LG
Loop-Diffusion: an equivariant diffusion model for designing and scoring protein loops,"Predicting protein functional characteristics from structure remains a
central problem in protein science, with broad implications from understanding
the mechanisms of disease to designing novel therapeutics. Unfortunately,
current machine learning methods are limited by scarce and biased experimental
data, and physics-based methods are either too slow to be useful, or too
simplified to be accurate. In this work, we present Loop-Diffusion, an energy
based diffusion model which leverages a dataset of general protein loops from
the entire protein universe to learn an energy function that generalizes to
functional prediction tasks. We evaluate Loop-Diffusion's performance on
scoring TCR-pMHC interfaces and demonstrate state-of-the-art results in
recognizing binding-enhancing mutations.",2024-09-26,"Kevin Borisiak, Gian Marco Visani, Armita Nourmohammad",http://arxiv.org/pdf/2409.18201v1,cs.LG
Autonomous Network Defence using Reinforcement Learning,"In the network security arms race, the defender is significantly
disadvantaged as they need to successfully detect and counter every malicious
attack. In contrast, the attacker needs to succeed only once. To level the
playing field, we investigate the effectiveness of autonomous agents in a
realistic network defence scenario. We first outline the problem, provide the
background on reinforcement learning and detail our proposed agent design.
Using a network environment simulation, with 13 hosts spanning 3 subnets, we
train a novel reinforcement learning agent and show that it can reliably defend
continual attacks by two advanced persistent threat (APT) red agents: one with
complete knowledge of the network layout and another which must discover
resources through exploration but is more general.",2024-09-26,"Myles Foley, Chris Hicks, Kate Highnam, Vasilios Mavroudis",http://arxiv.org/pdf/2409.18197v1,cs.LG
High-Fidelity 3D Lung CT Synthesis in ARDS Swine Models Using Score-Based 3D Residual Diffusion Models,"Acute respiratory distress syndrome (ARDS) is a severe condition
characterized by lung inflammation and respiratory failure, with a high
mortality rate of approximately 40%. Traditional imaging methods, such as chest
X-rays, provide only two-dimensional views, limiting their effectiveness in
fully assessing lung pathology. Three-dimensional (3D) computed tomography (CT)
offers a more comprehensive visualization, enabling detailed analysis of lung
aeration, atelectasis, and the effects of therapeutic interventions. However,
the routine use of CT in ARDS management is constrained by practical challenges
and risks associated with transporting critically ill patients to remote
scanners. In this study, we synthesize high-fidelity 3D lung CT from 2D
generated X-ray images with associated physiological parameters using a
score-based 3D residual diffusion model. Our preliminary results demonstrate
that this approach can produce high-quality 3D CT images that are validated
with ground truth, offering a promising solution for enhancing ARDS management.",2024-09-26,"Siyeop Yoon, Yujin Oh, Xiang Li, Yi Xin, Maurizio Cereda, Quanzheng Li",http://arxiv.org/pdf/2410.10826v1,cs.LG
Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography,"Contrastive Language-Image Pre-training (CLIP) demonstrates strong potential
in medical image analysis but requires substantial data and computational
resources. Due to these restrictions, existing CLIP applications in medical
imaging focus mainly on modalities like chest X-rays that have abundant
image-report data available, leaving many other important modalities
underexplored. Here, we propose one of the first adaptations of the full CLIP
model to mammography, which presents significant challenges due to labeled data
scarcity, high-resolution images with small regions of interest, and class-wise
imbalance. We first develop a specialized supervision framework for mammography
that leverages its multi-view nature. Furthermore, we design a symmetric local
alignment module to better focus on detailed features in high-resolution
images. Lastly, we incorporate a parameter-efficient fine-tuning approach for
large language models pre-trained with medical knowledge to address data
limitations. Our multi-view and multi-scale alignment (MaMA) method outperforms
state-of-the-art baselines for three different tasks on two large real-world
mammography datasets, EMBED and RSNA-Mammo, with only 52% model size compared
with the largest baseline. The code is available at
https://github.com/XYPB/MaMA",2024-09-26,"Yuexi Du, John Onofrey, Nicha C. Dvornek",http://arxiv.org/pdf/2409.18119v2,cs.LG
Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey,"Recent research demonstrates that the nascent fine-tuning-as-a-service
business model exposes serious safety concerns -- fine-tuning over a few
harmful data uploaded by the users can compromise the safety alignment of the
model. The attack, known as harmful fine-tuning attack, has raised a broad
research interest among the community. However, as the attack is still new,
\textbf{we observe that there are general misunderstandings within the research
community.} To clear up concern, this paper provide a comprehensive overview to
three aspects of harmful fine-tuning: attacks setting, defense design and
evaluation methodology. Specifically, we first present the threat model of the
problem, and introduce the harmful fine-tuning attack and its variants. Then we
systematically survey the existing literature on attacks/defenses/mechanical
analysis of the problem. Finally, we introduce the evaluation methodology and
outline future research directions that might contribute to the development of
the field. Additionally, we present a list of questions of interest, which
might be useful to refer to when reviewers in the peer review process question
the realism of the experiment/attack/defense setting. A curated list of
relevant papers is maintained and made accessible at:
https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers.",2024-09-26,"Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu",http://arxiv.org/pdf/2409.18169v5,cs.LG
Jump Diffusion-Informed Neural Networks with Transfer Learning for Accurate American Option Pricing under Data Scarcity,"Option pricing models, essential in financial mathematics and risk
management, have been extensively studied and recently advanced by AI
methodologies. However, American option pricing remains challenging due to the
complexity of determining optimal exercise times and modeling non-linear
payoffs resulting from stochastic paths. Moreover, the prevalent use of the
Black-Scholes formula in hybrid models fails to accurately capture the
discontinuity in the price process, limiting model performance, especially
under scarce data conditions. To address these issues, this study presents a
comprehensive framework for American option pricing consisting of six
interrelated modules, which combine nonlinear optimization algorithms,
analytical and numerical models, and neural networks to improve pricing
performance. Additionally, to handle the scarce data challenge, this framework
integrates the transfer learning through numerical data augmentation and a
physically constrained, jump diffusion process-informed neural network to
capture the leptokurtosis of the log return distribution. To increase training
efficiency, a warm-up period using Bayesian optimization is designed to provide
optimal data loss and physical loss coefficients. Experimental results of six
case studies demonstrate the accuracy, convergence, physical effectiveness, and
generalization of the framework. Moreover, the proposed model shows superior
performance in pricing deep out-of-the-money options.",2024-09-26,"Qiguo Sun, Hanyue Huang, XiBei Yang, Yuwei Zhang",http://arxiv.org/pdf/2409.18168v1,cs.LG
Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats,"Much of Earth's charismatic megafauna is endangered by human activities,
particularly the rhino, which is at risk of extinction due to the poaching
crisis in Africa. Monitoring rhinos' movement is crucial to their protection
but has unfortunately proven difficult because rhinos are elusive. Therefore,
instead of tracking rhinos, we propose the novel approach of mapping communal
defecation sites, called middens, which give information about rhinos' spatial
behavior valuable to anti-poaching, management, and reintroduction efforts.
This paper provides the first-ever mapping of rhino midden locations by
building classifiers to detect them using remotely sensed thermal, RGB, and
LiDAR imagery in passive and active learning settings. As existing active
learning methods perform poorly due to the extreme class imbalance in our
dataset, we design MultimodAL, an active learning system employing a ranking
technique and multimodality to achieve competitive performance with passive
learning models with 94% fewer labels. Our methods could therefore save over 76
hours in labeling time when used on a similarly-sized dataset. Unexpectedly,
our midden map reveals that rhino middens are not randomly distributed
throughout the landscape; rather, they are clustered. Consequently, rangers
should be targeted at areas with high midden densities to strengthen
anti-poaching efforts, in line with UN Target 15.7.",2024-09-26,"Lucia Gordon, Nikhil Behari, Samuel Collier, Elizabeth Bondi-Kelly, Jackson A. Killian, Catherine Ressijac, Peter Boucher, Andrew Davies, Milind Tambe",http://arxiv.org/pdf/2409.18104v1,cs.LG
MALPOLON: A Framework for Deep Species Distribution Modeling,"This paper describes a deep-SDM framework, MALPOLON. Written in Python and
built upon the PyTorch library, this framework aims to facilitate training and
inferences of deep species distribution models (deep-SDM) and sharing for users
with only general Python language skills (e.g., modeling ecologists) who are
interested in testing deep learning approaches to build new SDMs. More advanced
users can also benefit from the framework's modularity to run more specific
experiments by overriding existing classes while taking advantage of
press-button examples to train neural networks on multiple classification tasks
using custom or provided raw and pre-processed datasets. The framework is
open-sourced on GitHub and PyPi along with extensive documentation and examples
of use in various scenarios. MALPOLON offers straightforward installation,
YAML-based configuration, parallel computing, multi-GPU utilization, baseline
and foundational models for benchmarking, and extensive
tutorials/documentation, aiming to enhance accessibility and performance
scalability for ecologists and researchers.",2024-09-26,"Theo Larcher, Lukas Picek, Benjamin Deneu, Titouan Lorieul, Maximilien Servajean, Alexis Joly",http://arxiv.org/pdf/2409.18102v1,cs.LG
Self-supervised Pretraining for Cardiovascular Magnetic Resonance Cine Segmentation,"Self-supervised pretraining (SSP) has shown promising results in learning
from large unlabeled datasets and, thus, could be useful for automated
cardiovascular magnetic resonance (CMR) short-axis cine segmentation. However,
inconsistent reports of the benefits of SSP for segmentation have made it
difficult to apply SSP to CMR. Therefore, this study aimed to evaluate SSP
methods for CMR cine segmentation.
  To this end, short-axis cine stacks of 296 subjects (90618 2D slices) were
used for unlabeled pretraining with four SSP methods; SimCLR, positional
contrastive learning, DINO, and masked image modeling (MIM). Subsets of varying
numbers of subjects were used for supervised fine-tuning of 2D models for each
SSP method, as well as to train a 2D baseline model from scratch. The
fine-tuned models were compared to the baseline using the 3D Dice similarity
coefficient (DSC) in a test dataset of 140 subjects.
  The SSP methods showed no performance gains with the largest supervised
fine-tuning subset compared to the baseline (DSC = 0.89). When only 10 subjects
(231 2D slices) are available for supervised training, SSP using MIM (DSC =
0.86) improves over training from scratch (DSC = 0.82).
  This study found that SSP is valuable for CMR cine segmentation when labeled
training data is scarce, but does not aid state-of-the-art deep learning
methods when ample labeled data is available. Moreover, the choice of SSP
method is important. The code is publicly available at:
https://github.com/q-cardIA/ssp-cmr-cine-segmentation",2024-09-26,"Rob A. J. de Mooij, Josien P. W. Pluim, Cian M. Scannell",http://arxiv.org/pdf/2409.18100v1,cs.LG
Data-Prep-Kit: getting your data ready for LLM application development,"Data preparation is the first and a very important step towards any Large
Language Model (LLM) development. This paper introduces an easy-to-use,
extensible, and scale-flexible open-source data preparation toolkit called Data
Prep Kit (DPK). DPK is architected and designed to enable users to scale their
data preparation to their needs. With DPK they can prepare data on a local
machine or effortlessly scale to run on a cluster with thousands of CPU Cores.
DPK comes with a highly scalable, yet extensible set of modules that transform
natural language and code data. If the user needs additional transforms, they
can be easily developed using extensive DPK support for transform creation.
These modules can be used independently or pipelined to perform a series of
operations. In this paper, we describe DPK architecture and show its
performance from a small scale to a very large number of CPUs. The modules from
DPK have been used for the preparation of Granite Models [1] [2]. We believe
DPK is a valuable contribution to the AI community to easily prepare data to
enhance the performance of their LLM models or to fine-tune models with
Retrieval-Augmented Generation (RAG).",2024-09-26,"David Wood, Boris Lublinsky, Alexy Roytman, Shivdeep Singh, Constantin Adam, Abdulhamid Adebayo, Sungeun An, Yuan Chi Chang, Xuan-Hong Dang, Nirmit Desai, Michele Dolfi, Hajar Emami-Gohari, Revital Eres, Takuya Goto, Dhiraj Joshi, Yan Koyfman, Mohammad Nassar, Hima Patel, Paramesvaran Selvam, Yousaf Shah, Saptha Surendran, Daiki Tsuzuku, Petros Zerfos, Shahrokh Daijavad",http://arxiv.org/pdf/2409.18164v2,cs.LG
A Survey on Neural Architecture Search Based on Reinforcement Learning,"The automation of feature extraction of machine learning has been
successfully realized by the explosive development of deep learning. However,
the structures and hyperparameters of deep neural network architectures also
make huge difference on the performance in different tasks. The process of
exploring optimal structures and hyperparameters often involves a lot of
tedious human intervene. As a result, a legitimate question is to ask for the
automation of searching for optimal network structures and hyperparameters. The
work of automation of exploring optimal hyperparameters is done by
Hyperparameter Optimization. Neural Architecture Search is aimed to
automatically find the best network structure given specific tasks. In this
paper, we firstly introduced the overall development of Neural Architecture
Search and then focus mainly on providing an overall and understandable survey
about Neural Architecture Search works that are relevant with reinforcement
learning, including improvements and variants based on the hope of satisfying
more complex structures and resource-insufficient environment.",2024-09-26,Wenzhu Shao,http://arxiv.org/pdf/2409.18163v2,cs.LG
Infer Human's Intentions Before Following Natural Language Instructions,"For AI agents to be helpful to humans, they should be able to follow natural
language instructions to complete everyday cooperative tasks in human
environments. However, real human instructions inherently possess ambiguity,
because the human speakers assume sufficient prior knowledge about their hidden
goals and intentions. Standard language grounding and planning methods fail to
address such ambiguities because they do not model human internal goals as
additional partially observable factors in the environment. We propose a new
framework, Follow Instructions with Social and Embodied Reasoning (FISER),
aiming for better natural language instruction following in collaborative
embodied tasks. Our framework makes explicit inferences about human goals and
intentions as intermediate reasoning steps. We implement a set of
Transformer-based models and evaluate them over a challenging benchmark,
HandMeThat. We empirically demonstrate that using social reasoning to
explicitly infer human intentions before making action plans surpasses purely
end-to-end approaches. We also compare our implementation with strong
baselines, including Chain of Thought prompting on the largest available
pre-trained language models, and find that FISER provides better performance on
the embodied social reasoning tasks under investigation, reaching the
state-of-the-art on HandMeThat.",2024-09-26,"Yanming Wan, Yue Wu, Yiping Wang, Jiayuan Mao, Natasha Jaques",http://arxiv.org/pdf/2409.18073v1,cs.LG
Optimal Protocols for Continual Learning via Statistical Physics and Control Theory,"Artificial neural networks often struggle with catastrophic forgetting when
learning multiple tasks sequentially, as training on new tasks degrades the
performance on previously learned tasks. Recent theoretical work has addressed
this issue by analysing learning curves in synthetic frameworks under
predefined training protocols. However, these protocols relied on heuristics
and lacked a solid theoretical foundation assessing their optimality. In this
paper, we fill this gap by combining exact equations for training dynamics,
derived using statistical physics techniques, with optimal control methods. We
apply this approach to teacher-student models for continual learning and
multi-task problems, obtaining a theory for task-selection protocols maximising
performance while minimising forgetting. Our theoretical analysis offers
non-trivial yet interpretable strategies for mitigating catastrophic
forgetting, shedding light on how optimal learning protocols modulate
established effects, such as the influence of task similarity on forgetting.
Finally, we validate our theoretical findings with experiments on real-world
data.",2024-09-26,"Francesco Mori, Stefano Sarao Mannelli, Francesca Mignacco",http://arxiv.org/pdf/2409.18061v2,cs.LG
Inverse Reinforcement Learning with Multiple Planning Horizons,"In this work, we study an inverse reinforcement learning (IRL) problem where
the experts are planning under a shared reward function but with different,
unknown planning horizons. Without the knowledge of discount factors, the
reward function has a larger feasible solution set, which makes it harder for
existing IRL approaches to identify a reward function. To overcome this
challenge, we develop algorithms that can learn a global multi-agent reward
function with agent-specific discount factors that reconstruct the expert
policies. We characterize the feasible solution space of the reward function
and discount factors for both algorithms and demonstrate the generalizability
of the learned reward function across multiple domains.",2024-09-26,"Jiayu Yao, Weiwei Pan, Finale Doshi-Velez, Barbara E Engelhardt",http://arxiv.org/pdf/2409.18051v1,cs.LG
Revisit Anything: Visual Place Recognition via Image Segment Retrieval,"Accurately recognizing a revisited place is crucial for embodied agents to
localize and navigate. This requires visual representations to be distinct,
despite strong variations in camera viewpoint and scene appearance. Existing
visual place recognition pipelines encode the ""whole"" image and search for
matches. This poses a fundamental challenge in matching two images of the same
place captured from different camera viewpoints: ""the similarity of what
overlaps can be dominated by the dissimilarity of what does not overlap"". We
address this by encoding and searching for ""image segments"" instead of the
whole images. We propose to use open-set image segmentation to decompose an
image into `meaningful' entities (i.e., things and stuff). This enables us to
create a novel image representation as a collection of multiple overlapping
subgraphs connecting a segment with its neighboring segments, dubbed
SuperSegment. Furthermore, to efficiently encode these SuperSegments into
compact vector representations, we propose a novel factorized representation of
feature aggregation. We show that retrieving these partial representations
leads to significantly higher recognition recall than the typical whole image
based retrieval. Our segments-based approach, dubbed SegVLAD, sets a new
state-of-the-art in place recognition on a diverse selection of benchmark
datasets, while being applicable to both generic and task-specialized image
encoders. Finally, we demonstrate the potential of our method to ``revisit
anything'' by evaluating our method on an object instance retrieval task, which
bridges the two disparate areas of research: visual place recognition and
object-goal navigation, through their common aim of recognizing goal objects
specific to a place. Source code: https://github.com/AnyLoc/Revisit-Anything.",2024-09-26,"Kartik Garg, Sai Shubodh Puligilla, Shishir Kolathaya, Madhava Krishna, Sourav Garg",http://arxiv.org/pdf/2409.18049v1,cs.LG
IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning,"Recent advancements in image captioning have explored text-only training
methods to overcome the limitations of paired image-text data. However,
existing text-only training methods often overlook the modality gap between
using text data during training and employing images during inference. To
address this issue, we propose a novel approach called Image-like Retrieval,
which aligns text features with visually relevant features to mitigate the
modality gap. Our method further enhances the accuracy of generated captions by
designing a Fusion Module that integrates retrieved captions with input
features. Additionally, we introduce a Frequency-based Entity Filtering
technique that significantly improves caption quality. We integrate these
methods into a unified framework, which we refer to as IFCap
($\textbf{I}$mage-like Retrieval and $\textbf{F}$requency-based Entity
Filtering for Zero-shot $\textbf{Cap}$tioning). Through extensive
experimentation, our straightforward yet powerful approach has demonstrated its
efficacy, outperforming the state-of-the-art methods by a significant margin in
both image captioning and video captioning compared to zero-shot captioning
based on text-only training.",2024-09-26,"Soeun Lee, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim",http://arxiv.org/pdf/2409.18046v1,cs.LG
FlowBench: A Large Scale Benchmark for Flow Simulation over Complex Geometries,"Simulating fluid flow around arbitrary shapes is key to solving various
engineering problems. However, simulating flow physics across complex
geometries remains numerically challenging and computationally
resource-intensive, particularly when using conventional PDE solvers. Machine
learning methods offer attractive opportunities to create fast and adaptable
PDE solvers. However, benchmark datasets to measure the performance of such
methods are scarce, especially for flow physics across complex geometries. We
introduce FlowBench, a dataset for neural simulators with over 10K samples,
which is currently larger than any publicly available flow physics dataset.
FlowBench contains flow simulation data across complex geometries
(\textit{parametric vs. non-parametric}), spanning a range of flow conditions
(\textit{Reynolds number and Grashoff number}), capturing a diverse array of
flow phenomena (\textit{steady vs. transient; forced vs. free convection}), and
for both 2D and 3D. FlowBench contains over 10K data samples, with each sample
the outcome of a fully resolved, direct numerical simulation using a
well-validated simulator framework designed for modeling transport phenomena in
complex geometries. For each sample, we include velocity, pressure, and
temperature field data at 3 different resolutions and several summary
statistics features of engineering relevance (such as coefficients of lift and
drag, and Nusselt numbers). %Additionally, we include masks and signed distance
fields for each shape. We envision that FlowBench will enable evaluating the
interplay between complex geometry, coupled flow phenomena, and data
sufficiency on the performance of current, and future, neural PDE solvers. We
enumerate several evaluation metrics to help rank order the performance of
neural PDE solvers. We benchmark the performance of several baseline methods
including FNO, CNO, WNO, and DeepONet.",2024-09-26,"Ronak Tali, Ali Rabeh, Cheng-Hau Yang, Mehdi Shadkhah, Samundra Karki, Abhisek Upadhyaya, Suriya Dhakshinamoorthy, Marjan Saadati, Soumik Sarkar, Adarsh Krishnamurthy, Chinmay Hegde, Aditya Balu, Baskar Ganapathysubramanian",http://arxiv.org/pdf/2409.18032v1,cs.LG
An Adversarial Perspective on Machine Unlearning for AI Safety,"Large language models are finetuned to refuse questions about hazardous
knowledge, but these protections can often be bypassed. Unlearning methods aim
at completely removing hazardous capabilities from models and make them
inaccessible to adversaries. This work challenges the fundamental differences
between unlearning and traditional safety post-training from an adversarial
perspective. We demonstrate that existing jailbreak methods, previously
reported as ineffective against unlearning, can be successful when applied
carefully. Furthermore, we develop a variety of adaptive methods that recover
most supposedly unlearned capabilities. For instance, we show that finetuning
on 10 unrelated examples or removing specific directions in the activation
space can recover most hazardous capabilities for models edited with RMU, a
state-of-the-art unlearning method. Our findings challenge the robustness of
current unlearning approaches and question their advantages over safety
training.",2024-09-26,"Jakub Łucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tramèr, Javier Rando",http://arxiv.org/pdf/2409.18025v5,cs.LG
Spatiotemporal Learning on Cell-embedded Graphs,"Data-driven simulation of physical systems has recently kindled significant
attention, where many neural models have been developed. In particular,
mesh-based graph neural networks (GNNs) have demonstrated significant potential
in predicting spatiotemporal dynamics across arbitrary geometric domains.
However, the existing node-edge message passing mechanism in GNNs limits the
model's representation learning ability. In this paper, we proposed a
cell-embedded GNN model (aka CeGNN) to learn spatiotemporal dynamics with
lifted performance. Specifically, we introduce a learnable cell attribution to
the node-edge message passing process, which better captures the spatial
dependency of regional features. Such a strategy essentially upgrades the local
aggregation scheme from the first order (e.g., from edge to node) to a higher
order (e.g., from volume to edge and then to node), which takes advantage of
volumetric information in message passing. Meanwhile, a novel feature-enhanced
block is designed to further improve the performance of CeGNN and relieve the
over-smoothness problem, via treating the latent features as basis functions.
The extensive experiments on various PDE systems and one real-world dataset
demonstrate that CeGNN achieves superior performance compared with other
baseline models, particularly reducing the prediction error with up to 1 orders
of magnitude on several PDE systems.",2024-09-26,"Yuan Mi, Hao Sun",http://arxiv.org/pdf/2409.18013v1,cs.LG
Safe Time-Varying Optimization based on Gaussian Processes with Spatio-Temporal Kernel,"Ensuring safety is a key aspect in sequential decision making problems, such
as robotics or process control. The complexity of the underlying systems often
makes finding the optimal decision challenging, especially when the
safety-critical system is time-varying. Overcoming the problem of optimizing an
unknown time-varying reward subject to unknown time-varying safety constraints,
we propose TVSafeOpt, a new algorithm built on Bayesian optimization with a
spatio-temporal kernel. The algorithm is capable of safely tracking a
time-varying safe region without the need for explicit change detection.
Optimality guarantees are also provided for the algorithm when the optimization
problem becomes stationary. We show that TVSafeOpt compares favorably against
SafeOpt on synthetic data, both regarding safety and optimality. Evaluation on
a realistic case study with gas compressors confirms that TVSafeOpt ensures
safety when solving time-varying optimization problems with unknown reward and
safety functions.",2024-09-26,"Jialin Li, Marta Zagorowska, Giulia De Pasquale, Alisa Rupenyan, John Lygeros",http://arxiv.org/pdf/2409.18000v1,cs.LG
PhoCoLens: Photorealistic and Consistent Reconstruction in Lensless Imaging,"Lensless cameras offer significant advantages in size, weight, and cost
compared to traditional lens-based systems. Without a focusing lens, lensless
cameras rely on computational algorithms to recover the scenes from multiplexed
measurements. However, current algorithms struggle with inaccurate forward
imaging models and insufficient priors to reconstruct high-quality images. To
overcome these limitations, we introduce a novel two-stage approach for
consistent and photorealistic lensless image reconstruction. The first stage of
our approach ensures data consistency by focusing on accurately reconstructing
the low-frequency content with a spatially varying deconvolution method that
adjusts to changes in the Point Spread Function (PSF) across the camera's field
of view. The second stage enhances photorealism by incorporating a generative
prior from pre-trained diffusion models. By conditioning on the low-frequency
content retrieved in the first stage, the diffusion model effectively
reconstructs the high-frequency details that are typically lost in the lensless
imaging process, while also maintaining image fidelity. Our method achieves a
superior balance between data fidelity and visual quality compared to existing
methods, as demonstrated with two popular lensless systems, PhlatCam and
DiffuserCam. Project website: https://phocolens.github.io/.",2024-09-26,"Xin Cai, Zhiyuan You, Hailong Zhang, Wentao Liu, Jinwei Gu, Tianfan Xue",http://arxiv.org/pdf/2409.17996v2,cs.LG
Joint Localization and Planning using Diffusion,"Diffusion models have been successfully applied to robotics problems such as
manipulation and vehicle path planning. In this work, we explore their
application to end-to-end navigation -- including both perception and planning
-- by considering the problem of jointly performing global localization and
path planning in known but arbitrary 2D environments. In particular, we
introduce a diffusion model which produces collision-free paths in a global
reference frame given an egocentric LIDAR scan, an arbitrary map, and a desired
goal position. To this end, we implement diffusion in the space of paths in
SE(2), and describe how to condition the denoising process on both obstacles
and sensor observations. In our evaluation, we show that the proposed
conditioning techniques enable generalization to realistic maps of considerably
different appearance than the training environment, demonstrate our model's
ability to accurately describe ambiguous solutions, and run extensive
simulation experiments showcasing our model's use as a real-time, end-to-end
localization and planning stack.",2024-09-26,"L. Lao Beyer, S. Karaman",http://arxiv.org/pdf/2409.17995v1,cs.LG
LoopSR: Looping Sim-and-Real for Lifelong Policy Adaptation of Legged Robots,"Reinforcement Learning (RL) has shown its remarkable and generalizable
capability in legged locomotion through sim-to-real transfer. However, while
adaptive methods like domain randomization are expected to make policy more
robust to diverse environments, such comprehensiveness potentially detracts
from the policy's performance in any specific environment according to the No
Free Lunch theorem, leading to a suboptimal solution once deployed in the real
world. To address this issue, we propose a lifelong policy adaptation framework
named LoopSR, which utilizes a transformer-based encoder to project real-world
trajectories into a latent space, and accordingly reconstruct the real-world
environments back in simulation for further improvement. Autoencoder
architecture and contrastive learning methods are adopted to better extract the
characteristics of real-world dynamics. The simulation parameters for continual
training are derived by combining predicted parameters from the decoder with
retrieved parameters from the simulation trajectory dataset. By leveraging the
continual training, LoopSR achieves superior data efficiency compared with
strong baselines, with only a limited amount of data to yield eminent
performance in both sim-to-sim and sim-to-real experiments.",2024-09-26,"Peilin Wu, Weiji Xie, Jiahang Cao, Hang Lai, Weinan Zhang",http://arxiv.org/pdf/2409.17992v1,cs.LG
Dimension-independent learning rates for high-dimensional classification problems,"We study the problem of approximating and estimating classification functions
that have their decision boundary in the $RBV^2$ space. Functions of $RBV^2$
type arise naturally as solutions of regularized neural network learning
problems and neural networks can approximate these functions without the curse
of dimensionality. We modify existing results to show that every $RBV^2$
function can be approximated by a neural network with bounded weights.
Thereafter, we prove the existence of a neural network with bounded weights
approximating a classification function. And we leverage these bounds to
quantify the estimation rates. Finally, we present a numerical study that
analyzes the effect of different regularity conditions on the decision
boundaries.",2024-09-26,"Andres Felipe Lerma-Pineda, Philipp Petersen, Simon Frieder, Thomas Lukasiewicz",http://arxiv.org/pdf/2409.17991v1,cs.LG
Supra-Laplacian Encoding for Transformer on Dynamic Graphs,"Fully connected Graph Transformers (GT) have rapidly become prominent in the
static graph community as an alternative to Message-Passing models, which
suffer from a lack of expressivity, oversquashing, and under-reaching. However,
in a dynamic context, by interconnecting all nodes at multiple snapshots with
self-attention, GT loose both structural and temporal information. In this
work, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs
(SLATE), a new spatio-temporal encoding to leverage the GT architecture while
keeping spatio-temporal information. Specifically, we transform Discrete Time
Dynamic Graphs into multi-layer graphs and take advantage of the spectral
properties of their associated supra-Laplacian matrix. Our second contribution
explicitly model nodes' pairwise relationships with a cross-attention
mechanism, providing an accurate edge representation for dynamic link
prediction. SLATE outperforms numerous state-of-the-art methods based on
Message-Passing Graph Neural Networks combined with recurrent models (e.g
LSTM), and Dynamic Graph Transformers, on 9 datasets. Code is available at:
github.com/ykrmm/SLATE.",2024-09-26,"Yannis Karmim, Marc Lafon, Raphael Fournier S'niehotta, Nicolas Thome",http://arxiv.org/pdf/2409.17986v2,cs.LG
Hypergame Theory for Decentralized Resource Allocation in Multi-user Semantic Communications,"Semantic communications (SC) is an emerging communication paradigm in which
wireless devices can send only relevant information from a source of data while
relying on computing resources to regenerate missing data points. However, the
design of a multi-user SC system becomes more challenging because of the
computing and communication overhead required for coordination. Existing
solutions for learning the semantic language and performing resource allocation
often fail to capture the computing and communication tradeoffs involved in
multiuser SC. To address this gap, a novel framework for decentralized
computing and communication resource allocation in multiuser SC systems is
proposed. The challenge of efficiently allocating communication and computing
resources (for reasoning) in a decentralized manner to maximize the quality of
task experience for the end users is addressed through the application of
Stackelberg hyper game theory. Leveraging the concept of second-level hyper
games, novel analytical formulations are developed to model misperceptions of
the users about each other's communication and control strategies. Further,
equilibrium analysis of the learned resource allocation protocols examines the
convergence of the computing and communication strategies to a local
Stackelberg equilibria, considering misperceptions. Simulation results show
that the proposed Stackelberg hyper game results in efficient usage of
communication and computing resources while maintaining a high quality of
experience for the users compared to state-of-the-art that does not account for
the misperceptions.",2024-09-26,"Christo Kurisummoottil Thomas, Walid Saad",http://arxiv.org/pdf/2409.17985v2,cs.LG
HydraViT: Stacking Heads for a Scalable ViT,"The architecture of Vision Transformers (ViTs), particularly the Multi-head
Attention (MHA) mechanism, imposes substantial hardware demands. Deploying ViTs
on devices with varying constraints, such as mobile phones, requires multiple
models of different sizes. However, this approach has limitations, such as
training and storing each required model separately. This paper introduces
HydraViT, a novel approach that addresses these limitations by stacking
attention heads to achieve a scalable ViT. By repeatedly changing the size of
the embedded dimensions throughout each layer and their corresponding number of
attention heads in MHA during training, HydraViT induces multiple subnetworks.
Thereby, HydraViT achieves adaptability across a wide spectrum of hardware
environments while maintaining performance. Our experimental results
demonstrate the efficacy of HydraViT in achieving a scalable ViT with up to 10
subnetworks, covering a wide range of resource constraints. HydraViT achieves
up to 5 p.p. more accuracy with the same GMACs and up to 7 p.p. more accuracy
with the same throughput on ImageNet-1K compared to the baselines, making it an
effective solution for scenarios where hardware availability is diverse or
varies over time. Source code available at https://github.com/ds-kiel/HydraViT.",2024-09-26,"Janek Haberer, Ali Hojjat, Olaf Landsiedel",http://arxiv.org/pdf/2409.17978v2,cs.LG
Supervised Learning Model for Key Frame Identification from Cow Teat Videos,"This paper proposes a method for improving the accuracy of mastitis risk
assessment in cows using neural networks and video analysis. Mastitis, an
infection of the udder tissue, is a critical health problem for cows and can be
detected by examining the cow's teat. Traditionally, veterinarians assess the
health of a cow's teat during the milking process, but this process is limited
in time and can weaken the accuracy of the assessment. In commercial farms,
cows are recorded by cameras when they are milked in the milking parlor. This
paper uses a neural network to identify key frames in the recorded video where
the cow's udder appears intact. These key frames allow veterinarians to have
more flexible time to perform health assessments on the teat, increasing their
efficiency and accuracy. However, there are challenges in using cow teat video
for mastitis risk assessment, such as complex environments, changing cow
positions and postures, and difficulty in identifying the udder from the video.
To address these challenges, a fusion distance and an ensemble model are
proposed to improve the performance (F-score) of identifying key frames from
cow teat videos. The results show that these two approaches improve performance
compared to using a single distance measure or model.",2024-09-26,"Minghao Wang, Pinxue Lin",http://arxiv.org/pdf/2409.18797v1,cs.LG
Similarity Learning with neural networks,"In this work, we introduce a neural network algorithm designed to
automatically identify similarity relations from data. By uncovering these
similarity relations, our network approximates the underlying physical laws
that relate dimensionless quantities to their dimensionless variables and
coefficients. Additionally, we develop a linear algebra framework, accompanied
by code, to derive the symmetry groups associated with these similarity
relations. While our approach is general, we illustrate its application through
examples in fluid mechanics, including laminar Newtonian and non-Newtonian
flows in smooth pipes, as well as turbulent flows in both smooth and rough
pipes. Such examples are chosen to highlight the framework's capability to
handle both simple and intricate cases, and further validates its effectiveness
in discovering underlying physical laws from data.",2024-09-26,"Gabriel Sanfins, Fabio Ramos, Danilo Naiff",http://arxiv.org/pdf/2410.07214v1,cs.LG
BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and Adaptive Disambiguate based Efficient Tree Search,"Large Language Models (LLMs) have exhibited exceptional performance across a
broad range of tasks and domains. However, they still encounter difficulties in
solving mathematical problems due to the rigorous and logical nature of
mathematics. Previous studies have employed techniques such as supervised
fine-tuning (SFT), prompt engineering, and search-based methods to improve the
mathematical problem-solving abilities of LLMs. Despite these efforts, their
performance remains suboptimal and demands substantial computational resources.
To address this issue, we propose a novel approach, BEATS, to enhance
mathematical problem-solving abilities. Our method leverages newly designed
prompts that guide the model to iteratively rewrite, advance by one step, and
generate answers based on previous steps. Additionally, we introduce a new
back-verification technique that uses LLMs to validate the correctness of the
generated answers. Furthermore, we employ a pruning tree search to optimize
search time while achieving strong performance. Notably, our method improves
Qwen2-7b-Instruct's score from 36.94 to 61.52, outperforming GPT4's 42.5 on the
MATH benchmark.",2024-09-26,"Linzhuang Sun, Hao Liang, Jingxuan Wei, Bihui Yu, Conghui He, Zenan Zhou, Wentao Zhang",http://arxiv.org/pdf/2409.17972v2,cs.LG
On Translating Technical Terminology: A Translation Workflow for Machine-Translated Acronyms,"The typical workflow for a professional translator to translate a document
from its source language (SL) to a target language (TL) is not always focused
on what many language models in natural language processing (NLP) do - predict
the next word in a series of words. While high-resource languages like English
and French are reported to achieve near human parity using common metrics for
measurement such as BLEU and COMET, we find that an important step is being
missed: the translation of technical terms, specifically acronyms. Some
state-of-the art machine translation systems like Google Translate which are
publicly available can be erroneous when dealing with acronyms - as much as 50%
in our findings. This article addresses acronym disambiguation for MT systems
by proposing an additional step to the SL-TL (FR-EN) translation workflow where
we first offer a new acronym corpus for public consumption and then experiment
with a search-based thresholding algorithm that achieves nearly 10% increase
when compared to Google Translate and OpusMT.",2024-09-26,"Richard Yue, John E. Ortega, Kenneth Ward Church",http://arxiv.org/pdf/2409.17943v1,cs.LG
Predicting Anchored Text from Translation Memories for Machine Translation Using Deep Learning Methods,"Translation memories (TMs) are the backbone for professional translation
tools called computer-aided translation (CAT) tools. In order to perform a
translation using a CAT tool, a translator uses the TM to gather translations
similar to the desired segment to translate (s'). Many CAT tools offer a
fuzzy-match algorithm to locate segments (s) in the TM that are close in
distance to s'. After locating two similar segments, the CAT tool will present
parallel segments (s, t) that contain one segment in the source language along
with its translation in the target language. Additionally, CAT tools contain
fuzzy-match repair (FMR) techniques that will automatically use the parallel
segments from the TM to create new TM entries containing a modified version of
the original with the idea in mind that it will be the translation of s'. Most
FMR techniques use machine translation as a way of ""repairing"" those words that
have to be modified. In this article, we show that for a large part of those
words which are anchored, we can use other techniques that are based on machine
learning approaches such as Word2Vec. BERT, and even ChatGPT. Specifically, we
show that for anchored words that follow the continuous bag-of-words (CBOW)
paradigm, Word2Vec, BERT, and GPT-4 can be used to achieve similar and, for
some cases, better results than neural machine translation for translating
anchored words from French to English.",2024-09-26,"Richard Yue, John E. Ortega",http://arxiv.org/pdf/2409.17939v1,cs.LG
Adaptive Stream Processing on Edge Devices through Active Inference,"The current scenario of IoT is witnessing a constant increase on the volume
of data, which is generated in constant stream, calling for novel architectural
and logical solutions for processing it. Moving the data handling towards the
edge of the computing spectrum guarantees better distribution of load and, in
principle, lower latency and better privacy. However, managing such a structure
is complex, especially when requirements, also referred to Service Level
Objectives (SLOs), specified by applications' owners and infrastructure
managers need to be ensured. Despite the rich number of proposals of Machine
Learning (ML) based management solutions, researchers and practitioners yet
struggle to guarantee long-term prediction and control, and accurate
troubleshooting. Therefore, we present a novel ML paradigm based on Active
Inference (AIF) -- a concept from neuroscience that describes how the brain
constantly predicts and evaluates sensory information to decrease long-term
surprise. We implement it and evaluate it in a heterogeneous real stream
processing use case, where an AIF-based agent continuously optimizes the
fulfillment of three SLOs for three autonomous driving services running on
multiple devices. The agent used causal knowledge to gradually develop an
understanding of how its actions are related to requirements fulfillment, and
which configurations to favor. Through this approach, our agent requires up to
thirty iterations to converge to the optimal solution, showing the capability
of offering accurate results in a short amount of time. Furthermore, thanks to
AIF and its causal structures, our method guarantees full transparency on the
decision making, making the interpretation of the results and the
troubleshooting effortless.",2024-09-26,"Boris Sedlak, Victor Casamayor Pujol, Andrea Morichetta, Praveen Kumar Donta, Schahram Dustdar",http://arxiv.org/pdf/2409.17937v1,cs.LG
Sample Compression Unleashed: New Generalization Bounds for Real Valued Losses,"The sample compression theory provides generalization guarantees for
predictors that can be fully defined using a subset of the training dataset and
a (short) message string, generally defined as a binary sequence. Previous
works provided generalization bounds for the zero-one loss, which is
restrictive notably when applied to deep learning approaches. In this paper, we
present a general framework for deriving new sample compression bounds that
hold for real-valued unbounded losses. Using the Pick-To-Learn (P2L)
meta-algorithm, which transforms the training method of any machine-learning
predictor to yield sample-compressed predictors, we empirically demonstrate the
tightness of the bounds and their versatility by evaluating them on random
forests and multiple types of neural networks.",2024-09-26,"Mathieu Bazinet, Valentina Zantedeschi, Pascal Germain",http://arxiv.org/pdf/2409.17932v3,cs.LG
Remaining Useful Life Prediction for Batteries Utilizing an Explainable AI Approach with a Predictive Application for Decision-Making,"Accurately estimating the Remaining Useful Life (RUL) of a battery is
essential for determining its lifespan and recharge requirements. In this work,
we develop machine learning-based models to predict and classify battery RUL.
We introduce a two-level ensemble learning (TLE) framework and a CNN+MLP hybrid
model for RUL prediction, comparing their performance against traditional,
deep, and hybrid machine learning models. Our analysis evaluates various models
for both prediction and classification while incorporating interpretability
through SHAP. The proposed TLE model consistently outperforms baseline models
in RMSE, MAE, and R squared error, demonstrating its superior predictive
capabilities. Additionally, the XGBoost classifier achieves an impressive 99%
classification accuracy, validated through cross-validation techniques. The
models effectively predict relay-based charging triggers, enabling automated
and energy-efficient charging processes. This automation reduces energy
consumption and enhances battery performance by optimizing charging cycles.
SHAP interpretability analysis highlights the cycle index and charging
parameters as the most critical factors influencing RUL. To improve
accessibility, we developed a Tkinter-based GUI that allows users to input new
data and predict RUL in real time. This practical solution supports sustainable
battery management by enabling data-driven decisions about battery usage and
maintenance, contributing to energy-efficient and innovative battery life
prediction.",2024-09-26,"Biplov Paneru, Bipul Thapa, Durga Prasad Mainali, Bishwash Paneru, Krishna Bikram Shah",http://arxiv.org/pdf/2409.17931v2,cs.LG
Graph Reasoning with Large Language Models via Pseudo-code Prompting,"Large language models (LLMs) have recently achieved remarkable success in
various reasoning tasks in the field of natural language processing. This
success of LLMs has also motivated their use in graph-related tasks. Among
others, recent work has explored whether LLMs can solve graph problems such as
counting the number of connected components of a graph or computing the
shortest path distance between two nodes. Although LLMs possess preliminary
graph reasoning abilities, they might still struggle to solve some seemingly
simple problems. In this paper, we investigate whether prompting via
pseudo-code instructions can improve the performance of LLMs in solving graph
problems. Our experiments demonstrate that using pseudo-code instructions
generally improves the performance of all considered LLMs. The graphs,
pseudo-code prompts, and evaluation code are publicly available.",2024-09-26,"Konstantinos Skianis, Giannis Nikolentzos, Michalis Vazirgiannis",http://arxiv.org/pdf/2409.17906v1,cs.LG
"Designing Short-Stage CDC-XPUFs: Balancing Reliability, Cost, and Security in IoT Devices","The rapid expansion of Internet of Things (IoT) devices demands robust and
resource-efficient security solutions. Physically Unclonable Functions (PUFs),
which generate unique cryptographic keys from inherent hardware variations,
offer a promising approach. However, traditional PUFs like Arbiter PUFs (APUFs)
and XOR Arbiter PUFs (XOR-PUFs) are susceptible to machine learning (ML) and
reliability-based attacks. In this study, we investigate
Component-Differentially Challenged XOR-PUFs (CDC-XPUFs), a less explored
variant, to address these vulnerabilities. We propose an optimized CDC-XPUF
design that incorporates a pre-selection strategy to enhance reliability and
introduces a novel lightweight architecture to reduce hardware overhead.
Rigorous testing demonstrates that our design significantly lowers resource
consumption, maintains strong resistance to ML attacks, and improves
reliability, effectively mitigating reliability-based attacks. These results
highlight the potential of CDC-XPUFs as a secure and efficient candidate for
widespread deployment in resource-constrained IoT systems.",2024-09-26,"Gaoxiang Li, Yu Zhuang",http://arxiv.org/pdf/2409.17902v1,cs.LG
Model-Free versus Model-Based Reinforcement Learning for Fixed-Wing UAV Attitude Control Under Varying Wind Conditions,"This paper evaluates and compares the performance of model-free and
model-based reinforcement learning for the attitude control of fixed-wing
unmanned aerial vehicles using PID as a reference point. The comparison focuses
on their ability to handle varying flight dynamics and wind disturbances in a
simulated environment. Our results show that the Temporal Difference Model
Predictive Control agent outperforms both the PID controller and other
model-free reinforcement learning methods in terms of tracking accuracy and
robustness over different reference difficulties, particularly in nonlinear
flight regimes. Furthermore, we introduce actuation fluctuation as a key metric
to assess energy efficiency and actuator wear, and we test two different
approaches from the literature: action variation penalty and conditioning for
action policy smoothness. We also evaluate all control methods when subject to
stochastic turbulence and gusts separately, so as to measure their effects on
tracking performance, observe their limitations and outline their implications
on the Markov decision process formalism.",2024-09-26,"David Olivares, Pierre Fournier, Pavan Vasishta, Julien Marzat",http://arxiv.org/pdf/2409.17896v1,cs.LG
A multi-source data power load forecasting method using attention mechanism-based parallel cnn-gru,"Accurate power load forecasting is crucial for improving energy efficiency
and ensuring power supply quality. Considering the power load forecasting
problem involves not only dynamic factors like historical load variations but
also static factors such as climate conditions that remain constant over
specific periods. From the model-agnostic perspective, this paper proposes a
parallel structure network to extract important information from both dynamic
and static data. Firstly, based on complexity learning theory, it is
demonstrated that models integrated through parallel structures exhibit
superior generalization abilities compared to individual base learners.
Additionally, the higher the independence between base learners, the stronger
the generalization ability of the parallel structure model. This suggests that
the structure of machine learning models inherently contains significant
information. Building on this theoretical foundation, a parallel convolutional
neural network (CNN)-gate recurrent unit (GRU) attention model (PCGA) is
employed to address the power load forecasting issue, aiming to effectively
integrate the influences of dynamic and static features. The CNN module is
responsible for capturing spatial characteristics from static data, while the
GRU module captures long-term dependencies in dynamic time series data. The
attention layer is designed to focus on key information from the
spatial-temporal features extracted by the parallel CNN-GRU. To substantiate
the advantages of the parallel structure model in extracting and integrating
multi-source information, a series of experiments are conducted.",2024-09-26,"Chao Min, Yijia Wang, Bo Zhang, Xin Ma, Junyi Cui",http://arxiv.org/pdf/2409.17889v1,cs.LG
A method for identifying causality in the response of nonlinear dynamical systems,"Predicting the response of nonlinear dynamical systems subject to random,
broadband excitation is important across a range of scientific disciplines,
such as structural dynamics and neuroscience. Building data-driven models
requires experimental measurements of the system input and output, but it can
be difficult to determine whether inaccuracies in the model stem from modelling
errors or noise. This paper presents a novel method to identify the causal
component of the input-output data from measurements of a system in the
presence of output noise, as a function of frequency, without needing a high
fidelity model. An output prediction, calculated using an available model, is
optimally combined with noisy measurements of the output to predict the input
to the system. The parameters of the algorithm balance the two output signals
and are utilised to calculate a nonlinear coherence metric as a measure of
causality. This method is applicable to a broad class of nonlinear dynamical
systems. There are currently no solutions to this problem in the absence of a
complete benchmark model.",2024-09-26,"Joseph Massingham, Ole Nielsen, Tore Butlin",http://arxiv.org/pdf/2409.17872v1,cs.LG
Efficient Arbitrary Precision Acceleration for Large Language Models on GPU Tensor Cores,"Large language models (LLMs) have been widely applied but face challenges in
efficient inference. While quantization methods reduce computational demands,
ultra-low bit quantization with arbitrary precision is hindered by limited GPU
Tensor Core support and inefficient memory management, leading to suboptimal
acceleration. To address these challenges, we propose a comprehensive
acceleration scheme for arbitrary precision LLMs. At its core, we introduce a
novel bipolar-INT data format that facilitates parallel computing and supports
symmetric quantization, effectively reducing data redundancy. Building on this,
we implement an arbitrary precision matrix multiplication scheme that
decomposes and recovers matrices at the bit level, enabling flexible precision
while maximizing GPU Tensor Core utilization. Furthermore, we develop an
efficient matrix preprocessing method that optimizes data layout for subsequent
computations. Finally, we design a data recovery-oriented memory management
system that strategically utilizes fast shared memory, significantly enhancing
kernel execution speed and minimizing memory access latency. Experimental
results demonstrate our approach's effectiveness, with up to 2.4\times speedup
in matrix multiplication compared to NVIDIA's CUTLASS. When integrated into
LLMs, we achieve up to 6.7\times inference acceleration. These improvements
significantly enhance LLM inference efficiency, enabling broader and more
responsive applications of LLMs.",2024-09-26,"Shaobo Ma, Chao Fang, Haikuo Shao, Zhongfeng Wang",http://arxiv.org/pdf/2409.17870v2,cs.LG
Implementing a Nordic-Baltic Federated Health Data Network: a case report,"Background: Centralized collection and processing of healthcare data across
national borders pose significant challenges, including privacy concerns, data
heterogeneity and legal barriers. To address some of these challenges, we
formed an interdisciplinary consortium to develop a feder-ated health data
network, comprised of six institutions across five countries, to facilitate
Nordic-Baltic cooperation on secondary use of health data. The objective of
this report is to offer early insights into our experiences developing this
network. Methods: We used a mixed-method ap-proach, combining both experimental
design and implementation science to evaluate the factors affecting the
implementation of our network. Results: Technically, our experiments indicate
that the network functions without significant performance degradation compared
to centralized simu-lation. Conclusion: While use of interdisciplinary
approaches holds a potential to solve challeng-es associated with establishing
such collaborative networks, our findings turn the spotlight on the uncertain
regulatory landscape playing catch up and the significant operational costs.",2024-09-26,"Taridzo Chomutare, Aleksandar Babic, Laura-Maria Peltonen, Silja Elunurm, Peter Lundberg, Arne Jönsson, Emma Eneling, Ciprian-Virgil Gerstenberger, Troels Siggaard, Raivo Kolde, Oskar Jerdhaf, Martin Hansson, Alexandra Makhlysheva, Miroslav Muzny, Erik Ylipää, Søren Brunak, Hercules Dalianis",http://arxiv.org/pdf/2409.17865v1,cs.LG
A Multimodal Single-Branch Embedding Network for Recommendation in Cold-Start and Missing Modality Scenarios,"Most recommender systems adopt collaborative filtering (CF) and provide
recommendations based on past collective interactions. Therefore, the
performance of CF algorithms degrades when few or no interactions are
available, a scenario referred to as cold-start. To address this issue,
previous work relies on models leveraging both collaborative data and side
information on the users or items. Similar to multimodal learning, these models
aim at combining collaborative and content representations in a shared
embedding space. In this work we propose a novel technique for multimodal
recommendation, relying on a multimodal Single-Branch embedding network for
Recommendation (SiBraR). Leveraging weight-sharing, SiBraR encodes interaction
data as well as multimodal side information using the same single-branch
embedding network on different modalities. This makes SiBraR effective in
scenarios of missing modality, including cold start. Our extensive experiments
on large-scale recommendation datasets from three different recommendation
domains (music, movie, and e-commerce) and providing multimodal content
information (audio, text, image, labels, and interactions) show that SiBraR
significantly outperforms CF as well as state-of-the-art content-based RSs in
cold-start scenarios, and is competitive in warm scenarios. We show that
SiBraR's recommendations are accurate in missing modality scenarios, and that
the model is able to map different modalities to the same region of the shared
embedding space, hence reducing the modality gap.",2024-09-26,"Christian Ganhör, Marta Moscati, Anna Hausberger, Shah Nawaz, Markus Schedl",http://arxiv.org/pdf/2409.17864v1,cs.LG
How Feature Learning Can Improve Neural Scaling Laws,"We develop a solvable model of neural scaling laws beyond the kernel limit.
Theoretical analysis of this model shows how performance scales with model
size, training time, and the total amount of available data. We identify three
scaling regimes corresponding to varying task difficulties: hard, easy, and
super easy tasks. For easy and super-easy target functions, which lie in the
reproducing kernel Hilbert space (RKHS) defined by the initial infinite-width
Neural Tangent Kernel (NTK), the scaling exponents remain unchanged between
feature learning and kernel regime models. For hard tasks, defined as those
outside the RKHS of the initial NTK, we demonstrate both analytically and
empirically that feature learning can improve scaling with training time and
compute, nearly doubling the exponent for hard tasks. This leads to a different
compute optimal strategy to scale parameters and training time in the feature
learning regime. We support our finding that feature learning improves the
scaling law for hard tasks but not for easy and super-easy tasks with
experiments of nonlinear MLPs fitting functions with power-law Fourier spectra
on the circle and CNNs learning vision tasks.",2024-09-26,"Blake Bordelon, Alexander Atanasov, Cengiz Pehlevan",http://arxiv.org/pdf/2409.17858v2,cs.LG
AMARO: All Heavy-Atom Transferable Neural Network Potentials of Protein Thermodynamics,"All-atom molecular simulations offer detailed insights into macromolecular
phenomena, but their substantial computational cost hinders the exploration of
complex biological processes. We introduce Advanced Machine-learning Atomic
Representation Omni-force-field (AMARO), a new neural network potential (NNP)
that combines an O(3)-equivariant message-passing neural network architecture,
TensorNet, with a coarse-graining map that excludes hydrogen atoms. AMARO
demonstrates the feasibility of training coarser NNP, without prior energy
terms, to run stable protein dynamics with scalability and generalization
capabilities.",2024-09-26,"Antonio Mirarchi, Raul P. Pelaez, Guillem Simeon, Gianni De Fabritiis",http://arxiv.org/pdf/2409.17852v3,cs.LG
Machine Learning-based vs Deep Learning-based Anomaly Detection in Multivariate Time Series for Spacecraft Attitude Sensors,"In the framework of Failure Detection, Isolation and Recovery (FDIR) on
spacecraft, new AI-based approaches are emerging in the state of the art to
overcome the limitations commonly imposed by traditional threshold checking.
  The present research aims at characterizing two different approaches to the
problem of stuck values detection in multivariate time series coming from
spacecraft attitude sensors. The analysis reveals the performance differences
in the two approaches, while commenting on their interpretability and
generalization to different scenarios.",2024-09-26,"R. Gallon, F. Schiemenz, A. Krstova, A. Menicucci, E. Gill",http://arxiv.org/pdf/2409.17841v1,cs.LG
Language Models as Zero-shot Lossless Gradient Compressors: Towards General Neural Parameter Prior Models,"Despite the widespread use of statistical prior models in various fields,
such models for neural network gradients have long been overlooked. The
inherent challenge stems from their high-dimensional structures and complex
interdependencies, which complicate effective modeling. In this work, we
demonstrate the potential of large language models (LLMs) to act as gradient
priors in a zero-shot setting. We examine the property by considering lossless
gradient compression -- a critical application in distributed learning -- that
depends heavily on precise probability modeling. To achieve this, we introduce
LM-GC, a novel method that integrates LLMs with arithmetic coding. Our
technique converts plain gradients into text-like formats, enhancing token
efficiency by up to 38 times compared to their plain representations. We ensure
that this data conversion maintains a close alignment with the structure of
plain gradients and the symbols commonly recognized by LLMs. Our experiments
indicate that LM-GC surpasses existing state-of-the-art lossless compression
methods, improving compression rates by 10% up to 17.2% across various datasets
and architectures. Additionally, our approach shows promising compatibility
with lossy compression techniques such as quantization and sparsification.
These findings highlight the significant potential of LLMs as a model for
effectively handling gradients. Code is available at
https://github.com/hui-po-wang/LM-GC.",2024-09-26,"Hui-Po Wang, Mario Fritz",http://arxiv.org/pdf/2409.17836v2,cs.LG
Ordinary Differential Equations for Enhanced 12-Lead ECG Generation,"In the realm of artificial intelligence, the generation of realistic training
data for supervised learning tasks presents a significant challenge. This is
particularly true in the synthesis of electrocardiograms (ECGs), where the
objective is to develop a synthetic 12-lead ECG model. The primary complexity
of this task stems from accurately modeling the intricate biological and
physiological interactions among different ECG leads. Although mathematical
process simulators have shed light on these dynamics, effectively incorporating
this understanding into generative models is not straightforward. In this work,
we introduce an innovative method that employs ordinary differential equations
(ODEs) to enhance the fidelity of generating 12-lead ECG data. This approach
integrates a system of ODEs that represent cardiac dynamics directly into the
generative model's optimization process, allowing for the production of
biologically plausible ECG training data that authentically reflects real-world
variability and inter-lead dependencies. We conducted an empirical analysis of
thousands of ECGs and found that incorporating cardiac simulation insights into
the data generation process significantly improves the accuracy of heart
abnormality classifiers trained on this synthetic 12-lead ECG data.",2024-09-26,"Yakir Yehuda, Kira Radinsky",http://arxiv.org/pdf/2409.17833v1,cs.LG
Geospatial Road Cycling Race Results Data Set,"The field of cycling analytics has only recently started to develop due to
limited access to open data sources. Accordingly, research and data sources are
very divergent, with large differences in information used across studies. To
improve this, and facilitate further research in the field, we propose the
publication of a data set which links thousands of professional race results
from the period 2017-2023 to detailed geographic information about the courses,
an essential aspect in road cycling analytics. Initial use cases are proposed,
showcasing the usefulness in linking these two data sources.",2024-09-26,"Bram Janssens, Luca Pappalardo, Jelle De Bock, Matthias Bogaert, Steven Verstockt",http://arxiv.org/pdf/2410.09055v1,cs.LG
Decomposable Transformer Point Processes,"The standard paradigm of modeling marked point processes is by parameterizing
the intensity function using an attention-based (Transformer-style)
architecture. Despite the flexibility of these methods, their inference is
based on the computationally intensive thinning algorithm. In this work, we
propose a framework where the advantages of the attention-based architecture
are maintained and the limitation of the thinning algorithm is circumvented.
The framework depends on modeling the conditional distribution of inter-event
times with a mixture of log-normals satisfying a Markov property and the
conditional probability mass function for the marks with a Transformer-based
architecture. The proposed method attains state-of-the-art performance in
predicting the next event of a sequence given its history. The experiments also
reveal the efficacy of the methods that do not rely on the thinning algorithm
during inference over the ones they do. Finally, we test our method on the
challenging long-horizon prediction task and find that it outperforms a
baseline developed specifically for tackling this task; importantly, inference
requires just a fraction of time compared to the thinning-based baseline.",2024-09-26,Aristeidis Panos,http://arxiv.org/pdf/2409.18158v1,cs.LG
Physics-aligned Schrödinger bridge,"The reconstruction of physical fields from sparse measurements is pivotal in
both scientific research and engineering applications. Traditional methods are
increasingly supplemented by deep learning models due to their efficacy in
extracting features from data. However, except for the low accuracy on complex
physical systems, these models often fail to comply with essential physical
constraints, such as governing equations and boundary conditions. To overcome
this limitation, we introduce a novel data-driven field reconstruction
framework, termed the Physics-aligned Schr\""{o}dinger Bridge (PalSB). This
framework leverages a diffusion Schr\""{o}dinger bridge mechanism that is
specifically tailored to align with physical constraints. The PalSB approach
incorporates a dual-stage training process designed to address both local
reconstruction mapping and global physical principles. Additionally, a
boundary-aware sampling technique is implemented to ensure adherence to
physical boundary conditions. We demonstrate the effectiveness of PalSB through
its application to three complex nonlinear systems: cylinder flow from Particle
Image Velocimetry experiments, two-dimensional turbulence, and a
reaction-diffusion system. The results reveal that PalSB not only achieves
higher accuracy but also exhibits enhanced compliance with physical constraints
compared to existing methods. This highlights PalSB's capability to generate
high-quality representations of intricate physical interactions, showcasing its
potential for advancing field reconstruction techniques.",2024-09-26,"Zeyu Li, Hongkun Dou, Shen Fang, Wang Han, Yue Deng, Lijun Yang",http://arxiv.org/pdf/2409.17825v1,cs.LG
Generative Modeling of Molecular Dynamics Trajectories,"Molecular dynamics (MD) is a powerful technique for studying microscopic
phenomena, but its computational cost has driven significant interest in the
development of deep learning-based surrogate models. We introduce generative
modeling of molecular trajectories as a paradigm for learning flexible
multi-task surrogate models of MD from data. By conditioning on appropriately
chosen frames of the trajectory, we show such generative models can be adapted
to diverse tasks such as forward simulation, transition path sampling, and
trajectory upsampling. By alternatively conditioning on part of the molecular
system and inpainting the rest, we also demonstrate the first steps towards
dynamics-conditioned molecular design. We validate the full set of these
capabilities on tetrapeptide simulations and show that our model can produce
reasonable ensembles of protein monomers. Altogether, our work illustrates how
generative modeling can unlock value from MD data towards diverse downstream
tasks that are not straightforward to address with existing methods or even MD
itself. Code is available at https://github.com/bjing2016/mdgen.",2024-09-26,"Bowen Jing, Hannes Stärk, Tommi Jaakkola, Bonnie Berger",http://arxiv.org/pdf/2409.17808v1,cs.LG
Continual learning with task specialist,"Continual learning (CL) adapt the deep learning scenarios with timely updated
datasets. However, existing CL models suffer from the catastrophic forgetting
issue, where new knowledge replaces past learning. In this paper, we propose
Continual Learning with Task Specialists (CLTS) to address the issues of
catastrophic forgetting and limited labelled data in real-world datasets by
performing class incremental learning of the incoming stream of data. The model
consists of Task Specialists (T S) and Task Predictor (T P ) with pre-trained
Stable Diffusion (SD) module. Here, we introduce a new specialist to handle a
new task sequence and each T S has three blocks; i) a variational autoencoder
(V AE) to learn the task distribution in a low dimensional latent space, ii) a
K-Means block to perform data clustering and iii) Bootstrapping Language-Image
Pre-training (BLIP ) model to generate a small batch of captions from the input
data. These captions are fed as input to the pre-trained stable diffusion model
(SD) for the generation of task samples. The proposed model does not store any
task samples for replay, instead uses generated samples from SD to train the T
P module. A comparison study with four SOTA models conducted on three
real-world datasets shows that the proposed model outperforms all the selected
baselines",2024-09-26,"Indu Solomon, Aye Phyu Phyu Aung, Uttam Kumar, Senthilnath Jayavelu",http://arxiv.org/pdf/2409.17806v1,cs.LG
Enriched Functional Tree-Based Classifiers: A Novel Approach Leveraging Derivatives and Geometric Features,"The positioning of this research falls within the scalar-on-function
classification literature, a field of significant interest across various
domains, particularly in statistics, mathematics, and computer science. This
study introduces an advanced methodology for supervised classification by
integrating Functional Data Analysis (FDA) with tree-based ensemble techniques
for classifying high-dimensional time series. The proposed framework, Enriched
Functional Tree-Based Classifiers (EFTCs), leverages derivative and geometric
features, benefiting from the diversity inherent in ensemble methods to further
enhance predictive performance and reduce variance. While our approach has been
tested on the enrichment of Functional Classification Trees (FCTs), Functional
K-NN (FKNN), Functional Random Forest (FRF), Functional XGBoost (FXGB), and
Functional LightGBM (FLGBM), it could be extended to other tree-based and
non-tree-based classifiers, with appropriate considerations emerging from this
investigation. Through extensive experimental evaluations on seven real-world
datasets and six simulated scenarios, this proposal demonstrates fascinating
improvements over traditional approaches, providing new insights into the
application of FDA in complex, high-dimensional learning problems.",2024-09-26,"Fabrizio Maturo, Annamaria Porreca",http://arxiv.org/pdf/2409.17804v2,cs.LG
CASPFormer: Trajectory Prediction from BEV Images with Deformable Attention,"Motion prediction is an important aspect for Autonomous Driving (AD) and
Advance Driver Assistance Systems (ADAS). Current state-of-the-art motion
prediction methods rely on High Definition (HD) maps for capturing the
surrounding context of the ego vehicle. Such systems lack scalability in
real-world deployment as HD maps are expensive to produce and update in
real-time. To overcome this issue, we propose Context Aware Scene Prediction
Transformer (CASPFormer), which can perform multi-modal motion prediction from
rasterized Bird-Eye-View (BEV) images. Our system can be integrated with any
upstream perception module that is capable of generating BEV images. Moreover,
CASPFormer directly decodes vectorized trajectories without any postprocessing.
Trajectories are decoded recurrently using deformable attention, as it is
computationally efficient and provides the network with the ability to focus
its attention on the important spatial locations of the BEV images. In
addition, we also address the issue of mode collapse for generating multiple
scene-consistent trajectories by incorporating learnable mode queries. We
evaluate our model on the nuScenes dataset and show that it reaches
state-of-the-art across multiple metrics",2024-09-26,"Harsh Yadav, Maximilian Schaefer, Kun Zhao, Tobias Meisen",http://arxiv.org/pdf/2409.17790v1,cs.LG
Predicting the Stay Length of Patients in Hospitals using Convolutional Gated Recurrent Deep Learning Model,"Predicting hospital length of stay (LoS) stands as a critical factor in
shaping public health strategies. This data serves as a cornerstone for
governments to discern trends, patterns, and avenues for enhancing healthcare
delivery. In this study, we introduce a robust hybrid deep learning model, a
combination of Multi-layer Convolutional (CNNs) deep learning, Gated Recurrent
Units (GRU), and Dense neural networks, that outperforms 11 conventional and
state-of-the-art Machine Learning (ML) and Deep Learning (DL) methodologies in
accurately forecasting inpatient hospital stay duration. Our investigation
delves into the implementation of this hybrid model, scrutinising variables
like geographic indicators tied to caregiving institutions, demographic markers
encompassing patient ethnicity, race, and age, as well as medical attributes
such as the CCS diagnosis code, APR DRG code, illness severity metrics, and
hospital stay duration. Statistical evaluations reveal the pinnacle LoS
accuracy achieved by our proposed model (CNN-GRU-DNN), which averages at 89%
across a 10-fold cross-validation test, surpassing LSTM, BiLSTM, GRU, and
Convolutional Neural Networks (CNNs) by 19%, 18.2%, 18.6%, and 7%,
respectively. Accurate LoS predictions not only empower hospitals to optimise
resource allocation and curb expenses associated with prolonged stays but also
pave the way for novel strategies in hospital stay management. This avenue
holds promise for catalysing advancements in healthcare research and
innovation, inspiring a new era of precision-driven healthcare practices.",2024-09-26,"Mehdi Neshat, Michael Phipps, Chris A. Browne, Nicole T. Vargas, Seyedali Mirjalili",http://arxiv.org/pdf/2409.17786v1,cs.LG
Confidence intervals uncovered: Are we ready for real-world medical imaging AI?,"Medical imaging is spearheading the AI transformation of healthcare.
Performance reporting is key to determine which methods should be translated
into clinical practice. Frequently, broad conclusions are simply derived from
mean performance values. In this paper, we argue that this common practice is
often a misleading simplification as it ignores performance variability. Our
contribution is threefold. (1) Analyzing all MICCAI segmentation papers (n =
221) published in 2023, we first observe that more than 50% of papers do not
assess performance variability at all. Moreover, only one (0.5%) paper reported
confidence intervals (CIs) for model performance. (2) To address the reporting
bottleneck, we show that the unreported standard deviation (SD) in segmentation
papers can be approximated by a second-order polynomial function of the mean
Dice similarity coefficient (DSC). Based on external validation data from 56
previous MICCAI challenges, we demonstrate that this approximation can
accurately reconstruct the CI of a method using information provided in
publications. (3) Finally, we reconstructed 95% CIs around the mean DSC of
MICCAI 2023 segmentation papers. The median CI width was 0.03 which is three
times larger than the median performance gap between the first and second
ranked method. For more than 60% of papers, the mean performance of the
second-ranked method was within the CI of the first-ranked method. We conclude
that current publications typically do not provide sufficient evidence to
support which models could potentially be translated into clinical practice.",2024-09-26,"Evangelia Christodoulou, Annika Reinke, Rola Houhou, Piotr Kalinowski, Selen Erkan, Carole H. Sudre, Ninon Burgos, Sofiène Boutaj, Sophie Loizillon, Maëlys Solal, Nicola Rieke, Veronika Cheplygina, Michela Antonelli, Leon D. Mayer, Minu D. Tizabi, M. Jorge Cardoso, Amber Simpson, Paul F. Jäger, Annette Kopp-Schneider, Gaël Varoquaux, Olivier Colliot, Lena Maier-Hein",http://arxiv.org/pdf/2409.17763v2,cs.LG
Byzantine-Robust Aggregation for Securing Decentralized Federated Learning,"Federated Learning (FL) emerges as a distributed machine learning approach
that addresses privacy concerns by training AI models locally on devices.
Decentralized Federated Learning (DFL) extends the FL paradigm by eliminating
the central server, thereby enhancing scalability and robustness through the
avoidance of a single point of failure. However, DFL faces significant
challenges in optimizing security, as most Byzantine-robust algorithms proposed
in the literature are designed for centralized scenarios. In this paper, we
present a novel Byzantine-robust aggregation algorithm to enhance the security
of Decentralized Federated Learning environments, coined WFAgg. This proposal
handles the adverse conditions and strength robustness of dynamic decentralized
topologies at the same time by employing multiple filters to identify and
mitigate Byzantine attacks. Experimental results demonstrate the effectiveness
of the proposed algorithm in maintaining model accuracy and convergence in the
presence of various Byzantine attack scenarios, outperforming state-of-the-art
centralized Byzantine-robust aggregation schemes (such as Multi-Krum or
Clustering). These algorithms are evaluated on an IID image classification
problem in both centralized and decentralized scenarios.",2024-09-26,"Diego Cajaraville-Aboy, Ana Fernández-Vilas, Rebeca P. Díaz-Redondo, Manuel Fernández-Veiga",http://arxiv.org/pdf/2409.17754v1,cs.LG
Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model,"A supervised ranking model, despite its advantage of being effective, usually
involves complex processing - typically multiple stages of task-specific
pre-training and fine-tuning. This has motivated researchers to explore simpler
pipelines leveraging large language models (LLMs) that are capable of working
in a zero-shot manner. However, since zero-shot inference does not make use of
a training set of pairs of queries and their relevant documents, its
performance is mostly worse than that of supervised models, which are trained
on such example pairs. Motivated by the existing findings that training
examples generally improve zero-shot performance, in our work, we explore if
this also applies to ranking models. More specifically, given a query and a
pair of documents, the preference prediction task is improved by augmenting
examples of preferences for similar queries from a training set. Our proposed
pairwise few-shot ranker demonstrates consistent improvements over the
zero-shot baseline on both in-domain (TREC DL) and out-domain (BEIR subset)
retrieval benchmarks. Our method also achieves a close performance to that of a
supervised model without requiring any complex training pipeline.",2024-09-26,"Nilanjan Sinhababu, Andrew Parry, Debasis Ganguly, Debasis Samanta, Pabitra Mitra",http://arxiv.org/pdf/2409.17745v3,cs.LG
Autoregressive Generation Strategies for Top-K Sequential Recommendations,"The goal of modern sequential recommender systems is often formulated in
terms of next-item prediction. In this paper, we explore the applicability of
generative transformer-based models for the Top-K sequential recommendation
task, where the goal is to predict items a user is likely to interact with in
the ""near future"".
  We explore commonly used autoregressive generation strategies, including
greedy decoding, beam search, and temperature sampling, to evaluate their
performance for the Top-K sequential recommendation task. In addition, we
propose novel Reciprocal Rank Aggregation (RRA) and Relevance Aggregation (RA)
generation strategies based on multi-sequence generation with temperature
sampling and subsequent aggregation.
  Experiments on diverse datasets give valuable insights regarding commonly
used strategies' applicability and show that suggested approaches improve
performance on longer time horizons compared to widely-used Top-K prediction
approach and single-sequence autoregressive generation strategies.",2024-09-26,"Anna Volodkevich, Danil Gusak, Anton Klenitskiy, Alexey Vasilev",http://arxiv.org/pdf/2409.17730v1,cs.LG
Recent advances in interpretable machine learning using structure-based protein representations,"Recent advancements in machine learning (ML) are transforming the field of
structural biology. For example, AlphaFold, a groundbreaking neural network for
protein structure prediction, has been widely adopted by researchers. The
availability of easy-to-use interfaces and interpretable outcomes from the
neural network architecture, such as the confidence scores used to color the
predicted structures, have made AlphaFold accessible even to non-ML experts. In
this paper, we present various methods for representing protein 3D structures
from low- to high-resolution, and show how interpretable ML methods can support
tasks such as predicting protein structures, protein function, and
protein-protein interactions. This survey also emphasizes the significance of
interpreting and visualizing ML-based inference for structure-based protein
representations that enhance interpretability and knowledge discovery.
Developing such interpretable approaches promises to further accelerate fields
including drug development and protein design.",2024-09-26,"Luiz Felipe Vecchietti, Minji Lee, Begench Hangeldiyev, Hyunkyu Jung, Hahnbeom Park, Tae-Kyun Kim, Meeyoung Cha, Ho Min Kim",http://arxiv.org/pdf/2409.17726v1,cs.LG
QuForge: A Library for Qudits Simulation,"Quantum computing with qudits, an extension of qubits to multiple levels, is
a research field less mature than qubit-based quantum computing. However,
qudits can offer some advantages over qubits, by representing information with
fewer separated components. In this article, we present QuForge, a Python-based
library designed to simulate quantum circuits with qudits. This library
provides the necessary quantum gates for implementing quantum algorithms,
tailored to any chosen qudit dimension. Built on top of differentiable
frameworks, QuForge supports execution on accelerating devices such as GPUs and
TPUs, significantly speeding up simulations. It also supports sparse
operations, leading to a reduction in memory consumption compared to other
libraries. Additionally, by constructing quantum circuits as differentiable
graphs, QuForge facilitates the implementation of quantum machine learning
algorithms, enhancing the capabilities and flexibility of quantum computing
research.",2024-09-26,"Tiago de Souza Farias, Lucas Friedrich, Jonas Maziero",http://arxiv.org/pdf/2409.17716v1,cs.LG
Efficient Pointwise-Pairwise Learning-to-Rank for News Recommendation,"News recommendation is a challenging task that involves personalization based
on the interaction history and preferences of each user. Recent works have
leveraged the power of pretrained language models (PLMs) to directly rank news
items by using inference approaches that predominately fall into three
categories: pointwise, pairwise, and listwise learning-to-rank. While pointwise
methods offer linear inference complexity, they fail to capture crucial
comparative information between items that is more effective for ranking tasks.
Conversely, pairwise and listwise approaches excel at incorporating these
comparisons but suffer from practical limitations: pairwise approaches are
either computationally expensive or lack theoretical guarantees, and listwise
methods often perform poorly in practice. In this paper, we propose a novel
framework for PLM-based news recommendation that integrates both pointwise
relevance prediction and pairwise comparisons in a scalable manner. We present
a rigorous theoretical analysis of our framework, establishing conditions under
which our approach guarantees improved performance. Extensive experiments show
that our approach outperforms the state-of-the-art methods on the MIND and
Adressa news recommendation datasets.",2024-09-26,"Nithish Kannen, Yao Ma, Gerrit J. J. van den Burg, Jean Baptiste Faddoul",http://arxiv.org/pdf/2409.17711v1,cs.LG
Transfer Learning in $\ell_1$ Regularized Regression: Hyperparameter Selection Strategy based on Sharp Asymptotic Analysis,"Transfer learning techniques aim to leverage information from multiple
related datasets to enhance prediction quality against a target dataset. Such
methods have been adopted in the context of high-dimensional sparse regression,
and some Lasso-based algorithms have been invented: Trans-Lasso and Pretraining
Lasso are such examples. These algorithms require the statistician to select
hyperparameters that control the extent and type of information transfer from
related datasets. However, selection strategies for these hyperparameters, as
well as the impact of these choices on the algorithm's performance, have been
largely unexplored. To address this, we conduct a thorough, precise study of
the algorithm in a high-dimensional setting via an asymptotic analysis using
the replica method. Our approach reveals a surprisingly simple behavior of the
algorithm: Ignoring one of the two types of information transferred to the
fine-tuning stage has little effect on generalization performance, implying
that efforts for hyperparameter selection can be significantly reduced. Our
theoretical findings are also empirically supported by applications on
real-world and semi-artificial datasets using the IMDb and MNIST datasets,
respectively.",2024-09-26,"Koki Okajima, Tomoyuki Obuchi",http://arxiv.org/pdf/2409.17704v2,cs.LG
PGN: The RNN's New Successor is Effective for Long-Range Time Series Forecasting,"Due to the recurrent structure of RNN, the long information propagation path
poses limitations in capturing long-term dependencies, gradient
explosion/vanishing issues, and inefficient sequential execution. Based on
this, we propose a novel paradigm called Parallel Gated Network (PGN) as the
new successor to RNN. PGN directly captures information from previous time
steps through the designed Historical Information Extraction (HIE) layer and
leverages gated mechanisms to select and fuse it with the current time step
information. This reduces the information propagation path to $\mathcal{O}(1)$,
effectively addressing the limitations of RNN. To enhance PGN's performance in
long-range time series forecasting tasks, we propose a novel temporal modeling
framework called Temporal PGN (TPGN). TPGN incorporates two branches to
comprehensively capture the semantic information of time series. One branch
utilizes PGN to capture long-term periodic patterns while preserving their
local characteristics. The other branch employs patches to capture short-term
information and aggregate the global representation of the series. TPGN
achieves a theoretical complexity of $\mathcal{O}(\sqrt{L})$, ensuring
efficiency in its operations. Experimental results on five benchmark datasets
demonstrate the state-of-the-art (SOTA) performance and high efficiency of
TPGN, further confirming the effectiveness of PGN as the new successor to RNN
in long-range time series forecasting. The code is available in this
repository: \url{https://github.com/Water2sea/TPGN}.",2024-09-26,"Yuxin Jia, Youfang Lin, Jing Yu, Shuo Wang, Tianhao Liu, Huaiyu Wan",http://arxiv.org/pdf/2409.17703v1,cs.LG
"MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks","The proliferation of Large Language Models (LLMs) in diverse applications
underscores the pressing need for robust security measures to thwart potential
jailbreak attacks. These attacks exploit vulnerabilities within LLMs, endanger
data integrity and user privacy. Guardrails serve as crucial protective
mechanisms against such threats, but existing models often fall short in terms
of both detection accuracy, and computational efficiency. This paper advocates
for the significance of jailbreak attack prevention on LLMs, and emphasises the
role of input guardrails in safeguarding these models. We introduce MoJE
(Mixture of Jailbreak Expert), a novel guardrail architecture designed to
surpass current limitations in existing state-of-the-art guardrails. By
employing simple linguistic statistical techniques, MoJE excels in detecting
jailbreak attacks while maintaining minimal computational overhead during model
inference. Through rigorous experimentation, MoJE demonstrates superior
performance capable of detecting 90% of the attacks without compromising benign
prompts, enhancing LLMs security against jailbreak attacks.",2024-09-26,"Giandomenico Cornacchia, Giulio Zizzo, Kieran Fraser, Muhammad Zaid Hameed, Ambrish Rawat, Mark Purcell",http://arxiv.org/pdf/2409.17699v3,cs.LG
MIO: A Foundation Model on Multimodal Tokens,"In this paper, we introduce MIO, a novel foundation model built on multimodal
tokens, capable of understanding and generating speech, text, images, and
videos in an end-to-end, autoregressive manner. While the emergence of large
language models (LLMs) and multimodal large language models (MM-LLMs) propels
advancements in artificial general intelligence through their versatile
capabilities, they still lack true any-to-any understanding and generation.
Recently, the release of GPT-4o has showcased the remarkable potential of
any-to-any LLMs for complex real-world tasks, enabling omnidirectional input
and output across images, speech, and text. However, it is closed-source and
does not support the generation of multimodal interleaved sequences. To address
this gap, we present MIO, which is trained on a mixture of discrete tokens
across four modalities using causal multimodal modeling. MIO undergoes a
four-stage training process: (1) alignment pre-training, (2) interleaved
pre-training, (3) speech-enhanced pre-training, and (4) comprehensive
supervised fine-tuning on diverse textual, visual, and speech tasks. Our
experimental results indicate that MIO exhibits competitive, and in some cases
superior, performance compared to previous dual-modal baselines, any-to-any
model baselines, and even modality-specific baselines. Moreover, MIO
demonstrates advanced capabilities inherent to its any-to-any feature, such as
interleaved video-text generation, chain-of-visual-thought reasoning, visual
guideline generation, instructional image editing, etc.",2024-09-26,"Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang",http://arxiv.org/pdf/2409.17692v3,cs.LG
Efficient Bias Mitigation Without Privileged Information,"Deep neural networks trained via empirical risk minimisation often exhibit
significant performance disparities across groups, particularly when group and
task labels are spuriously correlated (e.g., ""grassy background"" and ""cows"").
Existing bias mitigation methods that aim to address this issue often either
rely on group labels for training or validation, or require an extensive
hyperparameter search. Such data and computational requirements hinder the
practical deployment of these methods, especially when datasets are too large
to be group-annotated, computational resources are limited, and models are
trained through already complex pipelines. In this paper, we propose Targeted
Augmentations for Bias Mitigation (TAB), a simple hyperparameter-free framework
that leverages the entire training history of a helper model to identify
spurious samples, and generate a group-balanced training set from which a
robust model can be trained. We show that TAB improves worst-group performance
without any group information or model selection, outperforming existing
methods while maintaining overall accuracy.",2024-09-26,"Mateo Espinosa Zarlenga, Swami Sankaranarayanan, Jerone T. A. Andrews, Zohreh Shams, Mateja Jamnik, Alice Xiang",http://arxiv.org/pdf/2409.17691v1,cs.LG
Graph Edit Distance with General Costs Using Neural Set Divergence,"Graph Edit Distance (GED) measures the (dis-)similarity between two given
graphs, in terms of the minimum-cost edit sequence that transforms one graph to
the other. However, the exact computation of GED is NP-Hard, which has recently
motivated the design of neural methods for GED estimation. However, they do not
explicitly account for edit operations with different costs. In response, we
propose GRAPHEDX, a neural GED estimator that can work with general costs
specified for the four edit operations, viz., edge deletion, edge addition,
node deletion and node addition. We first present GED as a quadratic assignment
problem (QAP) that incorporates these four costs. Then, we represent each graph
as a set of node and edge embeddings and use them to design a family of neural
set divergence surrogates. We replace the QAP terms corresponding to each
operation with their surrogates. Computing such neural set divergence require
aligning nodes and edges of the two graphs. We learn these alignments using a
Gumbel-Sinkhorn permutation generator, additionally ensuring that the node and
edge alignments are consistent with each other. Moreover, these alignments are
cognizant of both the presence and absence of edges between node-pairs.
Experiments on several datasets, under a variety of edit cost settings, show
that GRAPHEDX consistently outperforms state-of-the-art methods and heuristics
in terms of prediction error.",2024-09-26,"Eeshaan Jain, Indradyumna Roy, Saswat Meher, Soumen Chakrabarti, Abir De",http://arxiv.org/pdf/2409.17687v2,cs.LG
Feature-to-Image Data Augmentation: Improving Model Feature Extraction with Cluster-Guided Synthetic Samples,"One of the growing trends in machine learning is the use of data generation
techniques, since the performance of machine learning models is dependent on
the quantity of the training dataset. However, in many real-world applications,
particularly in medical and low-resource domains, collecting large datasets is
challenging due to resource constraints, which leads to overfitting and poor
generalization. This study introduces FICAug, a novel feature-to-image data
augmentation framework designed to improve model generalization under limited
data conditions by generating structured synthetic samples.
  FICAug first operates in the feature space, where original data are clustered
using the k-means algorithm. Within pure-label clusters, synthetic data are
generated through Gaussian sampling to increase diversity while maintaining
label consistency. These synthetic features are then projected back into the
image domain using a generative neural network, and a convolutional neural
network is trained on the reconstructed images to learn enhanced
representations.
  Experimental results demonstrate that FICAug significantly improves
classification accuracy. In feature space, it achieved a cross-validation
accuracy of 84.09%, while training a ResNet-18 model on the reconstructed
images further boosted performance to 88.63%, illustrating the effectiveness of
the proposed framework in extracting new and task-relevant features.",2024-09-26,"Yasaman Haghbin, Hadi Moradi, Reshad Hosseini",http://arxiv.org/pdf/2409.17685v2,cs.LG
Preserving logical and functional dependencies in synthetic tabular data,"Dependencies among attributes are a common aspect of tabular data. However,
whether existing tabular data generation algorithms preserve these dependencies
while generating synthetic data is yet to be explored. In addition to the
existing notion of functional dependencies, we introduce the notion of logical
dependencies among the attributes in this article. Moreover, we provide a
measure to quantify logical dependencies among attributes in tabular data.
Utilizing this measure, we compare several state-of-the-art synthetic data
generation algorithms and test their capability to preserve logical and
functional dependencies on several publicly available datasets. We demonstrate
that currently available synthetic tabular data generation algorithms do not
fully preserve functional dependencies when they generate synthetic datasets.
In addition, we also showed that some tabular synthetic data generation models
can preserve inter-attribute logical dependencies. Our review and comparison of
the state-of-the-art reveal research needs and opportunities to develop
task-specific synthetic tabular data generation models.",2024-09-26,"Chaithra Umesh, Kristian Schultz, Manjunath Mahendra, Saparshi Bej, Olaf Wolkenhauer",http://arxiv.org/pdf/2409.17684v1,cs.LG
On the Optimal Memorization Capacity of Transformers,"Recent research in the field of machine learning has increasingly focused on
the memorization capacity of Transformers, but how efficient they are is not
yet well understood. We demonstrate that Transformers can memorize labels with
$\tilde{O}(\sqrt{N})$ parameters in a next-token prediction setting for $N$
input sequences of length $n$, which is proved to be optimal up to logarithmic
factors. This indicates that Transformers can efficiently perform memorization
with little influence from the input length $n$ owing to the benefit of
parameter sharing. We also analyze the memorization capacity in the
sequence-to-sequence setting, and find that $\tilde{O}(\sqrt{nN})$ parameters
are not only sufficient, but also necessary at least for Transformers with
hardmax. These results suggest that while self-attention mechanisms can
efficiently identify input sequences, the feed-forward network becomes a
bottleneck when associating a label to each token.",2024-09-26,"Tokio Kajitsuka, Issei Sato",http://arxiv.org/pdf/2409.17677v2,cs.LG
Explanation Bottleneck Models,"Recent concept-based interpretable models have succeeded in providing
meaningful explanations by pre-defined concept sets. However, the dependency on
the pre-defined concepts restricts the application because of the limited
number of concepts for explanations. This paper proposes a novel interpretable
deep neural network called explanation bottleneck models (XBMs). XBMs generate
a text explanation from the input without pre-defined concepts and then predict
a final task prediction based on the generated explanation by leveraging
pre-trained vision-language encoder-decoder models. To achieve both the target
task performance and the explanation quality, we train XBMs through the target
task loss with the regularization penalizing the explanation decoder via the
distillation from the frozen pre-trained decoder. Our experiments, including a
comparison to state-of-the-art concept bottleneck models, confirm that XBMs
provide accurate and fluent natural language explanations without pre-defined
concept sets. Code is available at https://github.com/yshinya6/xbm/.",2024-09-26,"Shin'ya Yamaguchi, Kosuke Nishida",http://arxiv.org/pdf/2409.17663v3,cs.LG
Neural Contrast: Leveraging Generative Editing for Graphic Design Recommendations,"Creating visually appealing composites requires optimizing both text and
background for compatibility. Previous methods have focused on simple design
strategies, such as changing text color or adding background shapes for
contrast. These approaches are often destructive, altering text color or
partially obstructing the background image. Another method involves placing
design elements in non-salient and contrasting regions, but this isn't always
effective, especially with patterned backgrounds. To address these challenges,
we propose a generative approach using a diffusion model. This method ensures
the altered regions beneath design assets exhibit low saliency while enhancing
contrast, thereby improving the visibility of the design asset.",2024-09-26,"Marian Lupascu, Ionut Mironica, Mihai-Sorin Stupariu",http://arxiv.org/pdf/2410.07211v1,cs.LG
Efficient Fairness-Performance Pareto Front Computation,"There is a well known intrinsic trade-off between the fairness of a
representation and the performance of classifiers derived from the
representation. Due to the complexity of optimisation algorithms in most modern
representation learning approaches, for a given method it may be non-trivial to
decide whether the obtained fairness-performance curve of the method is
optimal, i.e., whether it is close to the true Pareto front for these
quantities for the underlying data distribution.
  In this paper we propose a new method to compute the optimal Pareto front,
which does not require the training of complex representation models. We show
that optimal fair representations possess several useful structural properties,
and that these properties enable a reduction of the computation of the Pareto
Front to a compact discrete problem. We then also show that these compact
approximating problems can be efficiently solved via off-the shelf
concave-convex programming methods.
  Since our approach is independent of the specific model of representations,
it may be used as the benchmark to which representation learning algorithms may
be compared. We experimentally evaluate the approach on a number of real world
benchmark datasets.",2024-09-26,"Mark Kozdoba, Binyamin Perets, Shie Mannor",http://arxiv.org/pdf/2409.17643v1,cs.LG
FlowMAC: Conditional Flow Matching for Audio Coding at Low Bit Rates,"This paper introduces FlowMAC, a novel neural audio codec for high-quality
general audio compression at low bit rates based on conditional flow matching
(CFM). FlowMAC jointly learns a mel spectrogram encoder, quantizer and decoder.
At inference time the decoder integrates a continuous normalizing flow via an
ODE solver to generate a high-quality mel spectrogram. This is the first time
that a CFM-based approach is applied to general audio coding, enabling a
scalable, simple and memory efficient training. Our subjective evaluations show
that FlowMAC at 3 kbps achieves similar quality as state-of-the-art GAN-based
and DDPM-based neural audio codecs at double the bit rate. Moreover, FlowMAC
offers a tunable inference pipeline, which permits to trade off complexity and
quality. This enables real-time coding on CPU, while maintaining high
perceptual quality.",2024-09-26,"Nicola Pia, Martin Strauss, Markus Multrus, Bernd Edler",http://arxiv.org/pdf/2409.17635v2,cs.LG
Least Squares and Marginal Log-Likelihood Model Predictive Control using Normalizing Flows,"Real-world (bio)chemical processes often exhibit stochastic dynamics with
non-trivial correlations and state-dependent fluctuations. Model predictive
control (MPC) often must consider these fluctuations to achieve reliable
performance. However, most process models simply add stationary noise terms to
a deterministic prediction. This work proposes using conditional normalizing
flows as discrete-time models to learn stochastic dynamics. Normalizing flows
learn the probability density function (PDF) of the states explicitly, given
prior states and control inputs. In addition to standard least squares (LSQ)
objectives, this work derives a marginal log-likelihood (MLL) objective based
on the explicit PDF and Markov chain simulations. In a reactor study, the
normalizing flow MPC reduces the setpoint error in open and closed-loop cases
to half that of a nominal controller. Furthermore, the chance constraints lead
to fewer constraint violations than the nominal controller. The MLL objective
yields slightly more stable results than the LSQ, particularly for small
scenario sets.",2024-09-26,Eike Cramer,http://arxiv.org/pdf/2409.17632v2,cs.LG
Convolutional Signal Propagation: A Simple Scalable Algorithm for Hypergraphs,"Last decade has seen the emergence of numerous methods for learning on
graphs, particularly Graph Neural Networks (GNNs). These methods, however, are
often not directly applicable to more complex structures like bipartite graphs
(equivalent to hypergraphs), which represent interactions among two entity
types (e.g. a user liking a movie). This paper proposes Convolutional Signal
Propagation (CSP), a non-parametric simple and scalable method that natively
operates on bipartite graphs (hypergraphs) and can be implemented with just a
few lines of code. After defining CSP, we demonstrate its relationship with
well-established methods like label propagation, Naive Bayes, and Hypergraph
Convolutional Networks. We evaluate CSP against several reference methods on
real-world datasets from multiple domains, focusing on retrieval and
classification tasks. Our results show that CSP offers competitive performance
while maintaining low computational complexity, making it an ideal first choice
as a baseline for hypergraph node classification and retrieval. Moreover,
despite operating on hypergraphs, CSP achieves good results in tasks typically
not associated with hypergraphs, such as natural language processing.",2024-09-26,"Pavel Procházka, Marek Dědič, Lukáš Bajer",http://arxiv.org/pdf/2409.17628v1,cs.LG
Benign Overfitting in Token Selection of Attention Mechanism,"Attention mechanism is a fundamental component of the transformer model and
plays a significant role in its success. However, the theoretical understanding
of how attention learns to select tokens is still an emerging area of research.
In this work, we study the training dynamics and generalization ability of the
attention mechanism under classification problems with label noise. We show
that, with the characterization of signal-to-noise ratio (SNR), the token
selection of attention mechanism achieves benign overfitting, i.e., maintaining
high generalization performance despite fitting label noise. Our work also
demonstrates an interesting delayed acquisition of generalization after an
initial phase of overfitting. Finally, we provide experiments to support our
theoretical analysis using both synthetic and real-world datasets.",2024-09-26,"Keitaro Sakamoto, Issei Sato",http://arxiv.org/pdf/2409.17625v3,cs.LG
Neural P$^3$M: A Long-Range Interaction Modeling Enhancer for Geometric GNNs,"Geometric graph neural networks (GNNs) have emerged as powerful tools for
modeling molecular geometry. However, they encounter limitations in effectively
capturing long-range interactions in large molecular systems. To address this
challenge, we introduce Neural P$^3$M, a versatile enhancer of geometric GNNs
to expand the scope of their capabilities by incorporating mesh points
alongside atoms and reimaging traditional mathematical operations in a
trainable manner. Neural P$^3$M exhibits flexibility across a wide range of
molecular systems and demonstrates remarkable accuracy in predicting energies
and forces, outperforming on benchmarks such as the MD22 dataset. It also
achieves an average improvement of 22% on the OE62 dataset while integrating
with various architectures.",2024-09-26,"Yusong Wang, Chaoran Cheng, Shaoning Li, Yuxuan Ren, Bin Shao, Ge Liu, Pheng-Ann Heng, Nanning Zheng",http://arxiv.org/pdf/2409.17622v1,cs.LG
A Survey of Spatio-Temporal EEG data Analysis: from Models to Applications,"In recent years, the field of electroencephalography (EEG) analysis has
witnessed remarkable advancements, driven by the integration of machine
learning and artificial intelligence. This survey aims to encapsulate the
latest developments, focusing on emerging methods and technologies that are
poised to transform our comprehension and interpretation of brain activity. We
delve into self-supervised learning methods that enable the robust
representation of brain signals, which are fundamental for a variety of
downstream applications. We also explore emerging discriminative methods,
including graph neural networks (GNN), foundation models, and large language
models (LLMs)-based approaches. Furthermore, we examine generative technologies
that harness EEG data to produce images or text, offering novel perspectives on
brain activity visualization and interpretation. The survey provides an
extensive overview of these cutting-edge techniques, their current
applications, and the profound implications they hold for future research and
clinical practice. The relevant literature and open-source materials have been
compiled and are consistently being refreshed at
\url{https://github.com/wpf535236337/LLMs4TS}",2024-09-26,"Pengfei Wang, Huanran Zheng, Silong Dai, Yiqiao Wang, Xiaotian Gu, Yuanbin Wu, Xiaoling Wang",http://arxiv.org/pdf/2410.08224v1,cs.LG
Diversity-Driven Synthesis: Enhancing Dataset Distillation through Directed Weight Adjustment,"The sharp increase in data-related expenses has motivated research into
condensing datasets while retaining the most informative features. Dataset
distillation has thus recently come to the fore. This paradigm generates
synthetic datasets that are representative enough to replace the original
dataset in training a neural network. To avoid redundancy in these synthetic
datasets, it is crucial that each element contains unique features and remains
diverse from others during the synthesis stage. In this paper, we provide a
thorough theoretical and empirical analysis of diversity within synthesized
datasets. We argue that enhancing diversity can improve the parallelizable yet
isolated synthesizing approach. Specifically, we introduce a novel method that
employs dynamic and directed weight adjustment techniques to modulate the
synthesis process, thereby maximizing the representativeness and diversity of
each synthetic instance. Our method ensures that each batch of synthetic data
mirrors the characteristics of a large, varying subset of the original dataset.
Extensive experiments across multiple datasets, including CIFAR, Tiny-ImageNet,
and ImageNet-1K, demonstrate the superior performance of our method,
highlighting its effectiveness in producing diverse and representative
synthetic datasets with minimal computational expense. Our code is available at
https://github.com/AngusDujw/Diversity-Driven-Synthesis.https://github.com/AngusDujw/Diversity-Driven-Synthesis.",2024-09-26,"Jiawei Du, Xin Zhang, Juncheng Hu, Wenxin Huang, Joey Tianyi Zhou",http://arxiv.org/pdf/2409.17612v3,cs.LG
Good Data Is All Imitation Learning Needs,"In this paper, we address the limitations of traditional teacher-student
models, imitation learning, and behaviour cloning in the context of
Autonomous/Automated Driving Systems (ADS), where these methods often struggle
with incomplete coverage of real-world scenarios. To enhance the robustness of
such models, we introduce the use of Counterfactual Explanations (CFEs) as a
novel data augmentation technique for end-to-end ADS. CFEs, by generating
training samples near decision boundaries through minimal input modifications,
lead to a more comprehensive representation of expert driver strategies,
particularly in safety-critical scenarios. This approach can therefore help
improve the model's ability to handle rare and challenging driving events, such
as anticipating darting out pedestrians, ultimately leading to safer and more
trustworthy decision-making for ADS. Our experiments in the CARLA simulator
demonstrate that CF-Driver outperforms the current state-of-the-art method,
achieving a higher driving score and lower infraction rates. Specifically,
CF-Driver attains a driving score of 84.2, surpassing the previous best model
by 15.02 percentage points. These results highlight the effectiveness of
incorporating CFEs in training end-to-end ADS. To foster further research, the
CF-Driver code is made publicly available.",2024-09-26,"Amir Samadi, Konstantinos Koufos, Kurt Debattista, Mehrdad Dianati",http://arxiv.org/pdf/2409.17605v1,cs.LG
RmGPT: Rotating Machinery Generative Pretrained Model,"In industry, the reliability of rotating machinery is critical for production
efficiency and safety. Current methods of Prognostics and Health Management
(PHM) often rely on task-specific models, which face significant challenges in
handling diverse datasets with varying signal characteristics, fault modes and
operating conditions. Inspired by advancements in generative pretrained models,
we propose RmGPT, a unified model for diagnosis and prognosis tasks. RmGPT
introduces a novel token-based framework, incorporating Signal Tokens, Prompt
Tokens, Time-Frequency Task Tokens and Fault Tokens to handle heterogeneous
data within a unified model architecture. We leverage self-supervised learning
for robust feature extraction and introduce a next signal token prediction
pretraining strategy, alongside efficient prompt learning for task-specific
adaptation. Extensive experiments demonstrate that RmGPT significantly
outperforms state-of-the-art algorithms, achieving near-perfect accuracy in
diagnosis tasks and exceptionally low errors in prognosis tasks. Notably, RmGPT
excels in few-shot learning scenarios, achieving 92% accuracy in 16-class
one-shot experiments, highlighting its adaptability and robustness. This work
establishes RmGPT as a powerful PHM foundation model for rotating machinery,
advancing the scalability and generalizability of PHM solutions.",2024-09-26,"Yilin Wang, Yifei Yu, Kong Sun, Peixuan Lei, Yuxuan Zhang, Enrico Zio, Aiguo Xia, Yuanxiang Li",http://arxiv.org/pdf/2409.17604v1,cs.LG
A novel application of Shapley values for large multidimensional time-series data: Applying explainable AI to a DNA profile classification neural network,"The application of Shapley values to high-dimensional, time-series-like data
is computationally challenging - and sometimes impossible. For $N$ inputs the
problem is $2^N$ hard. In image processing, clusters of pixels, referred to as
superpixels, are used to streamline computations. This research presents an
efficient solution for time-seres-like data that adapts the idea of superpixels
for Shapley value computation. Motivated by a forensic DNA classification
example, the method is applied to multivariate time-series-like data whose
features have been classified by a convolutional neural network (CNN). In DNA
processing, it is important to identify alleles from the background noise
created by DNA extraction and processing. A single DNA profile has $31,200$
scan points to classify, and the classification decisions must be defensible in
a court of law. This means that classification is routinely performed by human
readers - a monumental and time consuming process. The application of a CNN
with fast computation of meaningful Shapley values provides a potential
alternative to the classification. This research demonstrates the realistic,
accurate and fast computation of Shapley values for this massive task",2024-09-26,"Lauren Elborough, Duncan Taylor, Melissa Humphries",http://arxiv.org/pdf/2409.18156v1,cs.LG
Deep Manifold Part 1: Anatomy of Neural Network Manifold,"Based on the numerical manifold method principle, we developed a mathematical
framework of a neural network manifold: Deep Manifold and discovered that
neural networks: 1) is numerical computation combining forward and inverse; 2)
have near infinite degrees of freedom; 3) exponential learning capacity with
depth; 4) have self-progressing boundary conditions; 5) has training hidden
bottleneck. We also define two concepts: neural network learning space and deep
manifold space and introduce two concepts: neural network intrinsic pathway and
fixed point. We raise three fundamental questions: 1). What is the training
completion definition; 2). where is the deep learning convergence point (neural
network fixed point); 3). How important is token timestamp in training data
given negative time is critical in inverse problem.",2024-09-26,"Max Y. Ma, Gen-Hua Shi",http://arxiv.org/pdf/2409.17592v1,cs.LG
Conjugate Bayesian Two-step Change Point Detection for Hawkes Process,"The Bayesian two-step change point detection method is popular for the Hawkes
process due to its simplicity and intuitiveness. However, the non-conjugacy
between the point process likelihood and the prior requires most existing
Bayesian two-step change point detection methods to rely on non-conjugate
inference methods. These methods lack analytical expressions, leading to low
computational efficiency and impeding timely change point detection. To address
this issue, this work employs data augmentation to propose a conjugate Bayesian
two-step change point detection method for the Hawkes process, which proves to
be more accurate and efficient. Extensive experiments on both synthetic and
real data demonstrate the superior effectiveness and efficiency of our method
compared to baseline methods. Additionally, we conduct ablation studies to
explore the robustness of our method concerning various hyperparameters. Our
code is publicly available at https://github.com/Aurora2050/CoBay-CPD.",2024-09-26,"Zeyue Zhang, Xiaoling Lu, Feng Zhou",http://arxiv.org/pdf/2409.17591v4,cs.LG
Multimodal Banking Dataset: Understanding Client Needs through Event Sequences,"Financial organizations collect a huge amount of data about clients that
typically has a temporal (sequential) structure and is collected from various
sources (modalities). Due to privacy issues, there are no large-scale
open-source multimodal datasets of event sequences, which significantly limits
the research in this area. In this paper, we present the industrial-scale
publicly available multimodal banking dataset, MBD, that contains more than
1.5M corporate clients with several modalities: 950M bank transactions, 1B geo
position events, 5M embeddings of dialogues with technical support and monthly
aggregated purchases of four bank's products. All entries are properly
anonymized from real proprietary bank data. Using this dataset, we introduce a
novel benchmark with two business tasks: campaigning (purchase prediction in
the next month) and matching of clients. We provide numerical results that
demonstrate the superiority of our multi-modal baselines over single-modal
techniques for each task. As a result, the proposed dataset can open new
perspectives and facilitate the future development of practically important
large-scale multimodal algorithms for event sequences.
  HuggingFace Link: https://huggingface.co/datasets/ai-lab/MBD
  Github Link: https://github.com/Dzhambo/MBD",2024-09-26,"Mollaev Dzhambulat, Alexander Kostin, Postnova Maria, Ivan Karpukhin, Ivan A Kireev, Gleb Gusev, Andrey Savchenko",http://arxiv.org/pdf/2409.17587v1,cs.LG
Let the Quantum Creep In: Designing Quantum Neural Network Models by Gradually Swapping Out Classical Components,"Artificial Intelligence (AI), with its multiplier effect and wide
applications in multiple areas, could potentially be an important application
of quantum computing. Since modern AI systems are often built on neural
networks, the design of quantum neural networks becomes a key challenge in
integrating quantum computing into AI. To provide a more fine-grained
characterisation of the impact of quantum components on the performance of
neural networks, we propose a framework where classical neural network layers
are gradually replaced by quantum layers that have the same type of input and
output while keeping the flow of information between layers unchanged,
different from most current research in quantum neural network, which favours
an end-to-end quantum model. We start with a simple three-layer classical
neural network without any normalisation layers or activation functions, and
gradually change the classical layers to the corresponding quantum versions. We
conduct numerical experiments on image classification datasets such as the
MNIST, FashionMNIST and CIFAR-10 datasets to demonstrate the change of
performance brought by the systematic introduction of quantum components.
Through this framework, our research sheds new light on the design of future
quantum neural network models where it could be more favourable to search for
methods and frameworks that harness the advantages from both the classical and
quantum worlds.",2024-09-26,"Peiyong Wang, Casey. R. Myers, Lloyd C. L. Hollenberg, Udaya Parampalli",http://arxiv.org/pdf/2409.17583v1,cs.LG
Multiplicative Logit Adjustment Approximates Neural-Collapse-Aware Decision Boundary Adjustment,"Real-world data distributions are often highly skewed. This has spurred a
growing body of research on long-tailed recognition, aimed at addressing the
imbalance in training classification models. Among the methods studied,
multiplicative logit adjustment (MLA) stands out as a simple and effective
method. What theoretical foundation explains the effectiveness of this
heuristic method? We provide a justification for the effectiveness of MLA with
the following two-step process. First, we develop a theory that adjusts optimal
decision boundaries by estimating feature spread on the basis of neural
collapse. Second, we demonstrate that MLA approximates this optimal method.
Additionally, through experiments on long-tailed datasets, we illustrate the
practical usefulness of MLA under more realistic conditions. We also offer
experimental insights to guide the tuning of MLA hyperparameters.",2024-09-26,"Naoya Hasegawa, Issei Sato",http://arxiv.org/pdf/2409.17582v3,cs.LG
Derandomizing Multi-Distribution Learning,"Multi-distribution or collaborative learning involves learning a single
predictor that works well across multiple data distributions, using samples
from each during training. Recent research on multi-distribution learning,
focusing on binary loss and finite VC dimension classes, has shown near-optimal
sample complexity that is achieved with oracle efficient algorithms. That is,
these algorithms are computationally efficient given an efficient ERM for the
class. Unlike in classical PAC learning, where the optimal sample complexity is
achieved with deterministic predictors, current multi-distribution learning
algorithms output randomized predictors. This raises the question: can these
algorithms be derandomized to produce a deterministic predictor for multiple
distributions? Through a reduction to discrepancy minimization, we show that
derandomizing multi-distribution learning is computationally hard, even when
ERM is computationally efficient. On the positive side, we identify a
structural condition enabling an efficient black-box reduction, converting
existing randomized multi-distribution predictors into deterministic ones.",2024-09-26,"Kasper Green Larsen, Omar Montasser, Nikita Zhivotovskiy",http://arxiv.org/pdf/2409.17567v1,cs.LG
Pixel-Space Post-Training of Latent Diffusion Models,"Latent diffusion models (LDMs) have made significant advancements in the
field of image generation in recent years. One major advantage of LDMs is their
ability to operate in a compressed latent space, allowing for more efficient
training and deployment. However, despite these advantages, challenges with
LDMs still remain. For example, it has been observed that LDMs often generate
high-frequency details and complex compositions imperfectly. We hypothesize
that one reason for these flaws is due to the fact that all pre- and
post-training of LDMs are done in latent space, which is typically $8 \times 8$
lower spatial-resolution than the output images. To address this issue, we
propose adding pixel-space supervision in the post-training process to better
preserve high-frequency details. Experimentally, we show that adding a
pixel-space objective significantly improves both supervised quality
fine-tuning and preference-based post-training by a large margin on a
state-of-the-art DiT transformer and U-Net diffusion models in both visual
quality and visual flaw metrics, while maintaining the same text alignment
quality.",2024-09-26,"Christina Zhang, Simran Motwani, Matthew Yu, Ji Hou, Felix Juefei-Xu, Sam Tsai, Peter Vajda, Zijian He, Jialiang Wang",http://arxiv.org/pdf/2409.17565v1,cs.LG
Joint Source-Channel Coding: Fundamentals and Recent Progress in Practical Designs,"Semantic- and task-oriented communication has emerged as a promising approach
to reducing the latency and bandwidth requirements of next-generation mobile
networks by transmitting only the most relevant information needed to complete
a specific task at the receiver. This is particularly advantageous for
machine-oriented communication of high data rate content, such as images and
videos, where the goal is rapid and accurate inference, rather than perfect
signal reconstruction. While semantic- and task-oriented compression can be
implemented in conventional communication systems, joint source-channel coding
(JSCC) offers an alternative end-to-end approach by optimizing compression and
channel coding together, or even directly mapping the source signal to the
modulated waveform. Although all digital communication systems today rely on
separation, thanks to its modularity, JSCC is known to achieve higher
performance in finite blocklength scenarios, and to avoid cliff and the
levelling-off effects in time-varying channel scenarios. This article provides
an overview of the information theoretic foundations of JSCC, surveys practical
JSCC designs over the decades, and discusses the reasons for their limited
adoption in practical systems. We then examine the recent resurgence of JSCC,
driven by the integration of deep learning techniques, particularly through
DeepJSCC, highlighting its many surprising advantages in various scenarios.
Finally, we discuss why it may be time to reconsider today's strictly separate
architectures, and reintroduce JSCC to enable high-fidelity, low-latency
communications in critical applications such as autonomous driving, drone
surveillance, or wearable systems.",2024-09-26,"Deniz Gündüz, Michèle A. Wigger, Tze-Yang Tung, Ping Zhang, Yong Xiao",http://arxiv.org/pdf/2409.17557v1,cs.LG
Advancing Open-Set Domain Generalization Using Evidential Bi-Level Hardest Domain Scheduler,"In Open-Set Domain Generalization (OSDG), the model is exposed to both new
variations of data appearance (domains) and open-set conditions, where both
known and novel categories are present at test time. The challenges of this
task arise from the dual need to generalize across diverse domains and
accurately quantify category novelty, which is critical for applications in
dynamic environments. Recently, meta-learning techniques have demonstrated
superior results in OSDG, effectively orchestrating the meta-train and -test
tasks by employing varied random categories and predefined domain partition
strategies. These approaches prioritize a well-designed training schedule over
traditional methods that focus primarily on data augmentation and the
enhancement of discriminative feature learning. The prevailing meta-learning
models in OSDG typically utilize a predefined sequential domain scheduler to
structure data partitions. However, a crucial aspect that remains inadequately
explored is the influence brought by strategies of domain schedulers during
training. In this paper, we observe that an adaptive domain scheduler benefits
more in OSDG compared with prefixed sequential and random domain schedulers. We
propose the Evidential Bi-Level Hardest Domain Scheduler (EBiL-HaDS) to achieve
an adaptive domain scheduler. This method strategically sequences domains by
assessing their reliabilities in utilizing a follower network, trained with
confidence scores learned in an evidential manner, regularized by max rebiasing
discrepancy, and optimized in a bi-level manner. The results show that our
method substantially improves OSDG performance and achieves more discriminative
embeddings for both the seen and unseen categories. The source code is publicly
available at https://github.com/KPeng9510/EBiL-HaDS.",2024-09-26,"Kunyu Peng, Di Wen, Kailun Yang, Ao Luo, Yufan Chen, Jia Fu, M. Saquib Sarfraz, Alina Roitberg, Rainer Stiefelhagen",http://arxiv.org/pdf/2409.17555v2,cs.LG
A Simple but Strong Baseline for Sounding Video Generation: Effective Adaptation of Audio and Video Diffusion Models for Joint Generation,"In this work, we build a simple but strong baseline for sounding video
generation. Given base diffusion models for audio and video, we integrate them
with additional modules into a single model and train it to make the model
jointly generate audio and video. To enhance alignment between audio-video
pairs, we introduce two novel mechanisms in our model. The first one is
timestep adjustment, which provides different timestep information to each base
model. It is designed to align how samples are generated along with timesteps
across modalities. The second one is a new design of the additional modules,
termed Cross-Modal Conditioning as Positional Encoding (CMC-PE). In CMC-PE,
cross-modal information is embedded as if it represents temporal position
information, and the embeddings are fed into the model like positional
encoding. Compared with the popular cross-attention mechanism, CMC-PE provides
a better inductive bias for temporal alignment in the generated data.
Experimental results validate the effectiveness of the two newly introduced
mechanisms and also demonstrate that our method outperforms existing methods.",2024-09-26,"Masato Ishii, Akio Hayakawa, Takashi Shibuya, Yuki Mitsufuji",http://arxiv.org/pdf/2409.17550v3,cs.LG
Heuristics and Biases in AI Decision-Making: Implications for Responsible AGI,"We investigate the presence of cognitive biases in three large language
models (LLMs): GPT-4o, Gemma 2, and Llama 3.1. The study uses 1,500 experiments
across nine established cognitive biases to evaluate the models' responses and
consistency. GPT-4o demonstrated the strongest overall performance. Gemma 2
showed strengths in addressing the sunk cost fallacy and prospect theory,
however its performance varied across different biases. Llama 3.1 consistently
underperformed, relying on heuristics and exhibiting frequent inconsistencies
and contradictions. The findings highlight the challenges of achieving robust
and generalizable reasoning in LLMs, and underscore the need for further
development to mitigate biases in artificial general intelligence (AGI). The
study emphasizes the importance of integrating statistical reasoning and
ethical considerations in future AI development.",2024-09-26,"Payam Saeedi, Mahsa Goodarzi, M Abdullah Canbaz",http://arxiv.org/pdf/2410.02820v3,cs.LG
MASSFormer: Mobility-Aware Spectrum Sensing using Transformer-Driven Tiered Structure,"In this paper, we develop a novel mobility-aware transformer-driven tiered
structure (MASSFormer) based cooperative spectrum sensing method that
effectively models the spatio-temporal dynamics of user movements. Unlike
existing methods, our method considers a dynamic scenario involving mobile
primary users (PUs) and secondary users (SUs)and addresses the complexities
introduced by user mobility. The transformer architecture utilizes an attention
mechanism, enabling the proposed method to adeptly model the temporal dynamics
of user mobility by effectively capturing long-range dependencies within the
input data. The proposed method first computes tokens from the sequence of
covariance matrices (CMs) for each SU and processes them in parallel using the
SUtransformer network to learn the spatio-temporal features at SUlevel.
Subsequently, the collaborative transformer network learns the group-level PU
state from all SU-level feature representations. The attention-based sequence
pooling method followed by the transformer encoder adjusts the contributions of
all tokens. The main goal of predicting the PU states at each SU-level and
group-level is to improve detection performance even more. We conducted a
sufficient amount of simulations and compared the detection performance of
different SS methods. The proposed method is tested under imperfect reporting
channel scenarios to show robustness. The efficacy of our method is validated
with the simulation results demonstrating its higher performance compared with
existing methods in terms of detection probability, sensing error, and
classification accuracy.",2024-09-26,"Dimpal Janu, Sandeep Mandia, Kuldeep Singh, Sandeep Kumar",http://arxiv.org/pdf/2409.17546v1,cs.LG
"Modulated Intervention Preference Optimization (MIPO): Keep the Easy, Refine the Difficult","Preference optimization methods typically begin training with a well-trained
SFT model as a reference model. In RLHF and DPO, a regularization term is used
during the preference optimization process to prevent the policy model from
deviating too far from the reference model's distribution, thereby avoiding the
generation of anomalous responses. When the reference model is already
well-aligned with the given data or only requires slight adjustments, this
approach can produce a well-aligned model. However, if the reference model is
not aligned with the given data and requires significant deviation from its
current state, a regularization term may actually hinder the model alignment.
In this study, we propose \textbf{Modulated Intervention Preference
Optimization (MIPO)} to address this issue. MIPO modulates the degree of
intervention from the reference model based on how well the given data is
aligned with it. If the data is well-aligned, the intervention is increased to
prevent the policy model from diverging significantly from reference model.
Conversely, if the alignment is poor, the interference is reduced to facilitate
more extensive training. We compare the performance of MIPO and DPO using
Mistral-7B and Llama3-8B in Alpaca Eval 2.0 and MT-Bench. The experimental
results demonstrate that MIPO consistently outperforms DPO across various
evaluation scenarios.",2024-09-26,Cheolhun Jang,http://arxiv.org/pdf/2409.17545v2,cs.LG
Optimizing the Induced Correlation in Omnibus Joint Graph Embeddings,"Theoretical and empirical evidence suggests that joint graph embedding
algorithms induce correlation across the networks in the embedding space. In
the Omnibus joint graph embedding framework, previous results explicitly
delineated the dual effects of the algorithm-induced and model-inherent
correlations on the correlation across the embedded networks. Accounting for
and mitigating the algorithm-induced correlation is key to subsequent
inference, as sub-optimal Omnibus matrix constructions have been demonstrated
to lead to loss in inference fidelity. This work presents the first efforts to
automate the Omnibus construction in order to address two key questions in this
joint embedding framework: the correlation-to-OMNI problem and the flat
correlation problem. In the flat correlation problem, we seek to understand the
minimum algorithm-induced flat correlation (i.e., the same across all graph
pairs) produced by a generalized Omnibus embedding. Working in a subspace of
the fully general Omnibus matrices, we prove both a lower bound for this flat
correlation and that the classical Omnibus construction induces the maximal
flat correlation. In the correlation-to-OMNI problem, we present an algorithm
-- named corr2Omni -- that, from a given matrix of estimated pairwise graph
correlations, estimates the matrix of generalized Omnibus weights that induces
optimal correlation in the embedding space. Moreover, in both simulated and
real data settings, we demonstrate the increased effectiveness of our corr2Omni
algorithm versus the classical Omnibus construction.",2024-09-26,"Konstantinos Pantazis, Michael Trosset, William N. Frost, Carey E. Priebe, Vince Lyzinski",http://arxiv.org/pdf/2409.17544v2,cs.LG
On the Implicit Relation Between Low-Rank Adaptation and Differential Privacy,"A significant approach in natural language processing involves large-scale
pre-training of models on general domain data followed by their adaptation to
specific tasks or domains. As models grow in size, full fine-tuning all of
their parameters becomes increasingly impractical. To address this, some
methods for low-rank task adaptation of language models have been proposed,
e.g., LoRA and FLoRA. These methods keep the pre-trained model weights fixed
and incorporate trainable low-rank decomposition matrices into some layers of
the transformer architecture, called adapters. This approach significantly
reduces the number of trainable parameters required for downstream tasks
compared to full fine-tuning all parameters. In this work, we look at low-rank
adaptation from the lens of data privacy. We show theoretically that the
low-rank adaptation used in LoRA and FLoRA leads to the injection of some
random noise into the batch gradients w.r.t the adapter parameters. We quantify
the variance of the injected noise and show that the smaller the adaptation
rank, the larger the noise variance. By establishing a Berry-Esseen type bound
on the total variation distance between distribution of the injected noise and
a Gaussian distribution with the same variance, we show that the dynamics of
low-rank adaptation is close to that of differentially private fine-tuning of
the adapters. Finally, using Johnson-Lindenstrauss lemma, we show that when
augmented with gradient scaling, low-rank adaptation is very close to
performing DPSGD algorithm with a fixed noise scale to fine-tune the adapters.
Suggested by our theoretical findings and approved by our experimental results,
we show that low-rank adaptation, besides mitigating the space and
computational complexities, implicitly provides a privacy protection w.r.t the
fine-tuning data, without inducing the high space complexity of DPSGD.",2024-09-26,"Saber Malekmohammadi, Golnoosh Farnadi",http://arxiv.org/pdf/2409.17538v5,cs.LG
Dataset Distillation-based Hybrid Federated Learning on Non-IID Data,"In federated learning, the heterogeneity of client data has a great impact on
the performance of model training. Many heterogeneity issues in this process
are raised by non-independently and identically distributed (Non-IID) data.
This study focuses on the issue of label distribution skew. To address it, we
propose a hybrid federated learning framework called HFLDD, which integrates
dataset distillation to generate approximately independent and equally
distributed (IID) data, thereby improving the performance of model training.
Particularly, we partition the clients into heterogeneous clusters, where the
data labels among different clients within a cluster are unbalanced while the
data labels among different clusters are balanced. The cluster headers collect
distilled data from the corresponding cluster members, and conduct model
training in collaboration with the server. This training process is like
traditional federated learning on IID data, and hence effectively alleviates
the impact of Non-IID data on model training. Furthermore, we compare our
proposed method with typical baseline methods on public datasets. Experimental
results demonstrate that when the data labels are severely imbalanced, the
proposed HFLDD outperforms the baseline methods in terms of both test accuracy
and communication cost.",2024-09-26,"Xiufang Shi, Wei Zhang, Mincheng Wu, Guangyi Liu, Zhenyu Wen, Shibo He, Tejal Shah, Rajiv Ranjan",http://arxiv.org/pdf/2409.17517v1,cs.LG
Functional Classification of Spiking Signal Data Using Artificial Intelligence Techniques: A Review,"Human brain neuron activities are incredibly significant nowadays. Neuronal
behavior is assessed by analyzing signal data such as electroencephalography
(EEG), which can offer scientists valuable information about diseases and
human-computer interaction. One of the difficulties researchers confront while
evaluating these signals is the existence of large volumes of spike data.
Spikes are some considerable parts of signal data that can happen as a
consequence of vital biomarkers or physical issues such as electrode movements.
Hence, distinguishing types of spikes is important. From this spot, the spike
classification concept commences. Previously, researchers classified spikes
manually. The manual classification was not precise enough as it involves
extensive analysis. Consequently, Artificial Intelligence (AI) was introduced
into neuroscience to assist clinicians in classifying spikes correctly. This
review discusses the importance and use of AI in spike classification, focusing
on the recognition of neural activity noises. The task is divided into three
main components: preprocessing, classification, and evaluation. Existing
methods are introduced and their importance is determined. The review also
highlights the need for more efficient algorithms. The primary goal is to
provide a perspective on spike classification for future research and provide a
comprehensive understanding of the methodologies and issues involved. The
review organizes materials in the spike classification field for future
studies. In this work, numerous studies were extracted from different
databases. The PRISMA-related research guidelines were then used to choose
papers. Then, research studies based on spike classification using machine
learning and deep learning approaches with effective preprocessing were
selected.",2024-09-26,"Danial Sharifrazi, Nouman Javed, Javad Hassannataj Joloudari, Roohallah Alizadehsani, Prasad N. Paradkar, Ru-San Tan, U. Rajendra Acharya, Asim Bhatti",http://arxiv.org/pdf/2409.17516v1,cs.LG
"Comparing Unidirectional, Bidirectional, and Word2vec Models for Discovering Vulnerabilities in Compiled Lifted Code","Ransomware and other forms of malware cause significant financial and
operational damage to organizations by exploiting long-standing and often
difficult-to-detect software vulnerabilities. To detect vulnerabilities such as
buffer overflows in compiled code, this research investigates the application
of unidirectional transformer-based embeddings, specifically GPT-2. Using a
dataset of LLVM functions, we trained a GPT-2 model to generate embeddings,
which were subsequently used to build LSTM neural networks to differentiate
between vulnerable and non-vulnerable code. Our study reveals that embeddings
from the GPT-2 model significantly outperform those from bidirectional models
of BERT and RoBERTa, achieving an accuracy of 92.5% and an F1-score of 89.7%.
LSTM neural networks were developed with both frozen and unfrozen embedding
model layers. The model with the highest performance was achieved when the
embedding layers were unfrozen. Further, the research finds that, in exploring
the impact of different optimizers within this domain, the SGD optimizer
demonstrates superior performance over Adam. Overall, these findings reveal
important insights into the potential of unidirectional transformer-based
approaches in enhancing cybersecurity defenses.",2024-09-26,"Gary A. McCully, John D. Hastings, Shengjie Xu, Adam Fortier",http://arxiv.org/pdf/2409.17513v2,cs.LG
Variational Source-Channel Coding for Semantic Communication,"Semantic communication technology emerges as a pivotal bridge connecting AI
with classical communication. The current semantic communication systems are
generally modeled as an Auto-Encoder (AE). AE lacks a deep integration of AI
principles with communication strategies due to its inability to effectively
capture channel dynamics. This gap makes it difficult to justify the need for
joint source-channel coding (JSCC) and to explain why performance improves.
This paper begins by exploring lossless and lossy communication, highlighting
that the inclusion of data distortion distinguishes semantic communication from
classical communication. It breaks the conditions for the separation theorem to
hold and explains why the amount of data transferred by semantic communication
is less. Therefore, employing JSCC becomes imperative for achieving optimal
semantic communication. Moreover, a Variational Source-Channel Coding (VSCC)
method is proposed for constructing semantic communication systems based on
data distortion theory, integrating variational inference and channel
characteristics. Using a deep learning network, we develop a semantic
communication system employing the VSCC method and demonstrate its capability
for semantic transmission. We also establish semantic communication systems of
equivalent complexity employing the AE method and the VAE method. Experimental
results reveal that the VSCC model offers superior interpretability compared to
AE model, as it clearly captures the semantic features of the transmitted data,
represented as the variance of latent variables in our experiments. In
addition, VSCC model exhibits superior semantic transmission capabilities
compared to VAE model. At the same level of data distortion evaluated by PSNR,
VSCC model exhibits stronger human interpretability, which can be partially
assessed by SSIM.",2024-09-26,"Yulong Feng, Jing Xu, Liujun Hu, Guanghui Yu, Xiangyang Duan",http://arxiv.org/pdf/2410.08222v3,cs.LG
NeuroPath: A Neural Pathway Transformer for Joining the Dots of Human Connectomes,"Although modern imaging technologies allow us to study connectivity between
two distinct brain regions in-vivo, an in-depth understanding of how anatomical
structure supports brain function and how spontaneous functional fluctuations
emerge remarkable cognition is still elusive. Meanwhile, tremendous efforts
have been made in the realm of machine learning to establish the nonlinear
mapping between neuroimaging data and phenotypic traits. However, the absence
of neuroscience insight in the current approaches poses significant challenges
in understanding cognitive behavior from transient neural activities. To
address this challenge, we put the spotlight on the coupling mechanism of
structural connectivity (SC) and functional connectivity (FC) by formulating
such network neuroscience question into an expressive graph representation
learning problem for high-order topology. Specifically, we introduce the
concept of topological detour to characterize how a ubiquitous instance of FC
(direct link) is supported by neural pathways (detour) physically wired by SC,
which forms a cyclic loop interacted by brain structure and function. In the
clich\'e of machine learning, the multi-hop detour pathway underlying SC-FC
coupling allows us to devise a novel multi-head self-attention mechanism within
Transformer to capture multi-modal feature representation from paired graphs of
SC and FC. Taken together, we propose a biological-inspired deep model, coined
as NeuroPath, to find putative connectomic feature representations from the
unprecedented amount of neuroimages, which can be plugged into various
downstream applications such as task recognition and disease diagnosis. We have
evaluated NeuroPath on large-scale public datasets including HCP and UK Biobank
under supervised and zero-shot learning, where the state-of-the-art performance
by our NeuroPath indicates great potential in network neuroscience.",2024-09-26,"Ziquan Wei, Tingting Dan, Jiaqi Ding, Guorong Wu",http://arxiv.org/pdf/2409.17510v3,cs.LG
Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE,"Multi-modal large language models (MLLMs) have shown impressive capabilities
as a general-purpose interface for various visual and linguistic tasks.
However, building a unified MLLM for multi-task learning in the medical field
remains a thorny challenge. To mitigate the tug-of-war problem of multi-modal
multi-task optimization in MLLMs, recent advances primarily focus on improving
the LLM components, while neglecting the connector that bridges the gap between
modalities. In this paper, we introduce Uni-Med, a novel medical generalist
foundation model which consists of a universal visual feature extraction
module, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting
from the proposed CMoE that leverages a well-designed router with a mixture of
projection experts at the connector, Uni-Med achieves efficient solution to the
tug-of-war problem and can perform six different medical tasks including
question answering, visual question answering, report generation, referring
expression comprehension, referring expression generation and image
classification. To the best of our knowledge, Uni-Med is the first effort to
tackle multi-task interference at the connector in MLLMs. Extensive ablation
experiments validate the effectiveness of introducing CMoE under any
configuration, with up to an average 8% performance gains. We further provide
interpretation analysis of the tug-of-war problem from the perspective of
gradient optimization and parameter statistics. Compared to previous
state-of-the-art medical MLLMs, Uni-Med achieves competitive or superior
evaluation metrics on diverse tasks. Code and resources are available at
https://github.com/tsinghua-msiip/Uni-Med.",2024-09-26,"Xun Zhu, Ying Hu, Fanbin Mo, Miao Li, Ji Wu",http://arxiv.org/pdf/2409.17508v2,cs.LG
Mamba Meets Financial Markets: A Graph-Mamba Approach for Stock Price Prediction,"Stock markets play an important role in the global economy, where accurate
stock price predictions can lead to significant financial returns. While
existing transformer-based models have outperformed long short-term memory
networks and convolutional neural networks in financial time series prediction,
their high computational complexity and memory requirements limit their
practicality for real-time trading and long-sequence data processing. To
address these challenges, we propose SAMBA, an innovative framework for stock
return prediction that builds on the Mamba architecture and integrates graph
neural networks. SAMBA achieves near-linear computational complexity by
utilizing a bidirectional Mamba block to capture long-term dependencies in
historical price data and employing adaptive graph convolution to model
dependencies between daily stock features. Our experimental results demonstrate
that SAMBA significantly outperforms state-of-the-art baseline models in
prediction accuracy, maintaining low computational complexity. The code and
datasets are available at github.com/Ali-Meh619/SAMBA.",2024-09-26,"Ali Mehrabian, Ehsan Hoseinzade, Mahdi Mazloum, Xiaohong Chen",http://arxiv.org/pdf/2410.03707v2,cs.LG
Sequential Kernelized Stein Discrepancy,"We present a sequential version of the kernelized Stein discrepancy
goodness-of-fit test, which allows for conducting goodness-of-fit tests for
unnormalized densities that are continuously monitored and adaptively stopped.
That is, the sample size need not be fixed prior to data collection; the
practitioner can choose whether to stop the test or continue to gather evidence
at any time while controlling the false discovery rate. In stark contrast to
related literature, we do not impose uniform boundedness on the Stein kernel.
Instead, we exploit the potential boundedness of the Stein kernel at arbitrary
point evaluations to define test martingales, that give way to the subsequent
novel sequential tests. We prove the validity of the test, as well as an
asymptotic lower bound for the logarithmic growth of the wealth process under
the alternative. We further illustrate the empirical performance of the test
with a variety of distributions, including restricted Boltzmann machines.",2024-09-26,"Diego Martinez-Taboada, Aaditya Ramdas",http://arxiv.org/pdf/2409.17505v2,cs.LG
HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection,"The surge in applications of large language models (LLMs) has prompted
concerns about the generation of misleading or fabricated information, known as
hallucinations. Therefore, detecting hallucinations has become critical to
maintaining trust in LLM-generated content. A primary challenge in learning a
truthfulness classifier is the lack of a large amount of labeled truthful and
hallucinated data. To address the challenge, we introduce HaloScope, a novel
learning framework that leverages the unlabeled LLM generations in the wild for
hallucination detection. Such unlabeled data arises freely upon deploying LLMs
in the open world, and consists of both truthful and hallucinated information.
To harness the unlabeled data, we present an automated membership estimation
score for distinguishing between truthful and untruthful generations within
unlabeled mixture data, thereby enabling the training of a binary truthfulness
classifier on top. Importantly, our framework does not require extra data
collection and human annotations, offering strong flexibility and practicality
for real-world applications. Extensive experiments show that HaloScope can
achieve superior hallucination detection performance, outperforming the
competitive rivals by a significant margin. Code is available at
https://github.com/deeplearningwisc/haloscope.",2024-09-26,"Xuefeng Du, Chaowei Xiao, Yixuan Li",http://arxiv.org/pdf/2409.17504v1,cs.LG
Broadcast Product: Shape-aligned Element-wise Multiplication and Beyond,"We propose a new operator defined between two tensors, the broadcast product.
The broadcast product calculates the Hadamard product after duplicating
elements to align the shapes of the two tensors. Complex tensor operations in
libraries like \texttt{numpy} can be succinctly represented as mathematical
expressions using the broadcast product. Finally, we propose a novel tensor
decomposition using the broadcast product, highlighting its potential
applications in dimensionality reduction.",2024-09-26,"Yusuke Matsui, Tatsuya Yokota",http://arxiv.org/pdf/2409.17502v1,cs.LG
MemFusionMap: Working Memory Fusion for Online Vectorized HD Map Construction,"High-definition (HD) maps provide environmental information for autonomous
driving systems and are essential for safe planning. While existing methods
with single-frame input achieve impressive performance for online vectorized HD
map construction, they still struggle with complex scenarios and occlusions. We
propose MemFusionMap, a novel temporal fusion model with enhanced temporal
reasoning capabilities for online HD map construction. Specifically, we
contribute a working memory fusion module that improves the model's memory
capacity to reason across a history of frames. We also design a novel temporal
overlap heatmap to explicitly inform the model about the temporal overlap
information and vehicle trajectory in the Bird's Eye View space. By integrating
these two designs, MemFusionMap significantly outperforms existing methods
while also maintaining a versatile design for scalability. We conduct extensive
evaluation on open-source benchmarks and demonstrate a maximum improvement of
5.4% in mAP over state-of-the-art methods. The project page for MemFusionMap is
https://song-jingyu.github.io/MemFusionMap",2024-09-26,"Jingyu Song, Xudong Chen, Liupei Lu, Jie Li, Katherine A. Skinner",http://arxiv.org/pdf/2409.18737v2,cs.LG
Does Worst-Performing Agent Lead the Pack? Analyzing Agent Dynamics in Unified Distributed SGD,"Distributed learning is essential to train machine learning algorithms across
heterogeneous agents while maintaining data privacy. We conduct an asymptotic
analysis of Unified Distributed SGD (UD-SGD), exploring a variety of
communication patterns, including decentralized SGD and local SGD within
Federated Learning (FL), as well as the increasing communication interval in
the FL setting. In this study, we assess how different sampling strategies,
such as i.i.d. sampling, shuffling, and Markovian sampling, affect the
convergence speed of UD-SGD by considering the impact of agent dynamics on the
limiting covariance matrix as described in the Central Limit Theorem (CLT). Our
findings not only support existing theories on linear speedup and asymptotic
network independence, but also theoretically and empirically show how efficient
sampling strategies employed by individual agents contribute to overall
convergence in UD-SGD. Simulations reveal that a few agents using highly
efficient sampling can achieve or surpass the performance of the majority
employing moderately improved strategies, providing new insights beyond
traditional analyses focusing on the worst-performing agent.",2024-09-26,"Jie Hu, Yi-Ting Ma, Do Young Eun",http://arxiv.org/pdf/2409.17499v2,cs.LG
MathDSL: A Domain-Specific Language for Concise Mathematical Solutions Via Program Synthesis,"We present MathDSL, a Domain-Specific Language (DSL) for mathematical
equation solving, which, when deployed in program synthesis models, outperforms
state-of-the-art reinforcement-learning-based methods. We also introduce a
quantitative metric for measuring the conciseness of a mathematical solution
and demonstrate the improvement in the quality of generated solutions compared
to other methods. Our system demonstrates that a program synthesis system
(DreamCoder) using MathDSL can generate programs that solve linear equations
with greater accuracy and conciseness than using reinforcement learning
systems. Additionally, we demonstrate that if we use the action spaces of
previous reinforcement learning systems as DSLs, MathDSL outperforms the
action-space-DSLs. We use DreamCoder to store equation-solving strategies as
learned abstractions in its program library and demonstrate that by using
MathDSL, these can be converted into human-interpretable solution strategies
that could have applications in mathematical education.",2024-09-26,"Sagnik Anupam, Maddy Bowers, Omar Costilla-Reyes, Armando Solar-Lezama",http://arxiv.org/pdf/2409.17490v3,cs.LG
Heterogeneous Hyper-Graph Neural Networks for Context-aware Human Activity Recognition,"Context-aware Human Activity Recognition (CHAR) is challenging due to the
need to recognize the user's current activity from signals that vary
significantly with contextual factors such as phone placements and the varied
styles with which different users perform the same activity. In this paper, we
argue that context-aware activity visit patterns in realistic in-the-wild data
can equivocally be considered as a general graph representation learning task.
We posit that exploiting underlying graphical patterns in CHAR data can improve
CHAR task performance and representation learning. Building on the intuition
that certain activities are frequently performed with the phone placed in
certain positions, we focus on the context-aware human activity problem of
recognizing the <Activity, Phone Placement> tuple. We demonstrate that CHAR
data has an underlying graph structure that can be viewed as a heterogenous
hypergraph that has multiple types of nodes and hyperedges (an edge connecting
more than two nodes). Subsequently, learning <Activity, Phone Placement>
representations becomes a graph node representation learning problem. After
task transformation, we further propose a novel Heterogeneous HyperGraph Neural
Network architecture for Context-aware Human Activity Recognition (HHGNN-CHAR),
with three types of heterogeneous nodes (user, phone placement, and activity).
Connections between all types of nodes are represented by hyperedges. Rigorous
evaluation demonstrated that on an unscripted, in-the-wild CHAR dataset, our
proposed framework significantly outperforms state-of-the-art (SOTA) baselines
including CHAR models that do not exploit graphs, and GNN variants that do not
incorporate heterogeneous nodes or hyperedges with overall improvements 14.04%
on Matthews Correlation Coefficient (MCC) and 7.01% on Macro F1 scores.",2024-09-26,"Wen Ge, Guanyi Mou, Emmanuel O. Agu, Kyumin Lee",http://arxiv.org/pdf/2409.17483v1,cs.LG
MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models,"Large Language Models (LLMs) are distinguished by their massive parameter
counts, which typically result in significant redundancy. This work introduces
MaskLLM, a learnable pruning method that establishes Semi-structured (or
``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during
inference. Instead of developing a new importance criterion, MaskLLM explicitly
models N:M patterns as a learnable distribution through Gumbel Softmax
sampling. This approach facilitates end-to-end training on large-scale datasets
and offers two notable advantages: 1) High-quality Masks - our method
effectively scales to large datasets and learns accurate masks; 2)
Transferability - the probabilistic modeling of mask distribution enables the
transfer learning of sparsity across domains or tasks. We assessed MaskLLM
using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3,
with sizes ranging from 843M to 15B parameters, and our empirical results show
substantial improvements over state-of-the-art methods. For instance, leading
approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to
the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL
solely by learning the masks with frozen weights. Furthermore, MaskLLM's
learnable nature allows customized masks for lossless application of 2:4
sparsity to downstream tasks or domains. Code is available at
https://github.com/NVlabs/MaskLLM.",2024-09-26,"Gongfan Fang, Hongxu Yin, Saurav Muralidharan, Greg Heinrich, Jeff Pool, Jan Kautz, Pavlo Molchanov, Xinchao Wang",http://arxiv.org/pdf/2409.17481v2,cs.LG
On the Impact of Feature Heterophily on Link Prediction with Graph Neural Networks,"Heterophily, or the tendency of connected nodes in networks to have different
class labels or dissimilar features, has been identified as challenging for
many Graph Neural Network (GNN) models. While the challenges of applying GNNs
for node classification when class labels display strong heterophily are well
understood, it is unclear how heterophily affects GNN performance in other
important graph learning tasks where class labels are not available. In this
work, we focus on the link prediction task and systematically analyze the
impact of heterophily in node features on GNN performance. Theoretically, we
first introduce formal definitions of homophilic and heterophilic link
prediction tasks, and present a theoretical framework that highlights the
different optimizations needed for the respective tasks. We then analyze how
different link prediction encoders and decoders adapt to varying levels of
feature homophily and introduce designs for improved performance. Our empirical
analysis on a variety of synthetic and real-world datasets confirms our
theoretical insights and highlights the importance of adopting learnable
decoders and GNN encoders with ego- and neighbor-embedding separation in
message passing for link prediction tasks beyond homophily.",2024-09-26,"Jiong Zhu, Gaotang Li, Yao-An Yang, Jing Zhu, Xuehao Cui, Danai Koutra",http://arxiv.org/pdf/2409.17475v1,cs.LG
Reducing and Exploiting Data Augmentation Noise through Meta Reweighting Contrastive Learning for Text Classification,"Data augmentation has shown its effectiveness in resolving the data-hungry
problem and improving model's generalization ability. However, the quality of
augmented data can be varied, especially compared with the raw/original data.
To boost deep learning models' performance given augmented data/samples in text
classification tasks, we propose a novel framework, which leverages both meta
learning and contrastive learning techniques as parts of our design for
reweighting the augmented samples and refining their feature representations
based on their quality. As part of the framework, we propose novel
weight-dependent enqueue and dequeue algorithms to utilize augmented samples'
weight/quality information effectively. Through experiments, we show that our
framework can reasonably cooperate with existing deep learning models (e.g.,
RoBERTa-base and Text-CNN) and augmentation techniques (e.g., Wordnet and
Easydata) for specific supervised learning tasks. Experiment results show that
our framework achieves an average of 1.6%, up to 4.3% absolute improvement on
Text-CNN encoders and an average of 1.4%, up to 4.4% absolute improvement on
RoBERTa-base encoders on seven GLUE benchmark datasets compared with the best
baseline. We present an indepth analysis of our framework design, revealing the
non-trivial contributions of our network components. Our code is publicly
available for better reproducibility.",2024-09-26,"Guanyi Mou, Yichuan Li, Kyumin Lee",http://arxiv.org/pdf/2409.17474v1,cs.LG
Adjusting Regression Models for Conditional Uncertainty Calibration,"Conformal Prediction methods have finite-sample distribution-free marginal
coverage guarantees. However, they generally do not offer conditional coverage
guarantees, which can be important for high-stakes decisions. In this paper, we
propose a novel algorithm to train a regression function to improve the
conditional coverage after applying the split conformal prediction procedure.
We establish an upper bound for the miscoverage gap between the conditional
coverage and the nominal coverage rate and propose an end-to-end algorithm to
control this upper bound. We demonstrate the efficacy of our method empirically
on synthetic and real-world datasets.",2024-09-26,"Ruijiang Gao, Mingzhang Yin, James McInerney, Nathan Kallus",http://arxiv.org/pdf/2409.17466v1,cs.LG
RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking,"The rapid progress of Large Language Models (LLMs) has opened up new
opportunities across various domains and applications; yet it also presents
challenges related to potential misuse. To mitigate such risks, red teaming has
been employed as a proactive security measure to probe language models for
harmful outputs via jailbreak attacks. However, current jailbreak attack
approaches are single-turn with explicit malicious queries that do not fully
capture the complexity of real-world interactions. In reality, users can engage
in multi-turn interactions with LLM-based chat assistants, allowing them to
conceal their true intentions in a more covert manner. To bridge this gap, we,
first, propose a new jailbreak approach, RED QUEEN ATTACK. This method
constructs a multi-turn scenario, concealing the malicious intent under the
guise of preventing harm. We craft 40 scenarios that vary in turns and select
14 harmful categories to generate 56k multi-turn attack data points. We conduct
comprehensive experiments on the RED QUEEN ATTACK with four representative LLM
families of different sizes. Our experiments reveal that all LLMs are
vulnerable to RED QUEEN ATTACK, reaching 87.62% attack success rate on GPT-4o
and 75.4% on Llama3-70B. Further analysis reveals that larger models are more
susceptible to the RED QUEEN ATTACK, with multi-turn structures and concealment
strategies contributing to its success. To prioritize safety, we introduce a
straightforward mitigation strategy called RED QUEEN GUARD, which aligns LLMs
to effectively counter adversarial attacks. This approach reduces the attack
success rate to below 1% while maintaining the model's performance across
standard benchmarks. Full implementation and dataset are publicly accessible at
https://github.com/kriti-hippo/red_queen.",2024-09-26,"Yifan Jiang, Kriti Aggarwal, Tanmay Laud, Kashif Munir, Jay Pujara, Subhabrata Mukherjee",http://arxiv.org/pdf/2409.17458v1,cs.LG
A Novel Spinor-Based Embedding Model for Transformers,"This paper proposes a novel approach to word embeddings in Transformer models
by utilizing spinors from geometric algebra. Spinors offer a rich mathematical
framework capable of capturing complex relationships and transformations in
high-dimensional spaces. By encoding words as spinors, we aim to enhance the
expressiveness and robustness of language representations. We present the
theoretical foundations of spinors, detail their integration into Transformer
architectures, and discuss potential advantages and challenges.",2024-09-26,Rick White,http://arxiv.org/pdf/2410.00038v1,cs.LG
Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models,"Language models (LMs), despite their advances, often depend on spurious
correlations, undermining their accuracy and generalizability. This study
addresses the overlooked impact of subtler, more complex shortcuts that
compromise model reliability beyond oversimplified shortcuts. We introduce a
comprehensive benchmark that categorizes shortcuts into occurrence, style, and
concept, aiming to explore the nuanced ways in which these shortcuts influence
the performance of LMs. Through extensive experiments across traditional LMs,
large language models, and state-of-the-art robust models, our research
systematically investigates models' resilience and susceptibilities to
sophisticated shortcuts. Our benchmark and code can be found at:
https://github.com/yuqing-zhou/shortcut-learning-in-text-classification.",2024-09-26,"Yuqing Zhou, Ruixiang Tang, Ziyu Yao, Ziwei Zhu",http://arxiv.org/pdf/2409.17455v1,cs.LG
Description-based Controllable Text-to-Speech with Cross-Lingual Voice Control,"We propose a novel description-based controllable text-to-speech (TTS) method
with cross-lingual control capability. To address the lack of audio-description
paired data in the target language, we combine a TTS model trained on the
target language with a description control model trained on another language,
which maps input text descriptions to the conditional features of the TTS
model. These two models share disentangled timbre and style representations
based on self-supervised learning (SSL), allowing for disentangled voice
control, such as controlling speaking styles while retaining the original
timbre. Furthermore, because the SSL-based timbre and style representations are
language-agnostic, combining the TTS and description control models while
sharing the same embedding space effectively enables cross-lingual control of
voice characteristics. Experiments on English and Japanese TTS demonstrate that
our method achieves high naturalness and controllability for both languages,
even though no Japanese audio-description pairs are used.",2024-09-26,"Ryuichi Yamamoto, Yuma Shirahata, Masaya Kawamura, Kentaro Tachibana",http://arxiv.org/pdf/2409.17452v1,cs.LG
Efficient Federated Learning against Heterogeneous and Non-stationary Client Unavailability,"Addressing intermittent client availability is critical for the real-world
deployment of federated learning algorithms. Most prior work either overlooks
the potential non-stationarity in the dynamics of client unavailability or
requires substantial memory/computation overhead. We study federated learning
in the presence of heterogeneous and non-stationary client availability, which
may occur when the deployment environments are uncertain, or the clients are
mobile. The impacts of heterogeneity and non-stationarity on client
unavailability can be significant, as we illustrate using FedAvg, the most
widely adopted federated learning algorithm. We propose FedAPM, which includes
novel algorithmic structures that (i) compensate for missed computations due to
unavailability with only $O(1)$ additional memory and computation with respect
to standard FedAvg, and (ii) evenly diffuse local updates within the federated
learning system through implicit gossiping, despite being agnostic to
non-stationary dynamics. We show that FedAPM converges to a stationary point of
even non-convex objectives while achieving the desired linear speedup property.
We corroborate our analysis with numerical experiments over diversified client
unavailability dynamics on real-world data sets.",2024-09-26,"Ming Xiang, Stratis Ioannidis, Edmund Yeh, Carlee Joe-Wong, Lili Su",http://arxiv.org/pdf/2409.17446v2,cs.LG
Rejection Sampling IMLE: Designing Priors for Better Few-Shot Image Synthesis,"An emerging area of research aims to learn deep generative models with
limited training data. Prior generative models like GANs and diffusion models
require a lot of data to perform well, and their performance degrades when they
are trained on only a small amount of data. A recent technique called Implicit
Maximum Likelihood Estimation (IMLE) has been adapted to the few-shot setting,
achieving state-of-the-art performance. However, current IMLE-based approaches
encounter challenges due to inadequate correspondence between the latent codes
selected for training and those drawn during inference. This results in
suboptimal test-time performance. We theoretically show a way to address this
issue and propose RS-IMLE, a novel approach that changes the prior distribution
used for training. This leads to substantially higher quality image generation
compared to existing GAN and IMLE-based methods, as validated by comprehensive
experiments conducted on nine few-shot image datasets.",2024-09-26,"Chirag Vashist, Shichong Peng, Ke Li",http://arxiv.org/pdf/2409.17439v1,cs.LG
Minimizing Live Experiments in Recommender Systems: User Simulation to Evaluate Preference Elicitation Policies,"Evaluation of policies in recommender systems typically involves A/B testing
using live experiments on real users to assess a new policy's impact on
relevant metrics. This ``gold standard'' comes at a high cost, however, in
terms of cycle time, user cost, and potential user retention. In developing
policies for ``onboarding'' new users, these costs can be especially
problematic, since on-boarding occurs only once. In this work, we describe a
simulation methodology used to augment (and reduce) the use of live
experiments. We illustrate its deployment for the evaluation of ``preference
elicitation'' algorithms used to onboard new users of the YouTube Music
platform. By developing counterfactually robust user behavior models, and a
simulation service that couples such models with production infrastructure, we
are able to test new algorithms in a way that reliably predicts their
performance on key metrics when deployed live. We describe our domain, our
simulation models and platform, results of experiments and deployment, and
suggest future steps needed to further realistic simulation as a powerful
complement to live experiments.",2024-09-26,"Chih-Wei Hsu, Martin Mladenov, Ofer Meshi, James Pine, Hubert Pham, Shane Li, Xujian Liang, Anton Polishko, Li Yang, Ben Scheetz, Craig Boutilier",http://arxiv.org/pdf/2409.17436v1,cs.LG
Stress Detection from Photoplethysmography in a Virtual Reality Environment,"Personalized virtual reality exposure therapy is a therapeutic practice that
can adapt to an individual patient, leading to better health outcomes.
Measuring a patient's mental state to adjust the therapy is a critical but
difficult task. Most published studies use subjective methods to estimate a
patient's mental state, which can be inaccurate. This article proposes a
virtual reality exposure therapy (VRET) platform capable of assessing a
patient's mental state using non-intrusive and widely available physiological
signals such as photoplethysmography (PPG). In a case study, we evaluate how
PPG signals can be used to detect two binary classifications: peaceful and
stressful states. Sixteen healthy subjects were exposed to the two VR
environments (relaxed and stressful). Using LOSO cross-validation, our best
classification model could predict the two states with a 70.6% accuracy which
outperforms many more complex approaches.",2024-09-25,"Athar Mahmoudi-Nejad, Pierre Boulanger, Matthew Guzdial",http://arxiv.org/pdf/2409.17427v1,cs.LG
Website visits can predict angler presence using machine learning,"Understanding and predicting recreational angler effort is important for
sustainable fisheries management. However, conventional methods of measuring
angler effort, such as surveys, can be costly and limited in both time and
spatial extent. Models that predict angler effort based on environmental or
economic factors typically rely on historical data, which often limits their
spatial and temporal generalizability due to data scarcity. In this study,
high-resolution data from an online fishing platform and easily accessible
auxiliary data were tested to predict daily boat presence and aerial counts of
boats at almost 200 lakes over five years in Ontario, Canada. Lake-information
website visits alone enabled predicting daily angler boat presence with 78%
accuracy. While incorporating additional environmental, socio-ecological,
weather and angler-reported features into machine learning models did not
remarkably improve prediction performance of boat presence, they were
substantial for the prediction of boat counts. Models achieved an R2 of up to
0.77 at known lakes included in the model training, but they performed poorly
for unknown lakes (R2 = 0.21). The results demonstrate the value of integrating
data from online fishing platforms into predictive models and highlight the
potential of machine learning models to enhance fisheries management.",2024-09-25,"Julia S. Schmid, Sean Simmons, Mark A. Lewis, Mark S. Poesch, Pouria Ramazi",http://arxiv.org/pdf/2409.17425v2,cs.LG
Results of the Big ANN: NeurIPS'23 competition,"The 2023 Big ANN Challenge, held at NeurIPS 2023, focused on advancing the
state-of-the-art in indexing data structures and search algorithms for
practical variants of Approximate Nearest Neighbor (ANN) search that reflect
the growing complexity and diversity of workloads. Unlike prior challenges that
emphasized scaling up classical ANN search
~\cite{DBLP:conf/nips/SimhadriWADBBCH21}, this competition addressed filtered
search, out-of-distribution data, sparse and streaming variants of ANNS.
Participants developed and submitted innovative solutions that were evaluated
on new standard datasets with constrained computational resources. The results
showcased significant improvements in search accuracy and efficiency over
industry-standard baselines, with notable contributions from both academic and
industrial teams. This paper summarizes the competition tracks, datasets,
evaluation metrics, and the innovative approaches of the top-performing
submissions, providing insights into the current advancements and future
directions in the field of approximate nearest neighbor search.",2024-09-25,"Harsha Vardhan Simhadri, Martin Aumüller, Amir Ingber, Matthijs Douze, George Williams, Magdalen Dobson Manohar, Dmitry Baranchuk, Edo Liberty, Frank Liu, Ben Landrum, Mazin Karjikar, Laxman Dhulipala, Meng Chen, Yue Chen, Rui Ma, Kai Zhang, Yuzheng Cai, Jiayang Shi, Yizhuo Chen, Weiguo Zheng, Zihao Wan, Jie Yin, Ben Huang",http://arxiv.org/pdf/2409.17424v1,cs.LG
Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction,"Large Language Models (LLMs) have demonstrated remarkable capabilities in
handling long context inputs, but this comes at the cost of increased
computational resources and latency. Our research introduces a novel approach
for the long context bottleneck to accelerate LLM inference and reduce GPU
memory consumption. Our research demonstrates that LLMs can identify relevant
tokens in the early layers before generating answers to a query. Leveraging
this insight, we propose an algorithm that uses early layers of an LLM as
filters to select and compress input tokens, significantly reducing the context
length for subsequent processing. Our method, GemFilter, demonstrates
substantial improvements in both speed and memory efficiency compared to
existing techniques, such as standard attention and SnapKV/H2O. Notably, it
achieves a 2.4$\times$ speedup and 30\% reduction in GPU memory usage compared
to SOTA methods. Evaluation on the Needle in a Haystack task shows that
GemFilter significantly outperforms standard attention, SnapKV and demonstrates
comparable performance on the LongBench challenge. GemFilter is simple,
training-free, and broadly applicable across different LLMs. Crucially, it
provides interpretability by allowing humans to inspect the selected input
sequence. These findings not only offer practical benefits for LLM deployment,
but also enhance our understanding of LLM internal mechanisms, paving the way
for further optimizations in LLM design and inference. Our code is available at
\url{https://github.com/SalesforceAIResearch/GemFilter}.",2024-09-25,"Zhenmei Shi, Yifei Ming, Xuan-Phi Nguyen, Yingyu Liang, Shafiq Joty",http://arxiv.org/pdf/2409.17422v1,cs.LG
Solar Active Regions Emergence Prediction Using Long Short-Term Memory Networks,"We developed Long Short-Term Memory (LSTM) models to predict the formation of
active regions (ARs) on the solar surface. Using the Doppler shift velocity,
the continuum intensity, and the magnetic field observations from the Solar
Dynamics Observatory (SDO) Helioseismic and Magnetic Imager (HMI), we have
created time-series datasets of acoustic power and magnetic flux, which are
used to train LSTM models on predicting continuum intensity, 12 hours in
advance. These novel machine learning (ML) models are able to capture
variations of the acoustic power density associated with upcoming magnetic flux
emergence and continuum intensity decrease. Testing of the models' performance
was done on data for 5 ARs, unseen from the models during training. Model 8,
the best performing model trained, was able to make a successful prediction of
emergence for all testing active regions in an experimental setting and three
of them in an operational. The model predicted the emergence of AR11726,
AR13165, and AR13179 respectively 10, 29, and 5 hours in advance, and
variations of this model achieved average RMSE values of 0.11 for both active
and quiet areas on the solar disc. This work sets the foundations for ML-aided
prediction of solar ARs.",2024-09-25,"Spiridon Kasapis, Irina N. Kitiashvili, Alexander G. Kosovichev, John T. Stefan",http://arxiv.org/pdf/2409.17421v1,cs.LG
Spiders Based on Anxiety: How Reinforcement Learning Can Deliver Desired User Experience in Virtual Reality Personalized Arachnophobia Treatment,"The need to generate a spider to provoke a desired anxiety response arises in
the context of personalized virtual reality exposure therapy (VRET), a
treatment approach for arachnophobia. This treatment involves patients
observing virtual spiders in order to become desensitized and decrease their
phobia, which requires that the spiders elicit specific anxiety responses.
However, VRET approaches tend to require therapists to hand-select the
appropriate spider for each patient, which is a time-consuming process and
takes significant technical knowledge and patient insight. While automated
methods exist, they tend to employ rules-based approaches with minimal ability
to adapt to specific users. To address these challenges, we present a framework
for VRET utilizing procedural content generation (PCG) and reinforcement
learning (RL), which automatically adapts a spider to elicit a desired anxiety
response. We demonstrate the superior performance of this system compared to a
more common rules-based VRET method.",2024-09-25,"Athar Mahmoudi-Nejad, Matthew Guzdial, Pierre Boulanger",http://arxiv.org/pdf/2409.17406v1,cs.LG
AI Enabled Neutron Flux Measurement and Virtual Calibration in Boiling Water Reactors,"Accurately capturing the three dimensional power distribution within a
reactor core is vital for ensuring the safe and economical operation of the
reactor, compliance with Technical Specifications, and fuel cycle planning
(safety, control, and performance evaluation). Offline (that is, during cycle
planning and core design), a three dimensional neutronics simulator is used to
estimate the reactor's power, moderator, void, and flow distributions, from
which margin to thermal limits and fuel exposures can be approximated. Online,
this is accomplished with a system of local power range monitors (LPRMs)
designed to capture enough neutron flux information to infer the full nodal
power distribution. Certain problems with this process, ranging from
measurement and calibration to the power adaption process, pose challenges to
operators and limit the ability to design reload cores economically (e.g.,
engineering in insufficient margin or more margin than required). Artificial
intelligence (AI) and machine learning (ML) are being used to solve the
problems to reduce maintenance costs, improve the accuracy of online local
power measurements, and decrease the bias between offline and online power
distributions, thereby leading to a greater ability to design safe and
economical reload cores. We present ML models trained from two deep neural
network (DNN) architectures, SurrogateNet and LPRMNet, that demonstrate a
testing error of 1 percent and 3 percent, respectively. Applications of these
models can include virtual sensing capability for bypassed or malfunctioning
LPRMs, on demand virtual calibration of detectors between successive
calibrations, highly accurate nuclear end of life determinations for LPRMs, and
reduced bias between measured and predicted power distributions within the
core.",2024-09-25,"Anirudh Tunga, Jordan Heim, Michael Mueterthies, Thomas Gruenwald, Jonathan Nistor",http://arxiv.org/pdf/2409.17405v1,cs.LG
Enhancing Recommendation with Denoising Auxiliary Task,"The historical interaction sequences of users plays a crucial role in
training recommender systems that can accurately predict user preferences.
However, due to the arbitrariness of user behavior, the presence of noise in
these sequences poses a challenge to predicting their next actions in
recommender systems. To address this issue, our motivation is based on the
observation that training noisy sequences and clean sequences (sequences
without noise) with equal weights can impact the performance of the model. We
propose a novel self-supervised Auxiliary Task Joint Training (ATJT) method
aimed at more accurately reweighting noisy sequences in recommender systems.
Specifically, we strategically select subsets from users' original sequences
and perform random replacements to generate artificially replaced noisy
sequences. Subsequently, we perform joint training on these artificially
replaced noisy sequences and the original sequences. Through effective
reweighting, we incorporate the training results of the noise recognition model
into the recommender model. We evaluate our method on three datasets using a
consistent base model. Experimental results demonstrate the effectiveness of
introducing self-supervised auxiliary task to enhance the base model's
performance.",2024-09-25,"Pengsheng Liu, Linan Zheng, Jiale Chen, Guangfa Zhang, Yang Xu, Jinyun Fang",http://arxiv.org/pdf/2409.17402v1,cs.LG
Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference,"Reward inference (learning a reward model from human preferences) is a
critical intermediate step in the Reinforcement Learning from Human Feedback
(RLHF) pipeline for fine-tuning Large Language Models (LLMs). In practice, RLHF
faces fundamental challenges such as distribution shift, reward model
overfitting, and problem misspecification. An alternative approach is direct
policy optimization without reward inference, such as Direct Preference
Optimization (DPO), which provides a much simpler pipeline and has shown
empirical success in LLM applications. However, DPO utilizes the closed-form
expression between the optimal policy and the reward function, which is only
suitable under the bandit setting or deterministic MDPs. This paper develops
two RLHF algorithms without reward inference for general RL problems beyond
bandits and deterministic MDPs, and general preference models beyond the
Bradley-Terry model. The key idea is to estimate the local value function
difference from human preferences and then approximate the policy gradient with
a zeroth-order gradient approximator. For both algorithms, we establish
polynomial convergence rates in terms of the number of policy gradient
iterations, the number of trajectory samples, and human preference queries per
iteration. Numerical experiments in stochastic environments validate the
performance of our proposed algorithms, outperforming popular RLHF baselines
such as DPO and PPO. Our paper shows there exist provably efficient methods to
solve general RLHF problems without reward inference.",2024-09-25,"Qining Zhang, Lei Ying",http://arxiv.org/pdf/2409.17401v2,cs.LG
Building Multilingual Datasets for Predicting Mental Health Severity through LLMs: Prospects and Challenges,"Large Language Models (LLMs) are increasingly being integrated into various
medical fields, including mental health support systems. However, there is a
gap in research regarding the effectiveness of LLMs in non-English mental
health support applications. To address this problem, we present a novel
multilingual adaptation of widely-used mental health datasets, translated from
English into six languages (e.g., Greek, Turkish, French, Portuguese, German,
and Finnish). This dataset enables a comprehensive evaluation of LLM
performance in detecting mental health conditions and assessing their severity
across multiple languages. By experimenting with GPT and Llama, we observe
considerable variability in performance across languages, despite being
evaluated on the same translated dataset. This inconsistency underscores the
complexities inherent in multilingual mental health support, where
language-specific nuances and mental health data coverage can affect the
accuracy of the models. Through comprehensive error analysis, we emphasize the
risks of relying exclusively on LLMs in medical settings (e.g., their potential
to contribute to misdiagnoses). Moreover, our proposed approach offers
significant cost savings for multilingual tasks, presenting a major advantage
for broad-scale implementation.",2024-09-25,"Konstantinos Skianis, John Pavlopoulos, A. Seza Doğruöz",http://arxiv.org/pdf/2409.17397v2,cs.LG
Trading through Earnings Seasons using Self-Supervised Contrastive Representation Learning,"Earnings release is a key economic event in the financial markets and crucial
for predicting stock movements. Earnings data gives a glimpse into how a
company is doing financially and can hint at where its stock might go next.
However, the irregularity of its release cycle makes it a challenge to
incorporate this data in a medium-frequency algorithmic trading model and the
usefulness of this data fades fast after it is released, making it tough for
models to stay accurate over time. Addressing this challenge, we introduce the
Contrastive Earnings Transformer (CET) model, a self-supervised learning
approach rooted in Contrastive Predictive Coding (CPC), aiming to optimise the
utilisation of earnings data. To ascertain its effectiveness, we conduct a
comparative study of CET against benchmark models across diverse sectors. Our
research delves deep into the intricacies of stock data, evaluating how various
models, and notably CET, handle the rapidly changing relevance of earnings data
over time and over different sectors. The research outcomes shed light on CET's
distinct advantage in extrapolating the inherent value of earnings data over
time. Its foundation on CPC allows for a nuanced understanding, facilitating
consistent stock predictions even as the earnings data ages. This finding about
CET presents a fresh approach to better use earnings data in algorithmic
trading for predicting stock price trends.",2024-09-25,"Zhengxin Joseph Ye, Bjoern Schuller",http://arxiv.org/pdf/2409.17392v1,cs.LG
Beyond Redundancy: Information-aware Unsupervised Multiplex Graph Structure Learning,"Unsupervised Multiplex Graph Learning (UMGL) aims to learn node
representations on various edge types without manual labeling. However,
existing research overlooks a key factor: the reliability of the graph
structure. Real-world data often exhibit a complex nature and contain abundant
task-irrelevant noise, severely compromising UMGL's performance. Moreover,
existing methods primarily rely on contrastive learning to maximize mutual
information across different graphs, limiting them to multiplex graph redundant
scenarios and failing to capture view-unique task-relevant information. In this
paper, we focus on a more realistic and challenging task: to unsupervisedly
learn a fused graph from multiple graphs that preserve sufficient task-relevant
information while removing task-irrelevant noise. Specifically, our proposed
Information-aware Unsupervised Multiplex Graph Fusion framework (InfoMGF) uses
graph structure refinement to eliminate irrelevant noise and simultaneously
maximizes view-shared and view-unique task-relevant information, thereby
tackling the frontier of non-redundant multiplex graph. Theoretical analyses
further guarantee the effectiveness of InfoMGF. Comprehensive experiments
against various baselines on different downstream tasks demonstrate its
superior performance and robustness. Surprisingly, our unsupervised method even
beats the sophisticated supervised approaches. The source code and datasets are
available at https://github.com/zxlearningdeep/InfoMGF.",2024-09-25,"Zhixiang Shen, Shuo Wang, Zhao Kang",http://arxiv.org/pdf/2409.17386v1,cs.LG
SSTP: Efficient Sample Selection for Trajectory Prediction,"Trajectory prediction is a core task in autonomous driving. However, training
advanced trajectory prediction models on large-scale datasets is both
time-consuming and computationally expensive. In addition, the imbalanced
distribution of driving scenarios often biases models toward data-rich cases,
limiting performance in safety-critical, data-scarce conditions. To address
these challenges, we propose the Sample Selection for Trajectory Prediction
(SSTP) framework, which constructs a compact yet balanced dataset for
trajectory prediction. SSTP consists of two main stages (1) Extraction, in
which a pretrained trajectory prediction model computes gradient vectors for
each sample to capture their influence on parameter updates; and (2) Selection,
where a submodular function is applied to greedily choose a representative
subset that covers diverse driving scenarios. This approach significantly
reduces the dataset size and mitigates scenario imbalance, without sacrificing
prediction accuracy and even improving in high-density cases. We evaluate our
proposed SSTP on the Argoverse 1 and Argoverse 2 benchmarks using a wide range
of recent state-of-the-art models. Our experiments demonstrate that SSTP
achieves comparable performance to full-dataset training using only half the
data while delivering substantial improvements in high-density traffic scenes
and significantly reducing training time. Importantly, SSTP exhibits strong
generalization and robustness, and the selected subset is model-agnostic,
offering a broadly applicable solution.",2024-09-25,"Ruining Yang, Yi Xu, Yun Fu, Lili Su",http://arxiv.org/pdf/2409.17385v2,cs.LG
VectorSearch: Enhancing Document Retrieval with Semantic Embeddings and Optimized Search,"Traditional retrieval methods have been essential for assessing document
similarity but struggle with capturing semantic nuances. Despite advancements
in latent semantic analysis (LSA) and deep learning, achieving comprehensive
semantic understanding and accurate retrieval remains challenging due to high
dimensionality and semantic gaps. The above challenges call for new techniques
to effectively reduce the dimensions and close the semantic gaps. To this end,
we propose VectorSearch, which leverages advanced algorithms, embeddings, and
indexing techniques for refined retrieval. By utilizing innovative multi-vector
search operations and encoding searches with advanced language models, our
approach significantly improves retrieval accuracy. Experiments on real-world
datasets show that VectorSearch outperforms baseline metrics, demonstrating its
efficacy for large-scale retrieval tasks.",2024-09-25,"Solmaz Seyed Monir, Irene Lau, Shubing Yang, Dongfang Zhao",http://arxiv.org/pdf/2409.17383v1,cs.LG
Implicit Neural Representations for Simultaneous Reduction and Continuous Reconstruction of Multi-Altitude Climate Data,"The world is moving towards clean and renewable energy sources, such as wind
energy, in an attempt to reduce greenhouse gas emissions that contribute to
global warming. To enhance the analysis and storage of wind data, we introduce
a deep learning framework designed to simultaneously enable effective
dimensionality reduction and continuous representation of multi-altitude wind
data from discrete observations. The framework consists of three key
components: dimensionality reduction, cross-modal prediction, and
super-resolution. We aim to: (1) improve data resolution across diverse
climatic conditions to recover high-resolution details; (2) reduce data
dimensionality for more efficient storage of large climate datasets; and (3)
enable cross-prediction between wind data measured at different heights.
Comprehensive testing confirms that our approach surpasses existing methods in
both super-resolution quality and compression efficiency.",2024-09-25,"Alif Bin Abdul Qayyum, Xihaier Luo, Nathan M. Urban, Xiaoning Qian, Byung-Jun Yoon",http://arxiv.org/pdf/2409.17367v1,cs.LG
Topological Foundations of Reinforcement Learning,"The goal of this work is to serve as a foundation for deep studies of the
topology of state, action, and policy spaces in reinforcement learning. By
studying these spaces from a mathematical perspective, we expect to gain more
insight into how to build better algorithms to solve decision problems.
Therefore, we focus on presenting the connection between the Banach fixed point
theorem and the convergence of reinforcement learning algorithms, and we
illustrate how the insights gained from this can practically help in designing
more efficient algorithms. Before doing so, however, we first introduce
relevant concepts such as metric spaces, normed spaces and Banach spaces for
better understanding, before expressing the entire reinforcement learning
problem in terms of Markov decision processes. This allows us to properly
introduce the Banach contraction principle in a language suitable for
reinforcement learning, and to write the Bellman equations in terms of
operators on Banach spaces to show why reinforcement learning algorithms
converge. Finally, we show how the insights gained from the mathematical study
of convergence are helpful in reasoning about the best ways to make
reinforcement learning algorithms more efficient.",2024-09-25,David Krame Kadurha,http://arxiv.org/pdf/2410.03706v1,cs.LG
Data-driven Probabilistic Trajectory Learning with High Temporal Resolution in Terminal Airspace,"Predicting flight trajectories is a research area that holds significant
merit. In this paper, we propose a data-driven learning framework, that
leverages the predictive and feature extraction capabilities of the mixture
models and seq2seq-based neural networks while addressing prevalent challenges
caused by error propagation and dimensionality reduction. After training with
this framework, the learned model can improve long-step prediction accuracy
significantly given the past trajectories and the context information. The
accuracy and effectiveness of the approach are evaluated by comparing the
predicted trajectories with the ground truth. The results indicate that the
proposed method has outperformed the state-of-the-art predicting methods on a
terminal airspace flight trajectory dataset. The trajectories generated by the
proposed method have a higher temporal resolution(1 timestep per second vs 0.1
timestep per second) and are closer to the ground truth.",2024-09-25,"Jun Xiang, Jun Chen",http://arxiv.org/pdf/2409.17359v1,cs.LG
Revisiting inverse Hessian vector products for calculating influence functions,"Influence functions are a popular tool for attributing a model's output to
training data. The traditional approach relies on the calculation of inverse
Hessian-vector products (iHVP), but the classical solver ""Linear time
Stochastic Second-order Algorithm"" (LiSSA, Agarwal et al. (2017)) is often
deemed impractical for large models due to expensive computation and
hyperparameter tuning. We show that the three hyperparameters -- the scaling
factor, the batch size, and the number of steps -- can be chosen depending on
the spectral properties of the Hessian, particularly its trace and largest
eigenvalue. By evaluating with random sketching (Swartworth and Woodruff,
2023), we find that the batch size has to be sufficiently large for LiSSA to
converge; however, for all of the models we consider, the requirement is mild.
We confirm our findings empirically by comparing to Proximal Bregman Retraining
Functions (PBRF, Bae et al. (2022)). Finally, we discuss what role the inverse
Hessian plays in calculating the influence.",2024-09-25,"Yegor Klochkov, Yang Liu",http://arxiv.org/pdf/2409.17357v1,cs.LG
Learning Utilities from Demonstrations in Markov Decision Processes,"Our goal is to extract useful knowledge from demonstrations of behavior in
sequential decision-making problems. Although it is well-known that humans
commonly engage in risk-sensitive behaviors in the presence of stochasticity,
most Inverse Reinforcement Learning (IRL) models assume a risk-neutral agent.
Beyond introducing model misspecification, these models do not directly capture
the risk attitude of the observed agent, which can be crucial in many
applications. In this paper, we propose a novel model of behavior in Markov
Decision Processes (MDPs) that explicitly represents the agent's risk attitude
through a utility function. We then define the Utility Learning (UL) problem as
the task of inferring the observed agent's risk attitude, encoded via a utility
function, from demonstrations in MDPs, and we analyze the partial
identifiability of the agent's utility. Furthermore, we devise two provably
efficient algorithms for UL in a finite-data regime, and we analyze their
sample complexity. We conclude with proof-of-concept experiments that
empirically validate both our model and our algorithms.",2024-09-25,"Filippo Lazzati, Alberto Maria Metelli",http://arxiv.org/pdf/2409.17355v2,cs.LG
Non-asymptotic Convergence of Training Transformers for Next-token Prediction,"Transformers have achieved extraordinary success in modern machine learning
due to their excellent ability to handle sequential data, especially in
next-token prediction (NTP) tasks. However, the theoretical understanding of
their performance in NTP is limited, with existing studies focusing mainly on
asymptotic performance. This paper provides a fine-grained non-asymptotic
analysis of the training dynamics of a one-layer transformer consisting of a
self-attention module followed by a feed-forward layer. We first characterize
the essential structural properties of training datasets for NTP using a
mathematical framework based on partial orders. Then, we design a two-stage
training algorithm, where the pre-processing stage for training the
feed-forward layer and the main stage for training the attention layer exhibit
fast convergence performance. Specifically, both layers converge sub-linearly
to the direction of their corresponding max-margin solutions. We also show that
the cross-entropy loss enjoys a linear convergence rate. Furthermore, we show
that the trained transformer presents non-trivial prediction ability with
dataset shift, which sheds light on the remarkable generalization performance
of transformers. Our analysis technique involves the development of novel
properties on the attention gradient and further in-depth analysis of how these
properties contribute to the convergence of the training process. Our
experiments further validate our theoretical findings.",2024-09-25,"Ruiquan Huang, Yingbin Liang, Jing Yang",http://arxiv.org/pdf/2409.17335v2,cs.LG
The poison of dimensionality,"This paper advances the understanding of how the size of a machine learning
model affects its vulnerability to poisoning, despite state-of-the-art
defenses. Given isotropic random honest feature vectors and the geometric
median (or clipped mean) as the robust gradient aggregator rule, we essentially
prove that, perhaps surprisingly, linear and logistic regressions with $D \geq
169 H^2/P^2$ parameters are subject to arbitrary model manipulation by
poisoners, where $H$ and $P$ are the numbers of honestly labeled and poisoned
data points used for training. Our experiments go on exposing a fundamental
tradeoff between augmenting model expressivity and increasing the poisoners'
attack surface, on both synthetic data, and on MNIST & FashionMNIST data for
linear classifiers with random features. We also discuss potential implications
for source-based learning and neural nets.",2024-09-25,Lê-Nguyên Hoang,http://arxiv.org/pdf/2409.17328v1,cs.LG
"Most Influential Subset Selection: Challenges, Promises, and Beyond","How can we attribute the behaviors of machine learning models to their
training data? While the classic influence function sheds light on the impact
of individual samples, it often fails to capture the more complex and
pronounced collective influence of a set of samples. To tackle this challenge,
we study the Most Influential Subset Selection (MISS) problem, which aims to
identify a subset of training samples with the greatest collective influence.
We conduct a comprehensive analysis of the prevailing approaches in MISS,
elucidating their strengths and weaknesses. Our findings reveal that
influence-based greedy heuristics, a dominant class of algorithms in MISS, can
provably fail even in linear regression. We delineate the failure modes,
including the errors of influence function and the non-additive structure of
the collective influence. Conversely, we demonstrate that an adaptive version
of these heuristics which applies them iteratively, can effectively capture the
interactions among samples and thus partially address the issues. Experiments
on real-world datasets corroborate these theoretical findings and further
demonstrate that the merit of adaptivity can extend to more complex scenarios
such as classification tasks and non-linear neural networks. We conclude our
analysis by emphasizing the inherent trade-off between performance and
computational efficiency, questioning the use of additive metrics such as the
Linear Datamodeling Score, and offering a range of discussions.",2024-09-25,"Yuzheng Hu, Pingbang Hu, Han Zhao, Jiaqi W. Ma",http://arxiv.org/pdf/2409.18153v2,cs.LG
Accelerating Multi-Block Constrained Optimization Through Learning to Optimize,"Learning to Optimize (L2O) approaches, including algorithm unrolling,
plug-and-play methods, and hyperparameter learning, have garnered significant
attention and have been successfully applied to the Alternating Direction
Method of Multipliers (ADMM) and its variants. However, the natural extension
of L2O to multi-block ADMM-type methods remains largely unexplored. Such an
extension is critical, as multi-block methods leverage the separable structure
of optimization problems, offering substantial reductions in per-iteration
complexity. Given that classical multi-block ADMM does not guarantee
convergence, the Majorized Proximal Augmented Lagrangian Method (MPALM), which
shares a similar form with multi-block ADMM and ensures convergence, is more
suitable in this setting. Despite its theoretical advantages, MPALM's
performance is highly sensitive to the choice of penalty parameters. To address
this limitation, we propose a novel L2O approach that adaptively selects this
hyperparameter using supervised learning. We demonstrate the versatility and
effectiveness of our method by applying it to the Lasso problem and the optimal
transport problem. Our numerical results show that the proposed framework
outperforms popular alternatives. Given its applicability to generic linearly
constrained composite optimization problems, this work opens the door to a wide
range of potential real-world applications.",2024-09-25,"Ling Liang, Cameron Austin, Haizhao Yang",http://arxiv.org/pdf/2409.17320v1,cs.LG
KIPPS: Knowledge infusion in Privacy Preserving Synthetic Data Generation,"The integration of privacy measures, including differential privacy
techniques, ensures a provable privacy guarantee for the synthetic data.
However, challenges arise for Generative Deep Learning models when tasked with
generating realistic data, especially in critical domains such as Cybersecurity
and Healthcare. Generative Models optimized for continuous data struggle to
model discrete and non-Gaussian features that have domain constraints.
Challenges increase when the training datasets are limited and not diverse. In
such cases, generative models create synthetic data that repeats sensitive
features, which is a privacy risk. Moreover, generative models face
difficulties comprehending attribute constraints in specialized domains. This
leads to the generation of unrealistic data that impacts downstream accuracy.
To address these issues, this paper proposes a novel model, KIPPS, that infuses
Domain and Regulatory Knowledge from Knowledge Graphs into Generative Deep
Learning models for enhanced Privacy Preserving Synthetic data generation. The
novel framework augments the training of generative models with supplementary
context about attribute values and enforces domain constraints during training.
This added guidance enhances the model's capacity to generate realistic and
domain-compliant synthetic data. The proposed model is evaluated on real-world
datasets, specifically in the domains of Cybersecurity and Healthcare, where
domain constraints and rules add to the complexity of the data. Our experiments
evaluate the privacy resilience and downstream accuracy of the model against
benchmark methods, demonstrating its effectiveness in addressing the balance
between privacy preservation and data accuracy in complex domains.",2024-09-25,"Anantaa Kotal, Anupam Joshi",http://arxiv.org/pdf/2409.17315v1,cs.LG
BabyLlama-2: Ensemble-Distilled Models Consistently Outperform Teachers With Limited Data,"We present BabyLlama-2, a 345 million parameter model distillation-pretrained
from two teachers on a 10 million word corpus for the BabyLM competition. On
BLiMP and SuperGLUE benchmarks, BabyLlama-2 outperforms baselines trained on
both 10 and 100 million word datasets with the same data mix, as well as its
teacher models. Through an extensive hyperparameter sweep, we demonstrate that
the advantages of distillation cannot be attributed to suboptimal
hyperparameter selection of the teachers. Our findings underscore the need for
further investigation into distillation techniques, particularly in
data-limited settings.",2024-09-25,"Jean-Loup Tastet, Inar Timiryasov",http://arxiv.org/pdf/2409.17312v1,cs.LG
Consistent estimation of generative model representations in the data kernel perspective space,"Generative models, such as large language models and text-to-image diffusion
models, produce relevant information when presented a query. Different models
may produce different information when presented the same query. As the
landscape of generative models evolves, it is important to develop techniques
to study and analyze differences in model behaviour. In this paper we present
novel theoretical results for embedding-based representations of generative
models in the context of a set of queries. In particular, we establish
sufficient conditions for the consistent estimation of the model embeddings in
situations where the query set and the number of models grow.",2024-09-25,"Aranyak Acharyya, Michael W. Trosset, Carey E. Priebe, Hayden S. Helm",http://arxiv.org/pdf/2409.17308v2,cs.LG
Democratizing Signal Processing and Machine Learning: Math Learning Equity for Elementary and Middle School Students,"Signal Processing (SP) and Machine Learning (ML) rely on good math and coding
knowledge, in particular, linear algebra, probability, trigonometry, and
complex numbers. A good grasp of these relies on scalar algebra learned in
middle school. The ability to understand and use scalar algebra well, in turn,
relies on a good foundation in basic arithmetic. Because of various systemic
barriers, many students are not able to build a strong foundation in arithmetic
in elementary school. This leads them to struggle with algebra and everything
after that. Since math learning is cumulative, the gap between those without a
strong early foundation and everyone else keeps increasing over the school
years and becomes difficult to fill in college. In this article we discuss how
SP faculty, students, and professionals can play an important role in starting,
and participating in, university-run, or other, out-of-school math support
programs to supplement students' learning. Two example programs run by the
authors, CyMath at Iowa State and Algebra by 7th Grade (Ab7G) at Purdue, and
one run by the Actuarial Foundation, are described. We conclude with providing
some simple zero-cost suggestions for public schools that, if adopted, could
benefit a much larger number of students than what out-of-school programs can
reach.",2024-09-25,"Namrata Vaswani, Mohamed Y. Selim, Renee Serrell Gibert",http://arxiv.org/pdf/2409.17304v2,cs.LG
Neural Network Plasticity and Loss Sharpness,"In recent years, continual learning, a prediction setting in which the
problem environment may evolve over time, has become an increasingly popular
research field due to the framework's gearing towards complex, non-stationary
objectives. Learning such objectives requires plasticity, or the ability of a
neural network to adapt its predictions to a different task. Recent findings
indicate that plasticity loss on new tasks is highly related to loss landscape
sharpness in non-stationary RL frameworks. We explore the usage of sharpness
regularization techniques, which seek out smooth minima and have been touted
for their generalization capabilities in vanilla prediction settings, in
efforts to combat plasticity loss. Our findings indicate that such techniques
have no significant effect on reducing plasticity loss.",2024-09-25,"Max Koster, Jude Kukla",http://arxiv.org/pdf/2409.17300v1,cs.LG
"Sparsity, Regularization and Causality in Agricultural Yield: The Case of Paddy Rice in Peru","This study introduces a novel approach that integrates agricultural census
data with remotely sensed time series to develop precise predictive models for
paddy rice yield across various regions of Peru. By utilizing sparse regression
and Elastic-Net regularization techniques, the study identifies causal
relationships between key remotely sensed variables-such as NDVI,
precipitation, and temperature-and agricultural yield. To further enhance
prediction accuracy, the first- and second-order dynamic transformations
(velocity and acceleration) of these variables are applied, capturing
non-linear patterns and delayed effects on yield. The findings highlight the
improved predictive performance when combining regularization techniques with
climatic and geospatial variables, enabling more precise forecasts of yield
variability. The results confirm the existence of causal relationships in the
Granger sense, emphasizing the value of this methodology for strategic
agricultural management. This contributes to more efficient and sustainable
production in paddy rice cultivation.",2024-09-25,"Rita Rocio Guzman-Lopez, Luis Huamanchumo, Kevin Fernandez, Oscar Cutipa-Luque, Yhon Tiahuallpa, Helder Rojas",http://arxiv.org/pdf/2409.17298v1,cs.LG
Schrödinger bridge based deep conditional generative learning,"Conditional generative models represent a significant advancement in the
field of machine learning, allowing for the controlled synthesis of data by
incorporating additional information into the generation process. In this work
we introduce a novel Schr\""odinger bridge based deep generative method for
learning conditional distributions. We start from a unit-time diffusion process
governed by a stochastic differential equation (SDE) that transforms a fixed
point at time $0$ into a desired target conditional distribution at time $1$.
For effective implementation, we discretize the SDE with Euler-Maruyama method
where we estimate the drift term nonparametrically using a deep neural network.
We apply our method to both low-dimensional and high-dimensional conditional
generation problems. The numerical studies demonstrate that though our method
does not directly provide the conditional density estimation, the samples
generated by this method exhibit higher quality compared to those obtained by
several existing methods. Moreover, the generated samples can be effectively
utilized to estimate the conditional density and related statistical
quantities, such as conditional mean and conditional standard deviation.",2024-09-25,Hanwen Huang,http://arxiv.org/pdf/2409.17294v1,cs.LG
Building Real-time Awareness of Out-of-distribution in Trajectory Prediction for Autonomous Vehicles,"Accurate trajectory prediction is essential for the safe operation of
autonomous vehicles in real-world environments. Even well-trained machine
learning models may produce unreliable predictions due to discrepancies between
training data and real-world conditions encountered during inference. In
particular, the training dataset tends to overrepresent common scenes (e.g.,
straight lanes) while underrepresenting less frequent ones (e.g., traffic
circles). In addition, it often overlooks unpredictable real-world events such
as sudden braking or falling objects. To ensure safety, it is critical to
detect in real-time when a model's predictions become unreliable. Leveraging
the intuition that in-distribution (ID) scenes exhibit error patterns similar
to training data, while out-of-distribution (OOD) scenes do not, we introduce a
principled, real-time approach for OOD detection by framing it as a
change-point detection problem. We address the challenging settings where the
OOD scenes are deceptive, meaning that they are not easily detectable by human
intuitions. Our lightweight solutions can handle the occurrence of OOD at any
time during trajectory prediction inference. Experimental results on multiple
real-world datasets using a benchmark trajectory prediction model demonstrate
the effectiveness of our methods.",2024-09-25,"Tongfe Guo, Taposh Banerjee, Rui Liu, Lili Su",http://arxiv.org/pdf/2409.17277v2,cs.LG
Targeted Neural Architectures in Multi-Objective Frameworks for Complete Glioma Characterization from Multimodal MRI,"Brain tumors result from abnormal cell growth in brain tissue. If
undiagnosed, they cause neurological deficits, including cognitive impairment,
motor dysfunction, and sensory loss. As tumors grow, intracranial pressure
increases, potentially leading to fatal complications such as brain herniation.
Early diagnosis and treatment are crucial to controlling these effects and
slowing tumor progression. Deep learning (DL) and artificial intelligence (AI)
are increasingly used to assist doctors in early diagnosis through magnetic
resonance imaging (MRI) scans. Our research proposes targeted neural
architectures within multi-objective frameworks that can localize, segment, and
classify the grade of these gliomas from multimodal MRI images to solve this
critical issue. Our localization framework utilizes a targeted architecture
that enhances the LinkNet framework with an encoder inspired by VGG19 for
better multimodal feature extraction from the tumor along with spatial and
graph attention mechanisms that sharpen feature focus and inter-feature
relationships. For the segmentation objective, we deployed a specialized
framework using the SeResNet101 CNN model as the encoder backbone integrated
into the LinkNet architecture, achieving an IoU Score of 96%. The
classification objective is addressed through a distinct framework implemented
by combining the SeResNet152 feature extractor with Adaptive Boosting
classifier, reaching an accuracy of 98.53%. Our multi-objective approach with
targeted neural architectures demonstrated promising results for complete
glioma characterization, with the potential to advance medical AI by enabling
early diagnosis and providing more accurate treatment options for patients.",2024-09-25,"Shravan Venkatraman, Pandiyaraju V, Abeshek A, Aravintakshan S A, Pavan Kumar S, Kannan A, Madhan S",http://arxiv.org/pdf/2409.17273v3,cs.LG
Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,"Large Language Models (LLMs) have revolutionized natural language processing,
yet they struggle with inconsistent reasoning, particularly in novel domains
and complex logical sequences. This research introduces Proof of Thought, a
framework that enhances the reliability and transparency of LLM outputs. Our
approach bridges LLM-generated ideas with formal logic verification, employing
a custom interpreter to convert LLM outputs into First Order Logic constructs
for theorem prover scrutiny. Central to our method is an intermediary
JSON-based Domain-Specific Language, which by design balances precise logical
structures with intuitive human concepts. This hybrid representation enables
both rigorous validation and accessible human comprehension of LLM reasoning
processes. Key contributions include a robust type system with sort management
for enhanced logical integrity, explicit representation of rules for clear
distinction between factual and inferential knowledge, and a flexible
architecture that allows for easy extension to various domain-specific
applications. We demonstrate Proof of Thought's effectiveness through
benchmarking on StrategyQA and a novel multimodal reasoning task, showing
improved performance in open-ended scenarios. By providing verifiable and
interpretable results, our technique addresses critical needs for AI system
accountability and sets a foundation for human-in-the-loop oversight in
high-stakes domains.",2024-09-25,"Debargha Ganguly, Srinivasan Iyengar, Vipin Chaudhary, Shivkumar Kalyanaraman",http://arxiv.org/pdf/2409.17270v2,cs.LG
"Minimal Variance Model Aggregation: A principled, non-intrusive, and versatile integration of black box models","Whether deterministic or stochastic, models can be viewed as functions
designed to approximate a specific quantity of interest. We introduce Minimal
Empirical Variance Aggregation (MEVA), a data-driven framework that integrates
predictions from various models, enhancing overall accuracy by leveraging the
individual strengths of each. This non-intrusive, model-agnostic approach
treats the contributing models as black boxes and accommodates outputs from
diverse methodologies, including machine learning algorithms and traditional
numerical solvers. We advocate for a point-wise linear aggregation process and
consider two methods for optimizing this aggregate: Minimal Error Aggregation
(MEA), which minimizes the prediction error, and Minimal Variance Aggregation
(MVA), which focuses on reducing variance. We prove a theorem showing that MVA
can be more robustly estimated from data than MEA, making MEVA superior to
Minimal Empirical Error Aggregation (MEEA). Unlike MEEA, which interpolates
target values directly, MEVA formulates aggregation as an error estimation
problem, which can be performed using any backbone learning paradigm. We
demonstrate the versatility and effectiveness of our framework across various
applications, including data science and partial differential equations,
illustrating its ability to significantly enhance both robustness and accuracy.",2024-09-25,"Théo Bourdais, Houman Owhadi",http://arxiv.org/pdf/2409.17267v2,cs.LG
CodonMPNN for Organism Specific and Codon Optimal Inverse Folding,"Generating protein sequences conditioned on protein structures is an
impactful technique for protein engineering. When synthesizing engineered
proteins, they are commonly translated into DNA and expressed in an organism
such as yeast. One difficulty in this process is that the expression rates can
be low due to suboptimal codon sequences for expressing a protein in a host
organism. We propose CodonMPNN, which generates a codon sequence conditioned on
a protein backbone structure and an organism label. If naturally occurring DNA
sequences are close to codon optimality, CodonMPNN could learn to generate
codon sequences with higher expression yields than heuristic codon choices for
generated amino acid sequences. Experiments show that CodonMPNN retains the
performance of previous inverse folding approaches and recovers wild-type
codons more frequently than baselines. Furthermore, CodonMPNN has a higher
likelihood of generating high-fitness codon sequences than low-fitness codon
sequences for the same protein sequence. Code is available at
https://github.com/HannesStark/CodonMPNN.",2024-09-25,"Hannes Stark, Umesh Padia, Julia Balla, Cameron Diao, George Church",http://arxiv.org/pdf/2409.17265v1,cs.LG
Medha: Efficiently Serving Multi-Million Context Length LLM Inference Requests Without Approximations,"As large language models (LLMs) handle increasingly longer contexts, serving
long inference requests of millions of tokens presents unique challenges. We
show that existing work for long context inference is largely based on
techniques from long context training, and does not handle the high variability
in input lengths during inference. This leads to inefficient resource
utilization, server fragmentation, and head-of-line (HOL) blocking.
  We present Medha, an end-to-end system for efficient long-context LLM
inference that addresses these challenges through fine-grained time sharing.
Medha introduces three key innovations: (1) the mechanism of adaptive prefill
chunking to help mitigate HOL blocking with preemption; (2) two new parallelism
strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token
by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower
time-peroutput-token by distributing decoding across servers; and (3) a novel
input-length aware least remaining slack scheduling to meet Service Level
Objectives (SLOs).
  Medha enables exact inference scaling beyond 10 million tokens, maintaining
high throughput and low latency across mixed-length workloads. Compared to
state-of-the-art systems, Medha reduces server fragmentation, cuts median
latency by up to 30x, and improves throughput by over 5x, delivering
production-scale long-context inference without compromising performance on
shorter requests.",2024-09-25,"Amey Agrawal, Haoran Qiu, Junda Chen, Íñigo Goiri, Chaojie Zhang, Rayyan Shahid, Ramachandran Ramjee, Alexey Tumanov, Esha Choukse",http://arxiv.org/pdf/2409.17264v3,cs.LG
Disk2Planet: A Robust and Automated Machine Learning Tool for Parameter Inference in Disk-Planet Systems,"We introduce Disk2Planet, a machine learning-based tool to infer key
parameters in disk-planet systems from observed protoplanetary disk structures.
Disk2Planet takes as input the disk structures in the form of two-dimensional
density and velocity maps, and outputs disk and planet properties, that is, the
Shakura--Sunyaev viscosity, the disk aspect ratio, the planet--star mass ratio,
and the planet's radius and azimuth. We integrate the Covariance Matrix
Adaptation Evolution Strategy (CMA--ES), an evolutionary algorithm tailored for
complex optimization problems, and the Protoplanetary Disk Operator Network
(PPDONet), a neural network designed to predict solutions of disk--planet
interactions. Our tool is fully automated and can retrieve parameters in one
system in three minutes on an Nvidia A100 graphics processing unit. We
empirically demonstrate that our tool achieves percent-level or higher
accuracy, and is able to handle missing data and unknown levels of noise.",2024-09-25,"Shunyuan Mao, Ruobing Dong, Kwang Moo Yi, Lu Lu, Sifan Wang, Paris Perdikaris",http://arxiv.org/pdf/2409.17228v1,cs.LG
Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models,"Today's most advanced vision-language models (VLMs) remain proprietary. The
strongest open-weight models rely heavily on synthetic data from proprietary
VLMs to achieve good performance, effectively distilling these closed VLMs into
open ones. As a result, the community has been missing foundational knowledge
about how to build performant VLMs from scratch. We present Molmo, a new family
of VLMs that are state-of-the-art in their class of openness. Our key
contribution is a collection of new datasets called PixMo, including a dataset
of highly detailed image captions for pre-training, a free-form image Q&A
dataset for fine-tuning, and an innovative 2D pointing dataset, all collected
without the use of external VLMs. The success of our approach relies on careful
modeling choices, a well-tuned training pipeline, and, most critically, the
quality of our newly collected datasets. Our best-in-class 72B model not only
outperforms others in the class of open weight and data models, but also
outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini
1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and
on a large human evaluation. Our model weights, new datasets, and source code
are available at https://molmo.allenai.org/blog.",2024-09-25,"Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi",http://arxiv.org/pdf/2409.17146v2,cs.LG
DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion,"Leveraging pretrained 2D diffusion models and score distillation sampling
(SDS), recent methods have shown promising results for text-to-3D avatar
generation. However, generating high-quality 3D avatars capable of expressive
animation remains challenging. In this work, we present DreamWaltz-G, a novel
learning framework for animatable 3D avatar generation from text. The core of
this framework lies in Skeleton-guided Score Distillation and Hybrid 3D
Gaussian Avatar representation. Specifically, the proposed skeleton-guided
score distillation integrates skeleton controls from 3D human templates into 2D
diffusion models, enhancing the consistency of SDS supervision in terms of view
and human pose. This facilitates the generation of high-quality avatars,
mitigating issues such as multiple faces, extra limbs, and blurring. The
proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D
Gaussians, combining neural implicit fields and parameterized 3D meshes to
enable real-time rendering, stable SDS optimization, and expressive animation.
Extensive experiments demonstrate that DreamWaltz-G is highly effective in
generating and animating 3D avatars, outperforming existing methods in both
visual quality and animation expressiveness. Our framework further supports
diverse applications, including human video reenactment and multi-subject scene
composition.",2024-09-25,"Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, Xihui Liu",http://arxiv.org/pdf/2409.17145v1,cs.LG
Differential Privacy Regularization: Protecting Training Data Through Loss Function Regularization,"Training machine learning models based on neural networks requires large
datasets, which may contain sensitive information. The models, however, should
not expose private information from these datasets. Differentially private SGD
[DP-SGD] requires the modification of the standard stochastic gradient descent
[SGD] algorithm for training new models. In this short paper, a novel
regularization strategy is proposed to achieve the same goal in a more
efficient manner.",2024-09-25,"Francisco Aguilera-Martínez, Fernando Berzal",http://arxiv.org/pdf/2409.17144v1,cs.LG
FineZip : Pushing the Limits of Large Language Models for Practical Lossless Text Compression,"While the language modeling objective has been shown to be deeply connected
with compression, it is surprising that modern LLMs are not employed in
practical text compression systems. In this paper, we provide an in-depth
analysis of neural network and transformer-based compression techniques to
answer this question. We compare traditional text compression systems with
neural network and LLM-based text compression methods. Although LLM-based
systems significantly outperform conventional compression methods, they are
highly impractical. Specifically, LLMZip, a recent text compression system
using Llama3-8B requires 9.5 days to compress just 10 MB of text, although with
huge improvements in compression ratios. To overcome this, we present FineZip -
a novel LLM-based text compression system that combines ideas of online
memorization and dynamic context to reduce the compression time immensely.
FineZip can compress the above corpus in approximately 4 hours compared to 9.5
days, a 54 times improvement over LLMZip and comparable performance. FineZip
outperforms traditional algorithmic compression methods with a large margin,
improving compression ratios by approximately 50\%. With this work, we take the
first step towards making lossless text compression with LLMs a reality. While
FineZip presents a significant step in that direction, LLMs are still not a
viable solution for large-scale text compression. We hope our work paves the
way for future research and innovation to solve this problem.",2024-09-25,"Fazal Mittu, Yihuan Bu, Akshat Gupta, Ashok Devireddy, Alp Eren Ozdarendeli, Anant Singh, Gopala Anumanchipalli",http://arxiv.org/pdf/2409.17141v1,cs.LG
Learning with Dynamics: Autonomous Regulation of UAV Based Communication Networks with Dynamic UAV Crew,"Unmanned Aerial Vehicle (UAV) based communication networks (UCNs) are a key
component in future mobile networking. To handle the dynamic environments in
UCNs, reinforcement learning (RL) has been a promising solution attributed to
its strong capability of adaptive decision-making free of the environment
models. However, most existing RL-based research focus on control strategy
design assuming a fixed set of UAVs. Few works have investigated how UCNs
should be adaptively regulated when the serving UAVs change dynamically. This
article discusses RL-based strategy design for adaptive UCN regulation given a
dynamic UAV set, addressing both reactive strategies in general UCNs and
proactive strategies in solar-powered UCNs. An overview of the UCN and the RL
framework is first provided. Potential research directions with key challenges
and possible solutions are then elaborated. Some of our recent works are
presented as case studies to inspire innovative ways to handle dynamic UAV crew
with different RL algorithms.",2024-09-25,"Ran Zhang, Bowei Li, Liyuan Zhang, Jiang, Xie, Miao Wang",http://arxiv.org/pdf/2409.17139v1,cs.LG
Landscape of Policy Optimization for Finite Horizon MDPs with General State and Action,"Policy gradient methods are widely used in reinforcement learning. Yet, the
nonconvexity of policy optimization imposes significant challenges in
understanding the global convergence of policy gradient methods. For a class of
finite-horizon Markov Decision Processes (MDPs) with general state and action
spaces, we develop a framework that provides a set of easily verifiable
assumptions to ensure the Kurdyka-Lojasiewicz (KL) condition of the policy
optimization. Leveraging the KL condition, policy gradient methods converge to
the globally optimal policy with a non-asymptomatic rate despite nonconvexity.
Our results find applications in various control and operations models,
including entropy-regularized tabular MDPs, Linear Quadratic Regulator (LQR)
problems, stochastic inventory models, and stochastic cash balance problems,
for which we show an $\epsilon$-optimal policy can be obtained using a sample
size in $\tilde{\mathcal{O}}(\epsilon^{-1})$ and polynomial in terms of the
planning horizon by stochastic policy gradient methods. Our result establishes
the first sample complexity for multi-period inventory systems with
Markov-modulated demands and stochastic cash balance problems in the
literature.",2024-09-25,"Xin Chen, Yifan Hu, Minda Zhao",http://arxiv.org/pdf/2409.17138v1,cs.LG
PACE: Marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization,"Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained
transformers to downstream tasks. However, the optimization of tasks
performance often comes at the cost of generalizability in fine-tuned models.
To address this issue, we theoretically connect smaller weight gradient norms
during training and larger datasets to the improvements in model
generalization. Motivated by this connection, we propose reducing gradient
norms for enhanced generalization and aligning fine-tuned model with the
pre-trained counterpart to retain knowledge from large-scale pre-training data.
Yet, naive alignment does not guarantee gradient reduction and can potentially
cause gradient explosion, complicating efforts to manage gradients. To address
such an issue, we propose PACE, marrying generalization of PArameter-efficient
fine-tuning with Consistency rEgularization. We perturb features learned from
the adapter with the multiplicative noise and ensure the fine-tuned model
remains consistent for same sample under different perturbations. Theoretical
analysis shows that PACE not only implicitly regularizes gradients for enhanced
generalization, but also implicitly aligns the fine-tuned and pre-trained
models to retain knowledge. Experimental evidence supports our theories. PACE
surpasses existing PEFT methods in visual adaptation tasks (VTAB-1k, FGVC,
few-shot learning, domain adaptation) showcasing its potential for
resource-efficient fine-tuning. It also improves LoRA in text classification
(GLUE) and mathematical reasoning (GSM-8K). The code is available at
https://github.com/MaxwellYaoNi/PACE",2024-09-25,"Yao Ni, Shan Zhang, Piotr Koniusz",http://arxiv.org/pdf/2409.17137v4,cs.LG
"Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision, Physics Simulation, and a Robot with Reset","Generative AI systems have shown impressive capabilities in creating text,
code, and images. Inspired by the rich history of research in industrial
''Design for Assembly'', we introduce a novel problem: Generative
Design-for-Robot-Assembly (GDfRA). The task is to generate an assembly based on
a natural language prompt (e.g., ''giraffe'') and an image of available
physical components, such as 3D-printed blocks. The output is an assembly, a
spatial arrangement of these components, and instructions for a robot to build
this assembly. The output must 1) resemble the requested object and 2) be
reliably assembled by a 6 DoF robot arm with a suction gripper. We then present
Blox-Net, a GDfRA system that combines generative vision language models with
well-established methods in computer vision, simulation, perturbation analysis,
motion planning, and physical robot experimentation to solve a class of GDfRA
problems with minimal human supervision. Blox-Net achieved a Top-1 accuracy of
63.5% in the ''recognizability'' of its designed assemblies (eg, resembling
giraffe as judged by a VLM). These designs, after automated perturbation
redesign, were reliably assembled by a robot, achieving near-perfect success
across 10 consecutive assembly iterations with human intervention only during
reset prior to assembly. Surprisingly, this entire design process from textual
word (''giraffe'') to reliable physical assembly is performed with zero human
intervention.",2024-09-25,"Andrew Goldberg, Kavish Kondap, Tianshuang Qiu, Zehan Ma, Letian Fu, Justin Kerr, Huang Huang, Kaiyuan Chen, Kuan Fang, Ken Goldberg",http://arxiv.org/pdf/2409.17126v1,cs.LG
"Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Handy Appetizer","This book explores the role of Artificial Intelligence (AI), Machine Learning
(ML), and Deep Learning (DL) in driving the progress of big data analytics and
management. The book focuses on simplifying the complex mathematical concepts
behind deep learning, offering intuitive visualizations and practical case
studies to help readers understand how neural networks and technologies like
Convolutional Neural Networks (CNNs) work. It introduces several classic models
and technologies such as Transformers, GPT, ResNet, BERT, and YOLO,
highlighting their applications in fields like natural language processing,
image recognition, and autonomous driving. The book also emphasizes the
importance of pre-trained models and how they can enhance model performance and
accuracy, with instructions on how to apply these models in various real-world
scenarios. Additionally, it provides an overview of key big data management
technologies like SQL and NoSQL databases, as well as distributed computing
frameworks such as Apache Hadoop and Spark, explaining their importance in
managing and processing vast amounts of data. Ultimately, the book underscores
the value of mastering deep learning and big data management skills as critical
tools for the future workforce, making it an essential resource for both
beginners and experienced professionals.",2024-09-25,"Benji Peng, Xuanhe Pan, Yizhu Wen, Ziqian Bi, Keyu Chen, Ming Li, Ming Liu, Qian Niu, Junyu Liu, Jinlang Wang, Sen Zhang, Jiawei Xu, Pohsun Feng",http://arxiv.org/pdf/2409.17120v1,cs.LG
Programming Every Example: Lifting Pre-training Data Quality Like Experts at Scale,"Large language model pre-training has traditionally relied on human experts
to craft heuristics for improving the corpora quality, resulting in numerous
rules developed to date. However, these rules lack the flexibility to address
the unique characteristics of individual example effectively. Meanwhile,
applying tailored rules to every example is impractical for human experts. In
this paper, we demonstrate that even small language models, with as few as 0.3B
parameters, can exhibit substantial data refining capabilities comparable to
those of human experts. We introduce Programming Every Example (ProX), a novel
framework that treats data refinement as a programming task, enabling models to
refine corpora by generating and executing fine-grained operations, such as
string normalization, for each individual example at scale. Experimental
results show that models pre-trained on ProX-curated data outperform either
original data or data filtered by other selection methods by more than 2%
across various downstream benchmarks. Its effectiveness spans various model
sizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,
FineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in
domain-specific continual pre-training: without domain specific design, models
trained on OpenWebMath refined by ProX outperform human-crafted rule-based
methods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for
Llama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable
to models like Llemma-7B trained on 200B tokens. Further analysis highlights
that ProX significantly saves training FLOPs, offering a promising path for
efficient LLM pre-training. We are open-sourcing ProX with >500B corpus,
models, and sharing all training and implementation details for reproducible
research and future innovation. Code: https://github.com/GAIR-NLP/ProX",2024-09-25,"Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, Pengfei Liu",http://arxiv.org/pdf/2409.17115v2,cs.LG
Characterizing stable regions in the residual stream of LLMs,"We identify stable regions in the residual stream of Transformers, where the
model's output remains insensitive to small activation changes, but exhibits
high sensitivity at region boundaries. These regions emerge during training and
become more defined as training progresses or model size increases. The regions
appear to be much larger than previously studied polytopes. Our analysis
suggests that these stable regions align with semantic distinctions, where
similar prompts cluster within regions, and activations from the same region
lead to similar next token predictions. This work provides a promising research
direction for understanding the complexity of neural networks, shedding light
on training dynamics, and advancing interpretability.",2024-09-25,"Jett Janiak, Jacek Karwowski, Chatrik Singh Mangat, Giorgi Giglemiani, Nora Petrova, Stefan Heimersheim",http://arxiv.org/pdf/2409.17113v4,cs.LG
Non-asymptotic convergence analysis of the stochastic gradient Hamiltonian Monte Carlo algorithm with discontinuous stochastic gradient with applications to training of ReLU neural networks,"In this paper, we provide a non-asymptotic analysis of the convergence of the
stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm to a target
measure in Wasserstein-1 and Wasserstein-2 distance. Crucially, compared to the
existing literature on SGHMC, we allow its stochastic gradient to be
discontinuous. This allows us to provide explicit upper bounds, which can be
controlled to be arbitrarily small, for the expected excess risk of non-convex
stochastic optimization problems with discontinuous stochastic gradients,
including, among others, the training of neural networks with ReLU activation
function. To illustrate the applicability of our main results, we consider
numerical experiments on quantile estimation and on several optimization
problems involving ReLU neural networks relevant in finance and artificial
intelligence.",2024-09-25,"Luxu Liang, Ariel Neufeld, Ying Zhang",http://arxiv.org/pdf/2409.17107v2,cs.LG
Reinforcement Learning for Finite Space Mean-Field Type Games,"Mean field type games (MFTGs) describe Nash equilibria between large
coalitions: each coalition consists of a continuum of cooperative agents who
maximize the average reward of their coalition while interacting
non-cooperatively with a finite number of other coalitions. Although the theory
has been extensively developed, we are still lacking efficient and scalable
computational methods. Here, we develop reinforcement learning methods for such
games in a finite space setting with general dynamics and reward functions. We
start by proving that MFTG solution yields approximate Nash equilibria in
finite-size coalition games. We then propose two algorithms. The first is based
on quantization of mean-field spaces and Nash Q-learning. We provide
convergence and stability analysis. We then propose a deep reinforcement
learning algorithm, which can scale to larger spaces. Numerical experiments in
5 environments with mean-field distributions of dimension up to $200$ show the
scalability and efficiency of the proposed method.",2024-09-25,"Kai Shao, Jiacheng Shen, Chijie An, Mathieu Laurière",http://arxiv.org/pdf/2409.18152v2,cs.LG
Gradient Boosting Decision Trees on Medical Diagnosis over Tabular Data,"Medical diagnosis is a crucial task in the medical field, in terms of
providing accurate classification and respective treatments. Having
near-precise decisions based on correct diagnosis can affect a patient's life
itself, and may extremely result in a catastrophe if not classified correctly.
Several traditional machine learning (ML), such as support vector machines
(SVMs) and logistic regression, and state-of-the-art tabular deep learning (DL)
methods, including TabNet and TabTransformer, have been proposed and used over
tabular medical datasets. Additionally, due to the superior performances, lower
computational costs, and easier optimization over different tasks, ensemble
methods have been used in the field more recently. They offer a powerful
alternative in terms of providing successful medical decision-making processes
in several diagnosis tasks. In this study, we investigated the benefits of
ensemble methods, especially the Gradient Boosting Decision Tree (GBDT)
algorithms in medical classification tasks over tabular data, focusing on
XGBoost, CatBoost, and LightGBM. The experiments demonstrate that GBDT methods
outperform traditional ML and deep neural network architectures and have the
highest average rank over several benchmark tabular medical diagnosis datasets.
Furthermore, they require much less computational power compared to DL models,
creating the optimal methodology in terms of high performance and lower
complexity.",2024-09-25,"A. Yarkın Yıldız, Asli Kalayci",http://arxiv.org/pdf/2410.03705v3,cs.LG
Accumulator-Aware Post-Training Quantization,"Several recent studies have investigated low-precision accumulation,
reporting improvements in throughput, power, and area across various platforms.
However, the accompanying proposals have only considered the quantization-aware
training (QAT) paradigm, in which models are fine-tuned or trained from scratch
with quantization in the loop. As models continue to grow in size, QAT
techniques become increasingly more expensive, which has motivated the recent
surge in post-training quantization (PTQ) research. To the best of our
knowledge, ours marks the first formal study of accumulator-aware quantization
in the PTQ setting. To bridge this gap, we introduce AXE, a practical framework
of accumulator-aware extensions designed to endow overflow avoidance guarantees
to existing layer-wise PTQ algorithms. We theoretically motivate AXE and
demonstrate its flexibility by implementing it on top of two state-of-the-art
PTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage
accumulation for the first time, opening the door for full datapath
optimization and scaling to large language models (LLMs). We evaluate AXE
across image classification and language generation models, and observe
significant improvements in the trade-off between accumulator bit width and
model accuracy over baseline methods.",2024-09-25,"Ian Colbert, Fabian Grob, Giuseppe Franco, Jinjie Zhang, Rayan Saab",http://arxiv.org/pdf/2409.17092v1,cs.LG
Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence Classification,"In the medical field, the limited availability of large-scale datasets and
labor-intensive annotation processes hinder the performance of deep models.
Diffusion-based generative augmentation approaches present a promising solution
to this issue, having been proven effective in advancing downstream medical
recognition tasks. Nevertheless, existing works lack sufficient semantic and
sequential steerability for challenging video/3D sequence generation, and
neglect quality control of noisy synthesized samples, resulting in unreliable
synthetic databases and severely limiting the performance of downstream tasks.
In this work, we present Ctrl-GenAug, a novel and general generative
augmentation framework that enables highly semantic- and sequential-customized
sequence synthesis and suppresses incorrectly synthesized samples, to aid
medical sequence classification. Specifically, we first design a multimodal
conditions-guided sequence generator for controllably synthesizing
diagnosis-promotive samples. A sequential augmentation module is integrated to
enhance the temporal/stereoscopic coherence of generated samples. Then, we
propose a noisy synthetic data filter to suppress unreliable cases at semantic
and sequential levels. Extensive experiments on 3 medical datasets, using 11
networks trained on 3 paradigms, comprehensively analyze the effectiveness and
generality of Ctrl-GenAug, particularly in underrepresented high-risk
populations and out-domain conditions.",2024-09-25,"Xinrui Zhou, Yuhao Huang, Haoran Dou, Shijing Chen, Ao Chang, Jia Liu, Weiran Long, Jian Zheng, Erjiao Xu, Jie Ren, Ruobing Huang, Jun Cheng, Wufeng Xue, Dong Ni",http://arxiv.org/pdf/2409.17091v2,cs.LG
Locally Regularized Sparse Graph by Fast Proximal Gradient Descent,"Sparse graphs built by sparse representation has been demonstrated to be
effective in clustering high-dimensional data. Albeit the compelling empirical
performance, the vanilla sparse graph ignores the geometric information of the
data by performing sparse representation for each datum separately. In order to
obtain a sparse graph aligned with the local geometric structure of data, we
propose a novel Support Regularized Sparse Graph, abbreviated as SRSG, for data
clustering. SRSG encourages local smoothness on the neighborhoods of nearby
data points by a well-defined support regularization term. We propose a fast
proximal gradient descent method to solve the non-convex optimization problem
of SRSG with the convergence matching the Nesterov's optimal convergence rate
of first-order methods on smooth and convex objective function with Lipschitz
continuous gradient. Extensive experimental results on various real data sets
demonstrate the superiority of SRSG over other competing clustering methods.",2024-09-25,"Dongfang Sun, Yingzhen Yang",http://arxiv.org/pdf/2409.17090v1,cs.LG
SEN12-WATER: A New Dataset for Hydrological Applications and its Benchmarking,"Climate change and increasing droughts pose significant challenges to water
resource management around the world. These problems lead to severe water
shortages that threaten ecosystems, agriculture, and human communities. To
advance the fight against these challenges, we present a new dataset,
SEN12-WATER, along with a benchmark using a novel end-to-end Deep Learning (DL)
framework for proactive drought-related analysis. The dataset, identified as a
spatiotemporal datacube, integrates SAR polarization, elevation, slope, and
multispectral optical bands. Our DL framework enables the analysis and
estimation of water losses over time in reservoirs of interest, revealing
significant insights into water dynamics for drought analysis by examining
temporal changes in physical quantities such as water volume. Our methodology
takes advantage of the multitemporal and multimodal characteristics of the
proposed dataset, enabling robust generalization and advancing understanding of
drought, contributing to climate change resilience and sustainable water
resource management. The proposed framework involves, among the several
components, speckle noise removal from SAR data, a water body segmentation
through a U-Net architecture, the time series analysis, and the predictive
capability of a Time-Distributed-Convolutional Neural Network (TD-CNN). Results
are validated through ground truth data acquired on-ground via dedicated
sensors and (tailored) metrics, such as Precision, Recall, Intersection over
Union, Mean Squared Error, Structural Similarity Index Measure and Peak
Signal-to-Noise Ratio.",2024-09-25,"Luigi Russo, Francesco Mauro, Alessandro Sebastianelli, Paolo Gamba, Silvia Liberata Ullo",http://arxiv.org/pdf/2409.17087v1,cs.LG
Efficient Feature Interactions with Transformers: Improving User Spending Propensity Predictions in Gaming,"Dream11 is a fantasy sports platform that allows users to create their own
virtual teams for real-life sports events. We host multiple sports and matches
for our 200M+ user base. In this RMG (real money gaming) setting, users pay an
entry amount to participate in various contest products that we provide to
users. In our current work, we discuss the problem of predicting the user's
propensity to spend in a gaming round, so it can be utilized for various
downstream applications. e.g. Upselling users by incentivizing them marginally
as per their spending propensity, or personalizing the product listing based on
the user's propensity to spend.
  We aim to model the spending propensity of each user based on past
transaction data. In this paper, we benchmark tree-based and deep-learning
models that show good results on structured data, and we propose a new
architecture change that is specifically designed to capture the rich
interactions among the input features. We show that our proposed architecture
outperforms the existing models on the task of predicting the user's propensity
to spend in a gaming round. Our new transformer model surpasses the
state-of-the-art FT-Transformer, improving MAE by 2.5\% and MSE by 21.8\%.",2024-09-25,"Ved Prakash, Kartavya Kothari",http://arxiv.org/pdf/2409.17077v1,cs.LG
The Effect of Perceptual Metrics on Music Representation Learning for Genre Classification,"The subjective quality of natural signals can be approximated with objective
perceptual metrics. Designed to approximate the perceptual behaviour of human
observers, perceptual metrics often reflect structures found in natural signals
and neurological pathways. Models trained with perceptual metrics as loss
functions can capture perceptually meaningful features from the structures held
within these metrics. We demonstrate that using features extracted from
autoencoders trained with perceptual losses can improve performance on music
understanding tasks, i.e. genre classification, over using these metrics
directly as distances when learning a classifier. This result suggests improved
generalisation to novel signals when using perceptual metrics as loss functions
for representation learning.",2024-09-25,"Tashi Namgyal, Alexander Hepburn, Raul Santos-Rodriguez, Valero Laparra, Jesus Malo",http://arxiv.org/pdf/2409.17069v1,cs.LG
Benchmarking Domain Generalization Algorithms in Computational Pathology,"Deep learning models have shown immense promise in computational pathology
(CPath) tasks, but their performance often suffers when applied to unseen data
due to domain shifts. Addressing this requires domain generalization (DG)
algorithms. However, a systematic evaluation of DG algorithms in the CPath
context is lacking. This study aims to benchmark the effectiveness of 30 DG
algorithms on 3 CPath tasks of varying difficulty through 7,560
cross-validation runs. We evaluate these algorithms using a unified and robust
platform, incorporating modality-specific techniques and recent advances like
pretrained foundation models. Our extensive cross-validation experiments
provide insights into the relative performance of various DG strategies. We
observe that self-supervised learning and stain augmentation consistently
outperform other methods, highlighting the potential of pretrained models and
data augmentation. Furthermore, we introduce a new pan-cancer tumor detection
dataset (HISTOPANTUM) as a benchmark for future research. This study offers
valuable guidance to researchers in selecting appropriate DG approaches for
CPath tasks.",2024-09-25,"Neda Zamanitajeddin, Mostafa Jahanifar, Kesi Xu, Fouzia Siraj, Nasir Rajpoot",http://arxiv.org/pdf/2409.17063v1,cs.LG
First Place Solution to the ECCV 2024 BRAVO Challenge: Evaluating Robustness of Vision Foundation Models for Semantic Segmentation,"In this report, we present the first place solution to the ECCV 2024 BRAVO
Challenge, where a model is trained on Cityscapes and its robustness is
evaluated on several out-of-distribution datasets. Our solution leverages the
powerful representations learned by vision foundation models, by attaching a
simple segmentation decoder to DINOv2 and fine-tuning the entire model. This
approach outperforms more complex existing approaches, and achieves first place
in the challenge. Our code is publicly available at
https://github.com/tue-mps/benchmark-vfm-ss.",2024-09-25,"Tommie Kerssies, Daan de Geus, Gijs Dubbelman",http://arxiv.org/pdf/2409.17208v2,cs.LG
DRIM: Learning Disentangled Representations from Incomplete Multimodal Healthcare Data,"Real-life medical data is often multimodal and incomplete, fueling the
growing need for advanced deep learning models capable of integrating them
efficiently. The use of diverse modalities, including histopathology slides,
MRI, and genetic data, offers unprecedented opportunities to improve prognosis
prediction and to unveil new treatment pathways. Contrastive learning, widely
used for deriving representations from paired data in multimodal tasks, assumes
that different views contain the same task-relevant information and leverages
only shared information. This assumption becomes restrictive when handling
medical data since each modality also harbors specific knowledge relevant to
downstream tasks. We introduce DRIM, a new multimodal method for capturing
these shared and unique representations, despite data sparsity. More
specifically, given a set of modalities, we aim to encode a representation for
each one that can be divided into two components: one encapsulating
patient-related information common across modalities and the other,
encapsulating modality-specific details. This is achieved by increasing the
shared information among different patient modalities while minimizing the
overlap between shared and unique components within each modality. Our method
outperforms state-of-the-art algorithms on glioma patients survival prediction
tasks, while being robust to missing modalities. To promote reproducibility,
the code is made publicly available at https://github.com/Lucas-rbnt/DRIM",2024-09-25,"Lucas Robinet, Ahmad Berjaoui, Ziad Kheil, Elizabeth Cohen-Jonathan Moyal",http://arxiv.org/pdf/2409.17055v2,cs.LG
Predictive Covert Communication Against Multi-UAV Surveillance Using Graph Koopman Autoencoder,"Low Probability of Detection (LPD) communication aims to obscure the presence
of radio frequency (RF) signals to evade surveillance. In the context of mobile
surveillance utilizing unmanned aerial vehicles (UAVs), achieving LPD
communication presents significant challenges due to the UAVs' rapid and
continuous movements, which are characterized by unknown nonlinear dynamics.
Therefore, accurately predicting future locations of UAVs is essential for
enabling real-time LPD communication. In this paper, we introduce a novel
framework termed predictive covert communication, aimed at minimizing
detectability in terrestrial ad-hoc networks under multi-UAV surveillance. Our
data-driven method synergistically integrates graph neural networks (GNN) with
Koopman theory to model the complex interactions within a multi-UAV network and
facilitating long-term predictions by linearizing the dynamics, even with
limited historical data. Extensive simulation results substantiate that the
predicted trajectories using our method result in at least 63%-75% lower
probability of detection when compared to well-known state-of-the-art baseline
approaches, showing promise in enabling low-latency covert operations in
practical scenarios.",2024-09-25,"Sivaram Krishnan, Jihong Park, Gregory Sherman, Benjamin Campbell, Jinho Choi",http://arxiv.org/pdf/2409.17048v1,cs.LG
How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not,"The remarkable performance achieved by Large Language Models (LLM) has driven
research efforts to leverage them for a wide range of tasks and input
modalities. In speech-to-text (S2T) tasks, the emerging solution consists of
projecting the output of the encoder of a Speech Foundational Model (SFM) into
the LLM embedding space through an adapter module. However, no work has yet
investigated how much the downstream-task performance depends on each component
(SFM, adapter, LLM) nor whether the best design of the adapter depends on the
chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter
modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on
two widespread S2T tasks, namely Automatic Speech Recognition and Speech
Translation. Our results demonstrate that the SFM plays a pivotal role in
downstream performance, while the adapter choice has moderate impact and
depends on the SFM and LLM.",2024-09-25,"Francesco Verdini, Pierfrancesco Melucci, Stefano Perna, Francesco Cariaggi, Marco Gaido, Sara Papi, Szymon Mazurek, Marek Kasztelnik, Luisa Bentivogli, Sébastien Bratières, Paolo Merialdo, Simone Scardapane",http://arxiv.org/pdf/2409.17044v2,cs.LG
Counterfactual Token Generation in Large Language Models,"""Sure, I am happy to generate a story for you: Captain Lyra stood at the helm
of her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]
Lyra's eyes welled up with tears as she realized the bitter truth - she had
sacrificed everything for fleeting riches, and lost the love of her crew, her
family, and herself."" Although this story, generated by a large language model,
is captivating, one may wonder -- how would the story have unfolded if the
model had chosen ""Captain Maeve"" as the protagonist instead? We cannot know.
State-of-the-art large language models are stateless -- they maintain no
internal memory or state. Given a prompt, they generate a sequence of tokens as
an output using an autoregressive process. As a consequence, they cannot reason
about counterfactual alternatives to tokens they have generated in the past. In
this work, our goal is to enhance them with this functionality. To this end, we
develop a causal model of token generation that builds upon the Gumbel-Max
structural causal model. Our model allows any large language model to perform
counterfactual token generation at almost no cost in comparison with vanilla
token generation, it is embarrassingly simple to implement, and it does not
require any fine-tuning nor prompt engineering. We implement our model on Llama
3 8B-Instruct and Ministral-8B-Instruct and conduct a qualitative and a
quantitative analysis of counterfactually generated text. We conclude with a
demonstrative application of counterfactual token generation for bias
detection, unveiling interesting insights about the model of the world
constructed by large language models.",2024-09-25,"Ivi Chatzi, Nina Corvelo Benz, Eleni Straitouri, Stratis Tsirtsis, Manuel Gomez-Rodriguez",http://arxiv.org/pdf/2409.17027v3,cs.LG
CombU: A Combined Unit Activation for Fitting Mathematical Expressions with Neural Networks,"The activation functions are fundamental to neural networks as they introduce
non-linearity into data relationships, thereby enabling deep networks to
approximate complex data relations. Existing efforts to enhance neural network
performance have predominantly focused on developing new mathematical
functions. However, we find that a well-designed combination of existing
activation functions within a neural network can also achieve this objective.
In this paper, we introduce the Combined Units activation (CombU), which
employs different activation functions at various dimensions across different
layers. This approach can be theoretically proven to fit most mathematical
expressions accurately. The experiments conducted on four mathematical
expression datasets, compared against six State-Of-The-Art (SOTA) activation
function algorithms, demonstrate that CombU outperforms all SOTA algorithms in
10 out of 16 metrics and ranks in the top three for the remaining six metrics.",2024-09-25,"Jiayu Li, Zilong Zhao, Kevin Yee, Uzair Javaid, Biplab Sikdar",http://arxiv.org/pdf/2409.17021v1,cs.LG
CNN Mixture-of-Depths,"We introduce Mixture-of-Depths (MoD) for Convolutional Neural Networks
(CNNs), a novel approach that enhances the computational efficiency of CNNs by
selectively processing channels based on their relevance to the current
prediction. This method optimizes computational resources by dynamically
selecting key channels in feature maps for focused processing within the
convolutional blocks (Conv-Blocks), while skipping less relevant channels.
Unlike conditional computation methods that require dynamic computation graphs,
CNN MoD uses a static computation graph with fixed tensor sizes which improve
hardware efficiency. It speeds up the training and inference processes without
the need for customized CUDA kernels, unique loss functions, or finetuning. CNN
MoD either matches the performance of traditional CNNs with reduced inference
times, GMACs, and parameters, or exceeds their performance while maintaining
similar inference times, GMACs, and parameters. For example, on ImageNet,
ResNet86-MoD exceeds the performance of the standard ResNet50 by 0.45% with a
6% speedup on CPU and 5% on GPU. Moreover, ResNet75-MoD achieves the same
performance as ResNet50 with a 25% speedup on CPU and 15% on GPU.",2024-09-25,"Rinor Cakaj, Jens Mehnert, Bin Yang",http://arxiv.org/pdf/2409.17016v1,cs.LG
Immersion and Invariance-based Coding for Privacy-Preserving Federated Learning,"Federated learning (FL) has emerged as a method to preserve privacy in
collaborative distributed learning. In FL, clients train AI models directly on
their devices rather than sharing data with a centralized server, which can
pose privacy risks. However, it has been shown that despite FL's partial
protection of local data privacy, information about clients' data can still be
inferred from shared model updates during training. In recent years, several
privacy-preserving approaches have been developed to mitigate this privacy
leakage in FL, though they often provide privacy at the cost of model
performance or system efficiency. Balancing these trade-offs presents a
significant challenge in implementing FL schemes. In this manuscript, we
introduce a privacy-preserving FL framework that combines differential privacy
and system immersion tools from control theory. The core idea is to treat the
optimization algorithms used in standard FL schemes (e.g., gradient-based
algorithms) as a dynamical system that we seek to immerse into a
higher-dimensional system (referred to as the target optimization algorithm).
The target algorithm's dynamics are designed such that, first, the model
parameters of the original algorithm are immersed in its parameters; second, it
operates on distorted parameters; and third, it converges to an encoded version
of the true model parameters from the original algorithm. These encoded
parameters can then be decoded at the server to retrieve the original model
parameters. We demonstrate that the proposed privacy-preserving scheme can be
tailored to offer any desired level of differential privacy for both local and
global model parameters, while maintaining the same accuracy and convergence
rate as standard FL algorithms.",2024-09-25,"Haleh Hayati, Carlos Murguia, Nathan van de Wouw",http://arxiv.org/pdf/2409.17201v2,cs.LG
PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in Endoscopic Pituitary Surgery,"Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow
for anaesthetists to more accurately decide when to administer anaesthetic
agents and drugs, as well as to notify hospital staff to send in the next
patient. Therefore RSD plays an important role in improving patient care and
minimising surgical theatre costs via efficient scheduling. In endoscopic
pituitary surgery, it is uniquely challenging due to variable workflow
sequences with a selection of optional steps contributing to high variability
in surgery duration. This paper presents PitRSDNet for predicting RSD during
pituitary surgery, a spatio-temporal neural network model that learns from
historical data focusing on workflow sequences. PitRSDNet integrates workflow
knowledge into RSD prediction in two forms: 1) multi-task learning for
concurrently predicting step and RSD; and 2) incorporating prior steps as
context in temporal learning and inference. PitRSDNet is trained and evaluated
on a new endoscopic pituitary surgery dataset with 88 videos to show
competitive performance improvements over previous statistical and machine
learning methods. The findings also highlight how PitRSDNet improve RSD
precision on outlier cases utilising the knowledge of prior steps.",2024-09-25,"Anjana Wijekoon, Adrito Das, Roxana R. Herrera, Danyal Z. Khan, John Hanrahan, Eleanor Carter, Valpuri Luoma, Danail Stoyanov, Hani J. Marcus, Sophia Bano",http://arxiv.org/pdf/2409.16998v2,cs.LG
INT-FlashAttention: Enabling Flash Attention for INT8 Quantization,"As the foundation of large language models (LLMs), self-attention module
faces the challenge of quadratic time and memory complexity with respect to
sequence length. FlashAttention accelerates attention computation and reduces
its memory usage by leveraging the GPU memory hierarchy. A promising research
direction is to integrate FlashAttention with quantization methods. This paper
introduces INT-FlashAttention, the first INT8 quantization architecture
compatible with the forward workflow of FlashAttention, which significantly
improves the inference speed of FlashAttention on Ampere GPUs. We implement our
INT-FlashAttention prototype with fully INT8 activations and general
matrix-multiplication (GEMM) kernels, making it the first attention operator
with fully INT8 input. As a general token-level post-training quantization
framework, INT-FlashAttention is also compatible with other data formats like
INT4, etc. Experimental results show INT-FlashAttention achieves 72% faster
inference speed and 82% smaller quantization error compared to standard
FlashAttention with FP16 and FP8 data format.",2024-09-25,"Shimao Chen, Zirui Liu, Zhiying Wu, Ce Zheng, Peizhuang Cong, Zihan Jiang, Yuhan Wu, Lei Su, Tong Yang",http://arxiv.org/pdf/2409.16997v2,cs.LG
What is the relation between Slow Feature Analysis and the Successor Representation?,"Slow feature analysis (SFA) is an unsupervised method for extracting
representations from time series data. The successor representation (SR) is a
method for representing states in a Markov decision process (MDP) based on
transition statistics. While SFA and SR stem from distinct areas of machine
learning, they share important properties, both in terms of their mathematics
and the types of information they are sensitive to. This work studies their
connection along these two axes. In particular, both SFA and SR are explored
analytically, and in the setting of a one-hot encoded MDP, a formal equivalence
is demonstrated in terms of the grid-like representations that occur as
solutions/eigenvectors. Moreover, it is shown that the columns of the matrices
involved in SFA contain place-like representations, which are formally distinct
from place-cell models that have already been defined using SFA.",2024-09-25,"Eddie Seabrook, Laurenz Wiskott",http://arxiv.org/pdf/2409.16991v2,cs.LG
A Visual-Analytical Approach for Automatic Detection of Cyclonic Events in Satellite Observations,"Estimating the location and intensity of tropical cyclones holds crucial
significance for predicting catastrophic weather events. In this study, we
approach this task as a detection and regression challenge, specifically over
the North Indian Ocean (NIO) region where best tracks location and wind speed
information serve as the labels. The current process for cyclone detection and
intensity estimation involves physics-based simulation studies which are
time-consuming, only using image features will automate the process for
significantly faster and more accurate predictions. While conventional methods
typically necessitate substantial prior knowledge for training, we are
exploring alternative approaches to enhance efficiency. This research aims to
focus specifically on cyclone detection, intensity estimation and related
aspects using only image input and data-driven approaches and will lead to
faster inference time and automate the process as opposed to current NWP models
being utilized at SAC. In context to algorithm development, a novel two stage
detection and intensity estimation module is proposed. In the first level
detection we try to localize the cyclone over an entire image as captured by
INSAT3D over the NIO (North Indian Ocean). For the intensity estimation task,
we propose a CNN-LSTM network, which works on the cyclone centered images,
utilizing a ResNet-18 backbone, by which we are able to capture both temporal
and spatial characteristics.",2024-09-25,"Akash Agrawal, Mayesh Mohapatra, Abhinav Raja, Paritosh Tiwari, Vishwajeet Pattanaik, Neeru Jaiswal, Arpit Agarwal, Punit Rathore",http://arxiv.org/pdf/2410.08218v1,cs.LG
Application of AI-based Models for Online Fraud Detection and Analysis,"Fraud is a prevalent offence that extends beyond financial loss, causing
psychological and physical harm to victims. The advancements in online
communication technologies alowed for online fraud to thrive in this vast
network, with fraudsters increasingly using these channels for deception. With
the progression of technologies like AI, there is a growing concern that fraud
will scale up, using sophisticated methods, like deep-fakes in phishing
campaigns, all generated by language generation models like ChatGPT. However,
the application of AI in detecting and analyzing online fraud remains
understudied. We conduct a Systematic Literature Review on AI and NLP
techniques for online fraud detection. The review adhered the PRISMA-ScR
protocol, with eligibility criteria including relevance to online fraud, use of
text data, and AI methodologies. We screened 2,457 academic records, 350 met
our eligibility criteria, and included 223. We report the state-of-the-art NLP
techniques for analysing various online fraud categories; the training data
sources; the NLP algorithms and models built; and the performance metrics
employed for model evaluation. We find that current research on online fraud is
divided into various scam activitiesand identify 16 different frauds that
researchers focus on. This SLR enhances the academic understanding of AI-based
detection methods for online fraud and offers insights for policymakers, law
enforcement, and businesses on safeguarding against such activities. We
conclude that focusing on specific scams lacks generalization, as multiple
models are required for different fraud types. The evolving nature of scams
limits the effectiveness of models trained on outdated data. We also identify
issues in data limitations, training bias reporting, and selective presentation
of metrics in model performance reporting, which can lead to potential biases
in model evaluation.",2024-09-25,"Antonis Papasavva, Shane Johnson, Ed Lowther, Samantha Lundrigan, Enrico Mariconti, Anna Markovska, Nilufer Tuptuk",http://arxiv.org/pdf/2409.19022v2,cs.LG
Towards User-Focused Research in Training Data Attribution for Human-Centered Explainable AI,"While Explainable AI (XAI) aims to make AI understandable and useful to
humans, it has been criticised for relying too much on formalism and
solutionism, focusing more on mathematical soundness than user needs. We
propose an alternative to this bottom-up approach inspired by design thinking:
the XAI research community should adopt a top-down, user-focused perspective to
ensure user relevance. We illustrate this with a relatively young subfield of
XAI, Training Data Attribution (TDA). With the surge in TDA research and
growing competition, the field risks repeating the same patterns of
solutionism. We conducted a needfinding study with a diverse group of AI
practitioners to identify potential user needs related to TDA. Through
interviews (N=10) and a systematic survey (N=31), we uncovered new TDA tasks
that are currently largely overlooked. We invite the TDA and XAI communities to
consider these novel tasks and improve the user relevance of their research
outcomes.",2024-09-25,"Elisa Nguyen, Johannes Bertram, Evgenii Kortukov, Jean Y. Song, Seong Joon Oh",http://arxiv.org/pdf/2409.16978v1,cs.LG
MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and Improved GRU,"As financial markets grow increasingly complex in the big data era, accurate
stock prediction has become more critical. Traditional time series models, such
as GRUs, have been widely used but often struggle to capture the intricate
nonlinear dynamics of markets, particularly in the flexible selection and
effective utilization of key historical information. Recently, methods like
Graph Neural Networks and Reinforcement Learning have shown promise in stock
prediction but require high data quality and quantity, and they tend to exhibit
instability when dealing with data sparsity and noise. Moreover, the training
and inference processes for these models are typically complex and
computationally expensive, limiting their broad deployment in practical
applications. Existing approaches also generally struggle to capture
unobservable latent market states effectively, such as market sentiment and
expectations, microstructural factors, and participant behavior patterns,
leading to an inadequate understanding of market dynamics and subsequently
impact prediction accuracy. To address these challenges, this paper proposes a
stock prediction model, MCI-GRU, based on a multi-head cross-attention
mechanism and an improved GRU. First, we enhance the GRU model by replacing the
reset gate with an attention mechanism, thereby increasing the model's
flexibility in selecting and utilizing historical information. Second, we
design a multi-head cross-attention mechanism for learning unobservable latent
market state representations, which are further enriched through interactions
with both temporal features and cross-sectional features. Finally, extensive
experiments on four main stock markets show that the proposed method
outperforms SOTA techniques across multiple metrics. Additionally, its
successful application in real-world fund management operations confirms its
effectiveness and practicality.",2024-09-25,"Peng Zhu, Yuante Li, Yifan Hu, Sheng Xiang, Qinyuan Liu, Dawei Cheng, Yuqi Liang",http://arxiv.org/pdf/2410.20679v2,cs.LG
Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM Personalization,"Large language models (LLMs) have revolutionized how we interact with
technology, but their personalization to individual user preferences remains a
significant challenge, particularly in on-device applications. Traditional
methods often depend heavily on labeled datasets and can be resource-intensive.
To address these issues, we present Adaptive Self-Supervised Learning
Strategies (ASLS), which utilizes self-supervised learning techniques to
personalize LLMs dynamically. The framework comprises a user profiling layer
for collecting interaction data and a neural adaptation layer for real-time
model fine-tuning. This innovative approach enables continuous learning from
user feedback, allowing the model to generate responses that align closely with
user-specific contexts. The adaptive mechanisms of ASLS minimize computational
demands and enhance personalization efficiency. Experimental results across
various user scenarios illustrate the superior performance of ASLS in boosting
user engagement and satisfaction, highlighting its potential to redefine LLMs
as highly responsive and context-aware systems on-device.",2024-09-25,"Rafael Mendoza, Isabella Cruz, Richard Liu, Aarav Deshmukh, David Williams, Jesscia Peng, Rohan Iyer",http://arxiv.org/pdf/2409.16973v1,cs.LG
A random measure approach to reinforcement learning in continuous time,"We present a random measure approach for modeling exploration, i.e., the
execution of measure-valued controls, in continuous-time reinforcement learning
(RL) with controlled diffusion and jumps. First, we consider the case when
sampling the randomized control in continuous time takes place on a
discrete-time grid and reformulate the resulting stochastic differential
equation (SDE) as an equation driven by suitable random measures. The
construction of these random measures makes use of the Brownian motion and the
Poisson random measure (which are the sources of noise in the original model
dynamics) as well as the additional random variables, which are sampled on the
grid for the control execution. Then, we prove a limit theorem for these random
measures as the mesh-size of the sampling grid goes to zero, which leads to the
grid-sampling limit SDE that is jointly driven by white noise random measures
and a Poisson random measure. We also argue that the grid-sampling limit SDE
can substitute the exploratory SDE and the sample SDE of the recent
continuous-time RL literature, i.e., it can be applied for the theoretical
analysis of exploratory control problems and for the derivation of learning
algorithms.",2024-09-25,"Christian Bender, Nguyen Tran Thuan",http://arxiv.org/pdf/2409.17200v1,cs.LG
Bridge to Real Environment with Hardware-in-the-loop for Wireless Artificial Intelligence Paradigms,"Nowadays, many machine learning (ML) solutions to improve the wireless
standard IEEE802.11p for Vehicular Adhoc Network (VANET) are commonly evaluated
in the simulated world. At the same time, this approach could be cost-effective
compared to real-world testing due to the high cost of vehicles. There is a
risk of unexpected outcomes when these solutions are implemented in the real
world, potentially leading to wasted resources. To mitigate this challenge, the
hardware-in-the-loop is the way to move forward as it enables the opportunity
to test in the real world and simulated worlds together. Therefore, we have
developed what we believe is the pioneering hardware-in-the-loop for testing
artificial intelligence, multiple services, and HD map data (LiDAR), in both
simulated and real-world settings.",2024-09-25,"Jeffrey Redondo, Nauman Aslam, Juan Zhang, Zhenhui Yuan",http://arxiv.org/pdf/2409.16968v1,cs.LG
ABCFair: an Adaptable Benchmark approach for Comparing Fairness Methods,"Numerous methods have been implemented that pursue fairness with respect to
sensitive features by mitigating biases in machine learning. Yet, the problem
settings that each method tackles vary significantly, including the stage of
intervention, the composition of sensitive features, the fairness notion, and
the distribution of the output. Even in binary classification, these subtle
differences make it highly complicated to benchmark fairness methods, as their
performance can strongly depend on exactly how the bias mitigation problem was
originally framed.
  Hence, we introduce ABCFair, a benchmark approach which allows adapting to
the desiderata of the real-world problem setting, enabling proper comparability
between methods for any use case. We apply ABCFair to a range of pre-, in-, and
postprocessing methods on both large-scale, traditional datasets and on a dual
label (biased and unbiased) dataset to sidestep the fairness-accuracy
trade-off.",2024-09-25,"MaryBeth Defrance, Maarten Buyl, Tijl De Bie",http://arxiv.org/pdf/2409.16965v2,cs.LG
Informed deep hierarchical classification: a non-standard analysis inspired approach,"This work proposes a novel approach to the deep hierarchical classification
task, i.e., the problem of classifying data according to multiple labels
organized in a rigid parent-child structure. It consists in a multi-output deep
neural network equipped with specific projection operators placed before each
output layer. The design of such an architecture, called lexicographic hybrid
deep neural network (LH-DNN), has been possible by combining tools from
different and quite distant research fields: lexicographic multi-objective
optimization, non-standard analysis, and deep learning. To assess the efficacy
of the approach, the resulting network is compared against the B-CNN, a
convolutional neural network tailored for hierarchical classification tasks, on
the CIFAR10, CIFAR100 (where it has been originally and recently proposed
before being adopted and tuned for multiple real-world applications) and
Fashion-MNIST benchmarks. Evidence states that an LH-DNN can achieve comparable
if not superior performance, especially in the learning of the hierarchical
relations, in the face of a drastic reduction of the learning parameters,
training epochs, and computational time, without the need for ad-hoc loss
functions weighting values.",2024-09-25,"Lorenzo Fiaschi, Marco Cococcioni",http://arxiv.org/pdf/2409.16956v2,cs.LG
Dynamic Obstacle Avoidance through Uncertainty-Based Adaptive Planning with Diffusion,"By framing reinforcement learning as a sequence modeling problem, recent work
has enabled the use of generative models, such as diffusion models, for
planning. While these models are effective in predicting long-horizon state
trajectories in deterministic environments, they face challenges in dynamic
settings with moving obstacles. Effective collision avoidance demands
continuous monitoring and adaptive decision-making. While replanning at every
timestep could ensure safety, it introduces substantial computational overhead
due to the repetitive prediction of overlapping state sequences -- a process
that is particularly costly with diffusion models, known for their intensive
iterative sampling procedure. We propose an adaptive generative planning
approach that dynamically adjusts replanning frequency based on the uncertainty
of action predictions. Our method minimizes the need for frequent,
computationally expensive, and redundant replanning while maintaining robust
collision avoidance performance. In experiments, we obtain a 13.5% increase in
the mean trajectory length and a 12.7% increase in mean reward over
long-horizon planning, indicating a reduction in collision rates and an
improved ability to navigate the environment safely.",2024-09-25,"Vineet Punyamoorty, Pascal Jutras-Dubé, Ruqi Zhang, Vaneet Aggarwal, Damon Conover, Aniket Bera",http://arxiv.org/pdf/2409.16950v1,cs.LG
Decomposition of Equivariant Maps via Invariant Maps: Application to Universal Approximation under Symmetry,"In this paper, we develop a theory about the relationship between invariant
and equivariant maps with regard to a group $G$. We then leverage this theory
in the context of deep neural networks with group symmetries in order to obtain
novel insight into their mechanisms. More precisely, we establish a one-to-one
relationship between equivariant maps and certain invariant maps. This allows
us to reduce arguments for equivariant maps to those for invariant maps and
vice versa. As an application, we propose a construction of universal
equivariant architectures built from universal invariant networks. We, in turn,
explain how the universal architectures arising from our construction differ
from standard equivariant architectures known to be universal. Furthermore, we
explore the complexity, in terms of the number of free parameters, of our
models, and discuss the relation between invariant and equivariant networks'
complexity. Finally, we also give an approximation rate for G-equivariant deep
neural networks with ReLU activation functions for finite group G.",2024-09-25,"Akiyoshi Sannai, Yuuki Takai, Matthieu Cordonnier",http://arxiv.org/pdf/2409.16922v1,cs.LG
Discriminative Anchor Learning for Efficient Multi-view Clustering,"Multi-view clustering aims to study the complementary information across
views and discover the underlying structure. For solving the relatively high
computational cost for the existing approaches, works based on anchor have been
presented recently. Even with acceptable clustering performance, these methods
tend to map the original representation from multiple views into a fixed shared
graph based on the original dataset. However, most studies ignore the
discriminative property of the learned anchors, which ruin the representation
capability of the built model. Moreover, the complementary information among
anchors across views is neglected to be ensured by simply learning the shared
anchor graph without considering the quality of view-specific anchors. In this
paper, we propose discriminative anchor learning for multi-view clustering
(DALMC) for handling the above issues. We learn discriminative view-specific
feature representations according to the original dataset and build anchors
from different views based on these representations, which increase the quality
of the shared anchor graph. The discriminative feature learning and consensus
anchor graph construction are integrated into a unified framework to improve
each other for realizing the refinement. The optimal anchors from multiple
views and the consensus anchor graph are learned with the orthogonal
constraints. We give an iterative algorithm to deal with the formulated
problem. Extensive experiments on different datasets show the effectiveness and
efficiency of our method compared with other methods.",2024-09-25,"Yalan Qin, Nan Pu, Hanzhou Wu, Nicu Sebe",http://arxiv.org/pdf/2409.16904v1,cs.LG
Revisiting Space Mission Planning: A Reinforcement Learning-Guided Approach for Multi-Debris Rendezvous,"This research introduces a novel application of a masked Proximal Policy
Optimization (PPO) algorithm from the field of deep reinforcement learning
(RL), for determining the most efficient sequence of space debris visitation,
utilizing the Lambert solver as per Izzo's adaptation for individual
rendezvous. The aim is to optimize the sequence in which all the given debris
should be visited to get the least total time for rendezvous for the entire
mission. A neural network (NN) policy is developed, trained on simulated space
missions with varying debris fields. After training, the neural network
calculates approximately optimal paths using Izzo's adaptation of Lambert
maneuvers. Performance is evaluated against standard heuristics in mission
planning. The reinforcement learning approach demonstrates a significant
improvement in planning efficiency by optimizing the sequence for debris
rendezvous, reducing the total mission time by an average of approximately
{10.96\%} and {13.66\%} compared to the Genetic and Greedy algorithms,
respectively. The model on average identifies the most time-efficient sequence
for debris visitation across various simulated scenarios with the fastest
computational speed. This approach signifies a step forward in enhancing
mission planning strategies for space debris clearance.",2024-09-25,"Agni Bandyopadhyay, Guenther Waxenegger-Wilfing",http://arxiv.org/pdf/2409.16882v1,cs.LG
An Analysis of Minimum Error Entropy Loss Functions in Wireless Communications,"This paper introduces the minimum error entropy (MEE) criterion as an
advanced information-theoretic loss function tailored for deep learning
applications in wireless communications. The MEE criterion leverages
higher-order statistical properties, offering robustness in noisy scenarios
like Rayleigh fading and impulsive interference. In addition, we propose a less
computationally complex version of the MEE function to enhance practical
usability in wireless communications. The method is evaluated through
simulations on two critical applications: over-the-air regression and indoor
localization. Results indicate that the MEE criterion outperforms conventional
loss functions, such as mean squared error (MSE) and mean absolute error (MAE),
achieving significant performance improvements in terms of accuracy, over $20
\%$ gain over traditional methods, and convergence speed across various channel
conditions. This work establishes MEE as a promising alternative for wireless
communication tasks in deep learning models, enabling better resilience and
adaptability.",2024-09-25,"Rumeshika Pallewela, Eslam Eldeeb, Hirley Alves",http://arxiv.org/pdf/2410.07208v1,cs.LG
Feedforward Controllers from Learned Dynamic Local Model Networks with Application to Excavator Assistance Functions,"Complicated first principles modelling and controller synthesis can be
prohibitively slow and expensive for high-mix, low-volume products such as
hydraulic excavators. Instead, in a data-driven approach, recorded trajectories
from the real system can be used to train local model networks (LMNs), for
which feedforward controllers are derived via feedback linearization. However,
previous works required LMNs without zero dynamics for feedback linearization,
which restricts the model structure and thus modelling capacity of LMNs. In
this paper, we overcome this restriction by providing a criterion for when
feedback linearization of LMNs with zero dynamics yields a valid controller. As
a criterion we propose the bounded-input bounded-output stability of the
resulting controller. In two additional contributions, we extend this approach
to consider measured disturbance signals and multiple inputs and outputs. We
illustrate the effectiveness of our contributions in a hydraulic excavator
control application with hardware experiments. To this end, we train LMNs from
recorded, noisy data and derive feedforward controllers used as part of a
leveling assistance system on the excavator. In our experiments, incorporating
disturbance signals and multiple inputs and outputs enhances tracking
performance of the learned controller. A video of our experiments is available
at https://youtu.be/lrrWBx2ASaE.",2024-09-25,"Leon Greiser, Ozan Demir, Benjamin Hartmann, Henrik Hose, Sebastian Trimpe",http://arxiv.org/pdf/2409.16875v1,cs.LG
Ethical and Scalable Automation: A Governance and Compliance Framework for Business Applications,"The popularisation of applying AI in businesses poses significant challenges
relating to ethical principles, governance, and legal compliance. Although
businesses have embedded AI into their day-to-day processes, they lack a
unified approach for mitigating its potential risks. This paper introduces a
framework ensuring that AI must be ethical, controllable, viable, and
desirable. Balancing these factors ensures the design of a framework that
addresses its trade-offs, such as balancing performance against explainability.
A successful framework provides practical advice for businesses to meet
regulatory requirements in sectors such as finance and healthcare, where it is
critical to comply with standards like GPDR and the EU AI Act. Different case
studies validate this framework by integrating AI in both academic and
practical environments. For instance, large language models are cost-effective
alternatives for generating synthetic opinions that emulate attitudes to
environmental issues. These case studies demonstrate how having a structured
framework could enhance transparency and maintain performance levels as shown
from the alignment between synthetic and expected distributions. This alignment
is quantified using metrics like Chi-test scores, normalized mutual
information, and Jaccard indexes. Future research should explore the
framework's empirical validation in diverse industrial settings further,
ensuring the model's scalability and adaptability.",2024-09-25,Haocheng Lin,http://arxiv.org/pdf/2409.16872v2,cs.LG
Quantifying Visual Properties of GAM Shape Plots: Impact on Perceived Cognitive Load and Interpretability,"Generalized Additive Models (GAMs) offer a balance between performance and
interpretability in machine learning. The interpretability aspect of GAMs is
expressed through shape plots, representing the model's decision-making
process. However, the visual properties of these plots, e.g. number of kinks
(number of local maxima and minima), can impact their complexity and the
cognitive load imposed on the viewer, compromising interpretability. Our study,
including 57 participants, investigates the relationship between the visual
properties of GAM shape plots and cognitive load they induce. We quantify
various visual properties of shape plots and evaluate their alignment with
participants' perceived cognitive load, based on 144 plots. Our results
indicate that the number of kinks metric is the most effective, explaining
86.4% of the variance in users' ratings. We develop a simple model based on
number of kinks that provides a practical tool for predicting cognitive load,
enabling the assessment of one aspect of GAM interpretability without direct
user involvement.",2024-09-25,"Sven Kruschel, Lasse Bohlen, Julian Rosenberger, Patrick Zschech, Mathias Kraus",http://arxiv.org/pdf/2409.16870v1,cs.LG
Risk-averse learning with delayed feedback,"In real-world scenarios, the impacts of decisions may not manifest
immediately. Taking these delays into account facilitates accurate assessment
and management of risk in real-world environments, thereby ensuring the
efficacy of strategies. In this paper, we investigate risk-averse learning
using Conditional Value at Risk (CVaR) as risk measure, while incorporating
delayed feedback with unknown but bounded delays. We develop two risk-averse
learning algorithms that rely on one-point and two-point zeroth-order
optimization approaches, respectively. The regret achieved by the algorithms is
analyzed in terms of the cumulative delay and the number of total samplings.
The results suggest that the two-point risk-averse learning achieves a smaller
regret bound than the one-point algorithm. Furthermore, the one-point
risk-averse learning algorithm attains sublinear regret under certain delay
conditions, and the two-point risk-averse learning algorithm achieves sublinear
regret with minimal restrictions on the delay. We provide numerical experiments
on a dynamic pricing problem to demonstrate the performance of the proposed
algorithms.",2024-09-25,"Siyi Wang, Zifan Wang, Karl Henrik Johansson, Sandra Hirche",http://arxiv.org/pdf/2409.16866v1,cs.LG
Optimal starting point for time series forecasting,"Recent advances on time series forecasting mainly focus on improving the
forecasting models themselves. However, when the time series data suffer from
potential structural breaks or concept drifts, the forecasting performance
might be significantly reduced. In this paper, we introduce a novel approach
called Optimal Starting Point Time Series Forecast (OSP-TSP) for optimal
forecasting, which can be combined with existing time series forecasting
models. By adjusting the sequence length via leveraging the XGBoost and
LightGBM models, the proposed approach can determine the optimal starting point
(OSP) of the time series and then enhance the prediction performances of the
base forecasting models. To illustrate the effectiveness of the proposed
approach, comprehensive empirical analysis have been conducted on the M4
dataset and other real world datasets. Empirical results indicate that
predictions based on the OSP-TSP approach consistently outperform those using
the complete time series dataset. Moreover, comparison results reveals that
combining our approach with existing forecasting models can achieve better
prediction accuracy, which also reflect the advantages of the proposed
approach.",2024-09-25,"Yiming Zhong, Yinuo Ren, Guangyao Cao, Feng Li, Haobo Qi",http://arxiv.org/pdf/2409.16843v2,cs.LG
Demo2Vec: Learning Region Embedding with Demographic Information,"Demographic data, such as income, education level, and employment rate,
contain valuable information of urban regions, yet few studies have integrated
demographic information to generate region embedding. In this study, we show
how the simple and easy-to-access demographic data can improve the quality of
state-of-the-art region embedding and provide better predictive performances in
urban areas across three common urban tasks, namely check-in prediction, crime
rate prediction, and house price prediction. We find that existing pre-train
methods based on KL divergence are potentially biased towards mobility
information and propose to use Jenson-Shannon divergence as a more appropriate
loss function for multi-view representation learning. Experimental results from
both New York and Chicago show that mobility + income is the best pre-train
data combination, providing up to 10.22\% better predictive performances than
existing models. Considering that mobility big data can be hardly accessible in
many developing cities, we suggest geographic proximity + income to be a simple
but effective data combination for region embedding pre-training.",2024-09-25,"Ya Wen, Yulun Zhou",http://arxiv.org/pdf/2409.16837v1,cs.LG
Asynchronous Fractional Multi-Agent Deep Reinforcement Learning for Age-Minimal Mobile Edge Computing,"In the realm of emerging real-time networked applications like cyber-physical
systems (CPS), the Age of Information (AoI) has merged as a pivotal metric for
evaluating the timeliness. To meet the high computational demands, such as
those in intelligent manufacturing within CPS, mobile edge computing (MEC)
presents a promising solution for optimizing computing and reducing AoI. In
this work, we study the timeliness of computational-intensive updates and
explores jointly optimize the task updating and offloading policies to minimize
AoI. Specifically, we consider edge load dynamics and formulate a task
scheduling problem to minimize the expected time-average AoI. The fractional
objective introduced by AoI and the semi-Markov game nature of the problem
render this challenge particularly difficult, with existing approaches not
directly applicable. To this end, we present a comprehensive framework to
fractional reinforcement learning (RL). We first introduce a fractional
single-agent RL framework and prove its linear convergence. We then extend this
to a fractional multi-agent RL framework with a convergence analysis. To tackle
the challenge of asynchronous control in semi-Markov game, we further design an
asynchronous model-free fractional multi-agent RL algorithm, where each device
makes scheduling decisions with the hybrid action space without knowing the
system dynamics and decisions of other devices. Experimental results show that
our proposed algorithms reduce the average AoI by up to 52.6% compared with the
best baseline algorithm in our experiments.",2024-09-25,"Lyudong Jin, Ming Tang, Jiayu Pan, Meng Zhang, Hao Wang",http://arxiv.org/pdf/2409.16832v5,cs.LG
Learning phase-space flows using time-discrete implicit Runge-Kutta PINNs,"We present a computational framework for obtaining multidimensional
phase-space solutions of systems of non-linear coupled differential equations,
using high-order implicit Runge-Kutta Physics- Informed Neural Networks
(IRK-PINNs) schemes. Building upon foundational work originally solving
differential equations for fields depending on coordinates [J. Comput. Phys.
378, 686 (2019)], we adapt the scheme to a context where the coordinates are
treated as functions. This modification enables us to efficiently solve
equations of motion for a particle in an external field. Our scheme is
particularly useful for explicitly time-independent and periodic fields. We
apply this approach to successfully solve the equations of motion for a mass
particle placed in a central force field and a charged particle in a periodic
electric field.",2024-09-25,"Álvaro Fernández Corral, Nicolás Mendoza, Armin Iske, Andrey Yachmenev, Jochen Küpper",http://arxiv.org/pdf/2409.16826v1,cs.LG
Uncertainty Representations in State-Space Layers for Deep Reinforcement Learning under Partial Observability,"Optimal decision-making under partial observability requires reasoning about
the uncertainty of the environment's hidden state. However, most reinforcement
learning architectures handle partial observability with sequence models that
have no internal mechanism to incorporate uncertainty in their hidden state
representation, such as recurrent neural networks, deterministic state-space
models and transformers. Inspired by advances in probabilistic world models for
reinforcement learning, we propose a standalone Kalman filter layer that
performs closed-form Gaussian inference in linear state-space models and train
it end-to-end within a model-free architecture to maximize returns. Similar to
efficient linear recurrent layers, the Kalman filter layer processes sequential
data using a parallel scan, which scales logarithmically with the sequence
length. By design, Kalman filter layers are a drop-in replacement for other
recurrent layers in standard model-free architectures, but importantly they
include an explicit mechanism for probabilistic filtering of the latent state
representation. Experiments in a wide variety of tasks with partial
observability show that Kalman filter layers excel in problems where
uncertainty reasoning is key for decision-making, outperforming other stateful
models.",2024-09-25,"Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, Jan Peters",http://arxiv.org/pdf/2409.16824v2,cs.LG
A parametric framework for kernel-based dynamic mode decomposition using deep learning,"Surrogate modelling is widely applied in computational science and
engineering to mitigate computational efficiency issues for the real-time
simulations of complex and large-scale computational models or for many-query
scenarios, such as uncertainty quantification and design optimisation. In this
work, we propose a parametric framework for kernel-based dynamic mode
decomposition method based on the linear and nonlinear disambiguation
optimization (LANDO) algorithm. The proposed parametric framework consists of
two stages, offline and online. The offline stage prepares the essential
component for prediction, namely a series of LANDO models that emulate the
dynamics of the system with particular parameters from a training dataset. The
online stage leverages those LANDO models to generate new data at a desired
time instant, and approximate the mapping between parameters and the state with
the data using deep learning techniques. Moreover, dimensionality reduction
technique is applied to high-dimensional dynamical systems to reduce the
computational cost of training. Three numerical examples including
Lotka-Volterra model, heat equation and reaction-diffusion equation are
presented to demonstrate the efficiency and effectiveness of the proposed
framework.",2024-09-25,"Konstantinos Kevopoulos, Dongwei Ye",http://arxiv.org/pdf/2409.16817v1,cs.LG
Accelerating TinyML Inference on Microcontrollers through Approximate Kernels,"The rapid growth of microcontroller-based IoT devices has opened up numerous
applications, from smart manufacturing to personalized healthcare. Despite the
widespread adoption of energy-efficient microcontroller units (MCUs) in the
Tiny Machine Learning (TinyML) domain, they still face significant limitations
in terms of performance and memory (RAM, Flash). In this work, we combine
approximate computing and software kernel design to accelerate the inference of
approximate CNN models on MCUs. Our kernel-based approximation framework
firstly unpacks the operands of each convolution layer and then conducts an
offline calculation to determine the significance of each operand.
Subsequently, through a design space exploration, it employs a computation
skipping approximation strategy based on the calculated significance. Our
evaluation on an STM32-Nucleo board and 2 popular CNNs trained on the CIFAR-10
dataset shows that, compared to state-of-the-art exact inference, our Pareto
optimal solutions can feature on average 21% latency reduction with no
degradation in Top-1 classification accuracy, while for lower accuracy
requirements, the corresponding reduction becomes even more pronounced.",2024-09-25,"Giorgos Armeniakos, Georgios Mentzos, Dimitrios Soudris",http://arxiv.org/pdf/2409.16815v1,cs.LG
Embedding an ANN-Based Crystal Plasticity Model into the Finite Element Framework using an ABAQUS User-Material Subroutine,"This manuscript presents a practical method for incorporating trained Neural
Networks (NNs) into the Finite Element (FE) framework using a user material
(UMAT) subroutine. The work exemplifies crystal plasticity, a complex inelastic
non-linear path-dependent material response, with a wide range of applications
in ABAQUS UMAT. However, this approach can be extended to other material
behaviors and FE tools. The use of a UMAT subroutine serves two main purposes:
(1) it predicts and updates the stress or other mechanical properties of
interest directly from the strain history; (2) it computes the Jacobian matrix
either through backpropagation or numerical differentiation, which plays an
essential role in the solution convergence. By implementing NNs in a UMAT
subroutine, a trained machine learning model can be employed as a data-driven
constitutive law within the FEM framework, preserving multiscale information
that conventional constitutive laws often neglect or average. The versatility
of this method makes it a powerful tool for integrating machine learning into
mechanical simulation. While this approach is expected to provide higher
accuracy in reproducing realistic material behavior, the reliability of the
solution process and the convergence conditions must be paid special attention.
While the theory of the model is explained in [Heider et al. 2020], exemplary
source code is also made available for interested readers
[https://doi.org/10.25835/6n5uu50y]",2024-09-25,"Yuqing He, Yousef Heider, Bernd Markert",http://arxiv.org/pdf/2410.08214v1,cs.LG
Large Language Model Predicts Above Normal All India Summer Monsoon Rainfall in 2024,"Reliable prediction of the All India Summer Monsoon Rainfall (AISMR) is
pivotal for informed policymaking for the country, impacting the lives of
billions of people. However, accurate simulation of AISMR has been a persistent
challenge due to the complex interplay of various muti-scale factors and the
inherent variability of the monsoon system. This research focuses on adapting
and fine-tuning the latest LLM model, PatchTST, to accurately predict AISMR
with a lead time of three months. The fine-tuned PatchTST model, trained with
historical AISMR data, the Ni\~no3.4 index, and categorical Indian Ocean Dipole
values, outperforms several popular neural network models and statistical
models. This fine-tuned LLM model exhibits an exceptionally low RMSE percentage
of 0.07% and a Spearman correlation of 0.976. This is particularly impressive,
since it is nearly 80% more accurate than the best-performing NN models. The
model predicts an above-normal monsoon for the year 2024, with an accumulated
rainfall of 921.6 mm in the month of June-September for the entire country.",2024-09-25,"Ujjawal Sharma, Madhav Biyani, Akhil Dev Suresh, Debi Prasad Bhuyan, Saroj Kanta Mishra, Tanmoy Chakraborty",http://arxiv.org/pdf/2409.16799v1,cs.LG
Scalable Ensemble Diversification for OOD Generalization and Detection,"Training a diverse ensemble of models has several practical applications such
as providing candidates for model selection with better out-of-distribution
(OOD) generalization, and enabling the detection of OOD samples via Bayesian
principles. An existing approach to diverse ensemble training encourages the
models to disagree on provided OOD samples. However, the approach is
computationally expensive and it requires well-separated ID and OOD examples,
such that it has only been demonstrated in small-scale settings.
  $\textbf{Method.}$ This work presents a method for Scalable Ensemble
Diversification (SED) applicable to large-scale settings (e.g. ImageNet) that
does not require OOD samples. Instead, SED identifies hard training samples on
the fly and encourages the ensemble members to disagree on these. To improve
scaling, we show how to avoid the expensive computations in existing methods of
exhaustive pairwise disagreements across models.
  $\textbf{Results.}$ We evaluate the benefits of diversification with
experiments on ImageNet. First, for OOD generalization, we observe large
benefits from the diversification in multiple settings including output-space
(classical) ensembles and weight-space ensembles (model soups). Second, for OOD
detection, we turn the diversity of ensemble hypotheses into a novel
uncertainty score estimator that surpasses a large number of OOD detection
baselines.
  Code is available here:
https://github.com/AlexanderRubinstein/diverse-universe-public.",2024-09-25,"Alexander Rubinstein, Luca Scimeca, Damien Teney, Seong Joon Oh",http://arxiv.org/pdf/2409.16797v1,cs.LG
Symbolic State Partitioning for Reinforcement Learning,"Tabular reinforcement learning methods cannot operate directly on continuous
state spaces. One solution for this problem is to partition the state space. A
good partitioning enables generalization during learning and more efficient
exploitation of prior experiences. Consequently, the learning process becomes
faster and produces more reliable policies. However, partitioning introduces
approximation, which is particularly harmful in the presence of nonlinear
relations between state components. An ideal partition should be as coarse as
possible, while capturing the key structure of the state space for the given
problem. This work extracts partitions from the environment dynamics by
symbolic execution. We show that symbolic partitioning improves state space
coverage with respect to environmental behavior and allows reinforcement
learning to perform better for sparse rewards. We evaluate symbolic state space
partitioning with respect to precision, scalability, learning agent performance
and state space coverage for the learnt policies.",2024-09-25,"Mohsen Ghaffari, Mahsa Varshosaz, Einar Broch Johnsen, Andrzej Wąsowski",http://arxiv.org/pdf/2409.16791v3,cs.LG
Enhancing Feature Selection and Interpretability in AI Regression Tasks Through Feature Attribution,"Research in Explainable Artificial Intelligence (XAI) is increasing, aiming
to make deep learning models more transparent. Most XAI methods focus on
justifying the decisions made by Artificial Intelligence (AI) systems in
security-relevant applications. However, relatively little attention has been
given to using these methods to improve the performance and robustness of deep
learning algorithms. Additionally, much of the existing XAI work primarily
addresses classification problems. In this study, we investigate the potential
of feature attribution methods to filter out uninformative features in input
data for regression problems, thereby improving the accuracy and stability of
predictions. We introduce a feature selection pipeline that combines Integrated
Gradients with k-means clustering to select an optimal set of variables from
the initial data space. To validate the effectiveness of this approach, we
apply it to a real-world industrial problem - blade vibration analysis in the
development process of turbo machinery.",2024-09-25,"Alexander Hinterleitner, Thomas Bartz-Beielstein, Richard Schulz, Sebastian Spengler, Thomas Winter, Christoph Leitenmeier",http://arxiv.org/pdf/2409.16787v1,cs.LG
World Model-based Perception for Visual Legged Locomotion,"Legged locomotion over various terrains is challenging and requires precise
perception of the robot and its surroundings from both proprioception and
vision. However, learning directly from high-dimensional visual input is often
data-inefficient and intricate. To address this issue, traditional methods
attempt to learn a teacher policy with access to privileged information first
and then learn a student policy to imitate the teacher's behavior with visual
input. Despite some progress, this imitation framework prevents the student
policy from achieving optimal performance due to the information gap between
inputs. Furthermore, the learning process is unnatural since animals
intuitively learn to traverse different terrains based on their understanding
of the world without privileged knowledge. Inspired by this natural ability, we
propose a simple yet effective method, World Model-based Perception (WMP),
which builds a world model of the environment and learns a policy based on the
world model. We illustrate that though completely trained in simulation, the
world model can make accurate predictions of real-world trajectories, thus
providing informative signals for the policy controller. Extensive simulated
and real-world experiments demonstrate that WMP outperforms state-of-the-art
baselines in traversability and robustness. Videos and Code are available at:
https://wmp-loco.github.io/.",2024-09-25,"Hang Lai, Jiahang Cao, Jiafeng Xu, Hongtao Wu, Yunfeng Lin, Tao Kong, Yong Yu, Weinan Zhang",http://arxiv.org/pdf/2409.16784v1,cs.LG
Super Level Sets and Exponential Decay: A Synergistic Approach to Stable Neural Network Training,"The objective of this paper is to enhance the optimization process for neural
networks by developing a dynamic learning rate algorithm that effectively
integrates exponential decay and advanced anti-overfitting strategies. Our
primary contribution is the establishment of a theoretical framework where we
demonstrate that the optimization landscape, under the influence of our
algorithm, exhibits unique stability characteristics defined by Lyapunov
stability principles. Specifically, we prove that the superlevel sets of the
loss function, as influenced by our adaptive learning rate, are always
connected, ensuring consistent training dynamics. Furthermore, we establish the
""equiconnectedness"" property of these superlevel sets, which maintains uniform
stability across varying training conditions and epochs. This paper contributes
to the theoretical understanding of dynamic learning rate mechanisms in neural
networks and also pave the way for the development of more efficient and
reliable neural optimization techniques. This study intends to formalize and
validate the equiconnectedness of loss function as superlevel sets in the
context of neural network training, opening newer avenues for future research
in adaptive machine learning algorithms. We leverage previous theoretical
discoveries to propose training mechanisms that can effectively handle complex
and high-dimensional data landscapes, particularly in applications requiring
high precision and reliability.",2024-09-25,"Jatin Chaudhary, Dipak Nidhi, Jukka Heikkonen, Haari Merisaari, Rajiv Kanth",http://arxiv.org/pdf/2409.16769v1,cs.LG
Interpreting Deep Neural Network-Based Receiver Under Varying Signal-To-Noise Ratios,"We propose a novel method for interpreting neural networks, focusing on
convolutional neural network-based receiver model. The method identifies which
unit or units of the model contain most (or least) information about the
channel parameter(s) of the interest, providing insights at both global and
local levels -- with global explanations aggregating local ones. Experiments on
link-level simulations demonstrate the method's effectiveness in identifying
units that contribute most (and least) to signal-to-noise ratio processing.
Although we focus on a radio receiver model, the method generalizes to other
neural network architectures and applications, offering robust estimation even
in high-dimensional settings.",2024-09-25,"Marko Tuononen, Dani Korpi, Ville Hautamäki",http://arxiv.org/pdf/2409.16768v2,cs.LG
Exploring Information-Theoretic Metrics Associated with Neural Collapse in Supervised Training,"In this paper, we introduce matrix entropy as an analytical tool for studying
supervised learning, investigating the information content of data
representations and classification head vectors, as well as the dynamic
interactions between them during the supervised learning process. Our
experimental results reveal that matrix entropy effectively captures the
variations in information content of data representations and classification
head vectors as neural networks approach Neural Collapse during supervised
training, while also serving as a robust metric for measuring similarity among
data samples. Leveraging this property, we propose Cross-Model Alignment (CMA)
loss to optimize the fine-tuning of pretrained models. To characterize the
dynamics of neural networks nearing the Neural Collapse state, we introduce two
novel metrics: the Matrix Mutual Information Ratio (MIR) and the Matrix Entropy
Difference Ratio (HDR), which quantitatively assess the interactions between
data representations and classification heads in supervised learning, with
theoretical optimal values derived under the Neural Collapse state. Our
experiments demonstrate that MIR and HDR effectively explain various phenomena
in neural networks, including the dynamics of standard supervised training,
linear mode connectivity. Moreover, we use MIR and HDR to analyze the dynamics
of grokking, which is a fascinating phenomenon in supervised learning where a
model unexpectedly exhibits generalization long after achieving training data
fit.",2024-09-25,"Kun Song, Zhiquan Tan, Bochao Zou, Jiansheng Chen, Huimin Ma, Weiran Huang",http://arxiv.org/pdf/2409.16767v2,cs.LG
"MaViLS, a Benchmark Dataset for Video-to-Slide Alignment, Assessing Baseline Accuracy with a Multimodal Alignment Algorithm Leveraging Speech, OCR, and Visual Features","This paper presents a benchmark dataset for aligning lecture videos with
corresponding slides and introduces a novel multimodal algorithm leveraging
features from speech, text, and images. It achieves an average accuracy of 0.82
in comparison to SIFT (0.56) while being approximately 11 times faster. Using
dynamic programming the algorithm tries to determine the optimal slide
sequence. The results show that penalizing slide transitions increases
accuracy. Features obtained via optical character recognition (OCR) contribute
the most to a high matching accuracy, followed by image features. The findings
highlight that audio transcripts alone provide valuable information for
alignment and are beneficial if OCR data is lacking. Variations in matching
accuracy across different lectures highlight the challenges associated with
video quality and lecture style. The novel multimodal algorithm demonstrates
robustness to some of these challenges, underscoring the potential of the
approach.",2024-09-25,"Katharina Anderer, Andreas Reich, Matthias Wölfel",http://arxiv.org/pdf/2409.16765v1,cs.LG
Offline and Distributional Reinforcement Learning for Radio Resource Management,"Reinforcement learning (RL) has proved to have a promising role in future
intelligent wireless networks. Online RL has been adopted for radio resource
management (RRM), taking over traditional schemes. However, due to its reliance
on online interaction with the environment, its role becomes limited in
practical, real-world problems where online interaction is not feasible. In
addition, traditional RL stands short in front of the uncertainties and risks
in real-world stochastic environments. In this manner, we propose an offline
and distributional RL scheme for the RRM problem, enabling offline training
using a static dataset without any interaction with the environment and
considering the sources of uncertainties using the distributions of the return.
Simulation results demonstrate that the proposed scheme outperforms
conventional resource management models. In addition, it is the only scheme
that surpasses online RL with a 10 % gain over online RL.",2024-09-25,"Eslam Eldeeb, Hirley Alves",http://arxiv.org/pdf/2409.16764v2,cs.LG
GB-RVFL: Fusion of Randomized Neural Network and Granular Ball Computing,"The random vector functional link (RVFL) network is a prominent
classification model with strong generalization ability. However, RVFL treats
all samples uniformly, ignoring whether they are pure or noisy, and its
scalability is limited due to the need for inverting the entire training
matrix. To address these issues, we propose granular ball RVFL (GB-RVFL) model,
which uses granular balls (GBs) as inputs instead of training samples. This
approach enhances scalability by requiring only the inverse of the GB center
matrix and improves robustness against noise and outliers through the coarse
granularity of GBs. Furthermore, RVFL overlooks the dataset's geometric
structure. To address this, we propose graph embedding GB-RVFL (GE-GB-RVFL)
model, which fuses granular computing and graph embedding (GE) to preserve the
topological structure of GBs. The proposed GB-RVFL and GE-GB-RVFL models are
evaluated on KEEL, UCI, NDC and biomedical datasets, demonstrating superior
performance compared to baseline models.",2024-09-25,"M. Sajid, A. Quadir, M. Tanveer",http://arxiv.org/pdf/2409.16735v1,cs.LG
Verified Relative Safety Margins for Neural Network Twins,"Given two Deep Neural Network (DNN) classifiers with the same input and
output domains, our goal is to quantify the robustness of the two networks in
relation to each other. Towards this, we introduce the notion of Relative
Safety Margins (RSMs). Intuitively, given two classes and a common input, RSM
of one classifier with respect to another reflects the relative margins with
which decisions are made. The proposed notion is relevant in the context of
several applications domains, including to compare a trained network and its
corresponding compact network (e.g., pruned, quantized, distilled network). Not
only can RSMs establish whether decisions are preserved, but they can also
quantify their qualities. We also propose a framework to establish safe bounds
on RSM gains or losses given an input and a family of perturbations. We
evaluate our approach using the MNIST, CIFAR10, and two real-world medical
datasets, to show the relevance of our results.",2024-09-25,"Anahita Baninajjar, Kamran Hosseini, Ahmed Rezine, Amir Aminifar",http://arxiv.org/pdf/2409.16726v1,cs.LG
PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning,"Low-rank adaptation (LoRA) and its variants have recently gained much
interest due to their ability to avoid excessive inference costs. However, LoRA
still encounters the following challenges: (1) Limitation of low-rank
assumption; and (2) Its initialization method may be suboptimal. To this end,
we propose PMSS(Pre-trained Matrices Skeleton Selection), which enables
high-rank updates with low costs while leveraging semantic and linguistic
information inherent in pre-trained weight. It achieves this by selecting
skeletons from the pre-trained weight matrix and only learning a small matrix
instead. Experiments demonstrate that PMSS outperforms LoRA and other
fine-tuning methods across tasks with much less trainable parameters. We
demonstrate its effectiveness, especially in handling complex tasks such as
DROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math
reasoning(+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of
GSM8K). The code and model will be released soon.",2024-09-25,"Qibin Wang, Xiaolin Hu, Weikai Xu, Wei Liu, Jian Luan, Bin Wang",http://arxiv.org/pdf/2409.16722v1,cs.LG
Dashing for the Golden Snitch: Multi-Drone Time-Optimal Motion Planning with Multi-Agent Reinforcement Learning,"Recent innovations in autonomous drones have facilitated time-optimal flight
in single-drone configurations, and enhanced maneuverability in multi-drone
systems by applying optimal control and learning-based methods. However, few
studies have achieved time-optimal motion planning for multi-drone systems,
particularly during highly agile maneuvers or in dynamic scenarios. This paper
presents a decentralized policy network using multi-agent reinforcement
learning for time-optimal multi-drone flight. To strike a balance between
flight efficiency and collision avoidance, we introduce a soft collision-free
mechanism inspired by optimization-based methods. By customizing PPO in a
centralized training, decentralized execution (CTDE) fashion, we unlock higher
efficiency and stability in training while ensuring lightweight implementation.
Extensive simulations show that, despite slight performance trade-offs compared
to single-drone systems, our multi-drone approach maintains near-time-optimal
performance with a low collision rate. Real-world experiments validate our
method, with two quadrotors using the same network as in simulation achieving a
maximum speed of 13.65 m/s and a maximum body rate of 13.4 rad/s in a 5.5 m *
5.5 m * 2.0 m space across various tracks, relying entirely on onboard
computation.",2024-09-25,"Xian Wang, Jin Zhou, Yuanli Feng, Jiahao Mei, Jiming Chen, Shuo Li",http://arxiv.org/pdf/2409.16720v2,cs.LG
Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification,"Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed
the success of prompt tuning and adapter tuning, while the classic model
fine-tuning on inherent parameters seems to be overlooked. It is believed that
fine-tuning the parameters of VLMs with few-shot samples corrupts the
pre-trained knowledge since fine-tuning the CLIP model even degrades
performance. In this paper, we revisit this viewpoint, and propose a new
perspective: fine-tuning the specific parameters instead of all will uncover
the power of classic model fine-tuning on VLMs. Through our meticulous study,
we propose ClipFit, a simple yet effective method to fine-tune CLIP without
introducing any overhead of extra parameters. We demonstrate that by only
fine-tuning the specific bias terms and normalization layers, ClipFit can
improve the performance of zero-shot CLIP by 7.27\% average harmonic mean
accuracy. Lastly, to understand how fine-tuning in CLIPFit affects the
pre-trained models, we conducted extensive experimental analyses w.r.t. changes
in internal parameters and representations. We found that low-level text bias
layers and the first layer normalization layer change much more than other
layers. The code is available at \url{https://github.com/minglllli/CLIPFit}.",2024-09-25,"Ming Li, Jike Zhong, Chenxin Li, Liuzhuozheng Li, Nie Lin, Masashi Sugiyama",http://arxiv.org/pdf/2409.16718v2,cs.LG
Physics-Informed Graph-Mesh Networks for PDEs: A hybrid approach for complex problems,"The recent rise of deep learning has led to numerous applications, including
solving partial differential equations using Physics-Informed Neural Networks.
This approach has proven highly effective in several academic cases. However,
their lack of physical invariances, coupled with other significant weaknesses,
such as an inability to handle complex geometries or their lack of
generalization capabilities, make them unable to compete with classical
numerical solvers in industrial settings. In this work, a limitation regarding
the use of automatic differentiation in the context of physics-informed
learning is highlighted. A hybrid approach combining physics-informed graph
neural networks with numerical kernels from finite elements is introduced.
After studying the theoretical properties of our model, we apply it to complex
geometries, in two and three dimensions. Our choices are supported by an
ablation study, and we evaluate the generalisation capacity of the proposed
approach.",2024-09-25,"Marien Chenaud, Frédéric Magoulès, José Alves",http://arxiv.org/pdf/2410.02819v1,cs.LG
"Numerical Approximation Capacity of Neural Networks with Bounded Parameters: Do Limits Exist, and How Can They Be Measured?","The Universal Approximation Theorem posits that neural networks can
theoretically possess unlimited approximation capacity with a suitable
activation function and a freely chosen or trained set of parameters. However,
a more practical scenario arises when these neural parameters, especially the
nonlinear weights and biases, are bounded. This leads us to question:
\textbf{Does the approximation capacity of a neural network remain universal,
or does it have a limit when the parameters are practically bounded? And if it
has a limit, how can it be measured?}
  Our theoretical study indicates that while universal approximation is
theoretically feasible, in practical numerical scenarios, Deep Neural Networks
(DNNs) with any analytic activation functions (such as Tanh and Sigmoid) can
only be approximated by a finite-dimensional vector space under a bounded
nonlinear parameter space (NP space), whether in a continuous or discrete
sense. Based on this study, we introduce the concepts of \textit{$\epsilon$
outer measure} and \textit{Numerical Span Dimension (NSdim)} to quantify the
approximation capacity limit of a family of networks both theoretically and
practically.
  Furthermore, drawing on our new theoretical study and adopting a fresh
perspective, we strive to understand the relationship between back-propagation
neural networks and random parameter networks (such as the Extreme Learning
Machine (ELM)) with both finite and infinite width. We also aim to provide
fresh insights into regularization, the trade-off between width and depth,
parameter space, width redundancy, condensation, and other related important
issues.",2024-09-25,"Li Liu, Tengchao Yu, Heng Yong",http://arxiv.org/pdf/2409.16697v1,cs.LG
"A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms","Large language models (LLMs) have achieved remarkable advancements in natural
language processing, showcasing exceptional performance across various tasks.
However, the expensive memory and computational requirements present
significant challenges for their practical deployment. Low-bit quantization has
emerged as a critical approach to mitigate these challenges by reducing the
bit-width of model parameters, activations, and gradients, thus decreasing
memory usage and computational demands. This paper presents a comprehensive
survey of low-bit quantization methods tailored for LLMs, covering the
fundamental principles, system implementations, and algorithmic strategies. An
overview of basic concepts and new data formats specific to low-bit LLMs is
first introduced, followed by a review of frameworks and systems that
facilitate low-bit LLMs across various hardware platforms. Then, we categorize
and analyze techniques and toolkits for efficient low-bit training and
inference of LLMs. Finally, we conclude with a discussion of future trends and
potential advancements of low-bit LLMs. Our systematic overview from basic,
system, and algorithm perspectives can offer valuable insights and guidelines
for future works to enhance the efficiency and applicability of LLMs through
low-bit quantization.",2024-09-25,"Ruihao Gong, Yifu Ding, Zining Wang, Chengtao Lv, Xingyu Zheng, Jinyang Du, Haotong Qin, Jinyang Guo, Michele Magno, Xianglong Liu",http://arxiv.org/pdf/2409.16694v2,cs.LG
Layout-Corrector: Alleviating Layout Sticking Phenomenon in Discrete Diffusion Model,"Layout generation is a task to synthesize a harmonious layout with elements
characterized by attributes such as category, position, and size. Human
designers experiment with the placement and modification of elements to create
aesthetic layouts, however, we observed that current discrete diffusion models
(DDMs) struggle to correct inharmonious layouts after they have been generated.
In this paper, we first provide novel insights into layout sticking phenomenon
in DDMs and then propose a simple yet effective layout-assessment module
Layout-Corrector, which works in conjunction with existing DDMs to address the
layout sticking problem. We present a learning-based module capable of
identifying inharmonious elements within layouts, considering overall layout
harmony characterized by complex composition. During the generation process,
Layout-Corrector evaluates the correctness of each token in the generated
layout, reinitializing those with low scores to the ungenerated state. The DDM
then uses the high-scored tokens as clues to regenerate the harmonized tokens.
Layout-Corrector, tested on common benchmarks, consistently boosts
layout-generation performance when in conjunction with various state-of-the-art
DDMs. Furthermore, our extensive analysis demonstrates that the
Layout-Corrector (1) successfully identifies erroneous tokens, (2) facilitates
control over the fidelity-diversity trade-off, and (3) significantly mitigates
the performance drop associated with fast sampling.",2024-09-25,"Shoma Iwai, Atsuki Osanai, Shunsuke Kitada, Shinichiro Omachi",http://arxiv.org/pdf/2409.16689v1,cs.LG
Erase then Rectify: A Training-Free Parameter Editing Approach for Cost-Effective Graph Unlearning,"Graph unlearning, which aims to eliminate the influence of specific nodes,
edges, or attributes from a trained Graph Neural Network (GNN), is essential in
applications where privacy, bias, or data obsolescence is a concern. However,
existing graph unlearning techniques often necessitate additional training on
the remaining data, leading to significant computational costs, particularly
with large-scale graphs. To address these challenges, we propose a two-stage
training-free approach, Erase then Rectify (ETR), designed for efficient and
scalable graph unlearning while preserving the model utility. Specifically, we
first build a theoretical foundation showing that masking parameters critical
for unlearned samples enables effective unlearning. Building on this insight,
the Erase stage strategically edits model parameters to eliminate the impact of
unlearned samples and their propagated influence on intercorrelated nodes. To
further ensure the GNN's utility, the Rectify stage devises a gradient
approximation method to estimate the model's gradient on the remaining dataset,
which is then used to enhance model performance. Overall, ETR achieves graph
unlearning without additional training or full training data access,
significantly reducing computational overhead and preserving data privacy.
Extensive experiments on seven public datasets demonstrate the consistent
superiority of ETR in model utility, unlearning efficiency, and unlearning
effectiveness, establishing it as a promising solution for real-world graph
unlearning challenges.",2024-09-25,"Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu",http://arxiv.org/pdf/2409.16684v2,cs.LG
TSBP: Improving Object Detection in Histology Images via Test-time Self-guided Bounding-box Propagation,"A global threshold (e.g., 0.5) is often applied to determine which bounding
boxes should be included in the final results for an object detection task. A
higher threshold reduces false positives but may result in missing a
significant portion of true positives. A lower threshold can increase detection
recall but may also result in more false positives. Because of this, using a
preset global threshold (e.g., 0.5) applied to all the bounding box candidates
may lead to suboptimal solutions. In this paper, we propose a Test-time
Self-guided Bounding-box Propagation (TSBP) method, leveraging Earth Mover's
Distance (EMD) to enhance object detection in histology images. TSBP utilizes
bounding boxes with high confidence to influence those with low confidence,
leveraging visual similarities between them. This propagation mechanism enables
bounding boxes to be selected in a controllable, explainable, and robust
manner, which surpasses the effectiveness of using simple thresholds and
uncertainty calibration methods. Importantly, TSBP does not necessitate
additional labeled samples for model training or parameter estimation, unlike
calibration methods. We conduct experiments on gland detection and cell
detection tasks in histology images. The results show that our proposed TSBP
significantly improves detection outcomes when working in conjunction with
state-of-the-art deep learning-based detection networks. Compared to other
methods such as uncertainty calibration, TSBP yields more robust and accurate
object detection predictions while using no additional labeled samples. The
code is available at https://github.com/jwhgdeu/TSBP.",2024-09-25,"Tingting Yang, Liang Xiao, Yizhe Zhang",http://arxiv.org/pdf/2409.16678v1,cs.LG
CryptoTrain: Fast Secure Training on Encrypted Dataset,"Secure training, while protecting the confidentiality of both data and model
weights, typically incurs significant training overhead. Traditional Fully
Homomorphic Encryption (FHE)-based non-inter-active training models are heavily
burdened by computationally demanding bootstrapping. To develop an efficient
secure training system, we established a foundational framework, CryptoTrain-B,
utilizing a hybrid cryptographic protocol that merges FHE with Oblivious
Transfer (OT) for handling linear and non-linear operations, respectively. This
integration eliminates the need for costly bootstrapping. Although
CryptoTrain-B sets a new baseline in performance, reducing its training
overhead remains essential. We found that ciphertext-ciphertext multiplication
(CCMul) is a critical bottleneck in operations involving encrypted inputs and
models. Our solution, the CCMul-Precompute technique, involves precomputing
CCMul offline and resorting to the less resource-intensive ciphertext-plaintext
multiplication (CPMul) during private training. Furthermore, conventional
polynomial convolution in FHE systems tends to encode irrelevant and redundant
values into polynomial slots, necessitating additional polynomials and
ciphertexts for input representation and leading to extra multiplications.
Addressing this, we introduce correlated polynomial convolution, which encodes
only related input values into polynomials, thus drastically reducing the
number of computations and overheads. By integrating CCMul-Precompute and
correlated polynomial convolution into CryptoTrain-B, we facilitate a rapid and
efficient secure training framework, CryptoTrain. Extensive experiments
demonstrate that CryptoTrain achieves a ~5.3X training time reduction compared
to prior methods.",2024-09-25,"Jiaqi Xue, Yancheng Zhang, Yanshan Wang, Xueqiang Wang, Hao Zheng, Qian Lou",http://arxiv.org/pdf/2409.16675v2,cs.LG
SWE2: SubWord Enriched and Significant Word Emphasized Framework for Hate Speech Detection,"Hate speech detection on online social networks has become one of the
emerging hot topics in recent years. With the broad spread and fast propagation
speed across online social networks, hate speech makes significant impacts on
society by increasing prejudice and hurting people. Therefore, there are
aroused attention and concern from both industry and academia. In this paper,
we address the hate speech problem and propose a novel hate speech detection
framework called SWE2, which only relies on the content of messages and
automatically identifies hate speech. In particular, our framework exploits
both word-level semantic information and sub-word knowledge. It is intuitively
persuasive and also practically performs well under a situation with/without
character-level adversarial attack. Experimental results show that our proposed
model achieves 0.975 accuracy and 0.953 macro F1, outperforming 7
state-of-the-art baselines under no adversarial attack. Our model robustly and
significantly performed well under extreme adversarial attack (manipulation of
50% messages), achieving 0.967 accuracy and 0.934 macro F1.",2024-09-25,"Guanyi Mou, Pengyi Ye, Kyumin Lee",http://arxiv.org/pdf/2409.16673v1,cs.LG
DiaSynth: Synthetic Dialogue Generation Framework for Low Resource Dialogue Applications,"The scarcity of domain-specific dialogue datasets limits the development of
dialogue systems across applications. Existing research is constrained by
general or niche datasets that lack sufficient scale for training dialogue
systems. To address this gap, we introduce DiaSynth - a synthetic dialogue
generation framework capable of generating high-quality, contextually rich
dialogues across a wide range of domains. Unlike existing frameworks, DiaSynth
uses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to
generate dynamic, domain-specific dialogues with simulated personas and diverse
conversational features. We perform our experiments by generating synthetic
data using different LLMs and few-shot examples from DialogSum and SAMSum. The
pretrained language models fine-tuned on the synthetic data outperform the base
models by 16.47% on dialogue summarization, while the comparison between models
fine-tuned on in-domain data and synthetic data shows that the synthetic data
is able to capture 90.48% of the performance distribution of the in-domain data
on dialogue summarization. The quality of the data generated also increases as
we increase the size of LLM from 3B to 8B. These results validate DiaSynth's
potential as a robust alternative to traditional data collection methods. We
open source the code and data generated for future research.",2024-09-25,"Sathya Krishnan Suresh, Wu Mengjun, Tushar Pranav, Eng Siong Chng",http://arxiv.org/pdf/2409.19020v3,cs.LG
Learning Bipedal Walking for Humanoid Robots in Challenging Environments with Obstacle Avoidance,"Deep reinforcement learning has seen successful implementations on humanoid
robots to achieve dynamic walking. However, these implementations have been so
far successful in simple environments void of obstacles. In this paper, we aim
to achieve bipedal locomotion in an environment where obstacles are present
using a policy-based reinforcement learning. By adding simple distance reward
terms to a state of art reward function that can achieve basic bipedal
locomotion, the trained policy succeeds in navigating the robot towards the
desired destination without colliding with the obstacles along the way.",2024-09-25,"Marwan Hamze, Mitsuharu Morisawa, Eiichi Yoshida",http://arxiv.org/pdf/2410.08212v1,cs.LG
"An Effective, Robust and Fairness-aware Hate Speech Detection Framework","With the widespread online social networks, hate speeches are spreading
faster and causing more damage than ever before. Existing hate speech detection
methods have limitations in several aspects, such as handling data
insufficiency, estimating model uncertainty, improving robustness against
malicious attacks, and handling unintended bias (i.e., fairness). There is an
urgent need for accurate, robust, and fair hate speech classification in online
social networks. To bridge the gap, we design a data-augmented, fairness
addressed, and uncertainty estimated novel framework. As parts of the
framework, we propose Bidirectional Quaternion-Quasi-LSTM layers to balance
effectiveness and efficiency. To build a generalized model, we combine five
datasets collected from three platforms. Experiment results show that our model
outperforms eight state-of-the-art methods under both no attack scenario and
various attack scenarios, indicating the effectiveness and robustness of our
model. We share our code along with combined dataset for better future research",2024-09-25,"Guanyi Mou, Kyumin Lee",http://arxiv.org/pdf/2409.17191v1,cs.LG
Wildlife Product Trading in Online Social Networks: A Case Study on Ivory-Related Product Sales Promotion Posts,"Wildlife trafficking (WLT) has emerged as a global issue, with traffickers
expanding their operations from offline to online platforms, utilizing
e-commerce websites and social networks to enhance their illicit trade. This
paper addresses the challenge of detecting and recognizing wildlife product
sales promotion behaviors in online social networks, a crucial aspect in
combating these environmentally harmful activities. To counter these
environmentally damaging illegal operations, in this research, we focus on
wildlife product sales promotion behaviors in online social networks.
Specifically, 1) A scalable dataset related to wildlife product trading is
collected using a network-based approach. This dataset is labeled through a
human-in-the-loop machine learning process, distinguishing positive class
samples containing wildlife product selling posts and hard-negatives
representing normal posts misclassified as potential WLT posts, subsequently
corrected by human annotators. 2) We benchmark the machine learning results on
the proposed dataset and build a practical framework that automatically
identifies suspicious wildlife selling posts and accounts, sufficiently
leveraging the multi-modal nature of online social networks. 3) This research
delves into an in-depth analysis of trading posts, shedding light on the
systematic and organized selling behaviors prevalent in the current landscape.
We provide detailed insights into the nature of these behaviors, contributing
valuable information for understanding and countering illegal wildlife product
trading.",2024-09-25,"Guanyi Mou, Yun Yue, Kyumin Lee, Ziming Zhang",http://arxiv.org/pdf/2409.16671v1,cs.LG
GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for Cross-Graph Transfer Learning,"Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in
handling a range of graph analytical tasks across various domains, such as
e-commerce and social networks. Despite their versatility, GNNs face
significant challenges in transferability, limiting their utility in real-world
applications. Existing research in GNN transfer learning overlooks
discrepancies in distribution among various graph datasets, facing challenges
when transferring across different distributions. How to effectively adopt a
well-trained GNN to new graphs with varying feature and structural
distributions remains an under-explored problem. Taking inspiration from the
success of Low-Rank Adaptation (LoRA) in adapting large language models to
various domains, we propose GraphLoRA, an effective and parameter-efficient
method for transferring well-trained GNNs to diverse graph domains.
Specifically, we first propose a Structure-aware Maximum Mean Discrepancy
(SMMD) to align divergent node feature distributions across source and target
graphs. Moreover, we introduce low-rank adaptation by injecting a small
trainable GNN alongside the pre-trained one, effectively bridging structural
distribution gaps while mitigating the catastrophic forgetting. Additionally, a
structure-aware regularization objective is proposed to enhance the
adaptability of the pre-trained GNN to target graph with scarce supervision
labels. Extensive experiments on eight real-world datasets demonstrate the
effectiveness of GraphLoRA against fourteen baselines by tuning only 20% of
parameters, even across disparate graph domains. The code is available at
https://github.com/AllminerLab/GraphLoRA.",2024-09-25,"Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu",http://arxiv.org/pdf/2409.16670v2,cs.LG
Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models,"We propose the use of latent space generative world models to address the
covariate shift problem in autonomous driving. A world model is a neural
network capable of predicting an agent's next state given past states and
actions. By leveraging a world model during training, the driving policy
effectively mitigates covariate shift without requiring an excessive amount of
training data. During end-to-end training, our policy learns how to recover
from errors by aligning with states observed in human demonstrations, so that
at runtime it can recover from perturbations outside the training distribution.
Additionally, we introduce a novel transformer-based perception encoder that
employs multi-view cross-attention and a learned scene query. We present
qualitative and quantitative results, demonstrating significant improvements
upon prior state of the art in closed-loop testing in the CARLA simulator, as
well as showing the ability to handle perturbations in both CARLA and NVIDIA's
DRIVE Sim.",2024-09-25,"Alexander Popov, Alperen Degirmenci, David Wehr, Shashank Hegde, Ryan Oldja, Alexey Kamenev, Bertrand Douillard, David Nistér, Urs Muller, Ruchi Bhargava, Stan Birchfield, Nikolai Smolyanskiy",http://arxiv.org/pdf/2409.16663v4,cs.LG
Decentralized Federated Learning with Gradient Tracking over Time-Varying Directed Networks,"We investigate the problem of agent-to-agent interaction in decentralized
(federated) learning over time-varying directed graphs, and, in doing so,
propose a consensus-based algorithm called DSGTm-TV. The proposed algorithm
incorporates gradient tracking and heavy-ball momentum to distributively
optimize a global objective function, while preserving local data privacy.
Under DSGTm-TV, agents will update local model parameters and gradient
estimates using information exchange with neighboring agents enabled through
row- and column-stochastic mixing matrices, which we show guarantee both
consensus and optimality. Our analysis establishes that DSGTm-TV exhibits
linear convergence to the exact global optimum when exact gradient information
is available, and converges in expectation to a neighborhood of the global
optimum when employing stochastic gradients. Moreover, in contrast to existing
methods, DSGTm-TV preserves convergence for networks with uncoordinated
stepsizes and momentum parameters, for which we provide explicit bounds. These
results enable agents to operate in a fully decentralized manner, independently
optimizing their local hyper-parameters. We demonstrate the efficacy of our
approach via comparisons with state-of-the-art baselines on real-world image
classification and natural language processing tasks.",2024-09-25,"Duong Thuy Anh Nguyen, Su Wang, Duong Tung Nguyen, Angelia Nedich, H. Vincent Poor",http://arxiv.org/pdf/2409.17189v1,cs.LG
The Credibility Transformer,"Inspired by the large success of Transformers in Large Language Models, these
architectures are increasingly applied to tabular data. This is achieved by
embedding tabular data into low-dimensional Euclidean spaces resulting in
similar structures as time-series data. We introduce a novel credibility
mechanism to this Transformer architecture. This credibility mechanism is based
on a special token that should be seen as an encoder that consists of a
credibility weighted average of prior information and observation based
information. We demonstrate that this novel credibility mechanism is very
beneficial to stabilize training, and our Credibility Transformer leads to
predictive models that are superior to state-of-the-art deep learning models.",2024-09-25,"Ronald Richman, Salvatore Scognamiglio, Mario V. Wüthrich",http://arxiv.org/pdf/2409.16653v1,cs.LG
Learning Representation for Multitask learning through Self Supervised Auxiliary learning,"Multi-task learning is a popular machine learning approach that enables
simultaneous learning of multiple related tasks, improving algorithmic
efficiency and effectiveness. In the hard parameter sharing approach, an
encoder shared through multiple tasks generates data representations passed to
task-specific predictors. Therefore, it is crucial to have a shared encoder
that provides decent representations for every and each task. However, despite
recent advances in multi-task learning, the question of how to improve the
quality of representations generated by the shared encoder remains open. To
address this gap, we propose a novel approach called Dummy Gradient norm
Regularization that aims to improve the universality of the representations
generated by the shared encoder. Specifically, the method decreases the norm of
the gradient of the loss function with repect to dummy task-specific predictors
to improve the universality of the shared encoder's representations. Through
experiments on multiple multi-task learning benchmark datasets, we demonstrate
that DGR effectively improves the quality of the shared representations,
leading to better multi-task prediction performances. Applied to various
classifiers, the shared representations generated by DGR also show superior
performance compared to existing multi-task learning methods. Moreover, our
approach takes advantage of computational efficiency due to its simplicity. The
simplicity also allows us to seamlessly integrate DGR with the existing
multi-task learning algorithms.",2024-09-25,"Seokwon Shin, Hyungrok Do, Youngdoo Son",http://arxiv.org/pdf/2409.16651v1,cs.LG
Domain-Independent Automatic Generation of Descriptive Texts for Time-Series Data,"Due to scarcity of time-series data annotated with descriptive texts,
training a model to generate descriptive texts for time-series data is
challenging. In this study, we propose a method to systematically generate
domain-independent descriptive texts from time-series data. We identify two
distinct approaches for creating pairs of time-series data and descriptive
texts: the forward approach and the backward approach. By implementing the
novel backward approach, we create the Temporal Automated Captions for
Observations (TACO) dataset. Experimental results demonstrate that a
contrastive learning based model trained using the TACO dataset is capable of
generating descriptive texts for time-series data in novel domains.",2024-09-25,"Kota Dohi, Aoi Ito, Harsh Purohit, Tomoya Nishida, Takashi Endo, Yohei Kawaguchi",http://arxiv.org/pdf/2409.16647v1,cs.LG
Task Addition in Multi-Task Learning by Geometrical Alignment,"Training deep learning models on limited data while maintaining
generalization is one of the fundamental challenges in molecular property
prediction. One effective solution is transferring knowledge extracted from
abundant datasets to those with scarce data. Recently, a novel algorithm called
Geometrically Aligned Transfer Encoder (GATE) has been introduced, which uses
soft parameter sharing by aligning the geometrical shapes of task-specific
latent spaces. However, GATE faces limitations in scaling to multiple tasks due
to computational costs. In this study, we propose a task addition approach for
GATE to improve performance on target tasks with limited data while minimizing
computational complexity. It is achieved through supervised multi-task
pre-training on a large dataset, followed by the addition and training of
task-specific modules for each target task. Our experiments demonstrate the
superior performance of the task addition strategy for GATE over conventional
multi-task methods, with comparable computational costs.",2024-09-25,"Soorin Yim, Dae-Woong Jeong, Sung Moon Ko, Sumin Lee, Hyunseung Kim, Chanhui Lee, Sehui Han",http://arxiv.org/pdf/2409.16645v1,cs.LG
Examining the Rat in the Tunnel: Interpretable Multi-Label Classification of Tor-based Malware,"Despite being the most popular privacy-enhancing network, Tor is increasingly
adopted by cybercriminals to obfuscate malicious traffic, hindering the
identification of malware-related communications between compromised devices
and Command and Control (C&C) servers. This malicious traffic can induce
congestion and reduce Tor's performance, while encouraging network
administrators to block Tor traffic. Recent research, however, demonstrates the
potential for accurately classifying captured Tor traffic as malicious or
benign. While existing efforts have addressed malware class identification,
their performance remains limited, with micro-average precision and recall
values around 70%. Accurately classifying specific malware classes is crucial
for effective attack prevention and mitigation. Furthermore, understanding the
unique patterns and attack vectors employed by different malware classes helps
the development of robust and adaptable defence mechanisms.
  We utilise a multi-label classification technique based on Message-Passing
Neural Networks, demonstrating its superiority over previous approaches such as
Binary Relevance, Classifier Chains, and Label Powerset, by achieving
micro-average precision (MAP) and recall (MAR) exceeding 90%. Compared to
previous work, we significantly improve performance by 19.98%, 10.15%, and
59.21% in MAP, MAR, and Hamming Loss, respectively. Next, we employ Explainable
Artificial Intelligence (XAI) techniques to interpret the decision-making
process within these models. Finally, we assess the robustness of all
techniques by crafting adversarial perturbations capable of manipulating
classifier predictions and generating false positives and negatives.",2024-09-25,"Ishan Karunanayake, Mashael AlSabah, Nadeem Ahmed, Sanjay Jha",http://arxiv.org/pdf/2409.16639v1,cs.LG
PIFS-Rec: Process-In-Fabric-Switch for Large-Scale Recommendation System Inferences,"Deep Learning Recommendation Models (DLRMs) have become increasingly popular
and prevalent in today's datacenters, consuming most of the AI inference
cycles. The performance of DLRMs is heavily influenced by available bandwidth
due to their large vector sizes in embedding tables and concurrent accesses. To
achieve substantial improvements over existing solutions, novel approaches
towards DLRM optimization are needed, especially, in the context of emerging
interconnect technologies like CXL. This study delves into exploring
CXL-enabled systems, implementing a process-in-fabric-switch (PIFS) solution to
accelerate DLRMs while optimizing their memory and bandwidth scalability. We
present an in-depth characterization of industry-scale DLRM workloads running
on CXL-ready systems, identifying the predominant bottlenecks in existing CXL
systems. We, therefore, propose PIFS-Rec, a PIFS-based scheme that implements
near-data processing through downstream ports of the fabric switch. PIFS-Rec
achieves a latency that is 3.89x lower than Pond, an industry-standard
CXL-based system, and also outperforms BEACON, a state-of-the-art scheme, by
2.03x.",2024-09-25,"Pingyi Huo, Anusha Devulapally, Hasan Al Maruf, Minseo Park, Krishnakumar Nair, Meena Arunachalam, Gulsum Gudukbay Akbulut, Mahmut Taylan Kandemir, Vijaykrishnan Narayanan",http://arxiv.org/pdf/2409.16633v1,cs.LG
Functional Stochastic Gradient MCMC for Bayesian Neural Networks,"Classical parameter-space Bayesian inference for Bayesian neural networks
(BNNs) suffers from several unresolved prior issues, such as knowledge encoding
intractability and pathological behaviours in deep networks, which can lead to
improper posterior inference. To address these issues, functional Bayesian
inference has recently been proposed leveraging functional priors, such as the
emerging functional variational inference. In addition to variational methods,
stochastic gradient Markov Chain Monte Carlo (MCMC) is another scalable and
effective inference method for BNNs to asymptotically generate samples from the
true posterior by simulating continuous dynamics. However, existing MCMC
methods perform solely in parameter space and inherit the unresolved prior
issues, while extending these dynamics to function space is a non-trivial
undertaking. In this paper, we introduce novel functional MCMC schemes,
including stochastic gradient versions, based on newly designed diffusion
dynamics that can incorporate more informative functional priors. Moreover, we
prove that the stationary measure of these functional dynamics is the target
posterior over functions. Our functional MCMC schemes demonstrate improved
performance in both predictive accuracy and uncertainty quantification on
several tasks compared to naive parameter-space MCMC and functional variational
inference.",2024-09-25,"Mengjing Wu, Junyu Xuan, Jie Lu",http://arxiv.org/pdf/2409.16632v2,cs.LG
Stochastic Subsampling With Average Pooling,"Regularization of deep neural networks has been an important issue to achieve
higher generalization performance without overfitting problems. Although the
popular method of Dropout provides a regularization effect, it causes
inconsistent properties in the output, which may degrade the performance of
deep neural networks. In this study, we propose a new module called stochastic
average pooling, which incorporates Dropout-like stochasticity in pooling. We
describe the properties of stochastic subsampling and average pooling and
leverage them to design a module without any inconsistency problem. The
stochastic average pooling achieves a regularization effect without any
potential performance degradation due to the inconsistency issue and can easily
be plugged into existing architectures of deep neural networks. Experiments
demonstrate that replacing existing average pooling with stochastic average
pooling yields consistent improvements across a variety of tasks, datasets, and
models.",2024-09-25,"Bum Jun Kim, Sang Woo Kim",http://arxiv.org/pdf/2409.16630v1,cs.LG
Ascend HiFloat8 Format for Deep Learning,"This preliminary white paper proposes a novel 8-bit floating-point data
format HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features tapered
precision. For normal value encoding, it provides 7 exponent values with 3-bit
mantissa, 8 exponent values with 2-bit mantissa, and 16 exponent values with
1-bit mantissa. For denormal value encoding, it extends the dynamic range by 7
extra powers of 2, from 31 to 38 binades (notice that FP16 covers 40 binades).
Meanwhile, HiF8 encodes all the special values except that positive zero and
negative zero are represented by only one bit-pattern. Thanks to the better
balance between precision and dynamic range, HiF8 can be simultaneously used in
both forward and backward passes of AI training. In this paper, we will
describe the definition and rounding methods of HiF8, as well as the tentative
training and inference solutions. To demonstrate the efficacy of HiF8, massive
simulation results on various neural networks, including traditional neural
networks and large language models (LLMs), will also be presented.",2024-09-25,"Yuanyong Luo, Zhongxing Zhang, Richard Wu, Hu Liu, Ying Jin, Kai Zheng, Minmin Wang, Zhanying He, Guipeng Hu, Luyao Chen, Tianchi Hu, Junsong Wang, Minqi Chen, Mikhaylov Dmitry, Korviakov Vladimir, Bobrin Maxim, Yuhao Hu, Guanfu Chen, Zeyi Huang",http://arxiv.org/pdf/2409.16626v2,cs.LG
Random Forest Regression Feature Importance for Climate Impact Pathway Detection,"Disturbances to the climate system, both natural and anthropogenic, have far
reaching impacts that are not always easy to identify or quantify using
traditional climate science analyses or causal modeling techniques. In this
paper, we develop a novel technique for discovering and ranking the chain of
spatio-temporal downstream impacts of a climate source, referred to herein as a
source-impact pathway, using Random Forest Regression (RFR) and SHapley
Additive exPlanation (SHAP) feature importances. Rather than utilizing RFR for
classification or regression tasks (the most common use case for RFR), we
propose a fundamentally new workflow in which we: (i) train random forest (RF)
regressors on a set of spatio-temporal features of interest, (ii) calculate
their pair-wise feature importances using the SHAP weights associated with
those features, and (iii) translate these feature importances into a weighted
pathway network (i.e., a weighted directed graph), which can be used to trace
out and rank interdependencies between climate features and/or modalities.
Importantly, while herein we employ RFR and SHAP feature importance in steps
(i) and (ii) of our algorithm, our novel workflow is in no way tied to these
approaches, which could be replaced with any regression method and sensitivity
method. We adopt a tiered verification approach to verify our new pathway
identification methodology. In this approach, we apply our method to ensembles
of data generated by running two increasingly complex benchmarks: (i) a set of
synthetic coupled equations, and (ii) a fully coupled simulation of the 1991
eruption of Mount Pinatubo in the Philippines performed using a modified
version 2 of the U.S. Department of Energy's Energy Exascale Earth System Model
(E3SMv2). We find that our RFR feature importance-based approach can accurately
detect known pathways of impact for both test cases.",2024-09-25,"Meredith G. L. Brown, Matt Peterson, Irina Tezaur, Kara Peterson, Diana Bull",http://arxiv.org/pdf/2409.16609v2,cs.LG
Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications,"Recent studies have evaluated the creativity/novelty of large language models
(LLMs) primarily from a semantic perspective, using benchmarks from cognitive
science. However, accessing the novelty in scholarly publications is a largely
unexplored area in evaluating LLMs. In this paper, we introduce a scholarly
novelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in
scholarly papers. SchNovel consists of 15000 pairs of papers across six fields
sampled from the arXiv dataset with publication dates spanning 2 to 10 years
apart. In each pair, the more recently published paper is assumed to be more
novel. Additionally, we propose RAG-Novelty, which simulates the review process
taken by human reviewers by leveraging the retrieval of similar papers to
assess novelty. Extensive experiments provide insights into the capabilities of
different LLMs to assess novelty and demonstrate that RAG-Novelty outperforms
recent baseline models.",2024-09-25,"Ethan Lin, Zhiyuan Peng, Yi Fang",http://arxiv.org/pdf/2409.16605v1,cs.LG
Generative Pre-trained Ranking Model with Over-parameterization at Web-Scale (Extended Abstract),"Learning to rank (LTR) is widely employed in web searches to prioritize
pertinent webpages from retrieved content based on input queries. However,
traditional LTR models encounter two principal obstacles that lead to
suboptimal performance: (1) the lack of well-annotated query-webpage pairs with
ranking scores covering a diverse range of search query popularities, which
hampers their ability to address queries across the popularity spectrum, and
(2) inadequately trained models that fail to induce generalized representations
for LTR, resulting in overfitting. To address these challenges, we propose a
\emph{\uline{G}enerative \uline{S}emi-\uline{S}upervised \uline{P}re-trained}
(GS2P) LTR model. We conduct extensive offline experiments on both a publicly
available dataset and a real-world dataset collected from a large-scale search
engine. Furthermore, we deploy GS2P in a large-scale web search engine with
realistic traffic, where we observe significant improvements in the real-world
application.",2024-09-25,"Yuchen Li, Haoyi Xiong, Linghe Kong, Jiang Bian, Shuaiqiang Wang, Guihai Chen, Dawei Yin",http://arxiv.org/pdf/2409.16594v1,cs.LG
MambaJSCC: Adaptive Deep Joint Source-Channel Coding with Generalized State Space Model,"Lightweight and efficient neural network models for deep joint source-channel
coding (JSCC) are crucial for semantic communications. In this paper, we
propose a novel JSCC architecture, named MambaJSCC, that achieves
state-of-the-art performance with low computational and parameter overhead.
MambaJSCC utilizes the visual state space model with channel adaptation
(VSSM-CA) blocks as its backbone for transmitting images over wireless
channels, where the VSSM-CA primarily consists of the generalized state space
models (GSSM) and the zero-parameter, zero-computational channel adaptation
method (CSI-ReST). We design the GSSM module, leveraging reversible matrix
transformations to express generalized scan expanding operations, and
theoretically prove that two GSSM modules can effectively capture global
information. We discover that GSSM inherently possesses the ability to adapt to
channels, a form of endogenous intelligence. Based on this, we design the
CSI-ReST method, which injects channel state information (CSI) into the initial
state of GSSM to utilize its native response, and into the residual state to
mitigate CSI forgetting, enabling effective channel adaptation without
introducing additional computational and parameter overhead. Experimental
results show that MambaJSCC not only outperforms existing JSCC methods (e.g.,
SwinJSCC) across various scenarios but also significantly reduces parameter
size, computational overhead, and inference delay.",2024-09-25,"Tong Wu, Zhiyong Chen, Meixia Tao, Yaping Sun, Xiaodong Xu, Wenjun Zhang, Ping Zhang",http://arxiv.org/pdf/2409.16592v1,cs.LG
Pre-trained Graphformer-based Ranking at Web-scale Search (Extended Abstract),"Both Transformer and Graph Neural Networks (GNNs) have been employed in the
domain of learning to rank (LTR). However, these approaches adhere to two
distinct yet complementary problem formulations: ranking score regression based
on query-webpage pairs, and link prediction within query-webpage bipartite
graphs, respectively. While it is possible to pre-train GNNs or Transformers on
source datasets and subsequently fine-tune them on sparsely annotated LTR
datasets, the distributional shifts between the pair-based and bipartite graph
domains present significant challenges in integrating these heterogeneous
models into a unified LTR framework at web scale. To address this, we introduce
the novel MPGraf model, which leverages a modular and capsule-based
pre-training strategy, aiming to cohesively integrate the regression
capabilities of Transformers with the link prediction strengths of GNNs. We
conduct extensive offline and online experiments to rigorously evaluate the
performance of MPGraf.",2024-09-25,"Yuchen Li, Haoyi Xiong, Linghe Kong, Zeyi Sun, Hongyang Chen, Shuaiqiang Wang, Dawei Yin",http://arxiv.org/pdf/2409.16590v1,cs.LG
AutoSTF: Decoupled Neural Architecture Search for Cost-Effective Automated Spatio-Temporal Forecasting,"Spatio-temporal forecasting is a critical component of various smart city
applications, such as transportation optimization, energy management, and
socio-economic analysis. Recently, several automated spatio-temporal
forecasting methods have been proposed to automatically search the optimal
neural network architecture for capturing complex spatio-temporal dependencies.
However, the existing automated approaches suffer from expensive neural
architecture search overhead, which hinders their practical use and the further
exploration of diverse spatio-temporal operators in a finer granularity. In
this paper, we propose AutoSTF, a decoupled automatic neural architecture
search framework for cost-effective automated spatio-temporal forecasting. From
the efficiency perspective, we first decouple the mixed search space into
temporal space and spatial space and respectively devise representation
compression and parameter-sharing schemes to mitigate the parameter explosion.
The decoupled spatio-temporal search not only expedites the model optimization
process but also leaves new room for more effective spatio-temporal dependency
modeling. From the effectiveness perspective, we propose a multi-patch transfer
module to jointly capture multi-granularity temporal dependencies and extend
the spatial search space to enable finer-grained layer-wise spatial dependency
search. Extensive experiments on eight datasets demonstrate the superiority of
AutoSTF in terms of both accuracy and efficiency. Specifically, our proposed
method achieves up to 13.48x speed-up compared to state-of-the-art automatic
spatio-temporal forecasting methods while maintaining the best forecasting
accuracy.",2024-09-25,"Tengfei Lyu, Weijia Zhang, Jinliang Deng, Hao Liu",http://arxiv.org/pdf/2409.16586v2,cs.LG
FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning,"In recent years, the Robotics field has initiated several efforts toward
building generalist robot policies through large-scale multi-task Behavior
Cloning. However, direct deployments of these policies have led to
unsatisfactory performance, where the policy struggles with unseen states and
tasks. How can we break through the performance plateau of these models and
elevate their capabilities to new heights? In this paper, we propose FLaRe, a
large-scale Reinforcement Learning fine-tuning framework that integrates robust
pre-trained representations, large-scale training, and gradient stabilization
techniques. Our method aligns pre-trained policies towards task completion,
achieving state-of-the-art (SoTA) performance both on previously demonstrated
and on entirely novel tasks and embodiments. Specifically, on a set of
long-horizon mobile manipulation tasks, FLaRe achieves an average success rate
of 79.5% in unseen environments, with absolute improvements of +23.6% in
simulation and +30.7% on real robots over prior SoTA methods. By utilizing only
sparse rewards, our approach can enable generalizing to new capabilities beyond
the pretraining data with minimal human effort. Moreover, we demonstrate rapid
adaptation to new embodiments and behaviors with less than a day of
fine-tuning. Videos can be found on the project website at
https://robot-flare.github.io/",2024-09-25,"Jiaheng Hu, Rose Hendrix, Ali Farhadi, Aniruddha Kembhavi, Roberto Martin-Martin, Peter Stone, Kuo-Hao Zeng, Kiana Ehsani",http://arxiv.org/pdf/2409.16578v2,cs.LG
Efficient and generalizable nested Fourier-DeepONet for three-dimensional geological carbon sequestration,"Geological carbon sequestration (GCS) involves injecting CO$_2$ into
subsurface geological formations for permanent storage. Numerical simulations
could guide decisions in GCS projects by predicting CO$_2$ migration pathways
and the pressure distribution in storage formation. However, these simulations
are often computationally expensive due to highly coupled physics and large
spatial-temporal simulation domains. Surrogate modeling with data-driven
machine learning has become a promising alternative to accelerate physics-based
simulations. Among these, the Fourier neural operator (FNO) has been applied to
three-dimensional synthetic subsurface models. Here, to further improve
performance, we have developed a nested Fourier-DeepONet by combining the
expressiveness of the FNO with the modularity of a deep operator network
(DeepONet). This new framework is twice as efficient as a nested FNO for
training and has at least 80% lower GPU memory requirement due to its
flexibility to treat temporal coordinates separately. These performance
improvements are achieved without compromising prediction accuracy. In
addition, the generalization and extrapolation ability of nested
Fourier-DeepONet beyond the training range has been thoroughly evaluated.
Nested Fourier-DeepONet outperformed the nested FNO for extrapolation in time
with more than 50% reduced error. It also exhibited good extrapolation accuracy
beyond the training range in terms of reservoir properties, number of wells,
and injection rate.",2024-09-25,"Jonathan E. Lee, Min Zhu, Ziqiao Xi, Kun Wang, Yanhua O. Yuan, Lu Lu",http://arxiv.org/pdf/2409.16572v1,cs.LG
EMIT- Event-Based Masked Auto Encoding for Irregular Time Series,"Irregular time series, where data points are recorded at uneven intervals,
are prevalent in healthcare settings, such as emergency wards where vital signs
and laboratory results are captured at varying times. This variability, which
reflects critical fluctuations in patient health, is essential for informed
clinical decision-making. Existing self-supervised learning research on
irregular time series often relies on generic pretext tasks like forecasting,
which may not fully utilise the signal provided by irregular time series. There
is a significant need for specialised pretext tasks designed for the
characteristics of irregular time series to enhance model performance and
robustness, especially in scenarios with limited data availability. This paper
proposes a novel pretraining framework, EMIT, an event-based masking for
irregular time series. EMIT focuses on masking-based reconstruction in the
latent space, selecting masking points based on the rate of change in the data.
This method preserves the natural variability and timing of measurements while
enhancing the model's ability to process irregular intervals without losing
essential information. Extensive experiments on the MIMIC-III and PhysioNet
Challenge datasets demonstrate the superior performance of our event-based
masking strategy. The code has been released at
https://github.com/hrishi-ds/EMIT.",2024-09-25,"Hrishikesh Patel, Ruihong Qiu, Adam Irwin, Shazia Sadiq, Sen Wang",http://arxiv.org/pdf/2409.16554v2,cs.LG
AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned Quantization,"Model quantization has become a crucial technique to address the issues of
large memory consumption and long inference times associated with LLMs.
Mixed-precision quantization, which distinguishes between important and
unimportant parameters, stands out among numerous quantization schemes as it
achieves a balance between precision and compression rate. However, existing
approaches can only identify important parameters through qualitative analysis
and manual experiments without quantitatively analyzing how their importance is
determined. We propose a new criterion, so-called 'precision alignment', to
build a quantitative framework to holistically evaluate the importance of
parameters in mixed-precision quantization. Our observations on floating point
addition under various real-world scenarios suggest that two addends should
have identical precision, otherwise the information in the higher-precision
number will be wasted. Such an observation offers an essential principle to
determine the precision of each parameter in matrix multiplication operation.
As the first step towards applying the above discovery to large model
inference, we develop a dynamic KV-Cache quantization technique to effectively
reduce memory access latency. Different from existing quantization approaches
that focus on memory saving, this work directly aims to accelerate LLM
inference through quantifying floating numbers. The proposed technique attains
a 25% saving of memory access and delivers up to 1.3x speedup in the
computation of attention in the decoding phase of LLM, with almost no loss of
precision.",2024-09-25,"Yifan Tan, Haoze Wang, Chao Yan, Yangdong Deng",http://arxiv.org/pdf/2409.16546v2,cs.LG
Monge-Kantorovich Fitting With Sobolev Budgets,"Given $m < n$, we consider the problem of ``best'' approximating an
$n\text{-d}$ probability measure $\rho$ via an $m\text{-d}$ measure $\nu$ such
that $\mathrm{supp}\ \nu$ has bounded total ``complexity.'' When $\rho$ is
concentrated near an $m\text{-d}$ set we may interpret this as a manifold
learning problem with noisy data. However, we do not restrict our analysis to
this case, as the more general formulation has broader applications.
  We quantify $\nu$'s performance in approximating $\rho$ via the
Monge-Kantorovich (also called Wasserstein) $p$-cost $\mathbb{W}_p^p(\rho,
\nu)$, and constrain the complexity by requiring $\mathrm{supp}\ \nu$ to be
coverable by an $f : \mathbb{R}^{m} \to \mathbb{R}^{n}$ whose $W^{k,q}$ Sobolev
norm is bounded by $\ell \geq 0$. This allows us to reformulate the problem as
minimizing a functional $\mathscr J_p(f)$ under the Sobolev ``budget'' $\ell$.
This problem is closely related to (but distinct from) principal curves with
length constraints when $m=1, k = 1$ and an unsupervised analogue of smoothing
splines when $k > 1$. New challenges arise from the higher-order
differentiability condition.
  We study the ``gradient'' of $\mathscr J_p$, which is given by a certain
vector field that we call the barycenter field, and use it to prove a
nontrivial (almost) strict monotonicity result. We also provide a natural
discretization scheme and establish its consistency. We use this scheme as a
toy model for a generative learning task, and by analogy, propose novel
interpretations for the role regularization plays in improving training.",2024-09-25,"Forest Kobayashi, Jonathan Hayase, Young-Heon Kim",http://arxiv.org/pdf/2409.16541v2,cs.LG
Source-Free Domain Adaptation for YOLO Object Detection,"Source-free domain adaptation (SFDA) is a challenging problem in object
detection, where a pre-trained source model is adapted to a new target domain
without using any source domain data for privacy and efficiency reasons. Most
state-of-the-art SFDA methods for object detection have been proposed for
Faster-RCNN, a detector that is known to have high computational complexity.
This paper focuses on domain adaptation techniques for real-world vision
systems, particularly for the YOLO family of single-shot detectors known for
their fast baselines and practical applications. Our proposed SFDA method -
Source-Free YOLO (SF-YOLO) - relies on a teacher-student framework in which the
student receives images with a learned, target domain-specific augmentation,
allowing the model to be trained with only unlabeled target data and without
requiring feature alignment. A challenge with self-training using a
mean-teacher architecture in the absence of labels is the rapid decline of
accuracy due to noisy or drifting pseudo-labels. To address this issue, a
teacher-to-student communication mechanism is introduced to help stabilize the
training and reduce the reliance on annotated target data for model selection.
Despite its simplicity, our approach is competitive with state-of-the-art
detectors on several challenging benchmark datasets, even sometimes
outperforming methods that use source data for adaptation.",2024-09-25,"Simon Varailhon, Masih Aminbeidokhti, Marco Pedersoli, Eric Granger",http://arxiv.org/pdf/2409.16538v1,cs.LG
A QoE-Aware Split Inference Accelerating Algorithm for NOMA-based Edge Intelligence,"Even the AI has been widely used and significantly changed our life,
deploying the large AI models on resource limited edge devices directly is not
appropriate. Thus, the model split inference is proposed to improve the
performance of edge intelligence, in which the AI model is divided into
different sub models and the resource-intensive sub model is offloaded to edge
server wirelessly for reducing resource requirements and inference latency.
However, the previous works mainly concentrate on improving and optimizing the
system QoS, ignore the effect of QoE which is another critical item for the
users except for QoS. Even the QoE has been widely learned in EC, considering
the differences between task offloading in EC and split inference in EI, and
the specific issues in QoE which are still not addressed in EC and EI, these
algorithms cannot work effectively in edge split inference scenarios. Thus, an
effective resource allocation algorithm is proposed in this paper, for
accelerating split inference in EI and achieving the tradeoff between inference
delay, QoE, and resource consumption, abbreviated as ERA. Specifically, the ERA
takes the resource consumption, QoE, and inference latency into account to find
the optimal model split strategy and resource allocation strategy. Since the
minimum inference delay and resource consumption, and maximum QoE cannot be
satisfied simultaneously, the gradient descent based algorithm is adopted to
find the optimal tradeoff between them. Moreover, the loop iteration GD
approach is developed to reduce the complexity of the GD algorithm caused by
parameter discretization. Additionally, the properties of the proposed
algorithms are investigated, including convergence, complexity, and
approximation error. The experimental results demonstrate that the performance
of ERA is much better than that of the previous studies.",2024-09-25,"Xin Yuan, Ning Li, Quan Chen, Wenchao Xu, Zhaoxin Zhang, Song Guo",http://arxiv.org/pdf/2409.16537v1,cs.LG
RAGProbe: An Automated Approach for Evaluating RAG Applications,"Retrieval Augmented Generation (RAG) is increasingly being used when building
Generative AI applications. Evaluating these applications and RAG pipelines is
mostly done manually, via a trial and error process. Automating evaluation of
RAG pipelines requires overcoming challenges such as context misunderstanding,
wrong format, incorrect specificity, and missing content. Prior works therefore
focused on improving evaluation metrics as well as enhancing components within
the pipeline using available question and answer datasets. However, they have
not focused on 1) providing a schema for capturing different types of
question-answer pairs or 2) creating a set of templates for generating
question-answer pairs that can support automation of RAG pipeline evaluation.
In this paper, we present a technique for generating variations in
question-answer pairs to trigger failures in RAG pipelines. We validate 5
open-source RAG pipelines using 3 datasets. Our approach revealed the highest
failure rates when prompts combine multiple questions: 91% for questions when
spanning multiple documents and 78% for questions from a single document;
indicating a need for developers to prioritise handling these combined
questions. 60% failure rate was observed in academic domain dataset and 53% and
62% failure rates were observed in open-domain datasets. Our automated approach
outperforms the existing state-of-the-art methods, by increasing the failure
rate by 51% on average per dataset. Our work presents an automated approach for
continuously monitoring the health of RAG pipelines, which can be integrated
into existing CI/CD pipelines, allowing for improved quality.",2024-09-24,"Shangeetha Sivasothy, Scott Barnett, Stefanus Kurniawan, Zafaryab Rasool, Rajesh Vasa",http://arxiv.org/pdf/2409.19019v1,cs.LG
GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for Improved Visual Localization,"Although various visual localization approaches exist, such as scene
coordinate regression and camera pose regression, these methods often struggle
with optimization complexity or limited accuracy. To address these challenges,
we explore the use of novel view synthesis techniques, particularly 3D Gaussian
Splatting (3DGS), which enables the compact encoding of both 3D geometry and
scene appearance. We propose a two-stage procedure that integrates dense and
robust keypoint descriptors from the lightweight XFeat feature extractor into
3DGS, enhancing performance in both indoor and outdoor environments. The coarse
pose estimates are directly obtained via 2D-3D correspondences between the 3DGS
representation and query image descriptors. In the second stage, the initial
pose estimate is refined by minimizing the rendering-based photometric warp
loss. Benchmarking on widely used indoor and outdoor datasets demonstrates
improvements over recent neural rendering-based localization methods, such as
NeRFMatch and PNeRFLoc.",2024-09-24,"Gennady Sidorov, Malik Mohrat, Denis Gridusov, Ruslan Rakhimov, Sergey Kolyubin",http://arxiv.org/pdf/2409.16502v3,cs.LG
Learning Linear Dynamics from Bilinear Observations,"We consider the problem of learning a realization of a partially observed
dynamical system with linear state transitions and bilinear observations. Under
very mild assumptions on the process and measurement noises, we provide a
finite time analysis for learning the unknown dynamics matrices (up to a
similarity transform). Our analysis involves a regression problem with
heavy-tailed and dependent data. Moreover, each row of our design matrix
contains a Kronecker product of current input with a history of inputs, making
it difficult to guarantee persistence of excitation. We overcome these
challenges, first providing a data-dependent high probability error bound for
arbitrary but fixed inputs. Then, we derive a data-independent error bound for
inputs chosen according to a simple random design. Our main results provide an
upper bound on the statistical error rates and sample complexity of learning
the unknown dynamics matrices from a single finite trajectory of bilinear
observations.",2024-09-24,"Yahya Sattar, Yassir Jedra, Sarah Dean",http://arxiv.org/pdf/2409.16499v1,cs.LG
Flight: A FaaS-Based Framework for Complex and Hierarchical Federated Learning,"Federated Learning (FL) is a decentralized machine learning paradigm where
models are trained on distributed devices and are aggregated at a central
server. Existing FL frameworks assume simple two-tier network topologies where
end devices are directly connected to the aggregation server. While this is a
practical mental model, it does not exploit the inherent topology of real-world
distributed systems like the Internet-of-Things. We present Flight, a novel FL
framework that supports complex hierarchical multi-tier topologies,
asynchronous aggregation, and decouples the control plane from the data plane.
We compare the performance of Flight against Flower, a state-of-the-art FL
framework. Our results show that Flight scales beyond Flower, supporting up to
2048 simultaneous devices, and reduces FL makespan across several models.
Finally, we show that Flight's hierarchical FL model can reduce communication
overheads by more than 60%.",2024-09-24,"Nathaniel Hudson, Valerie Hayot-Sasson, Yadu Babuji, Matt Baughman, J. Gregory Pauloski, Ryan Chard, Ian Foster, Kyle Chard",http://arxiv.org/pdf/2409.16495v1,cs.LG
Exploring Knowledge Tracing in Tutor-Student Dialogues using LLMs,"Recent advances in large language models (LLMs) have led to the development
of artificial intelligence (AI)-powered tutoring chatbots, showing promise in
providing broad access to high-quality personalized education. Existing works
have studied how to make LLMs follow tutoring principles, but have not studied
broader uses of LLMs for supporting tutoring. Up until now, tracing student
knowledge and analyzing misconceptions has been difficult and time-consuming to
implement for open-ended dialogue tutoring. In this work, we investigate
whether LLMs can be supportive of this task: we first use LLM prompting methods
to identify the knowledge components/skills involved in each dialogue turn,
i.e., a tutor utterance posing a task or a student utterance that responds to
it. We also evaluate whether the student responds correctly to the tutor and
verify the LLM's accuracy using human expert annotations. We then apply a range
of knowledge tracing (KT) methods on the resulting labeled data to track
student knowledge levels over an entire dialogue. We conduct experiments on two
tutoring dialogue datasets, and show that a novel yet simple LLM-based method,
LLMKT, significantly outperforms existing KT methods in predicting student
response correctness in dialogues. We perform extensive qualitative analyses to
highlight the challenges in dialogueKT and outline multiple avenues for future
work.",2024-09-24,"Alexander Scarlatos, Ryan S. Baker, Andrew Lan",http://arxiv.org/pdf/2409.16490v2,cs.LG
Diffusion Models to Enhance the Resolution of Microscopy Images: A Tutorial,"Diffusion models have emerged as a prominent technique in generative modeling
with neural networks, making their mark in tasks like text-to-image translation
and super-resolution. In this tutorial, we provide a comprehensive guide to
build denoising diffusion probabilistic models (DDPMs) from scratch, with a
specific focus on transforming low-resolution microscopy images into their
corresponding high-resolution versions. We provide the theoretical background,
mathematical derivations, and a detailed Python code implementation using
PyTorch, along with techniques to enhance model performance.",2024-09-24,"Harshith Bachimanchi, Giovanni Volpe",http://arxiv.org/pdf/2409.16488v1,cs.LG
Generative AI-driven forecasting of oil production,"Forecasting oil production from oilfields with multiple wells is an important
problem in petroleum and geothermal energy extraction, as well as energy
storage technologies. The accuracy of oil forecasts is a critical determinant
of economic projections, hydrocarbon reserves estimation, construction of fluid
processing facilities, and energy price fluctuations. Leveraging generative AI
techniques, we model time series forecasting of oil and water productions
across four multi-well sites spanning four decades. Our goal is to effectively
model uncertainties and make precise forecasts to inform decision-making
processes at the field scale. We utilize an autoregressive model known as
TimeGrad and a variant of a transformer architecture named Informer, tailored
specifically for forecasting long sequence time series data. Predictions from
both TimeGrad and Informer closely align with the ground truth data. The
overall performance of the Informer stands out, demonstrating greater
efficiency compared to TimeGrad in forecasting oil production rates across all
sites.",2024-09-24,"Yash Gandhi, Kexin Zheng, Birendra Jha, Ken-ichi Nomura, Aiichiro Nakano, Priya Vashishta, Rajiv K. Kalia",http://arxiv.org/pdf/2409.16482v1,cs.LG
Score-based Neural Ordinary Differential Equations for Computing Mean Field Control Problems,"Classical neural ordinary differential equations (ODEs) are powerful tools
for approximating the log-density functions in high-dimensional spaces along
trajectories, where neural networks parameterize the velocity fields. This
paper proposes a system of neural differential equations representing first-
and second-order score functions along trajectories based on deep neural
networks. We reformulate the mean field control (MFC) problem with individual
noises into an unconstrained optimization problem framed by the proposed neural
ODE system. Additionally, we introduce a novel regularization term to enforce
characteristics of viscous Hamilton--Jacobi--Bellman (HJB) equations to be
satisfied based on the evolution of the second-order score function. Examples
include regularized Wasserstein proximal operators (RWPOs), probability flow
matching of Fokker--Planck (FP) equations, and linear quadratic (LQ) MFC
problems, which demonstrate the effectiveness and accuracy of the proposed
method.",2024-09-24,"Mo Zhou, Stanley Osher, Wuchen Li",http://arxiv.org/pdf/2409.16471v2,cs.LG
Communication and Energy Efficient Federated Learning using Zero-Order Optimization Technique,"Federated learning (FL) is a popular machine learning technique that enables
multiple users to collaboratively train a model while maintaining the user data
privacy. A significant challenge in FL is the communication bottleneck in the
upload direction, and thus the corresponding energy consumption of the devices,
attributed to the increasing size of the model/gradient. In this paper, we
address this issue by proposing a zero-order (ZO) optimization method that
requires the upload of a quantized single scalar per iteration by each device
instead of the whole gradient vector. We prove its theoretical convergence and
find an upper bound on its convergence rate in the non-convex setting, and we
discuss its implementation in practical scenarios. Our FL method and the
corresponding convergence analysis take into account the impact of quantization
and packet dropping due to wireless errors. We show also the superiority of our
method, in terms of communication overhead and energy consumption, as compared
to standard gradient-based FL methods.",2024-09-24,"Elissa Mhanna, Mohamad Assaad",http://arxiv.org/pdf/2409.16456v1,cs.LG
Transfer learning for financial data predictions: a systematic review,"Literature highlighted that financial time series data pose significant
challenges for accurate stock price prediction, because these data are
characterized by noise and susceptibility to news; traditional statistical
methodologies made assumptions, such as linearity and normality, which are not
suitable for the non-linear nature of financial time series; on the other hand,
machine learning methodologies are able to capture non linear relationship in
the data. To date, neural network is considered the main machine learning tool
for the financial prices prediction. Transfer Learning, as a method aimed at
transferring knowledge from source tasks to target tasks, can represent a very
useful methodological tool for getting better financial prediction capability.
Current reviews on the above body of knowledge are mainly focused on neural
network architectures, for financial prediction, with very little emphasis on
the transfer learning methodology; thus, this paper is aimed at going deeper on
this topic by developing a systematic review with respect to application of
Transfer Learning for financial market predictions and to challenges/potential
future directions of the transfer learning methodologies for stock market
predictions.",2024-09-24,V. Lanzetta,http://arxiv.org/pdf/2409.17183v1,cs.LG
A Multi-Agent Multi-Environment Mixed Q-Learning for Partially Decentralized Wireless Network Optimization,"Q-learning is a powerful tool for network control and policy optimization in
wireless networks, but it struggles with large state spaces. Recent
advancements, like multi-environment mixed Q-learning (MEMQ), improves
performance and reduces complexity by integrating multiple Q-learning
algorithms across multiple related environments so-called digital cousins.
However, MEMQ is designed for centralized single-agent networks and is not
suitable for decentralized or multi-agent networks. To address this challenge,
we propose a novel multi-agent MEMQ algorithm for partially decentralized
wireless networks with multiple mobile transmitters (TXs) and base stations
(BSs), where TXs do not have access to each other's states and actions. In
uncoordinated states, TXs act independently to minimize their individual costs.
In coordinated states, TXs use a Bayesian approach to estimate the joint state
based on local observations and share limited information with leader TX to
minimize joint cost. The cost of information sharing scales linearly with the
number of TXs and is independent of the joint state-action space size. The
proposed scheme is 50% faster than centralized MEMQ with only a 20% increase in
average policy error (APE) and is 25% faster than several advanced
decentralized Q-learning algorithms with 40% less APE. The convergence of the
algorithm is also demonstrated.",2024-09-24,"Talha Bozkus, Urbashi Mitra",http://arxiv.org/pdf/2409.16450v2,cs.LG
A novel open-source ultrasound dataset with deep learning benchmarks for spinal cord injury localization and anatomical segmentation,"While deep learning has catalyzed breakthroughs across numerous domains, its
broader adoption in clinical settings is inhibited by the costly and
time-intensive nature of data acquisition and annotation. To further facilitate
medical machine learning, we present an ultrasound dataset of 10,223
Brightness-mode (B-mode) images consisting of sagittal slices of porcine spinal
cords (N=25) before and after a contusion injury. We additionally benchmark the
performance metrics of several state-of-the-art object detection algorithms to
localize the site of injury and semantic segmentation models to label the
anatomy for comparison and creation of task-specific architectures. Finally, we
evaluate the zero-shot generalization capabilities of the segmentation models
on human ultrasound spinal cord images to determine whether training on our
porcine dataset is sufficient for accurately interpreting human data. Our
results show that the YOLOv8 detection model outperforms all evaluated models
for injury localization, achieving a mean Average Precision (mAP50-95) score of
0.606. Segmentation metrics indicate that the DeepLabv3 segmentation model
achieves the highest accuracy on unseen porcine anatomy, with a Mean Dice score
of 0.587, while SAMed achieves the highest Mean Dice score generalizing to
human anatomy (0.445). To the best of our knowledge, this is the largest
annotated dataset of spinal cord ultrasound images made publicly available to
researchers and medical professionals, as well as the first public report of
object detection and segmentation architectures to assess anatomical markers in
the spinal cord for methodology development and clinical applications.",2024-09-24,"Avisha Kumar, Kunal Kotkar, Kelly Jiang, Meghana Bhimreddy, Daniel Davidar, Carly Weber-Levine, Siddharth Krishnan, Max J. Kerensky, Ruixing Liang, Kelley Kempski Leadingham, Denis Routkevitch, Andrew M. Hersh, Kimberly Ashayeri, Betty Tyler, Ian Suk, Jennifer Son, Nicholas Theodore, Nitish Thakor, Amir Manbachi",http://arxiv.org/pdf/2409.16441v1,cs.LG
Lessons and Insights from a Unifying Study of Parameter-Efficient Fine-Tuning (PEFT) in Visual Recognition,"Parameter-efficient fine-tuning (PEFT) has attracted significant attention
due to the growth of pre-trained model sizes and the need to fine-tune (FT)
them for superior downstream performance. Despite a surge in new PEFT methods,
a systematic study to understand their performance and suitable application
scenarios is lacking, leaving questions like ""when to apply PEFT"" and ""which
method to use"" largely unanswered, especially in visual recognition. In this
paper, we conduct a unifying empirical study of representative PEFT methods
with Vision Transformers. We systematically tune their hyperparameters to
fairly compare their accuracy on downstream tasks. Our study offers a practical
user guide and unveils several new insights. First, if tuned carefully,
different PEFT methods achieve similar accuracy in the low-shot benchmark
VTAB-1K. This includes simple approaches like FT the bias terms that were
reported inferior. Second, despite similar accuracy, we find that PEFT methods
make different mistakes and high-confidence predictions, likely due to their
different inductive biases. Such an inconsistency (or complementarity) opens up
the opportunity for ensemble methods, and we make preliminary attempts at this.
Third, going beyond the commonly used low-shot tasks, we find that PEFT is also
useful in many-shot regimes, achieving comparable or better accuracy than full
FT while using significantly fewer parameters. Lastly, we investigate PEFT's
ability to preserve a pre-trained model's robustness to distribution shifts
(e.g., CLIP). Perhaps not surprisingly, PEFT approaches outperform full FT
alone. However, with weight-space ensembles, full FT can better balance target
distribution and distribution shift performance, suggesting a future research
direction for robust PEFT.",2024-09-24,"Zheda Mai, Ping Zhang, Cheng-Hao Tu, Hong-You Chen, Li Zhang, Wei-Lun Chao",http://arxiv.org/pdf/2409.16434v5,cs.LG
Leveraging Local Structure for Improving Model Explanations: An Information Propagation Approach,"Numerous explanation methods have been recently developed to interpret the
decisions made by deep neural network (DNN) models. For image classifiers,
these methods typically provide an attribution score to each pixel in the image
to quantify its contribution to the prediction. However, most of these
explanation methods appropriate attribution scores to pixels independently,
even though both humans and DNNs make decisions by analyzing a set of closely
related pixels simultaneously. Hence, the attribution score of a pixel should
be evaluated jointly by considering itself and its structurally-similar pixels.
We propose a method called IProp, which models each pixel's individual
attribution score as a source of explanatory information and explains the image
prediction through the dynamic propagation of information across all pixels. To
formulate the information propagation, IProp adopts the Markov Reward Process,
which guarantees convergence, and the final status indicates the desired
pixels' attribution scores. Furthermore, IProp is compatible with any existing
attribution-based explanation method. Extensive experiments on various
explanation methods and DNN models verify that IProp significantly improves
them on a variety of interpretability metrics.",2024-09-24,"Ruo Yang, Binghui Wang, Mustafa Bilgic",http://arxiv.org/pdf/2409.16429v1,cs.LG
Statistical tuning of artificial neural network,"Neural networks are often regarded as ""black boxes"" due to their complex
functions and numerous parameters, which poses significant challenges for
interpretability. This study addresses these challenges by introducing methods
to enhance the understanding of neural networks, focusing specifically on
models with a single hidden layer. We establish a theoretical framework by
demonstrating that the neural network estimator can be interpreted as a
nonparametric regression model. Building on this foundation, we propose
statistical tests to assess the significance of input neurons and introduce
algorithms for dimensionality reduction, including clustering and (PCA), to
simplify the network and improve its interpretability and accuracy. The key
contributions of this study include the development of a bootstrapping
technique for evaluating artificial neural network (ANN) performance, applying
statistical tests and logistic regression to analyze hidden neurons, and
assessing neuron efficiency. We also investigate the behavior of individual
hidden neurons in relation to out-put neurons and apply these methodologies to
the IDC and Iris datasets to validate their practical utility. This research
advances the field of Explainable Artificial Intelligence by presenting robust
statistical frameworks for interpreting neural networks, thereby facilitating a
clearer understanding of the relationships between inputs, outputs, and
individual network components.",2024-09-24,"Mohamad Yamen AL Mohamad, Hossein Bevrani, Ali Akbar Haydari",http://arxiv.org/pdf/2409.16426v1,cs.LG
Lessons for Editors of AI Incidents from the AI Incident Database,"As artificial intelligence (AI) systems become increasingly deployed across
the world, they are also increasingly implicated in AI incidents - harm events
to individuals and society. As a result, industry, civil society, and
governments worldwide are developing best practices and regulations for
monitoring and analyzing AI incidents. The AI Incident Database (AIID) is a
project that catalogs AI incidents and supports further research by providing a
platform to classify incidents for different operational and research-oriented
goals. This study reviews the AIID's dataset of 750+ AI incidents and two
independent taxonomies applied to these incidents to identify common challenges
to indexing and analyzing AI incidents. We find that certain patterns of AI
incidents present structural ambiguities that challenge incident databasing and
explore how epistemic uncertainty in AI incident reporting is unavoidable. We
therefore report mitigations to make incident processes more robust to
uncertainty related to cause, extent of harm, severity, or technical details of
implicated systems. With these findings, we discuss how to develop future AI
incident reporting practices.",2024-09-24,"Kevin Paeth, Daniel Atherton, Nikiforos Pittaras, Heather Frase, Sean McGregor",http://arxiv.org/pdf/2409.16425v1,cs.LG
Is All Learning (Natural) Gradient Descent?,"This paper shows that a wide class of effective learning rules -- those that
improve a scalar performance measure over a given time window -- can be
rewritten as natural gradient descent with respect to a suitably defined loss
function and metric. Specifically, we show that parameter updates within this
class of learning rules can be expressed as the product of a symmetric positive
definite matrix (i.e., a metric) and the negative gradient of a loss function.
We also demonstrate that these metrics have a canonical form and identify
several optimal ones, including the metric that achieves the minimum possible
condition number. The proofs of the main results are straightforward, relying
only on elementary linear algebra and calculus, and are applicable to
continuous-time, discrete-time, stochastic, and higher-order learning rules, as
well as loss functions that explicitly depend on time.",2024-09-24,"Lucas Shoji, Kenta Suzuki, Leo Kozachkov",http://arxiv.org/pdf/2409.16422v1,cs.LG
Evaluating Blocking Biases in Entity Matching,"Entity Matching (EM) is crucial for identifying equivalent data entities
across different sources, a task that becomes increasingly challenging with the
growth and heterogeneity of data. Blocking techniques, which reduce the
computational complexity of EM, play a vital role in making this process
scalable. Despite advancements in blocking methods, the issue of fairness;
where blocking may inadvertently favor certain demographic groups; has been
largely overlooked. This study extends traditional blocking metrics to
incorporate fairness, providing a framework for assessing bias in blocking
techniques. Through experimental analysis, we evaluate the effectiveness and
fairness of various blocking methods, offering insights into their potential
biases. Our findings highlight the importance of considering fairness in EM,
particularly in the blocking phase, to ensure equitable outcomes in data
integration tasks.",2024-09-24,"Mohammad Hossein Moslemi, Harini Balamurugan, Mostafa Milani",http://arxiv.org/pdf/2409.16410v1,cs.LG
Modern Hopfield Networks meet Encoded Neural Representations -- Addressing Practical Considerations,"Content-addressable memories such as Modern Hopfield Networks (MHN) have been
studied as mathematical models of auto-association and storage/retrieval in the
human declarative memory, yet their practical use for large-scale content
storage faces challenges. Chief among them is the occurrence of meta-stable
states, particularly when handling large amounts of high dimensional content.
This paper introduces Hopfield Encoding Networks (HEN), a framework that
integrates encoded neural representations into MHNs to improve pattern
separability and reduce meta-stable states. We show that HEN can also be used
for retrieval in the context of hetero association of images with natural
language queries, thus removing the limitation of requiring access to partial
content in the same domain. Experimental results demonstrate substantial
reduction in meta-stable states and increased storage capacity while still
enabling perfect recall of a significantly larger number of inputs advancing
the practical utility of associative memory networks for real-world tasks.",2024-09-24,"Satyananda Kashyap, Niharika S. D'Souza, Luyao Shi, Ken C. L. Wong, Hongzhi Wang, Tanveer Syeda-Mahmood",http://arxiv.org/pdf/2409.16408v2,cs.LG
Towards Representation Learning for Weighting Problems in Design-Based Causal Inference,"Reweighting a distribution to minimize a distance to a target distribution is
a powerful and flexible strategy for estimating a wide range of causal effects,
but can be challenging in practice because optimal weights typically depend on
knowledge of the underlying data generating process. In this paper, we focus on
design-based weights, which do not incorporate outcome information; prominent
examples include prospective cohort studies, survey weighting, and the
weighting portion of augmented weighting estimators. In such applications, we
explore the central role of representation learning in finding desirable
weights in practice. Unlike the common approach of assuming a well-specified
representation, we highlight the error due to the choice of a representation
and outline a general framework for finding suitable representations that
minimize this error. Building on recent work that combines balancing weights
and neural networks, we propose an end-to-end estimation procedure that learns
a flexible representation, while retaining promising theoretical properties. We
show that this approach is competitive in a range of common causal inference
tasks.",2024-09-24,"Oscar Clivio, Avi Feller, Chris Holmes",http://arxiv.org/pdf/2409.16407v1,cs.LG
Rao-Blackwellized POMDP Planning,"Partially Observable Markov Decision Processes (POMDPs) provide a structured
framework for decision-making under uncertainty, but their application requires
efficient belief updates. Sequential Importance Resampling Particle Filters
(SIRPF), also known as Bootstrap Particle Filters, are commonly used as belief
updaters in large approximate POMDP solvers, but they face challenges such as
particle deprivation and high computational costs as the system's state
dimension grows. To address these issues, this study introduces
Rao-Blackwellized POMDP (RB-POMDP) approximate solvers and outlines generic
methods to apply Rao-Blackwellization in both belief updates and online
planning. We compare the performance of SIRPF and Rao-Blackwellized Particle
Filters (RBPF) in a simulated localization problem where an agent navigates
toward a target in a GPS-denied environment using POMCPOW and RB-POMCPOW
planners. Our results not only confirm that RBPFs maintain accurate belief
approximations over time with fewer particles, but, more surprisingly, RBPFs
combined with quadrature-based integration improve planning quality
significantly compared to SIRPF-based planning under the same computational
limits.",2024-09-24,"Jiho Lee, Nisar R. Ahmed, Kyle H. Wray, Zachary N. Sunberg",http://arxiv.org/pdf/2409.16392v2,cs.LG
Patch-Based Contrastive Learning and Memory Consolidation for Online Unsupervised Continual Learning,"We focus on a relatively unexplored learning paradigm known as {\em Online
Unsupervised Continual Learning} (O-UCL), where an agent receives a
non-stationary, unlabeled data stream and progressively learns to identify an
increasing number of classes. This paradigm is designed to model real-world
applications where encountering novelty is the norm, such as exploring a
terrain with several unknown and time-varying entities. Unlike prior work in
unsupervised, continual, or online learning, O-UCL combines all three areas
into a single challenging and realistic learning paradigm. In this setting,
agents are frequently evaluated and must aim to maintain the best possible
representation at any point of the data stream, rather than at the end of
pre-specified offline tasks. The proposed approach, called \textbf{P}atch-based
\textbf{C}ontrastive learning and \textbf{M}emory \textbf{C}onsolidation
(PCMC), builds a compositional understanding of data by identifying and
clustering patch-level features. Embeddings for these patch-level features are
extracted with an encoder trained via patch-based contrastive learning. PCMC
incorporates new data into its distribution while avoiding catastrophic
forgetting, and it consolidates memory examples during ``sleep"" periods. We
evaluate PCMC's performance on streams created from the ImageNet and Places365
datasets. Additionally, we explore various versions of the PCMC algorithm and
compare its performance against several existing methods and simple baselines.",2024-09-24,"Cameron Taylor, Vassilis Vassiliades, Constantine Dovrolis",http://arxiv.org/pdf/2409.16391v1,cs.LG
SpaRG: Sparsely Reconstructed Graphs for Generalizable fMRI Analysis,"Deep learning can help uncover patterns in resting-state functional Magnetic
Resonance Imaging (rs-fMRI) associated with psychiatric disorders and personal
traits. Yet the problem of interpreting deep learning findings is rarely more
evident than in fMRI analyses, as the data is sensitive to scanning effects and
inherently difficult to visualize. We propose a simple approach to mitigate
these challenges grounded on sparsification and self-supervision. Instead of
extracting post-hoc feature attributions to uncover functional connections that
are important to the target task, we identify a small subset of highly
informative connections during training and occlude the rest. To this end, we
jointly train a (1) sparse input mask, (2) variational autoencoder (VAE), and
(3) downstream classifier in an end-to-end fashion. While we need a portion of
labeled samples to train the classifier, we optimize the sparse mask and VAE
with unlabeled data from additional acquisition sites, retaining only the input
features that generalize well. We evaluate our method - Sparsely Reconstructed
Graphs (SpaRG) - on the public ABIDE dataset for the task of sex
classification, training with labeled cases from 18 sites and adapting the
model to two additional out-of-distribution sites with a portion of unlabeled
samples. For a relatively coarse parcellation (64 regions), SpaRG utilizes only
1% of the original connections while improving the classification accuracy
across domains. Our code can be found at github.com/yanismiraoui/SpaRG.",2024-09-24,"Camila González, Yanis Miraoui, Yiran Fan, Ehsan Adeli, Kilian M. Pohl",http://arxiv.org/pdf/2410.07201v1,cs.LG
Development and Application of a Sentinel-2 Satellite Imagery Dataset for Deep-Learning Driven Forest Wildfire Detection,"Forest loss due to natural events, such as wildfires, represents an
increasing global challenge that demands advanced analytical methods for
effective detection and mitigation. To this end, the integration of satellite
imagery with deep learning (DL) methods has become essential. Nevertheless,
this approach requires substantial amounts of labeled data to produce accurate
results. In this study, we use bi-temporal Sentinel-2 satellite imagery sourced
from Google Earth Engine (GEE) to build the California Wildfire GeoImaging
Dataset (CWGID), a high-resolution labeled satellite imagery dataset with over
100,000 labeled before and after forest wildfire image pairs for wildfire
detection through DL. Our methods include data acquisition from authoritative
sources, data processing, and an initial dataset analysis using three
pre-trained Convolutional Neural Network (CNN) architectures. Our results show
that the EF EfficientNet-B0 model achieves the highest accuracy of over 92% in
detecting forest wildfires. The CWGID and the methodology used to build it,
prove to be a valuable resource for training and testing DL architectures for
forest wildfire detection.",2024-09-24,"Valeria Martin, K. Brent Venable, Derek Morgan",http://arxiv.org/pdf/2409.16380v1,cs.LG
Scalable quantum dynamics compilation via quantum machine learning,"Quantum dynamics compilation is an important task for improving quantum
simulation efficiency: It aims to synthesize multi-qubit target dynamics into a
circuit consisting of as few elementary gates as possible. Compared to
deterministic methods such as Trotterization, variational quantum compilation
(VQC) methods employ variational optimization to reduce gate costs while
maintaining high accuracy. In this work, we explore the potential of a VQC
scheme by making use of out-of-distribution generalization results in quantum
machine learning (QML): By learning the action of a given many-body dynamics on
a small data set of product states, we can obtain a unitary circuit that
generalizes to highly entangled states such as the Haar random states. The
efficiency in training allows us to use tensor network methods to compress such
time-evolved product states by exploiting their low entanglement features. Our
approach exceeds state-of-the-art compilation results in both system size and
accuracy in one dimension ($1$D). For the first time, we extend VQC to systems
on two-dimensional (2D) strips with a quasi-1D treatment, demonstrating a
significant resource advantage over standard Trotterization methods,
highlighting the method's promise for advancing quantum simulation tasks on
near-term quantum processors.",2024-09-24,"Yuxuan Zhang, Roeland Wiersema, Juan Carrasquilla, Lukasz Cincio, Yong Baek Kim",http://arxiv.org/pdf/2409.16346v1,cs.LG
Articulated Object Manipulation using Online Axis Estimation with SAM2-Based Tracking,"Articulated object manipulation requires precise object interaction, where
the object's axis must be carefully considered. Previous research employed
interactive perception for manipulating articulated objects, but typically,
open-loop approaches often suffer from overlooking the interaction dynamics. To
address this limitation, we present a closed-loop pipeline integrating
interactive perception with online axis estimation from segmented 3D point
clouds. Our method leverages any interactive perception technique as a
foundation for interactive perception, inducing slight object movement to
generate point cloud frames of the evolving dynamic scene. These point clouds
are then segmented using Segment Anything Model 2 (SAM2), after which the
moving part of the object is masked for accurate motion online axis estimation,
guiding subsequent robotic actions. Our approach significantly enhances the
precision and efficiency of manipulation tasks involving articulated objects.
Experiments in simulated environments demonstrate that our method outperforms
baseline approaches, especially in tasks that demand precise axis-based
control. Project Page:
https://hytidel.github.io/video-tracking-for-axis-estimation/.",2024-09-24,"Xi Wang, Tianxing Chen, Qiaojun Yu, Tianling Xu, Zanxin Chen, Yiting Fu, Ziqi He, Cewu Lu, Yao Mu, Ping Luo",http://arxiv.org/pdf/2409.16287v2,cs.LG
Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation,"How can robot manipulation policies generalize to novel tasks involving
unseen object types and new motions? In this paper, we provide a solution in
terms of predicting motion information from web data through human video
generation and conditioning a robot policy on the generated video. Instead of
attempting to scale robot data collection which is expensive, we show how we
can leverage video generation models trained on easily available web data, for
enabling generalization. Our approach Gen2Act casts language-conditioned
manipulation as zero-shot human video generation followed by execution with a
single policy conditioned on the generated video. To train the policy, we use
an order of magnitude less robot interaction data compared to what the video
prediction model was trained on. Gen2Act doesn't require fine-tuning the video
model at all and we directly use a pre-trained model for generating human
videos. Our results on diverse real-world scenarios show how Gen2Act enables
manipulating unseen object types and performing novel motions for tasks not
present in the robot data. Videos are at https://homangab.github.io/gen2act/",2024-09-24,"Homanga Bharadhwaj, Debidatta Dwibedi, Abhinav Gupta, Shubham Tulsiani, Carl Doersch, Ted Xiao, Dhruv Shah, Fei Xia, Dorsa Sadigh, Sean Kirmani",http://arxiv.org/pdf/2409.16283v1,cs.LG
Transformer based time series prediction of the maximum power point for solar photovoltaic cells,"This paper proposes an improved deep learning based maximum power point
tracking (MPPT) in solar photovoltaic cells considering various time series
based environmental inputs. Generally, artificial neural network based MPPT
algorithms use basic neural network architectures and inputs which do not
represent the ambient conditions in a comprehensive manner. In this article,
the ambient conditions of a location are represented through a comprehensive
set of environmental features. Furthermore, the inclusion of time based
features in the input data is considered to model cyclic patterns temporally
within the atmospheric conditions leading to robust modeling of the MPPT
algorithm. A transformer based deep learning architecture is trained as a time
series prediction model using multidimensional time series input features. The
model is trained on a dataset containing typical meteorological year data
points of ambient weather conditions from 50 locations. The attention mechanism
in the transformer modules allows the model to learn temporal patterns in the
data efficiently. The proposed model achieves a 0.47% mean average percentage
error of prediction on non zero operating voltage points in a test dataset
consisting of data collected over a period of 200 consecutive hours resulting
in the average power efficiency of 99.54% and peak power efficiency of 99.98%.
The proposed model is validated through real time simulations. The proposed
model performs power point tracking in a robust, dynamic, and nonlatent manner,
over a wide range of atmospheric conditions.",2024-09-24,"Palaash Agrawal, Hari Om Bansal, Aditya R. Gautam, Om Prakash Mahela, Baseem Khan",http://arxiv.org/pdf/2409.16342v1,cs.LG
Learning To Help: Training Models to Assist Legacy Devices,"Machine learning models implemented in hardware on physical devices may be
deployed for a long time. The computational abilities of the device may be
limited and become outdated with respect to newer improvements. Because of the
size of ML models, offloading some computation (e.g. to an edge cloud) can help
such legacy devices. We cast this problem in the framework of learning with
abstention (LWA) in which the expert (edge) must be trained to assist the
client (device). Prior work on LWA trains the client assuming the edge is
either an oracle or a human expert. In this work, we formalize the reverse
problem of training the expert for a fixed (legacy) client. As in LWA, the
client uses a rejection rule to decide when to offload inference to the expert
(at a cost). We find the Bayes-optimal rule, prove a generalization bound, and
find a consistent surrogate loss function. Empirical results show that our
framework outperforms confidence-based rejection rules.",2024-09-24,"Yu Wu, Anand Sarwate",http://arxiv.org/pdf/2409.16253v1,cs.LG
Fields of The World: A Machine Learning Benchmark Dataset For Global Agricultural Field Boundary Segmentation,"Crop field boundaries are foundational datasets for agricultural monitoring
and assessments but are expensive to collect manually. Machine learning (ML)
methods for automatically extracting field boundaries from remotely sensed
images could help realize the demand for these datasets at a global scale.
However, current ML methods for field instance segmentation lack sufficient
geographic coverage, accuracy, and generalization capabilities. Further,
research on improving ML methods is restricted by the lack of labeled datasets
representing the diversity of global agricultural fields. We present Fields of
The World (FTW) -- a novel ML benchmark dataset for agricultural field instance
segmentation spanning 24 countries on four continents (Europe, Africa, Asia,
and South America). FTW is an order of magnitude larger than previous datasets
with 70,462 samples, each containing instance and semantic segmentation masks
paired with multi-date, multi-spectral Sentinel-2 satellite images. We provide
results from baseline models for the new FTW benchmark, show that models
trained on FTW have better zero-shot and fine-tuning performance in held-out
countries than models that aren't pre-trained with diverse datasets, and show
positive qualitative zero-shot results of FTW models in a real-world scenario
-- running on Sentinel-2 scenes over Ethiopia.",2024-09-24,"Hannah Kerner, Snehal Chaudhari, Aninda Ghosh, Caleb Robinson, Adeel Ahmad, Eddie Choi, Nathan Jacobs, Chris Holmes, Matthias Mohr, Rahul Dodhia, Juan M. Lavista Ferres, Jennifer Marcus",http://arxiv.org/pdf/2409.16252v2,cs.LG
Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs,"Training large language models (LLMs) for external tool usage is a rapidly
expanding field, with recent research focusing on generating synthetic data to
address the shortage of available data. However, the absence of systematic data
quality checks poses complications for properly training and testing models. To
that end, we propose two approaches for assessing the reliability of data for
training LLMs to use external tools. The first approach uses intuitive,
human-defined correctness criteria. The second approach uses a model-driven
assessment with in-context evaluation. We conduct a thorough evaluation of data
quality on two popular benchmarks, followed by an extrinsic evaluation that
showcases the impact of data quality on model performance. Our results
demonstrate that models trained on high-quality data outperform those trained
on unvalidated data, even when trained with a smaller quantity of data. These
findings empirically support the significance of assessing and ensuring the
reliability of training data for tool-using LLMs.",2024-09-24,"Shadi Iskander, Nachshon Cohen, Zohar Karnin, Ori Shapira, Sofia Tolmach",http://arxiv.org/pdf/2409.16341v2,cs.LG
"Predicting Deterioration in Mild Cognitive Impairment with Survival Transformers, Extreme Gradient Boosting and Cox Proportional Hazard Modelling","The paper proposes a novel approach of survival transformers and extreme
gradient boosting models in predicting cognitive deterioration in individuals
with mild cognitive impairment (MCI) using metabolomics data in the ADNI
cohort. By leveraging advanced machine learning and transformer-based
techniques applied in survival analysis, the proposed approach highlights the
potential of these techniques for more accurate early detection and
intervention in Alzheimer's dementia disease. This research also underscores
the importance of non-invasive biomarkers and innovative modelling tools in
enhancing the accuracy of dementia risk assessments, offering new avenues for
clinical practice and patient care. A comprehensive Monte Carlo simulation
procedure consisting of 100 repetitions of a nested cross-validation in which
models were trained and evaluated, indicates that the survival machine learning
models based on Transformer and XGBoost achieved the highest mean C-index
performances, namely 0.85 and 0.8, respectively, and that they are superior to
the conventional survival analysis Cox Proportional Hazards model which
achieved a mean C-Index of 0.77. Moreover, based on the standard deviations of
the C-Index performances obtained in the Monte Carlo simulation, we established
that both survival machine learning models above are more stable than the
conventional statistical model.",2024-09-24,"Henry Musto, Daniel Stamate, Doina Logofatu, Daniel Stahl",http://arxiv.org/pdf/2409.16231v1,cs.LG
"Fine-Tuning is Fine, if Calibrated","Fine-tuning is arguably the most straightforward way to tailor a pre-trained
model (e.g., a foundation model) to downstream applications, but it also comes
with the risk of losing valuable knowledge the model had learned in
pre-training. For example, fine-tuning a pre-trained classifier capable of
recognizing a large number of classes to master a subset of classes at hand is
shown to drastically degrade the model's accuracy in the other classes it had
previously learned. As such, it is hard to further use the fine-tuned model
when it encounters classes beyond the fine-tuning data. In this paper, we
systematically dissect the issue, aiming to answer the fundamental question,
""What has been damaged in the fine-tuned model?"" To our surprise, we find that
the fine-tuned model neither forgets the relationship among the other classes
nor degrades the features to recognize these classes. Instead, the fine-tuned
model often produces more discriminative features for these other classes, even
if they were missing during fine-tuning! {What really hurts the accuracy is the
discrepant logit scales between the fine-tuning classes and the other classes},
implying that a simple post-processing calibration would bring back the
pre-trained model's capability and at the same time unveil the feature
improvement over all classes. We conduct an extensive empirical study to
demonstrate the robustness of our findings and provide preliminary explanations
underlying them, suggesting new directions for future theoretical analysis. Our
code is available at
https://github.com/OSU-MLB/Fine-Tuning-Is-Fine-If-Calibrated.",2024-09-24,"Zheda Mai, Arpita Chowdhury, Ping Zhang, Cheng-Hao Tu, Hong-You Chen, Vardaan Pahuja, Tanya Berger-Wolf, Song Gao, Charles Stewart, Yu Su, Wei-Lun Chao",http://arxiv.org/pdf/2409.16223v3,cs.LG
Problem-oriented AutoML in Clustering,"The Problem-oriented AutoML in Clustering (PoAC) framework introduces a
novel, flexible approach to automating clustering tasks by addressing the
shortcomings of traditional AutoML solutions. Conventional methods often rely
on predefined internal Clustering Validity Indexes (CVIs) and static
meta-features, limiting their adaptability and effectiveness across diverse
clustering tasks. In contrast, PoAC establishes a dynamic connection between
the clustering problem, CVIs, and meta-features, allowing users to customize
these components based on the specific context and goals of their task. At its
core, PoAC employs a surrogate model trained on a large meta-knowledge base of
previous clustering datasets and solutions, enabling it to infer the quality of
new clustering pipelines and synthesize optimal solutions for unseen datasets.
Unlike many AutoML frameworks that are constrained by fixed evaluation metrics
and algorithm sets, PoAC is algorithm-agnostic, adapting seamlessly to
different clustering problems without requiring additional data or retraining.
Experimental results demonstrate that PoAC not only outperforms
state-of-the-art frameworks on a variety of datasets but also excels in
specific tasks such as data visualization, and highlight its ability to
dynamically adjust pipeline configurations based on dataset complexity.",2024-09-24,"Matheus Camilo da Silva, Gabriel Marques Tavares, Eric Medvet, Sylvio Barbon Junior",http://arxiv.org/pdf/2409.16218v1,cs.LG
Neural Coordination and Capacity Control for Inventory Management,"This paper addresses the capacitated periodic review inventory control
problem, focusing on a retailer managing multiple products with limited shared
resources, such as storage or inbound labor at a facility. Specifically, this
paper is motivated by the questions of (1) what does it mean to backtest a
capacity control mechanism, (2) can we devise and backtest a capacity control
mechanism that is compatible with recent advances in deep reinforcement
learning for inventory management? First, because we only have a single
historic sample path of Amazon's capacity limits, we propose a method that
samples from a distribution of possible constraint paths covering a space of
real-world scenarios. This novel approach allows for more robust and realistic
testing of inventory management strategies. Second, we extend the exo-IDP
(Exogenous Decision Process) formulation of Madeka et al. 2022 to capacitated
periodic review inventory control problems and show that certain capacitated
control problems are no harder than supervised learning. Third, we introduce a
`neural coordinator', designed to produce forecasts of capacity prices, guiding
the system to adhere to target constraints in place of a traditional model
predictive controller. Finally, we apply a modified DirectBackprop algorithm
for learning a deep RL buying policy and a training the neural coordinator. Our
methodology is evaluated through large-scale backtests, demonstrating RL buying
policies with a neural coordinator outperforms classic baselines both in terms
of cumulative discounted reward and capacity adherence (we see improvements of
up to 50% in some cases).",2024-09-24,"Carson Eisenach, Udaya Ghai, Dhruv Madeka, Kari Torkkola, Dean Foster, Sham Kakade",http://arxiv.org/pdf/2410.02817v1,cs.LG
Deep Learning for Precision Agriculture: Post-Spraying Evaluation and Deposition Estimation,"Precision spraying evaluation requires automation primarily in post-spraying
imagery. In this paper we propose an eXplainable Artificial Intelligence (XAI)
computer vision pipeline to evaluate a precision spraying system post-spraying
without the need for traditional agricultural methods. The developed system can
semantically segment potential targets such as lettuce, chickweed, and
meadowgrass and correctly identify if targets have been sprayed. Furthermore,
this pipeline evaluates using a domain-specific Weakly Supervised Deposition
Estimation task, allowing for class-specific quantification of spray deposit
weights in {\mu}L. Estimation of coverage rates of spray deposition in a
class-wise manner allows for further understanding of effectiveness of
precision spraying systems. Our study evaluates different Class Activation
Mapping techniques, namely AblationCAM and ScoreCAM, to determine which is more
effective and interpretable for these tasks. In the pipeline, inference-only
feature fusion is used to allow for further interpretability and to enable the
automation of precision spraying evaluation post-spray. Our findings indicate
that a Fully Convolutional Network with an EfficientNet-B0 backbone and
inference-only feature fusion achieves an average absolute difference in
deposition values of 156.8 {\mu}L across three classes in our test set. The
dataset curated in this paper is publicly available at
https://github.com/Harry-Rogers/PSIE",2024-09-24,"Harry Rogers, Tahmina Zebin, Grzegorz Cielniak, Beatriz De La Iglesia, Ben Magri",http://arxiv.org/pdf/2409.16213v1,cs.LG
MaskBit: Embedding-free Image Generation via Bit Tokens,"Masked transformer models for class-conditional image generation have become
a compelling alternative to diffusion models. Typically comprising two stages -
an initial VQGAN model for transitioning between latent space and image space,
and a subsequent Transformer model for image generation within latent space -
these frameworks offer promising avenues for image synthesis. In this study, we
present two primary contributions: Firstly, an empirical and systematic
examination of VQGANs, leading to a modernized VQGAN. Secondly, a novel
embedding-free generation network operating directly on bit tokens - a binary
quantized representation of tokens with rich semantics. The first contribution
furnishes a transparent, reproducible, and high-performing VQGAN model,
enhancing accessibility and matching the performance of current
state-of-the-art methods while revealing previously undisclosed details. The
second contribution demonstrates that embedding-free image generation using bit
tokens achieves a new state-of-the-art FID of 1.52 on the ImageNet 256x256
benchmark, with a compact generator model of mere 305M parameters. The code for
this project is available on https://github.com/markweberdev/maskbit.",2024-09-24,"Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, Liang-Chieh Chen",http://arxiv.org/pdf/2409.16211v2,cs.LG
"Large-scale digital phenotyping: identifying depression and anxiety indicators in a general UK population with over 10,000 participants","Digital phenotyping offers a novel and cost-efficient approach for managing
depression and anxiety. Previous studies, often limited to small-to-medium or
specific populations, may lack generalizability. We conducted a cross-sectional
analysis of data from 10,129 participants recruited from a UK-based general
population between June 2020 and August 2022. Participants shared wearable
(Fitbit) data and self-reported questionnaires on depression (PHQ-8), anxiety
(GAD-7), and mood via a study app. We first examined the correlations between
PHQ-8/GAD-7 scores and wearable-derived features, demographics, health data,
and mood assessments. Subsequently, unsupervised clustering was used to
identify behavioural patterns associated with depression or anxiety. Finally,
we employed separate XGBoost models to predict depression and anxiety and
compared the results using different subsets of features. We observed
significant associations between the severity of depression and anxiety with
several factors, including mood, age, gender, BMI, sleep patterns, physical
activity, and heart rate. Clustering analysis revealed that participants
simultaneously exhibiting lower physical activity levels and higher heart rates
reported more severe symptoms. Prediction models incorporating all types of
variables achieved the best performance ($R^2$=0.41, MAE=3.42 for depression;
$R^2$=0.31, MAE=3.50 for anxiety) compared to those using subsets of variables.
This study identified potential indicators for depression and anxiety,
highlighting the utility of digital phenotyping and machine learning
technologies for rapid screening of mental disorders in general populations.
These findings provide robust real-world insights for future healthcare
applications.",2024-09-24,"Yuezhou Zhang, Callum Stewart, Yatharth Ranjan, Pauline Conde, Heet Sankesara, Zulqarnain Rashid, Shaoxiong Sun, Richard J B Dobson, Amos A Folarin",http://arxiv.org/pdf/2409.16339v1,cs.LG
"AUGUR, A flexible and efficient optimization algorithm for identification of optimal adsorption sites","In this paper, we propose a novel flexible optimization pipeline for
determining the optimal adsorption sites, named AUGUR (Aware of Uncertainty
Graph Unit Regression). Our model combines graph neural networks and Gaussian
processes to create a flexible, efficient, symmetry-aware, translation, and
rotation-invariant predictor with inbuilt uncertainty quantification. This
predictor is then used as a surrogate for a data-efficient Bayesian
Optimization scheme to determine the optimal adsorption positions. This
pipeline determines the optimal position of large and complicated clusters with
far fewer iterations than current state-of-the-art approaches. Further, it does
not rely on hand-crafted features and can be seamlessly employed on any
molecule without any alterations. Additionally, the pooling properties of
graphs allow for the processing of molecules of different sizes by the same
model. This allows the energy prediction of computationally demanding systems
by a model trained on comparatively smaller and less expensive ones",2024-09-24,"Ioannis Kouroudis, Poonam, Neel Misciaci, Felix Mayr, Leon Müller, Zhaosu Gu, Alessio Gagliardi",http://arxiv.org/pdf/2409.16204v1,cs.LG
Second Order Bounds for Contextual Bandits with Function Approximation,"Many works have developed no-regret algorithms for contextual bandits with
function approximation, where the mean reward function over context-action
pairs belongs to a function class. Although there are many approaches to this
problem, one that has gained in importance is the use of algorithms based on
the optimism principle such as optimistic least squares. It can be shown the
regret of this algorithm scales as square root of the product of the eluder
dimension (a statistical measure of the complexity of the function class), the
logarithm of the function class size and the time horizon. Unfortunately, even
if the variance of the measurement noise of the rewards at each time is
changing and is very small, the regret of the optimistic least squares
algorithm scales with square root of the time horizon. In this work we are the
first to develop algorithms that satisfy regret bounds of scaling not with the
square root of the time horizon, but the square root of the sum of the
measurement variances in the setting of contextual bandits with function
approximation when the variances are unknown. These bounds generalize existing
techniques for deriving second order bounds in contextual linear problems.",2024-09-24,Aldo Pacchiano,http://arxiv.org/pdf/2409.16197v3,cs.LG
Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering,"Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning
large language models (LLMs) to various domains due to its modular design and
widespread availability on platforms like Huggingface. This modularity has
sparked interest in combining multiple LoRAs to enhance LLM capabilities.
However, existing methods for LoRA composition primarily focus on task-specific
adaptations that require additional training, and current model merging
techniques often fail to fully leverage LoRA's modular nature, leading to
parameter interference and performance degradation. In this paper, we
investigate the feasibility of disassembling and reassembling multiple LoRAs at
a finer granularity, analogous to assembling LEGO blocks. We introduce the
concept of Minimal Semantic Units (MSUs), where the parameters corresponding to
each rank in LoRA function as independent units. These MSUs demonstrate
permutation invariance and concatenation-summation equivalence properties,
enabling flexible combinations to create new LoRAs. Building on these insights,
we propose the LoRA-LEGO framework. This framework conducts rank-wise parameter
clustering by grouping MSUs from different LoRAs into $k$ clusters. The
centroid of each cluster serves as a representative MSU, enabling the assembly
of a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual
reweighting strategy to optimize the scale of the merged LoRA. Experiments
across various benchmarks demonstrate that our method outperforms existing
approaches in LoRA merging.",2024-09-24,"Ziyu Zhao, Tao Shen, Didi Zhu, Zexi Li, Jing Su, Xuwu Wang, Kun Kuang, Fei Wu",http://arxiv.org/pdf/2409.16167v3,cs.LG
Seeing Faces in Things: A Model and Dataset for Pareidolia,"The human visual system is well-tuned to detect faces of all shapes and
sizes. While this brings obvious survival advantages, such as a better chance
of spotting unknown predators in the bush, it also leads to spurious face
detections. ``Face pareidolia'' describes the perception of face-like structure
among otherwise random stimuli: seeing faces in coffee stains or clouds in the
sky. In this paper, we study face pareidolia from a computer vision
perspective. We present an image dataset of ``Faces in Things'', consisting of
five thousand web images with human-annotated pareidolic faces. Using this
dataset, we examine the extent to which a state-of-the-art human face detector
exhibits pareidolia, and find a significant behavioral gap between humans and
machines. We find that the evolutionary need for humans to detect animal faces,
as well as human faces, may explain some of this gap. Finally, we propose a
simple statistical model of pareidolia in images. Through studies on human
subjects and our pareidolic face detectors we confirm a key prediction of our
model regarding what image conditions are most likely to induce pareidolia.
Dataset and Website: https://aka.ms/faces-in-things",2024-09-24,"Mark Hamilton, Simon Stent, Vasha DuTell, Anne Harrington, Jennifer Corbett, Ruth Rosenholtz, William T. Freeman",http://arxiv.org/pdf/2409.16143v1,cs.LG
Evaluation of state-of-the-art ASR Models in Child-Adult Interactions,"The ability to reliably transcribe child-adult conversations in a clinical
setting is valuable for diagnosis and understanding of numerous developmental
disorders such as Autism Spectrum Disorder. Recent advances in deep learning
architectures and availability of large scale transcribed data has led to
development of speech foundation models that have shown dramatic improvements
in ASR performance. However, the ability of these models to translate well to
conversational child-adult interactions is under studied. In this work, we
provide a comprehensive evaluation of ASR performance on a dataset containing
child-adult interactions from autism diagnostic sessions, using Whisper,
Wav2Vec2, HuBERT, and WavLM. We find that speech foundation models show a
noticeable performance drop (15-20% absolute WER) for child speech compared to
adult speech in the conversational setting. Then, we employ LoRA on the best
performing zero shot model (whisper-large) to probe the effectiveness of
fine-tuning in a low resource setting, resulting in ~8% absolute WER
improvement for child speech and ~13% absolute WER improvement for adult
speech.",2024-09-24,"Aditya Ashvin, Rimita Lahiri, Aditya Kommineni, Somer Bishop, Catherine Lord, Sudarsana Reddy Kadiri, Shrikanth Narayanan",http://arxiv.org/pdf/2409.16135v1,cs.LG
Towards Explainable Graph Neural Networks for Neurological Evaluation on EEG Signals,"After an acute stroke, accurately estimating stroke severity is crucial for
healthcare professionals to effectively manage patient's treatment. Graph
theory methods have shown that brain connectivity undergoes frequency-dependent
reorganization post-stroke, adapting to new conditions. Traditional methods
often rely on handcrafted features that may not capture the complexities of
clinical phenomena. In this study, we propose a novel approach using Graph
Neural Networks (GNNs) to predict stroke severity, as measured by the NIH
Stroke Scale (NIHSS). We analyzed electroencephalography (EEG) recordings from
71 patients at the time of hospitalization. For each patient, we generated five
graphs weighted by Lagged Linear Coherence (LLC) between signals from distinct
Brodmann Areas, covering $\delta$ (2-4 Hz), $\theta$ (4-8 Hz), $\alpha_1$
(8-10.5 Hz), $\alpha_2$ (10.5-13 Hz), and $\beta_1$ (13-20 Hz) frequency bands.
To emphasize key neurological connections and maintain sparsity, we applied a
sparsification process based on structural and functional brain network
properties. We then trained a graph attention model to predict the NIHSS. By
examining its attention coefficients, our model reveals insights into brain
reconfiguration, providing clinicians with a valuable tool for diagnosis,
personalized treatment, and early intervention in neurorehabilitation.",2024-09-24,"Andrea Protani, Lorenzo Giusti, Chiara Iacovelli, Albert Sund Aillet, Diogo Reis Santos, Giuseppe Reale, Aurelia Zauli, Marco Moci, Marta Garbuglia, Pierpaolo Brutti, Pietro Caliandro, Luigi Serio",http://arxiv.org/pdf/2410.07199v1,cs.LG
TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models,"Data collection is often difficult in critical fields such as medicine,
physics, and chemistry. As a result, classification methods usually perform
poorly with these small datasets, leading to weak predictive performance.
Increasing the training set with additional synthetic data, similar to data
augmentation in images, is commonly believed to improve downstream
classification performance. However, current tabular generative methods that
learn either the joint distribution $ p(\mathbf{x}, y) $ or the
class-conditional distribution $ p(\mathbf{x} \mid y) $ often overfit on small
datasets, resulting in poor-quality synthetic data, usually worsening
classification performance compared to using real data alone. To solve these
challenges, we introduce TabEBM, a novel class-conditional generative method
using Energy-Based Models (EBMs). Unlike existing methods that use a shared
model to approximate all class-conditional densities, our key innovation is to
create distinct EBM generative models for each class, each modelling its
class-specific data distribution individually. This approach creates robust
energy landscapes, even in ambiguous class distributions. Our experiments show
that TabEBM generates synthetic data with higher quality and better statistical
fidelity than existing methods. When used for data augmentation, our synthetic
data consistently improves the classification performance across diverse
datasets of various sizes, especially small ones. Code is available at
https://github.com/andreimargeloiu/TabEBM.",2024-09-24,"Andrei Margeloiu, Xiangjian Jiang, Nikola Simidjievski, Mateja Jamnik",http://arxiv.org/pdf/2409.16118v3,cs.LG
Self-attention as an attractor network: transient memories without backpropagation,"Transformers are one of the most successful architectures of modern neural
networks. At their core there is the so-called attention mechanism, which
recently interested the physics community as it can be written as the
derivative of an energy function in certain cases: while it is possible to
write the cross-attention layer as a modern Hopfield network, the same is not
possible for the self-attention, which is used in the GPT architectures and
other autoregressive models. In this work we show that it is possible to obtain
the self-attention layer as the derivative of local energy terms, which
resemble a pseudo-likelihood. We leverage the analogy with pseudo-likelihood to
design a recurrent model that can be trained without backpropagation: the
dynamics shows transient states that are strongly correlated with both train
and test examples. Overall we present a novel framework to interpret
self-attention as an attractor network, potentially paving the way for new
theoretical approaches inspired from physics to understand transformers.",2024-09-24,"Francesco D'Amico, Matteo Negri",http://arxiv.org/pdf/2409.16112v1,cs.LG
Refereeing the Referees: Evaluating Two-Sample Tests for Validating Generators in Precision Sciences,"We propose a robust methodology to evaluate the performance and computational
efficiency of non-parametric two-sample tests, specifically designed for
high-dimensional generative models in scientific applications such as in
particle physics. The study focuses on tests built from univariate integral
probability measures: the sliced Wasserstein distance and the mean of the
Kolmogorov-Smirnov statistics, already discussed in the literature, and the
novel sliced Kolmogorov-Smirnov statistic. These metrics can be evaluated in
parallel, allowing for fast and reliable estimates of their distribution under
the null hypothesis. We also compare these metrics with the recently proposed
unbiased Fr\'echet Gaussian Distance and the unbiased quadratic Maximum Mean
Discrepancy, computed with a quartic polynomial kernel. We evaluate the
proposed tests on various distributions, focusing on their sensitivity to
deformations parameterized by a single parameter $\epsilon$. Our experiments
include correlated Gaussians and mixtures of Gaussians in 5, 20, and 100
dimensions, and a particle physics dataset of gluon jets from the JetNet
dataset, considering both jet- and particle-level features. Our results
demonstrate that one-dimensional-based tests provide a level of sensitivity
comparable to other multivariate metrics, but with significantly lower
computational cost, making them ideal for evaluating generative models in
high-dimensional settings. This methodology offers an efficient, standardized
tool for model comparison and can serve as a benchmark for more advanced tests,
including machine-learning-based approaches.",2024-09-24,"Samuele Grossi, Marco Letizia, Riccardo Torre",http://arxiv.org/pdf/2409.16336v1,cs.LG
The Digital Transformation in Health: How AI Can Improve the Performance of Health Systems,"Mobile health has the potential to revolutionize health care delivery and
patient engagement. In this work, we discuss how integrating Artificial
Intelligence into digital health applications-focused on supply chain, patient
management, and capacity building, among other use cases-can improve the health
system and public health performance. We present an Artificial Intelligence and
Reinforcement Learning platform that allows the delivery of adaptive
interventions whose impact can be optimized through experimentation and
real-time monitoring. The system can integrate multiple data sources and
digital health applications. The flexibility of this platform to connect to
various mobile health applications and digital devices and send personalized
recommendations based on past data and predictions can significantly improve
the impact of digital tools on health system outcomes. The potential for
resource-poor settings, where the impact of this approach on health outcomes
could be more decisive, is discussed specifically. This framework is, however,
similarly applicable to improving efficiency in health systems where scarcity
is not an issue.",2024-09-24,"África Periáñez, Ana Fernández del Río, Ivan Nazarov, Enric Jané, Moiz Hassan, Aditya Rastogi, Dexian Tang",http://arxiv.org/pdf/2409.16098v2,cs.LG
From Pixels to Words: Leveraging Explainability in Face Recognition through Interactive Natural Language Processing,"Face Recognition (FR) has advanced significantly with the development of deep
learning, achieving high accuracy in several applications. However, the lack of
interpretability of these systems raises concerns about their accountability,
fairness, and reliability. In the present study, we propose an interactive
framework to enhance the explainability of FR models by combining
model-agnostic Explainable Artificial Intelligence (XAI) and Natural Language
Processing (NLP) techniques. The proposed framework is able to accurately
answer various questions of the user through an interactive chatbot. In
particular, the explanations generated by our proposed method are in the form
of natural language text and visual representations, which for example can
describe how different facial regions contribute to the similarity measure
between two faces. This is achieved through the automatic analysis of the
output's saliency heatmaps of the face images and a BERT question-answering
model, providing users with an interface that facilitates a comprehensive
understanding of the FR decisions. The proposed approach is interactive,
allowing the users to ask questions to get more precise information based on
the user's background knowledge. More importantly, in contrast to previous
studies, our solution does not decrease the face recognition performance. We
demonstrate the effectiveness of the method through different experiments,
highlighting its potential to make FR systems more interpretable and
user-friendly, especially in sensitive applications where decision-making
transparency is crucial.",2024-09-24,"Ivan DeAndres-Tame, Muhammad Faisal, Ruben Tolosana, Rouqaiah Al-Refai, Ruben Vera-Rodriguez, Philipp Terhörst",http://arxiv.org/pdf/2409.16089v2,cs.LG
Assessing Simplification Levels in Neural Networks: The Impact of Hyperparameter Configurations on Complexity and Sensitivity,"This paper presents an experimental study focused on understanding the
simplification properties of neural networks under different hyperparameter
configurations, specifically investigating the effects on Lempel Ziv complexity
and sensitivity. By adjusting key hyperparameters such as activation functions,
hidden layers, and learning rate, this study evaluates how these parameters
impact the complexity of network outputs and their robustness to input
perturbations. The experiments conducted using the MNIST dataset aim to provide
insights into the relationships between hyperparameters, complexity, and
sensitivity, contributing to a deeper theoretical understanding of these
concepts in neural networks.",2024-09-24,Huixin Guan,http://arxiv.org/pdf/2409.16086v1,cs.LG
Ultra-low latency quantum-inspired machine learning predictors implemented on FPGA,"Tensor Networks (TNs) are a computational paradigm used for representing
quantum many-body systems. Recent works have shown how TNs can also be applied
to perform Machine Learning (ML) tasks, yielding comparable results to standard
supervised learning techniques. In this work, we study the use of Tree Tensor
Networks (TTNs) in high-frequency real-time applications by exploiting the
low-latency hardware of the Field-Programmable Gate Array (FPGA) technology. We
present different implementations of TTN classifiers, capable of performing
inference on classical ML datasets as well as on complex physics data. A
preparatory analysis of bond dimensions and weight quantization is realized in
the training phase, together with entanglement entropy and correlation
measurements, that help setting the choice of the TTN architecture. The
generated TTNs are then deployed on a hardware accelerator; using an FPGA
integrated into a server, the inference of the TTN is completely offloaded.
Eventually, a classifier for High Energy Physics (HEP) applications is
implemented and executed fully pipelined with sub-microsecond latency.",2024-09-24,"Lorenzo Borella, Alberto Coppi, Jacopo Pazzini, Andrea Stanco, Marco Trenti, Andrea Triossi, Marco Zanetti",http://arxiv.org/pdf/2409.16075v2,cs.LG
Learning with Confidence: Training Better Classifiers from Soft Labels,"In supervised machine learning, models are typically trained using data with
hard labels, i.e., definite assignments of class membership. This traditional
approach, however, does not take the inherent uncertainty in these labels into
account. We investigate whether incorporating label uncertainty, represented as
discrete probability distributions over the class labels -- known as soft
labels -- improves the predictive performance of classification models. We
first demonstrate the potential value of soft label learning (SLL) for
estimating model parameters in a simulation experiment, particularly for
limited sample sizes and imbalanced data. Subsequently, we compare the
performance of various wrapper methods for learning from both hard and soft
labels using identical base classifiers. On real-world-inspired synthetic data
with clean labels, the SLL methods consistently outperform hard label methods.
Since real-world data is often noisy and precise soft labels are challenging to
obtain, we study the effect that noisy probability estimates have on model
performance. Alongside conventional noise models, our study examines four types
of miscalibration that are known to affect human annotators. The results show
that SLL methods outperform the hard label methods in the majority of settings.
Finally, we evaluate the methods on a real-world dataset with confidence
scores, where the SLL methods are shown to match the traditional methods for
predicting the (noisy) hard labels while providing more accurate confidence
estimates.",2024-09-24,"Sjoerd de Vries, Dirk Thierens",http://arxiv.org/pdf/2409.16071v1,cs.LG
A decision-theoretic model for a principal-agent collaborative learning problem,"In this technical note, we consider a collaborative learning framework with
principal-agent setting, in which the principal at each time-step determines a
set of appropriate aggregation coefficients based on how the current parameter
estimates from a group of $K$ agents effectively performed in connection with a
separate test dataset, which is not part of the agents' training model
datasets. Whereas, the agents, who act together as a team, then update their
parameter estimates using a discrete-time version of Langevin dynamics with
mean-field-like interaction term, but guided by their respective different
training model datasets. Here, we propose a decision-theoretic framework that
explicitly describes how the principal progressively determines a set of
nonnegative and sum to one aggregation coefficients used by the agents in their
mean-field-like interaction term, that eventually leading them to reach a
consensus optimal parameter estimate. Interestingly, due to the inherent
feedbacks and cooperative behavior among the agents, the proposed framework
offers some advantages in terms of stability and generalization, despite that
both the principal and the agents do not necessarily need to have any knowledge
of the sample distributions or the quality of each others' datasets.",2024-09-24,Getachew K Befekadu,http://arxiv.org/pdf/2409.16068v1,cs.LG
Denoising Graph Super-Resolution towards Improved Collider Event Reconstruction,"Accurately reconstructing particles from detector data is a critical
challenge in experimental particle physics, where the spatial resolution of
calorimeters has a crucial impact. This study explores the integration of
super-resolution techniques into an LHC-like reconstruction pipeline to
effectively enhance the granularity of calorimeter data and suppress noise. We
find that this software preprocessing step can significantly improve
reconstruction quality without physical changes to detectors. To demonstrate
the impact of our approach, we propose a novel particle flow model that offers
enhanced particle reconstruction quality and interpretability. These
advancements underline the potential of super-resolution to impact both current
and future particle physics experiments.",2024-09-24,"Nilotpal Kakati, Etienne Dreyer, Eilam Gross",http://arxiv.org/pdf/2409.16052v1,cs.LG
Whole-body End-Effector Pose Tracking,"Combining manipulation with the mobility of legged robots is essential for a
wide range of robotic applications. However, integrating an arm with a mobile
base significantly increases the system's complexity, making precise
end-effector control challenging. Existing model-based approaches are often
constrained by their modeling assumptions, leading to limited robustness.
Meanwhile, recent Reinforcement Learning (RL) implementations restrict the
arm's workspace to be in front of the robot or track only the position to
obtain decent tracking accuracy. In this work, we address these limitations by
introducing a whole-body RL formulation for end-effector pose tracking in a
large workspace on rough, unstructured terrains. Our proposed method involves a
terrain-aware sampling strategy for the robot's initial configuration and
end-effector pose commands, as well as a game-based curriculum to extend the
robot's operating range. We validate our approach on the ANYmal quadrupedal
robot with a six DoF robotic arm. Through our experiments, we show that the
learned controller achieves precise command tracking over a large workspace and
adapts across varying terrains such as stairs and slopes. On deployment, it
achieves a pose-tracking error of 2.64 cm and 3.64 degrees, outperforming
existing competitive baselines.",2024-09-24,"Tifanny Portela, Andrei Cramariuc, Mayank Mittal, Marco Hutter",http://arxiv.org/pdf/2409.16048v2,cs.LG
Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts,"Deep learning for time series forecasting has seen significant advancements
over the past decades. However, despite the success of large-scale pre-training
in language and vision domains, pre-trained time series models remain limited
in scale and operate at a high cost, hindering the development of larger
capable forecasting models in real-world applications. In response, we
introduce Time-MoE, a scalable and unified architecture designed to pre-train
larger, more capable forecasting foundation models while reducing inference
costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE
enhances computational efficiency by activating only a subset of networks for
each prediction, reducing computational load while maintaining high model
capacity. This allows Time-MoE to scale effectively without a corresponding
increase in inference costs. Time-MoE comprises a family of decoder-only
transformer models that operate in an auto-regressive manner and support
flexible forecasting horizons with varying input context lengths. We
pre-trained these models on our newly introduced large-scale data Time-300B,
which spans over 9 domains and encompassing over 300 billion time points. For
the first time, we scaled a time series foundation model up to 2.4 billion
parameters, achieving significantly improved forecasting precision. Our results
validate the applicability of scaling laws for training tokens and model size
in the context of time series forecasting. Compared to dense models with the
same number of activated parameters or equivalent computation budgets, our
models consistently outperform them by large margin. These advancements
position Time-MoE as a state-of-the-art solution for tackling real-world time
series forecasting challenges with superior capability, efficiency, and
flexibility.",2024-09-24,"Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, Ming Jin",http://arxiv.org/pdf/2409.16040v4,cs.LG
Robust Neural IDA-PBC: passivity-based stabilization under approximations,"In this paper, we restructure the Neural Interconnection and Damping
Assignment - Passivity Based Control (Neural IDA-PBC) design methodology, and
we formally analyze its closed-loop properties. Neural IDA-PBC redefines the
IDA-PBC design approach as an optimization problem by building on the framework
of Physics Informed Neural Networks (PINNs). However, the closed-loop stability
and robustness properties under Neural IDA-PBC remain unexplored. To address
the issue, we study the behavior of classical IDA-PBC under approximations. Our
theoretical analysis allows deriving conditions for practical and asymptotic
stability of the desired equilibrium point. Moreover, it extends the Neural
IDA-PBC applicability to port-Hamiltonian systems where the matching conditions
cannot be solved exactly. Our renewed optimization-based design introduces
three significant aspects: i) it involves a novel optimization objective
including stability and robustness constraints issued from our theoretical
analysis; ii) it employs separate Neural Networks (NNs), which can be
structured to reduce the search space to relevant functions; iii) it does not
require knowledge about the port-Hamiltonian formulation of the system's model.
Our methodology is validated with simulations on three standard benchmarks: a
double pendulum, a nonlinear mass-spring-damper and a cartpole. Notably,
classical IDA-PBC designs cannot be analytically derived for the latter.",2024-09-24,"Santiago Sanchez-Escalonilla, Samuele Zoboli, Bayu Jayawardhana",http://arxiv.org/pdf/2409.16008v1,cs.LG
Improvements to SDXL in NovelAI Diffusion V3,"In this technical report, we document the changes we made to SDXL in the
process of training NovelAI Diffusion V3, our state of the art anime image
generation model.",2024-09-24,"Juan Ossa, Eren Doğan, Alex Birch, F. Johnson",http://arxiv.org/pdf/2409.15997v2,cs.LG
Semi-strong Efficient Market of Bitcoin and Twitter: an Analysis of Semantic Vector Spaces of Extracted Keywords and Light Gradient Boosting Machine Models,"This study extends the examination of the Efficient-Market Hypothesis in
Bitcoin market during a five year fluctuation period, from September 1 2017 to
September 1 2022, by analyzing 28,739,514 qualified tweets containing the
targeted topic ""Bitcoin"". Unlike previous studies, we extracted fundamental
keywords as an informative proxy for carrying out the study of the EMH in the
Bitcoin market rather than focusing on sentiment analysis, information volume,
or price data. We tested market efficiency in hourly, 4-hourly, and daily time
periods to understand the speed and accuracy of market reactions towards the
information within different thresholds. A sequence of machine learning methods
and textual analyses were used, including measurements of distances of semantic
vector spaces of information, keywords extraction and encoding model, and Light
Gradient Boosting Machine (LGBM) classifiers. Our results suggest that 78.06%
(83.08%), 84.63% (87.77%), and 94.03% (94.60%) of hourly, 4-hourly, and daily
bullish (bearish) market movements can be attributed to public information
within organic tweets.",2024-09-24,"Fang Wang, Marko Gacesa",http://arxiv.org/pdf/2409.15988v1,cs.LG
Exploring the Impact of Outlier Variability on Anomaly Detection Evaluation Metrics,"Anomaly detection is a dynamic field, in which the evaluation of models plays
a critical role in understanding their effectiveness. The selection and
interpretation of the evaluation metrics are pivotal, particularly in scenarios
with varying amounts of anomalies. This study focuses on examining the
behaviors of three widely used anomaly detection metrics under different
conditions: F1 score, Receiver Operating Characteristic Area Under Curve (ROC
AUC), and Precision-Recall Curve Area Under Curve (AUCPR). Our study critically
analyzes the extent to which these metrics provide reliable and distinct
insights into model performance, especially considering varying levels of
outlier fractions and contamination thresholds in datasets.
  Through a comprehensive experimental setup involving widely recognized
algorithms for anomaly detection, we present findings that challenge the
conventional understanding of these metrics and reveal nuanced behaviors under
varying conditions. We demonstrated that while the F1 score and AUCPR are
sensitive to outlier fractions, the ROC AUC maintains consistency and is
unaffected by such variability. Additionally, under conditions of a fixed
outlier fraction in the test set, we observe an alignment between ROC AUC and
AUCPR, indicating that the choice between these two metrics may be less
critical in such scenarios.
  The results of our study contribute to a more refined understanding of metric
selection and interpretation in anomaly detection, offering valuable insights
for both researchers and practitioners in the field.",2024-09-24,"Minjae Ok, Simon Klüttermann, Emmanuel Müller",http://arxiv.org/pdf/2409.15986v1,cs.LG
Edge-device Collaborative Computing for Multi-view Classification,"Motivated by the proliferation of Internet-of-Thing (IoT) devices and the
rapid advances in the field of deep learning, there is a growing interest in
pushing deep learning computations, conventionally handled by the cloud, to the
edge of the network to deliver faster responses to end users, reduce bandwidth
consumption to the cloud, and address privacy concerns. However, to fully
realize deep learning at the edge, two main challenges still need to be
addressed: (i) how to meet the high resource requirements of deep learning on
resource-constrained devices, and (ii) how to leverage the availability of
multiple streams of spatially correlated data, to increase the effectiveness of
deep learning and improve application-level performance. To address the above
challenges, we explore collaborative inference at the edge, in which edge nodes
and end devices share correlated data and the inference computational burden by
leveraging different ways to split computation and fuse data. Besides
traditional centralized and distributed schemes for edge-end device
collaborative inference, we introduce selective schemes that decrease bandwidth
resource consumption by effectively reducing data redundancy. As a reference
scenario, we focus on multi-view classification in a networked system in which
sensing nodes can capture overlapping fields of view. The proposed schemes are
compared in terms of accuracy, computational expenditure at the nodes,
communication overhead, inference latency, robustness, and noise sensitivity.
Experimental results highlight that selective collaborative schemes can achieve
different trade-offs between the above performance metrics, with some of them
bringing substantial communication savings (from 18% to 74% of the transmitted
data with respect to centralized inference) while still keeping the inference
accuracy well above 90%.",2024-09-24,"Marco Palena, Tania Cerquitelli, Carla Fabiana Chiasserini",http://arxiv.org/pdf/2409.15973v1,cs.LG
Provably Efficient Exploration in Inverse Constrained Reinforcement Learning,"Optimizing objective functions subject to constraints is fundamental in many
real-world applications. However, these constraints are often not readily
defined and must be inferred from expert agent behaviors, a problem known as
Inverse Constraint Inference. Inverse Constrained Reinforcement Learning (ICRL)
is a common solver for recovering feasible constraints in complex environments,
relying on training samples collected from interactive environments. However,
the efficacy and efficiency of current sampling strategies remain unclear. We
propose a strategic exploration framework for sampling with guaranteed
efficiency to bridge this gap. By defining the feasible cost set for ICRL
problems, we analyze how estimation errors in transition dynamics and the
expert policy influence the feasibility of inferred constraints. Based on this
analysis, we introduce two exploratory algorithms to achieve efficient
constraint inference via 1) dynamically reducing the bounded aggregate error of
cost estimations or 2) strategically constraining the exploration policy around
plausibly optimal ones. Both algorithms are theoretically grounded with
tractable sample complexity, and their performance is validated empirically
across various environments.",2024-09-24,"Bo Yue, Jian Li, Guiliang Liu",http://arxiv.org/pdf/2409.15963v4,cs.LG
A Historical Trajectory Assisted Optimization Method for Zeroth-Order Federated Learning,"Federated learning heavily relies on distributed gradient descent techniques.
In the situation where gradient information is not available, the gradients
need to be estimated from zeroth-order information, which typically involves
computing finite-differences along isotropic random directions. This method
suffers from high estimation errors, as the geometric features of the objective
landscape may be overlooked during the isotropic sampling. In this work, we
propose a non-isotropic sampling method to improve the gradient estimation
procedure. Gradients in our method are estimated in a subspace spanned by
historical trajectories of solutions, aiming to encourage the exploration of
promising regions and hence improve the convergence. The proposed method uses a
covariance matrix for sampling which is a convex combination of two parts. The
first part is a thin projection matrix containing the basis of the subspace
which is designed to improve the exploitation ability. The second part is the
historical trajectories. We implement this method in zeroth-order federated
settings, and show that the convergence rate aligns with existing ones while
introducing no significant overheads in communication or local computation. The
effectiveness of our proposal is verified on several numerical experiments in
comparison to several commonly-used zeroth-order federated optimization
algorithms.",2024-09-24,"Chenlin Wu, Xiaoyu He, Zike Li, Jing Gong, Zibin Zheng",http://arxiv.org/pdf/2409.15955v5,cs.LG
Predicting Distance matrix with large language models,"Structural prediction has long been considered critical in RNA research,
especially following the success of AlphaFold2 in protein studies, which has
drawn significant attention to the field. While recent advances in machine
learning and data accumulation have effectively addressed many biological
tasks, particularly in protein related research. RNA structure prediction
remains a significant challenge due to data limitations. Obtaining RNA
structural data is difficult because traditional methods such as nuclear
magnetic resonance spectroscopy, Xray crystallography, and electron microscopy
are expensive and time consuming. Although several RNA 3D structure prediction
methods have been proposed, their accuracy is still limited. Predicting RNA
structural information at another level, such as distance maps, remains highly
valuable. Distance maps provide a simplified representation of spatial
constraints between nucleotides, capturing essential relationships without
requiring a full 3D model. This intermediate level of structural information
can guide more accurate 3D modeling and is computationally less intensive,
making it a useful tool for improving structural predictions. In this work, we
demonstrate that using only primary sequence information, we can accurately
infer the distances between RNA bases by utilizing a large pretrained RNA
language model coupled with a well trained downstream transformer.",2024-09-24,Jiaxing Yang,http://arxiv.org/pdf/2409.16333v1,cs.LG
Numerical determination of the width and shape of the effective string using Stochastic Normalizing Flows,"Flow-based architectures have recently proved to be an efficient tool for
numerical simulations of Effective String Theories regularized on the lattice
that otherwise cannot be efficiently sampled by standard Monte Carlo methods.
In this work we use Stochastic Normalizing Flows, a state-of-the-art deep
learning architecture based on non-equilibrium Monte Carlo simulations, to
study different effective string models. After testing the reliability of this
approach through a comparison with exact results for the Nambu-Got\={o} model,
we discuss results on observables that are challenging to study analytically,
such as the width of the string and the shape of the flux density. Furthermore,
we perform a novel numerical study of Effective String Theories with terms
beyond the Nambu-Got\={o} action, including a broader discussion on their
significance for lattice gauge theories. The combination of these findings
enables a quantitative description of the fine details of the confinement
mechanism in different lattice gauge theories. The results presented in this
work establish the reliability and feasibility of flow-based samplers for
Effective String Theories and pave the way for future applications on more
complex models.",2024-09-24,"Michele Caselle, Elia Cellini, Alessandro Nada",http://arxiv.org/pdf/2409.15937v2,cs.LG
Automated test generation to evaluate tool-augmented LLMs as conversational AI agents,"Tool-augmented LLMs are a promising approach to create AI agents that can
have realistic conversations, follow procedures, and call appropriate
functions. However, evaluating them is challenging due to the diversity of
possible conversations, and existing datasets focus only on single interactions
and function-calling. We present a test generation pipeline to evaluate LLMs as
conversational AI agents. Our framework uses LLMs to generate diverse tests
grounded on user-defined procedures. For that, we use intermediate graphs to
limit the LLM test generator's tendency to hallucinate content that is not
grounded on input procedures, and enforces high coverage of the possible
conversations. Additionally, we put forward ALMITA, a manually curated dataset
for evaluating AI agents in customer support, and use it to evaluate existing
LLMs. Our results show that while tool-augmented LLMs perform well in single
interactions, they often struggle to handle complete conversations. While our
focus is on customer support, our method is general and capable of AI agents
for different domains.",2024-09-24,"Samuel Arcadinho, David Aparicio, Mariana Almeida",http://arxiv.org/pdf/2409.15934v2,cs.LG
The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards,"While Vision-Language Models (VLMs) are increasingly used to generate reward
signals for training embodied agents to follow instructions, our research
reveals that agents guided by VLM rewards often underperform compared to those
employing only intrinsic (exploration-driven) rewards, contradicting
expectations set by recent work. We hypothesize that false positive rewards --
instances where unintended trajectories are incorrectly rewarded -- are more
detrimental than false negatives. Our analysis confirms this hypothesis,
revealing that the widely used cosine similarity metric is prone to false
positive reward estimates. To address this, we introduce BiMI ({Bi}nary
{M}utual {I}nformation), a novel reward function designed to mitigate noise.
BiMI significantly enhances learning efficiency across diverse and challenging
embodied navigation environments. Our findings offer a nuanced understanding of
how different types of reward noise impact agent learning and highlight the
importance of addressing multimodal reward signal noise when training embodied
agents",2024-09-24,"Sukai Huang, Shu-Wei Liu, Nir Lipovetzky, Trevor Cohn",http://arxiv.org/pdf/2409.15922v4,cs.LG
Deep convolutional framelets for dose reconstruction in BNCT with Compton camera detector,"Boron Neutron Capture Therapy (BNCT) is an innovative binary form of
radiation therapy with high selectivity towards cancer tissue based on the
neutron capture reaction 10B(n,$\alpha$)7Li, consisting in the exposition of
patients to neutron beams after administration of a boron compound with
preferential accumulation in cancer cells. The high linear energy transfer
products of the ensuing reaction deposit their energy at cell level, sparing
normal tissue. Although progress in accelerator-based BNCT has led to renewed
interest in this cancer treatment modality, in vivo dose monitoring during
treatment still remains not feasible and several approaches are under
investigation. While Compton imaging presents various advantages over other
imaging methods, it typically requires long reconstruction times, comparable
with BNCT treatment duration. This study aims to develop deep neural network
models to estimate the dose distribution by using a simulated dataset of BNCT
Compton camera images. The models pursue the avoidance of the iteration time
associated with the maximum-likelihood expectation-maximization algorithm
(MLEM), enabling a prompt dose reconstruction during the treatment. The U-Net
architecture and two variants based on the deep convolutional framelets
framework have been used for noise and artifacts reduction in few-iterations
reconstructed images, leading to promising results in terms of reconstruction
accuracy and processing time.",2024-09-24,"Angelo Didonna, Dayron Ramos Lopez, Giuseppe Iaselli, Nicola Amoroso, Nicola Ferrara, Gabriella Maria Incoronata Pugliese",http://arxiv.org/pdf/2409.15916v1,cs.LG
FedRepOpt: Gradient Re-parametrized Optimizers in Federated Learning,"Federated Learning (FL) has emerged as a privacy-preserving method for
training machine learning models in a distributed manner on edge devices.
However, on-device models face inherent computational power and memory
limitations, potentially resulting in constrained gradient updates. As the
model's size increases, the frequency of gradient updates on edge devices
decreases, ultimately leading to suboptimal training outcomes during any
particular FL round. This limits the feasibility of deploying advanced and
large-scale models on edge devices, hindering the potential for performance
enhancements. To address this issue, we propose FedRepOpt, a gradient
re-parameterized optimizer for FL. The gradient re-parameterized method allows
training a simple local model with a similar performance as a complex model by
modifying the optimizer's gradients according to a set of model-specific
hyperparameters obtained from the complex models. In this work, we focus on
VGG-style and Ghost-style models in the FL environment. Extensive experiments
demonstrate that models using FedRepOpt obtain a significant boost in
performance of 16.7% and 11.4% compared to the RepGhost-style and RepVGG-style
networks, while also demonstrating a faster convergence time of 11.7% and 57.4%
compared to their complex structure.",2024-09-24,"Kin Wai Lau, Yasar Abbas Ur Rehman, Pedro Porto Buarque de Gusmão, Lai-Man Po, Lan Ma, Yuyang Xie",http://arxiv.org/pdf/2409.15898v4,cs.LG
Self-Supervised Graph Embedding Clustering,"The K-means one-step dimensionality reduction clustering method has made some
progress in addressing the curse of dimensionality in clustering tasks.
However, it combines the K-means clustering and dimensionality reduction
processes for optimization, leading to limitations in the clustering effect due
to the introduced hyperparameters and the initialization of clustering centers.
Moreover, maintaining class balance during clustering remains challenging. To
overcome these issues, we propose a unified framework that integrates manifold
learning with K-means, resulting in the self-supervised graph embedding
framework. Specifically, we establish a connection between K-means and the
manifold structure, allowing us to perform K-means without explicitly defining
centroids. Additionally, we use this centroid-free K-means to generate labels
in low-dimensional space and subsequently utilize the label information to
determine the similarity between samples. This approach ensures consistency
between the manifold structure and the labels. Our model effectively achieves
one-step clustering without the need for redundant balancing hyperparameters.
Notably, we have discovered that maximizing the $\ell_{2,1}$-norm naturally
maintains class balance during clustering, a result that we have theoretically
proven. Finally, experiments on multiple datasets demonstrate that the
clustering results of Our-LPP and Our-MFA exhibit excellent and reliable
performance.",2024-09-24,"Fangfang Li, Quanxue Gao, Cheng Deng, Wei Xia",http://arxiv.org/pdf/2409.15887v2,cs.LG
On the calibration of powerset speaker diarization models,"End-to-end neural diarization models have usually relied on a
multilabel-classification formulation of the speaker diarization problem.
Recently, we proposed a powerset multiclass formulation that has beaten the
state-of-the-art on multiple datasets. In this paper, we propose to study the
calibration of a powerset speaker diarization model, and explore some of its
uses. We study the calibration in-domain, as well as out-of-domain, and explore
the data in low-confidence regions. The reliability of model confidence is then
tested in practice: we use the confidence of the pretrained model to
selectively create training and validation subsets out of unannotated data, and
compare this to random selection. We find that top-label confidence can be used
to reliably predict high-error regions. Moreover, training on low-confidence
regions provides a better calibrated model, and validating on low-confidence
regions can be more annotation-efficient than random regions.",2024-09-24,"Alexis Plaquet, Hervé Bredin",http://arxiv.org/pdf/2409.15885v1,cs.LG
Whisper in Medusa's Ear: Multi-head Efficient Decoding for Transformer-based ASR,"Large transformer-based models have significant potential for speech
transcription and translation. Their self-attention mechanisms and parallel
processing enable them to capture complex patterns and dependencies in audio
sequences. However, this potential comes with challenges, as these large and
computationally intensive models lead to slow inference speeds. Various
optimization strategies have been proposed to improve performance, including
efficient hardware utilization and algorithmic enhancements. In this paper, we
introduce Whisper-Medusa, a novel approach designed to enhance processing speed
with minimal impact on Word Error Rate (WER). The proposed model extends the
OpenAI's Whisper architecture by predicting multiple tokens per iteration,
resulting in a 50% reduction in latency. We showcase the effectiveness of
Whisper-Medusa across different learning setups and datasets.",2024-09-24,"Yael Segal-Feldman, Aviv Shamsian, Aviv Navon, Gill Hetz, Joseph Keshet",http://arxiv.org/pdf/2409.15869v1,cs.LG
Privacy Evaluation Benchmarks for NLP Models,"By inducing privacy attacks on NLP models, attackers can obtain sensitive
information such as training data and model parameters, etc. Although
researchers have studied, in-depth, several kinds of attacks in NLP models,
they are non-systematic analyses. It lacks a comprehensive understanding of the
impact caused by the attacks. For example, we must consider which scenarios can
apply to which attacks, what the common factors are that affect the performance
of different attacks, the nature of the relationships between different
attacks, and the influence of various datasets and models on the effectiveness
of the attacks, etc. Therefore, we need a benchmark to holistically assess the
privacy risks faced by NLP models. In this paper, we present a privacy attack
and defense evaluation benchmark in the field of NLP, which includes the
conventional/small models and large language models (LLMs). This benchmark
supports a variety of models, datasets, and protocols, along with standardized
modules for comprehensive evaluation of attacks and defense strategies. Based
on the above framework, we present a study on the association between auxiliary
data from different domains and the strength of privacy attacks. And we provide
an improved attack method in this scenario with the help of Knowledge
Distillation (KD). Furthermore, we propose a chained framework for privacy
attacks. Allowing a practitioner to chain multiple attacks to achieve a
higher-level attack objective. Based on this, we provide some defense and
enhanced attack strategies. The code for reproducing the results can be found
at https://github.com/user2311717757/nlp_doctor.",2024-09-24,"Wei Huang, Yinggui Wang, Cen Chen",http://arxiv.org/pdf/2409.15868v3,cs.LG
Online Planning for Multi-UAV Pursuit-Evasion in Unknown Environments Using Deep Reinforcement Learning,"Multi-UAV pursuit-evasion, where pursuers aim to capture evaders, poses a key
challenge for UAV swarm intelligence. Multi-agent reinforcement learning (MARL)
has demonstrated potential in modeling cooperative behaviors, but most RL-based
approaches remain constrained to simplified simulations with limited dynamics
or fixed scenarios. Previous attempts to deploy RL policy to real-world
pursuit-evasion are largely restricted to two-dimensional scenarios, such as
ground vehicles or UAVs at fixed altitudes. In this paper, we address multi-UAV
pursuit-evasion by considering UAV dynamics and physical constraints. We
introduce an evader prediction-enhanced network to tackle partial observability
in cooperative strategy learning. Additionally, we propose an adaptive
environment generator within MARL training, enabling higher exploration
efficiency and better policy generalization across diverse scenarios.
Simulations show our method significantly outperforms all baselines in
challenging scenarios, generalizing to unseen scenarios with a 100% capture
rate. Finally, we derive a feasible policy via a two-stage reward refinement
and deploy the policy on real quadrotors in a zero-shot manner. To our
knowledge, this is the first work to derive and deploy an RL-based policy using
collective thrust and body rates control commands for multi-UAV pursuit-evasion
in unknown environments. The open-source code and videos are available at
https://sites.google.com/view/pursuit-evasion-rl.",2024-09-24,"Jiayu Chen, Chao Yu, Guosheng Li, Wenhao Tang, Shilong Ji, Xinyi Yang, Botian Xu, Huazhong Yang, Yu Wang",http://arxiv.org/pdf/2409.15866v3,cs.LG
EEGUnity: Open-Source Tool in Facilitating Unified EEG Datasets Towards Large-Scale EEG Model,"The increasing number of dispersed EEG dataset publications and the
advancement of large-scale Electroencephalogram (EEG) models have increased the
demand for practical tools to manage diverse EEG datasets. However, the
inherent complexity of EEG data, characterized by variability in content data,
metadata, and data formats, poses challenges for integrating multiple datasets
and conducting large-scale EEG model research. To tackle the challenges, this
paper introduces EEGUnity, an open-source tool that incorporates modules of
'EEG Parser', 'Correction', 'Batch Processing', and 'Large Language Model
Boost'. Leveraging the functionality of such modules, EEGUnity facilitates the
efficient management of multiple EEG datasets, such as intelligent data
structure inference, data cleaning, and data unification. In addition, the
capabilities of EEGUnity ensure high data quality and consistency, providing a
reliable foundation for large-scale EEG data research. EEGUnity is evaluated
across 25 EEG datasets from different sources, offering several typical batch
processing workflows. The results demonstrate the high performance and
flexibility of EEGUnity in parsing and data processing. The project code is
publicly available at github.com/Baizhige/EEGUnity.",2024-09-24,"Chengxuan Qin, Rui Yang, Wenlong You, Zhige Chen, Longsheng Zhu, Mengjie Huang, Zidong Wang",http://arxiv.org/pdf/2410.07196v1,cs.LG
iGAiVA: Integrated Generative AI and Visual Analytics in a Machine Learning Workflow for Text Classification,"In developing machine learning (ML) models for text classification, one
common challenge is that the collected data is often not ideally distributed,
especially when new classes are introduced in response to changes of data and
tasks. In this paper, we present a solution for using visual analytics (VA) to
guide the generation of synthetic data using large language models. As VA
enables model developers to identify data-related deficiency, data synthesis
can be targeted to address such deficiency. We discuss different types of data
deficiency, describe different VA techniques for supporting their
identification, and demonstrate the effectiveness of targeted data synthesis in
improving model accuracy. In addition, we present a software tool, iGAiVA,
which maps four groups of ML tasks into four VA views, integrating generative
AI and VA into an ML workflow for developing and improving text classification
models.",2024-09-24,"Yuanzhe Jin, Adrian Carrasco-Revilla, Min Chen",http://arxiv.org/pdf/2409.15848v2,cs.LG
Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection,"We introduce adaptive learn-then-test (aLTT), an efficient hyperparameter
selection procedure that provides finite-sample statistical guarantees on the
population risk of AI models. Unlike the existing learn-then-test (LTT)
technique, which relies on conventional p-value-based multiple hypothesis
testing (MHT), aLTT implements sequential data-dependent MHT with early
termination by leveraging e-processes. As a result, aLTT can reduce the number
of testing rounds, making it particularly well-suited for scenarios in which
testing is costly or presents safety risks. Apart from maintaining statistical
validity, in applications such as online policy selection for offline
reinforcement learning and prompt engineering, aLTT is shown to achieve the
same performance as LTT while requiring only a fraction of the testing rounds.",2024-09-24,"Matteo Zecchin, Sangwoo Park, Osvaldo Simeone",http://arxiv.org/pdf/2409.15844v2,cs.LG
Textless NLP -- Zero Resource Challenge with Low Resource Compute,"This work addresses the persistent challenges of substantial training time
and GPU resource requirements even when training lightweight encoder-vocoder
models for Textless NLP. We reduce training steps significantly while improving
performance by a) leveraging learning rate schedulers for efficient and faster
convergence b) optimizing hop length and c) tuning the interpolation scale
factors for better audio quality. Additionally, we explore the latent space
representation for Indian languages such as Tamil and Bengali for the acoustic
unit discovery and voice conversion task. Our approach leverages a quantized
encoder architecture, in conjunction with a vocoder which utilizes the proposed
mixture of optimized hop length, tuned interpolation scale factors and a cyclic
learning rate scheduler. We obtain consistently good results across English,
Tamil and Bengali datasets. The proposed method excels in capturing complex
linguistic patterns, resulting in clear reconstructed audio during voice
conversion with significantly reduced training time.",2024-09-24,"Krithiga Ramadass, Abrit Pal Singh, Srihari J, Sheetal Kalyani",http://arxiv.org/pdf/2409.19015v1,cs.LG
Supervised Fine-Tuning Achieve Rapid Task Adaption Via Alternating Attention Head Activation Patterns,"LLMs' performance on complex tasks is still unsatisfactory. A key issue is
that presently LLMs learn in a data-driven schema, while the instructions about
these complex tasks are both scarce and hard to collect or construct. On the
contrary, a prominent phenomenon is that LLMs can learn rather fast on simpler
tasks with adequate prior knowledge captured during pretraining stage. Thus, if
the prerequisite and mechanism of such rapid generalization could be
elucidated, it could enhance the efficiency and effectiveness of the LLM's
ability to learn complex tasks. Thus, in this paper, we employ a gradient-based
method, to dissect the process that the SFT process adapts LLMs to downstream
tasks via the perspective of attention patterns. We find that: (1) LLMs
selectively activate task-specific attention heads during SFT; (2) activation
patterns for complex tasks are combinations of basic task patterns; and (3)
changes in a few parameters can significantly impact activation patterns after
SFT on a small number of samples.Based on these insights, experiments are
conducted to actually enhance the efficiency and effectiveness of SFT.",2024-09-24,"Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Ting Liu, Bing Qin",http://arxiv.org/pdf/2409.15820v2,cs.LG
Interactive Example-based Explanations to Improve Health Professionals' Onboarding with AI for Human-AI Collaborative Decision Making,"A growing research explores the usage of AI explanations on user's decision
phases for human-AI collaborative decision-making. However, previous studies
found the issues of overreliance on `wrong' AI outputs. In this paper, we
propose interactive example-based explanations to improve health professionals'
onboarding with AI for their better reliance on AI during AI-assisted
decision-making. We implemented an AI-based decision support system that
utilizes a neural network to assess the quality of post-stroke survivors'
exercises and interactive example-based explanations that systematically
surface the nearest neighborhoods of a test/task sample from the training set
of the AI model to assist users' onboarding with the AI model. To investigate
the effect of interactive example-based explanations, we conducted a study with
domain experts, health professionals to evaluate their performance and reliance
on AI. Our interactive example-based explanations during onboarding assisted
health professionals in having a better reliance on AI and making a higher
ratio of making `right' decisions and a lower ratio of `wrong' decisions than
providing only feature-based explanations during the decision-support phase.
Our study discusses new challenges of assisting user's onboarding with AI for
human-AI collaborative decision-making.",2024-09-24,"Min Hun Lee, Renee Bao Xuan Ng, Silvana Xinyi Choo, Shamala Thilarajah",http://arxiv.org/pdf/2409.15814v1,cs.LG
Aided design of bridge aesthetics based on Stable Diffusion fine-tuning,"Stable Diffusion fine-tuning technique is tried to assist bridge-type
innovation. The bridge real photo dataset is built, and Stable Diffusion is
fine tuned by using four methods that are Textual Inversion, Dreambooth,
Hypernetwork and Lora. All of them can capture the main characteristics of
dataset images and realize the personalized customization of Stable Diffusion.
Through fine-tuning, Stable Diffusion is not only a drawing tool, but also has
the designer's innovative thinking ability. The fine tuned model can generate a
large number of innovative new bridge types, which can provide rich inspiration
for human designers. The result shows that this technology can be used as an
engine of creativity and a power multiplier for human designers.",2024-09-24,"Leye Zhang, Xiangxiang Tian, Chengli Zhang, Hongjun Zhang",http://arxiv.org/pdf/2409.15812v1,cs.LG
A Multi-Level Approach for Class Imbalance Problem in Federated Learning for Remote Industry 4.0 Applications,"Deep neural network (DNN) models are effective solutions for industry 4.0
applications (\eg oil spill detection, fire detection, anomaly detection).
However, training a DNN network model needs a considerable amount of data
collected from various sources and transferred to the central cloud server that
can be expensive and sensitive to privacy. For instance, in the remote offshore
oil field where network connectivity is vulnerable, a federated fog environment
can be a potential computing platform. Hence it is feasible to perform
computation within the federation. On the contrary, performing a DNN model
training using fog systems poses a security issue that the federated learning
(FL) technique can resolve. In this case, the new challenge is the class
imbalance problem that can be inherited in local data sets and can degrade the
performance of the global model. Therefore, FL training needs to be performed
considering the class imbalance problem locally. In addition, an efficient
technique to select the relevant worker model needs to be adopted at the global
level to increase the robustness of the global model. Accordingly, we utilize
one of the suitable loss functions addressing the class imbalance in workers at
the local level. In addition, we employ a dynamic threshold mechanism with
user-defined worker's weight to efficiently select workers for aggregation that
improve the global model's robustness. Finally, we perform an extensive
empirical evaluation to explore the benefits of our solution and find up to
3-5% performance improvement than baseline federated learning methods.",2024-09-24,"Razin Farhan Hussain, Mohsen Amini Salehi",http://arxiv.org/pdf/2409.15802v1,cs.LG
Towards Universal Large-Scale Foundational Model for Natural Gas Demand Forecasting,"In the context of global energy strategy, accurate natural gas demand
forecasting is crucial for ensuring efficient resource allocation and
operational planning. Traditional forecasting methods struggle to cope with the
growing complexity and variability of gas consumption patterns across diverse
industries and commercial sectors. To address these challenges, we propose the
first foundation model specifically tailored for natural gas demand
forecasting. Foundation models, known for their ability to generalize across
tasks and datasets, offer a robust solution to the limitations of traditional
methods, such as the need for separate models for different customer segments
and their limited generalization capabilities. Our approach leverages
contrastive learning to improve prediction accuracy in real-world scenarios,
particularly by tackling issues such as noise in historical consumption data
and the potential misclassification of similar data samples, which can lead to
degradation in the quaility of the representation and thus the accuracy of
downstream forecasting tasks. By integrating advanced noise filtering
techniques within the contrastive learning framework, our model enhances the
quality of learned representations, leading to more accurate predictions.
Furthermore, the model undergoes industry-specific fine-tuning during
pretraining, enabling it to better capture the unique characteristics of gas
consumption across various sectors. We conducted extensive experiments using a
large-scale dataset from ENN Group, which includes data from over 10,000
industrial, commercial, and welfare-related customers across multiple regions.
Our model outperformed existing state-of-the-art methods, demonstrating a
relative improvement in MSE by 3.68\% and in MASE by 6.15\% compared to the
best available model.",2024-09-24,"Xinxing Zhou, Jiaqi Ye, Shubao Zhao, Ming Jin, Zhaoxiang Hou, Chengyi Yang, Zengxiang Li, Yanlong Wen, Xiaojie Yuan",http://arxiv.org/pdf/2409.15794v1,cs.LG
"Small Language Models: Survey, Measurements, and Insights","Small language models (SLMs), despite their widespread adoption in modern
smart devices, have received significantly less academic attention compared to
their large language model (LLM) counterparts, which are predominantly deployed
in data centers and cloud environments. While researchers continue to improve
the capabilities of LLMs in the pursuit of artificial general intelligence, SLM
research aims to make machine intelligence more accessible, affordable, and
efficient for everyday tasks. Focusing on transformer-based, decoder-only
language models with 100M-5B parameters, we survey 70 state-of-the-art
open-source SLMs, analyzing their technical innovations across three axes:
architectures, training datasets, and training algorithms. In addition, we
evaluate their capabilities in various domains, including commonsense
reasoning, mathematics, in-context learning, and long context. To gain further
insight into their on-device runtime costs, we benchmark their inference
latency and memory footprints. Through in-depth analysis of our benchmarking
data, we offer valuable insights to advance research in this field.",2024-09-24,"Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, Mengwei Xu",http://arxiv.org/pdf/2409.15790v3,cs.LG
Deep-learning real-time phase retrieval of imperfect diffraction patterns from X-ray free-electron lasers,"Machine learning is attracting surging interest across nearly all scientific
areas by enabling the analysis of large datasets and the extraction of
scientific information from incomplete data. Data-driven science is rapidly
growing, especially in X-ray methodologies, where advanced light sources and
detection technologies accumulate vast amounts of data that exceed meticulous
human inspection capabilities. Despite the increasing demands, the full
application of machine learning has been hindered by the need for data-specific
optimizations. In this study, we introduce a new deep-learning-based phase
retrieval method for imperfect diffraction data. This method provides robust
phase retrieval for simulated data and performs well on weak-signal
single-pulse diffraction data from X-ray free-electron lasers. Moreover, the
method significantly reduces data processing time, facilitating real-time image
reconstructions that are crucial for high-repetition-rate data acquisition.
Thus, this approach offers a reliable solution to the phase problem and is
expected to be widely adopted across various research areas.",2024-09-24,"Sung Yun Lee, Do Hyung Cho, Chulho Jung, Daeho Sung, Daewoong Nam, Sangsoo Kim, Changyong Song",http://arxiv.org/pdf/2409.15784v1,cs.LG
Zero-shot forecasting of chaotic systems,"Time-series forecasting is a challenging problem that traditionally requires
specialized models custom-trained for the specific task at hand. Recently,
inspired by the success of large language models, foundation models pre-trained
on vast amounts of time-series data from diverse domains have emerged as a
promising candidate for general-purpose time-series forecasting. The defining
characteristic of these foundation models is their ability to perform zero-shot
learning, that is, forecasting a new system from limited context data without
explicit re-training or fine-tuning. Here, we evaluate whether the zero-shot
learning paradigm extends to the challenging task of forecasting chaotic
systems. Across 135 distinct chaotic dynamical systems and $10^8$ timepoints,
we find that foundation models produce competitive forecasts compared to
custom-trained models (including NBEATS, TiDE, etc.), particularly when
training data is limited. Interestingly, even after point forecasts fail, large
foundation models are able to preserve the geometric and statistical properties
of the chaotic attractors. We attribute this success to foundation models'
ability to perform in-context learning and identify context parroting as a
simple mechanism used by these models to capture the long-term behavior of
chaotic dynamical systems. Our results highlight the potential of foundation
models as a tool for probing nonlinear and complex systems.",2024-09-24,"Yuanzhao Zhang, William Gilpin",http://arxiv.org/pdf/2409.15771v3,cs.LG
Spatial-Temporal Mixture-of-Graph-Experts for Multi-Type Crime Prediction,"As various types of crime continue to threaten public safety and economic
development, predicting the occurrence of multiple types of crimes becomes
increasingly vital for effective prevention measures. Although extensive
efforts have been made, most of them overlook the heterogeneity of different
crime categories and fail to address the issue of imbalanced spatial
distribution. In this work, we propose a Spatial-Temporal
Mixture-of-Graph-Experts (ST-MoGE) framework for collective multiple-type crime
prediction. To enhance the model's ability to identify diverse spatial-temporal
dependencies and mitigate potential conflicts caused by spatial-temporal
heterogeneity of different crime categories, we introduce an attentive-gated
Mixture-of-Graph-Experts (MGEs) module to capture the distinctive and shared
crime patterns of each crime category. Then, we propose Cross-Expert
Contrastive Learning(CECL) to update the MGEs and force each expert to focus on
specific pattern modeling, thereby reducing blending and redundancy.
Furthermore, to address the issue of imbalanced spatial distribution, we
propose a Hierarchical Adaptive Loss Re-weighting (HALR) approach to eliminate
biases and insufficient learning of data-scarce regions. To evaluate the
effectiveness of our methods, we conduct comprehensive experiments on two
real-world crime datasets and compare our results with twelve advanced
baselines. The experimental results demonstrate the superiority of our methods.",2024-09-24,"Ziyang Wu, Fan Liu, Jindong Han, Yuxuan Liang, Hao Liu",http://arxiv.org/pdf/2409.15764v1,cs.LG
TFG: Unified Training-Free Guidance for Diffusion Models,"Given an unconditional diffusion model and a predictor for a target property
of interest (e.g., a classifier), the goal of training-free guidance is to
generate samples with desirable target properties without additional training.
Existing methods, though effective in various individual applications, often
lack theoretical grounding and rigorous testing on extensive benchmarks. As a
result, they could even fail on simple tasks, and applying them to a new
problem becomes unavoidably difficult. This paper introduces a novel
algorithmic framework encompassing existing methods as special cases, unifying
the study of training-free guidance into the analysis of an algorithm-agnostic
design space. Via theoretical and empirical investigation, we propose an
efficient and effective hyper-parameter searching strategy that can be readily
applied to any downstream task. We systematically benchmark across 7 diffusion
models on 16 tasks with 40 targets, and improve performance by 8.5% on average.
Our framework and benchmark offer a solid foundation for conditional generation
in a training-free manner.",2024-09-24,"Haotian Ye, Haowei Lin, Jiaqi Han, Minkai Xu, Sheng Liu, Yitao Liang, Jianzhu Ma, James Zou, Stefano Ermon",http://arxiv.org/pdf/2409.15761v2,cs.LG
Development and Validation of Heparin Dosing Policies Using an Offline Reinforcement Learning Algorithm,"Appropriate medication dosages in the intensive care unit (ICU) are critical
for patient survival. Heparin, used to treat thrombosis and inhibit blood
clotting in the ICU, requires careful administration due to its complexity and
sensitivity to various factors, including patient clinical characteristics,
underlying medical conditions, and potential drug interactions. Incorrect
dosing can lead to severe complications such as strokes or excessive bleeding.
To address these challenges, this study proposes a reinforcement learning
(RL)-based personalized optimal heparin dosing policy that guides dosing
decisions reliably within the therapeutic range based on individual patient
conditions. A batch-constrained policy was implemented to minimize
out-of-distribution errors in an offline RL environment and effectively
integrate RL with existing clinician policies. The policy's effectiveness was
evaluated using weighted importance sampling, an off-policy evaluation method,
and the relationship between state representations and Q-values was explored
using t-SNE. Both quantitative and qualitative analyses were conducted using
the Medical Information Mart for Intensive Care III (MIMIC-III) database,
demonstrating the efficacy of the proposed RL-based medication policy.
Leveraging advanced machine learning techniques and extensive clinical data,
this research enhances heparin administration practices and establishes a
precedent for the development of sophisticated decision-support tools in
medicine.",2024-09-24,"Yooseok Lim, Inbeom Park, Sujee Lee",http://arxiv.org/pdf/2409.15753v1,cs.LG
The Roles of Generative Artificial Intelligence in Internet of Electric Vehicles,"With the advancements of generative artificial intelligence (GenAI) models,
their capabilities are expanding significantly beyond content generation and
the models are increasingly being used across diverse applications.
Particularly, GenAI shows great potential in addressing challenges in the
electric vehicle (EV) ecosystem ranging from charging management to
cyber-attack prevention. In this paper, we specifically consider Internet of
electric vehicles (IoEV) and we categorize GenAI for IoEV into four different
layers namely, EV's battery layer, individual EV layer, smart grid layer, and
security layer. We introduce various GenAI techniques used in each layer of
IoEV applications. Subsequently, public datasets available for training the
GenAI models are summarized. Finally, we provide recommendations for future
directions. This survey not only categorizes the applications of GenAI in IoEV
across different layers but also serves as a valuable resource for researchers
and practitioners by highlighting the design and implementation challenges
within each layer. Furthermore, it provides a roadmap for future research
directions, enabling the development of more robust and efficient IoEV systems
through the integration of advanced GenAI techniques.",2024-09-24,"Hanwen Zhang, Dusit Niyato, Wei Zhang, Changyuan Zhao, Hongyang Du, Abbas Jamalipour, Sumei Sun, Yiyang Pei",http://arxiv.org/pdf/2409.15750v3,cs.LG
Training Neural Networks for Modularity aids Interpretability,"An approach to improve network interpretability is via clusterability, i.e.,
splitting a model into disjoint clusters that can be studied independently. We
find pretrained models to be highly unclusterable and thus train models to be
more modular using an ``enmeshment loss'' function that encourages the
formation of non-interacting clusters. Using automated interpretability
measures, we show that our method finds clusters that learn different,
disjoint, and smaller circuits for CIFAR-10 labels. Our approach provides a
promising direction for making neural networks easier to interpret.",2024-09-24,"Satvik Golechha, Dylan Cope, Nandi Schoots",http://arxiv.org/pdf/2409.15747v1,cs.LG
Trust-Region Sequential Quadratic Programming for Stochastic Optimization with Random Models,"In this work, we consider solving optimization problems with a stochastic
objective and deterministic equality constraints. We propose a Trust-Region
Sequential Quadratic Programming method to find both first- and second-order
stationary points. Our method utilizes a random model to represent the
objective function, which is constructed from stochastic observations of the
objective and is designed to satisfy proper adaptive accuracy conditions with a
high but fixed probability. To converge to first-order stationary points, our
method computes a gradient step in each iteration defined by minimizing a
quadratic approximation of the objective subject to a (relaxed) linear
approximation of the problem constraints and a trust-region constraint. To
converge to second-order stationary points, our method additionally computes an
eigen step to explore the negative curvature of the reduced Hessian matrix, as
well as a second-order correction step to address the potential Maratos effect,
which arises due to the nonlinearity of the problem constraints. Such an effect
may impede the method from moving away from saddle points. Both gradient and
eigen step computations leverage a novel parameter-free decomposition of the
step and the trust-region radius, accounting for the proportions among the
feasibility residual, optimality residual, and negative curvature. We establish
global almost sure first- and second-order convergence guarantees for our
method, and present computational results on CUTEst problems, regression
problems, and saddle-point problems to demonstrate its superiority over
existing line-search-based stochastic methods.",2024-09-24,"Yuchen Fang, Sen Na, Michael W. Mahoney, Mladen Kolar",http://arxiv.org/pdf/2409.15734v2,cs.LG
EvoFA: Evolvable Fast Adaptation for EEG Emotion Recognition,"Electroencephalography (EEG)-based emotion recognition has gained significant
traction due to its accuracy and objectivity. However, the non-stationary
nature of EEG signals leads to distribution drift over time, causing severe
performance degradation when the model is reused. While numerous domain
adaptation (DA) approaches have been proposed in recent years to address this
issue, their reliance on large amounts of target data for calibration restricts
them to offline scenarios, rendering them unsuitable for real-time
applications. To address this challenge, this paper proposes Evolvable Fast
Adaptation (EvoFA), an online adaptive framework tailored for EEG data. EvoFA
organically integrates the rapid adaptation of Few-Shot Learning (FSL) and the
distribution matching of Domain Adaptation (DA) through a two-stage
generalization process. During the training phase, a robust base meta-learning
model is constructed for strong generalization. In the testing phase, a
designed evolvable meta-adaptation module iteratively aligns the marginal
distribution of target (testing) data with the evolving source (training) data
within a model-agnostic meta-learning framework, enabling the model to learn
the evolving trends of testing data relative to training data and improving
online testing performance. Experimental results demonstrate that EvoFA
achieves significant improvements compared to the basic FSL method and previous
online methods. The introduction of EvoFA paves the way for broader adoption of
EEG-based emotion recognition in real-world applications. Our code will be
released upon publication.",2024-09-24,"Ming Jin, Danni Zhang, Gangming Zhao, Changde Du, Jinpeng Li",http://arxiv.org/pdf/2409.15733v1,cs.LG
Federated Large Language Models: Current Progress and Future Directions,"Large language models are rapidly gaining popularity and have been widely
adopted in real-world applications. While the quality of training data is
essential, privacy concerns arise during data collection. Federated learning
offers a solution by allowing multiple clients to collaboratively train LLMs
without sharing local data. However, FL introduces new challenges, such as
model convergence issues due to heterogeneous data and high communication
costs. A comprehensive study is required to address these challenges and guide
future research. This paper surveys Federated learning for LLMs (FedLLM),
highlighting recent advances and future directions. We focus on two key
aspects: fine-tuning and prompt learning in a federated setting, discussing
existing work and associated research challenges. We finally propose potential
research directions for federated LLMs, including pre-training and how LLMs can
further enhance federated learning.",2024-09-24,"Yuhang Yao, Jianyi Zhang, Junda Wu, Chengkai Huang, Yu Xia, Tong Yu, Ruiyi Zhang, Sungchul Kim, Ryan Rossi, Ang Li, Lina Yao, Julian McAuley, Yiran Chen, Carlee Joe-Wong",http://arxiv.org/pdf/2409.15723v1,cs.LG
Applying Incremental Learning in Binary-Addition-Tree Algorithm for Dynamic Binary-State Network Reliability,"This paper presents a novel approach to enhance the Binary-Addition-Tree
algorithm (BAT) by integrating incremental learning techniques. BAT, known for
its simplicity in development, implementation, and application, is a powerful
implicit enumeration method for solving network reliability and optimization
problems. However, it traditionally struggles with dynamic and large-scale
networks due to its static nature. By introducing incremental learning, we
enable the BAT to adapt and improve its performance iteratively as it
encounters new data or network changes. This integration allows for more
efficient computation, reduced redundancy without searching minimal paths and
cuts, and improves overall performance in dynamic environments. Experimental
results demonstrate the effectiveness of the proposed method, showing
significant improvements in both computational efficiency and solution quality
compared to the traditional BAT and indirect algorithms, such as MP-based
algorithms and MC-based algorithms.",2024-09-24,Wei-Chang Yeh,http://arxiv.org/pdf/2409.15721v1,cs.LG
Adversarial Federated Consensus Learning for Surface Defect Classification Under Data Heterogeneity in IIoT,"The challenge of data scarcity hinders the application of deep learning in
industrial surface defect classification (SDC), as it's difficult to collect
and centralize sufficient training data from various entities in Industrial
Internet of Things (IIoT) due to privacy concerns. Federated learning (FL)
provides a solution by enabling collaborative global model training across
clients while maintaining privacy. However, performance may suffer due to data
heterogeneity-discrepancies in data distributions among clients. In this paper,
we propose a novel personalized FL (PFL) approach, named Adversarial Federated
Consensus Learning (AFedCL), for the challenge of data heterogeneity across
different clients in SDC. First, we develop a dynamic consensus construction
strategy to mitigate the performance degradation caused by data heterogeneity.
Through adversarial training, local models from different clients utilize the
global model as a bridge to achieve distribution alignment, alleviating the
problem of global knowledge forgetting. Complementing this strategy, we propose
a consensus-aware aggregation mechanism. It assigns aggregation weights to
different clients based on their efficacy in global knowledge learning, thereby
enhancing the global model's generalization capabilities. Finally, we design an
adaptive feature fusion module to further enhance global knowledge utilization
efficiency. Personalized fusion weights are gradually adjusted for each client
to optimally balance global and local features. Compared with state-of-the-art
FL methods like FedALA, the proposed AFedCL method achieves an accuracy
increase of up to 5.67% on three SDC datasets.",2024-09-24,"Jixuan Cui, Jun Li, Zhen Mei, Yiyang Ni, Wen Chen, Zengxiang Li",http://arxiv.org/pdf/2409.15711v2,cs.LG
Agent-state based policies in POMDPs: Beyond belief-state MDPs,"The traditional approach to POMDPs is to convert them into fully observed
MDPs by considering a belief state as an information state. However, a
belief-state based approach requires perfect knowledge of the system dynamics
and is therefore not applicable in the learning setting where the system model
is unknown. Various approaches to circumvent this limitation have been proposed
in the literature. We present a unified treatment of some of these approaches
by viewing them as models where the agent maintains a local recursively
updateable agent state and chooses actions based on the agent state. We
highlight the different classes of agent-state based policies and the various
approaches that have been proposed in the literature to find good policies
within each class. These include the designer's approach to find optimal
non-stationary agent-state based policies, policy search approaches to find a
locally optimal stationary agent-state based policies, and the approximate
information state to find approximately optimal stationary agent-state based
policies. We then present how ideas from the approximate information state
approach have been used to improve Q-learning and actor-critic algorithms for
learning in POMDPs.",2024-09-24,"Amit Sinha, Aditya Mahajan",http://arxiv.org/pdf/2409.15703v1,cs.LG
GISExplainer: On Explainability of Graph Neural Networks via Game-theoretic Interaction Subgraphs,"Explainability is crucial for the application of black-box Graph Neural
Networks (GNNs) in critical fields such as healthcare, finance, cybersecurity,
and more. Various feature attribution methods, especially the
perturbation-based methods, have been proposed to indicate how much each
node/edge contributes to the model predictions. However, these methods fail to
generate connected explanatory subgraphs that consider the causal interaction
between edges within different coalition scales, which will result in
unfaithful explanations. In our study, we propose GISExplainer, a novel
game-theoretic interaction based explanation method that uncovers what the
underlying GNNs have learned for node classification by discovering
human-interpretable causal explanatory subgraphs. First, GISExplainer defines a
causal attribution mechanism that considers the game-theoretic interaction of
multi-granularity coalitions in candidate explanatory subgraph to quantify the
causal effect of an edge on the prediction. Second, GISExplainer assumes that
the coalitions with negative effects on the predictions are also significant
for model interpretation, and the contribution of the computation graph stems
from the combined influence of both positive and negative interactions within
the coalitions. Then, GISExplainer regards the explanation task as a sequential
decision process, in which a salient edges is successively selected and
connected to the previously selected subgraph based on its causal effect to
form an explanatory subgraph, ultimately striving for better explanations.
Additionally, an efficiency optimization scheme is proposed for the causal
attribution mechanism through coalition sampling. Extensive experiments
demonstrate that GISExplainer achieves better performance than state-of-the-art
approaches w.r.t. two quantitative metrics: Fidelity and Sparsity.",2024-09-24,"Xingping Xian, Jianlu Liu, Chao Wang, Tao Wu, Shaojie Qiao, Xiaochuan Tang, Qun Liu",http://arxiv.org/pdf/2409.15698v2,cs.LG
Linear Contextual Bandits with Interference,"Interference, a key concept in causal inference, extends the reward modeling
process by accounting for the impact of one unit's actions on the rewards of
others. In contextual bandit (CB) settings, where multiple units are present in
the same round, potential interference can significantly affect the estimation
of expected rewards for different arms, thereby influencing the decision-making
process. Although some prior work has explored multi-agent and adversarial
bandits in interference-aware settings, the effect of interference in CB, as
well as the underlying theory, remains significantly underexplored. In this
paper, we introduce a systematic framework to address interference in Linear CB
(LinCB), bridging the gap between causal inference and online decision-making.
We propose a series of algorithms that explicitly quantify the interference
effect in the reward modeling process and provide comprehensive theoretical
guarantees, including sublinear regret bounds, finite sample upper bounds, and
asymptotic properties. The effectiveness of our approach is demonstrated
through simulations and a synthetic data generated based on MovieLens data.",2024-09-24,"Yang Xu, Wenbin Lu, Rui Song",http://arxiv.org/pdf/2409.15682v1,cs.LG
Distributed Online Bandit Nonconvex Optimization with One-Point Residual Feedback via Dynamic Regret,"This paper considers the distributed online bandit optimization problem with
nonconvex loss functions over a time-varying digraph. This problem can be
viewed as a repeated game between a group of online players and an adversary.
At each round, each player selects a decision from the constraint set, and then
the adversary assigns an arbitrary, possibly nonconvex, loss function to this
player. Only the loss value at the current round, rather than the entire loss
function or any other information (e.g. gradient), is privately revealed to the
player. Players aim to minimize a sequence of global loss functions, which are
the sum of local losses. We observe that traditional multi-point bandit
algorithms are unsuitable for online optimization, where the data for the loss
function are not all a priori, while the one-point bandit algorithms suffer
from poor regret guarantees. To address these issues, we propose a novel
one-point residual feedback distributed online algorithm. This algorithm
estimates the gradient using residuals from two points, effectively reducing
the regret bound while maintaining $\mathcal{O}(1)$ sampling complexity per
iteration. We employ a rigorous metric, dynamic regret, to evaluate the
algorithm's performance. By appropriately selecting the step size and smoothing
parameters, we demonstrate that the expected dynamic regret of our algorithm is
comparable to existing algorithms that use two-point feedback, provided the
deviation in the objective function sequence and the path length of the
minimization grows sublinearly. Finally, we validate the effectiveness of the
proposed algorithm through numerical simulations.",2024-09-24,"Youqing Hua, Shuai Liu, Yiguang Hong, Karl Henrik Johansson, Guangchen Wang",http://arxiv.org/pdf/2409.15680v1,cs.LG
Northeast Materials Database (NEMAD): Enabling Discovery of High Transition Temperature Magnetic Compounds,"The discovery of novel magnetic materials with greater operating temperature
ranges and optimized performance is essential for advanced applications.
Current data-driven approaches are challenging and limited due to the lack of
accurate, comprehensive, and feature-rich databases. This study aims to address
this challenge by introducing a new approach that uses Large Language Models
(LLMs) to create a comprehensive, experiment-based, magnetic materials database
named the Northeast Materials Database (NEMAD), which consists of 26,706
magnetic materials (www.nemad.org). The database incorporates chemical
composition, magnetic phase transition temperatures, structural details, and
magnetic properties. Enabled by NEMAD, machine learning models were developed
to classify materials and predict transition temperatures. Our classification
model achieved an accuracy of 90% in categorizing materials as ferromagnetic
(FM), antiferromagnetic (AFM), and non-magnetic (NM). The regression models
predict Curie (N\'eel) temperature with a coefficient of determination (R2) of
0.86 (0.85) and a mean absolute error (MAE) of 62K (32K). These models
identified 62 (19) FM (AFM) candidates with a predicted Curie (N\'eel)
temperature above 500K (100K) from the Materials Project. This work shows the
feasibility of combining LLMs for automated data extraction and machine
learning models in accelerating the discovery of magnetic materials.",2024-09-24,"Suman Itani, Yibo Zhang, Jiadong Zang",http://arxiv.org/pdf/2409.15675v1,cs.LG
Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer for Stock Time Series Forecasting,"Spatial-temporal graph neural networks (STGNNs) have achieved significant
success in various time series forecasting tasks. However, due to the lack of
explicit and fixed spatial relationships in stock prediction tasks, many STGNNs
fail to perform effectively in this domain. While some STGNNs learn spatial
relationships from time series, they often lack comprehensiveness. Research
indicates that modeling time series using feature changes as tokens reveals
entirely different information compared to using time steps as tokens. To more
comprehensively extract dynamic spatial information from stock data, we propose
a Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer
(DPA-STIFormer). DPA-STIFormer models each node via continuous changes in
features as tokens and introduces a Double Direction Self-adaptation Fusion
mechanism. This mechanism decomposes node encoding into temporal and feature
representations, simultaneously extracting different spatial correlations from
a double path approach, and proposes a Double-path gating mechanism to fuse
these two types of correlation information. Experiments conducted on four stock
market datasets demonstrate state-of-the-art results, validating the model's
superior capability in uncovering latent temporal-correlation patterns.",2024-09-24,"Wenbo Yan, Ying Tan",http://arxiv.org/pdf/2409.15662v1,cs.LG
FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL Benchmark,"Text-to-SQL systems have become crucial for translating natural language into
SQL queries in various industries, enabling non-technical users to perform
complex data operations. The need for accurate evaluation methods has increased
as these systems have grown more sophisticated. However, the Execution Accuracy
(EX), the most prevalent evaluation metric, still shows many false positives
and negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel
approach to evaluating text-to-SQL systems using large language models (LLMs)
to emulate human expert-level evaluation of SQL queries. Our metric improves
agreement with human experts (from 62 to 87.04 in Cohen's kappa) with
comprehensive context and sophisticated criteria. Our extensive experiments
yield several key insights: (1) Models' performance increases by over 2.6
points on average, substantially affecting rankings on Spider and BIRD
benchmarks; (2) The underestimation of models in EX primarily stems from
annotation quality issues; and (3) Model performance on particularly
challenging questions tends to be overestimated. This work contributes to a
more accurate and nuanced evaluation of text-to-SQL systems, potentially
reshaping our understanding of state-of-the-art performance in this field.",2024-09-24,"Heegyu Kim, Taeyang Jeon, Seunghwan Choi, Seungtaek Choi, Hyunsouk Cho",http://arxiv.org/pdf/2409.19014v4,cs.LG
M$^2$PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning,"Multimodal Large Language Models (MLLMs) demonstrate remarkable performance
across a wide range of domains, with increasing emphasis on enhancing their
zero-shot generalization capabilities for unseen tasks across various
modalities. Instruction tuning has emerged as an effective strategy for
achieving zero-shot generalization by finetuning pretrained models on diverse
multimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient
finetuning becomes increasingly critical. However, most existing
parameter-efficient approaches focus only on single modalities and often
overlook the multimodal characteristics during finetuning. In this work, we
introduce a novel Multimodal Prompt Tuning (M$^2$PT) approach for efficient
instruction tuning of MLLMs. M$^2$PT effectively integrates visual and textual
prompts into the vision encoder and language processor respectively during
finetuning, facilitating the extraction and alignment of features across
modalities. Empirical results on various multimodal evaluation datasets
demonstrate the superior performance of our approach compared to several
state-of-the-art baselines. A comprehensive set of ablation studies validates
the effectiveness of our prompt design and the efficiency of our approach.",2024-09-24,"Taowen Wang, Yiyang Liu, James Chenhao Liang, junhan zhao, Yiming Cui, Yuning Mao, Shaoliang Nie, Jiahao Liu, Fuli Feng, Zenglin Xu, Cheng Han, Lifu Huang, Qifan Wang, Dongfang Liu",http://arxiv.org/pdf/2409.15657v4,cs.LG
English offensive text detection using CNN based Bi-GRU model,"Over the years, the number of users of social media has increased
drastically. People frequently share their thoughts through social platforms,
and this leads to an increase in hate content. In this virtual community,
individuals share their views, express their feelings, and post photos, videos,
blogs, and more. Social networking sites like Facebook and Twitter provide
platforms to share vast amounts of content with a single click. However, these
platforms do not impose restrictions on the uploaded content, which may include
abusive language and explicit images unsuitable for social media. To resolve
this issue, a new idea must be implemented to divide the inappropriate content.
Numerous studies have been done to automate the process. In this paper, we
propose a new Bi-GRU-CNN model to classify whether the text is offensive or
not. The combination of the Bi-GRU and CNN models outperforms the existing
model.",2024-09-24,"Tonmoy Roy, Md Robiul Islam, Asif Ahammad Miazee, Anika Antara, Al Amin, Sunjim Hossain",http://arxiv.org/pdf/2409.15652v3,cs.LG
SurgIRL: Towards Life-Long Learning for Surgical Automation by Incremental Reinforcement Learning,"Surgical automation holds immense potential to improve the outcome and
accessibility of surgery. Recent studies use reinforcement learning to learn
policies that automate different surgical tasks. However, these policies are
developed independently and are limited in their reusability when the task
changes, making it more time-consuming when robots learn to solve multiple
tasks. Inspired by how human surgeons build their expertise, we train surgical
automation policies through Surgical Incremental Reinforcement Learning
(SurgIRL). SurgIRL aims to (1) acquire new skills by referring to external
policies (knowledge) and (2) accumulate and reuse these skills to solve
multiple unseen tasks incrementally (incremental learning). Our SurgIRL
framework includes three major components. We first define an expandable
knowledge set containing heterogeneous policies that can be helpful for
surgical tasks. Then, we propose Knowledge Inclusive Attention Network with
mAximum Coverage Exploration (KIAN-ACE), which improves learning efficiency by
maximizing the coverage of the knowledge set during the exploration process.
Finally, we develop incremental learning pipelines based on KIAN-ACE to
accumulate and reuse learned knowledge and solve multiple surgical tasks
sequentially. Our simulation experiments show that KIAN-ACE efficiently learns
to automate ten surgical tasks separately or incrementally. We also evaluate
our learned policies on the da Vinci Research Kit (dVRK) and demonstrate
successful sim-to-real transfers.",2024-09-24,"Yun-Jie Ho, Zih-Yun Chiu, Yuheng Zhi, Michael C. Yip",http://arxiv.org/pdf/2409.15651v1,cs.LG
Looped Transformers for Length Generalization,"Recent work has shown that Transformers trained from scratch can successfully
solve various arithmetic and algorithmic tasks, such as adding numbers and
computing parity. While these Transformers generalize well on unseen inputs of
the same length, they struggle with length generalization, i.e., handling
inputs of unseen lengths. In this work, we demonstrate that looped Transformers
with an adaptive number of steps significantly improve length generalization.
We focus on tasks with a known iterative solution, involving multiple
iterations of a RASP-L operation - a length-generalizable operation that can be
expressed by a finite-sized Transformer. We train looped Transformers using our
proposed learning algorithm and observe that they learn highly
length-generalizable solutions for various tasks.",2024-09-24,"Ying Fan, Yilun Du, Kannan Ramchandran, Kangwook Lee",http://arxiv.org/pdf/2409.15647v5,cs.LG
Personalized Federated Learning via Backbone Self-Distillation,"In practical scenarios, federated learning frequently necessitates training
personalized models for each client using heterogeneous data. This paper
proposes a backbone self-distillation approach to facilitate personalized
federated learning. In this approach, each client trains its local model and
only sends the backbone weights to the server. These weights are then
aggregated to create a global backbone, which is returned to each client for
updating. However, the client's local backbone lacks personalization because of
the common representation. To solve this problem, each client further performs
backbone self-distillation by using the global backbone as a teacher and
transferring knowledge to update the local backbone. This process involves
learning two components: the shared backbone for common representation and the
private head for local personalization, which enables effective global
knowledge transfer. Extensive experiments and comparisons with 12
state-of-the-art approaches demonstrate the effectiveness of our approach.",2024-09-24,"Pengju Wang, Bochao Liu, Dan Zeng, Chenggang Yan, Shiming Ge",http://arxiv.org/pdf/2409.15636v1,cs.LG
Data Augmentation for Sparse Multidimensional Learning Performance Data Using Generative AI,"Learning performance data describe correct and incorrect answers or
problem-solving attempts in adaptive learning, such as in intelligent tutoring
systems (ITSs). Learning performance data tend to be highly sparse
(80\%\(\sim\)90\% missing observations) in most real-world applications due to
adaptive item selection. This data sparsity presents challenges to using
learner models to effectively predict future performance explore new hypotheses
about learning. This article proposes a systematic framework for augmenting
learner data to address data sparsity in learning performance data. First,
learning performance is represented as a three-dimensional tensor of learners'
questions, answers, and attempts, capturing longitudinal knowledge states
during learning. Second, a tensor factorization method is used to impute
missing values in sparse tensors of collected learner data, thereby grounding
the imputation on knowledge tracing tasks that predict missing performance
values based on real observations. Third, a module for generating patterns of
learning is used. This study contrasts two forms of generative Artificial
Intelligence (AI), including Generative Adversarial Networks (GANs) and
Generate Pre-Trained Transformers (GPT) to generate data associated with
different clusters of learner data. We tested this approach on an adult
literacy dataset from AutoTutor lessons developed for Adult Reading
Comprehension (ARC). We found that: (1) tensor factorization improved the
performance in tracing and predicting knowledge mastery compared with other
knowledge tracing techniques without data augmentation, showing higher relative
fidelity for this imputation method, and (2) the GAN-based simulation showed
greater overall stability and less statistical bias based on a divergence
evaluation with varying simulation sample sizes compared to GPT.",2024-09-24,"Liang Zhang, Jionghao Lin, John Sabatini, Conrad Borchers, Daniel Weitekamp, Meng Cao, John Hollander, Xiangen Hu, Arthur C. Graesser",http://arxiv.org/pdf/2409.15631v3,cs.LG
Improving Academic Skills Assessment with NLP and Ensemble Learning,"This study addresses the critical challenges of assessing foundational
academic skills by leveraging advancements in natural language processing
(NLP). Traditional assessment methods often struggle to provide timely and
comprehensive feedback on key cognitive and linguistic aspects, such as
coherence, syntax, and analytical reasoning. Our approach integrates multiple
state-of-the-art NLP models, including BERT, RoBERTa, BART, DeBERTa, and T5,
within an ensemble learning framework. These models are combined through
stacking techniques using LightGBM and Ridge regression to enhance predictive
accuracy. The methodology involves detailed data preprocessing, feature
extraction, and pseudo-label learning to optimize model performance. By
incorporating sophisticated NLP techniques and ensemble learning, this study
significantly improves the accuracy and efficiency of assessments, offering a
robust solution that surpasses traditional methods and opens new avenues for
educational technology research focused on enhancing core academic
competencies.",2024-09-23,"Xinyi Huang, Yingyi Wu, Danyang Zhang, Jiacheng Hu, Yujian Long",http://arxiv.org/pdf/2409.19013v3,cs.LG
Reinforcement Feature Transformation for Polymer Property Performance Prediction,"Polymer property performance prediction aims to forecast specific features or
attributes of polymers, which has become an efficient approach to measuring
their performance. However, existing machine learning models face challenges in
effectively learning polymer representations due to low-quality polymer
datasets, which consequently impact their overall performance. This study
focuses on improving polymer property performance prediction tasks by
reconstructing an optimal and explainable descriptor representation space.
Nevertheless, prior research such as feature engineering and representation
learning can only partially solve this task since they are either
labor-incentive or unexplainable. This raises two issues: 1) automatic
transformation and 2) explainable enhancement. To tackle these issues, we
propose our unique Traceable Group-wise Reinforcement Generation Perspective.
Specifically, we redefine the reconstruction of the representation space into
an interactive process, combining nested generation and selection. Generation
creates meaningful descriptors, and selection eliminates redundancies to
control descriptor sizes. Our approach employs cascading reinforcement learning
with three Markov Decision Processes, automating descriptor and operation
selection, and descriptor crossing. We utilize a group-wise generation strategy
to explore and enhance reward signals for cascading agents. Ultimately, we
conduct experiments to indicate the effectiveness of our proposed framework.",2024-09-23,"Xuanming Hu, Dongjie Wang, Wangyang Ying, Yanjie Fu",http://arxiv.org/pdf/2409.15616v1,cs.LG
Revolutionizing Biomarker Discovery: Leveraging Generative AI for Bio-Knowledge-Embedded Continuous Space Exploration,"Biomarker discovery is vital in advancing personalized medicine, offering
insights into disease diagnosis, prognosis, and therapeutic efficacy.
Traditionally, the identification and validation of biomarkers heavily depend
on extensive experiments and statistical analyses. These approaches are
time-consuming, demand extensive domain expertise, and are constrained by the
complexity of biological systems. These limitations motivate us to ask: Can we
automatically identify the effective biomarker subset without substantial human
efforts? Inspired by the success of generative AI, we think that the intricate
knowledge of biomarker identification can be compressed into a continuous
embedding space, thus enhancing the search for better biomarkers. Thus, we
propose a new biomarker identification framework with two important modules:1)
training data preparation and 2) embedding-optimization-generation. The first
module uses a multi-agent system to automatically collect pairs of biomarker
subsets and their corresponding prediction accuracy as training data. These
data establish a strong knowledge base for biomarker identification. The second
module employs an encoder-evaluator-decoder learning paradigm to compress the
knowledge of the collected data into a continuous space. Then, it utilizes
gradient-based search techniques and autoregressive-based reconstruction to
efficiently identify the optimal subset of biomarkers. Finally, we conduct
extensive experiments on three real-world datasets to show the efficiency,
robustness, and effectiveness of our method.",2024-09-23,"Wangyang Ying, Dongjie Wang, Xuanming Hu, Ji Qiu, Jin Park, Yanjie Fu",http://arxiv.org/pdf/2409.15612v1,cs.LG
Deep Learning Approach for Knee Point Detection on Noisy Data,"A knee point on a curve is the one where the curve levels off after an
increase. In a computer system, it marks the point at which the system's
performance is no longer improving significantly despite adding extra
resources. Thus a knee point often represents an optimal point for decision.
However, identifying knee points in noisy data is a challenging task. All
previous works defined knee points based on the data in the original scale.
However, in this work, we define knee points based on normalized data and
provide a mathematical definition of curvature for normalized discrete data
points, based on the mathematical definition of curvature for continuous
functions. The impact of normalization exerted on curvature and the location of
knee points are also discussed. Nevertheless, assessing the effectiveness of
methods is difficult in the absence of ground truth data and benchmark
datasets, which makes comparing existing methods challenging. In view of this,
we create synthetic data that simulate real-world scenarios. We achieve this by
selecting a set of functions that possess the required characteristics in this
research and then introducing noise that satisfies the underlying distribution.
In addition, we present a deep-learning approach and employ a Convolutional
Neural Network (CNN) with a U-Net-like architecture, to accurately detect the
knee point(s) of the underlying true distribution. The proposed model is
evaluated against state-of-the-art methods. Experiments show that our network
outperforms existing methods in all synthetic datasets, regardless of whether
the samples have single or multiple knee points. In fact, our model achieves
the best $F_{1}$ scores among all existing methods in all the test sets.",2024-09-23,"Ting Yan Fok, Nong Ye",http://arxiv.org/pdf/2409.15608v1,cs.LG
Polyatomic Complexes: A topologically-informed learning representation for atomistic systems,"Developing robust representations of chemical structures that enable models
to learn topological inductive biases is challenging. In this manuscript, we
present a representation of atomistic systems. We begin by proving that our
representation satisfies all structural, geometric, efficiency, and
generalizability constraints. Afterward, we provide a general algorithm to
encode any atomistic system. Finally, we report performance comparable to
state-of-the-art methods on numerous tasks. We open-source all code and
datasets. The code and data are available at
https://github.com/rahulkhorana/PolyatomicComplexes.",2024-09-23,"Rahul Khorana, Marcus Noack, Jin Qian",http://arxiv.org/pdf/2409.15600v2,cs.LG
Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents,"Despite broad interest in modeling spoken dialogue agents, most approaches
are inherently ""half-duplex"" -- restricted to turn-based interaction with
responses requiring explicit prompting by the user or implicit tracking of
interruption or silence events. Human dialogue, by contrast, is ""full-duplex""
allowing for rich synchronicity in the form of quick and dynamic turn-taking,
overlapping speech, and backchanneling. Technically, the challenge of achieving
full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs
do not have a sense of ""time"". To bridge this gap, we propose Synchronous LLMs
for full-duplex spoken dialogue modeling. We design a novel mechanism to
integrate time information into Llama3-8b so that they run synchronously with
the real-world clock. We also introduce a training recipe that uses 212k hours
of synthetic spoken dialogue data generated from text dialogue data to create a
model that generates meaningful and natural spoken dialogue, with just 2k hours
of real-world spoken dialogue data. Synchronous LLMs outperform
state-of-the-art in dialogue meaningfulness while maintaining naturalness.
Finally, we demonstrate the model's ability to participate in full-duplex
dialogue by simulating interaction between two agents trained on different
datasets, while considering Internet-scale latencies of up to 240 ms. Webpage:
https://syncllm.cs.washington.edu/.",2024-09-23,"Bandhav Veluri, Benjamin N Peloquin, Bokai Yu, Hongyu Gong, Shyamnath Gollakota",http://arxiv.org/pdf/2409.15594v1,cs.LG
Unveiling the Potential of Graph Neural Networks in SME Credit Risk Assessment,"This paper takes the graph neural network as the technical framework,
integrates the intrinsic connections between enterprise financial indicators,
and proposes a model for enterprise credit risk assessment. The main research
work includes: Firstly, based on the experience of predecessors, we selected 29
enterprise financial data indicators, abstracted each indicator as a vertex,
deeply analyzed the relationships between the indicators, constructed a
similarity matrix of indicators, and used the maximum spanning tree algorithm
to achieve the graph structure mapping of enterprises; secondly, in the
representation learning phase of the mapped graph, a graph neural network model
was built to obtain its embedded representation. The feature vector of each
node was expanded to 32 dimensions, and three GraphSAGE operations were
performed on the graph, with the results pooled using the Pool operation, and
the final output of three feature vectors was averaged to obtain the graph's
embedded representation; finally, a classifier was constructed using a
two-layer fully connected network to complete the prediction task. Experimental
results on real enterprise data show that the model proposed in this paper can
well complete the multi-level credit level estimation of enterprises.
Furthermore, the tree-structured graph mapping deeply portrays the intrinsic
connections of various indicator data of the company, and according to the ROC
and other evaluation criteria, the model's classification effect is significant
and has good ""robustness"".",2024-09-23,"Bingyao Liu, Iris Li, Jianhua Yao, Yuan Chen, Guanming Huang, Jiajing Wang",http://arxiv.org/pdf/2409.17909v1,cs.LG
PipeFill: Using GPUs During Bubbles in Pipeline-parallel LLM Training,"Training Deep Neural Networks (DNNs) with billions of parameters generally
involves pipeline-parallel (PP) execution. Unfortunately, PP model training can
use GPUs inefficiently, especially at large scale, due to idle GPU time caused
by pipeline bubbles, which are often 15-30% and can exceed 60% of the training
job's GPU allocation. To improve the GPU utilization of PP model training, this
paper describes PipeFill, which fills pipeline bubbles with execution of other
pending jobs. By leveraging bubble GPU time, PipeFill reduces the GPU
utilization sacrifice associated with scaling-up of large-model training. To
context-switch between fill jobs and the main training job with minimal
overhead to the main job, and maximize fill job efficiency, PipeFill carefully
fits fill job work to measured bubble durations and GPU memory availability,
introduces explicit pipeline-bubble instructions, and orchestrates placement
and execution of fill jobs in pipeline bubbles. Experiments show that PipeFill
can increase overall utilization by up to 63% for GPUs used in large-scale LLM
training, with <2% slowdown of the training job, and 5-15% even for low-scale
LLM training. For large-scale LLM training on 8K GPUs, the 63% increase
translates to up to 2.6K additional GPUs worth of work completed.",2024-09-23,"Daiyaan Arfeen, Zhen Zhang, Xinwei Fu, Gregory R. Ganger, Yida Wang",http://arxiv.org/pdf/2410.07192v1,cs.LG
Generalization vs. Specialization under Concept Shift,"Machine learning models are often brittle under distribution shift, i.e.,
when data distributions at test time differ from those during training.
Understanding this failure mode is central to identifying and mitigating safety
risks of mass adoption of machine learning. Here we analyze ridge regression
under concept shift -- a form of distribution shift in which the input-label
relationship changes at test time. We derive an exact expression for prediction
risk in the high-dimensional limit. Our results reveal nontrivial effects of
concept shift on generalization performance, depending on the properties of
robust and nonrobust features of the input. We show that test performance can
exhibit a nonmonotonic data dependence, even when double descent is absent.
Finally, our experiments on MNIST and FashionMNIST suggest that this intriguing
behavior is present also in classification problems.",2024-09-23,"Alex Nguyen, David J. Schwab, Vudtiwat Ngampruetikorn",http://arxiv.org/pdf/2409.15582v1,cs.LG
Cross-Domain Latent Factors Sharing via Implicit Matrix Factorization,"Data sparsity has been one of the long-standing problems for recommender
systems. One of the solutions to mitigate this issue is to exploit knowledge
available in other source domains. However, many cross-domain recommender
systems introduce a complex architecture that makes them less scalable in
practice. On the other hand, matrix factorization methods are still considered
to be strong baselines for single-domain recommendations. In this paper, we
introduce the CDIMF, a model that extends the standard implicit matrix
factorization with ALS to cross-domain scenarios. We apply the Alternating
Direction Method of Multipliers to learn shared latent factors for overlapped
users while factorizing the interaction matrix. In a dual-domain setting,
experiments on industrial datasets demonstrate a competing performance of CDIMF
for both cold-start and warm-start. The proposed model can outperform most
other recent cross-domain and single-domain models. We also provide the code to
reproduce experiments on GitHub.",2024-09-23,"Abdulaziz Samra, Evgeney Frolov, Alexey Vasilev, Alexander Grigorievskiy, Anton Vakhrushev",http://arxiv.org/pdf/2409.15568v1,cs.LG
Asking an AI for salary negotiation advice is a matter of concern: Controlled experimental perturbation of ChatGPT for protected and non-protected group discrimination on a contextual task with no clear ground truth answers,"We conducted controlled experimental bias audits for four versions of
ChatGPT, which we asked to recommend an opening offer in salary negotiations
for a new hire. We submitted 98,800 prompts to each version, systematically
varying the employee's gender, university, and major, and tested prompts in
voice of each side of the negotiation: the employee versus employer. We find
ChatGPT as a multi-model platform is not robust and consistent enough to be
trusted for such a task. We observed statistically significant salary offers
when varying gender for all four models, although with smaller gaps than for
other attributes tested. The largest gaps were different model versions and
between the employee- vs employer-voiced prompts. We also observed substantial
gaps when varying university and major, but many of the biases were not
consistent across model versions. We tested for fictional and fraudulent
universities and found wildly inconsistent results across cases and model
versions. We make broader contributions to the AI/ML fairness literature. Our
scenario and our experimental design differ from mainstream AI/ML auditing
efforts in key ways. Bias audits typically test discrimination for protected
classes like gender, which we contrast with testing non-protected classes of
university and major. Asking for negotiation advice includes how aggressive one
ought to be in a negotiation relative to known empirical salary distributions
and scales, which is a deeply contextual and personalized task that has no
objective ground truth to validate. These results raise concerns for the
specific model versions we tested and ChatGPT as a multi-model platform in
continuous development. Our epistemology does not permit us to definitively
certify these models as either generally biased or unbiased on the attributes
we test, but our study raises matters of concern for stakeholders to further
investigate.",2024-09-23,"R. Stuart Geiger, Flynn O'Sullivan, Elsie Wang, Jonathan Lo",http://arxiv.org/pdf/2409.15567v3,cs.LG
CauSkelNet: Causal Representation Learning for Human Behaviour Analysis,"Traditional machine learning methods for movement recognition often struggle
with limited model interpretability and a lack of insight into human movement
dynamics. This study introduces a novel representation learning framework based
on causal inference to address these challenges. Our two-stage approach
combines the Peter-Clark (PC) algorithm and Kullback-Leibler (KL) divergence to
identify and quantify causal relationships between human joints. By capturing
joint interactions, the proposed causal Graph Convolutional Network (GCN)
produces interpretable and robust representations. Experimental results on the
EmoPain dataset demonstrate that the causal GCN outperforms traditional GCNs in
accuracy, F1 score, and recall, particularly in detecting protective behaviors.
This work contributes to advancing human motion analysis and lays a foundation
for adaptive and intelligent healthcare solutions.",2024-09-23,"Xingrui Gu, Chuyi Jiang, Erte Wang, Zekun Wu, Qiang Cui, Leimin Tian, Lianlong Wu, Siyang Song, Chuang Yu",http://arxiv.org/pdf/2409.15564v3,cs.LG
Identification and Mitigating Bias in Quantum Machine Learning,"As quantum machine learning (QML) emerges as a promising field at the
intersection of quantum computing and artificial intelligence, it becomes
crucial to address the biases and challenges that arise from the unique nature
of quantum systems. This research includes work on identification, diagnosis,
and response to biases in Quantum Machine Learning. This paper aims to provide
an overview of three key topics: How does bias unique to Quantum Machine
Learning look? Why and how can it occur? What can and should be done about it?",2024-09-23,"Nandhini Swaminathan, David Danks",http://arxiv.org/pdf/2409.19011v1,cs.LG
Stalactite: Toolbox for Fast Prototyping of Vertical Federated Learning Systems,"Machine learning (ML) models trained on datasets owned by different
organizations and physically located in remote databases offer benefits in many
real-world use cases. State regulations or business requirements often prevent
data transfer to a central location, making it difficult to utilize standard
machine learning algorithms. Federated Learning (FL) is a technique that
enables models to learn from distributed datasets without revealing the
original data. Vertical Federated learning (VFL) is a type of FL where data
samples are divided by features across several data owners. For instance, in a
recommendation task, a user can interact with various sets of items, and the
logs of these interactions are stored by different organizations. In this demo
paper, we present \emph{Stalactite} - an open-source framework for VFL that
provides the necessary functionality for building prototypes of VFL systems. It
has several advantages over the existing frameworks. In particular, it allows
researchers to focus on the algorithmic side rather than engineering and to
easily deploy learning in a distributed environment. It implements several VFL
algorithms and has a built-in homomorphic encryption layer. We demonstrate its
use on a real-world recommendation datasets.",2024-09-23,"Anastasiia Zakharova, Dmitriy Alexandrov, Maria Khodorchenko, Nikolay Butakov, Alexey Vasilev, Maxim Savchenko, Alexander Grigorievskiy",http://arxiv.org/pdf/2409.15558v2,cs.LG
Beyond Conformal Predictors: Adaptive Conformal Inference with Confidence Predictors,"Conformal prediction (CP) is a robust framework for distribution-free
uncertainty quantification, but it requires exchangeable data to ensure valid
prediction sets at a user-specified significance level. When this assumption is
violated, as in time-series or other structured data, the validity guarantees
of CP no longer hold. Adaptive conformal inference (ACI) was introduced to
address this limitation by adjusting the significance level dynamically,
ensuring finite-sample coverage guarantees even for non-exchangeable data. In
this paper, we show that ACI does not require the use of conformal predictors;
instead, it can be implemented with the more general confidence predictors,
which are computationally simpler and still maintain the crucial property of
nested prediction sets. Through experiments on synthetic and real-world data,
we demonstrate that confidence predictors can perform comparably to, or even
better than, conformal predictors, particularly in terms of computational
efficiency. These findings suggest that confidence predictors represent a
viable and efficient alternative to conformal predictors in non-exchangeable
data settings, although further studies are needed to identify when one method
is superior.",2024-09-23,Johan Hallberg Szabadváry,http://arxiv.org/pdf/2409.15548v3,cs.LG
Combining Open-box Simulation and Importance Sampling for Tuning Large-Scale Recommenders,"Growing scale of recommender systems require extensive tuning to respond to
market dynamics and system changes. We address the challenge of tuning a
large-scale ads recommendation platform with multiple continuous parameters
influencing key performance indicators (KPIs). Traditional methods like
open-box Monte Carlo simulators, while accurate, are computationally expensive
due to the high cost of evaluating numerous parameter settings. To mitigate
this, we propose a hybrid approach Simulator-Guided Importance Sampling (SGIS)
that combines open-box simulation with importance sampling (IS). SGIS leverages
the strengths of both techniques: it performs a coarse enumeration over the
parameter space to identify promising initial settings and then uses IS to
iteratively refine these settings. This approach significantly reduces
computational costs while maintaining high accuracy in KPI estimation. We
demonstrate the effectiveness of SGIS through simulations as well as real-world
experiments, showing that it achieves substantial improvements in KPIs with
lower computational overhead compared to traditional methods.",2024-09-23,"Kaushal Paneri, Michael Munje, Kailash Singh Maurya, Adith Swaminathan, Yifan Shi",http://arxiv.org/pdf/2410.03697v1,cs.LG
MRI Radiomics for IDH Genotype Prediction in Glioblastoma Diagnosis,"Radiomics is a relatively new field which utilises automatically identified
features from radiological scans. It has found a widespread application,
particularly in oncology because many of the important oncological biomarkers
are not visible to the naked eye. The recent advent of big data, including in
medical imaging, and the development of new ML techniques brought the
possibility of faster and more accurate oncological diagnosis. Furthermore,
standardised mathematical feature extraction based on radiomics helps to
eliminate possible radiologist bias. This paper reviews the recent development
in the oncological use of MRI radiomic features. It focuses on the
identification of the isocitrate dehydrogenase (IDH) mutation status, which is
an important biomarker for the diagnosis of glioblastoma and grade IV
astrocytoma.",2024-09-23,Stanislav Kozák,http://arxiv.org/pdf/2409.16329v1,cs.LG
Learning Diverse Robot Striking Motions with Diffusion Models and Kinematically Constrained Gradient Guidance,"Advances in robot learning have enabled robots to generate skills for a
variety of tasks. Yet, robot learning is typically sample inefficient,
struggles to learn from data sources exhibiting varied behaviors, and does not
naturally incorporate constraints. These properties are critical for fast,
agile tasks such as playing table tennis. Modern techniques for learning from
demonstration improve sample efficiency and scale to diverse data, but are
rarely evaluated on agile tasks. In the case of reinforcement learning,
achieving good performance requires training on high-fidelity simulators. To
overcome these limitations, we develop a novel diffusion modeling approach that
is offline, constraint-guided, and expressive of diverse agile behaviors. The
key to our approach is a kinematic constraint gradient guidance (KCGG)
technique that computes gradients through both the forward kinematics of the
robot arm and the diffusion model to direct the sampling process. KCGG
minimizes the cost of violating constraints while simultaneously keeping the
sampled trajectory in-distribution of the training data. We demonstrate the
effectiveness of our approach for time-critical robotic tasks by evaluating
KCGG in two challenging domains: simulated air hockey and real table tennis. In
simulated air hockey, we achieved a 25.4% increase in block rate, while in
table tennis, we saw a 17.3% increase in success rate compared to imitation
learning baselines.",2024-09-23,"Kin Man Lee, Sean Ye, Qingyu Xiao, Zixuan Wu, Zulfiqar Zaidi, David B. D'Ambrosio, Pannag R. Sanketi, Matthew Gombolay",http://arxiv.org/pdf/2409.15528v2,cs.LG
CANDERE-COACH: Reinforcement Learning from Noisy Feedback,"In recent times, Reinforcement learning (RL) has been widely applied to many
challenging tasks. However, in order to perform well, it requires access to a
good reward function which is often sparse or manually engineered with scope
for error. Introducing human prior knowledge is often seen as a possible
solution to the above-mentioned problem, such as imitation learning, learning
from preference, and inverse reinforcement learning. Learning from feedback is
another framework that enables an RL agent to learn from binary evaluative
signals describing the teacher's (positive or negative) evaluation of the
agent's action. However, these methods often make the assumption that
evaluative teacher feedback is perfect, which is a restrictive assumption. In
practice, such feedback can be noisy due to limited teacher expertise or other
exacerbating factors like cognitive load, availability, distraction, etc. In
this work, we propose the CANDERE-COACH algorithm, which is capable of learning
from noisy feedback by a nonoptimal teacher. We propose a noise-filtering
mechanism to de-noise online feedback data, thereby enabling the RL agent to
successfully learn with up to 40% of the teacher feedback being incorrect.
Experiments on three common domains demonstrate the effectiveness of the
proposed approach.",2024-09-23,"Yuxuan Li, Srijita Das, Matthew E. Taylor",http://arxiv.org/pdf/2409.15521v1,cs.LG
Enabling Efficient On-Device Fine-Tuning of LLMs Using Only Inference Engines,"Large Language Models (LLMs) are currently pre-trained and fine-tuned on
large cloud servers. The next frontier is LLM personalization, where a
foundation model can be fine-tuned with user/task-specific data. Given the
sensitive nature of such private data, it is desirable to fine-tune these
models on edge devices to improve user trust. However, fine-tuning on
resource-constrained edge devices presents significant challenges due to
substantial memory and computational demands, as well as limited infrastructure
support. We observe that inference engines (e.g., ExecuTorch) can be repurposed
for fine-tuning by leveraging zeroth-order (ZO) optimization, which uses
multiple forward passes to approximate gradients. However, directly applying ZO
methods on edge devices is impractical due to the high computational cost of
multiple model perturbations required to achieve accuracy improvements. Based
on these observations, we propose a memory- and computation-efficient LLM
fine-tuning method for edge devices. Our approach has three key innovations:
(1) We introduce a parallelized randomized gradient estimation (P-RGE)
technique that achieves high parallel efficiency by leveraging outer-loop and
inner-loop parallelization. This enables multiple function queries and forward
passes to be executed in parallel, reducing training time. (2) We integrate
P-RGE with parameter-efficient fine-tuning methods (e.g. LoRA) to further
reduce computational and memory overhead. (3) We implement a P-RGE LoRA-FA
module that fully supports fine-tuning with ExecuTorch. Our approach requires
no modifications to ExecuTorch's runtime code, as it can be implemented with
server-side code changes only. Experiments demonstrate that P-RGE achieves
substantial runtime speedups and memory savings while improving fine-tuning
accuracy, paving the way for practical deployment of LLMs in real-time,
on-device applications.",2024-09-23,"Lei Gao, Amir Ziashahabi, Yue Niu, Salman Avestimehr, Murali Annavaram",http://arxiv.org/pdf/2409.15520v2,cs.LG
Eagle: Efficient Training-Free Router for Multi-LLM Inference,"The proliferation of Large Language Models (LLMs) with varying capabilities
and costs has created a need for efficient model selection in AI systems. LLM
routers address this need by dynamically choosing the most suitable model for a
given query based on task requirements and budget constraints. However,
existing routers face challenges in scalability and real-time adaptation,
particularly in high-volume online environments. We present Eagle, a novel LLM
routing approach that combines global and local ELO ranking modules to overcome
these limitations. By evaluating both general and specialized LLM abilities,
Eagle provides a scalable, training-free solution that enhances model selection
quality while reducing computational overhead. Our experiments across multiple
datasets show Eagle consistently outperforms baseline methods, with
improvements of up to 23.52 percent in Area Under Curve (AUC) scores. Moreover,
Eagle demonstrates remarkable efficiency, requiring only 1/20 of baseline
methods' time for initialization and 100 to 200 times faster incremental
updates in online scenarios, making it well-suited for dynamic, high-volume
online serving environments.",2024-09-23,"Zesen Zhao, Shuowei Jin, Z. Morley Mao",http://arxiv.org/pdf/2409.15518v2,cs.LG
Curb Your Attention: Causal Attention Gating for Robust Trajectory Prediction in Autonomous Driving,"Trajectory prediction models in autonomous driving are vulnerable to
perturbations from non-causal agents whose actions should not affect the
ego-agent's behavior. Such perturbations can lead to incorrect predictions of
other agents' trajectories, potentially compromising the safety and efficiency
of the ego-vehicle's decision-making process. Motivated by this challenge, we
propose $\textit{Causal tRajecTory predICtion}$ $\textbf{(CRiTIC)}$, a novel
model that utilizes a $\textit{Causal Discovery Network}$ to identify
inter-agent causal relations over a window of past time steps. To incorporate
discovered causal relationships, we propose a novel $\textit{Causal Attention
Gating}$ mechanism to selectively filter information in the proposed
Transformer-based architecture. We conduct extensive experiments on two
autonomous driving benchmark datasets to evaluate the robustness of our model
against non-causal perturbations and its generalization capacity. Our results
indicate that the robustness of predictions can be improved by up to
$\textbf{54%}$ without a significant detriment to prediction accuracy. Lastly,
we demonstrate the superior domain generalizability of the proposed model,
which achieves up to $\textbf{29%}$ improvement in cross-domain performance.
These results underscore the potential of our model to enhance both robustness
and generalization capacity for trajectory prediction in diverse autonomous
driving domains. Further details can be found on our project page:
https://ehsan-ami.github.io/critic.",2024-09-23,"Ehsan Ahmadi, Ray Mercurius, Soheil Alizadeh, Kasra Rezaee, Amir Rasouli",http://arxiv.org/pdf/2410.07191v2,cs.LG
Bayesian computation with generative diffusion models by Multilevel Monte Carlo,"Generative diffusion models have recently emerged as a powerful strategy to
perform stochastic sampling in Bayesian inverse problems, delivering remarkably
accurate solutions for a wide range of challenging applications. However,
diffusion models often require a large number of neural function evaluations
per sample in order to deliver accurate posterior samples. As a result, using
diffusion models as stochastic samplers for Monte Carlo integration in Bayesian
computation can be highly computationally expensive, particularly in
applications that require a substantial number of Monte Carlo samples for
conducting uncertainty quantification analyses. This cost is especially high in
large-scale inverse problems such as computational imaging, which rely on large
neural networks that are expensive to evaluate. With quantitative imaging
applications in mind, this paper presents a Multilevel Monte Carlo strategy
that significantly reduces the cost of Bayesian computation with diffusion
models. This is achieved by exploiting cost-accuracy trade-offs inherent to
diffusion models to carefully couple models of different levels of accuracy in
a manner that significantly reduces the overall cost of the calculation,
without reducing the final accuracy. The proposed approach achieves a
$4\times$-to-$8\times$ reduction in computational cost w.r.t. standard
techniques across three benchmark imaging problems.",2024-09-23,"Abdul-Lateef Haji-Ali, Marcelo Pereyra, Luke Shaw, Konstantinos Zygalakis",http://arxiv.org/pdf/2409.15511v4,cs.LG
"A comprehensive study of on-device NLP applications -- VQA, automated Form filling, Smart Replies for Linguistic Codeswitching","Recent improvement in large language models, open doors for certain new
experiences for on-device applications which were not possible before. In this
work, we propose 3 such new experiences in 2 categories. First we discuss
experiences which can be powered in screen understanding i.e. understanding
whats on user screen namely - (1) visual question answering, and (2) automated
form filling based on previous screen. The second category of experience which
can be extended are smart replies to support for multilingual speakers with
code-switching. Code-switching occurs when a speaker alternates between two or
more languages. To the best of our knowledge, this is first such work to
propose these tasks and solutions to each of them, to bridge the gap between
latest research and real world impact of the research in on-device
applications.",2024-09-23,Naman Goyal,http://arxiv.org/pdf/2409.19010v1,cs.LG
Matérn Kernels for Tunable Implicit Surface Reconstruction,"We propose to use the family of Mat\'ern kernels for implicit surface
reconstruction, building upon the recent success of kernel methods for 3D
reconstruction of oriented point clouds. As we show from a theoretical and
practical perspective, Mat\'ern kernels have some appealing properties which
make them particularly well suited for surface reconstruction -- outperforming
state-of-the-art methods based on the arc-cosine kernel while being
significantly easier to implement, faster to compute, and scalable. Being
stationary, we demonstrate that Mat\'ern kernels allow for tunable surface
reconstruction in the same way as Fourier feature mappings help
coordinate-based MLPs overcome spectral bias. Moreover, we theoretically
analyze Mat\'ern kernels' connection to SIREN networks as well as their
relation to previously employed arc-cosine kernels. Finally, based on recently
introduced Neural Kernel Fields, we present data-dependent Mat\'ern kernels and
conclude that especially the Laplace kernel (being part of the Mat\'ern family)
is extremely competitive, performing almost on par with state-of-the-art
methods in the noise-free case while having a more than five times shorter
training time.",2024-09-23,"Maximilian Weiherer, Bernhard Egger",http://arxiv.org/pdf/2409.15466v2,cs.LG
Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking,"The release of ChatGPT in November 2022 sparked an explosion of interest in
post-training and an avalanche of new preference optimization (PO) methods.
These methods claim superior alignment by virtue of better correspondence with
human pairwise preferences, often measured by LLM-judges. In this work, we
attempt to answer the following question -- do LLM-judge preferences translate
to progress on other, more concrete metrics for alignment, and if not, why not?
We define a concrete metric for alignment, and introduce SOS-Bench (Substance
Outweighs Style Benchmark), which is to the best of our knowledge the largest
standardized, reproducible LLM meta-benchmark to date. We find that (1)
LLM-judge preferences do not correlate with concrete measures of safety, world
knowledge, and instruction following; (2) LLM-judges have powerful implicit
biases, prioritizing style over factuality and safety; and (3) the supervised
fine-tuning (SFT) stage of post-training, and not the PO stage, has the
greatest impact on alignment, with data scaling and prompt diversity as the
driving factors. Our codebase and complete results can be found at
https://github.com/penfever/sos-bench.",2024-09-23,"Benjamin Feuer, Micah Goldblum, Teresa Datta, Sanjana Nambiar, Raz Besaleli, Samuel Dooley, Max Cembalest, John P. Dickerson",http://arxiv.org/pdf/2409.15268v3,cs.LG
Peer-to-Peer Learning Dynamics of Wide Neural Networks,"Peer-to-peer learning is an increasingly popular framework that enables
beyond-5G distributed edge devices to collaboratively train deep neural
networks in a privacy-preserving manner without the aid of a central server.
Neural network training algorithms for emerging environments, e.g., smart
cities, have many design considerations that are difficult to tune in
deployment settings -- such as neural network architectures and
hyperparameters. This presents a critical need for characterizing the training
dynamics of distributed optimization algorithms used to train highly nonconvex
neural networks in peer-to-peer learning environments. In this work, we provide
an explicit characterization of the learning dynamics of wide neural networks
trained using popular distributed gradient descent (DGD) algorithms. Our
results leverage both recent advancements in neural tangent kernel (NTK) theory
and extensive previous work on distributed learning and consensus. We validate
our analytical results by accurately predicting the parameter and error
dynamics of wide neural networks trained for classification tasks.",2024-09-23,"Shreyas Chaudhari, Srinivasa Pranav, Emile Anand, José M. F. Moura",http://arxiv.org/pdf/2409.15267v3,cs.LG
UDA-Bench: Revisiting Common Assumptions in Unsupervised Domain Adaptation Using a Standardized Framework,"In this work, we take a deeper look into the diverse factors that influence
the efficacy of modern unsupervised domain adaptation (UDA) methods using a
large-scale, controlled empirical study. To facilitate our analysis, we first
develop UDA-Bench, a novel PyTorch framework that standardizes training and
evaluation for domain adaptation enabling fair comparisons across several UDA
methods. Using UDA-Bench, our comprehensive empirical study into the impact of
backbone architectures, unlabeled data quantity, and pre-training datasets
reveals that: (i) the benefits of adaptation methods diminish with advanced
backbones, (ii) current methods underutilize unlabeled data, and (iii)
pre-training data significantly affects downstream adaptation in both
supervised and self-supervised settings. In the context of unsupervised
adaptation, these observations uncover several novel and surprising properties,
while scientifically validating several others that were often considered
empirical heuristics or practitioner intuitions in the absence of a
standardized training and evaluation framework. The UDA-Bench framework and
trained models are publicly available at
https://github.com/ViLab-UCSD/UDABench_ECCV2024.",2024-09-23,"Tarun Kalluri, Sreyas Ravichandran, Manmohan Chandraker",http://arxiv.org/pdf/2409.15264v1,cs.LG
"The Palomar twilight survey of 'Ayló'chaxnim, Atiras, and comets","Near-sun sky twilight observations allow for the detection of asteroid
interior to the orbit of Venus (Aylos), the Earth (Atiras), and comets. We
present the results of observations with the Palomar 48-inch telescope
(P48)/Zwicky Transient Facility (ZTF) camera in 30 s r-band exposures taken
during evening astronomical twilight from 2019 Sep 20 to 2022 March 7 and
during morning astronomical twilight sky from 2019 Sep 21 to 2022 Sep 29. More
than 46,000 exposures were taken in evening and morning astronomical twilight
within 31 to 66 degrees from the Sun with an r-band limiting magnitude between
18.1 and 20.9. The twilight pointings show a slight seasonal dependence in
limiting magnitude and ability to point closer towards the Sun, with limiting
magnitude slightly improving during summer. In total, the one Aylo, (594913)
'Ayl\'o'chaxnim, and 4 Atiras, 2020 OV1, 2021 BS1, 2021 PB2, and 2021 VR3, were
discovered in evening and morning twilight observations. Additional twilight
survey discoveries also include 6 long-period comets: C/2020 T2, C/2020 V2,
C/2021 D2, C/2021 E3, C/2022 E3, and C/2022 P3, and two short-period comets:
P/2021 N1 and P/2022 P2 using deep learning comet detection pipelines. The
P48/ZTF twilight survey also recovered 11 known Atiras, one Aylo, three
short-period comes, two long-period comets, and one interstellar object.
Lastly, the Vera Rubin Observatory will conduct a twilight survey starting in
its first year of operations and will cover the sky within 45 degrees of the
Sun. Twilight surveys such as those by ZTF and future surveys will provide
opportunities for discovering asteroids inside the orbits of Earth and Venus.",2024-09-23,"B. T. Bolin, F. J. Masci, M. W. Coughlin, D. A. Duev, Ž. Ivezić, R. L. Jones, P. Yoachim, T. Ahumada, V. Bhalerao, H. Choudhary, C. Contreras, Y. -C. Cheng, C. M. Copperwheat, K. Deshmukh, C. Fremling, M. Granvik, K. K. Hardegree-Ullman, A. Y. Q. Ho, R. Jedicke, M. Kasliwal, H. Kumar, Z. -Y. Lin, A. Mahabal, A. Monson, J. D. Neill, D. Nesvorný, D. A. Perley, J. N. Purdum, R. Quimby, E. Serabyn, K. Sharma, V. Swain",http://arxiv.org/pdf/2409.15263v1,cs.LG
Identification and Localization of Cometary Activity in Solar System Objects with Machine Learning,"In this chapter, we will discuss the use of Machine Learning methods for the
identification and localization of cometary activity for Solar System objects
in ground and in space-based wide-field all-sky surveys. We will begin the
chapter by discussing the challenges of identifying known and unknown active,
extended Solar System objects in the presence of stellar-type sources and the
application of classical pre-ML identification techniques and their
limitations. We will then transition to the discussion of implementing ML
techniques to address the challenge of extended object identification. We will
finish with prospective future methods and the application to future surveys
such as the Vera C. Rubin Observatory.",2024-09-23,"Bryce T. Bolin, Michael W. Coughlin",http://arxiv.org/pdf/2409.15261v1,cs.LG
Archon: An Architecture Search Framework for Inference-Time Techniques,"Inference-time techniques are emerging as highly effective tools to enhance
large language model (LLM) capabilities. However, best practices for developing
systems that combine these techniques remain underdeveloped due to our limited
understanding of the utility of individual inference-time techniques and the
interactions between them. Additionally, efficiently and automatically
searching the space of model choices, inference-time techniques, and their
compositions is challenging due to the large design space. To address these
challenges, we introduce Archon, a modular framework for selecting, combining,
and stacking layers of inference-time techniques to construct optimized LLM
systems for target benchmarks. Rather than relying on a single LLM called once,
we leverage a diverse set of LLMs and inference-time techniques, creating LLM
systems greater than the sum of their parts. Archon defines an extensible
design space, encompassing techniques such as generation ensembling, repeated
sampling, ranking, fusion, critiquing, verification, and unit testing. It
transforms the problem of building LLM systems into a hyperparameter
optimization objective. Given the available LLMs, inference-time techniques,
and compute budget, Archon utilizes hyperparameter search techniques to
discover optimized architectures for target benchmark(s). We evaluate Archon
architectures across a range of instruction-following, reasoning, and coding
benchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval,
MixEval Hard, MATH, and CodeContests. Archon architectures outperform frontier
models, such as GPT-4o and Claude 3.5 Sonnet, on these benchmarks, achieving an
average accuracy increase of 15.1 percentage points by using all available
LLMs. We make our code and datasets available publicly on Github:
https://github.com/ScalingIntelligence/Archon.",2024-09-23,"Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha, E. Kelly Buchanan, Mayee Chen, Neel Guha, Christopher Ré, Azalia Mirhoseini",http://arxiv.org/pdf/2409.15254v5,cs.LG
Machine Learning Toric Duality in Brane Tilings,"We apply a variety of machine learning methods to the study of Seiberg
duality within 4d $\mathcal{N}=1$ quantum field theories arising on the
worldvolumes of D3-branes probing toric Calabi-Yau 3-folds. Such theories admit
an elegant description in terms of bipartite tessellations of the torus known
as brane tilings or dimer models. An intricate network of infrared dualities
interconnects the space of such theories and partitions it into universality
classes, the prediction and classification of which is a problem that naturally
lends itself to a machine learning investigation. In this paper, we address a
preliminary set of such enquiries. We begin by training a fully connected
neural network to identify classes of Seiberg dual theories realised on
$\mathbb{Z}_m\times\mathbb{Z}_n$ orbifolds of the conifold and achieve
$R^2=0.988$. Then, we evaluate various notions of robustness of our methods
against perturbations of the space of theories under investigation, and discuss
these results in terms of the nature of the neural network's learning. Finally,
we employ a more sophisticated residual architecture to classify the toric
phase space of the $Y^{6,0}$ theories, and to predict the individual gauged
linear $\sigma$-model multiplicities in toric diagrams thereof. In spite of the
non-trivial nature of this task, we achieve remarkably accurate results;
namely, upon fixing a choice of Kasteleyn matrix representative, the regressor
achieves a mean absolute error of $0.021$. We also discuss how the performance
is affected by relaxing these assumptions.",2024-09-23,"Pietro Capuozzo, Tancredi Schettini Gherardini, Benjamin Suzzoni",http://arxiv.org/pdf/2409.15251v1,cs.LG
On-Air Deep Learning Integrated Semantic Inference Models for Enhanced Earth Observation Satellite Networks,"Earth Observation (EO) systems are crucial for cartography, disaster
surveillance, and resource administration. Nonetheless, they encounter
considerable obstacles in the processing and transmission of extensive data,
especially in specialized domains such as precision agriculture and real-time
disaster response. Earth observation satellites, outfitted with remote sensing
technology, gather data from onboard sensors and IoT-enabled terrestrial
objects, delivering important information remotely. Domain-adapted Large
Language Models (LLMs) provide a solution by enabling the integration of raw
and processed EO data. Through domain adaptation, LLMs improve the assimilation
and analysis of many data sources, tackling the intricacies of specialized
datasets in agriculture and disaster response. This data synthesis, directed by
LLMs, enhances the precision and pertinence of conveyed information. This study
provides a thorough examination of using semantic inference and deep learning
for sophisticated EO systems. It presents an innovative architecture for
semantic communication in EO satellite networks, designed to improve data
transmission efficiency using semantic processing methodologies. Recent
advancements in onboard processing technologies enable dependable, adaptable,
and energy-efficient data management in orbit. These improvements guarantee
reliable performance in adverse space circumstances using radiation-hardened
and reconfigurable technology. Collectively, these advancements enable
next-generation satellite missions with improved processing capabilities,
crucial for operational flexibility and real-time decision-making in 6G
satellite communication.",2024-09-23,"Hong-fu Chou, Vu Nguyen Ha, Prabhu Thiruvasagam, Thanh-Dung Le, Geoffrey Eappen, Ti Ti Nguyen, Luis M. Garces-Socarras, Jorge L. Gonzalez-Rios, Juan Carlos Merlano-Duncan, Symeon Chatzinotas",http://arxiv.org/pdf/2409.15246v3,cs.LG
Fully automatic extraction of morphological traits from the Web: utopia or reality?,"Plant morphological traits, their observable characteristics, are fundamental
to understand the role played by each species within their ecosystem. However,
compiling trait information for even a moderate number of species is a
demanding task that may take experts years to accomplish. At the same time,
massive amounts of information about species descriptions is available online
in the form of text, although the lack of structure makes this source of data
impossible to use at scale. To overcome this, we propose to leverage recent
advances in large language models (LLMs) and devise a mechanism for gathering
and processing information on plant traits in the form of unstructured textual
descriptions, without manual curation. We evaluate our approach by
automatically replicating three manually created species-trait matrices. Our
method managed to find values for over half of all species-trait pairs, with an
F1-score of over 75%. Our results suggest that large-scale creation of
structured trait databases from unstructured online text is currently feasible
thanks to the information extraction capabilities of LLMs, being limited by the
availability of textual descriptions covering all the traits of interest.",2024-09-23,"Diego Marcos, Robert van de Vlasakker, Ioannis N. Athanasiadis, Pierre Bonnet, Hervé Goeau, Alexis Joly, W. Daniel Kissling, César Leblanc, André S. J. van Proosdij, Konstantinos P. Panousis",http://arxiv.org/pdf/2409.17179v2,cs.LG
Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping,"Given the popularity of generative AI, Large Language Models (LLMs) often
consume hundreds or thousands of GPUs for parallelizing and accelerating the
training process. Communication overhead becomes more pronounced when training
LLMs at scale. To eliminate communication overhead in distributed LLM training,
we propose Domino, which provides a generic scheme to hide communication behind
computation. By breaking data dependency of a single batch training into
smaller independent pieces, Domino pipelines these independent pieces training
and provides generic strategy of fine-grained communication and computation
overlapping. Extensive results show that, comparing with Megatron-LM, Domino
achieves up to 1.3x speedup for LLM training on Nvidia DGX-H100 GPUs.",2024-09-23,"Guanhua Wang, Chengming Zhang, Zheyu Shen, Ang Li, Olatunji Ruwase",http://arxiv.org/pdf/2409.15241v1,cs.LG
A Comprehensive Framework for Evaluating API-oriented Code Generation in Large Language Models,"Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as
powerful tools for code generation, significantly enhancing productivity and
accelerating software development. However, existing benchmarks primarily focus
on general code generation without considering API-oriented code generation,
i.e., generating code that invokes APIs from specific libraries. Given the
growing demand for API-oriented code generation, there is a pressing need for a
systematic and automated approach to evaluate LLM on API-oriented code
generation. To address this gap, we propose AutoAPIEval, a lightweight and
automated framework designed to evaluate the capabilities of LLMs in
API-oriented code generation. Our framework works with any library that
provides API documentation and focuses on two unit tasks: API recommendation
and code example generation, along with four metrics to evaluate the generated
APIs and code examples, such as the proportion of incorrect API recommendations
for Task 1, and the proportion of code examples where no specific API is
invoked and uncompilable/unexecutable code examples for Task 2. In addition, we
conducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)
and Java Runtime Environment 8 to demonstrate the framework's effectiveness.
Our findings reveal substantial variability in LLM performance across tasks,
with ChatGPT adhering better to instructions, while sharing similar
effectiveness in code example generation with its counterparts (i.e., MagiCoder
and DeekSeek Coder). We also identify key factors associated with code quality,
such as API popularity and model confidence, and build classifiers that achieve
high accuracy in detecting incorrect API recommendations and erroneous code
examples. Retrieval-augmented generation enhances the quality of code generated
by LLMs, though its effectiveness varies across different LLMs.",2024-09-23,"Yixi Wu, Pengfei He, Zehao Wang, Shaowei Wang, Yuan Tian, Tse-Hsun Chen",http://arxiv.org/pdf/2409.15228v3,cs.LG
Intelligent Routing Algorithm over SDN: Reusable Reinforcement Learning Approach,"Traffic routing is vital for the proper functioning of the Internet. As users
and network traffic increase, researchers try to develop adaptive and
intelligent routing algorithms that can fulfill various QoS requirements.
Reinforcement Learning (RL) based routing algorithms have shown better
performance than traditional approaches. We developed a QoS-aware, reusable RL
routing algorithm, RLSR-Routing over SDN. During the learning process, our
algorithm ensures loop-free path exploration. While finding the path for one
traffic demand (a source destination pair with certain amount of traffic),
RLSR-Routing learns the overall network QoS status, which can be used to speed
up algorithm convergence when finding the path for other traffic demands. By
adapting Segment Routing, our algorithm can achieve flow-based, source packet
routing, and reduce communications required between SDN controller and network
plane. Our algorithm shows better performance in terms of load balancing than
the traditional approaches. It also has faster convergence than the
non-reusable RL approach when finding paths for multiple traffic demands.",2024-09-23,"Wang Wumian, Sajal Saha, Anwar Haque, Greg Sidebottom",http://arxiv.org/pdf/2409.15226v1,cs.LG
Enhancing Pedestrian Trajectory Prediction with Crowd Trip Information,"Pedestrian trajectory prediction is essential for various applications in
active traffic management, urban planning, traffic control, crowd management,
and autonomous driving, aiming to enhance traffic safety and efficiency.
Accurately predicting pedestrian trajectories requires a deep understanding of
individual behaviors, social interactions, and road environments. Existing
studies have developed various models to capture the influence of social
interactions and road conditions on pedestrian trajectories. However, these
approaches are limited by the lack of a comprehensive view of social
interactions and road environments. To address these limitations and enhance
the accuracy of pedestrian trajectory prediction, we propose a novel approach
incorporating trip information as a new modality into pedestrian trajectory
models. We propose RNTransformer, a generic model that utilizes crowd trip
information to capture global information on social interactions. We
incorporated RNTransformer with various socially aware local pedestrian
trajectory prediction models to demonstrate its performance. Specifically, by
leveraging a pre-trained RNTransformer when training different pedestrian
trajectory prediction models, we observed improvements in performance metrics:
a 1.3/2.2% enhancement in ADE/FDE on Social-LSTM, a 6.5/28.4% improvement on
Social-STGCNN, and an 8.6/4.3% improvement on S-Implicit. Evaluation results
demonstrate that RNTransformer significantly enhances the accuracy of various
pedestrian trajectory prediction models across multiple datasets. Further
investigation reveals that the RNTransformer effectively guides local models to
more accurate directions due to the consideration of global information. By
exploring crowd behavior within the road network, our approach shows great
promise in improving pedestrian safety through accurate trajectory predictions.",2024-09-23,"Rei Tamaru, Pei Li, Bin Ran",http://arxiv.org/pdf/2409.15224v1,cs.LG
Dumpling GNN: Hybrid GNN Enables Better ADC Payload Activity Prediction Based on Chemical Structure,"Antibody-drug conjugates (ADCs) have emerged as a promising class of targeted
cancer therapeutics, but the design and optimization of their cytotoxic
payloads remain challenging. This study introduces DumplingGNN, a novel hybrid
Graph Neural Network architecture specifically designed for predicting ADC
payload activity based on chemical structure. By integrating Message Passing
Neural Networks (MPNN), Graph Attention Networks (GAT), and GraphSAGE layers,
DumplingGNN effectively captures multi-scale molecular features and leverages
both 2D topological and 3D structural information. We evaluate DumplingGNN on a
comprehensive ADC payload dataset focusing on DNA Topoisomerase I inhibitors,
as well as on multiple public benchmarks from MoleculeNet. DumplingGNN achieves
state-of-the-art performance across several datasets, including BBBP (96.4\%
ROC-AUC), ToxCast (78.2\% ROC-AUC), and PCBA (88.87\% ROC-AUC). On our
specialized ADC payload dataset, it demonstrates exceptional accuracy
(91.48\%), sensitivity (95.08\%), and specificity (97.54\%). Ablation studies
confirm the synergistic effects of the hybrid architecture and the critical
role of 3D structural information in enhancing predictive accuracy. The model's
strong interpretability, enabled by attention mechanisms, provides valuable
insights into structure-activity relationships. DumplingGNN represents a
significant advancement in molecular property prediction, with particular
promise for accelerating the design and optimization of ADC payloads in
targeted cancer therapy development.",2024-09-23,"Shengjie Xu, Lingxi Xie",http://arxiv.org/pdf/2410.05278v1,cs.LG
MotifDisco: Motif Causal Discovery For Time Series Motifs,"Many time series, particularly health data streams, can be best understood as
a sequence of phenomenon or events, which we call \textit{motifs}. A time
series motif is a short trace segment which may implicitly capture an
underlying phenomenon within the time series. Specifically, we focus on glucose
traces collected from continuous glucose monitors (CGMs), which inherently
contain motifs representing underlying human behaviors such as eating and
exercise. The ability to identify and quantify \textit{causal} relationships
amongst motifs can provide a mechanism to better understand and represent these
patterns, useful for improving deep learning and generative models and for
advanced technology development (e.g., personalized coaching and artificial
insulin delivery systems). However, no previous work has developed causal
discovery methods for time series motifs. Therefore, in this paper we develop
MotifDisco (\textbf{motif} \textbf{disco}very of causality), a novel causal
discovery framework to learn causal relations amongst motifs from time series
traces. We formalize a notion of \textit{Motif Causality (MC)}, inspired from
Granger Causality and Transfer Entropy, and develop a Graph Neural
Network-based framework that learns causality between motifs by solving an
unsupervised link prediction problem. We integrate MC with three model use
cases of forecasting, anomaly detection and clustering, to showcase the use of
MC as a building block for downstream tasks. Finally, we evaluate our framework
on different health data streams and find that Motif Causality provides a
significant performance improvement in all use cases.",2024-09-23,"Josephine Lamp, Mark Derdzinski, Christopher Hannemann, Sam Hatfield, Joost van der Linden",http://arxiv.org/pdf/2409.15219v2,cs.LG
FLeNS: Federated Learning with Enhanced Nesterov-Newton Sketch,"Federated learning faces a critical challenge in balancing communication
efficiency with rapid convergence, especially for second-order methods. While
Newton-type algorithms achieve linear convergence in communication rounds,
transmitting full Hessian matrices is often impractical due to quadratic
complexity. We introduce Federated Learning with Enhanced Nesterov-Newton
Sketch (FLeNS), a novel method that harnesses both the acceleration
capabilities of Nesterov's method and the dimensionality reduction benefits of
Hessian sketching. FLeNS approximates the centralized Newton's method without
relying on the exact Hessian, significantly reducing communication overhead. By
combining Nesterov's acceleration with adaptive Hessian sketching, FLeNS
preserves crucial second-order information while preserving the rapid
convergence characteristics. Our theoretical analysis, grounded in statistical
learning, demonstrates that FLeNS achieves super-linear convergence rates in
communication rounds - a notable advancement in federated optimization. We
provide rigorous convergence guarantees and characterize tradeoffs between
acceleration, sketch size, and convergence speed. Extensive empirical
evaluation validates our theoretical findings, showcasing FLeNS's
state-of-the-art performance with reduced communication requirements,
particularly in privacy-sensitive and edge-computing scenarios. The code is
available at https://github.com/sunnyinAI/FLeNS",2024-09-23,"Sunny Gupta, Mohit Jindal, Pankhi Kashyap, Pranav Jeevan, Amit Sethi",http://arxiv.org/pdf/2409.15216v2,cs.LG
HydroVision: LiDAR-Guided Hydrometric Prediction with Vision Transformers and Hybrid Graph Learning,"Hydrometric forecasting is crucial for managing water resources, flood
prediction, and environmental protection. Water stations are interconnected,
and this connectivity influences the measurements at other stations. However,
the dynamic and implicit nature of water flow paths makes it challenging to
extract a priori knowledge of the connectivity structure. We hypothesize that
terrain elevation significantly affects flow and connectivity. To incorporate
this, we use LiDAR terrain elevation data encoded through a Vision Transformer
(ViT). The ViT, which has demonstrated excellent performance in image
classification by directly applying transformers to sequences of image patches,
efficiently captures spatial features of terrain elevation. To account for both
spatial and temporal features, we employ GRU blocks enhanced with graph
convolution, a method widely used in the literature. We propose a hybrid graph
learning structure that combines static and dynamic graph learning. A static
graph, derived from transformer-encoded LiDAR data, captures terrain elevation
relationships, while a dynamic graph adapts to temporal changes, improving the
overall graph representation. We apply graph convolution in two layers through
these static and dynamic graphs. Our method makes daily predictions up to 12
days ahead. Empirical results from multiple water stations in Quebec
demonstrate that our method significantly reduces prediction error by an
average of 10\% across all days, with greater improvements for longer
forecasting horizons.",2024-09-23,"Naghmeh Shafiee Roudbari, Ursula Eicker, Charalambos Poullis, Zachary Patterson",http://arxiv.org/pdf/2409.15213v1,cs.LG
Fast and Accurate Triangle Counting in Graph Streams Using Predictions,"In this work, we present the first efficient and practical algorithm for
estimating the number of triangles in a graph stream using predictions. Our
algorithm combines waiting room sampling and reservoir sampling with a
predictor for the heaviness of edges, that is, the number of triangles in which
an edge is involved. As a result, our algorithm is fast, provides guarantees on
the amount of memory used, and exploits the additional information provided by
the predictor to produce highly accurate estimates. We also propose a simple
and domain-independent predictor, based on the degree of nodes, that can be
easily computed with one pass on a stream of edges when the stream is available
beforehand.
  Our analytical results show that, when the predictor provides useful
information on the heaviness of edges, it leads to estimates with reduced
variance compared to the state-of-the-art, even when the predictions are far
from perfect. Our experimental results show that, when analyzing a single graph
stream, our algorithm is faster than the state-of-the-art for a given memory
budget, while providing significantly more accurate estimates. Even more
interestingly, when sequences of hundreds of graph streams are analyzed, our
algorithm significantly outperforms the state-of-the-art using our simple
degree-based predictor built by analyzing only the first graph of the sequence.",2024-09-23,"Cristian Boldrin, Fabio Vandin",http://arxiv.org/pdf/2409.15205v1,cs.LG
RAMBO: Enhancing RAG-based Repository-Level Method Body Completion,"Code completion is essential in software development, helping developers by
predicting code snippets based on context. Among completion tasks, Method Body
Completion (MBC) is particularly challenging as it involves generating complete
method bodies based on their signatures and context. This task becomes
significantly harder in large repositories, where method bodies must integrate
repositoryspecific elements such as custom APIs, inter-module dependencies, and
project-specific conventions. In this paper, we introduce RAMBO, a novel
RAG-based approach for repository-level MBC. Instead of retrieving similar
method bodies, RAMBO identifies essential repository-specific elements, such as
classes, methods, and variables/fields, and their relevant usages. By
incorporating these elements and their relevant usages into the code generation
process, RAMBO ensures more accurate and contextually relevant method bodies.
Our experimental results with leading code LLMs across 40 Java projects show
that RAMBO significantly outperformed the state-of-the-art repository-level MBC
approaches, with the improvements of up to 46% in BLEU, 57% in CodeBLEU, 36% in
Compilation Rate, and up to 3X in Exact Match. Notably, RAMBO surpassed
RepoCoder Oracle method by up to 12% in Exact Match, setting a new benchmark
for repository-level MBC.",2024-09-23,"Tuan-Dung Bui, Duc-Thieu Luu-Van, Thanh-Phat Nguyen, Thu-Trang Nguyen, Son Nguyen, Hieu Dinh Vo",http://arxiv.org/pdf/2409.15204v2,cs.LG
ASTE Transformer Modelling Dependencies in Aspect-Sentiment Triplet Extraction,"Aspect-Sentiment Triplet Extraction (ASTE) is a recently proposed task of
aspect-based sentiment analysis that consists in extracting (aspect phrase,
opinion phrase, sentiment polarity) triples from a given sentence. Recent
state-of-the-art methods approach this task by first extracting all possible
text spans from a given text, then filtering the potential aspect and opinion
phrases with a classifier, and finally considering all their pairs with another
classifier that additionally assigns sentiment polarity to them. Although
several variations of the above scheme have been proposed, the common feature
is that the final result is constructed by a sequence of independent classifier
decisions. This hinders the exploitation of dependencies between extracted
phrases and prevents the use of knowledge about the interrelationships between
classifier predictions to improve performance. In this paper, we propose a new
ASTE approach consisting of three transformer-inspired layers, which enables
the modelling of dependencies both between phrases and between the final
classifier decisions. Experimental results show that the method achieves higher
performance in terms of F1 measure than other methods studied on popular
benchmarks. In addition, we show that a simple pre-training technique further
improves the performance of the model.",2024-09-23,"Iwo Naglik, Mateusz Lango",http://arxiv.org/pdf/2409.15202v2,cs.LG
Enabling Tensor Decomposition for Time-Series Classification via A Simple Pseudo-Laplacian Contrast,"Tensor decomposition has emerged as a prominent technique to learn
low-dimensional representation under the supervision of reconstruction error,
primarily benefiting data inference tasks like completion and imputation, but
not classification task. We argue that the non-uniqueness and rotation
invariance of tensor decomposition allow us to identify the directions with
largest class-variability and simple graph Laplacian can effectively achieve
this objective. Therefore we propose a novel Pseudo Laplacian Contrast (PLC)
tensor decomposition framework, which integrates the data augmentation and
cross-view Laplacian to enable the extraction of class-aware representations
while effectively capturing the intrinsic low-rank structure within
reconstruction constraint. An unsupervised alternative optimization algorithm
is further developed to iteratively estimate the pseudo graph and minimize the
loss using Alternating Least Square (ALS). Extensive experimental results on
various datasets demonstrate the effectiveness of our approach.",2024-09-23,"Man Li, Ziyue Li, Lijun Sun, Fugee Tsung",http://arxiv.org/pdf/2409.15200v1,cs.LG
Improving Emotion Recognition Accuracy with Personalized Clustering,"Emotion recognition through artificial intelligence and smart sensing of
physical and physiological signals (Affective Computing) is achieving very
interesting results in terms of accuracy, inference times, and user-independent
models. In this sense, there are applications related to the safety and
well-being of people (sexual aggressions, gender-based violence, children and
elderly abuse, mental health, etc.) that require even more improvements.
Emotion detection should be done with fast, discrete, and non-luxurious systems
working in real-time and real life (wearable devices, wireless communications,
battery-powered). Furthermore, emotional reactions to violence are not equal in
all people. Then, large general models cannot be applied to a multiuser system
for people protection, and customized and simple AI models would be welcomed by
health and social workers and law enforcement agents. These customized models
will be applicable to clusters of subjects sharing similarities in their
emotional reactions to external stimuli. This customization requires several
steps: creating clusters of subjects with similar behaviors, creating AI models
for every cluster, continually updating these models with new data, and
enrolling new subjects in clusters when required. A methodology for clustering
data compiled (physical and physiological data, together with emotional labels)
is presented in this work, as well as the method for including new subjects
once the AI model is generated. Experimental results demonstrate an improvement
of 4% in accuracy and 3% in f1-score w.r.t. the general model, along with a 14%
reduction in variability.",2024-09-23,"Laura Gutierrez-Martin, Celia Lopez Ongil, Jose M. Lanza-Gutierrez, Jose A. Miranda Calero",http://arxiv.org/pdf/2410.03696v1,cs.LG
Interpretability-Guided Test-Time Adversarial Defense,"We propose a novel and low-cost test-time adversarial defense by devising
interpretability-guided neuron importance ranking methods to identify neurons
important to the output classes. Our method is a training-free approach that
can significantly improve the robustness-accuracy tradeoff while incurring
minimal computational overhead. While being among the most efficient test-time
defenses (4x faster), our method is also robust to a wide range of black-box,
white-box, and adaptive attacks that break previous test-time defenses. We
demonstrate the efficacy of our method for CIFAR10, CIFAR100, and ImageNet-1k
on the standard RobustBench benchmark (with average gains of 2.6%, 4.9%, and
2.8% respectively). We also show improvements (average 1.5%) over the
state-of-the-art test-time defenses even under strong adaptive attacks.",2024-09-23,"Akshay Kulkarni, Tsui-Wei Weng",http://arxiv.org/pdf/2409.15190v1,cs.LG
Skills Made to Order: Efficient Acquisition of Robot Cooking Skills Guided by Multiple Forms of Internet Data,"This study explores the utility of various internet data sources to select
among a set of template robot behaviors to perform skills. Learning
contact-rich skills involving tool use from internet data sources has typically
been challenging due to the lack of physical information such as contact
existence, location, areas, and force in this data. Prior works have generally
used internet data and foundation models trained on this data to generate
low-level robot behavior. We hypothesize that these data and models may be
better suited to selecting among a set of basic robot behaviors to perform
these contact-rich skills. We explore three methods of template selection:
querying large language models, comparing video of robot execution to retrieved
human video using features from a pretrained video encoder common in prior
work, and performing the same comparison using features from an optic flow
encoder trained on internet data. Our results show that LLMs are surprisingly
capable template selectors despite their lack of visual information, optical
flow encoding significantly outperforms video encoders trained with an order of
magnitude more data, and important synergies exist between various forms of
internet data for template selection. By exploiting these synergies, we create
a template selector using multiple forms of internet data that achieves a 79\%
success rate on a set of 16 different cooking skills involving tool-use.",2024-09-23,"Mrinal Verghese, Christopher Atkeson",http://arxiv.org/pdf/2409.15172v1,cs.LG
Data-driven model discovery with Kolmogorov-Arnold networks,"Data-driven model discovery of complex dynamical systems is typically done
using sparse optimization, but it has a fundamental limitation: sparsity in
that the underlying governing equations of the system contain only a small
number of elementary mathematical terms. Examples where sparse optimization
fails abound, such as the classic Ikeda or optical-cavity map in nonlinear
dynamics and a large variety of ecosystems. Exploiting the recently articulated
Kolmogorov-Arnold networks, we develop a general model-discovery framework for
any dynamical systems including those that do not satisfy the sparsity
condition. In particular, we demonstrate non-uniqueness in that a large number
of approximate models of the system can be found which generate the same
invariant set with the correct statistics such as the Lyapunov exponents and
Kullback-Leibler divergence. An analogy to shadowing of numerical trajectories
in chaotic systems is pointed out.",2024-09-23,"Mohammadamin Moradi, Shirin Panahi, Erik M. Bollt, Ying-Cheng Lai",http://arxiv.org/pdf/2409.15167v1,cs.LG
Harmonic Path Integral Diffusion,"In this manuscript, we present a novel approach for sampling from a
continuous multivariate probability distribution, which may either be
explicitly known (up to a normalization factor) or represented via empirical
samples. Our method constructs a time-dependent bridge from a delta function
centered at the origin of the state space at $t=0$, optimally transforming it
into the target distribution at $t=1$. We formulate this as a Stochastic
Optimal Control problem of the Path Integral Control type, with a cost function
comprising (in its basic form) a quadratic control term, a quadratic state
term, and a terminal constraint. This framework, which we refer to as Harmonic
Path Integral Diffusion (H-PID), leverages an analytical solution through a
mapping to an auxiliary quantum harmonic oscillator in imaginary time.
  The H-PID framework results in a set of efficient sampling algorithms,
without the incorporation of Neural Networks. The algorithms are validated on
two standard use cases: a mixture of Gaussians over a grid and images from
CIFAR-10. The transparency of the method allows us to analyze the algorithms in
detail, particularly revealing that the current weighted state is an order
parameter for the dynamic phase transition, signaling earlier, at $t<1$, that
the sample generation process is almost complete. We contrast these algorithms
with other sampling methods, particularly simulated annealing and path integral
sampling, highlighting their advantages in terms of analytical control,
accuracy, and computational efficiency on benchmark problems.
  Additionally, we extend the methodology to more general cases where the
underlying stochastic differential equation includes an external deterministic,
possibly non-conservative force, and where the cost function incorporates a
gauge potential term.",2024-09-23,"Hamidreza Behjoo, Michael Chertkov",http://arxiv.org/pdf/2409.15166v2,cs.LG
A Gated Residual Kolmogorov-Arnold Networks for Mixtures of Experts,"This paper introduces KAMoE, a novel Mixture of Experts (MoE) framework based
on Gated Residual Kolmogorov-Arnold Networks (GRKAN). We propose GRKAN as an
alternative to the traditional gating function, aiming to enhance efficiency
and interpretability in MoE modeling. Through extensive experiments on digital
asset markets and real estate valuation, we demonstrate that KAMoE consistently
outperforms traditional MoE architectures across various tasks and model types.
Our results show that GRKAN exhibits superior performance compared to standard
Gating Residual Networks, particularly in LSTM-based models for sequential
tasks. We also provide insights into the trade-offs between model complexity
and performance gains in MoE and KAMoE architectures.",2024-09-23,"Hugo Inzirillo, Remi Genet",http://arxiv.org/pdf/2409.15161v2,cs.LG
Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling,"The remarkable success of large language pretraining and the discovery of
scaling laws signify a paradigm shift in machine learning. Notably, the primary
objective has evolved from minimizing generalization error to reducing
approximation error, and the most effective strategy has transitioned from
regularization (in a broad sense) to scaling up models. This raises a critical
question:
  Do the established principles that proved successful in the
generalization-centric era remain valid in this new era of scaling?
  This paper examines several influential regularization-based principles that
may no longer hold true in the scaling-centric, large language model (LLM) era.
These principles include explicit L2 regularization and implicit regularization
through small batch sizes and large learning rates. Additionally, we identify a
new phenomenon termed ``scaling law crossover,'' where two scaling curves
intersect at a certain scale, implying that methods effective at smaller scales
may not generalize to larger ones. Together, these observations highlight two
fundamental questions within this new paradigm:
  $\bullet$ Guiding Principles for Scaling: If regularization is no longer the
primary guiding principle for model design, what new principles are emerging to
guide scaling?
  $\bullet$ Model Comparison at Scale: How to reliably and effectively compare
models at the scale where only a single experiment is feasible?",2024-09-23,Lechao Xiao,http://arxiv.org/pdf/2409.15156v2,cs.LG
Micrometer: Micromechanics Transformer for Predicting Mechanical Responses of Heterogeneous Materials,"Heterogeneous materials, crucial in various engineering applications, exhibit
complex multiscale behavior, which challenges the effectiveness of traditional
computational methods. In this work, we introduce the Micromechanics
Transformer ({\em Micrometer}), an artificial intelligence (AI) framework for
predicting the mechanical response of heterogeneous materials, bridging the gap
between advanced data-driven methods and complex solid mechanics problems.
Trained on a large-scale high-resolution dataset of 2D fiber-reinforced
composites, Micrometer can achieve state-of-the-art performance in predicting
microscale strain fields across a wide range of microstructures, material
properties under any loading conditions and We demonstrate the accuracy and
computational efficiency of Micrometer through applications in computational
homogenization and multiscale modeling, where Micrometer achieves 1\% error in
predicting macroscale stress fields while reducing computational time by up to
two orders of magnitude compared to conventional numerical solvers. We further
showcase the adaptability of the proposed model through transfer learning
experiments on new materials with limited data, highlighting its potential to
tackle diverse scenarios in mechanical analysis of solid materials. Our work
represents a significant step towards AI-driven innovation in computational
solid mechanics, addressing the limitations of traditional numerical methods
and paving the way for more efficient simulations of heterogeneous materials
across various industrial applications.",2024-09-23,"Sifan Wang, Tong-Rui Liu, Shyam Sankaran, Paris Perdikaris",http://arxiv.org/pdf/2410.05281v1,cs.LG
Designing an Interpretable Interface for Contextual Bandits,"Contextual bandits have become an increasingly popular solution for
personalized recommender systems. Despite their growing use, the
interpretability of these systems remains a significant challenge, particularly
for the often non-expert operators tasked with ensuring their optimal
performance. In this paper, we address this challenge by designing a new
interface to explain to domain experts the underlying behaviour of a bandit.
Central is a metric we term ""value gain"", a measure derived from off-policy
evaluation to quantify the real-world impact of sub-components within a bandit.
We conduct a qualitative user study to evaluate the effectiveness of our
interface. Our findings suggest that by carefully balancing technical rigour
with accessible presentation, it is possible to empower non-experts to manage
complex machine learning systems. We conclude by outlining guiding principles
that other researchers should consider when building similar such interfaces in
future.",2024-09-23,"Andrew Maher, Matia Gobbo, Lancelot Lachartre, Subash Prabanantham, Rowan Swiers, Puli Liyanagama",http://arxiv.org/pdf/2409.15143v1,cs.LG
CAMAL: Optimizing LSM-trees via Active Learning,"We use machine learning to optimize LSM-tree structure, aiming to reduce the
cost of processing various read/write operations. We introduce a new approach
Camal, which boasts the following features: (1) ML-Aided: Camal is the first
attempt to apply active learning to tune LSM-tree based key-value stores. The
learning process is coupled with traditional cost models to improve the
training process; (2) Decoupled Active Learning: backed by rigorous analysis,
Camal adopts active learning paradigm based on a decoupled tuning of each
parameter, which further accelerates the learning process; (3) Easy
Extrapolation: Camal adopts an effective mechanism to incrementally update the
model with the growth of the data size; (4) Dynamic Mode: Camal is able to tune
LSM-tree online under dynamically changing workloads; (5) Significant System
Improvement: By integrating Camal into a full system RocksDB, the system
performance improves by 28% on average and up to 8x compared to a
state-of-the-art RocksDB design.",2024-09-23,"Weiping Yu, Siqiang Luo, Zihao Yu, Gao Cong",http://arxiv.org/pdf/2409.15130v1,cs.LG
The Number of Trials Matters in Infinite-Horizon General-Utility Markov Decision Processes,"The general-utility Markov decision processes (GUMDPs) framework generalizes
the MDPs framework by considering objective functions that depend on the
frequency of visitation of state-action pairs induced by a given policy. In
this work, we contribute with the first analysis on the impact of the number of
trials, i.e., the number of randomly sampled trajectories, in infinite-horizon
GUMDPs. We show that, as opposed to standard MDPs, the number of trials plays a
key-role in infinite-horizon GUMDPs and the expected performance of a given
policy depends, in general, on the number of trials. We consider both
discounted and average GUMDPs, where the objective function depends,
respectively, on discounted and average frequencies of visitation of
state-action pairs. First, we study policy evaluation under discounted GUMDPs,
proving lower and upper bounds on the mismatch between the finite and infinite
trials formulations for GUMDPs. Second, we address average GUMDPs, studying how
different classes of GUMDPs impact the mismatch between the finite and infinite
trials formulations. Third, we provide a set of empirical results to support
our claims, highlighting how the number of trajectories and the structure of
the underlying GUMDP influence policy evaluation.",2024-09-23,"Pedro P. Santos, Alberto Sardinha, Francisco S. Melo",http://arxiv.org/pdf/2409.15128v1,cs.LG
UTrace: Poisoning Forensics for Private Collaborative Learning,"Privacy-preserving machine learning (PPML) enables multiple data owners to
contribute their data privately to a set of servers that run a secure
multi-party computation (MPC) protocol to train a joint ML model. In these
protocols, the input data remains private throughout the training process, and
only the resulting model is made available. While this approach benefits
privacy, it also exacerbates the risks of data poisoning, where compromised
data owners induce undesirable model behavior by contributing malicious
datasets. Existing MPC mechanisms can mitigate certain poisoning attacks, but
these measures are not exhaustive. To complement existing poisoning defenses,
we introduce UTrace: a framework for User-level Traceback of poisoning attacks
in PPML. Utrace computes user responsibility scores using gradient similarity
metrics aggregated across the most relevant samples in an owner's dataset.
UTrace is effective at low poisoning rates and is resilient to poisoning
attacks distributed across multiple data owners, unlike existing
unlearning-based methods. We introduce methods for checkpointing gradients with
low storage overhead, enabling traceback in the absence of data owners at
deployment time. We also design several optimizations that reduce traceback
time and communication in MPC. We provide a comprehensive evaluation of UTrace
across four datasets from three data modalities (vision, text, and malware) and
show its effectiveness against 10 poisoning attacks.",2024-09-23,"Evan Rose, Hidde Lycklama, Harsh Chaudhari, Anwar Hithnawi, Alina Oprea",http://arxiv.org/pdf/2409.15126v1,cs.LG
The BRAVO Semantic Segmentation Challenge Results in UNCV2024,"We propose the unified BRAVO challenge to benchmark the reliability of
semantic segmentation models under realistic perturbations and unknown
out-of-distribution (OOD) scenarios. We define two categories of reliability:
(1) semantic reliability, which reflects the model's accuracy and calibration
when exposed to various perturbations; and (2) OOD reliability, which measures
the model's ability to detect object classes that are unknown during training.
The challenge attracted nearly 100 submissions from international teams
representing notable research institutions. The results reveal interesting
insights into the importance of large-scale pre-training and minimal
architectural design in developing robust and reliable semantic segmentation
models.",2024-09-23,"Tuan-Hung Vu, Eduardo Valle, Andrei Bursuc, Tommie Kerssies, Daan de Geus, Gijs Dubbelman, Long Qian, Bingke Zhu, Yingying Chen, Ming Tang, Jinqiao Wang, Tomáš Vojíř, Jan Šochman, Jiří Matas, Michael Smith, Frank Ferrie, Shamik Basu, Christos Sakaridis, Luc Van Gool",http://arxiv.org/pdf/2409.15107v2,cs.LG
CSPS: A Communication-Efficient Sequence-Parallelism based Serving System for Transformer based Models with Long Prompts,"Long-sequence generative large-language model (LLM) applications have become
increasingly popular. In this paper, through trace-based experiments, we found
that the existing method for long sequences results in a high
Time-To-First-Token (TTFT) due to sequential chunk processing, long
Time-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and
low throughput due to constrained key-value cache (KVC) for long sequences. To
address these issues, we propose two Sequence-Parallelism (SP) architectures
for both tensor parallelism (TP) and non-TP. However, SP introduces two
challenges: 1) network communication and computation become performance
bottlenecks; 2) the latter two issues above are mitigated but not resolved, and
SP's resultant KV value distribution across GPUs still requires communication
for decode, increasing TBT. Hence, we propose a Communication-efficient Sparse
Attention (CSA) and communication-computation-communication three-phase
pipelining. We also propose SP-based decode that processes decode separately
from prefill, distributes KV values of a request across different GPUs, and
novelly moves Query (Q) values instead of KV values to reduce communication
overhead. These methods constitute a communication-efficient
Sequence-Parallelism based LLM Serving System (SPS2). Our trace-driven
evaluation demonstrates that SPS2 improves the average TTFT, TBT, and response
time by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode
throughput by 8.2x and 5.2x while maintaining the accuracy compared to
Sarathi-Serve. We distributed our source code.",2024-09-23,"Zeyu Zhang, Haiying Shen",http://arxiv.org/pdf/2409.15104v1,cs.LG
Improving the Accessibility of Dating Websites for Individuals with Visual Impairments,"People now frequently meet and develop relationships through online dating.
Yet, due to their limited accessibility, utilizing dating services can be
difficult and irritating for people with visual impairments. The significance
of the research issue can be attributed to the fact that dating websites are
becoming more and more common and have a significant impact on how people
establish romantic connections. It can be challenging for people with visual
impairments to use dating services and develop lasting relationships because
many of them are not created with their requirements in mind. We can encourage
people with visual impairments to participate more completely in online dating
and possibly enhance the success of their romantic relationships by making
dating websites more accessible. There is some existing implementation that can
automatically recognize the facial expression, age, gender, presence of
child(ren) and other common objects from a profile photo in a dating platform.
The goal of this project is incorporate additional features (presence of any
common pets, indoor vs. outdoor image) to further enhance the capability of
existing system and come up with test viable solutions to accessibility issues
that people with visual impairments face when using dating websites.",2024-09-23,"Gyanendra Shrestha, Soumya Tejaswi Vadlamani",http://arxiv.org/pdf/2410.03695v1,cs.LG
Robust Federated Learning Over the Air: Combating Heavy-Tailed Noise with Median Anchored Clipping,"Leveraging over-the-air computations for model aggregation is an effective
approach to cope with the communication bottleneck in federated edge learning.
By exploiting the superposition properties of multi-access channels, this
approach facilitates an integrated design of communication and computation,
thereby enhancing system privacy while reducing implementation costs. However,
the inherent electromagnetic interference in radio channels often exhibits
heavy-tailed distributions, giving rise to exceptionally strong noise in
globally aggregated gradients that can significantly deteriorate the training
performance. To address this issue, we propose a novel gradient clipping
method, termed Median Anchored Clipping (MAC), to combat the detrimental
effects of heavy-tailed noise. We also derive analytical expressions for the
convergence rate of model training with analog over-the-air federated learning
under MAC, which quantitatively demonstrates the effect of MAC on training
performance. Extensive experimental results show that the proposed MAC
algorithm effectively mitigates the impact of heavy-tailed noise, hence
substantially enhancing system robustness.",2024-09-23,"Jiaxing Li, Zihan Chen, Kai Fong Ernest Chong, Bikramjit Das, Tony Q. S. Quek, Howard H. Yang",http://arxiv.org/pdf/2409.15100v5,cs.LG
Efficiently Dispatching Flash Attention For Partially Filled Attention Masks,"Transformers are widely used across various applications, many of which yield
sparse or partially filled attention matrices. Examples include attention masks
designed to reduce the quadratic complexity of attention, sequence packing
techniques, and recent innovations like tree masking for fast validation in
MEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art
algorithm Flash Attention still processes them with quadratic complexity as
though they were dense. In this paper, we introduce Binary Block Masking, a
highly efficient modification that enhances Flash Attention by making it
mask-aware. We further propose two optimizations: one tailored for masks with
contiguous non-zero patterns and another for extremely sparse masks. Our
experiments on attention masks derived from real-world scenarios demonstrate up
to a 9x runtime improvement. The implementation will be publicly released to
foster further research and application.",2024-09-23,"Agniv Sharma, Jonas Geiping",http://arxiv.org/pdf/2409.15097v2,cs.LG
AdapFair: Ensuring Continuous Fairness for Machine Learning Operations,"The biases and discrimination of machine learning algorithms have attracted
significant attention, leading to the development of various algorithms
tailored to specific contexts. However, these solutions often fall short of
addressing fairness issues inherent in machine learning operations. In this
paper, we present a debiasing framework designed to find an optimal fair
transformation of input data that maximally preserves data predictability. A
distinctive feature of our approach is its flexibility and efficiency. It can
be integrated with any downstream black-box classifiers, providing continuous
fairness guarantees with minimal retraining efforts, even in the face of
frequent data drifts, evolving fairness requirements, and batches of similar
tasks. To achieve this, we leverage the normalizing flows to enable efficient,
information-preserving data transformation, ensuring that no critical
information is lost during the debiasing process. Additionally, we incorporate
the Wasserstein distance as the unfairness measure to guide the optimization of
data transformations. Finally, we introduce an efficient optimization algorithm
with closed-formed gradient computations, making our framework scalable and
suitable for dynamic, real-world environments.",2024-09-23,"Yinghui Huang, Zihao Tang, Xiangyu Chang",http://arxiv.org/pdf/2409.15088v1,cs.LG
"Towards Accountable AI-Assisted Eye Disease Diagnosis: Workflow Design, External Validation, and Continual Learning","Timely disease diagnosis is challenging due to increasing disease burdens and
limited clinician availability. AI shows promise in diagnosis accuracy but
faces real-world application issues due to insufficient validation in clinical
workflows and diverse populations. This study addresses gaps in medical AI
downstream accountability through a case study on age-related macular
degeneration (AMD) diagnosis and severity classification. We designed and
implemented an AI-assisted diagnostic workflow for AMD, comparing diagnostic
performance with and without AI assistance among 24 clinicians from 12
institutions with real patient data sampled from the Age-Related Eye Disease
Study (AREDS). Additionally, we demonstrated continual enhancement of an
existing AI model by incorporating approximately 40,000 additional medical
images (named AREDS2 dataset). The improved model was then systematically
evaluated using both AREDS and AREDS2 test sets, as well as an external test
set from Singapore. AI assistance markedly enhanced diagnostic accuracy and
classification for 23 out of 24 clinicians, with the average F1-score
increasing by 20% from 37.71 (Manual) to 45.52 (Manual + AI) (P-value <
0.0001), achieving an improvement of over 50% in some cases. In terms of
efficiency, AI assistance reduced diagnostic times for 17 out of the 19
clinicians tracked, with time savings of up to 40%. Furthermore, a model
equipped with continual learning showed robust performance across three
independent datasets, recording a 29% increase in accuracy, and elevating the
F1-score from 42 to 54 in the Singapore population.",2024-09-23,"Qingyu Chen, Tiarnan D L Keenan, Elvira Agron, Alexis Allot, Emily Guan, Bryant Duong, Amr Elsawy, Benjamin Hou, Cancan Xue, Sanjeeb Bhandari, Geoffrey Broadhead, Chantal Cousineau-Krieger, Ellen Davis, William G Gensheimer, David Grasic, Seema Gupta, Luis Haddock, Eleni Konstantinou, Tania Lamba, Michele Maiberger, Dimosthenis Mantopoulos, Mitul C Mehta, Ayman G Nahri, Mutaz AL-Nawaflh, Arnold Oshinsky, Brittany E Powell, Boonkit Purt, Soo Shin, Hillary Stiefel, Alisa T Thavikulwat, Keith James Wroblewski, Tham Yih Chung, Chui Ming Gemmy Cheung, Ching-Yu Cheng, Emily Y Chew, Michelle R. Hribar, Michael F. Chiang, Zhiyong Lu",http://arxiv.org/pdf/2409.15087v1,cs.LG
Evaluating the Usability of LLMs in Threat Intelligence Enrichment,"Large Language Models (LLMs) have the potential to significantly enhance
threat intelligence by automating the collection, preprocessing, and analysis
of threat data. However, the usability of these tools is critical to ensure
their effective adoption by security professionals. Despite the advanced
capabilities of LLMs, concerns about their reliability, accuracy, and potential
for generating inaccurate information persist. This study conducts a
comprehensive usability evaluation of five LLMs ChatGPT, Gemini, Cohere,
Copilot, and Meta AI focusing on their user interface design, error handling,
learning curve, performance, and integration with existing tools in threat
intelligence enrichment. Utilizing a heuristic walkthrough and a user study
methodology, we identify key usability issues and offer actionable
recommendations for improvement. Our findings aim to bridge the gap between LLM
functionality and user experience, thereby promoting more efficient and
accurate threat intelligence practices by ensuring these tools are
user-friendly and reliable.",2024-09-23,"Sanchana Srikanth, Mohammad Hasanuzzaman, Farah Tasnur Meem",http://arxiv.org/pdf/2409.15072v1,cs.LG
SHFL: Secure Hierarchical Federated Learning Framework for Edge Networks,"Federated Learning (FL) is a distributed machine learning paradigm designed
for privacy-sensitive applications that run on resource-constrained devices
with non-Identically and Independently Distributed (IID) data. Traditional FL
frameworks adopt the client-server model with a single-level aggregation (AGR)
process, where the server builds the global model by aggregating all trained
local models received from client devices. However, this conventional approach
encounters challenges, including susceptibility to model/data poisoning
attacks. In recent years, advancements in the Internet of Things (IoT) and edge
computing have enabled the development of hierarchical FL systems with a
two-level AGR process running at edge and cloud servers. In this paper, we
propose a Secure Hierarchical FL (SHFL) framework to address poisoning attacks
in hierarchical edge networks. By aggregating trained models at the edge, SHFL
employs two novel methods to address model/data poisoning attacks in the
presence of client adversaries: 1) a client selection algorithm running at the
edge for choosing IoT devices to participate in training, and 2) a model AGR
method designed based on convex optimization theory to reduce the impact of
edge models from networks with adversaries in the process of computing the
global model (at the cloud level). The evaluation results reveal that compared
to state-of-the-art methods, SHFL significantly increases the maximum accuracy
achieved by the global model in the presence of client adversaries applying
model/data poisoning attacks.",2024-09-23,"Omid Tavallaie, Kanchana Thilakarathna, Suranga Seneviratne, Aruna Seneviratne, Albert Y. Zomaya",http://arxiv.org/pdf/2409.15067v1,cs.LG
AlphaZip: Neural Network-Enhanced Lossless Text Compression,"Data compression continues to evolve, with traditional information theory
methods being widely used for compressing text, images, and videos. Recently,
there has been growing interest in leveraging Generative AI for predictive
compression techniques. This paper introduces a lossless text compression
approach using a Large Language Model (LLM). The method involves two key steps:
first, prediction using a dense neural network architecture, such as a
transformer block; second, compressing the predicted ranks with standard
compression algorithms like Adaptive Huffman, LZ77, or Gzip. Extensive analysis
and benchmarking against conventional information-theoretic baselines
demonstrate that neural compression offers improved performance.",2024-09-23,"Swathi Shree Narashiman, Nitin Chandrachoodan",http://arxiv.org/pdf/2409.15046v1,cs.LG
Anomaly Detection from a Tensor Train Perspective,"We present a series of algorithms in tensor networks for anomaly detection in
datasets, by using data compression in a Tensor Train representation. These
algorithms consist of preserving the structure of normal data in compression
and deleting the structure of anomalous data. The algorithms can be applied to
any tensor network representation. We test the effectiveness of the methods
with digits and Olivetti faces datasets and a cybersecurity dataset to
determine cyber-attacks.",2024-09-23,"Alejandro Mata Ali, Aitor Moreno Fdez. de Leceta, Jorge López Rubio",http://arxiv.org/pdf/2409.15030v1,cs.LG
Region Mixup,"This paper introduces a simple extension of mixup (Zhang et al., 2018) data
augmentation to enhance generalization in visual recognition tasks. Unlike the
vanilla mixup method, which blends entire images, our approach focuses on
combining regions from multiple images.",2024-09-23,"Saptarshi Saha, Utpal Garain",http://arxiv.org/pdf/2409.15028v1,cs.LG
A Diagonal Structured State Space Model on Loihi 2 for Efficient Streaming Sequence Processing,"Deep State-Space Models (SSM) demonstrate state-of-the art performance on
long-range sequence modeling tasks. While the recurrent structure of SSMs can
be efficiently implemented as a convolution or as a parallel scan during
training, recurrent token-by-token processing cannot currently be implemented
efficiently on GPUs. Here, we demonstrate efficient token-by-token inference of
the SSM S4D on Intel's Loihi 2 state-of-the-art neuromorphic processor. We
compare this first ever neuromorphic-hardware implementation of an SSM on
sMNIST, psMNIST, and sCIFAR to a recurrent and a convolutional implementation
of S4D on Jetson Orin Nano (Jetson). While we find Jetson to perform better in
an offline sample-by-sample based batched processing mode, Loihi 2 outperforms
during token-by-token based processing, where it consumes 1000 times less
energy with a 75 times lower latency and a 75 times higher throughput compared
to the recurrent implementation of S4D on Jetson. This opens up new avenues
towards efficient real-time streaming applications of SSMs.",2024-09-23,"Svea Marie Meyer, Philipp Weidel, Philipp Plank, Leobardo Campos-Macias, Sumit Bam Shrestha, Philipp Stratmann, Mathis Richter",http://arxiv.org/pdf/2409.15022v1,cs.LG
Evaluating Synthetic Activations composed of SAE Latents in GPT-2,"Sparse Auto-Encoders (SAEs) are commonly employed in mechanistic
interpretability to decompose the residual stream into monosemantic SAE
latents. Recent work demonstrates that perturbing a model's activations at an
early layer results in a step-function-like change in the model's final layer
activations. Furthermore, the model's sensitivity to this perturbation differs
between model-generated (real) activations and random activations. In our
study, we assess model sensitivity in order to compare real activations to
synthetic activations composed of SAE latents. Our findings indicate that
synthetic activations closely resemble real activations when we control for the
sparsity and cosine similarity of the constituent SAE latents. This suggests
that real activations cannot be explained by a simple ""bag of SAE latents""
lacking internal structure, and instead suggests that SAE latents possess
significant geometric and statistical properties. Notably, we observe that our
synthetic activations exhibit less pronounced activation plateaus compared to
those typically surrounding real activations.",2024-09-23,"Giorgi Giglemiani, Nora Petrova, Chatrik Singh Mangat, Jett Janiak, Stefan Heimersheim",http://arxiv.org/pdf/2409.15019v2,cs.LG
Acting for the Right Reasons: Creating Reason-Sensitive Artificial Moral Agents,"We propose an extension of the reinforcement learning architecture that
enables moral decision-making of reinforcement learning agents based on
normative reasons. Central to this approach is a reason-based shield generator
yielding a moral shield that binds the agent to actions that conform with
recognized normative reasons so that our overall architecture restricts the
agent to actions that are (internally) morally justified. In addition, we
describe an algorithm that allows to iteratively improve the reason-based
shield generator through case-based feedback from a moral judge.",2024-09-23,"Kevin Baum, Lisa Dargasz, Felix Jahn, Timo P. Gros, Verena Wolf",http://arxiv.org/pdf/2409.15014v2,cs.LG
Designing Pre-training Datasets from Unlabeled Data for EEG Classification with Transformers,"Transformer neural networks require a large amount of labeled data to train
effectively. Such data is often scarce in electroencephalography, as
annotations made by medical experts are costly. This is why self-supervised
training, using unlabeled data, has to be performed beforehand. In this paper,
we present a way to design several labeled datasets from unlabeled
electroencephalogram (EEG) data. These can then be used to pre-train
transformers to learn representations of EEG signals. We tested this method on
an epileptic seizure forecasting task on the Temple University Seizure
Detection Corpus using a Multi-channel Vision Transformer. Our results suggest
that 1) Models pre-trained using our approach demonstrate significantly faster
training times, reducing fine-tuning duration by more than 50% for the specific
task, and 2) Pre-trained models exhibit improved accuracy, with an increase
from 90.93% to 92.16%, as well as a higher AUC, rising from 0.9648 to 0.9702
when compared to non-pre-trained models.",2024-09-23,"Tim Bary, Benoit Macq",http://arxiv.org/pdf/2410.07190v1,cs.LG
"Methods for Convex $(L_0,L_1)$-Smooth Optimization: Clipping, Acceleration, and Adaptivity","Due to the non-smoothness of optimization problems in Machine Learning,
generalized smoothness assumptions have been gaining a lot of attention in
recent years. One of the most popular assumptions of this type is
$(L_0,L_1)$-smoothness (Zhang et al., 2020). In this paper, we focus on the
class of (strongly) convex $(L_0,L_1)$-smooth functions and derive new
convergence guarantees for several existing methods. In particular, we derive
improved convergence rates for Gradient Descent with (Smoothed) Gradient
Clipping and for Gradient Descent with Polyak Stepsizes. In contrast to the
existing results, our rates do not rely on the standard smoothness assumption
and do not suffer from the exponential dependency from the initial distance to
the solution. We also extend these results to the stochastic case under the
over-parameterization assumption, propose a new accelerated method for convex
$(L_0,L_1)$-smooth optimization, and derive new convergence rates for Adaptive
Gradient Descent (Malitsky and Mishchenko, 2020).",2024-09-23,"Eduard Gorbunov, Nazarii Tupitsa, Sayantan Choudhury, Alen Aliev, Peter Richtárik, Samuel Horváth, Martin Takáč",http://arxiv.org/pdf/2409.14989v2,cs.LG
Dynamic Integration of Task-Specific Adapters for Class Incremental Learning,"Non-exemplar class Incremental Learning (NECIL) enables models to
continuously acquire new classes without retraining from scratch and storing
old task exemplars, addressing privacy and storage issues. However, the absence
of data from earlier tasks exacerbates the challenge of catastrophic forgetting
in NECIL. In this paper, we propose a novel framework called Dynamic
Integration of task-specific Adapters (DIA), which comprises two key
components: Task-Specific Adapter Integration (TSAI) and Patch-Level Model
Alignment. TSAI boosts compositionality through a patch-level adapter
integration strategy, which provides a more flexible compositional solution
while maintaining low computation costs. Patch-Level Model Alignment maintains
feature consistency and accurate decision boundaries via two specialized
mechanisms: Patch-Level Distillation Loss (PDL) and Patch-Level Feature
Reconstruction method (PFR). Specifically, the PDL preserves feature-level
consistency between successive models by implementing a distillation loss based
on the contributions of patch tokens to new class learning. The PFR facilitates
accurate classifier alignment by reconstructing old class features from
previous tasks that adapt to new task knowledge. Extensive experiments validate
the effectiveness of our DIA, revealing significant improvements on benchmark
datasets in the NECIL setting, maintaining an optimal balance between
computational complexity and accuracy.",2024-09-23,"Jiashuo Li, Shaokun Wang, Bo Qian, Yuhang He, Xing Wei, Qiang Wang, Yihong Gong",http://arxiv.org/pdf/2409.14983v2,cs.LG
On The Specialization of Neural Modules,"A number of machine learning models have been proposed with the goal of
achieving systematic generalization: the ability to reason about new situations
by combining aspects of previous experiences. These models leverage
compositional architectures which aim to learn specialized modules dedicated to
structures in a task that can be composed to solve novel problems with similar
structures. While the compositionality of these architectures is guaranteed by
design, the modules specializing is not. Here we theoretically study the
ability of network modules to specialize to useful structures in a dataset and
achieve systematic generalization. To this end we introduce a minimal space of
datasets motivated by practical systematic generalization benchmarks. From this
space of datasets we present a mathematical definition of systematicity and
study the learning dynamics of linear neural modules when solving components of
the task. Our results shed light on the difficulty of module specialization,
what is required for modules to successfully specialize, and the necessity of
modular architectures to achieve systematicity. Finally, we confirm that the
theoretical results in our tractable setting generalize to more complex
datasets and non-linear architectures.",2024-09-23,"Devon Jarvis, Richard Klein, Benjamin Rosman, Andrew M. Saxe",http://arxiv.org/pdf/2409.14981v1,cs.LG
(De)-regularized Maximum Mean Discrepancy Gradient Flow,"We introduce a (de)-regularization of the Maximum Mean Discrepancy (DrMMD)
and its Wasserstein gradient flow. Existing gradient flows that transport
samples from source distribution to target distribution with only target
samples, either lack tractable numerical implementation ($f$-divergence flows)
or require strong assumptions, and modifications such as noise injection, to
ensure convergence (Maximum Mean Discrepancy flows). In contrast, DrMMD flow
can simultaneously (i) guarantee near-global convergence for a broad class of
targets in both continuous and discrete time, and (ii) be implemented in closed
form using only samples. The former is achieved by leveraging the connection
between the DrMMD and the $\chi^2$-divergence, while the latter comes by
treating DrMMD as MMD with a de-regularized kernel. Our numerical scheme uses
an adaptive de-regularization schedule throughout the flow to optimally trade
off between discretization errors and deviations from the $\chi^2$ regime. The
potential application of the DrMMD flow is demonstrated across several
numerical experiments, including a large-scale setting of training
student/teacher networks.",2024-09-23,"Zonghao Chen, Aratrika Mustafi, Pierre Glaser, Anna Korba, Arthur Gretton, Bharath K. Sriperumbudur",http://arxiv.org/pdf/2409.14980v1,cs.LG
Blind Spatial Impulse Response Generation from Separate Room- and Scene-Specific Information,"For audio in augmented reality (AR), knowledge of the users' real acoustic
environment is crucial for rendering virtual sounds that seamlessly blend into
the environment. As acoustic measurements are usually not feasible in practical
AR applications, information about the room needs to be inferred from available
sound sources. Then, additional sound sources can be rendered with the same
room acoustic qualities. Crucially, these are placed at different positions
than the sources available for estimation. Here, we propose to use an encoder
network trained using a contrastive loss that maps input sounds to a
low-dimensional feature space representing only room-specific information.
Then, a diffusion-based spatial room impulse response generator is trained to
take the latent space and generate a new response, given a new source-receiver
position. We show how both room- and position-specific parameters are
considered in the final output.",2024-09-23,"Francesc Lluís, Nils Meyer-Kahlen",http://arxiv.org/pdf/2409.14971v1,cs.LG
GATher: Graph Attention Based Predictions of Gene-Disease Links,"Target selection is crucial in pharmaceutical drug discovery, directly
influencing clinical trial success. Despite its importance, drug development
remains resource-intensive, often taking over a decade with significant
financial costs. High failure rates highlight the need for better early-stage
target selection. We present GATher, a graph attention network designed to
predict therapeutic gene-disease links by integrating data from diverse
biomedical sources into a graph with over 4.4 million edges. GATher
incorporates GATv3, a novel graph attention convolution layer, and
GATv3HeteroConv, which aggregates transformations for each edge type, enhancing
its ability to manage complex interactions within this extensive dataset.
Utilizing hard negative sampling and multi-task pre-training, GATher addresses
topological imbalances and improves specificity. Trained on data up to 2018 and
evaluated through 2024, our results show GATher predicts clinical trial
outcomes with a ROC AUC of 0.69 for unmet efficacy failures and 0.79 for
positive efficacy. Feature attribution methods, using Captum, highlight key
nodes and relationships, enhancing model interpretability. By 2024, GATher
improved precision in prioritizing the top 200 clinical trial targets to 14.1%,
an absolute increase of over 3.5% compared to other methods. GATher outperforms
existing models like GAT, GATv2, and HGT in predicting clinical trial outcomes,
demonstrating its potential in enhancing target validation and predicting
clinical efficacy and safety.",2024-09-23,"David Narganes-Carlon, Anniek Myatt, Mani Mudaliar, Daniel J. Crowther",http://arxiv.org/pdf/2409.16327v1,cs.LG
Adaptive Learning on User Segmentation: Universal to Specific Representation via Bipartite Neural Interaction,"Recently, models for user representation learning have been widely applied in
click-through-rate (CTR) and conversion-rate (CVR) prediction. Usually, the
model learns a universal user representation as the input for subsequent
scenario-specific models. However, in numerous industrial applications (e.g.,
recommendation and marketing), the business always operates such applications
as various online activities among different user segmentation. These
segmentation are always created by domain experts. Due to the difference in
user distribution (i.e., user segmentation) and business objectives in
subsequent tasks, learning solely on universal representation may lead to
detrimental effects on both model performance and robustness. In this paper, we
propose a novel learning framework that can first learn general universal user
representation through information bottleneck. Then, merge and learn a
segmentation-specific or a task-specific representation through neural
interaction. We design the interactive learning process by leveraging a
bipartite graph architecture to model the representation learning and merging
between contextual clusters and each user segmentation. Our proposed method is
evaluated in two open-source benchmarks, two offline business datasets, and
deployed on two online marketing applications to predict users' CVR. The
results demonstrate that our method can achieve superior performance and
surpass the baseline methods.",2024-09-23,"Xiaoyu Tan, Yongxin Deng, Chao Qu, Siqiao Xue, Xiaoming Shi, James Zhang, Xihe Qiu",http://arxiv.org/pdf/2409.14945v1,cs.LG
FastGL: A GPU-Efficient Framework for Accelerating Sampling-Based GNN Training at Large Scale,"Graph Neural Networks (GNNs) have shown great superiority on non-Euclidean
graph data, achieving ground-breaking performance on various graph-related
tasks. As a practical solution to train GNN on large graphs with billions of
nodes and edges, the sampling-based training is widely adopted by existing
training frameworks. However, through an in-depth analysis, we observe that the
efficiency of existing sampling-based training frameworks is still limited due
to the key bottlenecks lying in all three phases of sampling-based training,
i.e., subgraph sample, memory IO, and computation. To this end, we propose
FastGL, a GPU-efficient Framework for accelerating sampling-based training of
GNN at Large scale by simultaneously optimizing all above three phases, taking
into account both GPU characteristics and graph structure. Specifically, by
exploiting the inherent overlap within graph structures, FastGL develops the
Match-Reorder strategy to reduce the data traffic, which accelerates the memory
IO without incurring any GPU memory overhead. Additionally, FastGL leverages a
Memory-Aware computation method, harnessing the GPU memory's hierarchical
nature to mitigate irregular data access during computation. FastGL further
incorporates the Fused-Map approach aimed at diminishing the synchronization
overhead during sampling. Extensive experiments demonstrate that FastGL can
achieve an average speedup of 11.8x, 2.2x and 1.5x over the state-of-the-art
frameworks PyG, DGL, and GNNLab, respectively.Our code is available at
https://github.com/a1bc2def6g/fastgl-ae.",2024-09-23,"Zeyu Zhu, Peisong Wang, Qinghao Hu, Gang Li, Xiaoyao Liang, Jian Cheng",http://arxiv.org/pdf/2409.14939v1,cs.LG
Neural Differential Appearance Equations,"We propose a method to reproduce dynamic appearance textures with
space-stationary but time-varying visual statistics. While most previous work
decomposes dynamic textures into static appearance and motion, we focus on
dynamic appearance that results not from motion but variations of fundamental
properties, such as rusting, decaying, melting, and weathering. To this end, we
adopt the neural ordinary differential equation (ODE) to learn the underlying
dynamics of appearance from a target exemplar. We simulate the ODE in two
phases. At the ""warm-up"" phase, the ODE diffuses a random noise to an initial
state. We then constrain the further evolution of this ODE to replicate the
evolution of visual feature statistics in the exemplar during the generation
phase. The particular innovation of this work is the neural ODE achieving both
denoising and evolution for dynamics synthesis, with a proposed temporal
training scheme. We study both relightable (BRDF) and non-relightable (RGB)
appearance models. For both we introduce new pilot datasets, allowing, for the
first time, to study such phenomena: For RGB we provide 22 dynamic textures
acquired from free online sources; For BRDFs, we further acquire a dataset of
21 flash-lit videos of time-varying materials, enabled by a simple-to-construct
setup. Our experiments show that our method consistently yields realistic and
coherent results, whereas prior works falter under pronounced temporal
appearance variations. A user study confirms our approach is preferred to
previous work for such exemplars.",2024-09-23,"Chen Liu, Tobias Ritschel",http://arxiv.org/pdf/2410.07128v2,cs.LG
A Realistic Simulation Framework for Analog/Digital Neuromorphic Architectures,"Developing dedicated mixed-signal neuromorphic computing systems optimized
for real-time sensory-processing in extreme edge-computing applications
requires time-consuming design, fabrication, and deployment of full-custom
neuromorphic processors. To ensure that initial prototyping efforts, exploring
the properties of different network architectures and parameter settings, lead
to realistic results, it is important to use simulation frameworks that match
as best as possible the properties of the final hardware. This is particularly
challenging for neuromorphic hardware platforms made using mixed-signal
analog/digital circuits, due to the variability and noise sensitivity of their
components. In this paper, we address this challenge by developing a software
spiking neural network simulator explicitly designed to account for the
properties of mixed-signal neuromorphic circuits, including device mismatch
variability.
  The simulator, called ARCANA (A Realistic Simulation Framework for
Analog/Digital Neuromorphic Architectures), is designed to reproduce the
dynamics of mixed-signal synapse and neuron electronic circuits with
autogradient differentiation for parameter optimization and GPU acceleration.
We demonstrate the effectiveness of this approach by matching software
simulation results with measurements made from an existing neuromorphic
processor. We show how the results obtained provide a reliable estimate of the
behavior of the spiking neural network trained in software, once deployed in
hardware. This framework enables the development and innovation of new learning
rules and processing architectures in neuromorphic embedded systems.",2024-09-23,"Fernando M. Quintana, Maryada, Pedro L. Galindo, Elisa Donati, Giacomo Indiveri, Fernando Perez-Peña",http://arxiv.org/pdf/2409.14918v2,cs.LG
Towards a Realistic Long-Term Benchmark for Open-Web Research Agents,"We present initial results of a forthcoming benchmark for evaluating LLM
agents on white-collar tasks of economic value. We evaluate agents on
real-world ""messy"" open-web research tasks of the type that are routine in
finance and consulting. In doing so, we lay the groundwork for an LLM agent
evaluation suite where good performance directly corresponds to a large
economic and societal impact. We built and tested several agent architectures
with o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.
On average, LLM agents powered by Claude-3.5 Sonnet and o1-preview
substantially outperformed agents using GPT-4o, with agents based on Llama 3.1
(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct
architecture with the ability to delegate subtasks to subagents performed best.
In addition to quantitative evaluations, we qualitatively assessed the
performance of the LLM agents by inspecting their traces and reflecting on
their observations. Our evaluation represents the first in-depth assessment of
agents' abilities to conduct challenging, economically valuable analyst-style
research on the real open web.",2024-09-23,"Peter Mühlbacher, Nikos I. Bosse, Lawrence Phillips",http://arxiv.org/pdf/2409.14913v2,cs.LG
Efficient Tabular Data Preprocessing of ML Pipelines,"Data preprocessing pipelines, which includes data decoding, cleaning, and
transforming, are a crucial component of Machine Learning (ML) training. Thy
are computationally intensive and often become a major bottleneck, due to the
increasing performance gap between the CPUs used for preprocessing and the GPUs
used for model training. Recent studies show that a significant number of CPUs
across several machines are required to achieve sufficient throughput to
saturate the GPUs, leading to increased resource and energy consumption. When
the pipeline involves vocabulary generation, the preprocessing performance
scales poorly due to significant row-wise synchronization overhead between
different CPU cores and servers. To address this limitation, in this paper we
present the design of Piper, a hardware accelerator for tabular data
preprocessing, prototype it on FPGAs, and demonstrate its potential for
training pipelines of commercial recommender systems. Piper achieves 4.7 $\sim$
71.3$\times$ speedup in latency over a 128-core CPU server and outperforms a
data-center GPU by 4.8$\sim$ 20.3$\times$ when using binary input. The
impressive performance showcases Piper's potential to increase the efficiency
of data preprocessing pipelines and significantly reduce their resource
consumption.",2024-09-23,"Yu Zhu, Wenqi Jiang, Gustavo Alonso",http://arxiv.org/pdf/2409.14912v1,cs.LG
Kriformer: A Novel Spatiotemporal Kriging Approach Based on Graph Transformers,"Accurately estimating data in sensor-less areas is crucial for understanding
system dynamics, such as traffic state estimation and environmental monitoring.
This study addresses challenges posed by sparse sensor deployment and
unreliable data by framing the problem as a spatiotemporal kriging task and
proposing a novel graph transformer model, Kriformer. This model estimates data
at locations without sensors by mining spatial and temporal correlations, even
with limited resources. Kriformer utilizes transformer architecture to enhance
the model's perceptual range and solve edge information aggregation challenges,
capturing spatiotemporal information effectively. A carefully constructed
positional encoding module embeds the spatiotemporal features of nodes, while a
sophisticated spatiotemporal attention mechanism enhances estimation accuracy.
The multi-head spatial interaction attention module captures subtle spatial
relationships between observed and unobserved locations. During training, a
random masking strategy prompts the model to learn with partial information
loss, allowing the spatiotemporal embedding and multi-head attention mechanisms
to synergistically capture correlations among locations. Experimental results
show that Kriformer excels in representation learning for unobserved locations,
validated on two real-world traffic speed datasets, demonstrating its
effectiveness in spatiotemporal kriging tasks.",2024-09-23,"Renbin Pan, Feng Xiao, Hegui Zhang, Minyu Shen",http://arxiv.org/pdf/2409.14906v1,cs.LG
Dual Stream Graph Transformer Fusion Networks for Enhanced Brain Decoding,"This paper presents the novel Dual Stream Graph-Transformer Fusion (DS-GTF)
architecture designed specifically for classifying task-based
Magnetoencephalography (MEG) data. In the spatial stream, inputs are initially
represented as graphs, which are then passed through graph attention networks
(GAT) to extract spatial patterns. Two methods, TopK and Thresholded Adjacency
are introduced for initializing the adjacency matrix used in the GAT. In the
temporal stream, the Transformer Encoder receives concatenated windowed input
MEG data and learns new temporal representations. The learned temporal and
spatial representations from both streams are fused before reaching the output
layer. Experimental results demonstrate an enhancement in classification
performance and a reduction in standard deviation across multiple test subjects
compared to other examined models.",2024-09-23,"Lucas Goene, Siamak Mehrkanoon",http://arxiv.org/pdf/2410.07189v1,cs.LG
CON: Continual Object Navigation via Data-Free Inter-Agent Knowledge Transfer in Unseen and Unfamiliar Places,"This work explores the potential of brief inter-agent knowledge transfer (KT)
to enhance the robotic object goal navigation (ON) in unseen and unfamiliar
environments. Drawing on the analogy of human travelers acquiring local
knowledge, we propose a framework in which a traveler robot (student)
communicates with local robots (teachers) to obtain ON knowledge through
minimal interactions. We frame this process as a data-free continual learning
(CL) challenge, aiming to transfer knowledge from a black-box model (teacher)
to a new model (student). In contrast to approaches like zero-shot ON using
large language models (LLMs), which utilize inherently communication-friendly
natural language for knowledge representation, the other two major ON
approaches -- frontier-driven methods using object feature maps and
learning-based ON using neural state-action maps -- present complex challenges
where data-free KT remains largely uncharted. To address this gap, we propose a
lightweight, plug-and-play KT module targeting non-cooperative black-box
teachers in open-world settings. Using the universal assumption that every
teacher robot has vision and mobility capabilities, we define state-action
history as the primary knowledge base. Our formulation leads to the development
of a query-based occupancy map that dynamically represents target object
locations, serving as an effective and communication-friendly knowledge
representation. We validate the effectiveness of our method through experiments
conducted in the Habitat environment.",2024-09-23,"Kouki Terashima, Daiki Iwata, Kanji Tanaka",http://arxiv.org/pdf/2409.14899v1,cs.LG
Built Different: Tactile Perception to Overcome Cross-Embodiment Capability Differences in Collaborative Manipulation,"Tactile sensing is a powerful means of implicit communication between a human
and a robot assistant. In this paper, we investigate how tactile sensing can
transcend cross-embodiment differences across robotic systems in the context of
collaborative manipulation. Consider tasks such as collaborative object
carrying where the human-robot interaction is force rich. Learning and
executing such skills requires the robot to comply to the human and to learn
behaviors at the joint-torque level. However, most robots do not offer this
compliance or provide access to their joint torques. To address this challenge,
we present an approach that uses tactile sensors to transfer policies from
robots with these capabilities to those without. We show how our method can
enable a cooperative task where a robot and human must work together to
maneuver objects through space. We first demonstrate the skill on an impedance
control-capable robot equipped with tactile sensing, then show the positive
transfer of the tactile policy to a planar prismatic robot that is only capable
of position control and does not come equipped with any sort of force/torque
feedback, yet is able to comply to the human motions only using tactile
feedback. Further details and videos can be found on our project website at
https://www.mmintlab.com/research/tactile-collaborative/.",2024-09-23,"William van den Bogert, Madhavan Iyengar, Nima Fazeli",http://arxiv.org/pdf/2409.14896v1,cs.LG
Novel Gradient Sparsification Algorithm via Bayesian Inference,"Error accumulation is an essential component of the Top-$k$ sparsification
method in distributed gradient descent. It implicitly scales the learning rate
and prevents the slow-down of lateral movement, but it can also deteriorate
convergence. This paper proposes a novel sparsification algorithm called
regularized Top-$k$ (RegTop-$k$) that controls the learning rate scaling of
error accumulation. The algorithm is developed by looking at the gradient
sparsification as an inference problem and determining a Bayesian optimal
sparsification mask via maximum-a-posteriori estimation. It utilizes past
aggregated gradients to evaluate posterior statistics, based on which it
prioritizes the local gradient entries. Numerical experiments with ResNet-18 on
CIFAR-10 show that at $0.1\%$ sparsification, RegTop-$k$ achieves about $8\%$
higher accuracy than standard Top-$k$.",2024-09-23,"Ali Bereyhi, Ben Liang, Gary Boudreau, Ali Afana",http://arxiv.org/pdf/2409.14893v1,cs.LG
Deploying Open-Source Large Language Models: A performance Analysis,"Since the release of ChatGPT in November 2022, large language models (LLMs)
have seen considerable success, including in the open-source community, with
many open-weight models available. However, the requirements to deploy such a
service are often unknown and difficult to evaluate in advance. To facilitate
this process, we conducted numerous tests at the Centre Inria de l'Universit\'e
de Bordeaux. In this article, we propose a comparison of the performance of
several models of different sizes (mainly Mistral and LLaMa) depending on the
available GPUs, using vLLM, a Python library designed to optimize the inference
of these models. Our results provide valuable information for private and
public groups wishing to deploy LLMs, allowing them to evaluate the performance
of different models based on their available hardware. This study thus
contributes to facilitating the adoption and use of these large language models
in various application domains.",2024-09-23,"Yannis Bendi-Ouis, Dan Dutartre, Xavier Hinaut",http://arxiv.org/pdf/2409.14887v3,cs.LG
Attack Atlas: A Practitioner's Perspective on Challenges and Pitfalls in Red Teaming GenAI,"As generative AI, particularly large language models (LLMs), become
increasingly integrated into production applications, new attack surfaces and
vulnerabilities emerge and put a focus on adversarial threats in natural
language and multi-modal systems. Red-teaming has gained importance in
proactively identifying weaknesses in these systems, while blue-teaming works
to protect against such adversarial attacks. Despite growing academic interest
in adversarial risks for generative AI, there is limited guidance tailored for
practitioners to assess and mitigate these challenges in real-world
environments. To address this, our contributions include: (1) a practical
examination of red- and blue-teaming strategies for securing generative AI, (2)
identification of key challenges and open questions in defense development and
evaluation, and (3) the Attack Atlas, an intuitive framework that brings a
practical approach to analyzing single-turn input attacks, placing it at the
forefront for practitioners. This work aims to bridge the gap between academic
insights and practical security measures for the protection of generative AI
systems.",2024-09-23,"Ambrish Rawat, Stefan Schoepf, Giulio Zizzo, Giandomenico Cornacchia, Muhammad Zaid Hameed, Kieran Fraser, Erik Miehling, Beat Buesser, Elizabeth M. Daly, Mark Purcell, Prasanna Sattigeri, Pin-Yu Chen, Kush R. Varshney",http://arxiv.org/pdf/2409.15398v1,cs.LG
The ParlaSpeech Collection of Automatically Generated Speech and Text Datasets from Parliamentary Proceedings,"Recent significant improvements in speech and language technologies come both
from self-supervised approaches over raw language data as well as various types
of explicit supervision. To ensure high-quality processing of spoken data, the
most useful type of explicit supervision is still the alignment between the
speech signal and its corresponding text transcript, which is a data type that
is not available for many languages. In this paper, we present our approach to
building large and open speech-and-text-aligned datasets of less-resourced
languages based on transcripts of parliamentary proceedings and their
recordings. Our starting point are the ParlaMint comparable corpora of
transcripts of parliamentary proceedings of 26 national European parliaments.
In the pilot run on expanding the ParlaMint corpora with aligned publicly
available recordings, we focus on three Slavic languages, namely Croatian,
Polish, and Serbian. The main challenge of our approach is the lack of any
global alignment between the ParlaMint texts and the available recordings, as
well as the sometimes varying data order in each of the modalities, which
requires a novel approach in aligning long sequences of text and audio in a
large search space. The results of this pilot run are three high-quality
datasets that span more than 5,000 hours of speech and accompanying text
transcripts. Although these datasets already make a huge difference in the
availability of spoken and textual data for the three languages, we want to
emphasize the potential of the presented approach in building similar datasets
for many more languages.",2024-09-23,"Nikola Ljubešić, Peter Rupnik, Danijel Koržinek",http://arxiv.org/pdf/2409.15397v2,cs.LG
Towards Ground-truth-free Evaluation of Any Segmentation in Medical Images,"We explore the feasibility and potential of building a ground-truth-free
evaluation model to assess the quality of segmentations generated by the
Segment Anything Model (SAM) and its variants in medical imaging. This
evaluation model estimates segmentation quality scores by analyzing the
coherence and consistency between the input images and their corresponding
segmentation predictions. Based on prior research, we frame the task of
training this model as a regression problem within a supervised learning
framework, using Dice scores (and optionally other metrics) along with mean
squared error to compute the training loss. The model is trained utilizing a
large collection of public datasets of medical images with segmentation
predictions from SAM and its variants. We name this model EvanySeg (Evaluation
of Any Segmentation in Medical Images). Our exploration of convolution-based
models (e.g., ResNet) and transformer-based models (e.g., ViT) suggested that
ViT yields better performance for this task. EvanySeg can be employed for
various tasks, including: (1) identifying poorly segmented samples by detecting
low-percentile segmentation quality scores; (2) benchmarking segmentation
models without ground truth by averaging quality scores across test samples;
(3) alerting human experts to poor-quality segmentation predictions during
human-AI collaboration by applying a threshold within the score space; and (4)
selecting the best segmentation prediction for each test sample at test time
when multiple segmentation models are available, by choosing the prediction
with the highest quality score. Models and code will be made available at
https://github.com/ahjolsenbics/EvanySeg.",2024-09-23,"Ahjol Senbi, Tianyu Huang, Fei Lyu, Qing Li, Yuhui Tao, Wei Shao, Qiang Chen, Chengyan Wang, Shuo Wang, Tao Zhou, Yizhe Zhang",http://arxiv.org/pdf/2409.14874v2,cs.LG
Testing Dependency of Weighted Random Graphs,"In this paper, we study the task of detecting the edge dependency between two
weighted random graphs. We formulate this task as a simple hypothesis testing
problem, where under the null hypothesis, the two observed graphs are
statistically independent, whereas under the alternative, the edges of one
graph are dependent on the edges of a uniformly and randomly vertex-permuted
version of the other graph. For general edge-weight distributions, we establish
thresholds at which optimal testing becomes information-theoretically possible
or impossible, as a function of the total number of nodes in the observed
graphs and the generative distributions of the weights. Finally, we identify a
statistical-computational gap, and present evidence suggesting that this gap is
inherent using the framework of low-degree polynomials.",2024-09-23,"Mor Oren, Vered Paslev, Wasim Huleihel",http://arxiv.org/pdf/2409.14870v2,cs.LG
Embedding Knowledge Graph in Function Spaces,"We introduce a novel embedding method diverging from conventional approaches
by operating within function spaces of finite dimension rather than finite
vector space, thus departing significantly from standard knowledge graph
embedding techniques. Initially employing polynomial functions to compute
embeddings, we progress to more intricate representations using neural networks
with varying layer complexities. We argue that employing functions for
embedding computation enhances expressiveness and allows for more degrees of
freedom, enabling operations such as composition, derivatives and primitive of
entities representation. Additionally, we meticulously outline the step-by-step
construction of our approach and provide code for reproducibility, thereby
facilitating further exploration and application in the field.",2024-09-23,"Louis Mozart Kamdem Teyou, Caglar Demir, Axel-Cyrille Ngonga Ngomo",http://arxiv.org/pdf/2409.14857v2,cs.LG
Disentanglement with Factor Quantized Variational Autoencoders,"Disentangled representation learning aims to represent the underlying
generative factors of a dataset in a latent representation independently of one
another. In our work, we propose a discrete variational autoencoder (VAE) based
model where the ground truth information about the generative factors are not
provided to the model. We demonstrate the advantages of learning discrete
representations over learning continuous representations in facilitating
disentanglement. Furthermore, we propose incorporating an inductive bias into
the model to further enhance disentanglement. Precisely, we propose scalar
quantization of the latent variables in a latent representation with scalar
values from a global codebook, and we add a total correlation term to the
optimization as an inductive bias. Our method called FactorQVAE combines
optimization based disentanglement approaches with discrete representation
learning, and it outperforms the former disentanglement methods in terms of two
disentanglement metrics (DCI and InfoMEC) while improving the reconstruction
performance. Our code can be found at
https://github.com/ituvisionlab/FactorQVAE.",2024-09-23,"Gulcin Baykal, Melih Kandemir, Gozde Unal",http://arxiv.org/pdf/2409.14851v2,cs.LG
GroCo: Ground Constraint for Metric Self-Supervised Monocular Depth,"Monocular depth estimation has greatly improved in the recent years but
models predicting metric depth still struggle to generalize across diverse
camera poses and datasets. While recent supervised methods mitigate this issue
by leveraging ground prior information at inference, their adaptability to
self-supervised settings is limited due to the additional challenge of scale
recovery. Addressing this gap, we propose in this paper a novel constraint on
ground areas designed specifically for the self-supervised paradigm. This
mechanism not only allows to accurately recover the scale but also ensures
coherence between the depth prediction and the ground prior. Experimental
results show that our method surpasses existing scale recovery techniques on
the KITTI benchmark and significantly enhances model generalization
capabilities. This improvement can be observed by its more robust performance
across diverse camera rotations and its adaptability in zero-shot conditions
with previously unseen driving datasets such as DDAD.",2024-09-23,"Aurélien Cecille, Stefan Duffner, Franck Davoine, Thibault Neveu, Rémi Agier",http://arxiv.org/pdf/2409.14850v1,cs.LG
Orthogonal Finetuning for Direct Preference Optimization,"DPO is an effective preference optimization algorithm. However, the DPO-tuned
models tend to overfit on the dispreferred samples, manifested as overly long
generations lacking diversity. While recent regularization approaches have
endeavored to alleviate this issue by modifying the objective function, they
achieved that at the cost of alignment performance degradation. In this paper,
we innovatively incorporate regularization from the perspective of weight
updating to curb alignment overfitting. Through the pilot experiment, we
discovered that there exists a positive correlation between overfitting and the
hyperspherical energy fluctuation. Hence, we introduce orthogonal finetuning
for DPO via a weight-Rotated Preference Optimization (RoPO) method, which
merely conducts rotational and magnitude-stretching updates on the weight
parameters to maintain the hyperspherical energy invariant, thereby preserving
the knowledge encoded in the angle between neurons. Extensive experiments
demonstrate that our model aligns perfectly with human preferences while
retaining the original expressive capacity using only 0.0086% of the trainable
parameters, suggesting an effective regularization against overfitting.
Specifically, RoPO outperforms DPO by up to 10 points on MT-Bench and by up to
2.8 points on AlpacaEval 2, while enhancing the generation diversity by an
average of 6 points.",2024-09-23,"Chenxu Yang, Ruipeng Jia, Naibin Gu, Zheng Lin, Siyuan Chen, Chao Pang, Weichong Yin, Yu Sun, Hua Wu, Weiping Wang",http://arxiv.org/pdf/2409.14836v2,cs.LG
Energy-Aware Federated Learning in Satellite Constellations,"Federated learning in satellite constellations, where the satellites
collaboratively train a machine learning model, is a promising technology
towards enabling globally connected intelligence and the integration of space
networks into terrestrial mobile networks. The energy required for this
computationally intensive task is provided either by solar panels or by an
internal battery if the satellite is in Earth's shadow. Careful management of
this battery and system's available energy resources is not only necessary for
reliable satellite operation, but also to avoid premature battery aging. We
propose a novel energy-aware computation time scheduler for satellite FL, which
aims to minimize battery usage without any impact on the convergence speed.
Numerical results indicate an increase of more than 3x in battery lifetime can
be achieved over energy-agnostic task scheduling.",2024-09-23,"Nasrin Razmi, Bho Matthiesen, Armin Dekorsy, Petar Popovski",http://arxiv.org/pdf/2409.14832v1,cs.LG
Identify As A Human Does: A Pathfinder of Next-Generation Anti-Cheat Framework for First-Person Shooter Games,"The gaming industry has experienced substantial growth, but cheating in
online games poses a significant threat to the integrity of the gaming
experience. Cheating, particularly in first-person shooter (FPS) games, can
lead to substantial losses for the game industry. Existing anti-cheat solutions
have limitations, such as client-side hardware constraints, security risks,
server-side unreliable methods, and both-sides suffer from a lack of
comprehensive real-world datasets. To address these limitations, the paper
proposes HAWK, a server-side FPS anti-cheat framework for the popular game
CS:GO. HAWK utilizes machine learning techniques to mimic human experts'
identification process, leverages novel multi-view features, and it is equipped
with a well-defined workflow. The authors evaluate HAWK with the first large
and real-world datasets containing multiple cheat types and cheating
sophistication, and it exhibits promising efficiency and acceptable overheads,
shorter ban times compared to the in-use anti-cheat, a significant reduction in
manual labor, and the ability to capture cheaters who evaded official
inspections.",2024-09-23,"Jiayi Zhang, Chenxin Sun, Yue Gu, Qingyu Zhang, Jiayi Lin, Xiaojiang Du, Chenxiong Qian",http://arxiv.org/pdf/2409.14830v1,cs.LG
VARADE: a Variational-based AutoRegressive model for Anomaly Detection on the Edge,"Detecting complex anomalies on massive amounts of data is a crucial task in
Industry 4.0, best addressed by deep learning. However, available solutions are
computationally demanding, requiring cloud architectures prone to latency and
bandwidth issues. This work presents VARADE, a novel solution implementing a
light autoregressive framework based on variational inference, which is best
suited for real-time execution on the edge. The proposed approach was validated
on a robotic arm, part of a pilot production line, and compared with several
state-of-the-art algorithms, obtaining the best trade-off between anomaly
detection accuracy, power consumption and inference frequency on two different
edge platforms.",2024-09-23,"Alessio Mascolini, Sebastiano Gaiardelli, Francesco Ponzio, Nicola Dall'Ora, Enrico Macii, Sara Vinco, Santa Di Cataldo, Franco Fummi",http://arxiv.org/pdf/2409.14816v2,cs.LG
From Commands to Prompts: LLM-based Semantic File System for AIOS,"Large language models (LLMs) have demonstrated significant potential in the
development of intelligent applications and systems such as LLM-based agents
and agent operating systems (AIOS). However, when these applications and
systems interact with the underlying file system, the file system still remains
the traditional paradigm: reliant on manual navigation through precise
commands. This paradigm poses a bottleneck to the usability of these systems as
users are required to navigate complex folder hierarchies and remember cryptic
file names. To address this limitation, we propose an LLM-based semantic file
system ( LSFS ) for prompt-driven file management. Unlike conventional
approaches, LSFS incorporates LLMs to enable users or agents to interact with
files through natural language prompts, facilitating semantic file management.
At the macro-level, we develop a comprehensive API set to achieve semantic file
management functionalities, such as semantic file retrieval, file update
monitoring and summarization, and semantic file rollback). At the micro-level,
we store files by constructing semantic indexes for them, design and implement
syscalls of different semantic operations (e.g., CRUD, group by, join) powered
by vector database. Our experiments show that LSFS offers significant
improvements over traditional file systems in terms of user convenience, the
diversity of supported functions, and the accuracy and efficiency of file
operations. Additionally, with the integration of LLM, our system enables more
intelligent file management tasks, such as content summarization and version
comparison, further enhancing its capabilities.",2024-09-23,"Zeru Shi, Kai Mei, Mingyu Jin, Yongye Su, Chaoji Zuo, Wenyue Hua, Wujiang Xu, Yujie Ren, Zirui Liu, Mengnan Du, Dong Deng, Yongfeng Zhang",http://arxiv.org/pdf/2410.11843v5,cs.LG
Pre-trained Language Model and Knowledge Distillation for Lightweight Sequential Recommendation,"Sequential recommendation models user interests based on historical behaviors
to provide personalized recommendation. Previous sequential recommendation
algorithms primarily employ neural networks to extract features of user
interests, achieving good performance. However, due to the recommendation
system datasets sparsity, these algorithms often employ small-scale network
frameworks, resulting in weaker generalization capability. Recently, a series
of sequential recommendation algorithms based on large pre-trained language
models have been proposed. Nonetheless, given the real-time demands of
recommendation systems, the challenge remains in applying pre-trained language
models for rapid recommendations in real scenarios. To address this, we propose
a sequential recommendation algorithm based on a pre-trained language model and
knowledge distillation. The key of proposed algorithm is to transfer
pre-trained knowledge across domains and achieve lightweight inference by
knowledge distillation. The algorithm operates in two stages: in the first
stage, we fine-tune the pre-trained language model on the recommendation
dataset to transfer the pre-trained knowledge to the recommendation task; in
the second stage, we distill the trained language model to transfer the learned
knowledge to a lightweight model. Extensive experiments on multiple public
recommendation datasets show that the proposed algorithm enhances
recommendation accuracy and provide timely recommendation services.",2024-09-23,"Li Li, Mingyue Cheng, Zhiding Liu, Hao Zhang, Qi Liu, Enhong Chen",http://arxiv.org/pdf/2409.14810v1,cs.LG
SDBA: A Stealthy and Long-Lasting Durable Backdoor Attack in Federated Learning,"Federated Learning is a promising approach for training machine learning
models while preserving data privacy, but its distributed nature makes it
vulnerable to backdoor attacks, particularly in NLP tasks while related
research remains limited. This paper introduces SDBA, a novel backdoor attack
mechanism designed for NLP tasks in FL environments. Our systematic analysis
across LSTM and GPT-2 models identifies the most vulnerable layers for backdoor
injection and achieves both stealth and long-lasting durability through
layer-wise gradient masking and top-k% gradient masking within these layers.
Experiments on next token prediction and sentiment analysis tasks show that
SDBA outperforms existing backdoors in durability and effectively bypasses
representative defense mechanisms, with notable performance in LLM such as
GPT-2. These results underscore the need for robust defense strategies in
NLP-based FL systems.",2024-09-23,"Minyeong Choe, Cheolhee Park, Changho Seo, Hyunil Kim",http://arxiv.org/pdf/2409.14805v1,cs.LG
Research on Dynamic Data Flow Anomaly Detection based on Machine Learning,"The sophistication and diversity of contemporary cyberattacks have rendered
the use of proxies, gateways, firewalls, and encrypted tunnels as a standalone
defensive strategy inadequate. Consequently, the proactive identification of
data anomalies has emerged as a prominent area of research within the field of
data security. The majority of extant studies concentrate on sample equilibrium
data, with the consequence that the detection effect is not optimal in the
context of unbalanced data. In this study, the unsupervised learning method is
employed to identify anomalies in dynamic data flows. Initially,
multi-dimensional features are extracted from real-time data, and a clustering
algorithm is utilised to analyse the patterns of the data. This enables the
potential outliers to be automatically identified. By clustering similar data,
the model is able to detect data behaviour that deviates significantly from
normal traffic without the need for labelled data. The results of the
experiments demonstrate that the proposed method exhibits high accuracy in the
detection of anomalies across a range of scenarios. Notably, it demonstrates
robust and adaptable performance, particularly in the context of unbalanced
data.",2024-09-23,"Liyang Wang, Yu Cheng, Hao Gong, Jiacheng Hu, Xirui Tang, Iris Li",http://arxiv.org/pdf/2409.14796v1,cs.LG
Adaptive Conformal Inference for Multi-Step Ahead Time-Series Forecasting Online,"The aim of this paper is to propose an adaptation of the well known adaptive
conformal inference (ACI) algorithm to achieve finite-sample coverage
guarantees in multi-step ahead time-series forecasting in the online setting.
ACI dynamically adjusts significance levels, and comes with finite-sample
guarantees on coverage, even for non-exchangeable data. Our multi-step ahead
ACI procedure inherits these guarantees at each prediction step, as well as for
the overall error rate. The multi-step ahead ACI algorithm can be used with
different target error and learning rates at different prediction steps, which
is illustrated in our numerical examples, where we employ a version of the
confromalised ridge regression algorithm, adapted to multi-input multi-output
forecasting. The examples serve to show how the method works in practice,
illustrating the effect of variable target error and learning rates for
different prediction steps, which suggests that a balance may be struck between
efficiency (interval width) and coverage.t",2024-09-23,Johan Hallberg Szabadváry,http://arxiv.org/pdf/2409.14792v1,cs.LG
Multiscale scattered data analysis in samplet coordinates,"We study multiscale scattered data interpolation schemes for globally
supported radial basis functions with focus on the Mat\'ern class. The
multiscale approximation is constructed through a sequence of residual
corrections, where radial basis functions with different lengthscale parameters
are combined to capture varying levels of detail. We prove that the condition
numbers of the the diagonal blocks of the corresponding multiscale system
remain bounded independently of the particular level, allowing us to use an
iterative solver with a bounded number of iterations for the numerical
solution. Employing an appropriate diagonal scaling, the multiscale system
becomes well conditioned. We exploit this fact to derive a general error
estimate bounding the consistency error issuing from a numerical approximation
of the multiscale system. To apply the multiscale approach to large data sets,
we suggest to represent each level of the multiscale system in samplet
coordinates. Samplets are localized, discrete signed measures exhibiting
vanishing moments and allow for the sparse approximation of generalized
Vandermonde matrices issuing from a vast class of radial basis functions. Given
a quasi-uniform set of $N$ data sites, and local approximation spaces with
exponentially decreasing dimension, the samplet compressed multiscale system
can be assembled with cost $\mathcal{O}(N \log^2 N)$. The overall cost of the
proposed approach is $\mathcal{O}(N \log^2 N)$. The theoretical findings are
accompanied by extensive numerical studies in two and three spatial dimensions.",2024-09-23,"Sara Avesani, Rüdiger Kempf, Michael Multerer, Holger Wendland",http://arxiv.org/pdf/2409.14791v2,cs.LG
Temporal Graph Memory Networks For Knowledge Tracing,"Tracing a student's knowledge growth given the past exercise answering is a
vital objective in automatic tutoring systems to customize the learning
experience. Yet, achieving this objective is a non-trivial task as it involves
modeling the knowledge state across multiple knowledge components (KCs) while
considering their temporal and relational dynamics during the learning process.
Knowledge tracing methods have tackled this task by either modeling KCs'
temporal dynamics using recurrent models or relational dynamics across KCs and
questions using graph models. Albeit, there is a lack of methods that could
learn joint embedding between relational and temporal dynamics of the task.
Moreover, many methods that count for the impact of a student's forgetting
behavior during the learning process use hand-crafted features, limiting their
generalization on different scenarios. In this paper, we propose a novel method
that jointly models the relational and temporal dynamics of the knowledge state
using a deep temporal graph memory network. In addition, we propose a generic
technique for representing a student's forgetting behavior using temporal decay
constraints on the graph memory module. We demonstrate the effectiveness of our
proposed method using multiple knowledge tracing benchmarks while comparing it
to state-of-the-art methods.",2024-09-23,"Seif Gad, Sherif Abdelfattah, Ghodai Abdelrahman",http://arxiv.org/pdf/2410.01836v1,cs.LG
Isometric Immersion Learning with Riemannian Geometry,"Manifold learning has been proven to be an effective method for capturing the
implicitly intrinsic structure of non-Euclidean data, in which one of the
primary challenges is how to maintain the distortion-free (isometry) of the
data representations. Actually, there is still no manifold learning method that
provides a theoretical guarantee of isometry. Inspired by Nash's isometric
theorem, we introduce a new concept called isometric immersion learning based
on Riemannian geometry principles. Following this concept, an unsupervised
neural network-based model that simultaneously achieves metric and manifold
learning is proposed by integrating Riemannian geometry priors. What's more, we
theoretically derive and algorithmically implement a maximum likelihood
estimation-based training method for the new model. In the simulation
experiments, we compared the new model with the state-of-the-art baselines on
various 3-D geometry datasets, demonstrating that the new model exhibited
significantly superior performance in multiple evaluation metrics. Moreover, we
applied the Riemannian metric learned from the new model to downstream
prediction tasks in real-world scenarios, and the accuracy was improved by an
average of 8.8%.",2024-09-23,"Zihao Chen, Wenyong Wang, Yu Xiang",http://arxiv.org/pdf/2409.14760v1,cs.LG
Automated Spatio-Temporal Weather Modeling for Load Forecasting,"Electricity is difficult to store, except at prohibitive cost, and therefore
the balance between generation and load must be maintained at all times.
Electricity is traditionally managed by anticipating demand and intermittent
production (wind, solar) and matching flexible production (hydro, nuclear, coal
and gas). Accurate forecasting of electricity load and renewable production is
therefore essential to ensure grid performance and stability. Both are highly
dependent on meteorological variables (temperature, wind, sunshine). These
dependencies are complex and difficult to model. On the one hand, spatial
variations do not have a uniform impact because population, industry, and wind
and solar farms are not evenly distributed across the territory. On the other
hand, temporal variations can have delayed effects on load (due to the thermal
inertia of buildings). With access to observations from different weather
stations and simulated data from meteorological models, we believe that both
phenomena can be modeled together. In today's state-of-the-art load forecasting
models, the spatio-temporal modeling of the weather is fixed. In this work, we
aim to take advantage of the automated representation and spatio-temporal
feature extraction capabilities of deep neural networks to improve
spatio-temporal weather modeling for load forecasting. We compare our deep
learning-based methodology with the state-of-the-art on French national load.
This methodology could also be fully adapted to forecasting renewable energy
production.",2024-09-23,"Julie Keisler, Margaux Bregere",http://arxiv.org/pdf/2409.16326v1,cs.LG
Neural Control Variates with Automatic Integration,"This paper presents a method to leverage arbitrary neural network
architecture for control variates. Control variates are crucial in reducing the
variance of Monte Carlo integration, but they hinge on finding a function that
both correlates with the integrand and has a known analytical integral.
Traditional approaches rely on heuristics to choose this function, which might
not be expressive enough to correlate well with the integrand. Recent research
alleviates this issue by modeling the integrands with a learnable parametric
model, such as a neural network. However, the challenge remains in creating an
expressive parametric model with a known analytical integral. This paper
proposes a novel approach to construct learnable parametric control variates
functions from arbitrary neural network architectures. Instead of using a
network to approximate the integrand directly, we employ the network to
approximate the anti-derivative of the integrand. This allows us to use
automatic differentiation to create a function whose integration can be
constructed by the antiderivative network. We apply our method to solve partial
differential equations using the Walk-on-sphere algorithm. Our results indicate
that this approach is unbiased and uses various network architectures to
achieve lower variance than other control variate methods.",2024-09-23,"Zilu Li, Guandao Yang, Qingqing Zhao, Xi Deng, Leonidas Guibas, Bharath Hariharan, Gordon Wetzstein",http://arxiv.org/pdf/2409.15394v1,cs.LG
EDSNet: Efficient-DSNet for Video Summarization,"Current video summarization methods largely rely on transformer-based
architectures, which, due to their quadratic complexity, require substantial
computational resources. In this work, we address these inefficiencies by
enhancing the Direct-to-Summarize Network (DSNet) with more resource-efficient
token mixing mechanisms. We show that replacing traditional attention with
alternatives like Fourier, Wavelet transforms, and Nystr\""omformer improves
efficiency and performance. Furthermore, we explore various pooling strategies
within the Regional Proposal Network, including ROI pooling, Fast Fourier
Transform pooling, and flat pooling. Our experimental results on TVSum and
SumMe datasets demonstrate that these modifications significantly reduce
computational costs while maintaining competitive summarization performance.
Thus, our work offers a more scalable solution for video summarization tasks.",2024-09-23,"Ashish Prasad, Pranav Jeevan, Amit Sethi",http://arxiv.org/pdf/2409.14724v1,cs.LG
Neural refractive index field: Unlocking the Potential of Background-oriented Schlieren Tomography in Volumetric Flow Visualization,"Background-oriented Schlieren tomography (BOST) is a prevalent method for
visualizing intricate turbulent flows, valued for its ease of implementation
and capacity to capture three-dimensional distributions of a multitude of flow
parameters. However, the voxel-based meshing scheme leads to significant
challenges, such as inadequate spatial resolution, substantial discretization
errors, poor noise immunity, and excessive computational costs. This work
presents an innovative reconstruction approach termed neural refractive index
field (NeRIF) which implicitly represents the flow field with a neural network,
which is trained with tailored strategies. Both numerical simulations and
experimental demonstrations on turbulent Bunsen flames suggest that our
approach can significantly improve the reconstruction accuracy and spatial
resolution while concurrently reducing computational expenses. Although
showcased in the context of background-oriented schlieren tomography here, the
key idea embedded in the NeRIF can be readily adapted to various other
tomographic modalities including tomographic absorption spectroscopy and
tomographic particle imaging velocimetry, broadening its potential impact
across different domains of flow visualization and analysis.",2024-09-23,"Yuanzhe He, Yutao Zheng, Shijie Xu, Chang Liu, Di Peng, Yingzheng Liu, Weiwei Cai",http://arxiv.org/pdf/2409.14722v2,cs.LG
MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification,"The complexity of text-embedded images presents a formidable challenge in
machine learning given the need for multimodal understanding of multiple
aspects of expression conveyed by them. While previous research in multimodal
analysis has primarily focused on singular aspects such as hate speech and its
subclasses, this study expands this focus to encompass multiple aspects of
linguistics: hate, targets of hate, stance, and humor. We introduce a novel
dataset PrideMM comprising 5,063 text-embedded images associated with the
LGBTQ+ Pride movement, thereby addressing a serious gap in existing resources.
We conduct extensive experimentation on PrideMM by using unimodal and
multimodal baseline methods to establish benchmarks for each task.
Additionally, we propose a novel framework MemeCLIP for efficient downstream
learning while preserving the knowledge of the pre-trained CLIP model. The
results of our experiments show that MemeCLIP achieves superior performance
compared to previously proposed frameworks on two real-world datasets. We
further compare the performance of MemeCLIP and zero-shot GPT-4 on the hate
classification task. Finally, we discuss the shortcomings of our model by
qualitatively analyzing misclassified samples. Our code and dataset are
publicly available at: https://github.com/SiddhantBikram/MemeCLIP.",2024-09-23,"Siddhant Bikram Shah, Shuvam Shiwakoti, Maheep Chaudhary, Haohan Wang",http://arxiv.org/pdf/2409.14703v2,cs.LG
Graph Network Models To Detect Illicit Transactions In Block Chain,"The use of cryptocurrencies has led to an increase in illicit activities such
as money laundering, with traditional rule-based approaches becoming less
effective in detecting and preventing such activities. In this paper, we
propose a novel approach to tackling this problem by applying graph attention
networks with residual network-like architecture (GAT-ResNet) to detect illicit
transactions related to anti-money laundering/combating the financing of
terrorism (AML/CFT) in blockchains. We train various models on the Elliptic
Bitcoin Transaction dataset, implementing logistic regression, Random Forest,
XGBoost, GCN, GAT, and our proposed GAT-ResNet model. Our results demonstrate
that the GAT-ResNet model has a potential to outperform the existing graph
network models in terms of accuracy, reliability and scalability. Our research
sheds light on the potential of graph related machine learning models to
improve efforts to combat financial crime and lays the foundation for further
research in this area.",2024-09-23,"Hrushyang Adloori, Vaishnavi Dasanapu, Abhijith Chandra Mergu",http://arxiv.org/pdf/2410.07150v1,cs.LG
Approximated Orthogonal Projection Unit: Stabilizing Regression Network Training Using Natural Gradient,"Neural networks (NN) are extensively studied in cutting-edge soft sensor
models due to their feature extraction and function approximation capabilities.
Current research into network-based methods primarily focuses on models'
offline accuracy. Notably, in industrial soft sensor context, online optimizing
stability and interpretability are prioritized, followed by accuracy. This
requires a clearer understanding of network's training process. To bridge this
gap, we propose a novel NN named the Approximated Orthogonal Projection Unit
(AOPU) which has solid mathematical basis and presents superior training
stability. AOPU truncates the gradient backpropagation at dual parameters,
optimizes the trackable parameters updates, and enhances the robustness of
training. We further prove that AOPU attains minimum variance estimation (MVE)
in NN, wherein the truncated gradient approximates the natural gradient (NG).
Empirical results on two chemical process datasets clearly show that AOPU
outperforms other models in achieving stable convergence, marking a significant
advancement in soft sensor field.",2024-09-23,"Shaoqi Wang, Chunjie Yang, Siwei Lou",http://arxiv.org/pdf/2409.15393v1,cs.LG
EDGE-Rec: Efficient and Data-Guided Edge Diffusion For Recommender Systems Graphs,"Most recommender systems research focuses on binary historical user-item
interaction encodings to predict future interactions. User features, item
features, and interaction strengths remain largely under-utilized in this space
or only indirectly utilized, despite proving largely effective in large-scale
production recommendation systems. We propose a new attention mechanism,
loosely based on the principles of collaborative filtering, called Row-Column
Separable Attention RCSA to take advantage of real-valued interaction weights
as well as user and item features directly. Building on this mechanism, we
additionally propose a novel Graph Diffusion Transformer GDiT architecture
which is trained to iteratively denoise the weighted interaction matrix of the
user-item interaction graph directly. The weighted interaction matrix is built
from the bipartite structure of the user-item interaction graph and
corresponding edge weights derived from user-item rating interactions. Inspired
by the recent progress in text-conditioned image generation, our method
directly produces user-item rating predictions on the same scale as the
original ratings by conditioning the denoising process on user and item
features with a principled approach.",2024-09-23,"Utkarsh Priyam, Hemit Shah, Edoardo Botta",http://arxiv.org/pdf/2409.14689v1,cs.LG
Robust Training Objectives Improve Embedding-based Retrieval in Industrial Recommendation Systems,"Improving recommendation systems (RS) can greatly enhance the user experience
across many domains, such as social media. Many RS utilize embedding-based
retrieval (EBR) approaches to retrieve candidates for recommendation. In an EBR
system, the embedding quality is key. According to recent literature,
self-supervised multitask learning (SSMTL) has showed strong performance on
academic benchmarks in embedding learning and resulted in an overall
improvement in multiple downstream tasks, demonstrating a larger resilience to
the adverse conditions between each downstream task and thereby increased
robustness and task generalization ability through the training objective.
However, whether or not the success of SSMTL in academia as a robust training
objectives translates to large-scale (i.e., over hundreds of million users and
interactions in-between) industrial RS still requires verification. Simply
adopting academic setups in industrial RS might entail two issues. Firstly,
many self-supervised objectives require data augmentations (e.g., embedding
masking/corruption) over a large portion of users and items, which is
prohibitively expensive in industrial RS. Furthermore, some self-supervised
objectives might not align with the recommendation task, which might lead to
redundant computational overheads or negative transfer. In light of these two
challenges, we evaluate using a robust training objective, specifically SSMTL,
through a large-scale friend recommendation system on a social media platform
in the tech sector, identifying whether this increase in robustness can work at
scale in enhancing retrieval in the production setting. Through online A/B
testing with SSMTL-based EBR, we observe statistically significant increases in
key metrics in the friend recommendations, with up to 5.45% improvements in new
friends made and 1.91% improvements in new friends made with cold-start users.",2024-09-23,"Matthew Kolodner, Mingxuan Ju, Zihao Fan, Tong Zhao, Elham Ghazizadeh, Yan Wu, Neil Shah, Yozen Liu",http://arxiv.org/pdf/2409.14682v1,cs.LG
"FeruzaSpeech: A 60 Hour Uzbek Read Speech Corpus with Punctuation, Casing, and Context","This paper introduces FeruzaSpeech, a read speech corpus of the Uzbek
language, containing transcripts in both Cyrillic and Latin alphabets, freely
available for academic research purposes. This corpus includes 60 hours of
high-quality recordings from a single native female speaker from Tashkent,
Uzbekistan. These recordings consist of short excerpts from a book and BBC
News. This paper discusses the enhancement of the Word Error Rates (WERs) on
CommonVoice 16.1's Uzbek data, Uzbek Speech Corpus data, and FeruzaSpeech data
upon integrating FeruzaSpeech.",2024-09-23,"Anna Povey, Katherine Povey",http://arxiv.org/pdf/2410.00035v1,cs.LG
Fourier neural operators for spatiotemporal dynamics in two-dimensional turbulence,"High-fidelity direct numerical simulation of turbulent flows for most
real-world applications remains an outstanding computational challenge. Several
machine learning approaches have recently been proposed to alleviate the
computational cost even though they become unstable or unphysical for long time
predictions. We identify that the Fourier neural operator (FNO) based models
combined with a partial differential equation (PDE) solver can accelerate fluid
dynamic simulations and thus address computational expense of large-scale
turbulence simulations. We treat the FNO model on the same footing as a PDE
solver and answer important questions about the volume and temporal resolution
of data required to build pre-trained models for turbulence. We also discuss
the pitfalls of purely data-driven approaches that need to be avoided by the
machine learning models to become viable and competitive tools for long time
simulations of turbulence.",2024-09-23,"Mohammad Atif, Pulkit Dubey, Pratik P. Aghor, Vanessa Lopez-Marrero, Tao Zhang, Abdullah Sharfuddin, Kwangmin Yu, Fan Yang, Foluso Ladeinde, Yangang Liu, Meifeng Lin, Lingda Li",http://arxiv.org/pdf/2409.14660v3,cs.LG
Federated Graph Learning with Adaptive Importance-based Sampling,"For privacy-preserving graph learning tasks involving distributed graph
datasets, federated learning (FL)-based GCN (FedGCN) training is required. A
key challenge for FedGCN is scaling to large-scale graphs, which typically
incurs high computation and communication costs when dealing with the
explosively increasing number of neighbors. Existing graph sampling-enhanced
FedGCN training approaches ignore graph structural information or dynamics of
optimization, resulting in high variance and inaccurate node embeddings. To
address this limitation, we propose the Federated Adaptive Importance-based
Sampling (FedAIS) approach. It achieves substantial computational cost saving
by focusing the limited resources on training important nodes, while reducing
communication overhead via adaptive historical embedding synchronization. The
proposed adaptive importance-based sampling method jointly considers the graph
structural heterogeneity and the optimization dynamics to achieve optimal
trade-off between efficiency and accuracy. Extensive evaluations against five
state-of-the-art baselines on five real-world graph datasets show that FedAIS
achieves comparable or up to 3.23% higher test accuracy, while saving
communication and computation costs by 91.77% and 85.59%.",2024-09-23,"Anran Li, Yuanyuan Chen, Chao Ren, Wenhan Wang, Ming Hu, Tianlin Li, Han Yu, Qingyu Chen",http://arxiv.org/pdf/2409.14655v1,cs.LG
Natural Language Query Engine for Relational Databases using Generative AI,"The growing reliance on data-driven decision-making highlights the need for
more intuitive ways to access and analyze information stored in relational
databases. However, the requirement of SQL knowledge has long been a
significant barrier for non-technical users. This article introduces an
innovative solution that leverages Generative AI to bridge this gap, enabling
users to query databases using natural language. Our approach automatically
translates natural language queries into SQL, ensuring both syntactic and
semantic correctness, while also generating clear, natural language responses
from the retrieved data. By streamlining the interaction between users and
databases, this method empowers individuals without technical expertise to
engage with data directly and efficiently, democratizing access to valuable
insights and enhancing productivity.",2024-09-23,Steve Tueno Fotso,http://arxiv.org/pdf/2410.07144v1,cs.LG
Demystifying Trajectory Recovery From Ash: An Open-Source Evaluation and Enhancement,"Once analysed, location trajectories can provide valuable insights beneficial
to various applications. However, such data is also highly sensitive, rendering
them susceptible to privacy risks in the event of mismanagement, for example,
revealing an individual's identity, home address, or political affiliations.
Hence, ensuring that privacy is preserved for this data is a priority. One
commonly taken measure to mitigate this concern is aggregation. Previous work
by Xu et al. shows that trajectories are still recoverable from anonymised and
aggregated datasets. However, the study lacks implementation details,
obfuscating the mechanisms of the attack. Additionally, the attack was
evaluated on commercial non-public datasets, rendering the results and
subsequent claims unverifiable. This study reimplements the trajectory recovery
attack from scratch and evaluates it on two open-source datasets, detailing the
preprocessing steps and implementation. Results confirm that privacy leakage
still exists despite common anonymisation and aggregation methods but also
indicate that the initial accuracy claims may have been overly ambitious. We
release all code as open-source to ensure the results are entirely reproducible
and, therefore, verifiable. Moreover, we propose a stronger attack by designing
a series of enhancements to the baseline attack. These enhancements yield
higher accuracies by up to 16%, providing an improved benchmark for future
research in trajectory recovery methods. Our improvements also enable online
execution of the attack, allowing partial attacks on larger datasets previously
considered unprocessable, thereby furthering the extent of privacy leakage. The
findings emphasise the importance of using strong privacy-preserving mechanisms
when releasing aggregated mobility data and not solely relying on aggregation
as a means of anonymisation.",2024-09-23,"Nicholas D'Silva, Toran Shahi, Øyvind Timian Dokk Husveg, Adith Sanjeeve, Erik Buchholz, Salil S. Kanhere",http://arxiv.org/pdf/2409.14645v2,cs.LG
Harmonising the Clinical Melody: Tuning Large Language Models for Hospital Course Summarisation in Clinical Coding,"The increasing volume and complexity of clinical documentation in Electronic
Medical Records systems pose significant challenges for clinical coders, who
must mentally process and summarise vast amounts of clinical text to extract
essential information needed for coding tasks. While large language models have
been successfully applied to shorter summarisation tasks in recent years, the
challenge of summarising a hospital course remains an open area for further
research and development. In this study, we adapted three pre trained LLMs,
Llama 3, BioMistral, Mistral Instruct v0.1 for the hospital course
summarisation task, using Quantized Low Rank Adaptation fine tuning. We created
a free text clinical dataset from MIMIC III data by concatenating various
clinical notes as the input clinical text, paired with ground truth Brief
Hospital Course sections extracted from the discharge summaries for model
training. The fine tuned models were evaluated using BERTScore and ROUGE
metrics to assess the effectiveness of clinical domain fine tuning.
Additionally, we validated their practical utility using a novel hospital
course summary assessment metric specifically tailored for clinical coding. Our
findings indicate that fine tuning pre trained LLMs for the clinical domain can
significantly enhance their performance in hospital course summarisation and
suggest their potential as assistive tools for clinical coding. Future work
should focus on refining data curation methods to create higher quality
clinical datasets tailored for hospital course summary tasks and adapting more
advanced open source LLMs comparable to proprietary models to further advance
this research.",2024-09-23,"Bokang Bi, Leibo Liu, Sanja Lujic, Louisa Jorm, Oscar Perez-Concha",http://arxiv.org/pdf/2409.14638v2,cs.LG
Not Only the Last-Layer Features for Spurious Correlations: All Layer Deep Feature Reweighting,"Spurious correlations are a major source of errors for machine learning
models, in particular when aiming for group-level fairness. It has been
recently shown that a powerful approach to combat spurious correlations is to
re-train the last layer on a balanced validation dataset, isolating robust
features for the predictor. However, key attributes can sometimes be discarded
by neural networks towards the last layer. In this work, we thus consider
retraining a classifier on a set of features derived from all layers. We
utilize a recently proposed feature selection strategy to select unbiased
features from all the layers. We observe this approach gives significant
improvements in worst-group accuracy on several standard benchmarks.",2024-09-23,"Humza Wajid Hameed, Geraldin Nanfack, Eugene Belilovsky",http://arxiv.org/pdf/2409.14637v1,cs.LG
Hierarchical end-to-end autonomous navigation through few-shot waypoint detection,"Human navigation is facilitated through the association of actions with
landmarks, tapping into our ability to recognize salient features in our
environment. Consequently, navigational instructions for humans can be
extremely concise, such as short verbal descriptions, indicating a small memory
requirement and no reliance on complex and overly accurate navigation tools.
Conversely, current autonomous navigation schemes rely on accurate positioning
devices and algorithms as well as extensive streams of sensory data collected
from the environment. Inspired by this human capability and motivated by the
associated technological gap, in this work we propose a hierarchical end-to-end
meta-learning scheme that enables a mobile robot to navigate in a previously
unknown environment upon presentation of only a few sample images of a set of
landmarks along with their corresponding high-level navigation actions. This
dramatically simplifies the wayfinding process and enables easy adoption to new
environments. For few-shot waypoint detection, we implement a metric-based
few-shot learning technique through distribution embedding. Waypoint detection
triggers the multi-task low-level maneuver controller module to execute the
corresponding high-level navigation action. We demonstrate the effectiveness of
the scheme using a small-scale autonomous vehicle on novel indoor navigation
tasks in several previously unseen environments.",2024-09-23,"Amin Ghafourian, Zhongying CuiZhu, Debo Shi, Ian Chuang, Francois Charette, Rithik Sachdeva, Iman Soltani",http://arxiv.org/pdf/2409.14633v1,cs.LG
From Lazy to Rich: Exact Learning Dynamics in Deep Linear Networks,"Biological and artificial neural networks develop internal representations
that enable them to perform complex tasks. In artificial networks, the
effectiveness of these models relies on their ability to build task specific
representation, a process influenced by interactions among datasets,
architectures, initialization strategies, and optimization algorithms. Prior
studies highlight that different initializations can place networks in either a
lazy regime, where representations remain static, or a rich/feature learning
regime, where representations evolve dynamically. Here, we examine how
initialization influences learning dynamics in deep linear neural networks,
deriving exact solutions for lambda-balanced initializations-defined by the
relative scale of weights across layers. These solutions capture the evolution
of representations and the Neural Tangent Kernel across the spectrum from the
rich to the lazy regimes. Our findings deepen the theoretical understanding of
the impact of weight initialization on learning regimes, with implications for
continual learning, reversal learning, and transfer learning, relevant to both
neuroscience and practical applications.",2024-09-22,"Clémentine C. J. Dominé, Nicolas Anguita, Alexandra M. Proca, Lukas Braun, Daniel Kunin, Pedro A. M. Mediano, Andrew M. Saxe",http://arxiv.org/pdf/2409.14623v2,cs.LG
LatentQGAN: A Hybrid QGAN with Classical Convolutional Autoencoder,"Quantum machine learning consists in taking advantage of quantum computations
to generate classical data. A potential application of quantum machine learning
is to harness the power of quantum computers for generating classical data, a
process essential to a multitude of applications such as enriching training
datasets, anomaly detection, and risk management in finance. Given the success
of Generative Adversarial Networks in classical image generation, the
development of its quantum versions has been actively conducted. However,
existing implementations on quantum computers often face significant
challenges, such as scalability and training convergence issues. To address
these issues, we propose LatentQGAN, a novel quantum model that uses a hybrid
quantum-classical GAN coupled with an autoencoder. Although it was initially
designed for image generation, the LatentQGAN approach holds potential for
broader application across various practical data generation tasks.
Experimental outcomes on both classical simulators and noisy intermediate scale
quantum computers have demonstrated significant performance enhancements over
existing quantum methods, alongside a significant reduction in quantum
resources overhead.",2024-09-22,"Alexis Vieloszynski, Soumaya Cherkaoui, Ola Ahmad, Jean-Frédéric Laprade, Oliver Nahman-Lévesque, Abdallah Aaraba, Shengrui Wang",http://arxiv.org/pdf/2409.14622v4,cs.LG
Protein-Mamba: Biological Mamba Models for Protein Function Prediction,"Protein function prediction is a pivotal task in drug discovery,
significantly impacting the development of effective and safe therapeutics.
Traditional machine learning models often struggle with the complexity and
variability inherent in predicting protein functions, necessitating more
sophisticated approaches. In this work, we introduce Protein-Mamba, a novel
two-stage model that leverages both self-supervised learning and fine-tuning to
improve protein function prediction. The pre-training stage allows the model to
capture general chemical structures and relationships from large, unlabeled
datasets, while the fine-tuning stage refines these insights using specific
labeled datasets, resulting in superior prediction performance. Our extensive
experiments demonstrate that Protein-Mamba achieves competitive performance,
compared with a couple of state-of-the-art methods across a range of protein
function datasets. This model's ability to effectively utilize both unlabeled
and labeled data highlights the potential of self-supervised learning in
advancing protein function prediction and offers a promising direction for
future research in drug discovery.",2024-09-22,"Bohao Xu, Yingzhou Lu, Yoshitaka Inoue, Namkyeong Lee, Tianfan Fu, Jintai Chen",http://arxiv.org/pdf/2409.14617v1,cs.LG
Patch Ranking: Efficient CLIP by Learning to Rank Local Patches,"Contrastive image-text pre-trained models such as CLIP have shown remarkable
adaptability to downstream tasks. However, they face challenges due to the high
computational requirements of the Vision Transformer (ViT) backbone. Current
strategies to boost ViT efficiency focus on pruning patch tokens but fall short
in addressing the multimodal nature of CLIP and identifying the optimal subset
of tokens for maximum performance. To address this, we propose greedy search
methods to establish a ""Golden Ranking"" and introduce a lightweight predictor
specifically trained to approximate this Ranking. To compensate for any
performance degradation resulting from token pruning, we incorporate learnable
visual tokens that aid in restoring and potentially enhancing the model's
performance. Our work presents a comprehensive and systematic investigation of
pruning tokens within the ViT backbone of CLIP models. Through our framework,
we successfully reduced 40% of patch tokens in CLIP's ViT while only suffering
a minimal average accuracy loss of 0.3 across seven datasets. Our study lays
the groundwork for building more computationally efficient multimodal models
without sacrificing their performance, addressing a key challenge in the
application of advanced vision-language models.",2024-09-22,"Cheng-En Wu, Jinhong Lin, Yu Hen Hu, Pedro Morgado",http://arxiv.org/pdf/2409.14607v2,cs.LG
Supply Risk-Aware Alloy Discovery and Design,"Materials design is a critical driver of innovation, yet overlooking the
technological, economic, and environmental risks inherent in materials and
their supply chains can lead to unsustainable and risk-prone solutions. To
address this, we present a novel risk-aware design approach that integrates
Supply-Chain Aware Design Strategies into the materials development process.
This approach leverages existing language models and text analysis to develop a
specialized model for predicting materials feedstock supply risk indices. To
efficiently navigate the multi-objective, multi-constraint design space, we
employ Batch Bayesian Optimization (BBO), enabling the identification of
Pareto-optimal high entropy alloys (HEAs) that balance performance objectives
with minimized supply risk. A case study using the MoNbTiVW system demonstrates
the efficacy of our approach in four scenarios, highlighting the significant
impact of incorporating supply risk into the design process. By optimizing for
both performance and supply risk, we ensure that the developed alloys are not
only high-performing but also sustainable and economically viable. This
integrated approach represents a critical step towards a future where materials
discovery and design seamlessly consider sustainability, supply chain dynamics,
and comprehensive life cycle analysis.",2024-09-22,"Mrinalini Mulukutla, Robert Robinson, Danial Khatamsaz, Brent Vela, Nhu Vu, Raymundo Arróyave",http://arxiv.org/pdf/2409.15391v1,cs.LG
Implicit Dynamical Flow Fusion (IDFF) for Generative Modeling,"Conditional Flow Matching (CFM) models can generate high-quality samples from
a non-informative prior, but they can be slow, often needing hundreds of
network evaluations (NFE). To address this, we propose Implicit Dynamical Flow
Fusion (IDFF); IDFF learns a new vector field with an additional momentum term
that enables taking longer steps during sample generation while maintaining the
fidelity of the generated distribution. Consequently, IDFFs reduce the NFEs by
a factor of ten (relative to CFMs) without sacrificing sample quality, enabling
rapid sampling and efficient handling of image and time-series data generation
tasks. We evaluate IDFF on standard benchmarks such as CIFAR-10 and CelebA for
image generation, where we achieve likelihood and quality performance
comparable to CFMs and diffusion-based models with fewer NFEs. IDFF also shows
superior performance on time-series datasets modeling, including molecular
simulation and sea surface temperature (SST) datasets, highlighting its
versatility and effectiveness across different
domains.\href{https://github.com/MrRezaeiUofT/IDFF}{Github Repository}",2024-09-22,"Mohammad R. Rezaei, Rahul G. Krishnan, Milos R. Popovic, Milad Lankarany",http://arxiv.org/pdf/2409.14599v3,cs.LG
Linear Independence of Generalized Neurons and Related Functions,"The linear independence of neurons plays a significant role in theoretical
analysis of neural networks. Specifically, given neurons $H_1, ..., H_n: \bR^N
\times \bR^d \to \bR$, we are interested in the following question: when are
$\{H_1(\theta_1, \cdot), ..., H_n(\theta_n, \cdot)\}$ are linearly independent
as the parameters $\theta_1, ..., \theta_n$ of these functions vary over
$\bR^N$. Previous works give a complete characterization of two-layer neurons
without bias, for generic smooth activation functions. In this paper, we study
the problem for neurons with arbitrary layers and widths, giving a simple but
complete characterization for generic analytic activation functions.",2024-09-22,Leyang Zhang,http://arxiv.org/pdf/2410.03693v1,cs.LG
"EchoAtt: Attend, Copy, then Adjust for More Efficient Large Language Models","Large Language Models (LLMs), with their increasing depth and number of
parameters, have demonstrated outstanding performance across a variety of
natural language processing tasks. However, this growth in scale leads to
increased computational demands, particularly during inference and fine-tuning.
To address these challenges, we introduce EchoAtt, a novel framework aimed at
optimizing transformer-based models by analyzing and leveraging the similarity
of attention patterns across layers. Our analysis reveals that many inner
layers in LLMs, especially larger ones, exhibit highly similar attention
matrices. By exploiting this similarity, EchoAtt enables the sharing of
attention matrices in less critical layers, significantly reducing
computational requirements without compromising performance. We incorporate
this approach within a knowledge distillation setup, where a pre-trained
teacher model guides the training of a smaller student model. The student model
selectively shares attention matrices in layers with high similarity while
inheriting key parameters from the teacher. Our best results with
TinyLLaMA-1.1B demonstrate that EchoAtt improves inference speed by 15\%,
training speed by 25\%, and reduces the number of parameters by approximately
4\%, all while improving zero-shot performance. These findings highlight the
potential of attention matrix sharing to enhance the efficiency of LLMs, making
them more practical for real-time and resource-limited applications.",2024-09-22,"Hossein Rajabzadeh, Aref Jafari, Aman Sharma, Benyamin Jami, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh",http://arxiv.org/pdf/2409.14595v1,cs.LG
Testing Causal Models with Hidden Variables in Polynomial Delay via Conditional Independencies,"Testing a hypothesized causal model against observational data is a key
prerequisite for many causal inference tasks. A natural approach is to test
whether the conditional independence relations (CIs) assumed in the model hold
in the data. While a model can assume exponentially many CIs (with respect to
the number of variables), testing all of them is both impractical and
unnecessary. Causal graphs, which encode these CIs in polynomial space, give
rise to local Markov properties that enable model testing with a significantly
smaller subset of CIs. Model testing based on local properties requires an
algorithm to list the relevant CIs. However, existing algorithms for realistic
settings with hidden variables and non-parametric distributions can take
exponential time to produce even a single CI constraint. In this paper, we
introduce the c-component local Markov property (C-LMP) for causal graphs with
hidden variables. Since C-LMP can still invoke an exponential number of CIs, we
develop a polynomial delay algorithm to list these CIs in poly-time intervals.
To our knowledge, this is the first algorithm that enables poly-delay testing
of CIs in causal graphs with hidden variables against arbitrary data
distributions. Experiments on real-world and synthetic data demonstrate the
practicality of our algorithm.",2024-09-22,"Hyunchai Jeong, Adiba Ejaz, Jin Tian, Elias Bareinboim",http://arxiv.org/pdf/2409.14593v1,cs.LG
Explainable AI needs formal notions of explanation correctness,"The use of machine learning (ML) in critical domains such as medicine poses
risks and requires regulation. One requirement is that decisions of ML systems
in high-risk applications should be human-understandable. The field of
""explainable artificial intelligence"" (XAI) seemingly addresses this need.
However, in its current form, XAI is unfit to provide quality control for ML;
it itself needs scrutiny. Popular XAI methods cannot reliably answer important
questions about ML models, their training data, or a given test input. We
recapitulate results demonstrating that popular XAI methods systematically
attribute importance to input features that are independent of the prediction
target. This limits their utility for purposes such as model and data
(in)validation, model improvement, and scientific discovery. We argue that the
fundamental reason for this limitation is that current XAI methods do not
address well-defined problems and are not evaluated against objective criteria
of explanation correctness. Researchers should formally define the problems
they intend to solve first and then design methods accordingly. This will lead
to notions of explanation correctness that can be theoretically verified and
objective metrics of explanation performance that can be assessed using
ground-truth data.",2024-09-22,"Stefan Haufe, Rick Wilming, Benedict Clark, Rustam Zhumagambetov, Danny Panknin, Ahcène Boubekki",http://arxiv.org/pdf/2409.14590v3,cs.LG
Backtracking Improves Generation Safety,"Text generation has a fundamental limitation almost by definition: there is
no taking back tokens that have been generated, even when they are clearly
problematic. In the context of language model safety, when a partial unsafe
generation is produced, language models by their nature tend to happily keep on
generating similarly unsafe additional text. This is in fact how safety
alignment of frontier models gets circumvented in the wild, despite great
efforts in improving their safety. Deviating from the paradigm of approaching
safety alignment as prevention (decreasing the probability of harmful
responses), we propose backtracking, a technique that allows language models to
""undo"" and recover from their own unsafe generation through the introduction of
a special [RESET] token. Our method can be incorporated into either SFT or DPO
training to optimize helpfulness and harmlessness. We show that models trained
to backtrack are consistently safer than baseline models: backtracking
Llama-3-8B is four times more safe than the baseline model (6.1\% $\to$ 1.5\%)
in our evaluations without regression in helpfulness. Our method additionally
provides protection against four adversarial attacks including an adaptive
attack, despite not being trained to do so.",2024-09-22,"Yiming Zhang, Jianfeng Chi, Hailey Nguyen, Kartikeya Upasani, Daniel M. Bikel, Jason Weston, Eric Michael Smith",http://arxiv.org/pdf/2409.14586v1,cs.LG
SARF: Enhancing Stock Market Prediction with Sentiment-Augmented Random Forest,"Stock trend forecasting, a challenging problem in the financial domain,
involves ex-tensive data and related indicators. Relying solely on empirical
analysis often yields unsustainable and ineffective results. Machine learning
researchers have demonstrated that the application of random forest algorithm
can enhance predictions in this context, playing a crucial auxiliary role in
forecasting stock trends. This study introduces a new approach to stock market
prediction by integrating sentiment analysis using FinGPT generative AI model
with the traditional Random Forest model. The proposed technique aims to
optimize the accuracy of stock price forecasts by leveraging the nuanced
understanding of financial sentiments provided by FinGPT. We present a new
methodology called ""Sentiment-Augmented Random Forest"" (SARF), which
in-corporates sentiment features into the Random Forest framework. Our
experiments demonstrate that SARF outperforms conventional Random Forest and
LSTM models with an average accuracy improvement of 9.23% and lower prediction
errors in pre-dicting stock market movements.",2024-09-22,"Saber Talazadeh, Dragan Perakovic",http://arxiv.org/pdf/2410.07143v1,cs.LG
Domain knowledge-guided machine learning framework for state of health estimation in Lithium-ion batteries,"Accurate estimation of battery state of health is crucial for effective
electric vehicle battery management. Here, we propose five health indicators
that can be extracted online from real-world electric vehicle operation and
develop a machine learning-based method to estimate the battery state of
health. The proposed indicators provide physical insights into the energy and
power fade of the battery and enable accurate capacity estimation even with
partially missing data. Moreover, they can be computed for portions of the
charging profile and real-world driving discharging conditions, facilitating
real-time battery degradation estimation. The indicators are computed using
experimental data from five cells aged under electric vehicle conditions, and a
linear regression model is used to estimate the state of health. The results
show that models trained with power autocorrelation and energy-based features
achieve capacity estimation with maximum absolute percentage error within 1.5%
to 2.5% .",2024-09-22,"Andrea Lanubile, Pietro Bosoni, Gabriele Pozzato, Anirudh Allam, Matteo Acquarone, Simona Onori",http://arxiv.org/pdf/2409.14575v1,cs.LG
Evaluating the Performance and Robustness of LLMs in Materials Science Q&A and Property Predictions,"Large Language Models (LLMs) have the potential to revolutionize scientific
research, yet their robustness and reliability in domain-specific applications
remain insufficiently explored. In this study, we evaluate the performance and
robustness of LLMs for materials science, focusing on domain-specific question
answering and materials property prediction across diverse real-world and
adversarial conditions. Three distinct datasets are used in this study: 1) a
set of multiple-choice questions from undergraduate-level materials science
courses, 2) a dataset including various steel compositions and yield strengths,
and 3) a band gap dataset, containing textual descriptions of material crystal
structures and band gap values. The performance of LLMs is assessed using
various prompting strategies, including zero-shot chain-of-thought, expert
prompting, and few-shot in-context learning. The robustness of these models is
tested against various forms of 'noise', ranging from realistic disturbances to
intentionally adversarial manipulations, to evaluate their resilience and
reliability under real-world conditions. Additionally, the study showcases
unique phenomena of LLMs during predictive tasks, such as mode collapse
behavior when the proximity of prompt examples is altered and performance
recovery from train/test mismatch. The findings aim to provide informed
skepticism for the broad use of LLMs in materials science and to inspire
advancements that enhance their robustness and reliability for practical
applications.",2024-09-22,"Hongchen Wang, Kangming Li, Scott Ramsay, Yao Fehlis, Edward Kim, Jason Hattrick-Simpers",http://arxiv.org/pdf/2409.14572v2,cs.LG
Floating-floating point: a highly accurate number representation with flexible Counting ranges,"Efficient number representation is essential for federated learning, natural
language processing, and network measurement solutions. Due to timing, area,
and power constraints, such applications use narrow bit-width (e.g., 8-bit)
number systems. The widely used floating-point systems exhibit a trade-off
between the counting range and accuracy. This paper introduces
Floating-Floating-Point (F2P) - a floating point number that varies the
partition between mantissa and exponent. Such flexibility leads to a large
counting range combined with improved accuracy over a selected sub-range. Our
evaluation demonstrates that moving to F2P from the state-of-the-art improves
network measurement accuracy and federated learning.",2024-09-22,"Itamar Cohen, Gil Einziger",http://arxiv.org/pdf/2410.03692v1,cs.LG
Exploiting Exogenous Structure for Sample-Efficient Reinforcement Learning,"We study Exo-MDPs, a structured class of Markov Decision Processes (MDPs)
where the state space is partitioned into exogenous and endogenous components.
Exogenous states evolve stochastically, independent of the agent's actions,
while endogenous states evolve deterministically based on both state components
and actions. Exo-MDPs are useful for applications including inventory control,
portfolio management, and ride-sharing. Our first result is structural,
establishing a representational equivalence between the classes of discrete
MDPs, Exo-MDPs, and discrete linear mixture MDPs. Specifically, any discrete
MDP can be represented as an Exo-MDP, and the transition and reward dynamics
can be written as linear functions of the exogenous state distribution, showing
that Exo-MDPs are instances of linear mixture MDPs. For unobserved exogenous
states, we prove a regret upper bound of $O(H^{3/2}d\sqrt{K})$ over $K$
trajectories of horizon $H$, with $d$ as the size of the exogenous state space,
and establish nearly-matching lower bounds. Our findings demonstrate how
Exo-MDPs decouple sample complexity from action and endogenous state sizes, and
we validate our theoretical insights with experiments on inventory control.",2024-09-22,"Jia Wan, Sean R. Sinclair, Devavrat Shah, Martin J. Wainwright",http://arxiv.org/pdf/2409.14557v3,cs.LG
Adaptive Feedforward Gradient Estimation in Neural ODEs,"Neural Ordinary Differential Equations (Neural ODEs) represent a significant
breakthrough in deep learning, promising to bridge the gap between machine
learning and the rich theoretical frameworks developed in various mathematical
fields over centuries. In this work, we propose a novel approach that leverages
adaptive feedforward gradient estimation to improve the efficiency,
consistency, and interpretability of Neural ODEs. Our method eliminates the
need for backpropagation and the adjoint method, reducing computational
overhead and memory usage while maintaining accuracy. The proposed approach has
been validated through practical applications, and showed good performance
relative to Neural ODEs state of the art methods.",2024-09-22,Jaouad Dabounou,http://arxiv.org/pdf/2409.14549v1,cs.LG
TrackNetV4: Enhancing Fast Sports Object Tracking with Motion Attention Maps,"Accurately detecting and tracking high-speed, small objects, such as balls in
sports videos, is challenging due to factors like motion blur and occlusion.
Although recent deep learning frameworks like TrackNetV1, V2, and V3 have
advanced tennis ball and shuttlecock tracking, they often struggle in scenarios
with partial occlusion or low visibility. This is primarily because these
models rely heavily on visual features without explicitly incorporating motion
information, which is crucial for precise tracking and trajectory prediction.
In this paper, we introduce an enhancement to the TrackNet family by fusing
high-level visual features with learnable motion attention maps through a
motion-aware fusion mechanism, effectively emphasizing the moving ball's
location and improving tracking performance. Our approach leverages frame
differencing maps, modulated by a motion prompt layer, to highlight key motion
regions over time. Experimental results on the tennis ball and shuttlecock
datasets show that our method enhances the tracking performance of both
TrackNetV2 and V3. We refer to our lightweight, plug-and-play solution, built
on top of the existing TrackNet, as TrackNetV4.",2024-09-22,"Arjun Raj, Lei Wang, Tom Gedeon",http://arxiv.org/pdf/2409.14543v1,cs.LG
Distributionally Robust Inverse Reinforcement Learning for Identifying Multi-Agent Coordinated Sensing,"We derive a minimax distributionally robust inverse reinforcement learning
(IRL) algorithm to reconstruct the utility functions of a multi-agent sensing
system. Specifically, we construct utility estimators which minimize the
worst-case prediction error over a Wasserstein ambiguity set centered at noisy
signal observations. We prove the equivalence between this robust estimation
and a semi-infinite optimization reformulation, and we propose a consistent
algorithm to compute solutions. We illustrate the efficacy of this robust IRL
scheme in numerical studies to reconstruct the utility functions of a cognitive
radar network from observed tracking signals.",2024-09-22,"Luke Snow, Vikram Krishnamurthy",http://arxiv.org/pdf/2409.14542v1,cs.LG
RobotFingerPrint: Unified Gripper Coordinate Space for Multi-Gripper Grasp Synthesis and Transfer,"We introduce a novel grasp representation named the Unified Gripper
Coordinate Space (UGCS) for grasp synthesis and grasp transfer. Our
representation leverages spherical coordinates to create a shared coordinate
space across different robot grippers, enabling it to synthesize and transfer
grasps for both novel objects and previously unseen grippers. The strength of
this representation lies in the ability to map palm and fingers of a gripper
and the unified coordinate space. Grasp synthesis is formulated as predicting
the unified spherical coordinates on object surface points via a conditional
variational autoencoder. The predicted unified gripper coordinates establish
exact correspondences between the gripper and object points, which is used to
optimize grasp pose and joint values. Grasp transfer is facilitated through the
point-to-point correspondence between any two (potentially unseen) grippers and
solved via a similar optimization. Extensive simulation and real-world
experiments showcase the efficacy of the unified grasp representation for grasp
synthesis in generating stable and diverse grasps. Similarly, we showcase
real-world grasp transfer from human demonstrations across different objects.",2024-09-22,"Ninad Khargonkar, Luis Felipe Casas, Balakrishnan Prabhakaran, Yu Xiang",http://arxiv.org/pdf/2409.14519v2,cs.LG
SPAQ-DL-SLAM: Towards Optimizing Deep Learning-based SLAM for Resource-Constrained Embedded Platforms,"Optimizing Deep Learning-based Simultaneous Localization and Mapping
(DL-SLAM) algorithms is essential for efficient implementation on
resource-constrained embedded platforms, enabling real-time on-board
computation in autonomous mobile robots. This paper presents SPAQ-DL-SLAM, a
framework that strategically applies Structured Pruning and Quantization (SPAQ)
to the architecture of one of the state-ofthe-art DL-SLAM algorithms,
DROID-SLAM, for resource and energy-efficiency. Specifically, we perform
structured pruning with fine-tuning based on layer-wise sensitivity analysis
followed by 8-bit post-training static quantization (PTQ) on the deep learning
modules within DROID-SLAM. Our SPAQ-DROIDSLAM model, optimized version of
DROID-SLAM model using our SPAQ-DL-SLAM framework with 20% structured pruning
and 8-bit PTQ, achieves an 18.9% reduction in FLOPs and a 79.8% reduction in
overall model size compared to the DROID-SLAM model. Our evaluations on the
TUM-RGBD benchmark shows that SPAQ-DROID-SLAM model surpasses the DROID-SLAM
model by an average of 10.5% on absolute trajectory error (ATE) metric.
Additionally, our results on the ETH3D SLAM training benchmark demonstrate
enhanced generalization capabilities of the SPAQ-DROID-SLAM model, seen by a
higher Area Under the Curve (AUC) score and success in 2 additional data
sequences compared to the DROIDSLAM model. Despite these improvements, the
model exhibits performance variance on the distinct Vicon Room sequences from
the EuRoC dataset, which are captured at high angular velocities. This varying
performance at some distinct scenarios suggests that designing DL-SLAM
algorithms taking operating environments and tasks in consideration can achieve
optimal performance and resource efficiency for deployment in
resource-constrained embedded platforms.",2024-09-22,"Niraj Pudasaini, Muhammad Abdullah Hanif, Muhammad Shafique",http://arxiv.org/pdf/2409.14515v1,cs.LG
Order of Magnitude Speedups for LLM Membership Inference,"Large Language Models (LLMs) have the promise to revolutionize computing
broadly, but their complexity and extensive training data also expose
significant privacy vulnerabilities. One of the simplest privacy risks
associated with LLMs is their susceptibility to membership inference attacks
(MIAs), wherein an adversary aims to determine whether a specific data point
was part of the model's training set. Although this is a known risk, state of
the art methodologies for MIAs rely on training multiple computationally costly
shadow models, making risk evaluation prohibitive for large models. Here we
adapt a recent line of work which uses quantile regression to mount membership
inference attacks; we extend this work by proposing a low-cost MIA that
leverages an ensemble of small quantile regression models to determine if a
document belongs to the model's training set or not. We demonstrate the
effectiveness of this approach on fine-tuned LLMs of varying families (OPT,
Pythia, Llama) and across multiple datasets. Across all scenarios we obtain
comparable or improved accuracy compared to state of the art shadow model
approaches, with as little as 6% of their computation budget. We demonstrate
increased effectiveness across multi-epoch trained target models, and
architecture miss-specification robustness, that is, we can mount an effective
attack against a model using a different tokenizer and architecture, without
requiring knowledge on the target model.",2024-09-22,"Rongting Zhang, Martin Bertran, Aaron Roth",http://arxiv.org/pdf/2409.14513v2,cs.LG
TabGraphs: A Benchmark and Strong Baselines for Learning on Graphs with Tabular Node Features,"Tabular machine learning is an important field for industry and science. In
this field, table rows are usually treated as independent data samples, but
additional information about relations between them is sometimes available and
can be used to improve predictive performance. Such information can be
naturally modeled with a graph, thus tabular machine learning may benefit from
graph machine learning methods. However, graph machine learning models are
typically evaluated on datasets with homogeneous node features, which have
little in common with heterogeneous mixtures of numerical and categorical
features present in tabular datasets. Thus, there is a critical difference
between the data used in tabular and graph machine learning studies, which does
not allow one to understand how successfully graph models can be transferred to
tabular data. To bridge this gap, we propose a new benchmark of diverse graphs
with heterogeneous tabular node features and realistic prediction tasks. We use
this benchmark to evaluate a vast set of models, including simple methods
previously overlooked in the literature. Our experiments show that graph neural
networks (GNNs) can indeed often bring gains in predictive performance for
tabular data, but standard tabular models also can be adapted to work with
graph data by using simple feature preprocessing, which sometimes enables them
to compete with and even outperform GNNs. Based on our empirical study, we
provide insights for researchers and practitioners in both tabular and graph
machine learning fields.",2024-09-22,"Gleb Bazhenov, Oleg Platonov, Liudmila Prokhorenkova",http://arxiv.org/pdf/2409.14500v2,cs.LG
Prediction and Detection of Terminal Diseases Using Internet of Medical Things: A Review,"The integration of Artificial Intelligence (AI) and the Internet of Medical
Things (IoMT) in healthcare, through Machine Learning (ML) and Deep Learning
(DL) techniques, has advanced the prediction and diagnosis of chronic diseases.
AI-driven models such as XGBoost, Random Forest, CNNs, and LSTM RNNs have
achieved over 98\% accuracy in predicting heart disease, chronic kidney disease
(CKD), Alzheimer's disease, and lung cancer, using datasets from platforms like
Kaggle, UCI, private institutions, and real-time IoMT sources. However,
challenges persist due to variations in data quality, patient demographics, and
formats from different hospitals and research sources. The incorporation of
IoMT data, which is vast and heterogeneous, adds complexities in ensuring
interoperability and security to protect patient privacy. AI models often
struggle with overfitting, performing well in controlled environments but less
effectively in real-world clinical settings. Moreover, multi-morbidity
scenarios especially for rare diseases like dementia, stroke, and cancers
remain insufficiently addressed. Future research should focus on data
standardization and advanced preprocessing techniques to improve data quality
and interoperability. Transfer learning and ensemble methods are crucial for
improving model generalizability across clinical settings. Additionally, the
exploration of disease interactions and the development of predictive models
for chronic illness intersections is needed. Creating standardized frameworks
and open-source tools for integrating federated learning, blockchain, and
differential privacy into IoMT systems will also ensure robust data privacy and
security.",2024-09-22,"Akeem Temitope Otapo, Alice Othmani, Ghazaleh Khodabandelou, Zuheng Ming",http://arxiv.org/pdf/2410.00034v1,cs.LG
SynBench: A Synthetic Benchmark for Non-rigid 3D Point Cloud Registration,"Non-rigid point cloud registration is a crucial task in computer vision.
Evaluating a non-rigid point cloud registration method requires a dataset with
challenges such as large deformation levels, noise, outliers, and
incompleteness. Despite the existence of several datasets for deformable point
cloud registration, the absence of a comprehensive benchmark with all
challenges makes it difficult to achieve fair evaluations among different
methods. This paper introduces SynBench, a new non-rigid point cloud
registration dataset created using SimTool, a toolset for soft body simulation
in Flex and Unreal Engine. SynBench provides the ground truth of corresponding
points between two point sets and encompasses key registration challenges,
including varying levels of deformation, noise, outliers, and incompleteness.
To the best of the authors' knowledge, compared to existing datasets, SynBench
possesses three particular characteristics: (1) it is the first benchmark that
provides various challenges for non-rigid point cloud registration, (2)
SynBench encompasses challenges of varying difficulty levels, and (3) it
includes ground truth corresponding points both before and after deformation.
The authors believe that SynBench enables future non-rigid point cloud
registration methods to present a fair comparison of their achievements.
SynBench is publicly available at: https://doi.org/10.11588/data/R9IKCF.",2024-09-22,"Sara Monji-Azad, Marvin Kinz, Claudia Scherl, David Männle, Jürgen Hesser, Nikolas Löw",http://arxiv.org/pdf/2409.14474v1,cs.LG
Exploring Multilingual Probing in Large Language Models: A Cross-Language Analysis,"Probing techniques for large language models (LLMs) have primarily focused on
English, overlooking the vast majority of the world's languages. In this paper,
we extend these probing methods to a multilingual context, investigating the
behaviors of LLMs across diverse languages. We conduct experiments on several
open-source LLM models, analyzing probing accuracy, trends across layers, and
similarities between probing vectors for multiple languages. Our key findings
reveal: (1) a consistent performance gap between high-resource and low-resource
languages, with high-resource languages achieving significantly higher probing
accuracy; (2) divergent layer-wise accuracy trends, where high-resource
languages show substantial improvement in deeper layers similar to English; and
(3) higher representational similarities among high-resource languages, with
low-resource languages demonstrating lower similarities both among themselves
and with high-resource languages. These results highlight significant
disparities in LLMs' multilingual capabilities and emphasize the need for
improved modeling of low-resource languages.",2024-09-22,"Daoyang Li, Haiyan Zhao, Qingcheng Zeng, Mengnan Du",http://arxiv.org/pdf/2409.14459v2,cs.LG
A High-Performance External Validity Index for Clustering with a Large Number of Clusters,"This paper introduces the Stable Matching Based Pairing (SMBP) algorithm, a
high-performance external validity index for clustering evaluation in
large-scale datasets with a large number of clusters. SMBP leverages the stable
matching framework to pair clusters across different clustering methods,
significantly reducing computational complexity to $O(N^2)$, compared to
traditional Maximum Weighted Matching (MWM) with $O(N^3)$ complexity. Through
comprehensive evaluations on real-world and synthetic datasets, SMBP
demonstrates comparable accuracy to MWM and superior computational efficiency.
It is particularly effective for balanced, unbalanced, and large-scale datasets
with a large number of clusters, making it a scalable and practical solution
for modern clustering tasks. Additionally, SMBP is easily implementable within
machine learning frameworks like PyTorch and TensorFlow, offering a robust tool
for big data applications. The algorithm is validated through extensive
experiments, showcasing its potential as a powerful alternative to existing
methods such as Maximum Match Measure (MMM) and Centroid Ratio (CR).",2024-09-22,"Mohammad Yasin Karbasian, Ramin Javadi",http://arxiv.org/pdf/2409.14455v1,cs.LG
A Unified Approach for Learning the Dynamics of Power System Generators and Inverter-based Resources,"The growing prevalence of inverter-based resources (IBRs) for renewable
energy integration and electrification greatly challenges power system dynamic
analysis. To account for both synchronous generators (SGs) and IBRs, this work
presents an approach for learning the model of an individual dynamic component.
The recurrent neural network (RNN) model is used to match the recursive
structure in predicting the key dynamical states of a component from its
terminal bus voltage and set-point input. To deal with the fast transients
especially due to IBRs, we develop a Stable Integral (SI-)RNN to mimic
high-order integral methods that can enhance the stability and accuracy for the
dynamic learning task. We demonstrate that the proposed SI-RNN model not only
can successfully predict the component's dynamic behaviors, but also offers the
possibility of efficiently computing the dynamic sensitivity relative to a
set-point change. These capabilities have been numerically validated based on
full-order Electromagnetic Transient (EMT) simulations on a small test system
with both SGs and IBRs, particularly for predicting the dynamics of
grid-forming inverters.",2024-09-22,"Shaohui Liu, Weiqian Cai, Hao Zhu, Brian Johnson",http://arxiv.org/pdf/2409.14454v1,cs.LG
SAC-KG: Exploiting Large Language Models as Skilled Automatic Constructors for Domain Knowledge Graphs,"Knowledge graphs (KGs) play a pivotal role in knowledge-intensive tasks
across specialized domains, where the acquisition of precise and dependable
knowledge is crucial. However, existing KG construction methods heavily rely on
human intervention to attain qualified KGs, which severely hinders the
practical applicability in real-world scenarios. To address this challenge, we
propose a general KG construction framework, named SAC-KG, to exploit large
language models (LLMs) as Skilled Automatic Constructors for domain Knowledge
Graph. SAC-KG effectively involves LLMs as domain experts to generate
specialized and precise multi-level KGs. Specifically, SAC-KG consists of three
components: Generator, Verifier, and Pruner. For a given entity, Generator
produces its relations and tails from raw domain corpora, to construct a
specialized single-level KG. Verifier and Pruner then work together to ensure
precision by correcting generation errors and determining whether newly
produced tails require further iteration for the next-level KG.Experiments
demonstrate that SAC-KG automatically constructs a domain KG at the scale of
over one million nodes and achieves a precision of 89.32%, leading to a
superior performance with over 20% increase in precision rate compared to
existing state-of-the-art methods for the KG construction task.",2024-09-22,"Hanzhu Chen, Xu Shen, Qitan Lv, Jie Wang, Xiaoqi Ni, Jieping Ye",http://arxiv.org/pdf/2410.02811v1,cs.LG
A Visualized Malware Detection Framework with CNN and Conditional GAN,"Malware visualization analysis incorporating with Machine Learning (ML) has
been proven to be a promising solution for improving security defenses on
different platforms. In this work, we propose an integrated framework for
addressing common problems experienced by ML utilizers in developing malware
detection systems. Namely, a pictorial presentation system with extensions is
designed to preserve the identities of benign/malign samples by encoding each
variable into binary digits and mapping them into black and white pixels. A
conditional Generative Adversarial Network based model is adopted to produce
synthetic images and mitigate issues of imbalance classes. Detection models
architected by Convolutional Neural Networks are for validating performances
while training on datasets with and without artifactual samples. Result
demonstrates accuracy rates of 98.51% and 97.26% for these two training
scenarios.",2024-09-22,"Fang Wang, Hussam Al Hamadi, Ernesto Damiani",http://arxiv.org/pdf/2409.14439v1,cs.LG
Challenging the Performance-Interpretability Trade-off: An Evaluation of Interpretable Machine Learning Models,"Machine learning is permeating every conceivable domain to promote
data-driven decision support. The focus is often on advanced black-box models
due to their assumed performance advantages, whereas interpretable models are
often associated with inferior predictive qualities. More recently, however, a
new generation of generalized additive models (GAMs) has been proposed that
offer promising properties for capturing complex, non-linear patterns while
remaining fully interpretable. To uncover the merits and limitations of these
models, this study examines the predictive performance of seven different GAMs
in comparison to seven commonly used machine learning models based on a
collection of twenty tabular benchmark datasets. To ensure a fair and robust
model comparison, an extensive hyperparameter search combined with
cross-validation was performed, resulting in 68,500 model runs. In addition,
this study qualitatively examines the visual output of the models to assess
their level of interpretability. Based on these results, the paper dispels the
misconception that only black-box models can achieve high accuracy by
demonstrating that there is no strict trade-off between predictive performance
and model interpretability for tabular data. Furthermore, the paper discusses
the importance of GAMs as powerful interpretable models for the field of
information systems and derives implications for future work from a
socio-technical perspective.",2024-09-22,"Sven Kruschel, Nico Hambauer, Sven Weinzierl, Sandra Zilker, Mathias Kraus, Patrick Zschech",http://arxiv.org/pdf/2409.14429v1,cs.LG
COSBO: Conservative Offline Simulation-Based Policy Optimization,"Offline reinforcement learning allows training reinforcement learning models
on data from live deployments. However, it is limited to choosing the best
combination of behaviors present in the training data. In contrast, simulation
environments attempting to replicate the live environment can be used instead
of the live data, yet this approach is limited by the simulation-to-reality
gap, resulting in a bias. In an attempt to get the best of both worlds, we
propose a method that combines an imperfect simulation environment with data
from the target environment, to train an offline reinforcement learning policy.
Our experiments demonstrate that the proposed method outperforms
state-of-the-art approaches CQL, MOPO, and COMBO, especially in scenarios with
diverse and challenging dynamics, and demonstrates robust behavior across a
variety of experimental conditions. The results highlight that using
simulator-generated data can effectively enhance offline policy learning
despite the sim-to-real gap, when direct interaction with the real-world is not
possible.",2024-09-22,"Eshagh Kargar, Ville Kyrki",http://arxiv.org/pdf/2409.14412v1,cs.LG
Investigating the Impact of Hard Samples on Accuracy Reveals In-class Data Imbalance,"In the AutoML domain, test accuracy is heralded as the quintessential metric
for evaluating model efficacy, underpinning a wide array of applications from
neural architecture search to hyperparameter optimization. However, the
reliability of test accuracy as the primary performance metric has been called
into question, notably through research highlighting how label noise can
obscure the true ranking of state-of-the-art models. We venture beyond, along
another perspective where the existence of hard samples within datasets casts
further doubt on the generalization capabilities inferred from test accuracy
alone. Our investigation reveals that the distribution of hard samples between
training and test sets affects the difficulty levels of those sets, thereby
influencing the perceived generalization capability of models. We unveil two
distinct generalization pathways-toward easy and hard samples-highlighting the
complexity of achieving balanced model evaluation. Finally, we propose a
benchmarking procedure for comparing hard sample identification methods,
facilitating the advancement of more nuanced approaches in this area. Our
primary goal is not to propose a definitive solution but to highlight the
limitations of relying primarily on test accuracy as an evaluation metric, even
when working with balanced datasets, by introducing the in-class data imbalance
problem. By doing so, we aim to stimulate a critical discussion within the
research community and open new avenues for research that consider a broader
spectrum of model evaluation criteria. The anonymous code is available at
https://github.com/PawPuk/CurvBIM blueunder the GPL-3.0 license.",2024-09-22,"Pawel Pukowski, Haiping Lu",http://arxiv.org/pdf/2409.14401v1,cs.LG
Flat-LoRA: Low-Rank Adaptation over a Flat Loss Landscape,"Fine-tuning large-scale pre-trained models is prohibitively expensive in
terms of computation and memory costs. Low-Rank Adaptation (LoRA), a popular
Parameter-Efficient Fine-Tuning (PEFT) method, offers an efficient solution by
optimizing only low-rank matrices. Despite recent progress in improving LoRA's
performance, the relationship between the LoRA optimization space and the full
parameter space is often overlooked. A solution that appears flat in the loss
landscape of the LoRA space may still exhibit sharp directions in the full
parameter space, potentially compromising generalization. We introduce
Flat-LoRA, which aims to identify a low-rank adaptation situated in a flat
region of the full parameter space. Instead of adopting the well-established
sharpness-aware minimization approach, which incurs significant computation and
memory overheads, we employ a Bayesian expectation loss objective to preserve
training efficiency. Further, we design a refined random perturbation
generation strategy for improved performance and carefully manage memory
overhead using random seeds. Experiments across diverse tasks-including
mathematical reasoning, coding abilities, dialogue generation, instruction
following, and text-to-image generation-demonstrate that Flat-LoRA improves
both in-domain and out-of-domain generalization. Code is available at
https://github.com/nblt/Flat-LoRA.",2024-09-22,"Tao Li, Zhengbao He, Yujun Li, Yasheng Wang, Lifeng Shang, Xiaolin Huang",http://arxiv.org/pdf/2409.14396v2,cs.LG
Investigating Layer Importance in Large Language Models,"Large language models (LLMs) have gained increasing attention due to their
prominent ability to understand and process texts. Nevertheless, LLMs largely
remain opaque. The lack of understanding of LLMs has obstructed the deployment
in safety-critical scenarios and hindered the development of better models. In
this study, we advance the understanding of LLM by investigating the
significance of individual layers in LLMs. We propose an efficient sampling
method to faithfully evaluate the importance of layers using Shapley values, a
widely used explanation framework in feature attribution and data valuation. In
addition, we conduct layer ablation experiments to assess the performance
degradation resulting from the exclusion of specific layers. Our findings
reveal the existence of cornerstone layers, wherein certain early layers can
exhibit a dominant contribution over others. Removing one cornerstone layer
leads to a drastic collapse of the model performance, often reducing it to
random guessing. Conversely, removing non-cornerstone layers results in only
marginal performance changes. This study identifies cornerstone layers in LLMs
and underscores their critical role for future research.",2024-09-22,"Yang Zhang, Yanfei Dong, Kenji Kawaguchi",http://arxiv.org/pdf/2409.14381v1,cs.LG
Sparse Low-Ranked Self-Attention Transformer for Remaining Useful Lifetime Prediction of Optical Fiber Amplifiers,"Optical fiber amplifiers are key elements in present optical networks.
Failures of these components result in high financial loss of income of the
network operator as the communication traffic over an affected link is
interrupted. Applying Remaining useful lifetime (RUL) prediction in the context
of Predictive Maintenance (PdM) to optical fiber amplifiers to predict upcoming
system failures at an early stage, so that network outages can be minimized
through planning of targeted maintenance actions, ensures reliability and
safety. Optical fiber amplifier are complex systems, that work under various
operating conditions, which makes correct forecasting a difficult task.
Increased monitoring capabilities of systems results in datasets that
facilitate the application of data-driven RUL prediction methods. Deep learning
models in particular have shown good performance, but generalization based on
comparatively small datasets for RUL prediction is difficult. In this paper, we
propose Sparse Low-ranked self-Attention Transformer (SLAT) as a novel RUL
prediction method. SLAT is based on an encoder-decoder architecture, wherein
two parallel working encoders extract features for sensors and time steps. By
utilizing the self-attention mechanism, long-term dependencies can be learned
from long sequences. The implementation of sparsity in the attention matrix and
a low-rank parametrization reduce overfitting and increase generalization.
Experimental application to optical fiber amplifiers exemplified on EDFA, as
well as a reference dataset from turbofan engines, shows that SLAT outperforms
the state-of-the-art methods.",2024-09-22,"Dominic Schneider, Lutz Rapp",http://arxiv.org/pdf/2409.14378v3,cs.LG
Using Natural Language Processing to find Indication for Burnout with Text Classification: From Online Data to Real-World Data,"Burnout, classified as a syndrome in the ICD-11, arises from chronic
workplace stress that has not been effectively managed. It is characterized by
exhaustion, cynicism, and reduced professional efficacy, and estimates of its
prevalence vary significantly due to inconsistent measurement methods. Recent
advancements in Natural Language Processing (NLP) and machine learning offer
promising tools for detecting burnout through textual data analysis, with
studies demonstrating high predictive accuracy. This paper contributes to
burnout detection in German texts by: (a) collecting an anonymous real-world
dataset including free-text answers and Oldenburg Burnout Inventory (OLBI)
responses; (b) demonstrating the limitations of a GermanBERT-based classifier
trained on online data; (c) presenting two versions of a curated
BurnoutExpressions dataset, which yielded models that perform well in
real-world applications; and (d) providing qualitative insights from an
interdisciplinary focus group on the interpretability of AI models used for
burnout detection. Our findings emphasize the need for greater collaboration
between AI researchers and clinical experts to refine burnout detection models.
Additionally, more real-world data is essential to validate and enhance the
effectiveness of current AI methods developed in NLP research, which are often
based on data automatically scraped from online sources and not evaluated in a
real-world context. This is essential for ensuring AI tools are well suited for
practical applications.",2024-09-22,"Mascha Kurpicz-Briki, Ghofrane Merhbene, Alexandre Puttick, Souhir Ben Souissi, Jannic Bieri, Thomas Jörg Müller, Christoph Golz",http://arxiv.org/pdf/2409.14357v1,cs.LG
A Feature Engineering Approach for Literary and Colloquial Tamil Speech Classification using 1D-CNN,"In ideal human computer interaction (HCI), the colloquial form of a language
would be preferred by most users, since it is the form used in their day-to-day
conversations. However, there is also an undeniable necessity to preserve the
formal literary form. By embracing the new and preserving the old, both service
to the common man (practicality) and service to the language itself
(conservation) can be rendered. Hence, it is ideal for computers to have the
ability to accept, process, and converse in both forms of the language, as
required. To address this, it is first necessary to identify the form of the
input speech, which in the current work is between literary and colloquial
Tamil speech. Such a front-end system must consist of a simple, effective, and
lightweight classifier that is trained on a few effective features that are
capable of capturing the underlying patterns of the speech signal. To
accomplish this, a one-dimensional convolutional neural network (1D-CNN) that
learns the envelope of features across time, is proposed. The network is
trained on a select number of handcrafted features initially, and then on Mel
frequency cepstral coefficients (MFCC) for comparison. The handcrafted features
were selected to address various aspects of speech such as the spectral and
temporal characteristics, prosody, and voice quality. The features are
initially analyzed by considering ten parallel utterances and observing the
trend of each feature with respect to time. The proposed 1D-CNN, trained using
the handcrafted features, offers an F1 score of 0.9803, while that trained on
the MFCC offers an F1 score of 0.9895. In light of this, feature ablation and
feature combination are explored. When the best ranked handcrafted features,
from the feature ablation study, are combined with the MFCC, they offer the
best results with an F1 score of 0.9946.",2024-09-22,"M. Nanmalar, S. Johanan Joysingh, P. Vijayalakshmi, T. Nagarajan",http://arxiv.org/pdf/2409.14348v1,cs.LG
Self-Supervised Audio-Visual Soundscape Stylization,"Speech sounds convey a great deal of information about the scenes, resulting
in a variety of effects ranging from reverberation to additional ambient
sounds. In this paper, we manipulate input speech to sound as though it was
recorded within a different scene, given an audio-visual conditional example
recorded from that scene. Our model learns through self-supervision, taking
advantage of the fact that natural video contains recurring sound events and
textures. We extract an audio clip from a video and apply speech enhancement.
We then train a latent diffusion model to recover the original speech, using
another audio-visual clip taken from elsewhere in the video as a conditional
hint. Through this process, the model learns to transfer the conditional
example's sound properties to the input speech. We show that our model can be
successfully trained using unlabeled, in-the-wild videos, and that an
additional visual signal can improve its sound prediction abilities. Please see
our project webpage for video results:
https://tinglok.netlify.app/files/avsoundscape/",2024-09-22,"Tingle Li, Renhao Wang, Po-Yao Huang, Andrew Owens, Gopala Anumanchipalli",http://arxiv.org/pdf/2409.14340v1,cs.LG
Transforming Multidimensional Time Series into Interpretable Event Sequences for Advanced Data Mining,"This paper introduces a novel spatiotemporal feature representation model
designed to address the limitations of traditional methods in multidimensional
time series (MTS) analysis. The proposed approach converts MTS into
one-dimensional sequences of spatially evolving events, preserving the complex
coupling relationships between dimensions. By employing a variable-length tuple
mining method, key spatiotemporal features are extracted, enhancing the
interpretability and accuracy of time series analysis. Unlike conventional
models, this unsupervised method does not rely on large training datasets,
making it adaptable across different domains. Experimental results from motion
sequence classification validate the model's superior performance in capturing
intricate patterns within the data. The proposed framework has significant
potential for applications across various fields, including backend services
for monitoring and optimizing IT infrastructure, medical diagnosis through
continuous patient monitoring and health trend analysis, and internet
businesses for tracking user behavior and forecasting sales. This work offers a
new theoretical foundation and technical support for advancing time series data
mining and its practical applications in human behavior recognition and other
domains.",2024-09-22,"Xu Yan, Yaoting Jiang, Wenyi Liu, Didi Yi, Jianjun Wei",http://arxiv.org/pdf/2409.14327v2,cs.LG
Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses,"Large language models (LLMs) equipped with chain-of-thoughts (CoT) prompting
have shown significant multi-step reasoning capabilities in factual content
like mathematics, commonsense, and logic. However, their performance in
narrative reasoning, which demands greater abstraction capabilities, remains
unexplored. This study utilizes tropes in movie synopses to assess the abstract
reasoning abilities of state-of-the-art LLMs and uncovers their low
performance. We introduce a trope-wise querying approach to address these
challenges and boost the F1 score by 11.8 points. Moreover, while prior studies
suggest that CoT enhances multi-step reasoning, this study shows CoT can cause
hallucinations in narrative content, reducing GPT-4's performance. We also
introduce an Adversarial Injection method to embed trope-related text tokens
into movie synopses without explicit tropes, revealing CoT's heightened
sensitivity to such injections. Our comprehensive analysis provides insights
for future research directions.",2024-09-22,"Hung-Ting Su, Ya-Ching Hsu, Xudong Lin, Xiang-Qian Shi, Yulei Niu, Han-Yuan Hsu, Hung-yi Lee, Winston H. Hsu",http://arxiv.org/pdf/2409.14324v1,cs.LG
Sketch 'n Solve: An Efficient Python Package for Large-Scale Least Squares Using Randomized Numerical Linear Algebra,"We present Sketch 'n Solve, an open-source Python package that implements
efficient randomized numerical linear algebra (RandNLA) techniques for solving
large-scale least squares problems. While sketch-and-solve algorithms have
demonstrated theoretical promise, their practical adoption has been limited by
the lack of robust, user-friendly implementations. Our package addresses this
gap by providing an optimized implementation built on NumPy and SciPy,
featuring both dense and sparse sketching operators with a clean API. Through
extensive benchmarking, we demonstrate that our implementation achieves up to
50x speedup over traditional LSQR while maintaining high accuracy, even for
ill-conditioned matrices. The package shows particular promise for applications
in machine learning optimization, signal processing, and scientific computing.",2024-09-22,Alex Lavaee,http://arxiv.org/pdf/2409.14309v2,cs.LG
Coverage and Bias of Street View Imagery in Mapping the Urban Environment,"Street View Imagery (SVI) has emerged as a valuable data form in urban
studies, enabling new ways to map and sense urban environments. However,
fundamental concerns regarding the representativeness, quality, and reliability
of SVI remain underexplored, e.g. to what extent can cities be captured by such
data and do data gaps result in bias. This research, positioned at the
intersection of spatial data quality and urban analytics, addresses these
concerns by proposing a novel and effective method to estimate SVI's
element-level coverage in the urban environment. The method integrates the
positional relationships between SVI and target elements, as well as the impact
of physical obstructions. Expanding the domain of data quality to SVI, we
introduce an indicator system that evaluates the extent of coverage, focusing
on the completeness and frequency dimensions. Taking London as a case study,
three experiments are conducted to identify potential biases in SVI's ability
to cover and represent urban environmental elements, using building facades as
an example. It is found that despite their high availability along urban road
networks, Google Street View covers only 62.4 % of buildings in the case study
area. The average facade coverage per building is 12.4 %. SVI tends to
over-represent non-residential buildings, thus possibly resulting in biased
analyses, and its coverage of environmental elements is position-dependent. The
research also highlights the variability of SVI coverage under different data
acquisition practices and proposes an optimal sampling interval range of 50-60
m for SVI collection. The findings suggest that while SVI offers valuable
insights, it is no panacea - its application in urban research requires careful
consideration of data coverage and element-level representativeness to ensure
reliable results.",2024-09-22,"Zicheng Fan, Chen-Chieh Feng, Filip Biljecki",http://arxiv.org/pdf/2409.15386v3,cs.LG
A competitive baseline for deep learning enhanced data assimilation using conditional Gaussian ensemble Kalman filtering,"Ensemble Kalman Filtering (EnKF) is a popular technique for data
assimilation, with far ranging applications. However, the vanilla EnKF
framework is not well-defined when perturbations are nonlinear. We study two
non-linear extensions of the vanilla EnKF - dubbed the conditional-Gaussian
EnKF (CG-EnKF) and the normal score EnKF (NS-EnKF) - which sidestep assumptions
of linearity by constructing the Kalman gain matrix with the `conditional
Gaussian' update formula in place of the traditional one. We then compare these
models against a state-of-the-art deep learning based particle filter called
the score filter (SF). This model uses an expensive score diffusion model for
estimating densities and also requires a strong assumption on the perturbation
operator for validity. In our comparison, we find that CG-EnKF and NS-EnKF
dramatically outperform SF for a canonical problem in high-dimensional
multiscale data assimilation given by the Lorenz-96 system. Our analysis also
demonstrates that the CG-EnKF and NS-EnKF can handle highly non-Gaussian
additive noise perturbations, with the latter typically outperforming the
former.",2024-09-22,"Zachariah Malik, Romit Maulik",http://arxiv.org/pdf/2409.14300v1,cs.LG
Towards Within-Class Variation in Alzheimer's Disease Detection from Spontaneous Speech,"Alzheimer's Disease (AD) detection has emerged as a promising research area
that employs machine learning classification models to distinguish between
individuals with AD and those without. Unlike conventional classification
tasks, we identify within-class variation as a critical challenge in AD
detection: individuals with AD exhibit a spectrum of cognitive impairments.
Given that many AD detection tasks lack fine-grained labels, simplistic binary
classification may overlook two crucial aspects: within-class differences and
instance-level imbalance. The former compels the model to map AD samples with
varying degrees of impairment to a single diagnostic label, disregarding
certain changes in cognitive function. While the latter biases the model
towards overrepresented severity levels. This work presents early efforts to
address these challenges. We propose two novel methods: Soft Target
Distillation (SoTD) and Instance-level Re-balancing (InRe), targeting two
problems respectively. Experiments on the ADReSS and ADReSSo datasets
demonstrate that the proposed methods significantly improve detection accuracy.
Further analysis reveals that SoTD effectively harnesses the strengths of
multiple component models, while InRe substantially alleviates model
over-fitting. These findings provide insights for developing more robust and
reliable AD detection models.",2024-09-22,"Jiawen Kang, Dongrui Han, Lingwei Meng, Jingyan Zhou, Jinchao Li, Xixin Wu, Helen Meng",http://arxiv.org/pdf/2409.16322v1,cs.LG
Opinion Mining on Offshore Wind Energy for Environmental Engineering,"In this paper, we conduct sentiment analysis on social media data to study
mass opinion about offshore wind energy. We adapt three machine learning
models, namely, TextBlob, VADER, and SentiWordNet because different functions
are provided by each model. TextBlob provides subjectivity analysis as well as
polarity classification. VADER offers cumulative sentiment scores. SentiWordNet
considers sentiments with reference to context and performs classification
accordingly. Techniques in NLP are harnessed to gather meaning from the textual
data in social media. Data visualization tools are suitably deployed to display
the overall results. This work is much in line with citizen science and smart
governance via involvement of mass opinion to guide decision support. It
exemplifies the role of Machine Learning and NLP here.",2024-09-22,"Isabele Bittencourt, Aparna S. Varde, Pankaj Lal",http://arxiv.org/pdf/2409.14292v1,cs.LG
Causal Inference with Double/Debiased Machine Learning for Evaluating the Health Effects of Multiple Mismeasured Pollutants,"One way to quantify exposure to air pollution and its constituents in
epidemiologic studies is to use an individual's nearest monitor. This strategy
results in potential inaccuracy in the actual personal exposure, introducing
bias in estimating the health effects of air pollution and its constituents,
especially when evaluating the causal effects of correlated multi-pollutant
constituents measured with correlated error. This paper addresses estimation
and inference for the causal effect of one constituent in the presence of other
PM2.5 constituents, accounting for measurement error and correlations. We used
a linear regression calibration model, fitted with generalized estimating
equations in an external validation study, and extended a double/debiased
machine learning (DML) approach to correct for measurement error and estimate
the effect of interest in the main study. We demonstrated that the DML
estimator with regression calibration is consistent and derived its asymptotic
variance. Simulations showed that the proposed estimator reduced bias and
attained nominal coverage probability across most simulation settings. We
applied this method to assess the causal effects of PM2.5 constituents on
cognitive function in the Nurses' Health Study and identified two PM2.5
constituents, Br and Mn, that showed a negative causal effect on cognitive
function after measurement error correction.",2024-09-22,"Gang Xu, Xin Zhou, Molin Wang, Boya Zhang, Wenhao Jiang, Francine Laden, Helen H. Suh, Adam A. Szpiro, Donna Spiegelman, Zuoheng Wang",http://arxiv.org/pdf/2410.07135v1,cs.LG
Diagnosis and Pathogenic Analysis of Autism Spectrum Disorder Using Fused Brain Connection Graph,"We propose a model for diagnosing Autism spectrum disorder (ASD) using
multimodal magnetic resonance imaging (MRI) data. Our approach integrates brain
connectivity data from diffusion tensor imaging (DTI) and functional MRI
(fMRI), employing graph neural networks (GNNs) for fused graph classification.
To improve diagnostic accuracy, we introduce a loss function that maximizes
inter-class and minimizes intra-class margins. We also analyze network node
centrality, calculating degree, subgraph, and eigenvector centralities on a
bimodal fused brain graph to identify pathological regions linked to ASD. Two
non-parametric tests assess the statistical significance of these centralities
between ASD patients and healthy controls. Our results reveal consistency
between the tests, yet the identified regions differ significantly across
centralities, suggesting distinct physiological interpretations. These findings
enhance our understanding of ASD's neurobiological basis and offer new
directions for clinical diagnosis.",2024-09-22,"Lu Wei, Yi Huang, Guosheng Yin, Fode Zhang, Manxue Zhang, Bin Liu",http://arxiv.org/pdf/2410.07138v1,cs.LG
Accelerated Stochastic ExtraGradient: Mixing Hessian and Gradient Similarity to Reduce Communication in Distributed and Federated Learning,"Modern realities and trends in learning require more and more generalization
ability of models, which leads to an increase in both models and training
sample size. It is already difficult to solve such tasks in a single device
mode. This is the reason why distributed and federated learning approaches are
becoming more popular every day. Distributed computing involves communication
between devices, which requires solving two key problems: efficiency and
privacy. One of the most well-known approaches to combat communication costs is
to exploit the similarity of local data. Both Hessian similarity and
homogeneous gradients have been studied in the literature, but separately. In
this paper, we combine both of these assumptions in analyzing a new method that
incorporates the ideas of using data similarity and clients sampling. Moreover,
to address privacy concerns, we apply the technique of additional noise and
analyze its impact on the convergence of the proposed method. The theory is
confirmed by training on real datasets.",2024-09-22,"Dmitry Bylinkin, Kirill Degtyarev, Aleksandr Beznosikov",http://arxiv.org/pdf/2409.14280v1,cs.LG
Proof Automation with Large Language Models,"Interactive theorem provers such as Coq are powerful tools to formally
guarantee the correctness of software. However, using these tools requires
significant manual effort and expertise. While Large Language Models (LLMs)
have shown promise in automatically generating informal proofs in natural
language, they are less effective at generating formal proofs in interactive
theorem provers. In this paper, we conduct a formative study to identify common
mistakes made by LLMs when asked to generate formal proofs. By analyzing 520
proof generation errors made by GPT-3.5, we found that GPT-3.5 often identified
the correct high-level structure of a proof, but struggled to get the
lower-level details correct. Based on this insight, we propose PALM, a novel
generate-then-repair approach that first prompts an LLM to generate an initial
proof and then leverages targeted symbolic methods to iteratively repair
low-level problems. We evaluate PALM on a large dataset that includes more than
10K theorems. Our results show that PALM significantly outperforms other
state-of-the-art approaches, successfully proving 76.6% to 180.4% more
theorems. Moreover, PALM proves 1270 theorems beyond the reach of existing
approaches. We also demonstrate the generalizability of PALM across different
LLMs.",2024-09-22,"Minghai Lu, Benjamin Delaware, Tianyi Zhang",http://arxiv.org/pdf/2409.14274v1,cs.LG
FeDETR: a Federated Approach for Stenosis Detection in Coronary Angiography,"Assessing the severity of stenoses in coronary angiography is critical to the
patient's health, as coronary stenosis is an underlying factor in heart
failure. Current practice for grading coronary lesions, i.e. fractional flow
reserve (FFR) or instantaneous wave-free ratio (iFR), suffers from several
drawbacks, including time, cost and invasiveness, alongside potential
interobserver variability. In this context, some deep learning methods have
emerged to assist cardiologists in automating the estimation of FFR/iFR values.
Despite the effectiveness of these methods, their reliance on large datasets is
challenging due to the distributed nature of sensitive medical data. Federated
learning addresses this challenge by aggregating knowledge from multiple nodes
to improve model generalization, while preserving data privacy. We propose the
first federated detection transformer approach, FeDETR, to assess stenosis
severity in angiography videos based on FFR/iFR values estimation. In our
approach, each node trains a detection transformer (DETR) on its local dataset,
with the central server federating the backbone part of the network. The
proposed method is trained and evaluated on a dataset collected from five
hospitals, consisting of 1001 angiographic examinations, and its performance is
compared with state-of-the-art federated learning methods.",2024-09-21,"Raffaele Mineo, Amelia Sorrenti, Federica Proietto Salanitri",http://arxiv.org/pdf/2409.14268v1,cs.LG
Structure Learning via Mutual Information,"This paper presents a novel approach to machine learning algorithm design
based on information theory, specifically mutual information (MI). We propose a
framework for learning and representing functional relationships in data using
MI-based features. Our method aims to capture the underlying structure of
information in datasets, enabling more efficient and generalizable learning
algorithms. We demonstrate the efficacy of our approach through experiments on
synthetic and real-world datasets, showing improved performance in tasks such
as function classification, regression, and cross-dataset transfer. This work
contributes to the growing field of metalearning and automated machine
learning, offering a new perspective on how to leverage information theory for
algorithm design and dataset analysis and proposing new mutual information
theoretic foundations to learning algorithms.",2024-09-21,Jeremy Nixon,http://arxiv.org/pdf/2409.14235v1,cs.LG
ReFine: Boosting Time Series Prediction of Extreme Events by Reweighting and Fine-tuning,"Extreme events are of great importance since they often represent impactive
occurrences. For instance, in terms of climate and weather, extreme events
might be major storms, floods, extreme heat or cold waves, and more. However,
they are often located at the tail of the data distribution. Consequently,
accurately predicting these extreme events is challenging due to their rarity
and irregularity. Prior studies have also referred to this as the
out-of-distribution (OOD) problem, which occurs when the distribution of the
test data is substantially different from that used for training. In this work,
we propose two strategies, reweighting and fine-tuning, to tackle the
challenge. Reweighting is a strategy used to force machine learning models to
focus on extreme events, which is achieved by a weighted loss function that
assigns greater penalties to the prediction errors for the extreme samples
relative to those on the remainder of the data. Unlike previous intuitive
reweighting methods based on simple heuristics of data distribution, we employ
meta-learning to dynamically optimize these penalty weights. To further boost
the performance on extreme samples, we start from the reweighted models and
fine-tune them using only rare extreme samples. Through extensive experiments
on multiple data sets, we empirically validate that our meta-learning-based
reweighting outperforms existing heuristic ones, and the fine-tuning strategy
can further increase the model performance. More importantly, these two
strategies are model-agnostic, which can be implemented on any type of neural
network for time series forecasting. The open-sourced code is available at
\url{https://github.com/JimengShi/ReFine}.",2024-09-21,"Jimeng Shi, Azam Shirali, Giri Narasimhan",http://arxiv.org/pdf/2409.14232v1,cs.LG
R-AIF: Solving Sparse-Reward Robotic Tasks from Pixels with Active Inference and World Models,"Although research has produced promising results demonstrating the utility of
active inference (AIF) in Markov decision processes (MDPs), there is relatively
less work that builds AIF models in the context of environments and problems
that take the form of partially observable Markov decision processes (POMDPs).
In POMDP scenarios, the agent must infer the unobserved environmental state
from raw sensory observations, e.g., pixels in an image. Additionally, less
work exists in examining the most difficult form of POMDP-centered control:
continuous action space POMDPs under sparse reward signals. In this work, we
address issues facing the AIF modeling paradigm by introducing novel prior
preference learning techniques and self-revision schedules to help the agent
excel in sparse-reward, continuous action, goal-based robotic control POMDP
environments. Empirically, we show that our agents offer improved performance
over state-of-the-art models in terms of cumulative rewards, relative
stability, and success rate. The code in support of this work can be found at
https://github.com/NACLab/robust-active-inference.",2024-09-21,"Viet Dung Nguyen, Zhizhuo Yang, Christopher L. Buckley, Alexander Ororbia",http://arxiv.org/pdf/2409.14216v1,cs.LG
Data-centric NLP Backdoor Defense from the Lens of Memorization,"Backdoor attack is a severe threat to the trustworthiness of DNN-based
language models. In this paper, we first extend the definition of memorization
of language models from sample-wise to more fine-grained sentence element-wise
(e.g., word, phrase, structure, and style), and then point out that language
model backdoors are a type of element-wise memorization. Through further
analysis, we find that the strength of such memorization is positively
correlated to the frequency of duplicated elements in the training dataset. In
conclusion, duplicated sentence elements are necessary for successful backdoor
attacks. Based on this, we propose a data-centric defense. We first detect
trigger candidates in training data by finding memorizable elements, i.e.,
duplicated elements, and then confirm real triggers by testing if the
candidates can activate backdoor behaviors (i.e., malicious elements). Results
show that our method outperforms state-of-the-art defenses in defending against
different types of NLP backdoors.",2024-09-21,"Zhenting Wang, Zhizhi Wang, Mingyu Jin, Mengnan Du, Juan Zhai, Shiqing Ma",http://arxiv.org/pdf/2409.14200v1,cs.LG
"Advancing Employee Behavior Analysis through Synthetic Data: Leveraging ABMs, GANs, and Statistical Models for Enhanced Organizational Efficiency","Success in todays data-driven corporate climate requires a deep understanding
of employee behavior. Companies aim to improve employee satisfaction, boost
output, and optimize workflow. This research study delves into creating
synthetic data, a powerful tool that allows us to comprehensively understand
employee performance, flexibility, cooperation, and team dynamics. Synthetic
data provides a detailed and accurate picture of employee activities while
protecting individual privacy thanks to cutting-edge methods like agent-based
models (ABMs), Generative Adversarial Networks (GANs), and statistical models.
Through the creation of multiple situations, this method offers insightful
viewpoints regarding increasing teamwork, improving adaptability, and
accelerating overall productivity. We examine how synthetic data has evolved
from a specialized field to an essential resource for researching employee
behavior and enhancing management efficiency. Keywords: Agent-Based Model,
Generative Adversarial Network, workflow optimization, organizational success",2024-09-21,"Rakshitha Jayashankar, Mahesh Balan",http://arxiv.org/pdf/2409.14197v1,cs.LG
On Lexical Invariance on Multisets and Graphs,"In this draft, we study a novel problem, called lexical invariance, using the
medium of multisets and graphs. Traditionally in the NLP domain, lexical
invariance indicates that the semantic meaning of a sentence should remain
unchanged regardless of the specific lexical or word-based representation of
the input. For example, ``The movie was extremely entertaining'' would have the
same meaning as ``The film was very enjoyable''. In this paper, we study a more
challenging setting, where the output of a function is invariant to any
injective transformation applied to the input lexical space. For example,
multiset {1,2,3,2} is equivalent to multiset {a,b,c,b} if we specify an
injective transformation that maps 1 to a, 2 to b and 3 to c. We study the
sufficient and necessary conditions for a most expressive lexical invariant
(and permutation invariant) function on multisets and graphs, and proves that
for multisets, the function must have a form that only takes the multiset of
counts of the unique elements in the original multiset as input. For example, a
most expressive lexical invariant function on {a,b,c,b} must have a form that
only operates on {1,1,2} (meaning that there are 1, 1, 2 unique elements
corresponding to a,c,b). For graphs, we prove that a most expressive lexical
invariant and permutation invariant function must have a form that only takes
the adjacency matrix and a difference matrix as input, where the (i,j)th
element of the difference matrix is 1 if node i and node j have the same
feature and 0 otherwise. We perform synthetic experiments on TU datasets to
verify our theorems.",2024-09-21,Muhan Zhang,http://arxiv.org/pdf/2409.14179v1,cs.LG
A Distribution-Aware Flow-Matching for Generating Unstructured Data for Few-Shot Reinforcement Learning,"Generating realistic and diverse unstructured data is a significant challenge
in reinforcement learning (RL), particularly in few-shot learning scenarios
with limited data availability. Traditional RL methods often rely on real data
for exploration, which can be time-consuming and inefficient. In this paper, we
introduce a distribution-aware flow matching approach designed to generate
synthetic unstructured data, specifically tailored for the few-shot RL
application of Dynamic Voltage and Frequency Scaling (DVFS) on embedded
processors. Our method leverages the flow matching algorithm as a
sample-efficient generative model and incorporates bootstrapping techniques to
enhance latent space diversity and generalization. Additionally, we apply
feature weighting using Random Forests to prioritize critical features,
improving the precision of the generated synthetic data.
  Our approach addresses key challenges in traditional model-based RL, such as
overfitting and data correlation, while aligning with the principles of the Law
of Large Numbers to support empirical consistency and policy improvement as the
number of samples increases. We validate our approach through extensive
experimentation on a DVFS application for low-energy processing. Results
demonstrate that our method achieves stable convergence in terms of maximum
Q-value while enhancing frame rates by 30\% in the initial timestamps. These
improvements make the proposed RL model more efficient in resource-constrained
environments.",2024-09-21,"Mohammad Pivezhandi, Abusayeed Saifullah",http://arxiv.org/pdf/2409.14178v2,cs.LG
QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling,"Large Language models (LLMs) have brought about substantial advancements in
the field of Question Answering (QA) systems. These models do remarkably well
in addressing intricate inquiries in a variety of disciplines. However, because
of domain-specific vocabulary, complex technological concepts, and the
requirement for exact responses applying LLMs to specialized sectors like
telecommunications presents additional obstacles. GPT-3.5 has been used in
recent work, to obtain noteworthy accuracy for telecom-related questions in a
Retrieval Augmented Generation (RAG) framework. Notwithstanding these
developments, the practical use of models such as GPT-3.5 is restricted by
their proprietary nature and high computing demands. This paper introduces
QMOS, an innovative approach which uses a Question-Masked loss and Option
Shuffling trick to enhance the performance of LLMs in answering Multiple-Choice
Questions in the telecommunications domain. Our focus was on using opensource,
smaller language models (Phi-2 and Falcon-7B) within an enhanced RAG framework.
Our multi-faceted approach involves several enhancements to the whole LLM-RAG
pipeline of finetuning, retrieval, prompt engineering and inference. Our
approaches significantly outperform existing results, achieving accuracy
improvements from baselines of 24.70% to 49.30% with Falcon-7B and from 42.07%
to 84.65% with Phi-2.",2024-09-21,"Blessed Guda, Gabrial Zencha Ashungafac, Lawrence Francis, Carlee Joe-Wong",http://arxiv.org/pdf/2409.14175v2,cs.LG
BurstM: Deep Burst Multi-scale SR using Fourier Space with Optical Flow,"Multi frame super-resolution(MFSR) achieves higher performance than single
image super-resolution (SISR), because MFSR leverages abundant information from
multiple frames. Recent MFSR approaches adapt the deformable convolution
network (DCN) to align the frames. However, the existing MFSR suffers from
misalignments between the reference and source frames due to the limitations of
DCN, such as small receptive fields and the predefined number of kernels. From
these problems, existing MFSR approaches struggle to represent high-frequency
information. To this end, we propose Deep Burst Multi-scale SR using Fourier
Space with Optical Flow (BurstM). The proposed method estimates the optical
flow offset for accurate alignment and predicts the continuous Fourier
coefficient of each frame for representing high-frequency textures. In
addition, we have enhanced the network flexibility by supporting various
super-resolution (SR) scale factors with the unimodel. We demonstrate that our
method has the highest performance and flexibility than the existing MFSR
methods. Our source code is available at https://github.com/Egkang-Luis/burstm",2024-09-21,"EungGu Kang, Byeonghun Lee, Sunghoon Im, Kyong Hwan Jin",http://arxiv.org/pdf/2409.15384v1,cs.LG
Component-based Sketching for Deep ReLU Nets,"Deep learning has made profound impacts in the domains of data mining and AI,
distinguished by the groundbreaking achievements in numerous real-world
applications and the innovative algorithm design philosophy. However, it
suffers from the inconsistency issue between optimization and generalization,
as achieving good generalization, guided by the bias-variance trade-off
principle, favors under-parameterized networks, whereas ensuring effective
convergence of gradient-based algorithms demands over-parameterized networks.
To address this issue, we develop a novel sketching scheme based on deep net
components for various tasks. Specifically, we use deep net components with
specific efficacy to build a sketching basis that embodies the advantages of
deep networks. Subsequently, we transform deep net training into a linear
empirical risk minimization problem based on the constructed basis,
successfully avoiding the complicated convergence analysis of iterative
algorithms. The efficacy of the proposed component-based sketching is validated
through both theoretical analysis and numerical experiments. Theoretically, we
show that the proposed component-based sketching provides almost optimal rates
in approximating saturated functions for shallow nets and also achieves almost
optimal generalization error bounds. Numerically, we demonstrate that, compared
with the existing gradient-based training methods, component-based sketching
possesses superior generalization performance with reduced training costs.",2024-09-21,"Di Wang, Shao-Bo Lin, Deyu Meng, Feilong Cao",http://arxiv.org/pdf/2409.14174v1,cs.LG
Towards Building Efficient Sentence BERT Models using Layer Pruning,"This study examines the effectiveness of layer pruning in creating efficient
Sentence BERT (SBERT) models. Our goal is to create smaller sentence embedding
models that reduce complexity while maintaining strong embedding similarity. We
assess BERT models like Muril and MahaBERT-v2 before and after pruning,
comparing them with smaller, scratch-trained models like MahaBERT-Small and
MahaBERT-Smaller. Through a two-phase SBERT fine-tuning process involving
Natural Language Inference (NLI) and Semantic Textual Similarity (STS), we
evaluate the impact of layer reduction on embedding quality. Our findings show
that pruned models, despite fewer layers, perform competitively with fully
layered versions. Moreover, pruned models consistently outperform similarly
sized, scratch-trained models, establishing layer pruning as an effective
strategy for creating smaller, efficient embedding models. These results
highlight layer pruning as a practical approach for reducing computational
demand while preserving high-quality embeddings, making SBERT models more
accessible for languages with limited technological resources.",2024-09-21,"Anushka Shelke, Riya Savant, Raviraj Joshi",http://arxiv.org/pdf/2409.14168v1,cs.LG
A Survey on Large Language Model-empowered Autonomous Driving,"Artificial intelligence (AI) plays a crucial role in autonomous driving (AD)
research, propelling its development towards intelligence and efficiency.
Currently, the development of AD technology follows two main technical paths:
modularization and end-to-end. Modularization decompose the driving task into
modules such as perception, prediction, planning, and control, and train them
separately. Due to the inconsistency of training objectives between modules,
the integrated effect suffers from bias. End-to-end attempts to address this
issue by utilizing a single model that directly maps from sensor data to
control signals. This path has limited learning capabilities in a comprehensive
set of features and struggles to handle unpredictable long-tail events and
complex urban traffic scenarios. In the face of challenges encountered in both
paths, many researchers believe that large language models (LLMs) with powerful
reasoning capabilities and extensive knowledge understanding may be the
solution, expecting LLMs to provide AD systems with deeper levels of
understanding and decision-making capabilities. In light of the challenges
faced by both paths, many researchers believe that LLMs, with their powerful
reasoning abilities and extensive knowledge, could offer a solution. To
understand if LLMs could enhance AD, this paper conducts a thorough analysis of
the potential applications of LLMs in AD systems, including exploring their
optimization strategies in both modular and end-to-end approaches, with a
particular focus on how LLMs can tackle the problems and challenges present in
current solutions. Furthermore, we discuss an important question: Can LLM-based
artificial general intelligence (AGI) be a key to achieve high-level AD? We
further analyze the potential limitations and challenges that LLMs may
encounter in promoting the development of AD technology.",2024-09-21,"Yuxuan Zhu, Shiyi Wang, Wenqing Zhong, Nianchen Shen, Yunqi Li, Siqi Wang, Zhiheng Li, Cathy Wu, Zhengbing He, Li Li",http://arxiv.org/pdf/2409.14165v3,cs.LG
PromptTA: Prompt-driven Text Adapter for Source-free Domain Generalization,"Source-free domain generalization (SFDG) tackles the challenge of adapting
models to unseen target domains without access to source domain data. To deal
with this challenging task, recent advances in SFDG have primarily focused on
leveraging the text modality of vision-language models such as CLIP. These
methods involve developing a transferable linear classifier based on diverse
style features extracted from the text and learned prompts or deriving
domain-unified text representations from domain banks. However, both style
features and domain banks have limitations in capturing comprehensive domain
knowledge. In this work, we propose Prompt-Driven Text Adapter (PromptTA)
method, which is designed to better capture the distribution of style features
and employ resampling to ensure thorough coverage of domain knowledge. To
further leverage this rich domain information, we introduce a text adapter that
learns from these style features for efficient domain information storage.
Extensive experiments conducted on four benchmark datasets demonstrate that
PromptTA achieves state-of-the-art performance. The code is available at
https://github.com/zhanghr2001/PromptTA.",2024-09-21,"Haoran Zhang, Shuanghao Bai, Wanqi Zhou, Jingwen Fu, Badong Chen",http://arxiv.org/pdf/2409.14163v1,cs.LG
On Importance of Pruning and Distillation for Efficient Low Resource NLP,"The rise of large transformer models has revolutionized Natural Language
Processing, leading to significant advances in tasks like text classification.
However, this progress demands substantial computational resources, escalating
training duration, and expenses with larger model sizes. Efforts have been made
to downsize and accelerate English models (e.g., Distilbert, MobileBert). Yet,
research in this area is scarce for low-resource languages.
  In this study, we explore the case of the low-resource Indic language
Marathi. Leveraging the marathi-topic-all-doc-v2 model as our baseline, we
implement optimization techniques to reduce computation time and memory usage.
Our focus is on enhancing the efficiency of Marathi transformer models while
maintaining top-tier accuracy and reducing computational demands. Using the
MahaNews document classification dataset and the marathi-topic-all-doc-v2 model
from L3Cube, we apply Block Movement Pruning, Knowledge Distillation, and Mixed
Precision methods individually and in combination to boost efficiency. We
demonstrate the importance of strategic pruning levels in achieving desired
efficiency gains. Furthermore, we analyze the balance between efficiency
improvements and environmental impact, highlighting how optimized model
architectures can contribute to a more sustainable computational ecosystem.
Implementing these techniques on a single GPU system, we determine that the
optimal configuration is 25\% pruning + knowledge distillation. This approach
yielded a 2.56x speedup in computation time while maintaining baseline accuracy
levels.",2024-09-21,"Aishwarya Mirashi, Purva Lingayat, Srushti Sonavane, Tejas Padhiyar, Raviraj Joshi, Geetanjali Kale",http://arxiv.org/pdf/2409.14162v1,cs.LG
When Witnesses Defend: A Witness Graph Topological Layer for Adversarial Graph Learning,"Capitalizing on the intuitive premise that shape characteristics are more
robust to perturbations, we bridge adversarial graph learning with the emerging
tools from computational topology, namely, persistent homology representations
of graphs. We introduce the concept of witness complex to adversarial analysis
on graphs, which allows us to focus only on the salient shape characteristics
of graphs, yielded by the subset of the most essential nodes (i.e., landmarks),
with minimal loss of topological information on the whole graph. The remaining
nodes are then used as witnesses, governing which higher-order graph
substructures are incorporated into the learning process. Armed with the
witness mechanism, we design Witness Graph Topological Layer (WGTL), which
systematically integrates both local and global topological graph feature
representations, the impact of which is, in turn, automatically controlled by
the robust regularized topological loss. Given the attacker's budget, we derive
the important stability guarantees of both local and global topology encodings
and the associated robust topological loss. We illustrate the versatility and
efficiency of WGTL by its integration with five GNNs and three existing
non-topological defense mechanisms. Our extensive experiments across six
datasets demonstrate that WGTL boosts the robustness of GNNs across a range of
perturbations and against a range of adversarial attacks. Our datasets and
source codes are available at https://github.com/toggled/WGTL.",2024-09-21,"Naheed Anjum Arafat, Debabrota Basu, Yulia Gel, Yuzhou Chen",http://arxiv.org/pdf/2409.14161v3,cs.LG
Towards Automated Patent Workflows: AI-Orchestrated Multi-Agent Framework for Intellectual Property Management and Analysis,"Patents are the currency of innovation, and like any currency, they need to
be managed and protected (Gavin Potenza). Patents, as legal documents that
secure intellectual property rights, play a critical role in technological
innovation. The growing complexity of patent documents and the surge in patent
applications have created a need for automated solutions in patent analysis. In
this work, we present PatExpert, an autonomous multi-agent conversational
framework designed to streamline and optimize patent-related tasks. The
framework consists of a metaagent that coordinates task-specific expert agents
for various patent-related tasks and a critique agent for error handling and
feedback provision. The meta-agent orchestrates specialized expert agents, each
fine-tuned for specific tasks such as patent classification, acceptance, claim
generation, abstractive summarization, multi-patent analysis, and scientific
hypothesis generation. For multi-patent analysis, the framework incorporates
advanced methods like Graph Retrieval-Augmented Generation (GRAG) to enhance
response accuracy and relevance by combining semantic similarity with knowledge
graphs. Error handling is managed by critique agents (Gold-LLM-as-a-Judge and
Reward-LLM-as-a-Judge), which evaluate output responses for accuracy and
provide iterative feedback. The framework also prioritizes explainability,
ensuring transparent justifications for decisions made during patent analysis.
Its comprehensive capabilities make it a valuable tool for automating complex
patent workflows, enhancing efficiency, accuracy, and compliance in
patent-related tasks. Empirical evidence demonstrates significant improvements
in patent processing tasks, concluding that the framework offers a robust
solution for automating and optimizing patent analysis.",2024-09-21,"Sakhinana Sagar Srinivas, Vijay Sri Vaikunth, Venkataramana Runkana",http://arxiv.org/pdf/2409.19006v2,cs.LG
Are Music Foundation Models Better at Singing Voice Deepfake Detection? Far-Better Fuse them with Speech Foundation Models,"In this study, for the first time, we extensively investigate whether music
foundation models (MFMs) or speech foundation models (SFMs) work better for
singing voice deepfake detection (SVDD), which has recently attracted attention
in the research community. For this, we perform a comprehensive comparative
study of state-of-the-art (SOTA) MFMs (MERT variants and music2vec) and SFMs
(pre-trained for general speech representation learning as well as speaker
recognition). We show that speaker recognition SFM representations perform the
best amongst all the foundation models (FMs), and this performance can be
attributed to its higher efficacy in capturing the pitch, tone, intensity, etc,
characteristics present in singing voices. To our end, we also explore the
fusion of FMs for exploiting their complementary behavior for improved SVDD,
and we propose a novel framework, FIONA for the same. With FIONA, through the
synchronization of x-vector (speaker recognition SFM) and MERT-v1-330M (MFM),
we report the best performance with the lowest Equal Error Rate (EER) of 13.74
%, beating all the individual FMs as well as baseline FM fusions and achieving
SOTA results.",2024-09-21,"Orchid Chetia Phukan, Sarthak Jain, Swarup Ranjan Behera, Arun Balaji Buduru, Rajesh Sharma, S. R Mahadeva Prasanna",http://arxiv.org/pdf/2409.14131v1,cs.LG
Present and Future Generalization of Synthetic Image Detectors,"The continued release of increasingly realistic image generation models
creates a demand for synthetic image detectors. To build effective detectors we
must first understand how factors like data source diversity, training
methodologies and image alterations affect their generalization capabilities.
This work conducts a systematic analysis and uses its insights to develop
practical guidelines for training robust synthetic image detectors. Model
generalization capabilities are evaluated across different setups (e.g. scale,
sources, transformations) including real-world deployment conditions. Through
an extensive benchmarking of state-of-the-art detectors across diverse and
recent datasets, we show that while current approaches excel in specific
scenarios, no single detector achieves universal effectiveness. Critical flaws
are identified in detectors, and workarounds are proposed to enable the
deployment of real-world detector applications enhancing accuracy, reliability
and robustness beyond the limitations of current systems.",2024-09-21,"Pablo Bernabeu-Perez, Enrique Lopez-Cuena, Dario Garcia-Gasulla",http://arxiv.org/pdf/2409.14128v2,cs.LG
A General Framework of the Consistency for Large Neural Networks,"Neural networks have shown remarkable success, especially in
overparameterized or ""large"" models. Despite increasing empirical evidence and
intuitive understanding, a formal mathematical justification for the behavior
of such models, particularly regarding overfitting, remains incomplete. In this
paper, we propose a general regularization framework to study the Mean
Integrated Squared Error (MISE) of neural networks. This framework includes
many commonly used neural networks and penalties, such as ReLu and Sigmoid
activations and $L^1$, $L^2$ penalties. Based on our frameworks, we find the
MISE curve has two possible shapes, namely the shape of double descents and
monotone decreasing. The latter phenomenon is new in literature and the causes
of these two phenomena are also studied in theory. These studies challenge
conventional statistical modeling frameworks and broadens recent findings on
the double descent phenomenon in neural networks.",2024-09-21,"Haoran Zhan, Yingcun Xia",http://arxiv.org/pdf/2409.14123v2,cs.LG
Efficient and Effective Model Extraction,"Model extraction aims to create a functionally similar copy from a machine
learning as a service (MLaaS) API with minimal overhead, typically for illicit
profit or as a precursor to further attacks, posing a significant threat to the
MLaaS ecosystem. However, recent studies have shown that model extraction is
highly inefficient, particularly when the target task distribution is
unavailable. In such cases, even substantially increasing the attack budget
fails to produce a sufficiently similar replica, reducing the adversary's
motivation to pursue extraction attacks. In this paper, we revisit the
elementary design choices throughout the extraction lifecycle. We propose an
embarrassingly simple yet dramatically effective algorithm, Efficient and
Effective Model Extraction (E3), focusing on both query preparation and
training routine. E3 achieves superior generalization compared to
state-of-the-art methods while minimizing computational costs. For instance,
with only 0.005 times the query budget and less than 0.2 times the runtime, E3
outperforms classical generative model based data-free model extraction by an
absolute accuracy improvement of over 50% on CIFAR-10. Our findings underscore
the persistent threat posed by model extraction and suggest that it could serve
as a valuable benchmarking algorithm for future security evaluations.",2024-09-21,"Hongyu Zhu, Wentao Hu, Sichu Liang, Fangqi Li, Wenwen Wang, Shilin Wang",http://arxiv.org/pdf/2409.14122v2,cs.LG
CONGRA: Benchmarking Automatic Conflict Resolution,"Resolving conflicts from merging different software versions is a challenging
task. To reduce the overhead of manual merging, researchers develop various
program analysis-based tools which only solve specific types of conflicts and
have a limited scope of application. With the development of language models,
researchers treat conflict code as text, which theoretically allows for
addressing almost all types of conflicts. However, the absence of effective
conflict difficulty grading methods hinders a comprehensive evaluation of large
language models (LLMs), making it difficult to gain a deeper understanding of
their limitations. Furthermore, there is a notable lack of large-scale open
benchmarks for evaluating the performance of LLMs in automatic conflict
resolution. To address these issues, we introduce ConGra, a CONflict-GRAded
benchmarking scheme designed to evaluate the performance of software merging
tools under varying complexity conflict scenarios. We propose a novel approach
to classify conflicts based on code operations and use it to build a
large-scale evaluation dataset based on 44,948 conflicts from 34 real-world
projects. We evaluate state-of-the-art LLMs on conflict resolution tasks using
this dataset. By employing the dataset, we assess the performance of multiple
state-of-the-art LLMs and code LLMs, ultimately uncovering two counterintuitive
yet insightful phenomena. ConGra will be released at
https://github.com/HKU-System-Security-Lab/ConGra.",2024-09-21,"Qingyu Zhang, Liangcai Su, Kai Ye, Chenxiong Qian",http://arxiv.org/pdf/2409.14121v1,cs.LG
Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm,"Parameter-efficient fine-tuning (PEFT) has become a key training strategy for
large language models. However, its reliance on fewer trainable parameters
poses security risks, such as task-agnostic backdoors. Despite their severe
impact on a wide range of tasks, there is no practical defense solution
available that effectively counters task-agnostic backdoors within the context
of PEFT. In this study, we introduce Obliviate, a PEFT-integrable backdoor
defense. We develop two techniques aimed at amplifying benign neurons within
PEFT layers and penalizing the influence of trigger tokens. Our evaluations
across three major PEFT architectures show that our method can significantly
reduce the attack success rate of the state-of-the-art task-agnostic backdoors
(83.6%$\downarrow$). Furthermore, our method exhibits robust defense
capabilities against both task-specific backdoors and adaptive attacks. Source
code will be obtained at https://github.com/obliviateARR/Obliviate.",2024-09-21,"Jaehan Kim, Minkyoo Song, Seung Ho Na, Seungwon Shin",http://arxiv.org/pdf/2409.14119v3,cs.LG
Generalization in birdsong classification: impact of transfer learning methods and dataset characteristics,"Animal sounds can be recognised automatically by machine learning, and this
has an important role to play in biodiversity monitoring. Yet despite
increasingly impressive capabilities, bioacoustic species classifiers still
exhibit imbalanced performance across species and habitats, especially in
complex soundscapes. In this study, we explore the effectiveness of transfer
learning in large-scale bird sound classification across various conditions,
including single- and multi-label scenarios, and across different model
architectures such as CNNs and Transformers. Our experiments demonstrate that
both fine-tuning and knowledge distillation yield strong performance, with
cross-distillation proving particularly effective in improving in-domain
performance on Xeno-canto data. However, when generalizing to soundscapes,
shallow fine-tuning exhibits superior performance compared to knowledge
distillation, highlighting its robustness and constrained nature. Our study
further investigates how to use multi-species labels, in cases where these are
present but incomplete. We advocate for more comprehensive labeling practices
within the animal sound community, including annotating background species and
providing temporal details, to enhance the training of robust bird sound
classifiers. These findings provide insights into the optimal reuse of
pretrained models for advancing automatic bioacoustic recognition.",2024-09-21,"Burooj Ghani, Vincent J. Kalkman, Bob Planqué, Willem-Pier Vellinga, Lisa Gill, Dan Stowell",http://arxiv.org/pdf/2409.15383v1,cs.LG
ESDS: AI-Powered Early Stunting Detection and Monitoring System using Edited Radius-SMOTE Algorithm,"Stunting detection is a significant issue in Indonesian healthcare, causing
lower cognitive function, lower productivity, a weakened immunity, delayed
neuro-development, and degenerative diseases. In regions with a high prevalence
of stunting and limited welfare resources, identifying children in need of
treatment is critical. The diagnostic process often raises challenges, such as
the lack of experience in medical workers, incompatible anthropometric
equipment, and inefficient medical bureaucracy. To counteract the issues, the
use of load cell sensor and ultrasonic sensor can provide suitable
anthropometric equipment and streamline the medical bureaucracy for stunting
detection. This paper also employs machine learning for stunting detection
based on sensor readings. The experiment results show that the sensitivity of
the load cell sensor and the ultrasonic sensor is 0.9919 and 0.9986,
respectively. Also, the machine learning test results have three classification
classes, which are normal, stunted, and stunting with an accuracy rate of 98\%.",2024-09-21,"A. A. Gde Yogi Pramana, Haidar Muhammad Zidan, Muhammad Fazil Maulana, Oskar Natan",http://arxiv.org/pdf/2409.14105v1,cs.LG
Quantum enhanced stratification of Breast Cancer: exploring quantum expressivity for real omics data,"Quantum Machine Learning (QML) is considered one of the most promising
applications of Quantum Computing in the Noisy Intermediate Scale Quantum
(NISQ) era for the impact it is thought to have in the near future. Although
promising theoretical assumptions, the exploration of how QML could foster new
discoveries in Medicine and Biology fields is still in its infancy with few
examples. In this study, we aimed to assess whether Quantum Kernels (QK) could
effectively classify subtypes of Breast Cancer (BC) patients on the basis of
molecular characteristics. We performed an heuristic exploration of encoding
configurations with different entanglement levels to determine a trade-off
between kernel expressivity and performances. Our results show that QKs yield
comparable clustering results with classical methods while using fewer data
points, and are able to fit the data with a higher number of clusters.
Additionally, we conducted the experiments on the Quantum Processing Unit (QPU)
to evaluate the effect of noise on the outcome. We found that less expressive
encodings showed a higher resilience to noise, indicating that the
computational pipeline can be reliably implemented on the NISQ devices. Our
findings suggest that QK methods show promises for application in Precision
Oncology, especially in scenarios where the dataset is limited in size and a
granular non-trivial stratification of complex molecular data cannot be
achieved classically.",2024-09-21,"Valeria Repetto, Elia Giuseppe Ceroni, Giuseppe Buonaiuto, Romina D'Aurizio",http://arxiv.org/pdf/2409.14089v1,cs.LG
BRep Boundary and Junction Detection for CAD Reverse Engineering,"In machining process, 3D reverse engineering of the mechanical system is an
integral, highly important, and yet time consuming step to obtain parametric
CAD models from 3D scans. Therefore, deep learning-based Scan-to-CAD modeling
can offer designers enormous editability to quickly modify CAD model, being
able to parse all its structural compositions and design steps. In this paper,
we propose a supervised boundary representation (BRep) detection network
BRepDetNet from 3D scans of CC3D and ABC dataset. We have carefully annotated
the 50K and 45K scans of both the datasets with appropriate topological
relations (e.g., next, mate, previous) between the geometrical primitives
(i.e., boundaries, junctions, loops, faces) of their BRep data structures. The
proposed solution decomposes the Scan-to-CAD problem in Scan-to-BRep ensuring
the right step towards feature-based modeling, and therefore, leveraging other
existing BRep-to-CAD modeling methods. Our proposed Scan-to-BRep neural network
learns to detect BRep boundaries and junctions by minimizing focal-loss and
non-maximal suppression (NMS) during training time. Experimental results show
that our BRepDetNet with NMS-Loss achieves impressive results.",2024-09-21,"Sk Aziz Ali, Mohammad Sadil Khan, Didier Stricker",http://arxiv.org/pdf/2409.14087v1,cs.LG
AMT-APC: Automatic Piano Cover by Fine-Tuning an Automatic Music Transcription Model,"There have been several studies on automatically generating piano covers, and
recent advancements in deep learning have enabled the creation of more
sophisticated covers. However, existing automatic piano cover models still have
room for improvement in terms of expressiveness and fidelity to the original.
To address these issues, we propose a learning algorithm called AMT-APC, which
leverages the capabilities of automatic music transcription models. By
utilizing the strengths of well-established automatic music transcription
models, we aim to improve the accuracy of piano cover generation. Our
experiments demonstrate that the AMT-APC model reproduces original tracks more
accurately than any existing models.",2024-09-21,"Kazuma Komiya, Yoshihisa Fukuhara",http://arxiv.org/pdf/2409.14086v1,cs.LG
One-shot World Models Using a Transformer Trained on a Synthetic Prior,"A World Model is a compressed spatial and temporal representation of a real
world environment that allows one to train an agent or execute planning
methods. However, world models are typically trained on observations from the
real world environment, and they usually do not enable learning policies for
other real environments. We propose One-Shot World Model (OSWM), a transformer
world model that is learned in an in-context learning fashion from purely
synthetic data sampled from a prior distribution. Our prior is composed of
multiple randomly initialized neural networks, where each network models the
dynamics of each state and reward dimension of a desired target environment. We
adopt the supervised learning procedure of Prior-Fitted Networks by masking
next-state and reward at random context positions and query OSWM to make
probabilistic predictions based on the remaining transition context. During
inference time, OSWM is able to quickly adapt to the dynamics of a simple grid
world, as well as the CartPole gym and a custom control environment by
providing 1k transition steps as context and is then able to successfully train
environment-solving agent policies. However, transferring to more complex
environments remains a challenge, currently. Despite these limitations, we see
this work as an important stepping-stone in the pursuit of learning world
models purely from synthetic data.",2024-09-21,"Fabio Ferreira, Moreno Schlageter, Raghu Rajan, Andre Biedenkapp, Frank Hutter",http://arxiv.org/pdf/2409.14084v2,cs.LG
Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation,"Recent studies show that text-to-image (T2I) models are vulnerable to
adversarial attacks, especially with noun perturbations in text prompts. In
this study, we investigate the impact of adversarial attacks on different POS
tags within text prompts on the images generated by T2I models. We create a
high-quality dataset for realistic POS tag token swapping and perform
gradient-based attacks to find adversarial suffixes that mislead T2I models
into generating images with altered tokens. Our empirical results show that the
attack success rate (ASR) varies significantly among different POS tag
categories, with nouns, proper nouns, and adjectives being the easiest to
attack. We explore the mechanism behind the steering effect of adversarial
suffixes, finding that the number of critical tokens and content fusion vary
among POS tags, while features like suffix transferability are consistent
across categories. We have made our implementation publicly available at -
https://github.com/shahariar-shibli/Adversarial-Attack-on-POS-Tags.",2024-09-21,"G M Shahariar, Jia Chen, Jiachen Li, Yue Dong",http://arxiv.org/pdf/2409.15381v1,cs.LG
KALIE: Fine-Tuning Vision-Language Models for Open-World Manipulation without Robot Data,"Building generalist robotic systems involves effectively endowing robots with
the capabilities to handle novel objects in an open-world setting. Inspired by
the advances of large pre-trained models, we propose Keypoint Affordance
Learning from Imagined Environments (KALIE), which adapts pre-trained Vision
Language Models (VLMs) for robotic control in a scalable manner. Instead of
directly producing motor commands, KALIE controls the robot by predicting
point-based affordance representations based on natural language instructions
and visual observations of the scene. The VLM is trained on 2D images with
affordances labeled by humans, bypassing the need for training data collected
on robotic systems. Through an affordance-aware data synthesis pipeline, KALIE
automatically creates massive high-quality training data based on limited
example data manually collected by humans. We demonstrate that KALIE can learn
to robustly solve new manipulation tasks with unseen objects given only 50
example data points. Compared to baselines using pre-trained VLMs, our approach
consistently achieves superior performance.",2024-09-21,"Grace Tang, Swetha Rajkumar, Yifei Zhou, Homer Rich Walke, Sergey Levine, Kuan Fang",http://arxiv.org/pdf/2409.14066v1,cs.LG
Temporally Consistent Factuality Probing for Large Language Models,"The prolific use of Large Language Models (LLMs) as an alternate knowledge
base requires them to be factually consistent, necessitating both correctness
and consistency traits for paraphrased queries. Recently, significant attempts
have been made to benchmark datasets and metrics to evaluate LLMs for these
traits. However, structural simplicity (subject-relation-object) and
contemporary association in their query formulation limit the broader
definition of factuality and consistency. In this study, we introduce TeCFaP, a
novel Temporally Consistent Factuality Probe task to expand the consistent
factuality probe in the temporal dimension. To this end, we propose TEMP-COFAC,
a high-quality dataset of prefix-style English query paraphrases. Subsequently,
we extend the definitions of existing metrics to represent consistent
factuality across temporal dimension. We experiment with a diverse set of LLMs
and find most of them performing poorly on TeCFaP. Next, we propose a novel
solution CoTSeLF (Consistent-Time-Sensitive Learning Framework) combining
multi-task instruction tuning (MT-IT) with consistent-time-sensitive
reinforcement learning (CTSRL) to improve temporally consistent factuality in
LLMs. Our experiments demonstrate the efficacy of CoTSeLF over several
baselines.",2024-09-21,"Ashutosh Bajpai, Aaryan Goyal, Atif Anwer, Tanmoy Chakraborty",http://arxiv.org/pdf/2409.14065v2,cs.LG
Recovering Global Data Distribution Locally in Federated Learning,"Federated Learning (FL) is a distributed machine learning paradigm that
enables collaboration among multiple clients to train a shared model without
sharing raw data. However, a major challenge in FL is the label imbalance,
where clients may exclusively possess certain classes while having numerous
minority and missing classes. Previous works focus on optimizing local updates
or global aggregation but ignore the underlying imbalanced label distribution
across clients. In this paper, we propose a novel approach ReGL to address this
challenge, whose key idea is to Recover the Global data distribution Locally.
Specifically, each client uses generative models to synthesize images that
complement the minority and missing classes, thereby alleviating label
imbalance. Moreover, we adaptively fine-tune the image generation process using
local real data, which makes the synthetic images align more closely with the
global distribution. Importantly, both the generation and fine-tuning processes
are conducted at the client-side without leaking data privacy. Through
comprehensive experiments on various image classification datasets, we
demonstrate the remarkable superiority of our approach over existing
state-of-the-art works in fundamentally tackling label imbalance in FL.",2024-09-21,Ziyu Yao,http://arxiv.org/pdf/2409.14063v1,cs.LG
WeatherFormer: Empowering Global Numerical Weather Forecasting with Space-Time Transformer,"Numerical Weather Prediction (NWP) system is an infrastructure that exerts
considerable impacts on modern society.Traditional NWP system, however,
resolves it by solving complex partial differential equations with a huge
computing cluster, resulting in tons of carbon emission. Exploring efficient
and eco-friendly solutions for NWP attracts interest from Artificial
Intelligence (AI) and earth science communities. To narrow the performance gap
between the AI-based methods and physic predictor, this work proposes a new
transformer-based NWP framework, termed as WeatherFormer, to model the complex
spatio-temporal atmosphere dynamics and empowering the capability of
data-driven NWP. WeatherFormer innovatively introduces the space-time
factorized transformer blocks to decrease the parameters and memory
consumption, in which Position-aware Adaptive Fourier Neural Operator (PAFNO)
is proposed for location sensible token mixing. Besides, two data augmentation
strategies are utilized to boost the performance and decrease training
consumption. Extensive experiments on WeatherBench dataset show WeatherFormer
achieves superior performance over existing deep learning methods and further
approaches the most advanced physical model.",2024-09-21,"Junchao Gong, Tao Han, Kang Chen, Lei Bai",http://arxiv.org/pdf/2409.16321v1,cs.LG
Implicit Neural Representations for Speed-of-Sound Estimation in Ultrasound,"Accurate estimation of the speed-of-sound (SoS) is important for ultrasound
(US) image reconstruction techniques and tissue characterization. Various
approaches have been proposed to calculate SoS, ranging from
tomography-inspired algorithms like CUTE to convolutional networks, and more
recently, physics-informed optimization frameworks based on differentiable
beamforming. In this work, we utilize implicit neural representations (INRs)
for SoS estimation in US. INRs are a type of neural network architecture that
encodes continuous functions, such as images or physical quantities, through
the weights of a network. Implicit networks may overcome the current
limitations of SoS estimation techniques, which mainly arise from the use of
non-adaptable and oversimplified physical models of tissue. Moreover,
convolutional networks for SoS estimation, usually trained using simulated
data, often fail when applied to real tissues due to out-of-distribution and
data-shift issues. In contrast, implicit networks do not require extensive
training datasets since each implicit network is optimized for an individual
data case. This adaptability makes them suitable for processing US data
collected from varied tissues and across different imaging protocols.
  We evaluated the proposed SoS estimation method based on INRs using data
collected from a tissue-mimicking phantom containing four cylindrical
inclusions, with SoS values ranging from 1480 m/s to 1600 m/s. The inclusions
were immersed in a material with an SoS value of 1540 m/s. In experiments, the
proposed method achieved strong performance, clearly demonstrating the
usefulness of implicit networks for quantitative US applications.",2024-09-21,"Michal Byra, Piotr Jarosik, Piotr Karwat, Ziemowit Klimonda, Marcin Lewandowski",http://arxiv.org/pdf/2409.14035v1,cs.LG
StateAct: Enhancing LLM Base Agents via Self-prompting and State-tracking,"Large language models (LLMs) are increasingly used as autonomous agents,
tackling tasks from robotics to web navigation. Their performance depends on
the underlying base agent. Existing methods, however, struggle with
long-context reasoning and goal adherence. We introduce StateAct, a novel and
efficient base agent that enhances decision-making through (1) self-prompting,
which reinforces task goals at every step, and (2) chain-of-states, an
extension of chain-of-thought that tracks state information over time. StateAct
outperforms ReAct, the previous best base agent, by over 10% on Alfworld, 30%
on Textcraft, and 7% on Webshop across multiple frontier LLMs. We also
demonstrate that StateAct can be used as a drop-in replacement for ReAct with
advanced LLM agent methods such as test-time scaling, yielding an additional
12% gain on Textcraft. By improving efficiency and long-range reasoning without
requiring additional training or retrieval, StateAct provides a scalable
foundation for LLM agents. We open source our code to support further research
at https://github.com/ai-nikolai/stateact .",2024-09-21,"Nikolai Rozanov, Marek Rei",http://arxiv.org/pdf/2410.02810v3,cs.LG
FAMOUS: Flexible Accelerator for the Attention Mechanism of Transformer on UltraScale+ FPGAs,"Transformer neural networks (TNNs) are being applied across a widening range
of application domains, including natural language processing (NLP), machine
translation, and computer vision (CV). Their popularity is largely attributed
to the exceptional performance of their multi-head self-attention blocks when
analyzing sequential data and extracting features. To date, there are limited
hardware accelerators tailored for this mechanism, which is the first step
before designing an accelerator for a complete model. This paper proposes
\textit{FAMOUS}, a flexible hardware accelerator for dense multi-head attention
(MHA) computation of TNNs on field-programmable gate arrays (FPGAs). It is
optimized for high utilization of processing elements and on-chip memories to
improve parallelism and reduce latency. An efficient tiling of large matrices
has been employed to distribute memory and computing resources across different
modules on various FPGA platforms. The design is evaluated on Xilinx Alveo U55C
and U200 data center cards containing Ultrascale+ FPGAs. Experimental results
are presented that show that it can attain a maximum throughput, number of
parallel attention heads, embedding dimension and tile size of 328 (giga
operations/second (GOPS)), 8, 768 and 64 respectively on the U55C. Furthermore,
it is 3.28$\times$ and 2.6$\times$ faster than the Intel Xeon Gold 5220R CPU
and NVIDIA V100 GPU respectively. It is also 1.3$\times$ faster than the
fastest state-of-the-art FPGA-based accelerator.",2024-09-21,"Ehsan Kabir, Md. Arafat Kabir, Austin R. J. Downey, Jason D. Bakos, David Andrews, Miaoqing Huang",http://arxiv.org/pdf/2409.14023v2,cs.LG
Enhancing Multivariate Time Series-based Solar Flare Prediction with Multifaceted Preprocessing and Contrastive Learning,"Accurate solar flare prediction is crucial due to the significant risks that
intense solar flares pose to astronauts, space equipment, and satellite
communication systems. Our research enhances solar flare prediction by
utilizing advanced data preprocessing and classification methods on a
multivariate time series-based dataset of photospheric magnetic field
parameters. First, our study employs a novel preprocessing pipeline that
includes missing value imputation, normalization, balanced sampling, near
decision boundary sample removal, and feature selection to significantly boost
prediction accuracy. Second, we integrate contrastive learning with a GRU
regression model to develop a novel classifier, termed ContReg, which employs
dual learning methodologies, thereby further enhancing prediction performance.
To validate the effectiveness of our preprocessing pipeline, we compare and
demonstrate the performance gain of each step, and to demonstrate the efficacy
of the ContReg classifier, we compare its performance to that of sequence-based
deep learning architectures, machine learning models, and findings from
previous studies. Our results illustrate exceptional True Skill Statistic (TSS)
scores, surpassing previous methods and highlighting the critical role of
precise data preprocessing and classifier development in time series-based
solar flare prediction.",2024-09-21,"MohammadReza EskandariNasab, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi",http://arxiv.org/pdf/2409.14016v1,cs.LG
Mitigating Exposure Bias in Score-Based Generation of Molecular Conformations,"Molecular conformation generation poses a significant challenge in the field
of computational chemistry. Recently, Diffusion Probabilistic Models (DPMs) and
Score-Based Generative Models (SGMs) are effectively used due to their capacity
for generating accurate conformations far beyond conventional physics-based
approaches. However, the discrepancy between training and inference rises a
critical problem known as the exposure bias. While this issue has been
extensively investigated in DPMs, the existence of exposure bias in SGMs and
its effective measurement remain unsolved, which hinders the use of
compensation methods for SGMs, including ConfGF and Torsional Diffusion as the
representatives. In this work, we first propose a method for measuring exposure
bias in SGMs used for molecular conformation generation, which confirms the
significant existence of exposure bias in these models and measures its value.
We design a new compensation algorithm Input Perturbation (IP), which is
adapted from a method originally designed for DPMs only. Experimental results
show that by introducing IP, SGM-based molecular conformation models can
significantly improve both the accuracy and diversity of the generated
conformations. Especially by using the IP-enhanced Torsional Diffusion model,
we achieve new state-of-the-art performance on the GEOM-Drugs dataset and are
on par on GEOM-QM9. We provide the code publicly at
https://github.com/jia-975/torsionalDiff-ip.",2024-09-21,"Sijia Wang, Chen Wang, Zhenhao Zhao, Jiqiang Zhang, Weiran Cai",http://arxiv.org/pdf/2409.14014v1,cs.LG
ChronoGAN: Supervised and Embedded Generative Adversarial Networks for Time Series Generation,"Generating time series data using Generative Adversarial Networks (GANs)
presents several prevalent challenges, such as slow convergence, information
loss in embedding spaces, instability, and performance variability depending on
the series length. To tackle these obstacles, we introduce a robust framework
aimed at addressing and mitigating these issues effectively. This advanced
framework integrates the benefits of an Autoencoder-generated embedding space
with the adversarial training dynamics of GANs. This framework benefits from a
time series-based loss function and oversight from a supervisory network, both
of which capture the stepwise conditional distributions of the data
effectively. The generator functions within the latent space, while the
discriminator offers essential feedback based on the feature space. Moreover,
we introduce an early generation algorithm and an improved neural network
architecture to enhance stability and ensure effective generalization across
both short and long time series. Through joint training, our framework
consistently outperforms existing benchmarks, generating high-quality time
series data across a range of real and synthetic datasets with diverse
characteristics.",2024-09-21,"MohammadReza EskandariNasab, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi",http://arxiv.org/pdf/2409.14013v1,cs.LG
Test Time Learning for Time Series Forecasting,"Time-series forecasting has seen significant advancements with the
introduction of token prediction mechanisms such as multi-head attention.
However, these methods often struggle to achieve the same performance as in
language modeling, primarily due to the quadratic computational cost and the
complexity of capturing long-range dependencies in time-series data.
State-space models (SSMs), such as Mamba, have shown promise in addressing
these challenges by offering efficient solutions with linear RNNs capable of
modeling long sequences with larger context windows. However, there remains
room for improvement in accuracy and scalability.
  We propose the use of Test-Time Training (TTT) modules in a parallel
architecture to enhance performance in long-term time series forecasting.
Through extensive experiments on standard benchmark datasets, we demonstrate
that TTT modules consistently outperform state-of-the-art models, including the
Mamba-based TimeMachine, particularly in scenarios involving extended sequence
and prediction lengths. Our results show significant improvements in Mean
Squared Error (MSE) and Mean Absolute Error (MAE), especially on larger
datasets such as Electricity, Traffic, and Weather, underscoring the
effectiveness of TTT in capturing long-range dependencies. Additionally, we
explore various convolutional architectures within the TTT framework, showing
that even simple configurations like 1D convolution with small filters can
achieve competitive results. This work sets a new benchmark for time-series
forecasting and lays the groundwork for future research in scalable,
high-performance forecasting models.",2024-09-21,"Panayiotis Christou, Shichu Chen, Xupeng Chen, Parijat Dube",http://arxiv.org/pdf/2409.14012v3,cs.LG
Developing a Thailand solar irradiance map using Himawari-8 satellite imageries and deep learning models,"This paper presents an online platform showing Thailand solar irradiance map
every 30 minutes, available at https://www.cusolarforecast.com. The methodology
for estimating global horizontal irradiance (GHI) across Thailand relies on
cloud index extracted from Himawari-8 satellite imagery, Ineichen clear-sky
model with locally-tuned Linke turbidity, and machine learning models. The
methods take clear-sky irradiance, cloud index, re-analyzed GHI and temperature
data from the MERRA-2 database, and date-time as inputs for GHI estimation
models, including LightGBM, LSTM, Informer, and Transformer. These are
benchmarked with the estimate from a commercial service X by evaluation of
15-minute ground GHI data from 53 ground stations over 1.5 years during
2022-2023. The results show that the four models exhibit comparable overall MAE
performance to the service X. The best model is LightGBM with an overall MAE of
78.58 W/sqm and RMSE of 118.97 W/sqm, while the service X achieves the lowest
MAE, RMSE, and MBE in cloudy condition. Obtaining re-analyzed MERRA-2 data for
the whole Thailand region is not economically feasible for deployment. When
removing these features, the Informer model has a winning performance in MAE of
78.67 W/sqm. The obtained performance aligns with existing literature by taking
the climate zone and time granularity of data into consideration. As the map
shows an estimate of GHI over 93,000 grids with a frequent update, the paper
also describes a computational framework for displaying the entire map. It
tests the runtime performance of deep learning models in the GHI estimation
process.",2024-09-21,"Suwichaya Suwanwimolkul, Natanon Tongamrak, Nuttamon Thungka, Naebboon Hoonchareon, Jitkomut Songsiri",http://arxiv.org/pdf/2409.16320v3,cs.LG
Boolean Product Graph Neural Networks,"Graph Neural Networks (GNNs) have recently achieved significant success, with
a key operation involving the aggregation of information from neighboring
nodes. Substantial researchers have focused on defining neighbors for
aggregation, predominantly based on observed adjacency matrices. However, in
many scenarios, the explicitly given graphs contain noise, which can be
amplified during the messages-passing process. Therefore, many researchers have
turned their attention to latent graph inference, specifically learning a
parametric graph. To mitigate fluctuations in latent graph structure learning,
this paper proposes a novel Boolean product-based graph residual connection in
GNNs to link the latent graph and the original graph. It computes the Boolean
product between the latent graph and the original graph at each layer to
correct the learning process. The Boolean product between two adjacency
matrices is equivalent to triangle detection. Accordingly, the proposed Boolean
product graph neural networks can be interpreted as discovering triangular
cliques from the original and the latent graph. We validate the proposed method
in benchmark datasets and demonstrate its ability to enhance the performance
and robustness of GNNs.",2024-09-21,"Ziyan Wang, Bin Liu, Ling Xiang",http://arxiv.org/pdf/2409.14001v1,cs.LG
ChemEval: A Comprehensive Multi-Level Chemical Evaluation for Large Language Models,"There is a growing interest in the role that LLMs play in chemistry which
lead to an increased focus on the development of LLMs benchmarks tailored to
chemical domains to assess the performance of LLMs across a spectrum of
chemical tasks varying in type and complexity. However, existing benchmarks in
this domain fail to adequately meet the specific requirements of chemical
research professionals. To this end, we propose \textbf{\textit{ChemEval}},
which provides a comprehensive assessment of the capabilities of LLMs across a
wide range of chemical domain tasks. Specifically, ChemEval identified 4
crucial progressive levels in chemistry, assessing 12 dimensions of LLMs across
42 distinct chemical tasks which are informed by open-source data and the data
meticulously crafted by chemical experts, ensuring that the tasks have
practical value and can effectively evaluate the capabilities of LLMs. In the
experiment, we evaluate 12 mainstream LLMs on ChemEval under zero-shot and
few-shot learning contexts, which included carefully selected demonstration
examples and carefully designed prompts. The results show that while general
LLMs like GPT-4 and Claude-3.5 excel in literature understanding and
instruction following, they fall short in tasks demanding advanced chemical
knowledge. Conversely, specialized LLMs exhibit enhanced chemical competencies,
albeit with reduced literary comprehension. This suggests that LLMs have
significant potential for enhancement when tackling sophisticated tasks in the
field of chemistry. We believe our work will facilitate the exploration of
their potential to drive progress in chemistry. Our benchmark and analysis will
be available at {\color{blue} \url{https://github.com/USTC-StarTeam/ChemEval}}.",2024-09-21,"Yuqing Huang, Rongyang Zhang, Xuesong He, Xuyang Zhi, Hao Wang, Xin Li, Feiyang Xu, Deguang Liu, Huadong Liang, Yi Li, Jian Cui, Zimu Liu, Shijin Wang, Guoping Hu, Guiquan Liu, Qi Liu, Defu Lian, Enhong Chen",http://arxiv.org/pdf/2409.13989v1,cs.LG
The trade-off between data minimization and fairness in collaborative filtering,"General Data Protection Regulations (GDPR) aim to safeguard individuals'
personal information from harm. While full compliance is mandatory in the
European Union and the California Privacy Rights Act (CPRA), it is not in other
places. GDPR requires simultaneous compliance with all the principles such as
fairness, accuracy, and data minimization. However, it overlooks the potential
contradictions within its principles. This matter gets even more complex when
compliance is required from decision-making systems. Therefore, it is essential
to investigate the feasibility of simultaneously achieving the goals of GDPR
and machine learning, and the potential tradeoffs that might be forced upon us.
This paper studies the relationship between the principles of data minimization
and fairness in recommender systems. We operationalize data minimization via
active learning (AL) because, unlike many other methods, it can preserve a high
accuracy while allowing for strategic data collection, hence minimizing the
amount of data collection. We have implemented several active learning
strategies (personalized and non-personalized) and conducted a comparative
analysis focusing on accuracy and fairness on two publicly available datasets.
The results demonstrate that different AL strategies may have different impacts
on the accuracy of recommender systems with nearly all strategies negatively
impacting fairness. There has been no to very limited work on the trade-off
between data minimization and fairness, the pros and cons of active learning
methods as tools for implementing data minimization, and the potential impacts
of AL on fairness. By exploring these critical aspects, we offer valuable
insights for developing recommender systems that are GDPR compliant.",2024-09-21,"Nasim Sonboli, Sipei Li, Mehdi Elahi, Asia Biega",http://arxiv.org/pdf/2410.07182v1,cs.LG
ProTEA: Programmable Transformer Encoder Acceleration on FPGA,"Transformer neural networks (TNN) have been widely utilized on a diverse
range of applications, including natural language processing (NLP), machine
translation, and computer vision (CV). Their widespread adoption has been
primarily driven by the exceptional performance of their multi-head
self-attention block used to extract key features from sequential data. The
multi-head self-attention block is followed by feedforward neural networks,
which play a crucial role in introducing non-linearity to assist the model in
learning complex patterns. Despite the popularity of TNNs, there has been
limited numbers of hardware accelerators targeting these two critical blocks.
Most prior works have concentrated on sparse architectures that are not
flexible for popular TNN variants. This paper introduces \textit{ProTEA}, a
runtime programmable accelerator tailored for the dense computations of most of
state-of-the-art transformer encoders. \textit{ProTEA} is designed to reduce
latency by maximizing parallelism. We introduce an efficient tiling of large
matrices that can distribute memory and computing resources across different
hardware components within the FPGA. We provide run time evaluations of
\textit{ProTEA} on a Xilinx Alveo U55C high-performance data center accelerator
card. Experimental results demonstrate that \textit{ProTEA} can host a wide
range of popular transformer networks and achieve near optimal performance with
a tile size of 64 in the multi-head self-attention block and 6 in the
feedforward networks block when configured with 8 parallel attention heads, 12
layers, and an embedding dimension of 768 on the U55C. Comparative results are
provided showing \textit{ProTEA} is 2.5$\times$ faster than an NVIDIA Titan XP
GPU. Results also show that it achieves 1.3 -- 2.8$\times$ speed up compared
with current state-of-the-art custom designed FPGA accelerators.",2024-09-21,"Ehsan Kabir, Jason D. Bakos, David Andrews, Miaoqing Huang",http://arxiv.org/pdf/2409.13975v1,cs.LG
"One Model, Any Conjunctive Query: Graph Neural Networks for Answering Complex Queries over Knowledge Graphs","Traditional query answering over knowledge graphs -- or broadly over
relational data -- is one of the most fundamental problems in data management.
Motivated by the incompleteness of modern knowledge graphs, a new setup for
query answering has emerged, where the goal is to predict answers that do not
necessarily appear in the knowledge graph, but are present in its completion.
In this work, we propose AnyCQ, a graph neural network model that can classify
answers to any conjunctive query on any knowledge graph, following training. At
the core of our framework lies a graph neural network model trained using a
reinforcement learning objective to answer Boolean queries. Our approach and
problem setup differ from existing query answering studies in multiple
dimensions. First, we focus on the problem of query answer classification:
given a query and a set of possible answers, classify these proposals as true
or false relative to the complete knowledge graph. Second, we study the problem
of query answer retrieval: given a query, retrieve an answer to the query
relative to the complete knowledge graph or decide that no correct solutions
exist. Trained on simple, small instances, AnyCQ can generalize to large
queries of arbitrary structure, reliably classifying and retrieving answers to
samples where existing approaches fail, which is empirically validated on new
and challenging benchmarks. Furthermore, we demonstrate that our AnyCQ models
effectively transfer to out-of-distribution knowledge graphs, when equipped
with a relevant link predictor, highlighting their potential to serve as a
general engine for query answering.",2024-09-21,"Krzysztof Olejniczak, Xingyue Huang, İsmail İlkan Ceylan, Mikhail Galkin",http://arxiv.org/pdf/2409.13959v1,cs.LG
Training Large ASR Encoders with Differential Privacy,"Self-supervised learning (SSL) methods for large speech models have proven to
be highly effective at ASR. With the interest in public deployment of large
pre-trained models, there is a rising concern for unintended memorization and
leakage of sensitive data points from the training data. In this paper, we
apply differentially private (DP) pre-training to a SOTA Conformer-based
encoder, and study its performance on a downstream ASR task assuming the
fine-tuning data is public. This paper is the first to apply DP to SSL for ASR,
investigating the DP noise tolerance of the BEST-RQ pre-training method.
Notably, we introduce a novel variant of model pruning called gradient-based
layer freezing that provides strong improvements in privacy-utility-compute
trade-offs. Our approach yields a LibriSpeech test-clean/other WER (%) of 3.78/
8.41 with ($10$, 1e^-9)-DP for extrapolation towards low dataset scales, and
2.81/ 5.89 with (10, 7.9e^-11)-DP for extrapolation towards high scales.",2024-09-21,"Geeticka Chauhan, Steve Chien, Om Thakkar, Abhradeep Thakurta, Arun Narayanan",http://arxiv.org/pdf/2409.13953v1,cs.LG
Deep learning for fast segmentation and critical dimension metrology & characterization enabling AR/VR design and fabrication,"Quantitative analysis of microscopy images is essential in the design and
fabrication of components used in augmented reality/virtual reality (AR/VR)
modules. However, segmenting regions of interest (ROIs) from these complex
images and extracting critical dimensions (CDs) requires novel techniques, such
as deep learning models which are key for actionable decisions on process,
material and device optimization. In this study, we report on the fine-tuning
of a pre-trained Segment Anything Model (SAM) using a diverse dataset of
electron microscopy images. We employed methods such as low-rank adaptation
(LoRA) to reduce training time and enhance the accuracy of ROI extraction. The
model's ability to generalize to unseen images facilitates zero-shot learning
and supports a CD extraction model that precisely extracts CDs from the
segmented ROIs. We demonstrate the accurate extraction of binary images from
cross-sectional images of surface relief gratings (SRGs) and Fresnel lenses in
both single and multiclass modes. Furthermore, these binary images are used to
identify transition points, aiding in the extraction of relevant CDs. The
combined use of the fine-tuned segmentation model and the CD extraction model
offers substantial advantages to various industrial applications by enhancing
analytical capabilities, time to data and insights, and optimizing
manufacturing processes.",2024-09-20,"Kundan Chaudhary, Subhei Shaar, Raja Muthinti",http://arxiv.org/pdf/2409.13951v1,cs.LG
PyGRF: An improved Python Geographical Random Forest model and case studies in public health and natural disasters,"Geographical random forest (GRF) is a recently developed and spatially
explicit machine learning model. With the ability to provide more accurate
predictions and local interpretations, GRF has already been used in many
studies. The current GRF model, however, has limitations in its determination
of the local model weight and bandwidth hyperparameters, potentially
insufficient numbers of local training samples, and sometimes high local
prediction errors. Also, implemented as an R package, GRF currently does not
have a Python version which limits its adoption among machine learning
practitioners who prefer Python. This work addresses these limitations by
introducing theory-informed hyperparameter determination, local training sample
expansion, and spatially-weighted local prediction. We also develop a
Python-based GRF model and package, PyGRF, to facilitate the use of the model.
We evaluate the performance of PyGRF on an example dataset and further
demonstrate its use in two case studies in public health and natural disasters.",2024-09-20,"Kai Sun, Ryan Zhenqi Zhou, Jiyeon Kim, Yingjie Hu",http://arxiv.org/pdf/2409.13947v1,cs.LG
Learning Recourse Costs from Pairwise Feature Comparisons,"This paper presents a novel technique for incorporating user input when
learning and inferring user preferences. When trying to provide users of
black-box machine learning models with actionable recourse, we often wish to
incorporate their personal preferences about the ease of modifying each
individual feature. These recourse finding algorithms usually require an
exhaustive set of tuples associating each feature to its cost of modification.
Since it is hard to obtain such costs by directly surveying humans, in this
paper, we propose the use of the Bradley-Terry model to automatically infer
feature-wise costs using non-exhaustive human comparison surveys. We propose
that users only provide inputs comparing entire recourses, with all candidate
feature modifications, determining which recourses are easier to implement
relative to others, without explicit quantification of their costs. We
demonstrate the efficient learning of individual feature costs using MAP
estimates, and show that these non-exhaustive human surveys, which do not
necessarily contain data for each feature pair comparison, are sufficient to
learn an exhaustive set of feature costs, where each feature is associated with
a modification cost.",2024-09-20,"Kaivalya Rawal, Himabindu Lakkaraju",http://arxiv.org/pdf/2409.13940v1,cs.LG
High-Resolution Flood Probability Mapping Using Generative Machine Learning with Large-Scale Synthetic Precipitation and Inundation Data,"High-resolution flood probability maps are instrumental for assessing flood
risk but are often limited by the availability of historical data.
Additionally, producing simulated data needed for creating probabilistic flood
maps using physics-based models involves significant computation and time
effort, which inhibit its feasibility. To address this gap, this study
introduces Precipitation-Flood Depth Generative Pipeline, a novel methodology
that leverages generative machine learning to generate large-scale synthetic
inundation data to produce probabilistic flood maps. With a focus on Harris
County, Texas, Precipitation-Flood Depth Generative Pipeline begins with
training a cell-wise depth estimator using a number of precipitation-flood
events model with a physics-based model. This cell-wise depth estimator, which
emphasizes precipitation-based features, outperforms universal models.
Subsequently, the Conditional Generative Adversarial Network (CTGAN) is used to
conditionally generate synthetic precipitation point cloud, which are filtered
using strategic thresholds to align with realistic precipitation patterns.
Hence, a precipitation feature pool is constructed for each cell, enabling
strategic sampling and the generation of synthetic precipitation events. After
generating 10,000 synthetic events, flood probability maps are created for
various inundation depths. Validation using similarity and correlation metrics
confirms the accuracy of the synthetic depth distributions. The
Precipitation-Flood Depth Generative Pipeline provides a scalable solution to
generate synthetic flood depth data needed for high-resolution flood
probability maps, which can enhance flood mitigation planning.",2024-09-20,"Lipai Huang, Federico Antolini, Ali Mostafavi, Russell Blessing, Matthew Garcia, Samuel D. Brody",http://arxiv.org/pdf/2409.13936v2,cs.LG
On-Device Collaborative Language Modeling via a Mixture of Generalists and Specialists,"On-device LLMs have gained increasing attention for their ability to enhance
privacy and provide a personalized user experience. To facilitate private
learning with scarce data, Federated Learning has become a standard approach.
However, it faces challenges such as computational resource heterogeneity and
data heterogeneity among end users. We propose CoMiGS ($\textbf{Co}$llaborative
learning with a $\textbf{Mi}$xture of $\textbf{G}$eneralists and
$\textbf{S}$pecialists), the first approach to address both challenges. A key
innovation of our method is the bi-level optimization formulation of the
Mixture-of-Experts learning objective, where the router is optimized using a
separate validation set to ensure alignment with the target distribution. We
solve our objective with alternating minimization, for which we provide a
theoretical analysis. Our method shares generalist experts across users while
localizing a varying number of specialist experts, thereby adapting to users'
computational resources and preserving privacy. Through extensive experiments,
we show CoMiGS effectively balances general and personalized knowledge for each
token generation. We demonstrate that CoMiGS remains robust against
overfitting-due to the generalists' regularizing effect-while adapting to local
data through specialist expertise. We open source our codebase for
collaborative LLMs.",2024-09-20,"Dongyang Fan, Bettina Messmer, Nikita Doikov, Martin Jaggi",http://arxiv.org/pdf/2409.13931v3,cs.LG
"One Model is All You Need: ByT5-Sanskrit, a Unified Model for Sanskrit NLP Tasks","Morphologically rich languages are notoriously challenging to process for
downstream NLP applications. This paper presents a new pretrained language
model, ByT5-Sanskrit, designed for NLP applications involving the
morphologically rich language Sanskrit. We evaluate ByT5-Sanskrit on
established Sanskrit word segmentation tasks, where it outperforms previous
data-driven approaches by a considerable margin and matches the performance of
the current best lexicon-based model. It is easier to deploy and more robust to
data not covered by external linguistic resources. It also achieves new
state-of-the-art results in Vedic Sanskrit dependency parsing and OCR
post-correction tasks. Additionally, based on the Digital Corpus of Sanskrit,
we introduce a novel multitask dataset for the joint training of Sanskrit word
segmentation, lemmatization, and morphosyntactic tagging tasks. We fine-tune
ByT5-Sanskrit on this dataset, creating a versatile multitask model for various
downstream Sanskrit applications. We have used this model in Sanskrit
linguistic annotation projects, in information retrieval setups, and as a
preprocessing step in a Sanskrit machine translation pipeline. We also show
that our approach yields new best scores for lemmatization and dependency
parsing of other morphologically rich languages. We thus demonstrate that
byte-level pretrained language models can achieve excellent performance for
morphologically rich languages, outperforming tokenizer-based models and
presenting an important vector of exploration when constructing NLP pipelines
for such languages.",2024-09-20,"Sebastian Nehrdich, Oliver Hellwig, Kurt Keutzer",http://arxiv.org/pdf/2409.13920v1,cs.LG
High-dimensional learning of narrow neural networks,"Recent years have been marked with the fast-pace diversification and
increasing ubiquity of machine learning applications. Yet, a firm theoretical
understanding of the surprising efficiency of neural networks to learn from
high-dimensional data still proves largely elusive. In this endeavour, analyses
inspired by statistical physics have proven instrumental, enabling the tight
asymptotic characterization of the learning of neural networks in high
dimensions, for a broad class of solvable models. This manuscript reviews the
tools and ideas underlying recent progress in this line of work. We introduce a
generic model -- the sequence multi-index model -- which encompasses numerous
previously studied models as special instances. This unified framework covers a
broad class of machine learning architectures with a finite number of hidden
units, including multi-layer perceptrons, autoencoders, attention mechanisms;
and tasks, including (un)supervised learning, denoising, contrastive learning,
in the limit of large data dimension, and comparably large number of samples.
We explicate in full detail the analysis of the learning of sequence
multi-index models, using statistical physics techniques such as the replica
method and approximate message-passing algorithms. This manuscript thus
provides a unified presentation of analyses reported in several previous works,
and a detailed overview of central techniques in the field of statistical
physics of machine learning. This review should be a useful primer for machine
learning theoreticians curious of statistical physics approaches; it should
also be of value to statistical physicists interested in the transfer of such
ideas to the study of neural networks.",2024-09-20,Hugo Cui,http://arxiv.org/pdf/2409.13904v2,cs.LG
PTQ4ADM: Post-Training Quantization for Efficient Text Conditional Audio Diffusion Models,"Denoising diffusion models have emerged as state-of-the-art in generative
tasks across image, audio, and video domains, producing high-quality, diverse,
and contextually relevant data. However, their broader adoption is limited by
high computational costs and large memory footprints. Post-training
quantization (PTQ) offers a promising approach to mitigate these challenges by
reducing model complexity through low-bandwidth parameters. Yet, direct
application of PTQ to diffusion models can degrade synthesis quality due to
accumulated quantization noise across multiple denoising steps, particularly in
conditional tasks like text-to-audio synthesis. This work introduces PTQ4ADM, a
novel framework for quantizing audio diffusion models(ADMs). Our key
contributions include (1) a coverage-driven prompt augmentation method and (2)
an activation-aware calibration set generation algorithm for text-conditional
ADMs. These techniques ensure comprehensive coverage of audio aspects and
modalities while preserving synthesis fidelity. We validate our approach on
TANGO, Make-An-Audio, and AudioLDM models for text-conditional audio
generation. Extensive experiments demonstrate PTQ4ADM's capability to reduce
the model size by up to 70\% while achieving synthesis quality metrics
comparable to full-precision models($<$5\% increase in FD scores). We show that
specific layers in the backbone network can be quantized to 4-bit weights and
8-bit activations without significant quality loss. This work paves the way for
more efficient deployment of ADMs in resource-constrained environments.",2024-09-20,"Jayneel Vora, Aditya Krishnan, Nader Bouacida, Prabhu RV Shankar, Prasant Mohapatra",http://arxiv.org/pdf/2409.13894v1,cs.LG
Causal Feature Selection Method for Contextual Multi-Armed Bandits in Recommender System,"Features (a.k.a. context) are critical for contextual multi-armed bandits
(MAB) performance. In practice of large scale online system, it is important to
select and implement important features for the model: missing important
features can led to sub-optimal reward outcome, and including irrelevant
features can cause overfitting, poor model interpretability, and implementation
cost. However, feature selection methods for conventional machine learning
models fail short for contextual MAB use cases, as conventional methods select
features correlated with the outcome variable, but not necessarily causing
heterogeneuous treatment effect among arms which are truely important for
contextual MAB. In this paper, we introduce model-free feature selection
methods designed for contexutal MAB problem, based on heterogeneous causal
effect contributed by the feature to the reward distribution. Empirical
evaluation is conducted based on synthetic data as well as real data from an
online experiment for optimizing content cover image in a recommender system.
The results show this feature selection method effectively selects the
important features that lead to higher contextual MAB reward than unimportant
features. Compared with model embedded method, this model-free method has
advantage of fast computation speed, ease of implementation, and prune of model
mis-specification issues.",2024-09-20,"Zhenyu Zhao, Yexi Jiang",http://arxiv.org/pdf/2409.13888v1,cs.LG
Learning to Play Video Games with Intuitive Physics Priors,"Video game playing is an extremely structured domain where algorithmic
decision-making can be tested without adverse real-world consequences. While
prevailing methods rely on image inputs to avoid the problem of hand-crafting
state space representations, this approach systematically diverges from the way
humans actually learn to play games. In this paper, we design object-based
input representations that generalize well across a number of video games.
Using these representations, we evaluate an agent's ability to learn games
similar to an infant - with limited world experience, employing simple
inductive biases derived from intuitive representations of physics from the
real world. Using such biases, we construct an object category representation
to be used by a Q-learning algorithm and assess how well it learns to play
multiple games based on observed object affordances. Our results suggest that a
human-like object interaction setup capably learns to play several video games,
and demonstrates superior generalizability, particularly for unfamiliar
objects. Further exploring such methods will allow machines to learn in a
human-centric way, thus incorporating more human-like learning benefits.",2024-09-20,"Abhishek Jaiswal, Nisheeth Srivastava",http://arxiv.org/pdf/2409.13886v1,cs.LG
A Multi-LLM Debiasing Framework,"Large Language Models (LLMs) are powerful tools with the potential to benefit
society immensely, yet, they have demonstrated biases that perpetuate societal
inequalities. Despite significant advancements in bias mitigation techniques
using data augmentation, zero-shot prompting, and model fine-tuning, biases
continuously persist, including subtle biases that may elude human detection.
Recent research has shown a growing interest in multi-LLM approaches, which
have been demonstrated to be effective in improving the quality of reasoning
and factuality in LLMs. Building on this approach, we propose a novel multi-LLM
debiasing framework aimed at reducing bias in LLMs. Our work is the first to
introduce and evaluate two distinct approaches within this framework for
debiasing LLMs: a centralized method, where the conversation is facilitated by
a single central LLM, and a decentralized method, where all models communicate
directly. Our findings reveal that our multi-LLM framework significantly
reduces bias in LLMs, outperforming the baseline method across several social
groups.",2024-09-20,"Deonna M. Owens, Ryan A. Rossi, Sungchul Kim, Tong Yu, Franck Dernoncourt, Xiang Chen, Ruiyi Zhang, Jiuxiang Gu, Hanieh Deilamsalehy, Nedim Lipka",http://arxiv.org/pdf/2409.13884v1,cs.LG
Tabular Data Generation using Binary Diffusion,"Generating synthetic tabular data is critical in machine learning, especially
when real data is limited or sensitive. Traditional generative models often
face challenges due to the unique characteristics of tabular data, such as
mixed data types and varied distributions, and require complex preprocessing or
large pretrained models. In this paper, we introduce a novel, lossless binary
transformation method that converts any tabular data into fixed-size binary
representations, and a corresponding new generative model called Binary
Diffusion, specifically designed for binary data. Binary Diffusion leverages
the simplicity of XOR operations for noise addition and removal and employs
binary cross-entropy loss for training. Our approach eliminates the need for
extensive preprocessing, complex noise parameter tuning, and pretraining on
large datasets. We evaluate our model on several popular tabular benchmark
datasets, demonstrating that Binary Diffusion outperforms existing
state-of-the-art models on Travel, Adult Income, and Diabetes datasets while
being significantly smaller in size. Code and models are available at:
https://github.com/vkinakh/binary-diffusion-tabular",2024-09-20,"Vitaliy Kinakh, Slava Voloshynovskiy",http://arxiv.org/pdf/2409.13882v2,cs.LG
Investigation of Time-Frequency Feature Combinations with Histogram Layer Time Delay Neural Networks,"While deep learning has reduced the prevalence of manual feature extraction,
transformation of data via feature engineering remains essential for improving
model performance, particularly for underwater acoustic signals. The methods by
which audio signals are converted into time-frequency representations and the
subsequent handling of these spectrograms can significantly impact performance.
This work demonstrates the performance impact of using different combinations
of time-frequency features in a histogram layer time delay neural network. An
optimal set of features is identified with results indicating that specific
feature combinations outperform single data features.",2024-09-20,"Amirmohammad Mohammadi, Iren'e Masabarakiza, Ethan Barnes, Davelle Carreiro, Alexandra Van Dine, Joshua Peeples",http://arxiv.org/pdf/2409.13881v2,cs.LG
Cross-Domain Knowledge Transfer for Underwater Acoustic Classification Using Pre-trained Models,"Transfer learning is commonly employed to leverage large, pre-trained models
and perform fine-tuning for downstream tasks. The most prevalent pre-trained
models are initially trained using ImageNet. However, their ability to
generalize can vary across different data modalities. This study compares
pre-trained Audio Neural Networks (PANNs) and ImageNet pre-trained models
within the context of underwater acoustic target recognition (UATR). It was
observed that the ImageNet pre-trained models slightly out-perform pre-trained
audio models in passive sonar classification. We also analyzed the impact of
audio sampling rates for model pre-training and fine-tuning. This study
contributes to transfer learning applications of UATR, illustrating the
potential of pre-trained models to address limitations caused by scarce,
labeled data in the UATR domain.",2024-09-20,"Amirmohammad Mohammadi, Tejashri Kelhe, Davelle Carreiro, Alexandra Van Dine, Joshua Peeples",http://arxiv.org/pdf/2409.13878v2,cs.LG
Achieving Predictive Precision: Leveraging LSTM and Pseudo Labeling for Volvo's Discovery Challenge at ECML-PKDD 2024,"This paper presents the second-place methodology in the Volvo Discovery
Challenge at ECML-PKDD 2024, where we used Long Short-Term Memory networks and
pseudo-labeling to predict maintenance needs for a component of Volvo trucks.
We processed the training data to mirror the test set structure and applied a
base LSTM model to label the test data iteratively. This approach refined our
model's predictive capabilities and culminated in a macro-average F1-score of
0.879, demonstrating robust performance in predictive maintenance. This work
provides valuable insights for applying machine learning techniques effectively
in industrial settings.",2024-09-20,"Carlo Metta, Marco Gregnanin, Andrea Papini, Silvia Giulia Galfrè, Andrea Fois, Francesco Morandin, Marco Fantozzi, Maurizio Parton",http://arxiv.org/pdf/2409.13877v1,cs.LG
Physics-Informed Variational State-Space Gaussian Processes,"Differential equations are important mechanistic models that are integral to
many scientific and engineering applications. With the abundance of available
data there has been a growing interest in data-driven physics-informed models.
Gaussian processes (GPs) are particularly suited to this task as they can model
complex, non-linear phenomena whilst incorporating prior knowledge and
quantifying uncertainty. Current approaches have found some success but are
limited as they either achieve poor computational scalings or focus only on the
temporal setting. This work addresses these issues by introducing a variational
spatio-temporal state-space GP that handles linear and non-linear physical
constraints while achieving efficient linear-in-time computation costs. We
demonstrate our methods in a range of synthetic and real-world settings and
outperform the current state-of-the-art in both predictive and computational
performance.",2024-09-20,"Oliver Hamelijnck, Arno Solin, Theodoros Damoulas",http://arxiv.org/pdf/2409.13876v2,cs.LG
Data Distribution Shifts in (Industrial) Federated Learning as a Privacy Issue,"We consider industrial federated learning, a collaboration between a small
number of powerful, potentially competing industrial players, mediated by a
third party aspiring to improve the service it provides to its customers. We
argue that this configuration harbours covert privacy risks that do not arise
in e.g. cross-device settings. Companies are very protective of their
intellectual property and production processes. Information about changes to
their production and the timing of which is to be kept private. We study a
scenario in which one of the collaborators infers changes to their competitors'
production by detecting potentially subtle temporal data distribution shifts.
In this framing, a data distribution shift is always problematic, even if it
has no negative effect on training convergence. Thus, our goal is to find means
that allow the detection of distributional shifts better than customary
evaluation metrics. Based on the assumption that even minor shifts translate
into the collaboratively learned machine learning model, the attacker tracks
the shared models' internal state with a selection of metrics from literature
in order to pick up on relevant changes. In an empirical study on benchmark
datasets, we show an honest-but-curious attacker to be capable of detecting
subtle distributional shifts on other clients, in some cases long before they
become obvious in evaluation.",2024-09-20,"David Brunner, Alessio Montuoro",http://arxiv.org/pdf/2409.13875v1,cs.LG
Instruct-Tuning Pretrained Causal Language Models for Ancient Greek Papyrology and Epigraphy,"This article presents an experiment in fine-tuning a pretrained causal
language model (Meta's Llama 3.1 8B Instruct) to assist with restoring missing
or illegible characters in ancient Greek inscriptions and documentary papyri.
Utilizing a straightforward instruction-based approach and a 95%/5% train/test
split, the papyrus restoration model achieved a character error rate (CER) of
14.9%, a top-1 accuracy of 73.5%, and a top-20 accuracy of 86.0% for sequences
up to 10 characters. A model was also fine-tuned for geographic attribution,
reaching a top-1 accuracy of 66.4% and a top-3 accuracy of 79.9%. In
chronological attribution, it demonstrated an average deviation of 21.7 years
from the actual terminus post/ante quem, with a median deviation of 0 years.
For inscriptions, the restoration model achieved a CER of 20.5%, a top-1
accuracy of 63.7%, and a top-20 accuracy of 83.0% for sequences up to 10
characters. In geographic attribution, it attained a top-1 accuracy of 75.0%
and a top-3 accuracy of 83.7%, while in dating, it had an average deviation of
37.1 years and a median deviation of 3 years from the actual date range.
Benchmarked against the state-of-the-art model (Ithaca) on a shared test set
and on recently edited inscriptions, the instruction-tuned models excelled in
text restoration, while also offering the practical advantage of ignoring
spaces during reconstruction, which aligns with the scriptio continua of
ancient textual artifacts. However, their performance in geographic and
chronological attribution was lower than Ithaca's. To evaluate the approach in
a more even setup, the instruction model was retrained with an 80%/10%/10%
train-validation-test split, and still outperformed Ithaca in text restoration.
The results suggest that fine-tuning larger pretrained causal language models
using instruction templates for emendations and conjectures to ancient texts
holds promise.",2024-09-20,Eric Cullhed,http://arxiv.org/pdf/2409.13870v3,cs.LG
Deep Learning-Based Channel Squeeze U-Structure for Lung Nodule Detection and Segmentation,"This paper introduces a novel deep-learning method for the automatic
detection and segmentation of lung nodules, aimed at advancing the accuracy of
early-stage lung cancer diagnosis. The proposed approach leverages a unique
""Channel Squeeze U-Structure"" that optimizes feature extraction and information
integration across multiple semantic levels of the network. This architecture
includes three key modules: shallow information processing, channel residual
structure, and channel squeeze integration. These modules enhance the model's
ability to detect and segment small, imperceptible, or ground-glass nodules,
which are critical for early diagnosis. The method demonstrates superior
performance in terms of sensitivity, Dice similarity coefficient, precision,
and mean Intersection over Union (IoU). Extensive experiments were conducted on
the Lung Image Database Consortium (LIDC) dataset using five-fold
cross-validation, showing excellent stability and robustness. The results
indicate that this approach holds significant potential for improving
computer-aided diagnosis systems, providing reliable support for radiologists
in clinical practice and aiding in the early detection of lung cancer,
especially in resource-limited settings",2024-09-20,"Mingxiu Sui, Jiacheng Hu, Tong Zhou, Zibo Liu, Likang Wen, Junliang Du",http://arxiv.org/pdf/2409.13868v1,cs.LG
MAGICS: Adversarial RL with Minimax Actors Guided by Implicit Critic Stackelberg for Convergent Neural Synthesis of Robot Safety,"While robust optimal control theory provides a rigorous framework to compute
robot control policies that are provably safe, it struggles to scale to
high-dimensional problems, leading to increased use of deep learning for
tractable synthesis of robot safety. Unfortunately, existing neural safety
synthesis methods often lack convergence guarantees and solution
interpretability. In this paper, we present Minimax Actors Guided by Implicit
Critic Stackelberg (MAGICS), a novel adversarial reinforcement learning (RL)
algorithm that guarantees local convergence to a minimax equilibrium solution.
We then build on this approach to provide local convergence guarantees for a
general deep RL-based robot safety synthesis algorithm. Through both simulation
studies on OpenAI Gym environments and hardware experiments with a
36-dimensional quadruped robot, we show that MAGICS can yield robust control
policies outperforming the state-of-the-art neural safety synthesis methods.",2024-09-20,"Justin Wang, Haimin Hu, Duy Phuong Nguyen, Jaime Fernández Fisac",http://arxiv.org/pdf/2409.13867v2,cs.LG
Persistent Backdoor Attacks in Continual Learning,"Backdoor attacks pose a significant threat to neural networks, enabling
adversaries to manipulate model outputs on specific inputs, often with
devastating consequences, especially in critical applications. While backdoor
attacks have been studied in various contexts, little attention has been given
to their practicality and persistence in continual learning, particularly in
understanding how the continual updates to model parameters, as new data
distributions are learned and integrated, impact the effectiveness of these
attacks over time. To address this gap, we introduce two persistent backdoor
attacks-Blind Task Backdoor and Latent Task Backdoor-each leveraging minimal
adversarial influence. Our blind task backdoor subtly alters the loss
computation without direct control over the training process, while the latent
task backdoor influences only a single task's training, with all other tasks
trained benignly. We evaluate these attacks under various configurations,
demonstrating their efficacy with static, dynamic, physical, and semantic
triggers. Our results show that both attacks consistently achieve high success
rates across different continual learning algorithms, while effectively evading
state-of-the-art defenses, such as SentiNet and I-BAU.",2024-09-20,"Zhen Guo, Abhinav Kumar, Reza Tourani",http://arxiv.org/pdf/2409.13864v1,cs.LG
Learning to Simulate Aerosol Dynamics with Graph Neural Networks,"Aerosol effects on climate, weather, and air quality depend on
characteristics of individual particles, which are tremendously diverse and
change in time. Particle-resolved models are the only models able to capture
this diversity in particle physiochemical properties, and these models are
computationally expensive. As a strategy for accelerating particle-resolved
microphysics models, we introduce Graph-based Learning of Aerosol Dynamics
(GLAD) and use this model to train a surrogate of the particle-resolved model
PartMC-MOSAIC. GLAD implements a Graph Network-based Simulator (GNS), a machine
learning framework that has been used to simulate particle-based fluid dynamics
models. In GLAD, each particle is represented as a node in a graph, and the
evolution of the particle population over time is simulated through learned
message passing. We demonstrate our GNS approach on a simple aerosol system
that includes condensation of sulfuric acid onto particles composed of sulfate,
black carbon, organic carbon, and water. A graph with particles as nodes is
constructed, and a graph neural network (GNN) is then trained using the model
output from PartMC-MOSAIC. The trained GNN can then be used for simulating and
predicting aerosol dynamics over time. Results demonstrate the framework's
ability to accurately learn chemical dynamics and generalize across different
scenarios, achieving efficient training and prediction times. We evaluate the
performance across three scenarios, highlighting the framework's robustness and
adaptability in modeling aerosol microphysics and chemistry.",2024-09-20,"Fabiana Ferracina, Payton Beeler, Mahantesh Halappanavar, Bala Krishnamoorthy, Marco Minutoli, Laura Fierce",http://arxiv.org/pdf/2409.13861v1,cs.LG
Wormhole: Concept-Aware Deep Representation Learning for Co-Evolving Sequences,"Identifying and understanding dynamic concepts in co-evolving sequences is
crucial for analyzing complex systems such as IoT applications, financial
markets, and online activity logs. These concepts provide valuable insights
into the underlying structures and behaviors of sequential data, enabling
better decision-making and forecasting. This paper introduces Wormhole, a novel
deep representation learning framework that is concept-aware and designed for
co-evolving time sequences. Our model presents a self-representation layer and
a temporal smoothness constraint to ensure robust identification of dynamic
concepts and their transitions. Additionally, concept transitions are detected
by identifying abrupt changes in the latent space, signifying a shift to new
behavior - akin to passing through a wormhole. This novel mechanism accurately
discerns concepts within co-evolving sequences and pinpoints the exact
locations of these wormholes, enhancing the interpretability of the learned
representations. Experiments demonstrate that this method can effectively
segment time series data into meaningful concepts, providing a valuable tool
for analyzing complex temporal patterns and advancing the detection of concept
drifts.",2024-09-20,"Kunpeng Xu, Lifei Chen, Shengrui Wang",http://arxiv.org/pdf/2409.13857v1,cs.LG
More Consideration for the Perceptron,"In this paper, we introduce the gated perceptron, an enhancement of the
conventional perceptron, which incorporates an additional input computed as the
product of the existing inputs. This allows the perceptron to capture
non-linear interactions between features, significantly improving its ability
to classify and regress on complex datasets. We explore its application in both
linear and non-linear regression tasks using the Iris dataset, as well as
binary and multi-class classification problems, including the PIMA Indian
dataset and Breast Cancer Wisconsin dataset. Our results demonstrate that the
gated perceptron can generate more distinct decision regions compared to
traditional perceptrons, enhancing its classification capabilities,
particularly in handling non-linear data. Performance comparisons show that the
gated perceptron competes with state-of-the-art classifiers while maintaining a
simple architecture.",2024-09-20,Slimane Larabi,http://arxiv.org/pdf/2409.13854v2,cs.LG
Unlocking Memorization in Large Language Models with Dynamic Soft Prompting,"Pretrained large language models (LLMs) have revolutionized natural language
processing (NLP) tasks such as summarization, question answering, and
translation. However, LLMs pose significant security risks due to their
tendency to memorize training data, leading to potential privacy breaches and
copyright infringement. Accurate measurement of this memorization is essential
to evaluate and mitigate these potential risks. However, previous attempts to
characterize memorization are constrained by either using prefixes only or by
prepending a constant soft prompt to the prefixes, which cannot react to
changes in input. To address this challenge, we propose a novel method for
estimating LLM memorization using dynamic, prefix-dependent soft prompts. Our
approach involves training a transformer-based generator to produce soft
prompts that adapt to changes in input, thereby enabling more accurate
extraction of memorized data. Our method not only addresses the limitations of
previous methods but also demonstrates superior performance in diverse
experimental settings compared to state-of-the-art techniques. In particular,
our method can achieve the maximum relative improvement of 112.75% and 32.26%
over the vanilla baseline in terms of discoverable memorization rate for the
text generation task and code generation task respectively.",2024-09-20,"Zhepeng Wang, Runxue Bao, Yawen Wu, Jackson Taylor, Cao Xiao, Feng Zheng, Weiwen Jiang, Shangqian Gao, Yanfu Zhang",http://arxiv.org/pdf/2409.13853v1,cs.LG
Learning Ordering in Crystalline Materials with Symmetry-Aware Graph Neural Networks,"Graph convolutional neural networks (GCNNs) have become a machine learning
workhorse for screening the chemical space of crystalline materials in fields
such as catalysis and energy storage, by predicting properties from structures.
Multicomponent materials, however, present a unique challenge since they can
exhibit chemical (dis)order, where a given lattice structure can encompass a
variety of elemental arrangements ranging from highly ordered structures to
fully disordered solid solutions. Critically, properties like stability,
strength, and catalytic performance depend not only on structures but also on
orderings. To enable rigorous materials design, it is thus critical to ensure
GCNNs are capable of distinguishing among atomic orderings. However, the
ordering-aware capability of GCNNs has been poorly understood. Here, we
benchmark various neural network architectures for capturing the
ordering-dependent energetics of multicomponent materials in a custom-made
dataset generated with high-throughput atomistic simulations. Conventional
symmetry-invariant GCNNs were found unable to discern the structural difference
between the diverse symmetrically inequivalent atomic orderings of the same
material, while symmetry-equivariant model architectures could inherently
preserve and differentiate the distinct crystallographic symmetries of various
orderings.",2024-09-20,"Jiayu Peng, James Damewood, Jessica Karaguesian, Jaclyn R. Lunger, Rafael Gómez-Bombarelli",http://arxiv.org/pdf/2409.13851v1,cs.LG
Segment Discovery: Enhancing E-commerce Targeting,"Modern e-commerce services frequently target customers with incentives or
interventions to engage them in their products such as games, shopping, video
streaming, etc. This customer engagement increases acquisition of more
customers and retention of existing ones, leading to more business for the
company while improving customer experience. Often, customers are either
randomly targeted or targeted based on the propensity of desirable behavior.
However, such policies can be suboptimal as they do not target the set of
customers who would benefit the most from the intervention and they may also
not take account of any constraints. In this paper, we propose a policy
framework based on uplift modeling and constrained optimization that identifies
customers to target for a use-case specific intervention so as to maximize the
value to the business, while taking account of any given constraints. We
demonstrate improvement over state-of-the-art targeting approaches using two
large-scale experimental studies and a production implementation.",2024-09-20,"Qiqi Li, Roopali Singh, Charin Polpanumas, Tanner Fiez, Namita Kumar, Shreya Chakrabarti",http://arxiv.org/pdf/2409.13847v2,cs.LG
Multi-Modality Conditioned Variational U-Net for Field-of-View Extension in Brain Diffusion MRI,"An incomplete field-of-view (FOV) in diffusion magnetic resonance imaging
(dMRI) can severely hinder the volumetric and bundle analyses of whole-brain
white matter connectivity. Although existing works have investigated imputing
the missing regions using deep generative models, it remains unclear how to
specifically utilize additional information from paired multi-modality data and
whether this can enhance the imputation quality and be useful for downstream
tractography. To fill this gap, we propose a novel framework for imputing dMRI
scans in the incomplete part of the FOV by integrating the learned diffusion
features in the acquired part of the FOV to the complete brain anatomical
structure. We hypothesize that by this design the proposed framework can
enhance the imputation performance of the dMRI scans and therefore be useful
for repairing whole-brain tractography in corrupted dMRI scans with incomplete
FOV. We tested our framework on two cohorts from different sites with a total
of 96 subjects and compared it with a baseline imputation method that treats
the information from T1w and dMRI scans equally. The proposed framework
achieved significant improvements in imputation performance, as demonstrated by
angular correlation coefficient (p < 1E-5), and in downstream tractography
accuracy, as demonstrated by Dice score (p < 0.01). Results suggest that the
proposed framework improved imputation performance in dMRI scans by
specifically utilizing additional information from paired multi-modality data,
compared with the baseline method. The imputation achieved by the proposed
framework enhances whole brain tractography, and therefore reduces the
uncertainty when analyzing bundles associated with neurodegenerative.",2024-09-20,"Zhiyuan Li, Tianyuan Yao, Praitayini Kanakaraj, Chenyu Gao, Shunxing Bao, Lianrui Zuo, Michael E. Kim, Nancy R. Newlin, Gaurav Rudravaram, Nazirah M. Khairi, Yuankai Huo, Kurt G. Schilling, Walter A. Kukull, Arthur W. Toga, Derek B. Archer, Timothy J. Hohman, Bennett A. Landman",http://arxiv.org/pdf/2409.13846v1,cs.LG
The Impact of Large Language Models in Academia: from Writing to Speaking,"Large language models (LLMs) are increasingly impacting human society,
particularly in textual information. Based on more than 30,000 papers and 1,000
presentations from machine learning conferences, we examined and compared the
words used in writing and speaking, representing the first large-scale study of
how LLMs influence the two main modes of verbal communication and expression
within the same group of people. Our empirical results show that LLM-style
words such as ""significant"" have been used more frequently in abstracts and
oral presentations. The impact on speaking is beginning to emerge and is likely
to grow in the future, calling attention to the implicit influence and ripple
effect of LLMs on human society.",2024-09-20,"Mingmeng Geng, Caixi Chen, Yanru Wu, Dongping Chen, Yao Wan, Pan Zhou",http://arxiv.org/pdf/2409.13686v2,cs.LG
The FIX Benchmark: Extracting Features Interpretable to eXperts,"Feature-based methods are commonly used to explain model predictions, but
these methods often implicitly assume that interpretable features are readily
available. However, this is often not the case for high-dimensional data, and
it can be hard even for domain experts to mathematically specify which features
are important. Can we instead automatically extract collections or groups of
features that are aligned with expert knowledge? To address this gap, we
present FIX (Features Interpretable to eXperts), a benchmark for measuring how
well a collection of features aligns with expert knowledge. In collaboration
with domain experts, we propose FIXScore, a unified expert alignment measure
applicable to diverse real-world settings across cosmology, psychology, and
medicine domains in vision, language, and time series data modalities. With
FIXScore, we find that popular feature-based explanation methods have poor
alignment with expert-specified knowledge, highlighting the need for new
methods that can better identify features interpretable to experts.",2024-09-20,"Helen Jin, Shreya Havaldar, Chaehyeon Kim, Anton Xue, Weiqiu You, Helen Qu, Marco Gatti, Daniel A Hashimoto, Bhuvnesh Jain, Amin Madani, Masao Sako, Lyle Ungar, Eric Wong",http://arxiv.org/pdf/2409.13684v3,cs.LG
SoloParkour: Constrained Reinforcement Learning for Visual Locomotion from Privileged Experience,"Parkour poses a significant challenge for legged robots, requiring navigation
through complex environments with agility and precision based on limited
sensory inputs. In this work, we introduce a novel method for training
end-to-end visual policies, from depth pixels to robot control commands, to
achieve agile and safe quadruped locomotion. We formulate robot parkour as a
constrained reinforcement learning (RL) problem designed to maximize the
emergence of agile skills within the robot's physical limits while ensuring
safety. We first train a policy without vision using privileged information
about the robot's surroundings. We then generate experience from this
privileged policy to warm-start a sample efficient off-policy RL algorithm from
depth images. This allows the robot to adapt behaviors from this privileged
experience to visual locomotion while circumventing the high computational
costs of RL directly from pixels. We demonstrate the effectiveness of our
method on a real Solo-12 robot, showcasing its capability to perform a variety
of parkour skills such as walking, climbing, leaping, and crawling.",2024-09-20,"Elliot Chane-Sane, Joseph Amigo, Thomas Flayols, Ludovic Righetti, Nicolas Mansard",http://arxiv.org/pdf/2409.13678v1,cs.LG
Blockchain-Enabled Variational Information Bottleneck for Data Extraction Based on Mutual Information in Internet of Vehicles,"The Internet of Vehicles (IoV) network can address the issue of limited
computing resources and data processing capabilities of individual vehicles,
but it also brings the risk of privacy leakage to vehicle users. Applying
blockchain technology can establish secure data links within the IoV, solving
the problems of insufficient computing resources for each vehicle and the
security of data transmission over the network. However, with the development
of the IoV, the amount of data interaction between multiple vehicles and
between vehicles and base stations, roadside units, etc., is continuously
increasing. There is a need to further reduce the interaction volume, and
intelligent data compression is key to solving this problem. The VIB technique
facilitates the training of encoding and decoding models, substantially
diminishing the volume of data that needs to be transmitted. This paper
introduces an innovative approach that integrates blockchain with VIB, referred
to as BVIB, designed to lighten computational workloads and reinforce the
security of the network. We first construct a new network framework by
separating the encoding and decoding networks to address the computational
burden issue, and then propose a new algorithm to enhance the security of IoV
networks. We also discuss the impact of the data extraction rate on system
latency to determine the most suitable data extraction rate. An experimental
framework combining Python and C++ has been established to substantiate the
efficacy of our BVIB approach. Comprehensive simulation studies indicate that
the BVIB consistently excels in comparison to alternative foundational
methodologies.",2024-09-20,"Cui Zhang, Wenjun Zhang, Qiong Wu, Pingyi Fan, Nan Cheng, Wen Chen, Khaled B. Letaief",http://arxiv.org/pdf/2409.17287v1,cs.LG
Recent Advances in Non-convex Smoothness Conditions and Applicability to Deep Linear Neural Networks,"The presence of non-convexity in smooth optimization problems arising from
deep learning have sparked new smoothness conditions in the literature and
corresponding convergence analyses. We discuss these smoothness conditions,
order them, provide conditions for determining whether they hold, and evaluate
their applicability to training a deep linear neural network for binary
classification.",2024-09-20,"Vivak Patel, Christian Varner",http://arxiv.org/pdf/2409.13672v1,cs.LG
A Generative Framework for Predictive Modeling of Multiple Chronic Conditions Using Graph Variational Autoencoder and Bandit-Optimized Graph Neural Network,"Predicting the emergence of multiple chronic conditions (MCC) is crucial for
early intervention and personalized healthcare, as MCC significantly impacts
patient outcomes and healthcare costs. Graph neural networks (GNNs) are
effective methods for modeling complex graph data, such as those found in MCC.
However, a significant challenge with GNNs is their reliance on an existing
graph structure, which is not readily available for MCC. To address this
challenge, we propose a novel generative framework for GNNs that constructs a
representative underlying graph structure by utilizing the distribution of the
data to enhance predictive analytics for MCC. Our framework employs a graph
variational autoencoder (GVAE) to capture the complex relationships in patient
data. This allows for a comprehensive understanding of individual health
trajectories and facilitates the creation of diverse patient stochastic
similarity graphs while preserving the original feature set. These variations
of patient stochastic similarity graphs, generated from the GVAE decoder, are
then processed by a GNN using a novel Laplacian regularization technique to
refine the graph structure over time and improves the prediction accuracy of
MCC. A contextual Bandit is designed to evaluate the stochastically generated
graphs and identify the best-performing graph for the GNN model iteratively
until model convergence. We validate the performance of the proposed contextual
Bandit algorithm against $\varepsilon$-Greedy and multi-armed Bandit algorithms
on a large cohort (n = 1,592) of patients with MCC. These advancements
highlight the potential of the proposed approach to transform predictive
healthcare analytics, enabling a more personalized and proactive approach to
MCC management.",2024-09-20,"Julian Carvajal Rico, Adel Alaeddini, Syed Hasib Akhter Faruqui, Susan P Fisher-Hoch, Joseph B Mccormick",http://arxiv.org/pdf/2409.13671v2,cs.LG
DiffFluid: Plain Diffusion Models are Effective Predictors of Flow Dynamics,"We showcase the plain diffusion models with Transformers are effective
predictors of fluid dynamics under various working conditions, e.g., Darcy flow
and high Reynolds number. Unlike traditional fluid dynamical solvers that
depend on complex architectures to extract intricate correlations and learn
underlying physical states, our approach formulates the prediction of flow
dynamics as the image translation problem and accordingly leverage the plain
diffusion model to tackle the problem. This reduction in model design
complexity does not compromise its ability to capture complex physical states
and geometric features of fluid dynamical equations, leading to high-precision
solutions. In preliminary tests on various fluid-related benchmarks, our
DiffFluid achieves consistent state-of-the-art performance, particularly in
solving the Navier-Stokes equations in fluid dynamics, with a relative
precision improvement of +44.8%. In addition, we achieved relative improvements
of +14.0% and +11.3% in the Darcy flow equation and the airfoil problem with
Euler's equation, respectively. Code will be released at
https://github.com/DongyuLUO/DiffFluid upon acceptance.",2024-09-20,"Dongyu Luo, Jianyu Wu, Jing Wang, Hairun Xie, Xiangyu Yue, Shixiang Tang",http://arxiv.org/pdf/2409.13665v1,cs.LG
Analysis of Gene Regulatory Networks from Gene Expression Using Graph Neural Networks,"Unraveling the complexities of Gene Regulatory Networks (GRNs) is crucial for
understanding cellular processes and disease mechanisms. Traditional
computational methods often struggle with the dynamic nature of these networks.
This study explores the use of Graph Neural Networks (GNNs), a powerful
approach for modeling graph-structured data like GRNs. Utilizing a Graph
Attention Network v2 (GATv2), our study presents a novel approach to the
construction and interrogation of GRNs, informed by gene expression data and
Boolean models derived from literature. The model's adeptness in accurately
predicting regulatory interactions and pinpointing key regulators is attributed
to advanced attention mechanisms, a hallmark of the GNN framework. These
insights suggest that GNNs are primed to revolutionize GRN analysis, addressing
traditional limitations and offering richer biological insights. The success of
GNNs, as highlighted by our model's reliance on high-quality data, calls for
enhanced data collection methods to sustain progress. The integration of GNNs
in GRN research is set to pioneer developments in personalized medicine, drug
discovery, and our grasp of biological systems, bolstered by the structural
analysis of networks for improved node and edge prediction.",2024-09-20,"Hakan T. Otal, Abdulhamit Subasi, Furkan Kurt, M. Abdullah Canbaz, Yasin Uzun",http://arxiv.org/pdf/2409.13664v1,cs.LG
Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models,"Simulation-based testing is widely used to assess the reliability of
Autonomous Driving Systems (ADS), but its effectiveness is limited by the
operational design domain (ODD) conditions available in such simulators. To
address this limitation, in this work, we explore the integration of generative
artificial intelligence techniques with physics-based simulators to enhance ADS
system-level testing. Our study evaluates the effectiveness and computational
overhead of three generative strategies based on diffusion models, namely
instruction-editing, inpainting, and inpainting with refinement. Specifically,
we assess these techniques' capabilities to produce augmented
simulator-generated images of driving scenarios representing new ODDs. We
employ a novel automated detector for invalid inputs based on semantic
segmentation to ensure semantic preservation and realism of the neural
generated images. We then perform system-level testing to evaluate the ADS's
generalization ability to newly synthesized ODDs. Our findings show that
diffusion models help increase the ODD coverage for system-level testing of
ADS. Our automated semantic validator achieved a percentage of false positives
as low as 3%, retaining the correctness and quality of the generated images for
testing. Our approach successfully identified new ADS system failures before
real-world testing.",2024-09-20,"Luciano Baresi, Davide Yi Xian Hu, Andrea Stocco, Paolo Tonella",http://arxiv.org/pdf/2409.13661v3,cs.LG
Adaptive Mixture Importance Sampling for Automated Ads Auction Tuning,"This paper introduces Adaptive Mixture Importance Sampling (AMIS) as a novel
approach for optimizing key performance indicators (KPIs) in large-scale
recommender systems, such as online ad auctions. Traditional importance
sampling (IS) methods face challenges in dynamic environments, particularly in
navigating through complexities of multi-modal landscapes and avoiding
entrapment in local optima for the optimization task. Instead of updating
importance weights and mixing samples across iterations, as in canonical
adaptive IS and multiple IS, our AMIS framework leverages a mixture
distribution as the proposal distribution and dynamically adjusts both the
mixture parameters and their mixing rates at each iteration, thereby enhancing
search diversity and efficiency.
  Through extensive offline simulations, we demonstrate that AMIS significantly
outperforms simple Gaussian Importance Sampling (GIS), particularly in noisy
environments. Moreover, our approach is validated in real-world scenarios
through online A/B experiments on a major search engine, where AMIS
consistently identifies optimal tuning points that are more likely to be
adopted as mainstream configurations. These findings indicate that AMIS
enhances convergence in noisy environments, leading to more accurate and
reliable decision-making in the context of importance sampling off-policy
estimators.",2024-09-20,"Yimeng Jia, Kaushal Paneri, Rong Huang, Kailash Singh Maurya, Pavan Mallapragada, Yifan Shi",http://arxiv.org/pdf/2409.13655v1,cs.LG
Neural filtering for Neural Network-based Models of Dynamic Systems,"The application of neural networks in modeling dynamic systems has become
prominent due to their ability to estimate complex nonlinear functions. Despite
their effectiveness, neural networks face challenges in long-term predictions,
where the prediction error diverges over time, thus degrading their accuracy.
This paper presents a neural filter to enhance the accuracy of long-term state
predictions of neural network-based models of dynamic systems. Motivated by the
extended Kalman filter, the neural filter combines the neural network state
predictions with the measurements from the physical system to improve the
estimated state's accuracy. The neural filter's improvements in prediction
accuracy are demonstrated through applications to four nonlinear dynamical
systems. Numerical experiments show that the neural filter significantly
improves prediction accuracy and bounds the state estimate covariance,
outperforming the neural network predictions.",2024-09-20,"Parham Oveissi, Turibius Rozario, Ankit Goel",http://arxiv.org/pdf/2409.13654v1,cs.LG
OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition,"The recent paradigm shift to large-scale foundation models has brought about
a new era for deep learning that, while has found great success in practice,
has also been plagued by prohibitively expensive costs in terms of high memory
consumption and compute. To mitigate these issues, there has been a concerted
effort in post-hoc neural network pruning techniques that do not require costly
retraining. Despite the considerable progress being made, existing methods
often exhibit a steady drop in model performance as the compression increases.
In this paper, we present a novel approach to compressing large transformers,
coined OATS, that utilizes the second moment information in the input
embeddings to decompose the model weights into a sum of sparse and low-rank
matrices. Without any retraining, OATS achieves state-of-the-art performance
when compressing models by up to $60\%$ on large language models such as
Llama-3 and Phi-3 and vision transformers such as ViT and DINOv2 while
delivering up to $1.37\times$ the CPU acceleration versus a model that was
comparably pruned.",2024-09-20,"Stephen Zhang, Vardan Papyan",http://arxiv.org/pdf/2409.13652v3,cs.LG
DP$^2$-FedSAM: Enhancing Differentially Private Federated Learning Through Personalized Sharpness-Aware Minimization,"Federated learning (FL) is a distributed machine learning approach that
allows multiple clients to collaboratively train a model without sharing their
raw data. To prevent sensitive information from being inferred through the
model updates shared in FL, differentially private federated learning (DPFL)
has been proposed. DPFL ensures formal and rigorous privacy protection in FL by
clipping and adding random noise to the shared model updates. However, the
existing DPFL methods often result in severe model utility degradation,
especially in settings with data heterogeneity. To enhance model utility, we
propose a novel DPFL method named DP$^2$-FedSAM: Differentially Private and
Personalized Federated Learning with Sharpness-Aware Minimization.
DP$^2$-FedSAM leverages personalized partial model-sharing and sharpness-aware
minimization optimizer to mitigate the adverse impact of noise addition and
clipping, thereby significantly improving model utility without sacrificing
privacy. From a theoretical perspective, we provide a rigorous theoretical
analysis of the privacy and convergence guarantees of our proposed method. To
evaluate the effectiveness of DP$^2$-FedSAM, we conduct extensive evaluations
based on common benchmark datasets. Our results verify that our method improves
the privacy-utility trade-off compared to the existing DPFL methods,
particularly in heterogeneous data settings.",2024-09-20,"Zhenxiao Zhang, Yuanxiong Guo, Yanmin Gong",http://arxiv.org/pdf/2409.13645v1,cs.LG
"Non-overlapping, Schwarz-type Domain Decomposition Method for Physics and Equality Constrained Artificial Neural Networks","We present a non-overlapping, Schwarz-type domain decomposition method with a
generalized interface condition, designed for physics-informed machine learning
of partial differential equations (PDEs) in both forward and inverse contexts.
Our approach employs physics and equality-constrained artificial neural
networks (PECANN) within each subdomain. Unlike the original PECANN method,
which relies solely on initial and boundary conditions to constrain PDEs, our
method uses both boundary conditions and the governing PDE to constrain a
unique interface loss function for each subdomain. This modification improves
the learning of subdomain-specific interface parameters while reducing
communication overhead by delaying information exchange between neighboring
subdomains. To address the constrained optimization in each subdomain, we apply
an augmented Lagrangian method with a conditionally adaptive update strategy,
transforming the problem into an unconstrained dual optimization. A distinct
advantage of our domain decomposition method is its ability to learn solutions
to both Poisson's and Helmholtz equations, even in cases with high-wavenumber
and complex-valued solutions. Through numerical experiments with up to 64
subdomains, we demonstrate that our method consistently generalizes well as the
number of subdomains increases.",2024-09-20,"Qifeng Hu, Shamsulhaq Basir, Inanc Senocak",http://arxiv.org/pdf/2409.13644v2,cs.LG
Benchmarking Reliability of Deep Learning Models for Pathological Gait Classification,"Early detection of neurodegenerative disorders is an important open problem,
since early diagnosis and treatment may yield a better prognosis. Researchers
have recently sought to leverage advances in machine learning algorithms to
detect symptoms of altered gait, possibly corresponding to the emergence of
neurodegenerative etiologies. However, while several claims of positive and
accurate detection have been made in the recent literature, using a variety of
sensors and algorithms, solutions are far from being realized in practice. This
paper analyzes existing approaches to identify gaps inhibiting translation.
Using a set of experiments across three Kinect-simulated and one real
Parkinson's patient datasets, we highlight possible sources of errors and
generalization failures in these approaches. Based on these observations, we
propose our strong baseline called Asynchronous Multi-Stream Graph
Convolutional Network (AMS-GCN) that can reliably differentiate multiple
categories of pathological gaits across datasets.",2024-09-20,"Abhishek Jaiswal, Nisheeth Srivastava",http://arxiv.org/pdf/2409.13643v1,cs.LG
Transformers in Uniform TC$^0$,"Previous work has shown that the languages recognized by average-hard
attention transformers (AHATs) and softmax-attention transformers (SMATs) are
within the circuit complexity class TC$^0$. However, these results assume
limited-precision arithmetic: using floating-point numbers with O(log n) bits
(where n is the length of the input string), Strobl showed that AHATs can be
approximated in L-uniform TC$^0$, and Merrill and Sabharwal showed that SMATs
can be approximated in DLOGTIME-uniform TC$^0$. Here, we improve these results,
showing that AHATs with no approximation, SMATs with O(poly(n)) bits of
floating-point precision, and SMATs with at most $2^{-O(poly(n))}$ absolute
error are all in DLOGTIME-uniform TC$^0$.",2024-09-20,David Chiang,http://arxiv.org/pdf/2409.13629v2,cs.LG
Beauty Beyond Words: Explainable Beauty Product Recommendations Using Ingredient-Based Product Attributes,"Accurate attribute extraction is critical for beauty product recommendations
and building trust with customers. This remains an open problem, as existing
solutions are often unreliable and incomplete. We present a system to extract
beauty-specific attributes using end-to-end supervised learning based on beauty
product ingredients. A key insight to our system is a novel energy-based
implicit model architecture. We show that this implicit model architecture
offers significant benefits in terms of accuracy, explainability, robustness,
and flexibility. Furthermore, our implicit model can be easily fine-tuned to
incorporate additional attributes as they become available, making it more
useful in real-world applications. We validate our model on a major e-commerce
skincare product catalog dataset and demonstrate its effectiveness. Finally, we
showcase how ingredient-based attribute extraction contributes to enhancing the
explainability of beauty recommendations.",2024-09-20,"Siliang Liu, Rahul Suresh, Amin Banitalebi-Dehkordi",http://arxiv.org/pdf/2409.13628v1,cs.LG
Improved Unet brain tumor image segmentation based on GSConv module and ECA attention mechanism,"An improved model of medical image segmentation for brain tumor is discussed,
which is a deep learning algorithm based on U-Net architecture. Based on the
traditional U-Net, we introduce GSConv module and ECA attention mechanism to
improve the performance of the model in medical image segmentation tasks. With
these improvements, the new U-Net model is able to extract and utilize
multi-scale features more efficiently while flexibly focusing on important
channels, resulting in significantly improved segmentation results. During the
experiment, the improved U-Net model is trained and evaluated systematically.
By looking at the loss curves of the training set and the test set, we find
that the loss values of both rapidly decline to the lowest point after the
eighth epoch, and then gradually converge and stabilize. This shows that our
model has good learning ability and generalization ability. In addition, by
monitoring the change in the mean intersection ratio (mIoU), we can see that
after the 35th epoch, the mIoU gradually approaches 0.8 and remains stable,
which further validates the model. Compared with the traditional U-Net, the
improved version based on GSConv module and ECA attention mechanism shows
obvious advantages in segmentation effect. Especially in the processing of
brain tumor image edges, the improved model can provide more accurate
segmentation results. This achievement not only improves the accuracy of
medical image analysis, but also provides more reliable technical support for
clinical diagnosis.",2024-09-20,"Qiyuan Tian, Zhuoyue Wang, Xiaoling Cui",http://arxiv.org/pdf/2409.13626v1,cs.LG
pAE: An Efficient Autoencoder Architecture for Modeling the Lateral Geniculate Nucleus by Integrating Feedforward and Feedback Streams in Human Visual System,"The visual cortex is a vital part of the brain, responsible for
hierarchically identifying objects. Understanding the role of the lateral
geniculate nucleus (LGN) as a prior region of the visual cortex is crucial when
processing visual information in both bottom-up and top-down pathways. When
visual stimuli reach the retina, they are transmitted to the LGN area for
initial processing before being sent to the visual cortex for further
processing. In this study, we introduce a deep convolutional model that closely
approximates human visual information processing. We aim to approximate the
function for the LGN area using a trained shallow convolutional model which is
designed based on a pruned autoencoder (pAE) architecture. The pAE model
attempts to integrate feed forward and feedback streams from/to the V1 area
into the problem. This modeling framework encompasses both temporal and
non-temporal data feeding modes of the visual stimuli dataset containing
natural images captured by a fixed camera in consecutive frames, featuring two
categories: images with animals (in motion), and images without animals.
Subsequently, we compare the results of our proposed deep-tuned model with
wavelet filter bank methods employing Gabor and biorthogonal wavelet functions.
Our experiments reveal that the proposed method based on the deep-tuned model
not only achieves results with high similarity in comparison with human
benchmarks but also performs significantly better than other models. The pAE
model achieves the final 99.26% prediction performance and demonstrates a
notable improvement of around 28% over human results in the temporal mode.",2024-09-20,"Moslem Gorji, Amin Ranjbar, Mohammad Bagher Menhaj",http://arxiv.org/pdf/2409.13622v1,cs.LG
Transforming disaster risk reduction with AI and big data: Legal and interdisciplinary perspectives,"Managing complex disaster risks requires interdisciplinary efforts. Breaking
down silos between law, social sciences, and natural sciences is critical for
all processes of disaster risk reduction. This enables adaptive systems for the
rapid evolution of AI technology, which has significantly impacted the
intersection of law and natural environments. Exploring how AI influences legal
frameworks and environmental management, while also examining how legal and
environmental considerations can confine AI within the socioeconomic domain, is
essential.
  From a co-production review perspective, drawing on insights from lawyers,
social scientists, and environmental scientists, principles for responsible
data mining are proposed based on safety, transparency, fairness,
accountability, and contestability. This discussion offers a blueprint for
interdisciplinary collaboration to create adaptive law systems based on AI
integration of knowledge from environmental and social sciences. Discrepancies
in the use of language between environmental scientists and decision-makers in
terms of usefulness and accuracy hamper how AI can be used based on the
principles of legal considerations for a safe, trustworthy, and contestable
disaster management framework.
  When social networks are useful for mitigating disaster risks based on AI,
the legal implications related to privacy and liability of the outcomes of
disaster management must be considered. Fair and accountable principles
emphasise environmental considerations and foster socioeconomic discussions
related to public engagement. AI also has an important role to play in
education, bringing together the next generations of law, social sciences, and
natural sciences to work on interdisciplinary solutions in harmony.",2024-09-20,"Kwok P Chun, Thanti Octavianti, Nilay Dogulu, Hristos Tyralis, Georgia Papacharalampous, Ryan Rowberry, Pingyu Fan, Mark Everard, Maria Francesch-Huidobro, Wellington Migliari, David M. Hannah, John Travis Marshall, Rafael Tolosana Calasanz, Chad Staddon, Ida Ansharyani, Bastien Dieppois, Todd R Lewis, Juli Ponce, Silvia Ibrean, Tiago Miguel Ferreira, Chinkie Peliño-Golle, Ye Mu, Manuel Delgado, Elizabeth Silvestre Espinoza, Martin Keulertz, Deepak Gopinath, Cheng Li",http://arxiv.org/pdf/2410.07123v1,cs.LG
Towards Child-Inclusive Clinical Video Understanding for Autism Spectrum Disorder,"Clinical videos in the context of Autism Spectrum Disorder are often
long-form interactions between children and caregivers/clinical professionals,
encompassing complex verbal and non-verbal behaviors. Objective analyses of
these videos could provide clinicians and researchers with nuanced insights
into the behavior of children with Autism Spectrum Disorder. Manually coding
these videos is a time-consuming task and requires a high level of domain
expertise. Hence, the ability to capture these interactions computationally can
augment the manual effort and enable supporting the diagnostic procedure. In
this work, we investigate the use of foundation models across three modalities:
speech, video, and text, to analyse child-focused interaction sessions. We
propose a unified methodology to combine multiple modalities by using large
language models as reasoning agents. We evaluate their performance on two tasks
with different information granularity: activity recognition and abnormal
behavior detection. We find that the proposed multimodal pipeline provides
robustness to modality-specific limitations and improves performance on the
clinical video analysis compared to unimodal settings.",2024-09-20,"Aditya Kommineni, Digbalay Bose, Tiantian Feng, So Hyun Kim, Helen Tager-Flusberg, Somer Bishop, Catherine Lord, Sudarsana Kadiri, Shrikanth Narayanan",http://arxiv.org/pdf/2409.13606v1,cs.LG
Prithvi WxC: Foundation Model for Weather and Climate,"Triggered by the realization that AI emulators can rival the performance of
traditional numerical weather prediction models running on HPC systems, there
is now an increasing number of large AI models that address use cases such as
forecasting, downscaling, or nowcasting. While the parallel developments in the
AI literature focus on foundation models -- models that can be effectively
tuned to address multiple, different use cases -- the developments on the
weather and climate side largely focus on single-use cases with particular
emphasis on mid-range forecasting. We close this gap by introducing Prithvi
WxC, a 2.3 billion parameter foundation model developed using 160 variables
from the Modern-Era Retrospective Analysis for Research and Applications,
Version 2 (MERRA-2). Prithvi WxC employs an encoder-decoder-based architecture,
incorporating concepts from various recent transformer models to effectively
capture both regional and global dependencies in the input data. The model has
been designed to accommodate large token counts to model weather phenomena in
different topologies at fine resolutions. Furthermore, it is trained with a
mixed objective that combines the paradigms of masked reconstruction with
forecasting. We test the model on a set of challenging downstream tasks namely:
Autoregressive rollout forecasting, Downscaling, Gravity wave flux
parameterization, and Extreme events estimation. The pretrained model with 2.3
billion parameters, along with the associated fine-tuning workflows, has been
publicly released as an open-source contribution via Hugging Face.",2024-09-20,"Johannes Schmude, Sujit Roy, Will Trojak, Johannes Jakubik, Daniel Salles Civitarese, Shraddha Singh, Julian Kuehnert, Kumar Ankur, Aman Gupta, Christopher E Phillips, Romeo Kienzler, Daniela Szwarcman, Vishal Gaur, Rajat Shinde, Rohit Lal, Arlindo Da Silva, Jorge Luis Guevara Diaz, Anne Jones, Simon Pfreundschuh, Amy Lin, Aditi Sheshadri, Udaysankar Nair, Valentine Anantharaj, Hendrik Hamann, Campbell Watson, Manil Maskey, Tsengdar J Lee, Juan Bernabe Moreno, Rahul Ramachandran",http://arxiv.org/pdf/2409.13598v1,cs.LG
Neurosymbolic Conformal Classification,"The last decades have seen a drastic improvement of Machine Learning (ML),
mainly driven by Deep Learning (DL). However, despite the resounding successes
of ML in many domains, the impossibility to provide guarantees of conformity
and the fragility of ML systems (faced with distribution shifts, adversarial
attacks, etc.) have prevented the design of trustworthy AI systems. Several
research paths have been investigated to mitigate this fragility and provide
some guarantees regarding the behavior of ML systems, among which are
neurosymbolic AI and conformal prediction. Neurosymbolic artificial
intelligence is a growing field of research aiming to combine neural network
learning capabilities with the reasoning abilities of symbolic systems. One of
the objective of this hybridization can be to provide theoritical guarantees
that the output of the system will comply with some prior knowledge. Conformal
prediction is a set of techniques that enable to take into account the
uncertainty of ML systems by transforming the unique prediction into a set of
predictions, called a confidence set. Interestingly, this comes with
statistical guarantees regarding the presence of the true label inside the
confidence set. Both approaches are distribution-free and model-agnostic. In
this paper, we see how these two approaches can complement one another. We
introduce several neurosymbolic conformal prediction techniques and explore
their different characteristics (size of confidence sets, computational
complexity, etc.).",2024-09-20,"Arthur Ledaguenel, Céline Hudelot, Mostepha Khouadjia",http://arxiv.org/pdf/2409.13585v1,cs.LG
"Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Tensorflow Pretrained Models","The application of TensorFlow pre-trained models in deep learning is
explored, with an emphasis on practical guidance for tasks such as image
classification and object detection. The study covers modern architectures,
including ResNet, MobileNet, and EfficientNet, and demonstrates the
effectiveness of transfer learning through real-world examples and experiments.
A comparison of linear probing and model fine-tuning is presented, supplemented
by visualizations using techniques like PCA, t-SNE, and UMAP, allowing for an
intuitive understanding of the impact of these approaches. The work provides
complete example code and step-by-step instructions, offering valuable insights
for both beginners and advanced users. By integrating theoretical concepts with
hands-on practice, the paper equips readers with the tools necessary to address
deep learning challenges efficiently.",2024-09-20,"Keyu Chen, Ziqian Bi, Qian Niu, Junyu Liu, Benji Peng, Sen Zhang, Ming Liu, Ming Li, Xuanhe Pan, Jiawei Xu, Jinlang Wang, Pohsun Feng",http://arxiv.org/pdf/2409.13566v2,cs.LG
A preliminary study on continual learning in computer vision using Kolmogorov-Arnold Networks,"Deep learning has long been dominated by multi-layer perceptrons (MLPs),
which have demonstrated superiority over other optimizable models in various
domains. Recently, a new alternative to MLPs has emerged - Kolmogorov-Arnold
Networks (KAN)- which are based on a fundamentally different mathematical
framework. According to their authors, KANs address several major issues in
MLPs, such as catastrophic forgetting in continual learning scenarios. However,
this claim has only been supported by results from a regression task on a toy
1D dataset. In this paper, we extend the investigation by evaluating the
performance of KANs in continual learning tasks within computer vision,
specifically using the MNIST datasets. To this end, we conduct a structured
analysis of the behavior of MLPs and two KAN-based models in a
class-incremental learning scenario, ensuring that the architectures involved
have the same number of trainable parameters. Our results demonstrate that an
efficient version of KAN outperforms both traditional MLPs and the original KAN
implementation. We further analyze the influence of hyperparameters in MLPs and
KANs, as well as the impact of certain trainable parameters in KANs, such as
bias and scale weights. Additionally, we provide a preliminary investigation of
recent KAN-based convolutional networks and compare their performance with that
of traditional convolutional neural networks. Our codes can be found at
https://github.com/MrPio/KAN-Continual_Learning_tests.",2024-09-20,"Alessandro Cacciatore, Valerio Morelli, Federica Paganica, Emanuele Frontoni, Lucia Migliorelli, Daniele Berardini",http://arxiv.org/pdf/2409.13550v2,cs.LG
Certified Adversarial Robustness via Partition-based Randomized Smoothing,"A reliable application of deep neural network classifiers requires robustness
certificates against adversarial perturbations. Gaussian smoothing is a widely
analyzed approach to certifying robustness against norm-bounded perturbations,
where the certified prediction radius depends on the variance of the Gaussian
noise and the confidence level of the neural net's prediction under the
additive Gaussian noise. However, in application to high-dimensional image
datasets, the certified radius of the plain Gaussian smoothing could be
relatively small, since Gaussian noise with high variances can significantly
harm the visibility of an image. In this work, we propose the Pixel
Partitioning-based Randomized Smoothing (PPRS) methodology to boost the neural
net's confidence score and thus the robustness radius of the certified
prediction. We demonstrate that the proposed PPRS algorithm improves the
visibility of the images under additive Gaussian noise. We discuss the
numerical results of applying PPRS to standard computer vision datasets and
neural network architectures. Our empirical findings indicate a considerable
improvement in the certified accuracy and stability of the prediction model to
the additive Gaussian noise in randomized smoothing.",2024-09-20,"Hossein Goli, Farzan Farnia",http://arxiv.org/pdf/2409.13546v1,cs.LG
Graph Similarity Regularized Softmax for Semi-Supervised Node Classification,"Graph Neural Networks (GNNs) are powerful deep learning models designed for
graph-structured data, demonstrating effectiveness across a wide range of
applications.The softmax function is the most commonly used classifier for
semi-supervised node classification. However, the softmax function lacks
spatial information of the graph structure. In this paper, we propose a graph
similarity regularized softmax for GNNs in semi-supervised node classification.
By incorporating non-local total variation (TV) regularization into the softmax
activation function, we can more effectively capture the spatial information
inherent in graphs. The weights in the non-local gradient and divergence
operators are determined based on the graph's adjacency matrix. We apply the
proposed method into the architecture of GCN and GraphSAGE, testing them on
citation and webpage linking datasets, respectively. Numerical experiments
demonstrate its good performance in node classification and generalization
capabilities. These results indicate that the graph similarity regularized
softmax is effective on both assortative and disassortative graphs.",2024-09-20,"Yiming Yang, Jun Liu, Wei Wan",http://arxiv.org/pdf/2409.13544v1,cs.LG
First Place Solution to the Multiple-choice Video QA Track of The Second Perception Test Challenge,"In this report, we present our first-place solution to the Multiple-choice
Video Question Answering (QA) track of The Second Perception Test Challenge.
This competition posed a complex video understanding task, requiring models to
accurately comprehend and answer questions about video content. To address this
challenge, we leveraged the powerful QwenVL2 (7B) model and fine-tune it on the
provided training set. Additionally, we employed model ensemble strategies and
Test Time Augmentation to boost performance. Through continuous optimization,
our approach achieved a Top-1 Accuracy of 0.7647 on the leaderboard.",2024-09-20,"Yingzhe Peng, Yixiao Yuan, Zitian Ao, Huapeng Zhou, Kangqi Wang, Qipeng Zhu, Xu Yang",http://arxiv.org/pdf/2409.13538v1,cs.LG
Using High-Level Patterns to Estimate How Humans Predict a Robot will Behave,"Humans interacting with robots often form predictions of what the robot will
do next. For instance, based on the recent behavior of an autonomous car, a
nearby human driver might predict that the car is going to remain in the same
lane. It is important for the robot to understand the human's prediction for
safe and seamless interaction: e.g., if the autonomous car knows the human
thinks it is not merging -- but the autonomous car actually intends to merge --
then the car can adjust its behavior to prevent an accident. Prior works
typically assume that humans make precise predictions of robot behavior.
However, recent research on human-human prediction suggests the opposite:
humans tend to approximate other agents by predicting their high-level
behaviors. We apply this finding to develop a second-order theory of mind
approach that enables robots to estimate how humans predict they will behave.
To extract these high-level predictions directly from data, we embed the recent
human and robot trajectories into a discrete latent space. Each element of this
latent space captures a different type of behavior (e.g., merging in front of
the human, remaining in the same lane) and decodes into a vector field across
the state space that is consistent with the underlying behavior type. We
hypothesize that our resulting high-level and course predictions of robot
behavior will correspond to actual human predictions. We provide initial
evidence in support of this hypothesis through proof-of-concept simulations,
testing our method's predictions against those of real users, and experiments
on a real-world interactive driving dataset.",2024-09-20,"Sagar Parekh, Lauren Bramblett, Nicola Bezzo, Dylan P. Losey",http://arxiv.org/pdf/2409.13533v2,cs.LG
Towards Long-Context Time Series Foundation Models,"Time series foundation models have shown impressive performance on a variety
of tasks, across a wide range of domains, even in zero-shot settings. However,
most of these models are designed to handle short univariate time series as an
input. This limits their practical use, especially in domains such as
healthcare with copious amounts of long and multivariate data with strong
temporal and intra-variate dependencies. Our study bridges this gap by
cataloging and systematically comparing various context expansion techniques
from both language and time series domains, and introducing a novel compressive
memory mechanism to allow encoder-only TSFMs to effectively model intra-variate
dependencies. We demonstrate the benefits of our approach by imbuing MOMENT, a
recent family of multi-task time series foundation models, with the
multivariate context.",2024-09-20,"Nina Żukowska, Mononito Goswami, Michał Wiliński, Willa Potosnak, Artur Dubrawski",http://arxiv.org/pdf/2409.13530v1,cs.LG
End-Cloud Collaboration Framework for Advanced AI Customer Service in E-commerce,"In recent years, the e-commerce industry has seen a rapid increase in the
demand for advanced AI-driven customer service solutions. Traditional
cloud-based models face limitations in terms of latency, personalized services,
and privacy concerns. Furthermore, end devices often lack the computational
resources to deploy large AI models effectively. In this paper, we propose an
innovative End-Cloud Collaboration (ECC) framework for advanced AI customer
service in e-commerce. This framework integrates the advantages of large cloud
models and mid/small-sized end models by deeply exploring the generalization
potential of cloud models and effectively utilizing the computing power
resources of terminal chips, alleviating the strain on computing resources to
some extent. Specifically, the large cloud model acts as a teacher, guiding and
promoting the learning of the end model, which significantly reduces the end
model's reliance on large-scale, high-quality data and thereby addresses the
data bottleneck in traditional end model training, offering a new paradigm for
the rapid deployment of industry applications. Additionally, we introduce an
online evolutive learning strategy that enables the end model to continuously
iterate and upgrade based on guidance from the cloud model and real-time user
feedback. This strategy ensures that the model can flexibly adapt to the rapid
changes in application scenarios while avoiding the uploading of sensitive
information by performing local fine-tuning, achieving the dual goals of
privacy protection and personalized service. %We make systematic contributions
to the customized model fine-tuning methods in the e-commerce domain. To
conclude, we implement in-depth corpus collection (e.g., data organization,
cleaning, and preprocessing) and train an ECC-based industry-specific model for
e-commerce customer service.",2024-09-20,"Liangyu Teng, Yang Liu, Jing Liu, Liang Song",http://arxiv.org/pdf/2410.07122v1,cs.LG
SatFed: A Resource-Efficient LEO Satellite-Assisted Heterogeneous Federated Learning Framework,"Traditional federated learning (FL) frameworks rely heavily on terrestrial
networks, where coverage limitations and increasing bandwidth congestion
significantly hinder model convergence. Fortunately, the advancement of
low-Earth orbit (LEO) satellite networks offers promising new communication
avenues to augment traditional terrestrial FL. Despite this potential, the
limited satellite-ground communication bandwidth and the heterogeneous
operating environments of ground devices-including variations in data,
bandwidth, and computing power-pose substantial challenges for effective and
robust satellite-assisted FL. To address these challenges, we propose SatFed, a
resource-efficient satellite-assisted heterogeneous FL framework. SatFed
implements freshness-based model prioritization queues to optimize the use of
highly constrained satellite-ground bandwidth, ensuring the transmission of the
most critical models. Additionally, a multigraph is constructed to capture
real-time heterogeneous relationships between devices, including data
distribution, terrestrial bandwidth, and computing capability. This multigraph
enables SatFed to aggregate satellite-transmitted models into peer guidance,
enhancing local training in heterogeneous environments. Extensive experiments
with real-world LEO satellite networks demonstrate that SatFed achieves
superior performance and robustness compared to state-of-the-art benchmarks.",2024-09-20,"Yuxin Zhang, Zheng Lin, Zhe Chen, Zihan Fang, Wenjun Zhu, Xianhao Chen, Jin Zhao, Yue Gao",http://arxiv.org/pdf/2409.13503v3,cs.LG
Transfer Learning for E-commerce Query Product Type Prediction,"Getting a good understanding of the customer intent is essential in
e-commerce search engines. In particular, associating the correct product type
to a search query plays a vital role in surfacing correct products to the
customers. Query product type classification (Q2PT) is a particularly
challenging task because search queries are short and ambiguous, the number of
existing product categories is extremely large, spanning thousands of values.
Moreover, international marketplaces face additional challenges, such as
language and dialect diversity and cultural differences, influencing the
interpretation of the query. In this work we focus on Q2PT prediction in the
global multilocale e-commerce markets. The common approach of training Q2PT
models for each locale separately shows significant performance drops in
low-resource stores. Moreover, this method does not allow for a smooth
expansion to a new country, requiring to collect the data and train a new
locale-specific Q2PT model from scratch. To tackle this, we propose to use
transfer learning from the highresource to the low-resource locales, to achieve
global parity of Q2PT performance. We benchmark the per-locale Q2PT model
against the unified one, which shares the training data and model structure
across all worldwide stores. Additionally, we compare locale-aware and
locale-agnostic Q2PT models, showing the task dependency on the
country-specific traits. We conduct extensive quantiative and qualitative
analysis of Q2PT models on the large-scale e-commerce dataset across 20
worldwide locales, which shows that unified locale-aware Q2PT model has
superior performance over the alternatives.",2024-09-20,"Anna Tigunova, Thomas Ricatte, Ghadir Eraisha",http://arxiv.org/pdf/2410.07121v1,cs.LG
Invertible ResNets for Inverse Imaging Problems: Competitive Performance with Provable Regularization Properties,"Learning-based methods have demonstrated remarkable performance in solving
inverse problems, particularly in image reconstruction tasks. Despite their
success, these approaches often lack theoretical guarantees, which are crucial
in sensitive applications such as medical imaging. Recent works by Arndt et al
(2023 Inverse Problems 39 125018, 2024 Inverse Problems 40 045021) addressed
this gap by analyzing a data-driven reconstruction method based on invertible
residual networks (iResNets). They revealed that, under reasonable assumptions,
this approach constitutes a convergent regularization scheme. However, the
performance of the reconstruction method was only validated on academic toy
problems and small-scale iResNet architectures. In this work, we address this
gap by evaluating the performance of iResNets on two real-world imaging tasks:
a linear blurring operator and a nonlinear diffusion operator. To do so, we
extend some of the theoretical results from Arndt et al to encompass nonlinear
inverse problems and offer insights for the design of large-scale performant
iResNet architectures. Through numerical experiments, we compare the
performance of our iResNet models against state-of-the-art neural networks,
confirming their efficacy. Additionally, we numerically investigate the
theoretical guarantees of this approach and demonstrate how the invertibility
of the network enables a deeper analysis of the learned forward operator and
its learned regularization.",2024-09-20,"Clemens Arndt, Judith Nickel",http://arxiv.org/pdf/2409.13482v2,cs.LG
Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models,"Machine unlearning aims to efficiently eliminate the influence of specific
training data, known as the forget set, from the model. However, existing
unlearning methods for Large Language Models (LLMs) face a critical challenge:
they rely solely on negative feedback to suppress responses related to the
forget set, which often results in nonsensical or inconsistent outputs,
diminishing model utility and posing potential privacy risks. To address this
limitation, we propose a novel approach called Alternate Preference
Optimization (AltPO), which combines negative feedback with in-domain positive
feedback on the forget set. Additionally, we introduce new evaluation metrics
to assess the quality of responses related to the forget set. Extensive
experiments show that our approach not only enables effective unlearning but
also avoids undesirable model behaviors while maintaining overall model
performance. Our implementation can be found at
https://github.com/molereddy/Alternate-Preference-Optimization.",2024-09-20,"Anmol Mekala, Vineeth Dorna, Shreya Dubey, Abhishek Lalwani, David Koleczek, Mukund Rungta, Sadid Hasan, Elita Lobo",http://arxiv.org/pdf/2409.13474v3,cs.LG
Flotta: a Secure and Flexible Spark-inspired Federated Learning Framework,"We present Flotta, a Federated Learning framework designed to train machine
learning models on sensitive data distributed across a multi-party consortium
conducting research in contexts requiring high levels of security, such as the
biomedical field. Flotta is a Python package, inspired in several aspects by
Apache Spark, which provides both flexibility and security and allows
conducting research using solely machines internal to the consortium. In this
paper, we describe the main components of the framework together with a
practical use case to illustrate the framework's capabilities and highlight its
security, flexibility and user-friendliness.",2024-09-20,"Claudio Bonesana, Daniele Malpetti, Sandra Mitrović, Francesca Mangili, Laura Azzimonti",http://arxiv.org/pdf/2409.13473v1,cs.LG
Stimulus-to-Stimulus Learning in RNNs with Cortical Inductive Biases,"Animals learn to predict external contingencies from experience through a
process of conditioning. A natural mechanism for conditioning is stimulus
substitution, whereby the neuronal response to a stimulus with no prior
behavioral significance becomes increasingly identical to that generated by a
behaviorally significant stimulus it reliably predicts. We propose a recurrent
neural network model of stimulus substitution which leverages two forms of
inductive bias pervasive in the cortex: representational inductive bias in the
form of mixed stimulus representations, and architectural inductive bias in the
form of two-compartment pyramidal neurons that have been shown to serve as a
fundamental unit of cortical associative learning. The properties of these
neurons allow for a biologically plausible learning rule that implements
stimulus substitution, utilizing only information available locally at the
synapses. We show that the model generates a wide array of conditioning
phenomena, and can learn large numbers of associations with an amount of
training commensurate with animal experiments, without relying on parameter
fine-tuning for each individual experimental task. In contrast, we show that
commonly used Hebbian rules fail to learn generic stimulus-stimulus
associations with mixed selectivity, and require task-specific parameter
fine-tuning. Our framework highlights the importance of multi-compartment
neuronal processing in the cortex, and showcases how it might confer cortical
animals the evolutionary edge.",2024-09-20,"Pantelis Vafidis, Antonio Rangel",http://arxiv.org/pdf/2409.13471v1,cs.LG
Deterministic versus stochastic dynamical classifiers: opposing random adversarial attacks with noise,"The Continuous-Variable Firing Rate (CVFR) model, widely used in neuroscience
to describe the intertangled dynamics of excitatory biological neurons, is here
trained and tested as a veritable dynamically assisted classifier. To this end
the model is supplied with a set of planted attractors which are
self-consistently embedded in the inter-nodes coupling matrix, via its spectral
decomposition. Learning to classify amounts to sculp the basin of attraction of
the imposed equilibria, directing different items towards the corresponding
destination target, which reflects the class of respective pertinence. A
stochastic variant of the CVFR model is also studied and found to be robust to
aversarial random attacks, which corrupt the items to be classified. This
remarkable finding is one of the very many surprising effects which arise when
noise and dynamical attributes are made to mutually resonate.",2024-09-20,"Lorenzo Chicchi, Duccio Fanelli, Diego Febbe, Lorenzo Buffoni, Francesca Di Patti, Lorenzo Giambagli, Raffele Marino",http://arxiv.org/pdf/2409.13470v1,cs.LG
Higher-Order Message Passing for Glycan Representation Learning,"Glycans are the most complex biological sequence, with monosaccharides
forming extended, non-linear sequences. As post-translational modifications,
they modulate protein structure, function, and interactions. Due to their
diversity and complexity, predictive models of glycan properties and functions
are still insufficient.
  Graph Neural Networks (GNNs) are deep learning models designed to process and
analyze graph-structured data. These architectures leverage the connectivity
and relational information in graphs to learn effective representations of
nodes, edges, and entire graphs. Iteratively aggregating information from
neighboring nodes, GNNs capture complex patterns within graph data, making them
particularly well-suited for tasks such as link prediction or graph
classification across domains.
  This work presents a new model architecture based on combinatorial complexes
and higher-order message passing to extract features from glycan structures
into a latent space representation. The architecture is evaluated on an
improved GlycanML benchmark suite, establishing a new state-of-the-art
performance. We envision that these improvements will spur further advances in
computational glycosciences and reveal the roles of glycans in biology.",2024-09-20,"Roman Joeres, Daniel Bojar",http://arxiv.org/pdf/2409.13467v3,cs.LG
Global Outlier Detection in a Federated Learning Setting with Isolation Forest,"We present a novel strategy for detecting global outliers in a federated
learning setting, targeting in particular cross-silo scenarios. Our approach
involves the use of two servers and the transmission of masked local data from
clients to one of the servers. The masking of the data prevents the disclosure
of sensitive information while still permitting the identification of outliers.
Moreover, to further safeguard privacy, a permutation mechanism is implemented
so that the server does not know which client owns any masked data point. The
server performs outlier detection on the masked data, using either Isolation
Forest or its extended version, and then communicates outlier information back
to the clients, allowing them to identify and remove outliers in their local
datasets before starting any subsequent federated model training. This approach
provides comparable results to a centralized execution of Isolation Forest
algorithms on plain data.",2024-09-20,"Daniele Malpetti, Laura Azzimonti",http://arxiv.org/pdf/2409.13466v1,cs.LG
Data Compression using Rank-1 Lattices for Parameter Estimation in Machine Learning,"The mean squared error and regularized versions of it are standard loss
functions in supervised machine learning. However, calculating these losses for
large data sets can be computationally demanding. Modifying an approach of J.
Dick and M. Feischl [Journal of Complexity 67 (2021)], we present algorithms to
reduce extensive data sets to a smaller size using rank-1 lattices. Rank-1
lattices are quasi-Monte Carlo (QMC) point sets that are, if carefully chosen,
well-distributed in a multidimensional unit cube. The compression strategy in
the preprocessing step assigns every lattice point a pair of weights depending
on the original data and responses, representing its relative importance. As a
result, the compressed data makes iterative loss calculations in optimization
steps much faster. We analyze the errors of our QMC data compression algorithms
and the cost of the preprocessing step for functions whose Fourier coefficients
decay sufficiently fast so that they lie in certain Wiener algebras or Korobov
spaces. In particular, we prove that our approach can lead to arbitrary high
convergence rates as long as the functions are sufficiently smooth.",2024-09-20,"Michael Gnewuch, Kumar Harsha, Marcin Wnuk",http://arxiv.org/pdf/2409.13453v1,cs.LG
Noise-Robust and Resource-Efficient ADMM-based Federated Learning,"Federated learning (FL) leverages client-server communications to train
global models on decentralized data. However, communication noise or errors can
impair model accuracy. To address this problem, we propose a novel FL algorithm
that enhances robustness against communication noise while also reducing
communication load. We derive the proposed algorithm through solving the
weighted least-squares (WLS) regression problem as an illustrative example. We
first frame WLS regression as a distributed convex optimization problem over a
federated network employing random scheduling for improved communication
efficiency. We then apply the alternating direction method of multipliers
(ADMM) to iteratively solve this problem. To counteract the detrimental effects
of cumulative communication noise, we introduce a key modification by
eliminating the dual variable and implementing a new local model update at each
participating client. This subtle yet effective change results in using a
single noisy global model update at each client instead of two, improving
robustness against additive communication noise. Furthermore, we incorporate
another modification enabling clients to continue local updates even when not
selected by the server, leading to substantial performance improvements. Our
theoretical analysis confirms the convergence of our algorithm in both mean and
the mean-square senses, even when the server communicates with a random subset
of clients over noisy links at each iteration. Numerical results validate the
effectiveness of our proposed algorithm and corroborate our theoretical
findings.",2024-09-20,"Ehsan Lari, Reza Arablouei, Vinay Chakravarthi Gogineni, Stefan Werner",http://arxiv.org/pdf/2409.13451v1,cs.LG
Differentially Private Multimodal Laplacian Dropout (DP-MLD) for EEG Representative Learning,"Recently, multimodal electroencephalogram (EEG) learning has shown great
promise in disease detection. At the same time, ensuring privacy in clinical
studies has become increasingly crucial due to legal and ethical concerns. One
widely adopted scheme for privacy protection is differential privacy (DP)
because of its clear interpretation and ease of implementation. Although
numerous methods have been proposed under DP, it has not been extensively
studied for multimodal EEG data due to the complexities of models and signal
data considered there. In this paper, we propose a novel Differentially Private
Multimodal Laplacian Dropout (DP-MLD) scheme for multimodal EEG learning. Our
approach proposes a novel multimodal representative learning model that
processes EEG data by language models as text and other modal data by vision
transformers as images, incorporating well-designed cross-attention mechanisms
to effectively extract and integrate cross-modal features. To achieve DP, we
design a novel adaptive feature-level Laplacian dropout scheme, where
randomness allocation and performance are dynamically optimized within given
privacy budgets. In the experiment on an open-source multimodal dataset of
Freezing of Gait (FoG) in Parkinson's Disease (PD), our proposed method
demonstrates an approximate 4\% improvement in classification accuracy, and
achieves state-of-the-art performance in multimodal EEG learning under DP.",2024-09-20,"Xiaowen Fu, Bingxin Wang, Xinzhou Guo, Guoqing Liu, Yang Xiang",http://arxiv.org/pdf/2409.13440v1,cs.LG
A User Study on Contrastive Explanations for Multi-Effector Temporal Planning with Non-Stationary Costs,"In this paper, we adopt constrastive explanations within an end-user
application for temporal planning of smart homes. In this application, users
have requirements on the execution of appliance tasks, pay for energy according
to dynamic energy tariffs, have access to high-capacity battery storage, and
are able to sell energy to the grid. The concurrent scheduling of devices makes
this a multi-effector planning problem, while the dynamic tariffs yield costs
that are non-stationary (alternatively, costs that are stationary but depend on
exogenous events). These characteristics are such that the planning problems
are generally not supported by existing PDDL-based planners, so we instead
design a custom domain-dependent planner that scales to reasonable appliance
numbers and time horizons. We conduct a controlled user study with 128
participants using an online crowd-sourcing platform based on two user stories.
Our results indicate that users provided with contrastive questions and
explanations have higher levels of satisfaction, tend to gain improved
understanding, and rate the helpfulness more favourably with the recommended AI
schedule compared to those without access to these features.",2024-09-20,"Xiaowei Liu, Kevin McAreavey, Weiru Liu",http://arxiv.org/pdf/2409.13427v1,cs.LG
Causal Reinforcement Learning for Optimisation of Robot Dynamics in Unknown Environments,"Autonomous operations of robots in unknown environments are challenging due
to the lack of knowledge of the dynamics of the interactions, such as the
objects' movability. This work introduces a novel Causal Reinforcement Learning
approach to enhancing robotics operations and applies it to an urban search and
rescue (SAR) scenario. Our proposed machine learning architecture enables
robots to learn the causal relationships between the visual characteristics of
the objects, such as texture and shape, and the objects' dynamics upon
interaction, such as their movability, significantly improving their
decision-making processes. We conducted causal discovery and RL experiments
demonstrating the Causal RL's superior performance, showing a notable reduction
in learning times by over 24.5% in complex situations, compared to non-causal
models.",2024-09-20,"Julian Gerald Dcruz, Sam Mahoney, Jia Yun Chua, Adoundeth Soukhabandith, John Mugabe, Weisi Guo, Miguel Arana-Catania",http://arxiv.org/pdf/2409.13423v1,cs.LG
"State space models, emergence, and ergodicity: How many parameters are needed for stable predictions?","How many parameters are required for a model to execute a given task? It has
been argued that large language models, pre-trained via self-supervised
learning, exhibit emergent capabilities such as multi-step reasoning as their
number of parameters reach a critical scale. In the present work, we explore
whether this phenomenon can analogously be replicated in a simple theoretical
model. We show that the problem of learning linear dynamical systems -- a
simple instance of self-supervised learning -- exhibits a corresponding phase
transition. Namely, for every non-ergodic linear system there exists a critical
threshold such that a learner using fewer parameters than said threshold cannot
achieve bounded error for large sequence lengths. Put differently, in our model
we find that tasks exhibiting substantial long-range correlation require a
certain critical number of parameters -- a phenomenon akin to emergence. We
also investigate the role of the learner's parametrization and consider a
simple version of a linear dynamical system with hidden state -- an imperfectly
observed random walk in $\mathbb{R}$. For this situation, we show that there
exists no learner using a linear filter which can succesfully learn the random
walk unless the filter length exceeds a certain threshold depending on the
effective memory length and horizon of the problem.",2024-09-20,"Ingvar Ziemann, Nikolai Matni, George J. Pappas",http://arxiv.org/pdf/2409.13421v1,cs.LG
Occupancy-Based Dual Contouring,"We introduce a dual contouring method that provides state-of-the-art
performance for occupancy functions while achieving computation times of a few
seconds. Our method is learning-free and carefully designed to maximize the use
of GPU parallelization. The recent surge of implicit neural representations has
led to significant attention to occupancy fields, resulting in a wide range of
3D reconstruction and generation methods based on them. However, the outputs of
such methods have been underestimated due to the bottleneck in converting the
resulting occupancy function to a mesh. Marching Cubes tends to produce
staircase-like artifacts, and most subsequent works focusing on exploiting
signed distance functions as input also yield suboptimal results for occupancy
functions. Based on Manifold Dual Contouring (MDC), we propose Occupancy-Based
Dual Contouring (ODC), which mainly modifies the computation of grid edge
points (1D points) and grid cell points (3D points) to not use any distance
information. We introduce auxiliary 2D points that are used to compute local
surface normals along with the 1D points, helping identify 3D points via the
quadric error function. To search the 1D, 2D, and 3D points, we develop fast
algorithms that are parallelizable across all grid edges, faces, and cells. Our
experiments with several 3D neural generative models and a 3D mesh dataset
demonstrate that our method achieves the best fidelity compared to prior works.",2024-09-20,"Jisung Hwang, Minhyuk Sung",http://arxiv.org/pdf/2409.13418v1,cs.LG
Longitudinal Segmentation of MS Lesions via Temporal Difference Weighting,"Accurate segmentation of Multiple Sclerosis (MS) lesions in longitudinal MRI
scans is crucial for monitoring disease progression and treatment efficacy.
Although changes across time are taken into account when assessing images in
clinical practice, most existing deep learning methods treat scans from
different timepoints separately. Among studies utilizing longitudinal images, a
simple channel-wise concatenation is the primary albeit suboptimal method
employed to integrate timepoints. We introduce a novel approach that explicitly
incorporates temporal differences between baseline and follow-up scans through
a unique architectural inductive bias called Difference Weighting Block. It
merges features from two timepoints, emphasizing changes between scans. We
achieve superior scores in lesion segmentation (Dice Score, Hausdorff distance)
as well as lesion detection (lesion-level $F_1$ score) as compared to
state-of-the-art longitudinal and single timepoint models across two datasets.
Our code is made publicly available at
www.github.com/MIC-DKFZ/Longitudinal-Difference-Weighting.",2024-09-20,"Maximilian Rokuss, Yannick Kirchhoff, Saikat Roy, Balint Kovacs, Constantin Ulrich, Tassilo Wald, Maximilian Zenk, Stefan Denner, Fabian Isensee, Philipp Vollmuth, Jens Kleesiek, Klaus Maier-Hein",http://arxiv.org/pdf/2409.13416v1,cs.LG
Credit Card Fraud Detection: A Deep Learning Approach,"Credit card is one of the most extensive methods of instalment for both
online and offline mode of payment for electronic transactions in recent times.
credit cards invention has provided significant ease in electronic
transactions. However, it has also provided new fraud opportunities for
criminals, which results in increased fraud rates. Substantial amount of money
has been lost by many institutions and individuals due to fraudulent credit
card transactions. Adapting improved and dynamic fraud recognition frameworks
thus became essential for all credit card distributing banks to mitigate their
losses. In fact, the problem of fraudulent credit card transactions implicates
a number of relevant real-time challenges, namely: Concept drift, Class
imbalance, and Verification latency. However, the vast majority of current
systems are based on artificial intelligence (AI), Fuzzy logic, Machine
Learning, Data mining, Genetic Algorithms, and so on, rely on assumptions that
hardly address all the relevant challenges of fraud-detection system (FDS).
This paper aims to understand & implement Deep Learning algorithms in order to
obtain a high fraud coverage with very low false positive rate. Also, it aims
to implement an auto-encoder as an unsupervised (semi-supervised) method of
learning common patterns. Keywords: Credit card fraud, Fraud-detection system
(FDS), Electronic transactions, Concept drift, Class imbalance, Verification
latency, Machine Learning, Deep Learning",2024-09-20,"Sourav Verma, Joydip Dhar",http://arxiv.org/pdf/2409.13406v1,cs.LG
Hydrogen under Pressure as a Benchmark for Machine-Learning Interatomic Potentials,"Machine-learning interatomic potentials (MLPs) are fast, data-driven
surrogate models of atomistic systems' potential energy surfaces that can
accelerate ab-initio molecular dynamics (MD) simulations by several orders of
magnitude. The performance of MLPs is commonly measured as the prediction error
in energies and forces on data not used in their training. While low prediction
errors on a test set are necessary, they do not guarantee good performance in
MD simulations. The latter requires physically motivated performance measures
obtained from running accelerated simulations. However, the adoption of such
measures has been limited by the effort and domain knowledge required to
calculate and interpret them.
  To overcome this limitation, we present a benchmark that automatically
quantifies the performance of MLPs in MD simulations of a liquid-liquid phase
transition in hydrogen under pressure, a challenging benchmark system. The
benchmark's h-llpt-24 dataset provides reference geometries, energies, forces,
and stresses from density functional theory MD simulations at different
temperatures and mass densities. The benchmark's Python code automatically runs
MLP-accelerated MD simulations and calculates, quantitatively compares and
visualizes pressures, stable molecular fractions, diffusion coefficients, and
radial distribution functions. Employing this benchmark, we show that several
state-of-the-art MLPs fail to reproduce the liquid-liquid phase transition.",2024-09-20,"Thomas Bischoff, Bastian Jäckl, Matthias Rupp",http://arxiv.org/pdf/2409.13390v1,cs.LG
ALPEC: A Comprehensive Evaluation Framework and Dataset for Machine Learning-Based Arousal Detection in Clinical Practice,"Detecting arousals in sleep is essential for diagnosing sleep disorders.
However, using Machine Learning (ML) in clinical practice is impeded by
fundamental issues, primarily due to mismatches between clinical protocols and
ML methods. Clinicians typically annotate only the onset of arousals, while ML
methods rely on annotations for both the beginning and end. Additionally, there
is no standardized evaluation methodology tailored to clinical needs for
arousal detection models. This work addresses these issues by introducing a
novel post-processing and evaluation framework emphasizing approximate
localization and precise event count (ALPEC) of arousals. We recommend that ML
practitioners focus on detecting arousal onsets, aligning with clinical
practice. We examine the impact of this shift on current training and
evaluation schemes, addressing simplifications and challenges. We utilize a
novel comprehensive polysomnographic dataset (CPS) that reflects the
aforementioned clinical annotation constraints and includes modalities not
present in existing polysomnographic datasets. We release the dataset alongside
this paper, demonstrating the benefits of leveraging multimodal data for
arousal onset detection. Our findings significantly contribute to integrating
ML-based arousal detection in clinical settings, reducing the gap between
technological advancements and clinical needs.",2024-09-20,"Stefan Kraft, Andreas Theissler, Vera Wienhausen-Wilke, Philipp Walter, Gjergji Kasneci",http://arxiv.org/pdf/2409.13367v1,cs.LG
FPBoost: Fully Parametric Gradient Boosting for Survival Analysis,"Survival analysis is a statistical framework for modeling time-to-event data.
It plays a pivotal role in medicine, reliability engineering, and social
science research, where understanding event dynamics even with few data samples
is critical. Recent advancements in machine learning, particularly those
employing neural networks and decision trees, have introduced sophisticated
algorithms for survival modeling. However, many of these methods rely on
restrictive assumptions about the underlying event-time distribution, such as
proportional hazard, time discretization, or accelerated failure time. In this
study, we propose FPBoost, a survival model that combines a weighted sum of
fully parametric hazard functions with gradient boosting. Distribution
parameters are estimated with decision trees trained by maximizing the full
survival likelihood. We show how FPBoost is a universal approximator of hazard
functions, offering full event-time modeling flexibility while maintaining
interpretability through the use of well-established parametric distributions.
We evaluate concordance and calibration of FPBoost across multiple benchmark
datasets, showcasing its robustness and versatility as a new tool for survival
estimation.",2024-09-20,"Alberto Archetti, Eugenio Lomurno, Diego Piccinotti, Matteo Matteucci",http://arxiv.org/pdf/2409.13363v2,cs.LG
Continual Learning for Multimodal Data Fusion of a Soft Gripper,"Continual learning (CL) refers to the ability of an algorithm to continuously
and incrementally acquire new knowledge from its environment while retaining
previously learned information. A model trained on one data modality often
fails when tested with a different modality. A straightforward approach might
be to fuse the two modalities by concatenating their features and training the
model on the fused data. However, this requires retraining the model from
scratch each time it encounters a new domain. In this paper, we introduce a
continual learning algorithm capable of incrementally learning different data
modalities by leveraging both class-incremental and domain-incremental learning
scenarios in an artificial environment where labeled data is scarce, yet
non-iid (independent and identical distribution) unlabeled data from the
environment is plentiful. The proposed algorithm is efficient and only requires
storing prototypes for each class. We evaluate the algorithm's effectiveness on
a challenging custom multimodal dataset comprising of tactile data from a soft
pneumatic gripper, and visual data from non-stationary images of objects
extracted from video sequences. Additionally, we conduct an ablation study on
the custom dataset and the Core50 dataset to highlight the contributions of
different components of the algorithm. To further demonstrate the robustness of
the algorithm, we perform a real-time experiment for object classification
using the soft gripper and an external independent camera setup, all
synchronized with the Robot Operating System (ROS) framework.",2024-09-20,"Nilay Kushawaha, Egidio Falotico",http://arxiv.org/pdf/2409.13792v1,cs.LG
Multi-omics data integration for early diagnosis of hepatocellular carcinoma (HCC) using machine learning,"The complementary information found in different modalities of patient data
can aid in more accurate modelling of a patient's disease state and a better
understanding of the underlying biological processes of a disease. However, the
analysis of multi-modal, multi-omics data presents many challenges, including
high dimensionality and varying size, statistical distribution, scale and
signal strength between modalities. In this work we compare the performance of
a variety of ensemble machine learning algorithms that are capable of late
integration of multi-class data from different modalities. The ensemble methods
and their variations tested were i) a voting ensemble, with hard and soft vote,
ii) a meta learner, iii) a multi-modal Adaboost model using a hard vote, a soft
vote and a meta learner to integrate the modalities on each boosting round, the
PB-MVBoost model and a novel application of a mixture of experts model. These
were compared to simple concatenation as a baseline. We examine these methods
using data from an in-house study on hepatocellular carcinoma (HCC), along with
four validation datasets on studies from breast cancer and irritable bowel
disease (IBD). Using the area under the receiver operating curve as a measure
of performance we develop models that achieve a performance value of up to 0.85
and find that two boosted methods, PB-MVBoost and Adaboost with a soft vote
were the overall best performing models. We also examine the stability of
features selected, and the size of the clinical signature determined. Finally,
we provide recommendations for the integration of multi-modal multi-class data.",2024-09-20,"Annette Spooner, Mohammad Karimi Moridani, Azadeh Safarchi, Salim Maher, Fatemeh Vafaee, Amany Zekry, Arcot Sowmya",http://arxiv.org/pdf/2409.13791v1,cs.LG
Validity of Feature Importance in Low-Performing Machine Learning for Tabular Biomedical Data,"In tabular biomedical data analysis, tuning models to high accuracy is
considered a prerequisite for discussing feature importance, as medical
practitioners expect the validity of feature importance to correlate with
performance. In this work, we challenge the prevailing belief, showing that
low-performing models may also be used for feature importance. We propose
experiments to observe changes in feature rank as performance degrades
sequentially. Using three synthetic datasets and six real biomedical datasets,
we compare the rank of features from full datasets to those with reduced sample
sizes (data cutting) or fewer features (feature cutting). In synthetic
datasets, feature cutting does not change feature rank, while data cutting
shows higher discrepancies with lower performance. In real datasets, feature
cutting shows similar or smaller changes than data cutting, though some
datasets exhibit the opposite. When feature interactions are controlled by
removing correlations, feature cutting consistently shows better stability. By
analyzing the distribution of feature importance values and theoretically
examining the probability that the model cannot distinguish feature importance
between features, we reveal that models can still distinguish feature
importance despite performance degradation through feature cutting, but not
through data cutting. We conclude that the validity of feature importance can
be maintained even at low performance levels if the data size is adequate,
which is a significant factor contributing to suboptimal performance in tabular
medical data analysis. This paper demonstrates the potential for utilizing
feature importance analysis alongside statistical analysis to compare features
relatively, even when classifier performance is not satisfactory.",2024-09-20,"Youngro Lee, Giacomo Baruzzo, Jeonghwan Kim, Jongmo Seo, Barbara Di Camillo",http://arxiv.org/pdf/2409.13342v1,cs.LG
Revisiting Synthetic Human Trajectories: Imitative Generation and Benchmarks Beyond Datasaurus,"Human trajectory data, which plays a crucial role in various applications
such as crowd management and epidemic prevention, is challenging to obtain due
to practical constraints and privacy concerns. In this context, synthetic human
trajectory data is generated to simulate as close as possible to real-world
human trajectories, often under summary statistics and distributional
similarities. However, these similarities oversimplify complex human mobility
patterns (a.k.a. ``Datasaurus''), resulting in intrinsic biases in both
generative model design and benchmarks of the generated trajectories. Against
this background, we propose MIRAGE, a huMan-Imitative tRAjectory GenErative
model designed as a neural Temporal Point Process integrating an Exploration
and Preferential Return model. It imitates the human decision-making process in
trajectory generation, rather than fitting any specific statistical
distributions as traditional methods do, thus avoiding the Datasaurus issue. We
also propose a comprehensive task-based evaluation protocol beyond Datasaurus
to systematically benchmark trajectory generative models on four typical
downstream tasks, integrating multiple techniques and evaluation metrics for
each task, to assess the ultimate utility of the generated trajectories. We
conduct a thorough evaluation of MIRAGE on three real-world user trajectory
datasets against a sizeable collection of baselines. Results show that compared
to the best baselines, MIRAGE-generated trajectory data not only achieves the
best statistical and distributional similarities with 59.0-67.7% improvement,
but also yields the best performance in the task-based evaluation with
10.9-33.4% improvement. A series of ablation studies also validate the key
design choices of MIRAGE.",2024-09-20,"Bangchao Deng, Xin Jing, Tianyue Yang, Bingqing Qu, Dingqi Yang, Philippe Cudre-Mauroux",http://arxiv.org/pdf/2409.13790v2,cs.LG
Classification of Buried Objects from Ground Penetrating Radar Images by using Second Order Deep Learning Models,"In this paper, a new classification model based on covariance matrices is
built in order to classify buried objects. The inputs of the proposed models
are the hyperbola thumbnails obtained with a classical Ground Penetrating Radar
(GPR) system. These thumbnails are then inputs to the first layers of a
classical CNN, which then produces a covariance matrix using the outputs of the
convolutional filters. Next, the covariance matrix is given to a network
composed of specific layers to classify Symmetric Positive Definite (SPD)
matrices. We show in a large database that our approach outperform shallow
networks designed for GPR data and conventional CNNs typically used in computer
vision applications, particularly when the number of training data decreases
and in the presence of mislabeled data. We also illustrate the interest of our
models when training data and test sets are obtained from different weather
modes or considerations.",2024-09-20,"Douba Jafuno, Ammar Mian, Guillaume Ginolhac, Nickolas Stelzenmuller",http://arxiv.org/pdf/2410.07117v2,cs.LG
Generative Aerodynamic Design with Diffusion Probabilistic Models,"The optimization of geometries for aerodynamic design often relies on a large
number of expensive simulations to evaluate and iteratively improve the
geometries. It is possible to reduce the number of simulations by providing a
starting geometry that has properties close to the desired requirements, often
in terms of lift and drag, aerodynamic moments and surface areas. We show that
generative models have the potential to provide such starting geometries by
generalizing geometries over a large dataset of simulations. In particular, we
leverage diffusion probabilistic models trained on XFOIL simulations to
synthesize two-dimensional airfoil geometries conditioned on given aerodynamic
features and constraints. The airfoils are parameterized with Bernstein
polynomials, ensuring smoothness of the generated designs. We show that the
models are able to generate diverse candidate designs for identical
requirements and constraints, effectively exploring the design space to provide
multiple starting points to optimization procedures. However, the quality of
the candidate designs depends on the distribution of the simulated designs in
the dataset. Importantly, the geometries in this dataset must satisfy other
requirements and constraints that are not used in conditioning of the diffusion
model, to ensure that the generated geometries are physical.",2024-09-20,"Thomas Wagenaar, Simone Mancini, Andrés Mateo-Gabín",http://arxiv.org/pdf/2409.13328v1,cs.LG
SLaVA-CXR: Small Language and Vision Assistant for Chest X-ray Report Automation,"Inspired by the success of large language models (LLMs), there is growing
research interest in developing LLMs in the medical domain to assist
clinicians. However, for hospitals, using closed-source commercial LLMs
involves privacy issues, and developing open-source public LLMs requires
large-scale computational resources, which are usually limited, especially in
resource-efficient regions and low-income countries. We propose an open-source
Small Language and Vision Assistant (SLaVA-CXR) that can be used for Chest
X-Ray report automation. To efficiently train a small assistant, we first
propose the Re$^3$Training method, which simulates the cognitive development of
radiologists and optimizes the model in the Recognition, Reasoning, and
Reporting training manner. Then, we introduce a data synthesis method, RADEX,
which can generate a high-quality and diverse training corpus with privacy
regulation compliance. The extensive experiments show that our SLaVA-CXR built
on a 2.7B backbone not only outperforms but also achieves 6 times faster
inference efficiency than previous state-of-the-art larger models.",2024-09-20,"Jinge Wu, Yunsoo Kim, Daqian Shi, David Cliffton, Fenglin Liu, Honghan Wu",http://arxiv.org/pdf/2409.13321v1,cs.LG
A Ring-Based Distributed Algorithm for Learning High-Dimensional Bayesian Networks,"Learning Bayesian Networks (BNs) from high-dimensional data is a complex and
time-consuming task. Although there are approaches based on horizontal
(instances) or vertical (variables) partitioning in the literature, none can
guarantee the same theoretical properties as the Greedy Equivalence Search
(GES) algorithm, except those based on the GES algorithm itself. In this paper,
we propose a directed ring-based distributed method that uses GES as the local
learning algorithm, ensuring the same theoretical properties as GES but
requiring less CPU time. The method involves partitioning the set of possible
edges and constraining each processor in the ring to work only with its
received subset. The global learning process is an iterative algorithm that
carries out several rounds until a convergence criterion is met. In each round,
each processor receives a BN from its predecessor in the ring, fuses it with
its own BN model, and uses the result as the starting solution for a local
learning process constrained to its set of edges. Subsequently, it sends the
model obtained to its successor in the ring. Experiments were carried out on
three large domains (400-1000 variables), demonstrating our proposal's
effectiveness compared to GES and its fast version (fGES).",2024-09-20,"Jorge D. Laborda, Pablo Torrijos, José M. Puerta, José A. Gámez",http://arxiv.org/pdf/2409.13314v1,cs.LG
MeMoir: A Software-Driven Covert Channel based on Memory Usage,"Covert channel attacks have been continuously studied as severe threats to
modern computing systems. Software-based covert channels are a typically
hard-to-detect branch of these attacks, since they leverage virtual resources
to establish illegitimate communication between malicious actors. In this work,
we present MeMoir: a novel software-driven covert channel that, for the first
time, utilizes memory usage as the medium for the channel. We implemented the
new covert channel on two real-world platforms with different architectures: a
general-purpose Intel x86-64-based desktop computer and an ARM64-based embedded
system. Our results show that our new architecture- and hardware-agnostic
covert channel is effective and achieves moderate transmission rates with very
low error. Moreover, we present a real use-case for our attack where we were
able to communicate information from a Hyper-V virtualized enviroment to a
Windows 11 host system. In addition, we implement a machine learning-based
detector that can predict whether an attack is present in the system with an
accuracy of more than 95% with low false positive and false negative rates by
monitoring the use of system memory. Finally, we introduce a noise-based
countermeasure that effectively mitigates the attack while inducing a low power
overhead in the system compared to other normal applications.",2024-09-20,"Jeferson Gonzalez-Gomez, Jose Alejandro Ibarra-Campos, Jesus Yamir Sandoval-Morales, Lars Bauer, Jörg Henkel",http://arxiv.org/pdf/2409.13310v1,cs.LG
Predicting DNA fragmentation: A non-destructive analogue to chemical assays using machine learning,"Globally, infertility rates are increasing, with 2.5\% of all births being
assisted by in vitro fertilisation (IVF) in 2022. Male infertility is the cause
for approximately half of these cases. The quality of sperm DNA has substantial
impact on the success of IVF. The assessment of sperm DNA is traditionally done
through chemical assays which render sperm cells ineligible for IVF. Many
compounding factors lead to the population crisis, with fertility rates
dropping globally in recent history. As such assisted reproductive technologies
(ART) have been the focus of recent research efforts. Simultaneously,
artificial intelligence has grown ubiquitous and is permeating more aspects of
modern life. With the advent of state-of-the-art machine learning and its
exceptional performance in many sectors, this work builds on these successes
and proposes a novel framework for the prediction of sperm cell DNA
fragmentation from images of unstained sperm. Rendering a predictive model
which preserves sperm integrity and allows for optimal selection of sperm for
IVF.",2024-09-20,"Byron A Jacobs, Ifthakaar Shaik, Frando Lin",http://arxiv.org/pdf/2409.13306v2,cs.LG
OMG-RL:Offline Model-based Guided Reward Learning for Heparin Treatment,"Accurate medication dosing holds an important position in the overall patient
therapeutic process. Therefore, much research has been conducted to develop
optimal administration strategy based on Reinforcement learning (RL). However,
Relying solely on a few explicitly defined reward functions makes it difficult
to learn a treatment strategy that encompasses the diverse characteristics of
various patients. Moreover, the multitude of drugs utilized in clinical
practice makes it infeasible to construct a dedicated reward function for each
medication. Here, we tried to develop a reward network that captures
clinicians' therapeutic intentions, departing from explicit rewards, and to
derive an optimal heparin dosing policy. In this study, we introduce Offline
Model-based Guided Reward Learning (OMG-RL), which performs offline inverse RL
(IRL). Through OMG-RL, we learn a parameterized reward function that captures
the expert's intentions from limited data, thereby enhancing the agent's
policy. We validate the proposed approach on the heparin dosing task. We show
that OMG-RL policy is positively reinforced not only in terms of the learned
reward network but also in activated partial thromboplastin time (aPTT), a key
indicator for monitoring the effects of heparin. This means that the OMG-RL
policy adequately reflects clinician's intentions. This approach can be widely
utilized not only for the heparin dosing problem but also for RL-based
medication dosing tasks in general.",2024-09-20,"Yooseok Lim, Sujee Lee",http://arxiv.org/pdf/2409.13299v2,cs.LG
Learning to Generalize Unseen Domains via Multi-Source Meta Learning for Text Classification,"With the rapid development of deep learning methods, there have been many
breakthroughs in the field of text classification. Models developed for this
task have been shown to achieve high accuracy. However, most of these models
are trained using labeled data from seen domains. It is difficult for these
models to maintain high accuracy in a new challenging unseen domain, which is
directly related to the generalization of the model. In this paper, we study
the multi-source Domain Generalization of text classification and propose a
framework to use multiple seen domains to train a model that can achieve high
accuracy in an unseen domain. Specifically, we propose a multi-source
meta-learning Domain Generalization framework to simulate the process of model
generalization to an unseen domain, so as to extract sufficient domain-related
features. We introduced a memory mechanism to store domain-specific features,
which coordinate with the meta-learning framework. Besides, we adopt the novel
""jury"" mechanism that enables the model to learn sufficient domain-invariant
features. Experiments demonstrate that our meta-learning framework can
effectively enhance the ability of the model to generalize to an unseen domain
and can outperform the state-of-the-art methods on multi-source text
classification datasets.",2024-09-20,"Yuxuan Hu, Chenwei Zhang, Min Yang, Xiaodan Liang, Chengming Li, Xiping Hu",http://arxiv.org/pdf/2409.13787v1,cs.LG
Time Distributed Deep Learning models for Purely Exogenous Forecasting. Application to Water Table Depth Prediction using Weather Image Time Series,"Groundwater resources are one of the most relevant elements in the water
cycle, therefore developing models to accurately predict them is a pivotal task
in the sustainable resources management framework. Deep Learning (DL) models
have been revealed very effective in hydrology, especially by feeding spatially
distributed data (e.g. raster data). In many regions, hydrological measurements
are difficult to obtain regularly or periodically in time, and in some cases,
last available data are not up to date. Reversely, weather data, which
significantly impacts water resources, are usually more available and with
higher quality. More specifically, we have proposed two different DL models to
predict the water table depth in the Grana-Maira catchment (Piemonte, IT) using
only exogenous weather image time series. To deal with the image time series,
both models are made of a first Time Distributed Convolutional Neural Network
(TDC) which encodes the image available at each time step into a vectorial
representation. The first model, TDC-LSTM uses then a Sequential Module based
on an LSTM layer to learn temporal relations and output the predictions. The
second model, TDC-UnPWaveNet uses instead a new version of the WaveNet
architecture, adapted here to output a sequence shorter and completely shifted
in the future with respect to the input one. To this aim, and to deal with the
different sequence lengths in the UnPWaveNet, we have designed a new Channel
Distributed layer, that acts like a Time Distributed one but on the channel
dimension, i.e. applying the same set of operations to each channel of the
input. TDC-LSTM and TDC-UnPWaveNet have shown both remarkable results. However,
the two models have focused on different learnable information: TDC-LSTM has
focused more on lowering the bias, while the TDC-UnPWaveNet has focused more on
the temporal dynamics maximising correlation and KGE.",2024-09-20,"Matteo Salis, Abdourrahmane M. Atto, Stefano Ferraris, Rosa Meo",http://arxiv.org/pdf/2409.13284v1,cs.LG
Efficient Training of Deep Neural Operator Networks via Randomized Sampling,"Neural operators (NOs) employ deep neural networks to learn mappings between
infinite-dimensional function spaces. Deep operator network (DeepONet), a
popular NO architecture, has demonstrated success in the real-time prediction
of complex dynamics across various scientific and engineering applications. In
this work, we introduce a random sampling technique to be adopted during the
training of DeepONet, aimed at improving the generalization ability of the
model, while significantly reducing the computational time. The proposed
approach targets the trunk network of the DeepONet model that outputs the basis
functions corresponding to the spatiotemporal locations of the bounded domain
on which the physical system is defined. Traditionally, while constructing the
loss function, DeepONet training considers a uniform grid of spatiotemporal
points at which all the output functions are evaluated for each iteration. This
approach leads to a larger batch size, resulting in poor generalization and
increased memory demands, due to the limitations of the stochastic gradient
descent (SGD) optimizer. The proposed random sampling over the inputs of the
trunk net mitigates these challenges, improving generalization and reducing
memory requirements during training, resulting in significant computational
gains. We validate our hypothesis through three benchmark examples,
demonstrating substantial reductions in training time while achieving
comparable or lower overall test errors relative to the traditional training
approach. Our results indicate that incorporating randomization in the trunk
network inputs during training enhances the efficiency and robustness of
DeepONet, offering a promising avenue for improving the framework's performance
in modeling complex physical systems.",2024-09-20,"Sharmila Karumuri, Lori Graham-Brady, Somdatta Goswami",http://arxiv.org/pdf/2409.13280v1,cs.LG
Physics-informed kernel learning,"Physics-informed machine learning typically integrates physical priors into
the learning process by minimizing a loss function that includes both a
data-driven term and a partial differential equation (PDE) regularization.
Building on the formulation of the problem as a kernel regression task, we use
Fourier methods to approximate the associated kernel, and propose a tractable
estimator that minimizes the physics-informed risk function. We refer to this
approach as physics-informed kernel learning (PIKL). This framework provides
theoretical guarantees, enabling the quantification of the physical prior's
impact on convergence speed. We demonstrate the numerical performance of the
PIKL estimator through simulations, both in the context of hybrid modeling and
in solving PDEs. In particular, we show that PIKL can outperform
physics-informed neural networks in terms of both accuracy and computation
time. Additionally, we identify cases where PIKL surpasses traditional PDE
solvers, particularly in scenarios with noisy boundary conditions.",2024-09-20,"Nathan Doumèche, Francis Bach, Gérard Biau, Claire Boyer",http://arxiv.org/pdf/2409.13786v1,cs.LG
Inductive Spatial Temporal Prediction Under Data Drift with Informative Graph Neural Network,"Inductive spatial temporal prediction can generalize historical data to
predict unseen data, crucial for highly dynamic scenarios (e.g., traffic
systems, stock markets). However, external events (e.g., urban structural
growth, market crash) and emerging new entities (e.g., locations, stocks) can
undermine prediction accuracy by inducing data drift over time. Most existing
studies extract invariant patterns to counter data drift but ignore pattern
diversity, exhibiting poor generalization to unseen entities. To address this
issue, we design an Informative Graph Neural Network (INF-GNN) to distill
diversified invariant patterns and improve prediction accuracy under data
drift. Firstly, we build an informative subgraph with a uniquely designed
metric, Relation Importance (RI), that can effectively select stable entities
and distinct spatial relationships. This subgraph further generalizes new
entities' data via neighbors merging. Secondly, we propose an informative
temporal memory buffer to help the model emphasize valuable timestamps
extracted using influence functions within time intervals. This memory buffer
allows INF-GNN to discern influential temporal patterns. Finally, RI loss
optimization is designed for pattern consolidation. Extensive experiments on
real-world dataset under substantial data drift demonstrate that INF-GNN
significantly outperforms existing alternatives.",2024-09-20,"Jialun Zheng, Divya Saxena, Jiannong Cao, Hanchen Yang, Penghui Ruan",http://arxiv.org/pdf/2409.13253v1,cs.LG
Exploring energy minimization to model strain localization as a strong discontinuity using Physics Informed Neural Networks,"We explore the possibilities of using energy minimization for the numerical
modeling of strain localization in solids as a sharp discontinuity in the
displacement field. For this purpose, we consider (regularized) strong
discontinuity kinematics in elastoplastic solids. The corresponding
mathematical model is discretized using Artificial Neural Networks (ANNs),
aiming to predict both the magnitude and location of the displacement jump from
energy minimization, $\textit{i.e.}$, within a variational setting. The
architecture takes care of the kinematics, while the loss function takes care
of the variational statement of the boundary value problem. The main idea
behind this approach is to solve both the equilibrium problem and the location
of the localization band by means of trainable parameters in the ANN. As a
proof of concept, we show through both 1D and 2D numerical examples that the
computational modeling of strain localization for elastoplastic solids using
energy minimization is feasible.",2024-09-20,"Omar León, Víctor Rivera, Angel Vázquez-Patiño, Jacinto Ulloa, Esteban Samaniego",http://arxiv.org/pdf/2409.13241v2,cs.LG
Balancing Label Imbalance in Federated Environments Using Only Mixup and Artificially-Labeled Noise,"Clients in a distributed or federated environment will often hold data skewed
towards differing subsets of labels. This scenario, referred to as
heterogeneous or non-iid federated learning, has been shown to significantly
hinder model training and performance. In this work, we explore the limits of a
simple yet effective augmentation strategy for balancing skewed label
distributions: filling in underrepresented samples of a particular label class
using pseudo-images. While existing algorithms exclusively train on
pseudo-images such as mixups of local training data, our augmented client
datasets consist of both real and pseudo-images. In further contrast to other
literature, we (1) use a DP-Instahide variant to reduce the decodability of our
image encodings and (2) as a twist, supplement local data using artificially
labeled, training-free 'natural noise' generated by an untrained StyleGAN.
These noisy images mimic the power spectra patterns present in natural scenes
which, together with mixup images, help homogenize label distribution among
clients. We demonstrate that small amounts of augmentation via mixups and
natural noise markedly improve label-skewed CIFAR-10 and MNIST training.",2024-09-20,"Kyle Sang, Tahseen Rabbani, Furong Huang",http://arxiv.org/pdf/2409.13235v1,cs.LG
Relationship between Uncertainty in DNNs and Adversarial Attacks,"Deep Neural Networks (DNNs) have achieved state of the art results and even
outperformed human accuracy in many challenging tasks, leading to DNNs adoption
in a variety of fields including natural language processing, pattern
recognition, prediction, and control optimization. However, DNNs are
accompanied by uncertainty about their results, causing them to predict an
outcome that is either incorrect or outside of a certain level of confidence.
These uncertainties stem from model or data constraints, which could be
exacerbated by adversarial attacks. Adversarial attacks aim to provide
perturbed input to DNNs, causing the DNN to make incorrect predictions or
increase model uncertainty. In this review, we explore the relationship between
DNN uncertainty and adversarial attacks, emphasizing how adversarial attacks
might raise DNN uncertainty.",2024-09-20,"Mabel Ogonna, Abigail Adeniran, Adewale Adeyemo",http://arxiv.org/pdf/2409.13232v2,cs.LG
Incremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,"Few-shot adaptation is an important capability for intelligent robots that
perform tasks in open-world settings such as everyday environments or flexible
production. In this paper, we propose a novel approach for non-prehensile
manipulation which incrementally adapts a physics-based dynamics model for
model-predictive control (MPC). The model prediction is aligned with a few
examples of robot-object interactions collected with the MPC. This is achieved
by using a parallelizable rigid-body physics simulation as dynamic world model
and sampling-based optimization of the model parameters. In turn, the optimized
dynamics model can be used for MPC using efficient sampling-based optimization.
We evaluate our few-shot adaptation approach in object pushing experiments in
simulation and with a real robot.",2024-09-20,"Fabian Baumeister, Lukas Mack, Joerg Stueckler",http://arxiv.org/pdf/2409.13228v2,cs.LG
Optimizing RLHF Training for Large Language Models with Stage Fusion,"We present RLHFuse, an efficient training system with stage fusion for
Reinforcement Learning from Human Feedback (RLHF). Due to the intrinsic nature
of RLHF training, i.e., the data skewness in the generation stage and the
pipeline bubbles in the training stage, existing RLHF systems suffer from low
GPU utilization. RLHFuse breaks the traditional view of RLHF workflow as a
composition of individual tasks, splitting each task into finer-grained
subtasks, and performing stage fusion to improve GPU utilization. RLHFuse
contains two key ideas. First, for generation and inference tasks, RLHFuse
splits them into sample-level subtasks, enabling efficient inter-stage fusion
to overlap the execution of generation and inference stages, thus mitigating
the original generation bottleneck dominated by long-tailed samples. Second,
for training tasks, RLHFuse breaks them into subtasks of micro-batches and
performs intra-stage fusion to concurrently execute these subtasks in the
training stage with a fused pipeline schedule, effectively mitigating the
pipeline bubbles. The experiments show that RLHFuse increases the training
throughput by up to $3.7\times$, compared to existing systems.",2024-09-20,"Yinmin Zhong, Zili Zhang, Bingyang Wu, Shengyu Liu, Yukun Chen, Changyi Wan, Hanpeng Hu, Lei Xia, Ranchen Ming, Yibo Zhu, Xin Jin",http://arxiv.org/pdf/2409.13221v3,cs.LG
MalMixer: Few-Shot Malware Classification with Retrieval-Augmented Semi-Supervised Learning,"Recent growth and proliferation of malware have tested practitioners ability
to promptly classify new samples according to malware families. In contrast to
labor-intensive reverse engineering efforts, machine learning approaches have
demonstrated increased speed and accuracy. However, most existing deep-learning
malware family classifiers must be calibrated using a large number of samples
that are painstakingly manually analyzed before training. Furthermore, as novel
malware samples arise that are beyond the scope of the training set, additional
reverse engineering effort must be employed to update the training set. The
sheer volume of new samples found in the wild creates substantial pressure on
practitioners ability to reverse engineer enough malware to adequately train
modern classifiers. In this paper, we present MalMixer, a malware family
classifier using semi-supervised learning that achieves high accuracy with
sparse training data. We present a domain-knowledge-aware data augmentation
technique for malware feature representations, enhancing few-shot performance
of semi-supervised malware family classification. We show that MalMixer
achieves state-of-the-art performance in few-shot malware family classification
settings. Our research confirms the feasibility and effectiveness of
lightweight, domain-knowledge-aware data augmentation methods for malware
features and shows the capabilities of similar semi-supervised classifiers in
addressing malware classification issues.",2024-09-20,"Jiliang Li, Yifan Zhang, Yu Huang, Kevin Leach",http://arxiv.org/pdf/2409.13213v4,cs.LG
A Unified Causal Framework for Auditing Recommender Systems for Ethical Concerns,"As recommender systems become widely deployed in different domains, they
increasingly influence their users' beliefs and preferences. Auditing
recommender systems is crucial as it not only ensures the continuous
improvement of recommendation algorithms but also safeguards against potential
issues like biases and ethical concerns. In this paper, we view recommender
system auditing from a causal lens and provide a general recipe for defining
auditing metrics. Under this general causal auditing framework, we categorize
existing auditing metrics and identify gaps in them -- notably, the lack of
metrics for auditing user agency while accounting for the multi-step dynamics
of the recommendation process. We leverage our framework and propose two
classes of such metrics:future- and past-reacheability and stability, that
measure the ability of a user to influence their own and other users'
recommendations, respectively. We provide both a gradient-based and a black-box
approach for computing these metrics, allowing the auditor to compute them
under different levels of access to the recommender system. In our experiments,
we demonstrate the efficacy of methods for computing the proposed metrics and
inspect the design of recommender systems through these proposed metrics.",2024-09-20,"Vibhhu Sharma, Shantanu Gupta, Nil-Jana Akpinar, Zachary C. Lipton, Liu Leqi",http://arxiv.org/pdf/2409.13210v1,cs.LG
Redefining Data Pairing for Motion Retargeting Leveraging a Human Body Prior,"We propose MR HuBo(Motion Retargeting leveraging a HUman BOdy prior), a
cost-effective and convenient method to collect high-quality upper body paired
<robot, human> pose data, which is essential for data-driven motion retargeting
methods. Unlike existing approaches which collect <robot, human> pose data by
converting human MoCap poses into robot poses, our method goes in reverse. We
first sample diverse random robot poses, and then convert them into human
poses. However, since random robot poses can result in extreme and infeasible
human poses, we propose an additional technique to sort out extreme poses by
exploiting a human body prior trained from a large amount of human pose data.
Our data collection method can be used for any humanoid robots, if one designs
or optimizes the system's hyperparameters which include a size scale factor and
the joint angle ranges for sampling. In addition to this data collection
method, we also present a two-stage motion retargeting neural network that can
be trained via supervised learning on a large amount of paired data. Compared
to other learning-based methods trained via unsupervised learning, we found
that our deep neural network trained with ample high-quality paired data
achieved notable performance. Our experiments also show that our data filtering
method yields better retargeting results than training the model with raw and
noisy data. Our code and video results are available on
https://sites.google.com/view/mr-hubo/",2024-09-20,"Xiyana Figuera, Soogeun Park, Hyemin Ahn",http://arxiv.org/pdf/2409.13208v3,cs.LG
Unveiling Population Heterogeneity in Health Risks Posed by Environmental Hazards Using Regression-Guided Neural Network,"Environmental hazards place certain individuals at disproportionately higher
risks. As these hazards increasingly endanger human health, precise
identification of the most vulnerable population subgroups is critical for
public health. Moderated multiple regression (MMR) offers a straightforward
method for investigating this by adding interaction terms between the exposure
to a hazard and other population characteristics to a linear regression model.
However, when the vulnerabilities are hidden within a cross-section of many
characteristics, MMR is often limited in its capabilities to find any
meaningful discoveries. Here, we introduce a hybrid method, named
regression-guided neural networks (ReGNN), which utilizes artificial neural
networks (ANNs) to non-linearly combine predictors, generating a latent
representation that interacts with a focal predictor (i.e. variable measuring
exposure to an environmental hazard). We showcase the use of ReGNN for
investigating the population heterogeneity in the health effects of exposure to
air pollution (PM2.5) on cognitive functioning scores. We demonstrate that
population heterogeneity that would otherwise be hidden using traditional MMR
can be found using ReGNN by comparing its results to the fit results of the
traditional MMR models. In essence, ReGNN is a novel tool that enhances
traditional regression models by effectively summarizing and quantifying an
individual's susceptibility to health risks.",2024-09-20,"Jong Woo Nam, Eun Young Choi, Jennifer A. Ailshire, Yao-Yi Chiang",http://arxiv.org/pdf/2409.13205v1,cs.LG
Exploring Scaling Laws for Local SGD in Large Language Model Training,"This paper investigates scaling laws for local SGD in LLM training, a
distributed optimization algorithm that facilitates training on loosely
connected devices. Through extensive experiments, we show that local SGD
achieves competitive results compared to conventional methods, given equivalent
model parameters, datasets, and computational resources. Furthermore, we
explore the application of local SGD in various practical scenarios, including
multi-cluster setups and edge computing environments. Our findings elucidate
the necessary conditions for effective multi-cluster LLM training and examine
the potential and limitations of leveraging edge computing resources in the LLM
training process. This demonstrates its viability as an alternative to single
large-cluster training.",2024-09-20,"Qiaozhi He, Xiaomin Zhuang, Zhihua Wu",http://arxiv.org/pdf/2409.13198v1,cs.LG
BoilerTAI: A Platform for Enhancing Instruction Using Generative AI in Educational Forums,"Contribution: This Full paper in the Research Category track describes a
practical, scalable platform that seamlessly integrates Generative AI (GenAI)
with online educational forums, offering a novel approach to augment the
instructional capabilities of staff. The platform empowers instructional staff
to efficiently manage, refine, and approve responses by facilitating
interaction between student posts and a Large Language Model (LLM). This
contribution enhances the efficiency and effectiveness of instructional support
and significantly improves the quality and speed of responses provided to
students, thereby enriching the overall learning experience.
  Background: Grounded in Vygotsky's socio-cultural theory and the concept of
the More Knowledgeable Other (MKO), the study examines how GenAI can act as an
auxiliary MKO to enrich educational dialogue between students and instructors.
  Research Question: How effective is GenAI in reducing the workload of
instructional staff when used to pre-answer student questions posted on
educational discussion forums?
  Methodology: Using a mixed-methods approach in large introductory programming
courses, human Teaching Assistants (AI-TAs) employed an AI-assisted platform to
pre-answer student queries. We analyzed efficiency indicators like the
frequency of modifications to AI-generated responses and gathered qualitative
feedback from AI-TAs.
  Findings: The findings indicate no significant difference in student
reception to responses generated by AI-TAs compared to those provided by human
instructors. This suggests that GenAI can effectively meet educational needs
when adequately managed. Moreover, AI-TAs experienced a reduction in the
cognitive load required for responding to queries, pointing to GenAI's
potential to enhance instructional efficiency without compromising the quality
of education.",2024-09-20,"Anvit Sinha, Shruti Goyal, Zachary Sy, Rhianna Kuperus, Ethan Dickey, Andres Bejarano",http://arxiv.org/pdf/2409.13196v1,cs.LG
ControlMath: Controllable Data Generation Promotes Math Generalist Models,"Utilizing large language models (LLMs) for data augmentation has yielded
encouraging results in mathematical reasoning. However, these approaches face
constraints in problem diversity, potentially restricting them to
in-domain/distribution data generation. To this end, we propose ControlMath, an
iterative method involving an equation-generator module and two LLM-based
agents. The module creates diverse equations, which the Problem-Crafter agent
then transforms into math word problems. The Reverse-Agent filters and selects
high-quality data, adhering to the ""less is more"" principle, achieving better
results with fewer data points. This approach enables the generation of diverse
math problems, not limited to specific domains or distributions. As a result,
we collect ControlMathQA, which involves 190k math word problems. Extensive
results prove that combining our dataset with in-domain datasets like GSM8K can
help improve the model's mathematical ability to generalize, leading to
improved performances both within and beyond specific domains.",2024-09-20,"Nuo Chen, Ning Wu, Jianhui Chang, Jia Li",http://arxiv.org/pdf/2409.15376v1,cs.LG
ChemDFM-X: Towards Large Multimodal Model for Chemistry,"Rapid developments of AI tools are expected to offer unprecedented assistance
to the research of natural science including chemistry. However, neither
existing unimodal task-specific specialist models nor emerging general large
multimodal models (LMM) can cover the wide range of chemical data modality and
task categories. To address the real demands of chemists, a cross-modal
Chemical General Intelligence (CGI) system, which serves as a truly practical
and useful research assistant utilizing the great potential of LMMs, is in
great need. In this work, we introduce the first Cross-modal Dialogue
Foundation Model for Chemistry (ChemDFM-X). Diverse multimodal data are
generated from an initial modality by approximate calculations and
task-specific model predictions. This strategy creates sufficient chemical
training corpora, while significantly reducing excessive expense, resulting in
an instruction-tuning dataset containing 7.6M data. After instruction
finetuning, ChemDFM-X is evaluated on extensive experiments of different
chemical tasks with various data modalities. The results demonstrate the
capacity of ChemDFM-X for multimodal and inter-modal knowledge comprehension.
ChemDFM-X marks a significant milestone toward aligning all modalities in
chemistry, a step closer to CGI.",2024-09-20,"Zihan Zhao, Bo Chen, Jingpiao Li, Lu Chen, Liyang Wen, Pengyu Wang, Zichen Zhu, Danyang Zhang, Ziping Wan, Yansi Li, Zhongyang Dai, Xin Chen, Kai Yu",http://arxiv.org/pdf/2409.13194v2,cs.LG
Diabetica: Adapting Large Language Model to Enhance Multiple Medical Tasks in Diabetes Care and Management,"Diabetes is a chronic disease with a significant global health burden,
requiring multi-stakeholder collaboration for optimal management. Large
language models (LLMs) have shown promise in various healthcare scenarios, but
their effectiveness across diverse diabetes tasks remains unproven. Our study
introduced a framework to train and validate diabetes-specific LLMs. We first
developed a comprehensive data processing pipeline that includes data
collection, filtering, augmentation and refinement. This created a
high-quality, diabetes-specific dataset and evaluation benchmarks from scratch.
Fine-tuned on the collected training dataset, our diabetes-specific LLM family
demonstrated state-of-the-art proficiency in processing various diabetes tasks
compared to other LLMs. Furthermore, clinical studies revealed the potential
applications of our models in diabetes care, including providing personalized
healthcare, assisting medical education, and streamlining clinical tasks.
Generally, our introduced framework helps develop diabetes-specific LLMs and
highlights their potential to enhance clinical practice and provide
personalized, data-driven support for diabetes management across different end
users. Our codes, benchmarks and models are available at
https://github.com/waltonfuture/Diabetica.",2024-09-20,"Lai Wei, Zhen Ying, Muyang He, Yutong Chen, Qian Yang, Yanzhe Hong, Jiaping Lu, Kaipeng Zheng, Shaoting Zhang, Xiaoying Li, Weiran Huang, Ying Chen",http://arxiv.org/pdf/2409.13191v2,cs.LG
ASPINN: An asymptotic strategy for solving singularly perturbed differential equations,"Solving Singularly Perturbed Differential Equations (SPDEs) presents
challenges due to the rapid change of their solutions at the boundary layer. In
this manuscript, We propose Asymptotic Physics-Informed Neural Networks
(ASPINN), a generalization of Physics-Informed Neural Networks (PINN) and
General-Kindred Physics-Informed Neural Networks (GKPINN) approaches. This is a
decomposition method based on the idea of asymptotic analysis. Compared to
PINN, the ASPINN method has a strong fitting ability for solving SPDEs due to
the placement of exponential layers at the boundary layer. Unlike GKPINN,
ASPINN lessens the number of fully connected layers, thereby reducing the
training cost more effectively. Moreover, ASPINN theoretically approximates the
solution at the boundary layer more accurately, which accuracy is also improved
compared to GKPINN. We demonstrate the effect of ASPINN by solving diverse
classes of SPDEs, which clearly shows that the ASPINN method is promising in
boundary layer problems. Furthermore, we introduce Chebyshev Kolmogorov-Arnold
Networks (Chebyshev-KAN) instead of MLP, achieving better performance in
various experiments.",2024-09-20,"Sen Wang, Peizhi Zhao, Tao Song",http://arxiv.org/pdf/2409.13185v1,cs.LG
Overcoming Data Limitations in Internet Traffic Forecasting: LSTM Models with Transfer Learning and Wavelet Augmentation,"Effective internet traffic prediction in smaller ISP networks is challenged
by limited data availability. This paper explores this issue using transfer
learning and data augmentation techniques with two LSTM-based models,
LSTMSeq2Seq and LSTMSeq2SeqAtn, initially trained on a comprehensive dataset
provided by Juniper Networks and subsequently applied to smaller datasets. The
datasets represent real internet traffic telemetry, offering insights into
diverse traffic patterns across different network domains. Our study revealed
that while both models performed well in single-step predictions, multi-step
forecasts were challenging, particularly in terms of long-term accuracy. In
smaller datasets, LSTMSeq2Seq generally outperformed LSTMSeq2SeqAtn, indicating
that higher model complexity does not necessarily translate to better
performance. The models' effectiveness varied across different network domains,
reflecting the influence of distinct traffic characteristics. To address data
scarcity, Discrete Wavelet Transform was used for data augmentation, leading to
significant improvements in model performance, especially in shorter-term
forecasts. Our analysis showed that data augmentation is crucial in scenarios
with limited data. Additionally, the study included an analysis of the models'
variability and consistency, with attention mechanisms in LSTMSeq2SeqAtn
providing better short-term forecasting consistency but greater variability in
longer forecasts. The results highlight the benefits and limitations of
different modeling approaches in traffic prediction. Overall, this research
underscores the importance of transfer learning and data augmentation in
enhancing the accuracy of traffic prediction models, particularly in smaller
ISP networks with limited data availability.",2024-09-20,"Sajal Saha, Anwar Haque, Greg Sidebottom",http://arxiv.org/pdf/2409.13181v1,cs.LG
ConvLSTMTransNet: A Hybrid Deep Learning Approach for Internet Traffic Telemetry,"In this paper, we present a novel hybrid deep learning model, named
ConvLSTMTransNet, designed for time series prediction, with a specific
application to internet traffic telemetry. This model integrates the strengths
of Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM)
networks, and Transformer encoders to capture complex spatial-temporal
relationships inherent in time series data. The ConvLSTMTransNet model was
evaluated against three baseline models: RNN, LSTM, and Gated Recurrent Unit
(GRU), using real internet traffic data sampled from high-speed ports on a
provider edge router. Performance metrics such as Mean Absolute Error (MAE),
Root Mean Squared Error (RMSE), and Weighted Absolute Percentage Error (WAPE)
were used to assess each model's accuracy. Our findings demonstrate that
ConvLSTMTransNet significantly outperforms the baseline models by approximately
10% in terms of prediction accuracy. ConvLSTMTransNet surpasses traditional
models due to its innovative architectural features, which enhance its ability
to capture temporal dependencies and extract spatial features from internet
traffic data. Overall, these findings underscore the importance of employing
advanced architectures tailored to the complexities of internet traffic data
for achieving more precise predictions.",2024-09-20,"Sajal Saha, Saikat Das, Glaucio H. S. Carvalho",http://arxiv.org/pdf/2409.13179v1,cs.LG
An Adaptive End-to-End IoT Security Framework Using Explainable AI and LLMs,"The exponential growth of the Internet of Things (IoT) has significantly
increased the complexity and volume of cybersecurity threats, necessitating the
development of advanced, scalable, and interpretable security frameworks. This
paper presents an innovative, comprehensive framework for real-time IoT attack
detection and response that leverages Machine Learning (ML), Explainable AI
(XAI), and Large Language Models (LLM). By integrating XAI techniques such as
SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable
Model-agnostic Explanations) with a model-independent architecture, we ensure
our framework's adaptability across various ML algorithms. Additionally, the
incorporation of LLMs enhances the interpretability and accessibility of
detection decisions, providing system administrators with actionable,
human-understandable explanations of detected threats. Our end-to-end framework
not only facilitates a seamless transition from model development to deployment
but also represents a real-world application capability that is often lacking
in existing research. Based on our experiments with the CIC-IOT-2023 dataset
\cite{neto2023ciciot2023}, Gemini and OPENAI LLMS demonstrate unique strengths
in attack mitigation: Gemini offers precise, focused strategies, while OPENAI
provides extensive, in-depth security measures. Incorporating SHAP and LIME
algorithms within XAI provides comprehensive insights into attack detection,
emphasizing opportunities for model improvement through detailed feature
analysis, fine-tuning, and the adaptation of misclassifications to enhance
accuracy.",2024-09-20,"Sudipto Baral, Sajal Saha, Anwar Haque",http://arxiv.org/pdf/2409.13177v1,cs.LG
RPAF: A Reinforcement Prediction-Allocation Framework for Cache Allocation in Large-Scale Recommender Systems,"Modern recommender systems are built upon computation-intensive
infrastructure, and it is challenging to perform real-time computation for each
request, especially in peak periods, due to the limited computational
resources. Recommending by user-wise result caches is widely used when the
system cannot afford a real-time recommendation. However, it is challenging to
allocate real-time and cached recommendations to maximize the users' overall
engagement. This paper shows two key challenges to cache allocation, i.e., the
value-strategy dependency and the streaming allocation. Then, we propose a
reinforcement prediction-allocation framework (RPAF) to address these issues.
RPAF is a reinforcement-learning-based two-stage framework containing
prediction and allocation stages. The prediction stage estimates the values of
the cache choices considering the value-strategy dependency, and the allocation
stage determines the cache choices for each individual request while satisfying
the global budget constraint. We show that the challenge of training RPAF
includes globality and the strictness of budget constraints, and a relaxed
local allocator (RLA) is proposed to address this issue. Moreover, a PoolRank
algorithm is used in the allocation stage to deal with the streaming allocation
problem. Experiments show that RPAF significantly improves users' engagement
under computational budget constraints.",2024-09-20,"Shuo Su, Xiaoshuang Chen, Yao Wang, Yulin Wu, Ziqiang Zhang, Kaiqiao Zhan, Ben Wang, Kun Gai",http://arxiv.org/pdf/2409.13175v1,cs.LG
Bilateral Sharpness-Aware Minimization for Flatter Minima,"Sharpness-Aware Minimization (SAM) enhances generalization by reducing a
Max-Sharpness (MaxS). Despite the practical success, we empirically found that
the MAxS behind SAM's generalization enhancements face the ""Flatness Indicator
Problem"" (FIP), where SAM only considers the flatness in the direction of
gradient ascent, resulting in a next minimization region that is not
sufficiently flat. A better Flatness Indicator (FI) would bring a better
generalization of neural networks. Because SAM is a greedy search method in
nature. In this paper, we propose to utilize the difference between the
training loss and the minimum loss over the neighborhood surrounding the
current weight, which we denote as Min-Sharpness (MinS). By merging MaxS and
MinS, we created a better FI that indicates a flatter direction during the
optimization. Specially, we combine this FI with SAM into the proposed
Bilateral SAM (BSAM) which finds a more flatter minimum than that of SAM. The
theoretical analysis proves that BSAM converges to local minima. Extensive
experiments demonstrate that BSAM offers superior generalization performance
and robustness compared to vanilla SAM across various tasks, i.e.,
classification, transfer learning, human pose estimation, and network
quantization. Code is publicly available at: https://github.com/ajiaaa/BSAM.",2024-09-20,"Jiaxin Deng, Junbiao Pang, Baochang Zhang, Qingming Huang",http://arxiv.org/pdf/2409.13173v1,cs.LG
Deep Learning based Optical Image Super-Resolution via Generative Diffusion Models for Layerwise in-situ LPBF Monitoring,"The stochastic formation of defects during Laser Powder Bed Fusion (L-PBF)
negatively impacts its adoption for high-precision use cases. Optical
monitoring techniques can be used to identify defects based on layer-wise
imaging, but these methods are difficult to scale to high resolutions due to
cost and memory constraints. Therefore, we implement generative deep learning
models to link low-cost, low-resolution images of the build plate to detailed
high-resolution optical images of the build plate, enabling cost-efficient
process monitoring. To do so, a conditional latent probabilistic diffusion
model is trained to produce realistic high-resolution images of the build plate
from low-resolution webcam images, recovering the distribution of small-scale
features and surface roughness. We first evaluate the performance of the model
by analyzing the reconstruction quality of the generated images using
peak-signal-to-noise-ratio (PSNR), structural similarity index measure (SSIM)
and wavelet covariance metrics that describe the preservation of high-frequency
information. Additionally, we design a framework based upon the Segment
Anything foundation model to recreate the 3D morphology of the printed part and
analyze the surface roughness of the reconstructed samples. Finally, we explore
the zero-shot generalization capabilities of the implemented framework to other
part geometries by creating synthetic low-resolution data.",2024-09-20,"Francis Ogoke, Sumesh Kalambettu Suresh, Jesse Adamczyk, Dan Bolintineanu, Anthony Garland, Michael Heiden, Amir Barati Farimani",http://arxiv.org/pdf/2409.13171v1,cs.LG
Hidden Activations Are Not Enough: A General Approach to Neural Network Predictions,"We introduce a novel mathematical framework for analyzing neural networks
using tools from quiver representation theory. This framework enables us to
quantify the similarity between a new data sample and the training data, as
perceived by the neural network. By leveraging the induced quiver
representation of a data sample, we capture more information than traditional
hidden layer outputs. This quiver representation abstracts away the complexity
of the computations of the forward pass into a single matrix, allowing us to
employ simple geometric and statistical arguments in a matrix space to study
neural network predictions. Our mathematical results are architecture-agnostic
and task-agnostic, making them broadly applicable. As proof of concept
experiments, we apply our results for the MNIST and FashionMNIST datasets on
the problem of detecting adversarial examples on different MLP architectures
and several adversarial attack methods. Our experiments can be reproduced with
our
\href{https://github.com/MarcoArmenta/Hidden-Activations-are-not-Enough}{publicly
available repository}.",2024-09-20,"Samuel Leblanc, Aiky Rasolomanana, Marco Armenta",http://arxiv.org/pdf/2409.13163v1,cs.LG
DS2TA: Denoising Spiking Transformer with Attenuated Spatiotemporal Attention,"Vision Transformers (ViT) are current high-performance models of choice for
various vision applications. Recent developments have given rise to
biologically inspired spiking transformers that thrive in ultra-low power
operations on neuromorphic hardware, however, without fully unlocking the
potential of spiking neural networks. We introduce DS2TA, a Denoising Spiking
transformer with attenuated SpatioTemporal Attention, designed specifically for
vision applications. DS2TA introduces a new spiking attenuated spatiotemporal
attention mechanism that considers input firing correlations occurring in both
time and space, thereby fully harnessing the computational power of spiking
neurons at the core of the transformer architecture. Importantly, DS2TA
facilitates parameter-efficient spatiotemporal attention computation without
introducing extra weights. DS2TA employs efficient hashmap-based nonlinear
spiking attention denoisers to enhance the robustness and expressive power of
spiking attention maps. DS2TA demonstrates state-of-the-art performances on
several widely adopted static image and dynamic neuromorphic datasets. Operated
over 4 time steps, DS2TA achieves 94.92% top-1 accuracy on CIFAR10 and 77.47%
top-1 accuracy on CIFAR100, as well as 79.1% and 94.44% on CIFAR10-DVS and
DVS-Gesture using 10 time steps.",2024-09-20,"Boxun Xu, Hejia Geng, Yuxuan Yin, Peng Li",http://arxiv.org/pdf/2409.15375v1,cs.LG
Convergence of Distributed Adaptive Optimization with Local Updates,"We study distributed adaptive algorithms with local updates (intermittent
communication). Despite the great empirical success of adaptive methods in
distributed training of modern machine learning models, the theoretical
benefits of local updates within adaptive methods, particularly in terms of
reducing communication complexity, have not been fully understood yet. In this
paper, for the first time, we prove that \em Local SGD \em with momentum (\em
Local \em SGDM) and \em Local \em Adam can outperform their minibatch
counterparts in convex and weakly convex settings in certain regimes,
respectively. Our analysis relies on a novel technique to prove contraction
during local iterations, which is a crucial yet challenging step to show the
advantages of local updates, under generalized smoothness assumption and
gradient clipping strategy.",2024-09-20,"Ziheng Cheng, Margalit Glasgow",http://arxiv.org/pdf/2409.13155v2,cs.LG
Score-Based Multibeam Point Cloud Denoising,"Multibeam echo-sounder (MBES) is the de-facto sensor for bathymetry mapping.
In recent years, cheaper MBES sensors and global mapping initiatives have led
to exponential growth of available data. However, raw MBES data contains 1-25%
of noise that requires semi-automatic filtering using tools such as Combined
Uncertainty and Bathymetric Estimator (CUBE). In this work, we draw
inspirations from the 3D point cloud community and adapted a score-based point
cloud denoising network for MBES outlier detection and denoising. We trained
and evaluated this network on real MBES survey data. The proposed method was
found to outperform classical methods, and can be readily integrated into
existing MBES standard workflow. To facilitate future research, the code and
pretrained model are available online.",2024-09-20,"Li Ling, Yiping Xie, Nils Bore, John Folkesson",http://arxiv.org/pdf/2409.13143v1,cs.LG
Learning to Compare Hardware Designs for High-Level Synthesis,"High-level synthesis (HLS) is an automated design process that transforms
high-level code into hardware designs, enabling the rapid development of
hardware accelerators. HLS relies on pragmas, which are directives inserted
into the source code to guide the synthesis process, and pragmas have various
settings and values that significantly impact the resulting hardware design.
State-of-the-art ML-based HLS methods, such as HARP, first train a deep
learning model, typically based on graph neural networks (GNNs) applied to
graph-based representations of the source code and pragmas. They then perform
design space exploration (DSE) to explore the pragma design space, rank
candidate designs using the model, and return the top designs. However,
traditional DSE methods face challenges due to the highly nonlinear
relationship between pragma settings and performance metrics, along with
complex interactions between pragmas that affect performance in non-obvious
ways.
  To address these challenges, we propose compareXplore, a novel approach that
learns to compare hardware designs for effective HLS optimization.
CompareXplore introduces a hybrid loss function that combines pairwise
preference learning with pointwise performance prediction, enabling the model
to capture both relative preferences and absolute performance. Moreover, we
introduce a novel node difference attention module that focuses on the most
informative differences between designs, enabling the model to identify
critical pragmas impacting performance. CompareXplore adopts a two-stage DSE,
where a pointwise prediction model is used for the initial design pruning,
followed by a pairwise comparison stage for precise performance verification.
In extensive experiments, compareXplore achieves significant improvements in
ranking metrics and generates high-quality HLS results for the selected
designs, outperforming the existing SOTA method.",2024-09-20,"Yunsheng Bai, Atefeh Sohrabizadeh, Zijian Ding, Rongjian Liang, Weikai Li, Ding Wang, Haoxing Ren, Yizhou Sun, Jason Cong",http://arxiv.org/pdf/2409.13138v2,cs.LG
Federated Learning with Label-Masking Distillation,"Federated learning provides a privacy-preserving manner to collaboratively
train models on data distributed over multiple local clients via the
coordination of a global server. In this paper, we focus on label distribution
skew in federated learning, where due to the different user behavior of the
client, label distributions between different clients are significantly
different. When faced with such cases, most existing methods will lead to a
suboptimal optimization due to the inadequate utilization of label distribution
information in clients. Inspired by this, we propose a label-masking
distillation approach termed FedLMD to facilitate federated learning via
perceiving the various label distributions of each client. We classify the
labels into majority and minority labels based on the number of examples per
class during training. The client model learns the knowledge of majority labels
from local data. The process of distillation masks out the predictions of
majority labels from the global model, so that it can focus more on preserving
the minority label knowledge of the client. A series of experiments show that
the proposed approach can achieve state-of-the-art performance in various
cases. Moreover, considering the limited resources of the clients, we propose a
variant FedLMD-Tf that does not require an additional teacher, which
outperforms previous lightweight approaches without increasing computational
costs. Our code is available at https://github.com/wnma3mz/FedLMD.",2024-09-20,"Jianghu Lu, Shikun Li, Kexin Bao, Pengju Wang, Zhenxing Qian, Shiming Ge",http://arxiv.org/pdf/2409.13136v1,cs.LG
CorBin-FL: A Differentially Private Federated Learning Mechanism using Common Randomness,"Federated learning (FL) has emerged as a promising framework for distributed
machine learning. It enables collaborative learning among multiple clients,
utilizing distributed data and computing resources. However, FL faces
challenges in balancing privacy guarantees, communication efficiency, and
overall model accuracy. In this work, we introduce CorBin-FL, a privacy
mechanism that uses correlated binary stochastic quantization to achieve
differential privacy while maintaining overall model accuracy. The approach
uses secure multi-party computation techniques to enable clients to perform
correlated quantization of their local model updates without compromising
individual privacy. We provide theoretical analysis showing that CorBin-FL
achieves parameter-level local differential privacy (PLDP), and that it
asymptotically optimizes the privacy-utility trade-off between the mean square
error utility measure and the PLDP privacy measure. We further propose
AugCorBin-FL, an extension that, in addition to PLDP, achieves user-level and
sample-level central differential privacy guarantees. For both mechanisms, we
derive bounds on privacy parameters and mean squared error performance
measures. Extensive experiments on MNIST and CIFAR10 datasets demonstrate that
our mechanisms outperform existing differentially private FL mechanisms,
including Gaussian and Laplacian mechanisms, in terms of model accuracy under
equal PLDP privacy budgets.",2024-09-20,"Hojat Allah Salehi, Md Jueal Mia, S. Sandeep Pradhan, M. Hadi Amini, Farhad Shirani",http://arxiv.org/pdf/2409.13133v1,cs.LG
Explainable AI for Autism Diagnosis: Identifying Critical Brain Regions Using fMRI Data,"Early diagnosis and intervention for Autism Spectrum Disorder (ASD) has been
shown to significantly improve the quality of life of autistic individuals.
However, diagnostics methods for ASD rely on assessments based on clinical
presentation that are prone to bias and can be challenging to arrive at an
early diagnosis. There is a need for objective biomarkers of ASD which can help
improve diagnostic accuracy. Deep learning (DL) has achieved outstanding
performance in diagnosing diseases and conditions from medical imaging data.
Extensive research has been conducted on creating models that classify ASD
using resting-state functional Magnetic Resonance Imaging (fMRI) data. However,
existing models lack interpretability. This research aims to improve the
accuracy and interpretability of ASD diagnosis by creating a DL model that can
not only accurately classify ASD but also provide explainable insights into its
working. The dataset used is a preprocessed version of the Autism Brain Imaging
Data Exchange (ABIDE) with 884 samples. Our findings show a model that can
accurately classify ASD and highlight critical brain regions differing between
ASD and typical controls, with potential implications for early diagnosis and
understanding of the neural basis of ASD. These findings are validated by
studies in the literature that use different datasets and modalities,
confirming that the model actually learned characteristics of ASD and not just
the dataset. This study advances the field of explainable AI in medical imaging
by providing a robust and interpretable model, thereby contributing to a future
with objective and reliable ASD diagnostics.",2024-09-19,"Suryansh Vidya, Kush Gupta, Amir Aly, Andy Wills, Emmanuel Ifeachor, Rohit Shankar",http://arxiv.org/pdf/2409.15374v2,cs.LG
Disentangling Recognition and Decision Regrets in Image-Based Reinforcement Learning,"In image-based reinforcement learning (RL), policies usually operate in two
steps: first extracting lower-dimensional features from raw images (the
""recognition"" step), and then taking actions based on the extracted features
(the ""decision"" step). Extracting features that are spuriously correlated with
performance or irrelevant for decision-making can lead to poor generalization
performance, known as observational overfitting in image-based RL. In such
cases, it can be hard to quantify how much of the error can be attributed to
poor feature extraction vs. poor decision-making. To disentangle the two
sources of error, we introduce the notions of recognition regret and decision
regret. Using these notions, we characterize and disambiguate the two distinct
causes behind observational overfitting: over-specific representations, which
include features that are not needed for optimal decision-making (leading to
high decision regret), vs. under-specific representations, which only include a
limited set of features that were spuriously correlated with performance during
training (leading to high recognition regret). Finally, we provide illustrative
examples of observational overfitting due to both over-specific and
under-specific representations in maze environments and the Atari game Pong.",2024-09-19,"Alihan Hüyük, Arndt Ryo Koblitz, Atefeh Mohajeri, Matthew Andrews",http://arxiv.org/pdf/2409.13108v2,cs.LG
What Would You Ask When You First Saw $a^2+b^2=c^2$? Evaluating LLM on Curiosity-Driven Questioning,"Large language models (LLMs) can store a massive amount of knowledge, yet
their potential to acquire new knowledge remains unknown. We propose a novel
evaluation framework that evaluates this capability. This framework prompts
LLMs to generate questions about a statement introducing scientific knowledge,
simulating a curious person when facing the statement for the first time. We
score the qualities of the generated questions, thereby evaluating the
knowledge acquisition potential of the LLM. We apply controlled ablation
studies to validate our scoring procedures. Additionally, we created a
synthetic dataset consisting of 1101 statements in physics, chemistry, and
maths with distinct levels of difficulties, 300 general knowledge statements,
and 567 incorrect statements. Human evaluations were conducted to validate our
model assessments, achieving an approximate weighted Cohen's kappa of 0.7 on
all three metrics considered. We find that while large models like GPT-4 and
Mistral 8x7b are adept at generating coherent and relevant questions, the
smaller Phi-2 model is equally or more effective. This indicates that size does
not solely determine a model's knowledge acquisition potential. The proposed
framework quantifies a critical model capability that was commonly overlooked
and opens up research opportunities for developing more knowledgeable AI
systems",2024-09-19,"Shashidhar Reddy Javaji, Zining Zhu",http://arxiv.org/pdf/2409.17172v1,cs.LG
ERIC: Estimating Rainfall with Commodity Doorbell Camera for Precision Residential Irrigation,"Current state-of-the-art residential irrigation systems, such as WaterMyYard,
rely on rainfall data from nearby weather stations to adjust irrigation
amounts. However, the accuracy of rainfall data is compromised by the limited
spatial resolution of rain gauges and the significant variability of hyperlocal
rainfall, leading to substantial water waste. To improve irrigation efficiency,
we developed a cost-effective irrigation system, dubbed ERIC, which employs
machine learning models to estimate rainfall from commodity doorbell camera
footage and optimizes irrigation schedules without human intervention.
Specifically, we: a) designed novel visual and audio features with lightweight
neural network models to infer rainfall from the camera at the edge, preserving
user privacy; b) built a complete end-to-end irrigation system on Raspberry Pi
4, costing only \$75. We deployed the system across five locations (collecting
over 750 hours of video) with varying backgrounds and light conditions.
Comprehensive evaluation validates that ERIC achieves state-of-the-art rainfall
estimation performance ($\sim$ 5mm/day), saving 9,112 gallons/month of water,
translating to \$28.56/month in utility savings. Data and code are available at
https://github.com/LENSS/ERIC-BuildSys2024.git",2024-09-19,"Tian Liu, Liuyi Jin, Radu Stoleru, Amran Haroon, Charles Swanson, Kexin Feng",http://arxiv.org/pdf/2409.13104v2,cs.LG
Predicting soccer matches with complex networks and machine learning,"Soccer attracts the attention of many researchers and professionals in the
sports industry. Therefore, the incorporation of science into the sport is
constantly growing, with increasing investments in performance analysis and
sports prediction industries. This study aims to (i) highlight the use of
complex networks as an alternative tool for predicting soccer match outcomes,
and (ii) show how the combination of structural analysis of passing networks
with match statistical data can provide deeper insights into the game patterns
and strategies used by teams. In order to do so, complex network metrics and
match statistics were used to build machine learning models that predict the
wins and losses of soccer teams in different leagues. The results showed that
models based on passing networks were as effective as ``traditional'' models,
which use general match statistics. Another finding was that by combining both
approaches, more accurate models were obtained than when they were used
separately, demonstrating that the fusion of such approaches can offer a deeper
understanding of game patterns, allowing the comprehension of tactics employed
by teams relationships between players, their positions, and interactions
during matches. It is worth mentioning that both network metrics and match
statistics were important and impactful for the mixed model. Furthermore, the
use of networks with a lower granularity of temporal evolution (such as
creating a network for each half of the match) performed better than a single
network for the entire game.",2024-09-19,"Eduardo Alves Baratela, Felipe Jordão Xavier, Thomas Peron, Paulino Ribeiro Villas-Boas, Francisco Aparecido Rodrigues",http://arxiv.org/pdf/2409.13098v1,cs.LG
Fast decision tree learning solves hard coding-theoretic problems,"We connect the problem of properly PAC learning decision trees to the
parameterized Nearest Codeword Problem ($k$-NCP). Despite significant effort by
the respective communities, algorithmic progress on both problems has been
stuck: the fastest known algorithm for the former runs in quasipolynomial time
(Ehrenfeucht and Haussler 1989) and the best known approximation ratio for the
latter is $O(n/\log n)$ (Berman and Karpinsky 2002; Alon, Panigrahy, and
Yekhanin 2009). Research on both problems has thus far proceeded independently
with no known connections.
  We show that $\textit{any}$ improvement of Ehrenfeucht and Haussler's
algorithm will yield $O(\log n)$-approximation algorithms for $k$-NCP, an
exponential improvement of the current state of the art. This can be
interpreted either as a new avenue for designing algorithms for $k$-NCP, or as
one for establishing the optimality of Ehrenfeucht and Haussler's algorithm.
Furthermore, our reduction along with existing inapproximability results for
$k$-NCP already rule out polynomial-time algorithms for properly learning
decision trees. A notable aspect of our hardness results is that they hold even
in the setting of $\textit{weak}$ learning whereas prior ones were limited to
the setting of strong learning.",2024-09-19,"Caleb Koch, Carmen Strassle, Li-Yang Tan",http://arxiv.org/pdf/2409.13096v2,cs.LG
Personalized Speech Recognition for Children with Test-Time Adaptation,"Accurate automatic speech recognition (ASR) for children is crucial for
effective real-time child-AI interaction, especially in educational
applications. However, off-the-shelf ASR models primarily pre-trained on adult
data tend to generalize poorly to children's speech due to the data domain
shift from adults to children. Recent studies have found that supervised
fine-tuning on children's speech data can help bridge this domain shift, but
human annotations may be impractical to obtain for real-world applications and
adaptation at training time can overlook additional domain shifts occurring at
test time. We devised a novel ASR pipeline to apply unsupervised test-time
adaptation (TTA) methods for child speech recognition, so that ASR models
pre-trained on adult speech can be continuously adapted to each child speaker
at test time without further human annotations. Our results show that ASR
models adapted with TTA methods significantly outperform the unadapted
off-the-shelf ASR baselines both on average and statistically across individual
child speakers. Our analysis also discovered significant data domain shifts
both between child speakers and within each child speaker, which further
motivates the need for test-time adaptation.",2024-09-19,"Zhonghao Shi, Harshvardhan Srivastava, Xuan Shi, Shrikanth Narayanan, Maja J. Matarić",http://arxiv.org/pdf/2409.13095v1,cs.LG
Embedding Geometries of Contrastive Language-Image Pre-Training,"Since the publication of CLIP, the approach of using InfoNCE loss for
contrastive pre-training has become widely popular for bridging two or more
modalities. Despite its wide adoption, CLIP's original design choices of L2
normalization and cosine similarity logit have rarely been revisited. We have
systematically experimented with alternative geometries and softmax logits for
language-image pre-training and identified that variants with intuitive
Euclidean geometry, Euclidean CLIP (EuCLIP), match or exceed the performance of
CLIP and support hierarchical relationships at least as well as more
complicated hyperbolic alternative.",2024-09-19,"Jason Chuan-Chih Chou, Nahid Alam",http://arxiv.org/pdf/2409.13079v1,cs.LG
What does guidance do? A fine-grained analysis in a simple setting,"The use of guidance in diffusion models was originally motivated by the
premise that the guidance-modified score is that of the data distribution
tilted by a conditional likelihood raised to some power. In this work we
clarify this misconception by rigorously proving that guidance fails to sample
from the intended tilted distribution.
  Our main result is to give a fine-grained characterization of the dynamics of
guidance in two cases, (1) mixtures of compactly supported distributions and
(2) mixtures of Gaussians, which reflect salient properties of guidance that
manifest on real-world data. In both cases, we prove that as the guidance
parameter increases, the guided model samples more heavily from the boundary of
the support of the conditional distribution. We also prove that for any nonzero
level of score estimation error, sufficiently large guidance will result in
sampling away from the support, theoretically justifying the empirical finding
that large guidance results in distorted generations.
  In addition to verifying these results empirically in synthetic settings, we
also show how our theoretical insights can offer useful prescriptions for
practical deployment.",2024-09-19,"Muthu Chidambaram, Khashayar Gatmiry, Sitan Chen, Holden Lee, Jianfeng Lu",http://arxiv.org/pdf/2409.13074v1,cs.LG
E-Sort: Empowering End-to-end Neural Network for Multi-channel Spike Sorting with Transfer Learning and Fast Post-processing,"Decoding extracellular recordings is a crucial task in electrophysiology and
brain-computer interfaces. Spike sorting, which distinguishes spikes and their
putative neurons from extracellular recordings, becomes computationally
demanding with the increasing number of channels in modern neural probes. To
address the intensive workload and complex neuron interactions, we propose
E-Sort, an end-to-end neural network-based spike sorter with transfer learning
and parallelizable post-processing. Our framework reduces the required number
of annotated spikes for training by 44% compared to training from scratch,
achieving up to 25.68% higher accuracy. Additionally, our novel post-processing
algorithm is compatible with deep learning frameworks, making E-Sort
significantly faster than state-of-the-art spike sorters. On synthesized
Neuropixels recordings, E-Sort achieves comparable accuracy with Kilosort4
while sorting 50 seconds of data in only 1.32 seconds. Our method demonstrates
robustness across various probe geometries, noise levels, and drift conditions,
offering a substantial improvement in both accuracy and runtime efficiency
compared to existing spike sorters.",2024-09-19,"Yuntao Han, Shiwei Wang",http://arxiv.org/pdf/2409.13067v2,cs.LG
Improved Image Classification with Manifold Neural Networks,"Graph Neural Networks (GNNs) have gained popularity in various learning
tasks, with successful applications in fields like molecular biology,
transportation systems, and electrical grids. These fields naturally use graph
data, benefiting from GNNs' message-passing framework. However, the potential
of GNNs in more general data representations, especially in the image domain,
remains underexplored. Leveraging the manifold hypothesis, which posits that
high-dimensional data lies in a low-dimensional manifold, we explore GNNs'
potential in this context. We construct an image manifold using variational
autoencoders, then sample the manifold to generate graphs where each node is an
image. This approach reduces data dimensionality while preserving geometric
information. We then train a GNN to predict node labels corresponding to the
image labels in the classification task, and leverage convergence of GNNs to
manifold neural networks to analyze GNN generalization. Experiments on MNIST
and CIFAR10 datasets demonstrate that GNNs generalize effectively to unseen
graphs, achieving competitive accuracy in classification tasks.",2024-09-19,"Caio F. Deberaldini Netto, Zhiyang Wang, Luana Ruiz",http://arxiv.org/pdf/2409.13063v1,cs.LG
Comprehensive Overview of Artificial Intelligence Applications in Modern Industries,"Artificial Intelligence (AI) is fundamentally reshaping various industries by
enhancing decision-making processes, optimizing operations, and unlocking new
opportunities for innovation. This paper explores the applications of AI across
four key sectors: healthcare, finance, manufacturing, and retail. Each section
delves into the specific challenges faced by these industries, the AI
technologies employed to address them, and the measurable impact on business
outcomes and societal welfare. We also discuss the implications of AI
integration, including ethical considerations, the future trajectory of AI
development, and its potential to drive economic growth while posing challenges
that need to be managed responsibly.",2024-09-19,"Yijie Weng, Jianhao Wu, Tara Kelly, William Johnson",http://arxiv.org/pdf/2409.13059v1,cs.LG
LLM Surgery: Efficient Knowledge Unlearning and Editing in Large Language Models,"Large language models (LLMs) have revolutionized various domains, yet their
utility comes with significant challenges related to outdated or problematic
knowledge embedded during pretraining. This paper addresses the challenge of
modifying LLMs to unlearn problematic and outdated information while
efficiently integrating new knowledge without retraining from scratch. Here, we
propose LLM Surgery, a framework to efficiently modify LLM behaviour by
optimizing a three component objective function that: (1) Performs reverse
gradient on unlearning dataset (problematic and outdated information), (2)
Performs gradient descent on the update dataset (new and updated information),
and (3) Minimizes the KL divergence on the retain dataset (small subset of
unchanged text), ensuring alignment between pretrained and modified model
outputs. Due to the lack of publicly available datasets specifically tailored
for our novel task, we compiled a new dataset and an evaluation benchmark.
Using Llama2-7B, we demonstrate that LLM Surgery can achieve significant
forgetting on the unlearn set, a 20\% increase in accuracy on the update set,
and maintain performance on the retain set.",2024-09-19,"Akshaj Kumar Veldanda, Shi-Xiong Zhang, Anirban Das, Supriyo Chakraborty, Stephen Rawls, Sambit Sahu, Milind Naphade",http://arxiv.org/pdf/2409.13054v1,cs.LG
Towards Unbiased Evaluation of Time-series Anomaly Detector,"Time series anomaly detection (TSAD) is an evolving area of research
motivated by its critical applications, such as detecting seismic activity,
sensor failures in industrial plants, predicting crashes in the stock market,
and so on. Across domains, anomalies occur significantly less frequently than
normal data, making the F1-score the most commonly adopted metric for anomaly
detection. However, in the case of time series, it is not straightforward to
use standard F1-score because of the dissociation between `time points' and
`time events'. To accommodate this, anomaly predictions are adjusted, called as
point adjustment (PA), before the $F_1$-score evaluation. However, these
adjustments are heuristics-based, and biased towards true positive detection,
resulting in over-estimated detector performance. In this work, we propose an
alternative adjustment protocol called ``Balanced point adjustment'' (BA). It
addresses the limitations of existing point adjustment methods and provides
guarantees of fairness backed by axiomatic definitions of TSAD evaluation.",2024-09-19,"Debarpan Bhattacharya, Sumanta Mukherjee, Chandramouli Kamanchi, Vijay Ekambaram, Arindam Jati, Pankaj Dayama",http://arxiv.org/pdf/2409.13053v1,cs.LG
TACE: Tumor-Aware Counterfactual Explanations,"The application of deep learning in medical imaging has significantly
advanced diagnostic capabilities, enhancing both accuracy and efficiency.
Despite these benefits, the lack of transparency in these AI models, often
termed ""black boxes,"" raises concerns about their reliability in clinical
settings. Explainable AI (XAI) aims to mitigate these concerns by developing
methods that make AI decisions understandable and trustworthy. In this study,
we propose Tumor Aware Counterfactual Explanations (TACE), a framework designed
to generate reliable counterfactual explanations for medical images. Unlike
existing methods, TACE focuses on modifying tumor-specific features without
altering the overall organ structure, ensuring the faithfulness of the
counterfactuals. We achieve this by including an additional step in the
generation process which allows to modify only the region of interest (ROI),
thus yielding more reliable counterfactuals as the rest of the organ remains
unchanged. We evaluate our method on mammography images and brain MRI. We find
that our method far exceeds existing state-of-the-art techniques in quality,
faithfulness, and generation speed of counterfactuals. Indeed, more faithful
explanations lead to a significant improvement in classification success rates,
with a 10.69% increase for breast cancer and a 98.02% increase for brain
tumors. The code of our work is available at https://github.com/ispamm/TACE.",2024-09-19,"Eleonora Beatrice Rossi, Eleonora Lopez, Danilo Comminiello",http://arxiv.org/pdf/2409.13045v1,cs.LG
Enhancing Performance and Scalability of Large-Scale Recommendation Systems with Jagged Flash Attention,"The integration of hardware accelerators has significantly advanced the
capabilities of modern recommendation systems, enabling the exploration of
complex ranking paradigms previously deemed impractical. However, the GPU-based
computational costs present substantial challenges. In this paper, we
demonstrate our development of an efficiency-driven approach to explore these
paradigms, moving beyond traditional reliance on native PyTorch modules. We
address the specific challenges posed by ranking models' dependence on
categorical features, which vary in length and complicate GPU utilization. We
introduce Jagged Feature Interaction Kernels, a novel method designed to
extract fine-grained insights from long categorical features through efficient
handling of dynamically sized tensors. We further enhance the performance of
attention mechanisms by integrating Jagged tensors with Flash Attention. Our
novel Jagged Flash Attention achieves up to 9x speedup and 22x memory reduction
compared to dense attention. Notably, it also outperforms dense flash
attention, with up to 3x speedup and 53% more memory efficiency. In production
models, we observe 10% QPS improvement and 18% memory savings, enabling us to
scale our recommendation systems with longer features and more complex
architectures.",2024-09-19,"Rengan Xu, Junjie Yang, Yifan Xu, Hong Li, Xing Liu, Devashish Shankar, Haoci Zhang, Meng Liu, Boyang Li, Yuxi Hu, Mingwei Tang, Zehua Zhang, Tunhou Zhang, Dai Li, Sijia Chen, Gian-Paolo Musumeci, Jiaqi Zhai, Bill Zhu, Hong Yan, Srihari Reddy",http://arxiv.org/pdf/2409.15373v1,cs.LG
TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement Learning,"The increasing prevalence of large language models (LLMs) such as GPT-4 in
various applications has led to a surge in the size of prompts required for
optimal performance, leading to challenges in computational efficiency. Prompt
compression aims to reduce the inference cost by minimizing input tokens
without compromising on the task performance. However, existing prompt
compression techniques either rely on sub-optimal metrics such as information
entropy or model it as a task-agnostic token classification problem that fails
to capture task-specific information. To address these issues, we propose a
novel and efficient reinforcement learning (RL) based task-aware prompt
compression method. To ensure low latency requirements, we leverage existing
Transformer encoder-based token classification model while guiding the learning
process with task-specific reward signals using lightweight REINFORCE
algorithm. We evaluate the performance of our method on three diverse and
challenging tasks including text summarization, question answering and code
summarization. We demonstrate that our RL-guided compression method improves
the task performance by 8% - 189% across these three scenarios over
state-of-the-art compression techniques while satisfying the same compression
rate and latency requirements.",2024-09-19,"Shivam Shandilya, Menglin Xia, Supriyo Ghosh, Huiqiang Jiang, Jue Zhang, Qianhui Wu, Victor Rühle",http://arxiv.org/pdf/2409.13035v3,cs.LG
Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free Manner,"Advancements in Large Language Models (LLMs) inspire various strategies for
integrating video modalities. A key approach is Video-LLMs, which incorporate
an optimizable interface linking sophisticated video encoders to LLMs. However,
due to computation and data limitations, these Video-LLMs are typically
pre-trained to process only short videos, limiting their broader application
for understanding longer video content. Additionally, fine-tuning Video-LLMs to
handle longer videos is cost-prohibitive. Consequently, it becomes essential to
explore the interpolation of Video-LLMs under a completely training-free
setting. In this paper, we first identify the primary challenges in
interpolating Video-LLMs: (1) the video encoder and modality alignment
projector are fixed, preventing the integration of additional frames into
Video-LLMs, and (2) the LLM backbone is limited in its content length
capabilities, which complicates the processing of an increased number of video
tokens. To address these challenges, we propose a specific INTerPolation method
for Video-LLMs (INTP-Video-LLMs). We introduce an alternative video token
rearrangement technique that circumvents limitations imposed by the fixed video
encoder and alignment projector. Furthermore, we introduce a training-free LLM
context window extension method to enable Video-LLMs to understand a
correspondingly increased number of visual tokens.",2024-09-19,"Yuzhang Shang, Bingxin Xu, Weitai Kang, Mu Cai, Yuheng Li, Zehao Wen, Zhen Dong, Kurt Keutzer, Yong Jae Lee, Yan Yan",http://arxiv.org/pdf/2409.12963v2,cs.LG
MURI: High-Quality Instruction Tuning Datasets for Low-Resource Languages via Reverse Instructions,"Instruction tuning enhances large language models (LLMs) by aligning them
with human preferences across diverse tasks. Traditional approaches to create
instruction tuning datasets face serious challenges for low-resource languages
due to their dependence on data annotation. This work introduces a novel
method, Multilingual Reverse Instructions (MURI), which generates high-quality
instruction tuning datasets for low-resource languages without requiring human
annotators or pre-existing multilingual models. Utilizing reverse instructions
and a translation pipeline, MURI produces instruction-output pairs from
existing human-written texts in low-resource languages. This method ensures
cultural relevance and diversity by sourcing texts from different native
domains and applying filters to eliminate inappropriate content. Our dataset,
MURI-IT, includes more than 2 million instruction-output pairs across 200
languages. Evaluation by native speakers and fine-tuning experiments with mT5
models demonstrate the approach's effectiveness for both NLU and open-ended
generation. We publicly release datasets and models at
https://github.com/akoksal/muri.",2024-09-19,"Abdullatif Köksal, Marion Thaler, Ayyoob Imani, Ahmet Üstün, Anna Korhonen, Hinrich Schütze",http://arxiv.org/pdf/2409.12958v1,cs.LG
The Gaussian Discriminant Variational Autoencoder (GdVAE): A Self-Explainable Model with Counterfactual Explanations,"Visual counterfactual explanation (CF) methods modify image concepts, e.g,
shape, to change a prediction to a predefined outcome while closely resembling
the original query image. Unlike self-explainable models (SEMs) and heatmap
techniques, they grant users the ability to examine hypothetical ""what-if""
scenarios. Previous CF methods either entail post-hoc training, limiting the
balance between transparency and CF quality, or demand optimization during
inference. To bridge the gap between transparent SEMs and CF methods, we
introduce the GdVAE, a self-explainable model based on a conditional
variational autoencoder (CVAE), featuring a Gaussian discriminant analysis
(GDA) classifier and integrated CF explanations. Full transparency is achieved
through a generative classifier that leverages class-specific prototypes for
the downstream task and a closed-form solution for CFs in the latent space. The
consistency of CFs is improved by regularizing the latent space with the
explainer function. Extensive comparisons with existing approaches affirm the
effectiveness of our method in producing high-quality CF explanations while
preserving transparency. Code and models are public.",2024-09-19,"Anselm Haselhoff, Kevin Trelenberg, Fabian Küppers, Jonas Schneider",http://arxiv.org/pdf/2409.12952v1,cs.LG
Geometric Interpretation of Layer Normalization and a Comparative Analysis with RMSNorm,"This paper presents a novel geometric interpretation of LayerNorm and
explores how LayerNorm influences the norm and orientation of hidden vectors in
the representation space. With these geometric insights, we prepare the
foundation for comparing LayerNorm with RMSNorm. We show that the definition of
LayerNorm is innately linked to the uniform vector, defined as $\boldsymbol{1}
= [1, 1, 1, 1, \cdots, 1]^T \in \mathbb{R}^d$. We then show that the
standardization step in LayerNorm can be understood in three simple steps: (i)
remove the component of a vector along the uniform vector, (ii) normalize the
remaining vector, and (iii) scale the resultant vector by $\sqrt{d}$, where $d$
is the dimensionality of the representation space. We also provide additional
insights into how LayerNorm operates at inference time. Finally, we compare the
hidden representations of LayerNorm-based LLMs with models trained using
RMSNorm and show that all LLMs naturally operate orthogonal to the uniform
vector at inference time, that is, on average they do not have a component
along the uniform vector during inference. This presents the first mechanistic
evidence that removing the component along the uniform vector in LayerNorm is a
redundant step. These results advocate for using RMSNorm over LayerNorm which
is also more computationally efficient.",2024-09-19,"Akshat Gupta, Atahan Ozdemir, Gopala Anumanchipalli",http://arxiv.org/pdf/2409.12951v2,cs.LG
Unrolled denoising networks provably learn optimal Bayesian inference,"Much of Bayesian inference centers around the design of estimators for
inverse problems which are optimal assuming the data comes from a known prior.
But what do these optimality guarantees mean if the prior is unknown? In recent
years, algorithm unrolling has emerged as deep learning's answer to this
age-old question: design a neural network whose layers can in principle
simulate iterations of inference algorithms and train on data generated by the
unknown prior. Despite its empirical success, however, it has remained unclear
whether this method can provably recover the performance of its optimal,
prior-aware counterparts.
  In this work, we prove the first rigorous learning guarantees for neural
networks based on unrolling approximate message passing (AMP). For compressed
sensing, we prove that when trained on data drawn from a product prior, the
layers of the network approximately converge to the same denoisers used in
Bayes AMP. We also provide extensive numerical experiments for compressed
sensing and rank-one matrix estimation demonstrating the advantages of our
unrolled architecture - in addition to being able to obliviously adapt to
general priors, it exhibits improvements over Bayes AMP in more general
settings of low dimensions, non-Gaussian designs, and non-product priors.",2024-09-19,"Aayush Karan, Kulin Shah, Sitan Chen, Yonina C. Eldar",http://arxiv.org/pdf/2409.12947v1,cs.LG
Revisiting Semi-supervised Adversarial Robustness via Noise-aware Online Robust Distillation,"The robust self-training (RST) framework has emerged as a prominent approach
for semi-supervised adversarial training. To explore the possibility of
tackling more complicated tasks with even lower labeling budgets, unlike prior
approaches that rely on robust pretrained models, we present SNORD - a simple
yet effective framework that introduces contemporary semi-supervised learning
techniques into the realm of adversarial training. By enhancing pseudo labels
and managing noisy training data more effectively, SNORD showcases impressive,
state-of-the-art performance across diverse datasets and labeling budgets, all
without the need for pretrained models. Compared to full adversarial
supervision, SNORD achieves a 90% relative robust accuracy under epsilon =
8/255 AutoAttack, requiring less than 0.1%, 2%, and 10% labels for CIFAR-10,
CIFAR-100, and TinyImageNet-200, respectively. Additional experiments confirm
the efficacy of each component and demonstrate the adaptability of integrating
SNORD with existing adversarial pretraining strategies to further bolster
robustness.",2024-09-19,"Tsung-Han Wu, Hung-Ting Su, Shang-Tse Chen, Winston H. Hsu",http://arxiv.org/pdf/2409.12946v1,cs.LG
iCost: A Novel Instance Complexity Based Cost-Sensitive Learning Framework,"Class imbalance in data presents significant challenges for classification
tasks. It is fairly common and requires careful handling to obtain desirable
performance. Traditional classification algorithms become biased toward the
majority class. One way to alleviate the scenario is to make the classifiers
cost-sensitive. This is achieved by assigning a higher misclassification cost
to minority-class instances. One issue with this implementation is that all the
minority-class instances are treated equally, and assigned with the same
penalty value. However, the learning difficulties of all the instances are not
the same. Instances that are located in the overlapping region or near the
decision boundary are harder to classify, whereas those further away are
easier. Without taking into consideration the instance complexity and naively
weighting all the minority-class samples uniformly, results in an unwarranted
bias and consequently, a higher number of misclassifications of the
majority-class instances. This is undesirable and to overcome the situation, we
propose a novel instance complexity-based cost-sensitive approach (termed
'iCost') in this study. We first categorize all the minority-class instances
based on their difficulty level and then the instances are penalized
accordingly. This ensures a more equitable instance weighting and prevents
excessive penalization. The performance of the proposed approach is tested on
65 binary and 10 multiclass imbalanced datasets against the traditional
cost-sensitive learning frameworks. A significant improvement in performance
has been observed, demonstrating the effectiveness of the proposed strategy.",2024-09-19,"Asif Newaz, Asif Ur Rahman Adib, Taskeed Jabid",http://arxiv.org/pdf/2409.13007v2,cs.LG
Training Language Models to Self-Correct via Reinforcement Learning,"Self-correction is a highly desirable capability of large language models
(LLMs), yet it has consistently been found to be largely ineffective in modern
LLMs. Current methods for training self-correction typically depend on either
multiple models, a more advanced model, or additional forms of supervision. To
address these shortcomings, we develop a multi-turn online reinforcement
learning (RL) approach, SCoRe, that significantly improves an LLM's
self-correction ability using entirely self-generated data. To build SCoRe, we
first show that variants of supervised fine-tuning (SFT) on offline
model-generated correction traces are often insufficient for instilling
self-correction behavior. In particular, we observe that training via SFT falls
prey to either a distribution mismatch between mistakes made by the
data-collection policy and the model's own responses, or to behavior collapse,
where learning implicitly prefers only a certain mode of correction behavior
that is often not effective at self-correction on test problems. SCoRe
addresses these challenges by training under the model's own distribution of
self-generated correction traces and using appropriate regularization to steer
the learning process into learning a self-correction behavior that is effective
at test time as opposed to fitting high-reward responses for a given prompt.
This regularization process includes an initial phase of multi-turn RL on a
base model to generate a policy initialization that is less susceptible to
collapse, followed by using a reward bonus to amplify self-correction. With
Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves
state-of-the-art self-correction performance, improving the base models'
self-correction by 15.6% and 9.1% respectively on MATH and HumanEval.",2024-09-19,"Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, Aleksandra Faust",http://arxiv.org/pdf/2409.12917v2,cs.LG
Online Proximal ADMM for Graph Learning from Streaming Smooth Signals,"Graph signal processing deals with algorithms and signal representations that
leverage graph structures for multivariate data analysis. Often said graph
topology is not readily available and may be time-varying, hence (dynamic)
graph structure learning from nodal (e.g., sensor) observations becomes a
critical first step. In this paper, we develop a novel algorithm for online
graph learning using observation streams, assumed to be smooth on the latent
graph. Unlike batch algorithms for topology identification from smooth signals,
our modus operandi is to process graph signals sequentially and thus keep
memory and computational costs in check. To solve the resulting
smoothness-regularized, time-varying inverse problem, we develop online and
lightweight iterations built upon the proximal variant of the alternating
direction method of multipliers (ADMM), well known for its fast convergence in
batch settings. The proximal term in the topology updates seamlessly implements
a temporal-variation regularization, and we argue the online procedure exhibits
sublinear static regret under some simplifying assumptions. Reproducible
experiments with synthetic and real graphs demonstrate the effectiveness of our
method in adapting to streaming signals and tracking slowly-varying network
connectivity. The proposed approach also exhibits better tracking performance
(in terms of suboptimality), when compared to state-of-the-art online graph
learning baselines.",2024-09-19,"Hector Chahuara, Gonzalo Mateos",http://arxiv.org/pdf/2409.12916v1,cs.LG
Exploring Representations and Interventions in Time Series Foundation Models,"Time series foundation models (TSFMs) promise to be powerful tools for a wide
range of applications. However, their internal representations and learned
concepts are still not well understood. In this study, we investigate the
structure and redundancy of representations across various TSFMs, examining the
self-similarity of model layers within and across different model sizes. This
analysis reveals block-like redundancy in the representations, which can be
utilized for informed pruning to improve inference speed and efficiency.
Additionally, we explore the concepts learned by these models - such as
periodicity and trends - and how these can be manipulated through latent space
steering to influence model behavior. Our experiments show that steering
interventions can introduce new features, e.g., adding periodicity or trends to
signals that initially lacked them. These findings underscore the value of
representational analysis for optimizing models and demonstrate how conceptual
steering offers new possibilities for more controlled and efficient time series
analysis with TSFMs.",2024-09-19,"Michał Wiliński, Mononito Goswami, Nina Żukowska, Willa Potosnak, Artur Dubrawski",http://arxiv.org/pdf/2409.12915v3,cs.LG
Evaluating Defences against Unsafe Feedback in RLHF,"While there has been progress towards aligning Large Language Models (LLMs)
with human values and ensuring safe behaviour at inference time, safety guards
can easily be removed when fine tuned on unsafe and harmful datasets. While
this setting has been treated extensively, another popular training paradigm,
learning from unsafe feedback with reinforcement learning, has previously been
unexplored. This is concerning due to the widespread deployment of feedback
collection systems. We address this gap by providing an analysis of learning
settings where feedback is harmful, i.e. that unsafe samples are preferred over
safe ones despite model developers goal to maintain safety. We find that
safety-aligned LLMs easily explore unsafe action spaces via generating harmful
text and optimize for reward that violates safety constraints indicating that
current safety guards are not enough to prevent learning from unsafe feedback.
In order to protect against this vulnerability, we adapt a number of both
""implict"" and ""explicit"" harmful fine-tuning defences to evaluate whether they
are effective as learning constraints in an RLHF setting finding that no method
is generally effective pointing to the need for more defence research. We end
the paper with the observation that some defences work by performing ""harmless
reward hacking"" for which we provide a theoretical explanation drawn from the
theory of Constrained Markov Decision Processes and provide some direction for
future defence development.",2024-09-19,"Domenic Rosati, Giles Edkins, Harsh Raj, David Atanasov, Subhabrata Majumdar, Janarthanan Rajendran, Frank Rudzicz, Hassan Sajjad",http://arxiv.org/pdf/2409.12914v3,cs.LG
Universal approximation theorem for neural networks with inputs from a topological vector space,"We study feedforward neural networks with inputs from a topological vector
space (TVS-FNNs). Unlike traditional feedforward neural networks, TVS-FNNs can
process a broader range of inputs, including sequences, matrices, functions and
more. We prove a universal approximation theorem for TVS-FNNs, which
demonstrates their capacity to approximate any continuous function defined on
this expanded input space.",2024-09-19,Vugar Ismailov,http://arxiv.org/pdf/2409.12913v1,cs.LG
Data Poisoning and Leakage Analysis in Federated Learning,"Data poisoning and leakage risks impede the massive deployment of federated
learning in the real world. This chapter reveals the truths and pitfalls of
understanding two dominating threats: {\em training data privacy intrusion} and
{\em training data poisoning}. We first investigate training data privacy
threat and present our observations on when and how training data may be leaked
during the course of federated training. One promising defense strategy is to
perturb the raw gradient update by adding some controlled randomized noise
prior to sharing during each round of federated learning. We discuss the
importance of determining the proper amount of randomized noise and the proper
location to add such noise for effective mitigation of gradient leakage threats
against training data privacy. Then we will review and compare different
training data poisoning threats and analyze why and when such data poisoning
induced model Trojan attacks may lead to detrimental damage on the performance
of the global model. We will categorize and compare representative poisoning
attacks and the effectiveness of their mitigation techniques, delivering an
in-depth understanding of the negative impact of data poisoning. Finally, we
demonstrate the potential of dynamic model perturbation in simultaneously
ensuring privacy protection, poisoning resilience, and model performance. The
chapter concludes with a discussion on additional risk factors in federated
learning, including the negative impact of skewness, data and algorithmic
biases, as well as misinformation in training data. Powered by empirical
evidence, our analytical study offers some transformative insights into
effective privacy protection and security assurance strategies in
attack-resilient federated learning.",2024-09-19,"Wenqi Wei, Tiansheng Huang, Zachary Yahn, Anoop Singhal, Margaret Loper, Ling Liu",http://arxiv.org/pdf/2409.13004v1,cs.LG
Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization,"The pre-training phase of language models often begins with randomly
initialized parameters. With the current trends in scaling models, training
their large number of parameters can be extremely slow and costly. In contrast,
small language models are less expensive to train, but they often cannot
achieve the accuracy of large models. In this paper, we explore an intriguing
idea to connect these two different regimes: Can we develop a method to
initialize large language models using smaller pre-trained models? Will such
initialization bring any benefits in terms of training time and final accuracy?
In this paper, we introduce HyperCloning, a method that can expand the
parameters of a pre-trained language model to those of a larger model with
increased hidden dimensions. Our method ensures that the larger model retains
the functionality of the smaller model. As a result, the larger model already
inherits the predictive power and accuracy of the smaller model before the
training starts. We demonstrate that training such an initialized model results
in significant savings in terms of GPU hours required for pre-training large
language models.",2024-09-19,"Mohammad Samragh, Iman Mirzadeh, Keivan Alizadeh Vahid, Fartash Faghri, Minsik Cho, Moin Nabi, Devang Naik, Mehrdad Farajtabar",http://arxiv.org/pdf/2409.12903v2,cs.LG
Fast End-to-End Generation of Belief Space Paths for Minimum Sensing Navigation,"We revisit the problem of motion planning in the Gaussian belief space.
Motivated by the fact that most existing sampling-based planners suffer from
high computational costs due to the high-dimensional nature of the problem, we
propose an approach that leverages a deep learning model to predict optimal
path candidates directly from the problem description. Our proposed approach
consists of three steps. First, we prepare a training dataset comprising a
large number of input-output pairs: the input image encodes the problem to be
solved (e.g., start states, goal states, and obstacle locations), whereas the
output image encodes the solution (i.e., the ground truth of the shortest
path). Any existing planner can be used to generate this training dataset.
Next, we leverage the U-Net architecture to learn the dependencies between the
input and output data. Finally, a trained U-Net model is applied to a new
problem encoded as an input image. From the U-Net's output image, which is
interpreted as a distribution of paths,an optimal path candidate is
reconstructed. The proposed method significantly reduces computation time
compared to the sampling-based baseline algorithm.",2024-09-19,"Lukas Taus, Vrushabh Zinage, Takashi Tanaka, Richard Tsai",http://arxiv.org/pdf/2409.12902v1,cs.LG
Automatic Classification of White Blood Cell Images using Convolutional Neural Network,"Human immune system contains white blood cells (WBC) that are good indicator
of many diseases like bacterial infections, AIDS, cancer, spleen, etc. White
blood cells have been sub classified into four types: monocytes, lymphocytes,
eosinophils and neutrophils on the basis of their nucleus, shape and cytoplasm.
Traditionally in laboratories, pathologists and hematologists analyze these
blood cells through microscope and then classify them manually. This manual
process takes more time and increases the chance of human error. Hence, there
is a need to automate this process. In this paper, first we have used different
CNN pre-train models such as ResNet-50, InceptionV3, VGG16 and MobileNetV2 to
automatically classify the white blood cells. These pre-train models are
applied on Kaggle dataset of microscopic images. Although we achieved
reasonable accuracy ranging between 92 to 95%, still there is need to enhance
the performance. Hence, inspired by these architectures, a framework has been
proposed to automatically categorize the four kinds of white blood cells with
increased accuracy. The aim is to develop a convolution neural network (CNN)
based classification system with decent generalization ability. The proposed
CNN model has been tested on white blood cells images from Kaggle and LISC
datasets. Accuracy achieved is 99.57% and 98.67% for both datasets
respectively. Our proposed convolutional neural network-based model provides
competitive performance as compared to previous results reported in literature.",2024-09-19,"Rabia Asghar, Arslan Shaukat, Usman Akram, Rimsha Tariq",http://arxiv.org/pdf/2409.13442v4,cs.LG
Fuzzy Rule based Intelligent Cardiovascular Disease Prediction using Complex Event Processing,"Cardiovascular disease (CVDs) is a rapidly rising global concern due to
unhealthy diets, lack of physical activity, and other factors. According to the
World Health Organization (WHO), primary risk factors include elevated blood
pressure, glucose, blood lipids, and obesity. Recent research has focused on
accurate and timely disease prediction to reduce risk and fatalities, often
relying on predictive models trained on large datasets, which require intensive
training. An intelligent system for CVDs patients could greatly assist in
making informed decisions by effectively analyzing health parameters. Complex
Event Processing (CEP) has emerged as a valuable method for solving real-time
challenges by aggregating patterns of interest and their causes and effects on
end users. In this work, we propose a fuzzy rule-based system for monitoring
clinical data to provide real-time decision support. We designed fuzzy rules
based on clinical and WHO standards to ensure accurate predictions. Our
integrated approach uses Apache Kafka and Spark for data streaming, and the
Siddhi CEP engine for event processing. Additionally, we pass numerous
cardiovascular disease-related parameters through CEP engines to ensure fast
and reliable prediction decisions. To validate the effectiveness of our
approach, we simulated real-time, unseen data to predict cardiovascular
disease. Using synthetic data (1000 samples), we categorized it into ""Very Low
Risk, Low Risk, Medium Risk, High Risk, and Very High Risk."" Validation results
showed that 20% of samples were categorized as very low risk, 15-45% as low
risk, 35-65% as medium risk, 55-85% as high risk, and 75% as very high risk.",2024-09-19,"Shashi Shekhar Kumar, Anurag Harsh, Ritesh Chandra, Sonali Agarwal",http://arxiv.org/pdf/2409.15372v1,cs.LG
On the Hardness of Decentralized Multi-Agent Policy Evaluation under Byzantine Attacks,"In this paper, we study a fully-decentralized multi-agent policy evaluation
problem, which is an important sub-problem in cooperative multi-agent
reinforcement learning, in the presence of up to $f$ faulty agents. In
particular, we focus on the so-called Byzantine faulty model with model
poisoning setting. In general, policy evaluation is to evaluate the value
function of any given policy. In cooperative multi-agent system, the
system-wide rewards are usually modeled as the uniform average of rewards from
all agents. We investigate the multi-agent policy evaluation problem in the
presence of Byzantine agents, particularly in the setting of heterogeneous
local rewards. Ideally, the goal of the agents is to evaluate the accumulated
system-wide rewards, which are uniform average of rewards of the normal agents
for a given policy. It means that all agents agree upon common values (the
consensus part) and furthermore, the consensus values are the value functions
(the convergence part). However, we prove that this goal is not achievable.
Instead, we consider a relaxed version of the problem, where the goal of the
agents is to evaluate accumulated system-wide reward, which is an appropriately
weighted average reward of the normal agents. We further prove that there is no
correct algorithm that can guarantee that the total number of positive weights
exceeds $|\mathcal{N}|-f $, where $|\mathcal{N}|$ is the number of normal
agents. Towards the end, we propose a Byzantine-tolerant decentralized temporal
difference algorithm that can guarantee asymptotic consensus under scalar
function approximation. We then empirically test the effective of the proposed
algorithm.",2024-09-19,"Hairi, Minghong Fang, Zifan Zhang, Alvaro Velasquez, Jia Liu",http://arxiv.org/pdf/2409.12882v2,cs.LG
Enhancing E-commerce Product Title Translation with Retrieval-Augmented Generation and Large Language Models,"E-commerce stores enable multilingual product discovery which require
accurate product title translation. Multilingual large language models (LLMs)
have shown promising capacity to perform machine translation tasks, and it can
also enhance and translate product titles cross-lingually in one step. However,
product title translation often requires more than just language conversion
because titles are short, lack context, and contain specialized terminology.
This study proposes a retrieval-augmented generation (RAG) approach that
leverages existing bilingual product information in e-commerce by retrieving
similar bilingual examples and incorporating them as few-shot prompts to
enhance LLM-based product title translation. Experiment results show that our
proposed RAG approach improve product title translation quality with chrF score
gains of up to 15.3% for language pairs where the LLM has limited proficiency.",2024-09-19,"Bryan Zhang, Taichi Nakatani, Stephan Walter",http://arxiv.org/pdf/2409.12880v1,cs.LG
Impact of ML Optimization Tactics on Greener Pre-Trained ML Models,"Background: Given the fast-paced nature of today's technology, which has
surpassed human performance in tasks like image classification, visual
reasoning, and English understanding, assessing the impact of Machine Learning
(ML) on energy consumption is crucial. Traditionally, ML projects have
prioritized accuracy over energy, creating a gap in energy consumption during
model inference.
  Aims: This study aims to (i) analyze image classification datasets and
pre-trained models, (ii) improve inference efficiency by comparing optimized
and non-optimized models, and (iii) assess the economic impact of the
optimizations.
  Method: We conduct a controlled experiment to evaluate the impact of various
PyTorch optimization techniques (dynamic quantization, torch.compile, local
pruning, and global pruning) to 42 Hugging Face models for image
classification. The metrics examined include GPU utilization, power and energy
consumption, accuracy, time, computational complexity, and economic costs. The
models are repeatedly evaluated to quantify the effects of these software
engineering tactics.
  Results: Dynamic quantization demonstrates significant reductions in
inference time and energy consumption, making it highly suitable for
large-scale systems. Additionally, torch.compile balances accuracy and energy.
In contrast, local pruning shows no positive impact on performance, and global
pruning's longer optimization times significantly impact costs.
  Conclusions: This study highlights the role of software engineering tactics
in achieving greener ML models, offering guidelines for practitioners to make
informed decisions on optimization methods that align with sustainability
goals.",2024-09-19,"Alexandra González Álvarez, Joel Castaño, Xavier Franch, Silverio Martínez-Fernández",http://arxiv.org/pdf/2409.12878v1,cs.LG
Semi-overcomplete convolutional auto-encoder embedding as shape priors for deep vessel segmentation,"The extraction of blood vessels has recently experienced a widespread
interest in medical image analysis. Automatic vessel segmentation is highly
desirable to guide clinicians in computer-assisted diagnosis, therapy or
surgical planning. Despite a good ability to extract large anatomical
structures, the capacity of U-Net inspired architectures to automatically
delineate vascular systems remains a major issue, especially given the scarcity
of existing datasets. In this paper, we present a novel approach that
integrates into deep segmentation shape priors from a Semi-Overcomplete
Convolutional Auto-Encoder (S-OCAE) embedding. Compared to standard
Convolutional Auto-Encoders (CAE), it exploits an over-complete branch that
projects data onto higher dimensions to better characterize tiny structures.
Experiments on retinal and liver vessel extraction, respectively performed on
publicly-available DRIVE and 3D-IRCADb datasets, highlight the effectiveness of
our method compared to U-Net trained without and with shape priors from a
traditional CAE.",2024-09-19,"Amine Sadikine, Bogdan Badic, Jean-Pierre Tasu, Vincent Noblet, Dimitris Visvikis, Pierre-Henri Conze",http://arxiv.org/pdf/2409.13001v1,cs.LG
Deep Learning-Based Detection of Referable Diabetic Retinopathy and Macular Edema Using Ultra-Widefield Fundus Imaging,"Diabetic retinopathy and diabetic macular edema are significant complications
of diabetes that can lead to vision loss. Early detection through
ultra-widefield fundus imaging enhances patient outcomes but presents
challenges in image quality and analysis scale. This paper introduces deep
learning solutions for automated UWF image analysis within the framework of the
MICCAI 2024 UWF4DR challenge. We detail methods and results across three tasks:
image quality assessment, detection of referable DR, and identification of DME.
Employing advanced convolutional neural network architectures such as
EfficientNet and ResNet, along with preprocessing and augmentation strategies,
our models demonstrate robust performance in these tasks. Results indicate that
deep learning can significantly aid in the automated analysis of UWF images,
potentially improving the efficiency and accuracy of DR and DME detection in
clinical settings.",2024-09-19,"Philippe Zhang, Pierre-Henri Conze, Mathieu Lamard, Gwenolé Quellec, Mostafa El Habib Daho",http://arxiv.org/pdf/2409.12854v1,cs.LG
A Margin-Maximizing Fine-Grained Ensemble Method,"Ensemble learning has achieved remarkable success in machine learning, but
its reliance on numerous base learners limits its application in
resource-constrained environments. This paper introduces an innovative
""Margin-Maximizing Fine-Grained Ensemble Method"" that achieves performance
surpassing large-scale ensembles by meticulously optimizing a small number of
learners and enhancing generalization capability. We propose a novel learnable
confidence matrix, quantifying each classifier's confidence for each category,
precisely capturing category-specific advantages of individual learners.
Furthermore, we design a margin-based loss function, constructing a smooth and
partially convex objective using the logsumexp technique. This approach
improves optimization, eases convergence, and enables adaptive confidence
allocation. Finally, we prove that the loss function is Lipschitz continuous,
based on which we develop an efficient gradient optimization algorithm that
simultaneously maximizes margins and dynamically adjusts learner weights.
Extensive experiments demonstrate that our method outperforms traditional
random forests using only one-tenth of the base learners and other
state-of-the-art ensemble methods.",2024-09-19,"Jinghui Yuan, Hao Chen, Renwei Luo, Feiping Nie",http://arxiv.org/pdf/2409.12849v1,cs.LG
How the (Tensor-) Brain uses Embeddings and Embodiment to Encode Senses and Symbols,"The Tensor Brain (TB) has been introduced as a computational model for
perception and memory. This paper provides an overview of the TB model,
incorporating recent developments and insights into its functionality. The TB
is composed of two primary layers: the representation layer and the index
layer. The representation layer serves as a model for the subsymbolic global
workspace, a concept derived from consciousness research. Its state represents
the cognitive brain state, capturing the dynamic interplay of sensory and
cognitive processes. The index layer, in contrast, contains symbolic
representations for concepts, time instances, and predicates. In a bottom-up
operation, sensory input activates the representation layer, which then
triggers associated symbolic labels in the index layer. Conversely, in a
top-down operation, symbols in the index layer activate the representation
layer, which in turn influences earlier processing layers through embodiment.
This top-down mechanism underpins semantic memory, enabling the integration of
abstract knowledge into perceptual and cognitive processes. A key feature of
the TB is its use of concept embeddings, which function as connection weights
linking the index layer to the representation layer. As a concept's ``DNA,''
these embeddings consolidate knowledge from diverse experiences, sensory
modalities, and symbolic representations, providing a unified framework for
learning and memory.",2024-09-19,"Volker Tresp, Hang Li",http://arxiv.org/pdf/2409.12846v2,cs.LG
Introducing the Large Medical Model: State of the art healthcare cost and risk prediction with transformers trained on patient event sequences,"With U.S. healthcare spending approaching $5T (NHE Fact Sheet 2024), and 25%
of it estimated to be wasteful (Waste in the US the health care system:
estimated costs and potential for savings, n.d.), the need to better predict
risk and optimal patient care is evermore important. This paper introduces the
Large Medical Model (LMM), a generative pre-trained transformer (GPT) designed
to guide and predict the broad facets of patient care and healthcare
administration. The model is trained on medical event sequences from over 140M
longitudinal patient claims records with a specialized vocabulary built from
medical terminology systems and demonstrates a superior capability to forecast
healthcare costs and identify potential risk factors. Through experimentation
and validation, we showcase the LMM's proficiency in not only in cost and risk
predictions, but also in discerning intricate patterns within complex medical
conditions and an ability to identify novel relationships in patient care. The
LMM is able to improve both cost prediction by 14.1% over the best commercial
models and chronic conditions prediction by 1.9% over the best transformer
models in research predicting a broad set of conditions. The LMM is a
substantial advancement in healthcare analytics, offering the potential to
significantly enhance risk assessment, cost management, and personalized
medicine.",2024-09-19,"Ricky Sahu, Eric Marriott, Ethan Siegel, David Wagner, Flore Uzan, Troy Yang, Asim Javed",http://arxiv.org/pdf/2409.13000v2,cs.LG
CMINNs: Compartment Model Informed Neural Networks -- Unlocking Drug Dynamics,"In the field of pharmacokinetics and pharmacodynamics (PKPD) modeling, which
plays a pivotal role in the drug development process, traditional models
frequently encounter difficulties in fully encapsulating the complexities of
drug absorption, distribution, and their impact on targets. Although
multi-compartment models are frequently utilized to elucidate intricate drug
dynamics, they can also be overly complex. To generalize modeling while
maintaining simplicity, we propose an innovative approach that enhances PK and
integrated PK-PD modeling by incorporating fractional calculus or time-varying
parameter(s), combined with constant or piecewise constant parameters. These
approaches effectively model anomalous diffusion, thereby capturing drug
trapping and escape rates in heterogeneous tissues, which is a prevalent
phenomenon in drug dynamics. Furthermore, this method provides insight into the
dynamics of drug in cancer in multi-dose administrations. Our methodology
employs a Physics-Informed Neural Network (PINN) and fractional
Physics-Informed Neural Networks (fPINNs), integrating ordinary differential
equations (ODEs) with integer/fractional derivative order from compartmental
modeling with neural networks. This integration optimizes parameter estimation
for variables that are time-variant, constant, piecewise constant, or related
to the fractional derivative order. The results demonstrate that this
methodology offers a robust framework that not only markedly enhances the
model's depiction of drug absorption rates and distributed delayed responses
but also unlocks different drug-effect dynamics, providing new insights into
absorption rates, anomalous diffusion, drug resistance, peristance and
pharmacokinetic tolerance, all within a system of just two (fractional) ODEs
with explainable results.",2024-09-19,"Nazanin Ahmadi Daryakenari, Shupeng Wang, George Karniadakis",http://arxiv.org/pdf/2409.12998v1,cs.LG
VCAT: Vulnerability-aware and Curiosity-driven Adversarial Training for Enhancing Autonomous Vehicle Robustness,"Autonomous vehicles (AVs) face significant threats to their safe operation in
complex traffic environments. Adversarial training has emerged as an effective
method of enabling AVs to preemptively fortify their robustness against
malicious attacks. Train an attacker using an adversarial policy, allowing the
AV to learn robust driving through interaction with this attacker. However,
adversarial policies in existing methodologies often get stuck in a loop of
overexploiting established vulnerabilities, resulting in poor improvement for
AVs. To overcome the limitations, we introduce a pioneering framework termed
Vulnerability-aware and Curiosity-driven Adversarial Training (VCAT).
Specifically, during the traffic vehicle attacker training phase, a surrogate
network is employed to fit the value function of the AV victim, providing dense
information about the victim's inherent vulnerabilities. Subsequently, random
network distillation is used to characterize the novelty of the environment,
constructing an intrinsic reward to guide the attacker in exploring unexplored
territories. In the victim defense training phase, the AV is trained in
critical scenarios in which the pretrained attacker is positioned around the
victim to generate attack behaviors. Experimental results revealed that the
training methodology provided by VCAT significantly improved the robust control
capabilities of learning-based AVs, outperforming both conventional training
modalities and alternative reinforcement learning counterparts, with a marked
reduction in crash rates. The code is available at
https://github.com/caixxuan/VCAT.",2024-09-19,"Xuan Cai, Zhiyong Cui, Xuesong Bai, Ruimin Ke, Zhenshu Ma, Haiyang Yu, Yilong Ren",http://arxiv.org/pdf/2409.12997v1,cs.LG
Machine-learning based high-bandwidth magnetic sensing,"Recent years have seen significant growth of quantum technologies, and
specifically quantum sensing, both in terms of the capabilities of advanced
platforms and their applications. One of the leading platforms in this context
is nitrogen-vacancy (NV) color centers in diamond, providing versatile,
high-sensitivity, and high-resolution magnetic sensing. Nevertheless, current
schemes for spin resonance magnetic sensing (as applied by NV quantum sensing)
suffer from tradeoffs associated with sensitivity, dynamic range, and
bandwidth. Here we address this issue, and implement machine learning tools to
enhance NV magnetic sensing in terms of the sensitivity/bandwidth tradeoff in
large dynamic range scenarios. We experimentally demonstrate this new approach,
reaching an improvement in the relevant figure of merit by a factor of up to 5.
Our results promote quantum machine learning protocols for sensing applications
towards more feasible and efficient quantum technologies.",2024-09-19,"Galya Haim, Stefano Martina, John Howell, Nir Bar-Gill, Filippo Caruso",http://arxiv.org/pdf/2409.12820v1,cs.LG
pyrtklib: An open-source package for tightly coupled deep learning and GNSS integration for positioning in urban canyons,"Artificial intelligence (AI) is revolutionizing numerous fields, with
increasing applications in Global Navigation Satellite Systems (GNSS)
positioning algorithms in intelligent transportation systems (ITS) via deep
learning. However, a significant technological disparity exists as traditional
GNSS algorithms are often developed in Fortran or C, contrasting with the
Python-based implementation prevalent in deep learning tools. To address this
discrepancy, this paper introduces pyrtklib, a Python binding for the widely
utilized open-source GNSS tool, RTKLIB. This binding makes all RTKLIB
functionalities accessible in Python, facilitating seamless integration.
Moreover, we present a deep learning subsystem under pyrtklib, which is a novel
deep learning framework that leverages pyrtklib to accurately predict weights
and biases within the GNSS positioning process. The use of pyrtklib enables
developers to easily and quickly prototype and implement deep learning-aided
GNSS algorithms, showcasing its potential to enhance positioning accuracy
significantly.",2024-09-19,"Runzhi Hu, Penghui Xu, Yihan Zhong, Weisong Wen",http://arxiv.org/pdf/2409.12996v1,cs.LG
Hierarchical Gradient-Based Genetic Sampling for Accurate Prediction of Biological Oscillations,"Biological oscillations are periodic changes in various signaling processes
crucial for the proper functioning of living organisms. These oscillations are
modeled by ordinary differential equations, with coefficient variations leading
to diverse periodic behaviors, typically measured by oscillatory frequencies.
This paper explores sampling techniques for neural networks to model the
relationship between system coefficients and oscillatory frequency. However,
the scarcity of oscillations in the vast coefficient space results in many
samples exhibiting non-periodic behaviors, and small coefficient changes near
oscillation boundaries can significantly alter oscillatory properties. This
leads to non-oscillatory bias and boundary sensitivity, making accurate
predictions difficult. While existing importance and uncertainty sampling
approaches partially mitigate these challenges, they either fail to resolve the
sensitivity problem or result in redundant sampling. To address these
limitations, we propose the Hierarchical Gradient-based Genetic Sampling (HGGS)
framework, which improves the accuracy of neural network predictions for
biological oscillations. The first layer, Gradient-based Filtering, extracts
sensitive oscillation boundaries and removes redundant non-oscillatory samples,
creating a balanced coarse dataset. The second layer, Multigrid Genetic
Sampling, utilizes residual information to refine these boundaries and explore
new high-residual regions, increasing data diversity for model training.
Experimental results demonstrate that HGGS outperforms seven comparative
sampling methods across four biological systems, highlighting its effectiveness
in enhancing sampling and prediction accuracy.",2024-09-19,"Heng Rao, Yu Gu, Jason Zipeng Zhang, Ge Yu, Yang Cao, Minghan Chen",http://arxiv.org/pdf/2409.12816v2,cs.LG
A quest through interconnected datasets: lessons from highly-cited ICASSP papers,"As audio machine learning outcomes are deployed in societally impactful
applications, it is important to have a sense of the quality and origins of the
data used. Noticing that being explicit about this sense is not trivially
rewarded in academic publishing in applied machine learning domains, and
neither is included in typical applied machine learning curricula, we present a
study into dataset usage connected to the top-5 cited papers at the
International Conference on Acoustics, Speech, and Signal Processing (ICASSP).
In this, we conduct thorough depth-first analyses towards origins of used
datasets, often leading to searches that had to go beyond what was reported in
official papers, and ending into unclear or entangled origins. Especially in
the current pull towards larger, and possibly generative AI models, awareness
of the need for accountability on data provenance is increasing. With this, we
call on the community to not only focus on engineering larger models, but
create more room and reward for explicitizing the foundations on which such
models should be built.",2024-09-19,"Cynthia C. S. Liem, Doğa Taşcılar, Andrew M. Demetriou",http://arxiv.org/pdf/2410.03676v1,cs.LG
Robust estimation of the intrinsic dimension of data sets with quantum cognition machine learning,"We propose a new data representation method based on Quantum Cognition
Machine Learning and apply it to manifold learning, specifically to the
estimation of intrinsic dimension of data sets. The idea is to learn a
representation of each data point as a quantum state, encoding both local
properties of the point as well as its relation with the entire data. Inspired
by ideas from quantum geometry, we then construct from the quantum states a
point cloud equipped with a quantum metric. The metric exhibits a spectral gap
whose location corresponds to the intrinsic dimension of the data. The proposed
estimator is based on the detection of this spectral gap. When tested on
synthetic manifold benchmarks, our estimates are shown to be robust with
respect to the introduction of point-wise Gaussian noise. This is in contrast
to current state-of-the-art estimators, which tend to attribute artificial
``shadow dimensions'' to noise artifacts, leading to overestimates. This is a
significant advantage when dealing with real data sets, which are inevitably
affected by unknown levels of noise. We show the applicability and robustness
of our method on real data, by testing it on the ISOMAP face database, MNIST,
and the Wisconsin Breast Cancer Dataset.",2024-09-19,"Luca Candelori, Alexander G. Abanov, Jeffrey Berger, Cameron J. Hogan, Vahagn Kirakosyan, Kharen Musaelian, Ryan Samson, James E. T. Smith, Dario Villani, Martin T. Wells, Mengjia Xu",http://arxiv.org/pdf/2409.12805v1,cs.LG
The Central Role of the Loss Function in Reinforcement Learning,"This paper illustrates the central role of loss functions in data-driven
decision making, providing a comprehensive survey on their influence in
cost-sensitive classification (CSC) and reinforcement learning (RL). We
demonstrate how different regression loss functions affect the sample
efficiency and adaptivity of value-based decision making algorithms. Across
multiple settings, we prove that algorithms using the binary cross-entropy loss
achieve first-order bounds scaling with the optimal policy's cost and are much
more efficient than the commonly used squared loss. Moreover, we prove that
distributional algorithms using the maximum likelihood loss achieve
second-order bounds scaling with the policy variance and are even sharper than
first-order bounds. This in particular proves the benefits of distributional
RL. We hope that this paper serves as a guide analyzing decision making
algorithms with varying loss functions, and can inspire the reader to seek out
better loss functions to improve any decision making algorithm.",2024-09-19,"Kaiwen Wang, Nathan Kallus, Wen Sun",http://arxiv.org/pdf/2409.12799v3,cs.LG
Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL,"The temporal credit assignment problem is a central challenge in
Reinforcement Learning (RL), concerned with attributing the appropriate
influence to each actions in a trajectory for their ability to achieve a goal.
However, when feedback is delayed and sparse, the learning signal is poor, and
action evaluation becomes harder. Canonical solutions, such as reward shaping
and options, require extensive domain knowledge and manual intervention,
limiting their scalability and applicability. In this work, we lay the
foundations for Credit Assignment with Language Models (CALM), a novel approach
that leverages Large Language Models (LLMs) to automate credit assignment via
reward shaping and options discovery. CALM uses LLMs to decompose a task into
elementary subgoals and assess the achievement of these subgoals in
state-action transitions. Every time an option terminates, a subgoal is
achieved, and CALM provides an auxiliary reward. This additional reward signal
can enhance the learning process when the task reward is sparse and delayed
without the need for human-designed rewards. We provide a preliminary
evaluation of CALM using a dataset of human-annotated demonstrations from
MiniHack, suggesting that LLMs can be effective in assigning credit in
zero-shot settings, without examples or LLM fine-tuning. Our preliminary
results indicate that the knowledge of LLMs is a promising prior for credit
assignment in RL, facilitating the transfer of human knowledge into value
functions.",2024-09-19,"Eduardo Pignatelli, Johan Ferret, Tim Rockäschel, Edward Grefenstette, Davide Paglieri, Samuel Coward, Laura Toni",http://arxiv.org/pdf/2409.12798v1,cs.LG
Efficient Identification of Direct Causal Parents via Invariance and Minimum Error Testing,"Invariant causal prediction (ICP) is a popular technique for finding causal
parents (direct causes) of a target via exploiting distribution shifts and
invariance testing (Peters et al., 2016). However, since ICP needs to run an
exponential number of tests and fails to identify parents when distribution
shifts only affect a few variables, applying ICP to practical large scale
problems is challenging. We propose MMSE-ICP and fastICP, two approaches which
employ an error inequality to address the identifiability problem of ICP. The
inequality states that the minimum prediction error of the predictor using
causal parents is the smallest among all predictors which do not use
descendants. fastICP is an efficient approximation tailored for large problems
as it exploits the inequality and a heuristic to run fewer tests. MMSE-ICP and
fastICP not only outperform competitive baselines in many simulations but also
achieve state-of-the-art result on a large scale real data benchmark.",2024-09-19,"Minh Nguyen, Mert R. Sabuncu",http://arxiv.org/pdf/2409.12797v1,cs.LG
Multi-Source and Multi-Sequence Myocardial Pathology Segmentation Using a Cascading Refinement CNN,"Myocardial infarction (MI) is one of the most prevalent cardiovascular
diseases and consequently, a major cause for mortality and morbidity worldwide.
Accurate assessment of myocardial tissue viability for post-MI patients is
critical for diagnosis and treatment planning, e.g. allowing surgical
revascularization, or to determine the risk of adverse cardiovascular events in
the future. Fine-grained analysis of the myocardium and its surrounding
anatomical structures can be performed by combining the information obtained
from complementary medical imaging techniques. In this work, we use late
gadolinium enhanced (LGE) magnetic resonance (MR), T2-weighted (T2) MR and
balanced steady-state free precession (bSSFP) cine MR in order to semantically
segment the left and right ventricle, healthy and scarred myocardial tissue, as
well as edema. To this end, we propose the Multi-Sequence Cascading Refinement
CNN (MS-CaRe-CNN), a 2-stage CNN cascade that receives multi-sequence data and
generates predictions of the anatomical structures of interest without
considering tissue viability at Stage 1. The prediction of Stage 1 is then
further refined in Stage 2, where the model additionally distinguishes
myocardial tissue based on viability, i.e. healthy, scarred and edema regions.
Our proposed method is set up as a 5-fold ensemble and semantically segments
scar tissue achieving 62.31% DSC and 82.65% precision, as well as 63.78% DSC
and 87.69% precision for the combined scar and edema region. These promising
results for such small and challenging structures confirm that MS-CaRe-CNN is
well-suited to generate semantic segmentations to assess the viability of
myocardial tissue, enabling downstream tasks like personalized therapy
planning.",2024-09-19,"Franz Thaler, Darko Stern, Gernot Plank, Martin Urschler",http://arxiv.org/pdf/2409.12792v1,cs.LG
"Optimal or Greedy Decision Trees? Revisiting their Objectives, Tuning, and Performance","Recently there has been a surge of interest in optimal decision tree (ODT)
methods that globally optimize accuracy directly, in contrast to traditional
approaches that locally optimize an impurity or information metric. However,
the value of optimal methods is not well understood yet, as the literature
provides conflicting results, with some demonstrating superior out-of-sample
performance of ODTs over greedy approaches, while others show the opposite.
Through a novel extensive experimental study, we provide new insights into the
design and behavior of learning decision trees. In particular, we identify and
analyze two relatively unexplored aspects of ODTs: the objective function used
in training trees, and tuning techniques. Thus, we address these three
questions: what objective to optimize in ODTs; how to tune ODTs; and how do
optimal and greedy methods compare? Our experimental evaluation examines 11
objective functions, six tuning methods, and six claims from the literature on
optimal and greedy methods on 180 real and synthetic data sets. Through our
analysis, both conceptually and experimentally, we show the effect of
(non-)concave objectives in greedy and optimal approaches; we highlight the
importance of proper tuning of ODTs; support and refute several claims from the
literature; provide clear recommendations for researchers and practitioners on
the usage of greedy and optimal methods; and code for future comparisons.",2024-09-19,"Jacobus G. M. van der Linden, Daniël Vos, Mathijs M. de Weerdt, Sicco Verwer, Emir Demirović",http://arxiv.org/pdf/2409.12788v2,cs.LG
Improving generalisability of 3D binding affinity models in low data regimes,"Predicting protein-ligand binding affinity is an essential part of
computer-aided drug design. However, generalisable and performant global
binding affinity models remain elusive, particularly in low data regimes.
Despite the evolution of model architectures, current benchmarks are not
well-suited to probe the generalisability of 3D binding affinity models.
Furthermore, 3D global architectures such as GNNs have not lived up to
performance expectations. To investigate these issues, we introduce a novel
split of the PDBBind dataset, minimizing similarity leakage between train and
test sets and allowing for a fair and direct comparison between various model
architectures. On this low similarity split, we demonstrate that, in general,
3D global models are superior to protein-specific local models in low data
regimes. We also demonstrate that the performance of GNNs benefits from three
novel contributions: supervised pre-training via quantum mechanical data,
unsupervised pre-training via small molecule diffusion, and explicitly modeling
hydrogen atoms in the input graph. We believe that this work introduces
promising new approaches to unlock the potential of GNN architectures for
binding affinity modelling.",2024-09-19,"Julia Buhmann, Ward Haddadin, Lukáš Pravda, Alan Bilsland, Hagen Triendl",http://arxiv.org/pdf/2409.12995v1,cs.LG
Investigation on domain adaptation of additive manufacturing monitoring systems to enhance digital twin reusability,"Powder bed fusion (PBF) is an emerging metal additive manufacturing (AM)
technology that enables rapid fabrication of complex geometries. However,
defects such as pores and balling may occur and lead to structural
unconformities, thus compromising the mechanical performance of the part. This
has become a critical challenge for quality assurance as the nature of some
defects is stochastic during the process and invisible from the exterior. To
address this issue, digital twin (DT) using machine learning (ML)-based
modeling can be deployed for AM process monitoring and control. Melt pool is
one of the most commonly observed physical phenomena for process monitoring,
usually by high-speed cameras. Once labeled and preprocessed, the melt pool
images are used to train ML-based models for DT applications such as process
anomaly detection and print quality evaluation. Nonetheless, the reusability of
DTs is restricted due to the wide variability of AM settings, including AM
machines and monitoring instruments. The performance of the ML models trained
using the dataset collected from one setting is usually compromised when
applied to other settings. This paper proposes a knowledge transfer pipeline
between different AM settings to enhance the reusability of AM DTs. The source
and target datasets are collected from the National Institute of Standards and
Technology and National Cheng Kung University with different cameras,
materials, AM machines, and process parameters. The proposed pipeline consists
of four steps: data preprocessing, data augmentation, domain alignment, and
decision alignment. Compared with the model trained only using the source
dataset, this pipeline increased the melt pool anomaly detection accuracy by
31% without any labeled training data from the target dataset.",2024-09-19,"Jiarui Xie, Zhuo Yang, Chun-Chun Hu, Haw-Ching Yang, Yan Lu, Yaoyao Fiona Zhao",http://arxiv.org/pdf/2409.12785v2,cs.LG
The Robustness of Spiking Neural Networks in Communication and its Application towards Network Efficiency in Federated Learning,"Spiking Neural Networks (SNNs) have recently gained significant interest in
on-chip learning in embedded devices and emerged as an energy-efficient
alternative to conventional Artificial Neural Networks (ANNs). However, to
extend SNNs to a Federated Learning (FL) setting involving collaborative model
training, the communication between the local devices and the remote server
remains the bottleneck, which is often restricted and costly. In this paper, we
first explore the inherent robustness of SNNs under noisy communication in FL.
Building upon this foundation, we propose a novel Federated Learning with Top-K
Sparsification (FLTS) algorithm to reduce the bandwidth usage for FL training.
We discover that the proposed scheme with SNNs allows more bandwidth savings
compared to ANNs without impacting the model's accuracy. Additionally, the
number of parameters to be communicated can be reduced to as low as 6 percent
of the size of the original model. We further improve the communication
efficiency by enabling dynamic parameter compression during model training.
Extensive experiment results demonstrate that our proposed algorithms
significantly outperform the baselines in terms of communication cost and model
accuracy and are promising for practical network-efficient FL with SNNs.",2024-09-19,"Manh V. Nguyen, Liang Zhao, Bobin Deng, William Severa, Honghui Xu, Shaoen Wu",http://arxiv.org/pdf/2409.12769v1,cs.LG
Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space,"The use of synthetic speech as data augmentation is gaining increasing
popularity in fields such as automatic speech recognition and speech
classification tasks. Despite novel text-to-speech systems with voice cloning
capabilities, that allow the usage of a larger amount of voices based on short
audio segments, it is known that these systems tend to hallucinate and
oftentimes produce bad data that will most likely have a negative impact on the
downstream task. In the present work, we conduct a set of experiments around
zero-shot learning with synthetic speech data for the specific task of speech
commands classification. Our results on the Google Speech Commands dataset show
that a simple ASR-based filtering method can have a big impact in the quality
of the generated data, translating to a better performance. Furthermore,
despite the good quality of the generated speech data, we also show that
synthetic and real speech can still be easily distinguishable when using
self-supervised (WavLM) features, an aspect further explored with a CycleGAN to
bridge the gap between the two types of speech material.",2024-09-19,"Sebastião Quintas, Isabelle Ferrané, Thomas Pellegrini",http://arxiv.org/pdf/2409.12745v1,cs.LG
PRAGA: Prototype-aware Graph Adaptive Aggregation for Spatial Multi-modal Omics Analysis,"Spatial multi-modal omics technology, highlighted by Nature Methods as an
advanced biological technique in 2023, plays a critical role in resolving
biological regulatory processes with spatial context. Recently, graph neural
networks based on K-nearest neighbor (KNN) graphs have gained prominence in
spatial multi-modal omics methods due to their ability to model semantic
relations between sequencing spots. However, the fixed KNN graph fails to
capture the latent semantic relations hidden by the inevitable data
perturbations during the biological sequencing process, resulting in the loss
of semantic information. In addition, the common lack of spot annotation and
class number priors in practice further hinders the optimization of spatial
multi-modal omics models. Here, we propose a novel spatial multi-modal omics
resolved framework, termed PRototype-Aware Graph Adaptative Aggregation for
Spatial Multi-modal Omics Analysis (PRAGA). PRAGA constructs a dynamic graph to
capture latent semantic relations and comprehensively integrate spatial
information and feature semantics. The learnable graph structure can also
denoise perturbations by learning cross-modal knowledge. Moreover, a dynamic
prototype contrastive learning is proposed based on the dynamic adaptability of
Bayesian Gaussian Mixture Models to optimize the multi-modal omics
representations for unknown biological priors. Quantitative and qualitative
experiments on simulated and real datasets with 7 competing methods demonstrate
the superior performance of PRAGA. Code is available at
https://github.com/Xubin-s-Lab/PRAGA.",2024-09-19,"Xinlei Huang, Zhiqi Ma, Dian Meng, Yanran Liu, Shiwei Ruan, Qingqiang Sun, Xubin Zheng, Ziyue Qiao",http://arxiv.org/pdf/2409.12728v5,cs.LG
Performance and Power: Systematic Evaluation of AI Workloads on Accelerators with CARAML,"The rapid advancement of machine learning (ML) technologies has driven the
development of specialized hardware accelerators designed to facilitate more
efficient model training. This paper introduces the CARAML benchmark suite,
which is employed to assess performance and energy consumption during the
training of transformer-based large language models and computer vision models
on a range of hardware accelerators, including systems from NVIDIA, AMD, and
Graphcore. CARAML provides a compact, automated, extensible, and reproducible
framework for assessing the performance and energy of ML workloads across
various novel hardware architectures. The design and implementation of CARAML,
along with a custom power measurement tool called jpwr, are discussed in
detail.",2024-09-19,"Chelsea Maria John, Stepan Nassyr, Carolin Penke, Andreas Herten",http://arxiv.org/pdf/2409.12994v2,cs.LG
Rapid aerodynamic prediction of swept wings via physics-embedded transfer learning,"Machine learning-based models provide a promising way to rapidly acquire
transonic swept wing flow fields but suffer from large computational costs in
establishing training datasets. Here, we propose a physics-embedded transfer
learning framework to efficiently train the model by leveraging the idea that a
three-dimensional flow field around wings can be analyzed with two-dimensional
flow fields around cross-sectional airfoils. An airfoil aerodynamics prediction
model is pretrained with airfoil samples. Then, an airfoil-to-wing transfer
model is fine-tuned with a few wing samples to predict three-dimensional flow
fields based on two-dimensional results on each spanwise cross section. Sweep
theory is embedded when determining the corresponding airfoil geometry and
operating conditions, and to obtain the sectional airfoil lift coefficient,
which is one of the operating conditions, the low-fidelity vortex lattice
method and data-driven methods are proposed and evaluated. Compared to a
nontransfer model, introducing the pretrained model reduces the error by 30%,
while introducing sweep theory further reduces the error by 9%. When reducing
the dataset size, less than half of the wing training samples are need to reach
the same error level as the nontransfer framework, which makes establishing the
model much easier.",2024-09-19,"Yunjia Yang, Runze Li, Yufei Zhang, Lu Lu, Haixin Chen",http://arxiv.org/pdf/2409.12711v2,cs.LG
SeqRisk: Transformer-augmented latent variable model for improved survival prediction with longitudinal data,"In healthcare, risk assessment of different patient outcomes has for long
time been based on survival analysis, i.e.\ modeling time-to-event
associations. However, conventional approaches rely on data from a single
time-point, making them suboptimal for fully leveraging longitudinal patient
history and capturing temporal regularities. Focusing on clinical real-world
data and acknowledging its challenges, we utilize latent variable models to
effectively handle irregular, noisy, and sparsely observed longitudinal data.
We propose SeqRisk, a method that combines variational autoencoder (VAE) or
longitudinal VAE (LVAE) with a transformer encoder and Cox proportional hazards
module for risk prediction. SeqRisk captures long-range interactions, improves
patient trajectory representations, enhances predictive accuracy and
generalizability, as well as provides partial explainability for sample
population characteristics in attempts to identify high-risk patients. We
demonstrate that SeqRisk performs competitively compared to existing approaches
on both simulated and real-world datasets.",2024-09-19,"Mine Öğretir, Miika Koskinen, Juha Sinisalo, Risto Renkonen, Harri Lähdesmäki",http://arxiv.org/pdf/2409.12709v1,cs.LG
Machine-learning-based multipoint optimization of fluidic injection parameters for improving nozzle performance,"Fluidic injection provides a promising solution to improve the performance of
overexpanded single expansion ramp nozzle (SERN) during vehicle acceleration.
However, determining the injection parameters for the best overall performance
under multiple nozzle operating conditions is still a challenge. The
gradient-based optimization method requires gradients of injection parameters
at each design point, leading to high computational costs if traditional
computational fluid dynamic (CFD) simulations are adopted. This paper uses a
pretrained neural network model to replace CFD during optimization to quickly
calculate the nozzle flow field at multiple design points. Considering the
physical characteristics of the nozzle flow field, a prior-based prediction
strategy is adopted to enhance the model's transferability. In addition, the
back-propagation algorithm of the neural network is adopted to quickly evaluate
the gradients by calling the computation process only once, thereby greatly
reducing the gradient computation time compared to the finite differential
method. As a test case, the average nozzle thrust coefficient of a SERN at
seven design points is optimized. An improvement in the thrust coefficient of
1.14% is achieved, and the time cost is greatly reduced compared with the
traditional optimization methods, even when the time to establish the database
for training is considered.",2024-09-19,"Yunjia Yang, Jiazhe Li, Yufei Zhang, Haixin Chen",http://arxiv.org/pdf/2409.12707v1,cs.LG
Generation and Editing of Mandrill Faces: Application to Sex Editing and Assessment,"Generative AI has seen major developments in recent years, enhancing the
realism of synthetic images, also known as computer-generated images. In
addition, generative AI has also made it possible to modify specific image
characteristics through image editing. Previous work has developed methods
based on generative adversarial networks (GAN) for generating realistic images,
in particular faces, but also to modify specific features. However, this work
has never been applied to specific animal species. Moreover, the assessment of
the results has been generally done subjectively, rather than quantitatively.
In this paper, we propose an approach based on methods for generating images of
faces of male or female mandrills, a non-human primate. The main novelty of
proposed method is the ability to edit their sex by identifying a sex axis in
the latent space of a specific GAN. In addition, we have developed an
assessment of the sex levels based on statistical features extracted from real
image distributions. The experimental results we obtained from a specific
database are not only realistic, but also accurate, meeting a need for future
work in behavioral experiments with wild mandrills.",2024-09-19,"Nicolas M. Dibot, Julien P. Renoult, William Puech",http://arxiv.org/pdf/2409.12705v1,cs.LG
PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs),"The capability of generating high-quality source code using large language
models (LLMs) reduces software development time and costs. However, they often
introduce security vulnerabilities due to training on insecure open-source
data. This highlights the need for ensuring secure and functional code
generation. This paper introduces PromSec, an algorithm for prom optimization
for secure and functioning code generation using LLMs. In PromSec, we combine
1) code vulnerability clearing using a generative adversarial graph neural
network, dubbed as gGAN, to fix and reduce security vulnerabilities in
generated codes and 2) code generation using an LLM into an interactive loop,
such that the outcome of the gGAN drives the LLM with enhanced prompts to
generate secure codes while preserving their functionality. Introducing a new
contrastive learning approach in gGAN, we formulate code-clearing and
generation as a dual-objective optimization problem, enabling PromSec to
notably reduce the number of LLM inferences. PromSec offers a cost-effective
and practical solution for generating secure, functional code. Extensive
experiments conducted on Python and Java code datasets confirm that PromSec
effectively enhances code security while upholding its intended functionality.
Our experiments show that while a state-of-the-art approach fails to address
all code vulnerabilities, PromSec effectively resolves them. Moreover, PromSec
achieves more than an order-of-magnitude reduction in operation time, number of
LLM queries, and security analysis costs. Furthermore, prompts optimized with
PromSec for a certain LLM are transferable to other LLMs across programming
languages and generalizable to unseen vulnerabilities in training. This study
is a step in enhancing the trustworthiness of LLMs for secure and functional
code generation, supporting their integration into real-world software
development.",2024-09-19,"Mahmoud Nazzal, Issa Khalil, Abdallah Khreishah, NhatHai Phan",http://arxiv.org/pdf/2409.12699v1,cs.LG
(Un)certainty of (Un)fairness: Preference-Based Selection of Certainly Fair Decision-Makers,"Fairness metrics are used to assess discrimination and bias in
decision-making processes across various domains, including machine learning
models and human decision-makers in real-world applications. This involves
calculating the disparities between probabilistic outcomes among social groups,
such as acceptance rates between male and female applicants. However,
traditional fairness metrics do not account for the uncertainty in these
processes and lack of comparability when two decision-makers exhibit the same
disparity. Using Bayesian statistics, we quantify the uncertainty of the
disparity to enhance discrimination assessments. We represent each
decision-maker, whether a machine learning model or a human, by its disparity
and the corresponding uncertainty in that disparity. We define preferences over
decision-makers and utilize brute-force to choose the optimal decision-maker
according to a utility function that ranks decision-makers based on these
preferences. The decision-maker with the highest utility score can be
interpreted as the one for whom we are most certain that it is fair.",2024-09-19,"Manh Khoi Duong, Stefan Conrad",http://arxiv.org/pdf/2409.12677v1,cs.LG
Deep generative models as an adversarial attack strategy for tabular machine learning,"Deep Generative Models (DGMs) have found application in computer vision for
generating adversarial examples to test the robustness of machine learning (ML)
systems. Extending these adversarial techniques to tabular ML presents unique
challenges due to the distinct nature of tabular data and the necessity to
preserve domain constraints in adversarial examples. In this paper, we adapt
four popular tabular DGMs into adversarial DGMs (AdvDGMs) and evaluate their
effectiveness in generating realistic adversarial examples that conform to
domain constraints.",2024-09-19,"Salijona Dyrmishi, Mihaela Cătălina Stoian, Eleonora Giunchiglia, Maxime Cordy",http://arxiv.org/pdf/2409.12642v1,cs.LG
Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries,"We introduce Michelangelo: a minimal, synthetic, and unleaked long-context
reasoning evaluation for large language models which is also easy to
automatically score. This evaluation is derived via a novel, unifying framework
for evaluations over arbitrarily long contexts which measure the model's
ability to do more than retrieve a single piece of information from its
context. The central idea of the Latent Structure Queries framework (LSQ) is to
construct tasks which require a model to ``chisel away'' the irrelevant
information in the context, revealing a latent structure in the context. To
verify a model's understanding of this latent structure, we query the model for
details of the structure. Using LSQ, we produce three diagnostic long-context
evaluations across code and natural-language domains intended to provide a
stronger signal of long-context language model capabilities. We perform
evaluations on several state-of-the-art models and demonstrate both that a) the
proposed evaluations are high-signal and b) that there is significant room for
improvement in synthesizing long-context information.",2024-09-19,"Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, Rohan Anil, Ethan Dyer, Siamak Shakeri, Roopali Vij, Harsh Mehta, Vinay Ramasesh, Quoc Le, Ed Chi, Yifeng Lu, Orhan Firat, Angeliki Lazaridou, Jean-Baptiste Lespiau, Nithya Attaluri, Kate Olszewska",http://arxiv.org/pdf/2409.12640v2,cs.LG
Enhancing TinyBERT for Financial Sentiment Analysis Using GPT-Augmented FinBERT Distillation,"In the rapidly evolving field of financial sentiment analysis, the efficiency
and accuracy of predictive models are critical due to their significant impact
on financial markets. Transformer based models like BERT and large language
models (LLMs) like GPT-4, have advanced NLP tasks considerably. Despite their
advantages, BERT-based models face challenges with computational intensity in
edge computing environments, and the substantial size and compute requirements
of LLMs limit their practical deployment. This study proposes leveraging the
generative capabilities of LLMs, such as GPT-4 Omni, to create synthetic,
domain-specific training data. This approach addresses the challenge of data
scarcity and enhances the performance of smaller models by making them
competitive with their larger counterparts. The research specifically aims to
enhance FinBERT, a BERT model fine-tuned for financial sentiment analysis, and
develop TinyFinBERT, a compact transformer model, through a structured,
two-tiered knowledge distillation strategy. Using data augmented by GPT-4 Omni,
which involves generating new training examples and transforming existing data,
we significantly improved the accuracy of FinBERT, preparing it to serve as a
teacher model. This enhanced FinBERT then distilled knowledge to TinyFinBERT,
employing both GPT-4 Omni and GPT-3.5 Turbo augmented data. The distillation
strategy incorporated both logit and intermediate layer distillation. The
training and evaluation of TinyFinBERT utilized the PhraseBank dataset and the
FiQA 2018 Task1 dataset, achieving performance comparable to FinBERT while
being substantially smaller and more efficient. This research demonstrates how
LLMs can effectively contribute to the advancement of financial sentiment
analysis by enhancing the capabilities of smaller, more efficient models
through innovative data augmentation and distillation techniques.",2024-09-19,Graison Jos Thomas,http://arxiv.org/pdf/2409.18999v1,cs.LG
Image inpainting for corrupted images by using the semi-super resolution GAN,"Image inpainting is a valuable technique for enhancing images that have been
corrupted. The primary challenge in this research revolves around the extent of
corruption in the input image that the deep learning model must restore. To
address this challenge, we introduce a Generative Adversarial Network (GAN) for
learning and replicating the missing pixels. Additionally, we have developed a
distinct variant of the Super-Resolution GAN (SRGAN), which we refer to as the
Semi-SRGAN (SSRGAN). Furthermore, we leveraged three diverse datasets to assess
the robustness and accuracy of our proposed model. Our training process
involves varying levels of pixel corruption to attain optimal accuracy and
generate high-quality images.",2024-09-19,"Mehrshad Momen-Tayefeh, Mehrdad Momen-Tayefeh, Amir Ali Ghafourian Ghahramani",http://arxiv.org/pdf/2409.12636v1,cs.LG
Exploring bat song syllable representations in self-supervised audio encoders,"How well can deep learning models trained on human-generated sounds
distinguish between another species' vocalization types? We analyze the
encoding of bat song syllables in several self-supervised audio encoders, and
find that models pre-trained on human speech generate the most distinctive
representations of different syllable types. These findings form first steps
towards the application of cross-species transfer learning in bat bioacoustics,
as well as an improved understanding of out-of-distribution signal processing
in audio encoder models.",2024-09-19,"Marianne de Heer Kloots, Mirjam Knörnschild",http://arxiv.org/pdf/2409.12634v1,cs.LG
Counterfactual Explanations for Clustering Models,"Clustering algorithms rely on complex optimisation processes that may be
difficult to comprehend, especially for individuals who lack technical
expertise. While many explainable artificial intelligence techniques exist for
supervised machine learning, unsupervised learning -- and clustering in
particular -- has been largely neglected. To complicate matters further, the
notion of a ``true'' cluster is inherently challenging to define. These facets
of unsupervised learning and its explainability make it difficult to foster
trust in such methods and curtail their adoption. To address these challenges,
we propose a new, model-agnostic technique for explaining clustering algorithms
with counterfactual statements. Our approach relies on a novel soft-scoring
method that captures the spatial information utilised by clustering models. It
builds upon a state-of-the-art Bayesian counterfactual generator for supervised
learning to deliver high-quality explanations. We evaluate its performance on
five datasets and two clustering algorithms, and demonstrate that introducing
soft scores to guide counterfactual search significantly improves the results.",2024-09-19,"Aurora Spagnol, Kacper Sokol, Pietro Barbiero, Marc Langheinrich, Martin Gjoreski",http://arxiv.org/pdf/2409.12632v1,cs.LG
Green Federated Learning: A new era of Green Aware AI,"The development of AI applications, especially in large-scale wireless
networks, is growing exponentially, alongside the size and complexity of the
architectures used. Particularly, machine learning is acknowledged as one of
today's most energy-intensive computational applications, posing a significant
challenge to the environmental sustainability of next-generation intelligent
systems. Achieving environmental sustainability entails ensuring that every AI
algorithm is designed with sustainability in mind, integrating green
considerations from the architectural phase onwards. Recently, Federated
Learning (FL), with its distributed nature, presents new opportunities to
address this need. Hence, it's imperative to elucidate the potential and
challenges stemming from recent FL advancements and their implications for
sustainability. Moreover, it's crucial to furnish researchers, stakeholders,
and interested parties with a roadmap to navigate and understand existing
efforts and gaps in green-aware AI algorithms. This survey primarily aims to
achieve this objective by identifying and analyzing over a hundred FL works,
assessing their contributions to green-aware artificial intelligence for
sustainable environments, with a specific focus on IoT research. It delves into
current issues in green federated learning from an energy-efficient standpoint,
discussing potential challenges and future prospects for green IoT application
research.",2024-09-19,"Dipanwita Thakur, Antonella Guzzo, Giancarlo Fortino, Francesco Piccialli",http://arxiv.org/pdf/2409.12626v2,cs.LG
Theoretical Analysis of Heteroscedastic Gaussian Processes with Posterior Distributions,"This study introduces a novel theoretical framework for analyzing
heteroscedastic Gaussian processes (HGPs) that identify unknown systems in a
data-driven manner. Although HGPs effectively address the heteroscedasticity of
noise in complex training datasets, calculating the exact posterior
distributions of the HGPs is challenging, as these distributions are no longer
multivariate normal. This study derives the exact means, variances, and
cumulative distributions of the posterior distributions. Furthermore, the
derived theoretical findings are applied to a chance-constrained tracking
controller. After an HGP identifies an unknown disturbance in a plant system,
the controller can handle chance constraints regarding the system despite the
presence of the disturbance.",2024-09-19,Yuji Ito,http://arxiv.org/pdf/2409.12622v1,cs.LG
Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning,"Iterative human engagement is a common and effective means of leveraging the
advanced language processing power of large language models (LLMs). Using
well-structured prompts in a conversational manner, human users can effectively
influence an LLM to develop more thoughtful and accurate responses. Motivated
by this insight, we propose the Iteration of Thought (IoT) framework for
enhancing LLM responses by generating ""thought""-provoking prompts vis a vis an
input query and the current iteration of an LLM's response. Unlike static or
semi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),
IoT adapts its reasoning path dynamically, based on evolving context, and
without generating alternate explorative thoughts which are ultimately
discarded. The three components of the IoT framework are (1) an Inner Dialogue
Agent (IDA) responsible for generating instructive, context-specific prompts;
(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;
and (3) an iterative prompting loop that implements a conversation between the
former two components. We introduce two variants of our framework: Autonomous
Iteration of Thought (AIoT), where an LLM decides when to stop iterating, and
Guided Iteration of Thought (GIoT), which always forces a fixed number
iterations. We investigate the performance of IoT across various datasets,
spanning complex reasoning tasks from the GPQA dataset, explorative
problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop
question answering from the HotpotQA dataset. Our results show that IoT
represents a viable paradigm for autonomous response refinement in LLMs,
showcasing significant improvements over CoT and thereby enabling more adaptive
and efficient reasoning systems that minimize human intervention.",2024-09-19,"Santosh Kumar Radha, Yasamin Nouri Jelyani, Ara Ghukasyan, Oktay Goktas",http://arxiv.org/pdf/2409.12618v2,cs.LG
CF-GO-Net: A Universal Distribution Learner via Characteristic Function Networks with Graph Optimizers,"Generative models aim to learn the distribution of datasets, such as images,
so as to be able to generate samples that statistically resemble real data.
However, learning the underlying probability distribution can be very
challenging and intractable. To this end, we introduce an approach which
employs the characteristic function (CF), a probabilistic descriptor that
directly corresponds to the distribution. However, unlike the probability
density function (pdf), the characteristic function not only always exists, but
also provides an additional degree of freedom, hence enhances flexibility in
learning distributions. This removes the critical dependence on pdf-based
assumptions, which limit the applicability of traditional methods. While
several works have attempted to use CF in generative modeling, they often
impose strong constraints on the training process. In contrast, our approach
calculates the distance between query points in the CF domain, which is an
unconstrained and well defined problem. Next, to deal with the sampling
strategy, which is crucial to model performance, we propose a graph neural
network (GNN)-based optimizer for the sampling process, which identifies
regions where the difference between CFs is most significant. In addition, our
method allows the use of a pre-trained model, such as a well-trained
autoencoder, and is capable of learning directly in its feature space, without
modifying its parameters. This offers a flexible and robust approach to
generative modeling, not only provides broader applicability and improved
performance, but also equips any latent space world with the ability to become
a generative model.",2024-09-19,"Zeyang Yu, Shengxi Li, Danilo Mandic",http://arxiv.org/pdf/2409.12610v1,cs.LG
Hybrid Ensemble Deep Graph Temporal Clustering for Spatiotemporal Data,"Classifying subsets based on spatial and temporal features is crucial to the
analysis of spatiotemporal data given the inherent spatial and temporal
variability. Since no single clustering algorithm ensures optimal results,
researchers have increasingly explored the effectiveness of ensemble
approaches. Ensemble clustering has attracted much attention due to increased
diversity, better generalization, and overall improved clustering performance.
While ensemble clustering may yield promising results on simple datasets, it
has not been fully explored on complex multivariate spatiotemporal data. For
our contribution to this field, we propose a novel hybrid ensemble deep graph
temporal clustering (HEDGTC) method for multivariate spatiotemporal data.
HEDGTC integrates homogeneous and heterogeneous ensemble methods and adopts a
dual consensus approach to address noise and misclassification from traditional
clustering. It further applies a graph attention autoencoder network to improve
clustering performance and stability. When evaluated on three real-world
multivariate spatiotemporal data, HEDGTC outperforms state-of-the-art ensemble
clustering models by showing improved performance and stability with consistent
results. This indicates that HEDGTC can effectively capture implicit temporal
patterns in complex spatiotemporal data.",2024-09-19,"Francis Ndikum Nji, Omar Faruque, Mostafa Cham, Janeja Vandana, Jianwu Wang",http://arxiv.org/pdf/2409.12590v1,cs.LG
Is Tokenization Needed for Masked Particle Modelling?,"In this work, we significantly enhance masked particle modeling (MPM), a
self-supervised learning scheme for constructing highly expressive
representations of unordered sets relevant to developing foundation models for
high-energy physics. In MPM, a model is trained to recover the missing elements
of a set, a learning objective that requires no labels and can be applied
directly to experimental data. We achieve significant performance improvements
over previous work on MPM by addressing inefficiencies in the implementation
and incorporating a more powerful decoder. We compare several pre-training
tasks and introduce new reconstruction methods that utilize conditional
generative models without data tokenization or discretization. We show that
these new methods outperform the tokenized learning objective from the original
MPM on a new test bed for foundation models for jets, which includes using a
wide variety of downstream tasks relevant to jet physics, such as
classification, secondary vertex finding, and track identification.",2024-09-19,"Matthew Leigh, Samuel Klein, François Charton, Tobias Golling, Lukas Heinrich, Michael Kagan, Inês Ochoa, Margarita Osadchy",http://arxiv.org/pdf/2409.12589v2,cs.LG
Test-Time Augmentation Meets Variational Bayes,"Data augmentation is known to contribute significantly to the robustness of
machine learning models. In most instances, data augmentation is utilized
during the training phase. Test-Time Augmentation (TTA) is a technique that
instead leverages these data augmentations during the testing phase to achieve
robust predictions. More precisely, TTA averages the predictions of multiple
data augmentations of an instance to produce a final prediction. Although the
effectiveness of TTA has been empirically reported, it can be expected that the
predictive performance achieved will depend on the set of data augmentation
methods used during testing. In particular, the data augmentation methods
applied should make different contributions to performance. That is, it is
anticipated that there may be differing degrees of contribution in the set of
data augmentation methods used for TTA, and these could have a negative impact
on prediction performance. In this study, we consider a weighted version of the
TTA based on the contribution of each data augmentation. Some variants of TTA
can be regarded as considering the problem of determining the appropriate
weighting. We demonstrate that the determination of the coefficients of this
weighted TTA can be formalized in a variational Bayesian framework. We also
show that optimizing the weights to maximize the marginal log-likelihood
suppresses candidates of unwanted data augmentations at the test phase.",2024-09-19,"Masanari Kimura, Howard Bondell",http://arxiv.org/pdf/2409.12587v1,cs.LG
Deep Transfer Hashing for Adaptive Learning on Federated Streaming Data,"This extended abstract explores the integration of federated learning with
deep transfer hashing for distributed prediction tasks, emphasizing
resource-efficient client training from evolving data streams. Federated
learning allows multiple clients to collaboratively train a shared model while
maintaining data privacy - by incorporating deep transfer hashing,
high-dimensional data can be converted into compact hash codes, reducing data
transmission size and network loads. The proposed framework utilizes transfer
learning, pre-training deep neural networks on a central server, and
fine-tuning on clients to enhance model accuracy and adaptability. A selective
hash code sharing mechanism using a privacy-preserving global memory bank
further supports client fine-tuning. This approach addresses challenges in
previous research by improving computational efficiency and scalability.
Practical applications include Car2X event predictions, where a shared model is
collectively trained to recognize traffic patterns, aiding in tasks such as
traffic density assessment and accident detection. The research aims to develop
a robust framework that combines federated learning, deep transfer hashing and
transfer learning for efficient and secure downstream task execution.",2024-09-19,"Manuel Röder, Frank-Michael Schleif",http://arxiv.org/pdf/2409.12575v1,cs.LG
Trustworthy Intrusion Detection: Confidence Estimation Using Latent Space,"This work introduces a novel method for enhancing confidence in anomaly
detection in Intrusion Detection Systems (IDS) through the use of a Variational
Autoencoder (VAE) architecture. By developing a confidence metric derived from
latent space representations, we aim to improve the reliability of IDS
predictions against cyberattacks. Applied to the NSL-KDD dataset, our approach
focuses on binary classification tasks to effectively distinguish between
normal and malicious network activities. The methodology demonstrates a
significant enhancement in anomaly detection, evidenced by a notable
correlation of 0.45 between the reconstruction error and the proposed metric.
Our findings highlight the potential of employing VAEs for more accurate and
trustworthy anomaly detection in network security.",2024-09-19,"Ioannis Pitsiorlas, George Arvanitakis, Marios Kountouris",http://arxiv.org/pdf/2409.13774v1,cs.LG
Scaling FP8 training to trillion-token LLMs,"We train, for the first time, large language models using FP8 precision on
datasets up to 2 trillion tokens -- a 20-fold increase over previous limits.
Through these extended training runs, we uncover critical instabilities in FP8
training that were not observable in earlier works with shorter durations. We
trace these instabilities to outlier amplification by the SwiGLU activation
function. Interestingly, we show, both analytically and empirically, that this
amplification happens only over prolonged training periods, and link it to a
SwiGLU weight alignment process. To address this newly identified issue, we
introduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training
without altering function behavior. We also demonstrate, for the first time,
FP8 quantization of both Adam optimizer moments. Combining these innovations,
we successfully train a 7B parameter model using FP8 precision on 256 Intel
Gaudi2 accelerators, achieving on-par results with the BF16 baseline while
delivering up to a $\sim 34 \%$ throughput improvement. A reference
implementation is supplied in
https://github.com/Anonymous1252022/Megatron-DeepSpeed.",2024-09-19,"Maxim Fishman, Brian Chmiel, Ron Banner, Daniel Soudry",http://arxiv.org/pdf/2409.12517v2,cs.LG
DiffEditor: Enhancing Speech Editing with Semantic Enrichment and Acoustic Consistency,"As text-based speech editing becomes increasingly prevalent, the demand for
unrestricted free-text editing continues to grow. However, existing speech
editing techniques encounter significant challenges, particularly in
maintaining intelligibility and acoustic consistency when dealing with
out-of-domain (OOD) text. In this paper, we introduce, DiffEditor, a novel
speech editing model designed to enhance performance in OOD text scenarios
through semantic enrichment and acoustic consistency. To improve the
intelligibility of the edited speech, we enrich the semantic information of
phoneme embeddings by integrating word embeddings extracted from a pretrained
language model. Furthermore, we emphasize that interframe smoothing properties
are critical for modeling acoustic consistency, and thus we propose a
first-order loss function to promote smoother transitions at editing boundaries
and enhance the overall fluency of the edited speech. Experimental results
demonstrate that our model achieves state-of-the-art performance in both
in-domain and OOD text scenarios.",2024-09-19,"Yang Chen, Yuhang Jia, Shiwan Zhao, Ziyue Jiang, Haoran Li, Jiarong Kang, Yong Qin",http://arxiv.org/pdf/2409.12992v1,cs.LG
"ConvexECG: Lightweight and Explainable Neural Networks for Personalized, Continuous Cardiac Monitoring","We present ConvexECG, an explainable and resource-efficient method for
reconstructing six-lead electrocardiograms (ECG) from single-lead data, aimed
at advancing personalized and continuous cardiac monitoring. ConvexECG
leverages a convex reformulation of a two-layer ReLU neural network, enabling
the potential for efficient training and deployment in resource constrained
environments, while also having deterministic and explainable behavior. Using
data from 25 patients, we demonstrate that ConvexECG achieves accuracy
comparable to larger neural networks while significantly reducing computational
overhead, highlighting its potential for real-time, low-resource monitoring
applications.",2024-09-19,"Rayan Ansari, John Cao, Sabyasachi Bandyopadhyay, Sanjiv M. Narayan, Albert J. Rogers, Mert Pilanci",http://arxiv.org/pdf/2409.12493v1,cs.LG
CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling Acceleration in LLMs,"Large language models have achieved notable success across various domains,
yet efficient inference is still limited by the quadratic computation
complexity of the attention mechanism. The inference consists of prefilling and
decoding phases. Although several attempts have been made to accelerate
decoding, the inefficiency of the prefilling phase, especially for long-context
tasks, remains a challenge. In this paper, we observe a locality in query
criticality during the prefilling phase of long-context processing: adjacent
query tokens tend to focus on similar subsets of the past Key-Value (KV) cache.
Based on this observation, we propose CritiPrefill, a criticality-based
segment-wise prefilling method. This method partitions the input sequence's
queries and KV cache into segments and blocks, utilizing a segment-wise
algorithm to estimate the query criticality. By pruning non-critical
computations between query segments and cache blocks in the self-attention
mechanism, the prefilling process can be significantly accelerated. Extensive
evaluations on multiple long-context datasets show up to 2.7x speedup on
Llama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100
GPU, with minimal quality degradation.",2024-09-19,"Junlin Lv, Yuan Feng, Xike Xie, Xin Jia, Qirong Peng, Guiming Xie",http://arxiv.org/pdf/2409.12490v2,cs.LG
Learning Multi-Manifold Embedding for Out-Of-Distribution Detection,"Detecting out-of-distribution (OOD) samples is crucial for trustworthy AI in
real-world applications. Leveraging recent advances in representation learning
and latent embeddings, Various scoring algorithms estimate distributions beyond
the training data. However, a single embedding space falls short in
characterizing in-distribution data and defending against diverse OOD
conditions. This paper introduces a novel Multi-Manifold Embedding Learning
(MMEL) framework, optimizing hypersphere and hyperbolic spaces jointly for
enhanced OOD detection. MMEL generates representative embeddings and employs a
prototype-aware scoring function to differentiate OOD samples. It operates with
very few OOD samples and requires no model retraining. Experiments on six open
datasets demonstrate MMEL's significant reduction in FPR while maintaining a
high AUC compared to state-of-the-art distance-based OOD detection methods. We
analyze the effects of learning multiple manifolds and visualize OOD score
distributions across datasets. Notably, enrolling ten OOD samples without
retraining achieves comparable FPR and AUC to modern outlier exposure methods
using 80 million outlier samples for model training.",2024-09-19,"Jeng-Lin Li, Ming-Ching Chang, Wei-Chao Chen",http://arxiv.org/pdf/2409.12479v1,cs.LG
ViolinDiff: Enhancing Expressive Violin Synthesis with Pitch Bend Conditioning,"Modeling the natural contour of fundamental frequency (F0) plays a critical
role in music audio synthesis. However, transcribing and managing multiple F0
contours in polyphonic music is challenging, and explicit F0 contour modeling
has not yet been explored for polyphonic instrumental synthesis. In this paper,
we present ViolinDiff, a two-stage diffusion-based synthesis framework. For a
given violin MIDI file, the first stage estimates the F0 contour as pitch bend
information, and the second stage generates mel spectrogram incorporating these
expressive details. The quantitative metrics and listening test results show
that the proposed model generates more realistic violin sounds than the model
without explicit pitch bend modeling. Audio samples are available online:
daewoung.github.io/ViolinDiff-Demo.",2024-09-19,"Daewoong Kim, Hao-Wen Dong, Dasaem Jeong",http://arxiv.org/pdf/2409.12477v2,cs.LG
Familiarity-Aware Evidence Compression for Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) improves large language models (LMs) by
incorporating non-parametric knowledge through evidence retrieved from external
sources. However, it often struggles to cope with inconsistent and irrelevant
information that can distract the LM from its tasks, especially when multiple
evidence pieces are required. While compressing the retrieved evidence with a
compression model aims to address this issue, the compressed evidence may still
be unfamiliar to the target model used for downstream tasks, potentially
failing to utilize the evidence effectively. We propose FaviComp
(Familarity-Aware Evidence Compression), a novel training-free evidence
compression technique that makes retrieved evidence more familiar to the target
model, while seamlessly integrating parametric knowledge from the model.
Experimental results show that FaviComp consistently outperforms most recent
evidence compression baselines across multiple open-domain QA datasets,
improving accuracy by up to 28.1% while achieving high compression rates.
Additionally, we demonstrate the effective integration of both parametric and
non-parametric knowledge during evidence compression.",2024-09-19,"Dongwon Jung, Qin Liu, Tenghao Huang, Ben Zhou, Muhao Chen",http://arxiv.org/pdf/2409.12468v2,cs.LG
SurgPLAN++: Universal Surgical Phase Localization Network for Online and Offline Inference,"Surgical phase recognition is critical for assisting surgeons in
understanding surgical videos. Existing studies focused more on online surgical
phase recognition, by leveraging preceding frames to predict the current frame.
Despite great progress, they formulated the task as a series of frame-wise
classification, which resulted in a lack of global context of the entire
procedure and incoherent predictions. Moreover, besides online analysis,
accurate offline surgical phase recognition is also in significant clinical
need for retrospective analysis, and existing online algorithms do not fully
analyze the entire video, thereby limiting accuracy in offline analysis. To
overcome these challenges and enhance both online and offline inference
capabilities, we propose a universal Surgical Phase Localization Network, named
SurgPLAN++, with the principle of temporal detection. To ensure a global
understanding of the surgical procedure, we devise a phase localization
strategy for SurgPLAN++ to predict phase segments across the entire video
through phase proposals. For online analysis, to generate high-quality phase
proposals, SurgPLAN++ incorporates a data augmentation strategy to extend the
streaming video into a pseudo-complete video through mirroring,
center-duplication, and down-sampling. For offline analysis, SurgPLAN++
capitalizes on its global phase prediction framework to continuously refine
preceding predictions during each online inference step, thereby significantly
improving the accuracy of phase recognition. We perform extensive experiments
to validate the effectiveness, and our SurgPLAN++ achieves remarkable
performance in both online and offline modes, which outperforms
state-of-the-art methods. The source code is available at
https://github.com/franciszchen/SurgPLAN-Plus.",2024-09-19,"Zhen Chen, Xingjian Luo, Jinlin Wu, Long Bai, Zhen Lei, Hongliang Ren, Sebastien Ourselin, Hongbin Liu",http://arxiv.org/pdf/2409.12467v2,cs.LG
Unsupervised Reward-Driven Image Segmentation in Automated Scanning Transmission Electron Microscopy Experiments,"Automated experiments in scanning transmission electron microscopy (STEM)
require rapid image segmentation to optimize data representation for human
interpretation, decision-making, site-selective spectroscopies, and atomic
manipulation. Currently, segmentation tasks are typically performed using
supervised machine learning methods, which require human-labeled data and are
sensitive to out-of-distribution drift effects caused by changes in resolution,
sampling, or beam shape. Here, we operationalize and benchmark a recently
proposed reward-driven optimization workflow for on-the fly image analysis in
STEM. This unsupervised approach is much more robust, as it does not rely on
human labels and is fully explainable. The explanatory feedback can help the
human to verify the decision making and potentially tune the model by selecting
the position along the Pareto frontier of reward functions. We establish the
timing and effectiveness of this method, demonstrating its capability for
real-time performance in high-throughput and dynamic automated STEM
experiments. The reward driven approach allows to construct explainable robust
analysis workflows and can be generalized to a broad range of image analysis
tasks in electron and scanning probe microscopy and chemical imaging.",2024-09-19,"Kamyar Barakati, Utkarsh Pratiush, Austin C. Houston, Gerd Duscher, Sergei V. Kalinin",http://arxiv.org/pdf/2409.12462v2,cs.LG
FoME: A Foundation Model for EEG using Adaptive Temporal-Lateral Attention Scaling,"Electroencephalography (EEG) is a vital tool to measure and record brain
activity in neuroscience and clinical applications, yet its potential is
constrained by signal heterogeneity, low signal-to-noise ratios, and limited
labeled datasets. In this paper, we propose FoME (Foundation Model for EEG), a
novel approach using adaptive temporal-lateral attention scaling to address
above-mentioned challenges. FoME is pre-trained on a diverse 1.7TB dataset of
scalp and intracranial EEG recordings, comprising 745M parameters trained for
1,096k steps. Our model introduces two key innovations: a time-frequency fusion
embedding technique and an adaptive time-lateral attention scaling (ATLAS)
mechanism. These components synergistically capture complex temporal and
spectral EEG dynamics, enabling FoME to adapt to varying patterns across
diverse data streams and facilitate robust multi-channel modeling. Evaluations
across four downstream tasks demonstrate FoME's superior performance in
classification and forecasting applications, consistently achieving
state-of-the-art results. To conclude, FoME establishes a new paradigm for EEG
analysis, offering a versatile foundation that advances brain-computer
interfaces, clinical diagnostics, and cognitive research across neuroscience
and related fields. Our code will be available at
https://github.com/1061413241/FoME.",2024-09-19,"Enze Shi, Kui Zhao, Qilong Yuan, Jiaqi Wang, Huawen Hu, Sigang Yu, Shu Zhang",http://arxiv.org/pdf/2409.12454v1,cs.LG
Neural Networks Generalize on Low Complexity Data,"We show that feedforward neural networks with ReLU activation generalize on
low complexity data, suitably defined. Given i.i.d. data generated from a
simple programming language, the minimum description length (MDL) feedforward
neural network which interpolates the data generalizes with high probability.
We define this simple programming language, along with a notion of description
length of such networks. We provide several examples on basic computational
tasks, such as checking primality of a natural number, and more. For primality
testing, our theorem shows the following. Suppose that we draw an i.i.d. sample
of $\Theta(N^{\delta}\ln N)$ numbers uniformly at random from $1$ to $N$, where
$\delta\in (0,1)$. For each number $x_i$, let $y_i = 1$ if $x_i$ is a prime and
$0$ if it is not. Then with high probability, the MDL network fitted to this
data accurately answers whether a newly drawn number between $1$ and $N$ is a
prime or not, with test error $\leq O(N^{-\delta})$. Note that the network is
not designed to detect primes; minimum description learning discovers a network
which does so.",2024-09-19,"Sourav Chatterjee, Timothy Sudijono",http://arxiv.org/pdf/2409.12446v2,cs.LG
Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data,"Despite recent advances in training and prompting strategies for Large
Language Models (LLMs), these models continue to face challenges with complex
logical reasoning tasks that involve long reasoning chains. In this work, we
explore the potential and limitations of using graph-based synthetic reasoning
data as training signals to enhance LLMs' reasoning capabilities. Our extensive
experiments, conducted on two established natural language reasoning tasks --
inductive reasoning and spatial reasoning -- demonstrate that supervised
fine-tuning (SFT) with synthetic graph-based reasoning data effectively
enhances LLMs' reasoning performance without compromising their effectiveness
on other standard evaluation benchmarks.",2024-09-19,"Jiaming Zhou, Abbas Ghaddar, Ge Zhang, Liheng Ma, Yaochen Hu, Soumyasundar Pal, Mark Coates, Bin Wang, Yingxue Zhang, Jianye Hao",http://arxiv.org/pdf/2409.12437v2,cs.LG
Optimizing food taste sensory evaluation through neural network-based taste electroencephalogram channel selection,"The taste electroencephalogram (EEG) evoked by the taste stimulation can
reflect different brain patterns and be used in applications such as sensory
evaluation of food. However, considering the computational cost and efficiency,
EEG data with many channels has to face the critical issue of channel
selection. This paper proposed a channel selection method called class
activation mapping with attention (CAM-Attention). The CAM-Attention method
combined a convolutional neural network with channel and spatial attention
(CNN-CSA) model with a gradient-weighted class activation mapping (Grad-CAM)
model. The CNN-CSA model exploited key features in EEG data by attention
mechanism, and the Grad-CAM model effectively realized the visualization of
feature regions. Then, channel selection was effectively implemented based on
feature regions. Finally, the CAM-Attention method reduced the computational
burden of taste EEG recognition and effectively distinguished the four tastes.
In short, it has excellent recognition performance and provides effective
technical support for taste sensory evaluation.",2024-09-19,"Xiuxin Xia, Qun Wang, He Wang, Chenrui Liu, Pengwei Li, Yan Shi, Hong Men",http://arxiv.org/pdf/2410.03559v1,cs.LG
Is it Still Fair? A Comparative Evaluation of Fairness Algorithms through the Lens of Covariate Drift,"Over the last few decades, machine learning (ML) applications have grown
exponentially, yielding several benefits to society. However, these benefits
are tempered with concerns of discriminatory behaviours exhibited by ML models.
In this regard, fairness in machine learning has emerged as a priority research
area. Consequently, several fairness metrics and algorithms have been developed
to mitigate against discriminatory behaviours that ML models may possess. Yet
still, very little attention has been paid to the problem of naturally
occurring changes in data patterns (\textit{aka} data distributional drift),
and its impact on fairness algorithms and metrics. In this work, we study this
problem comprehensively by analyzing 4 fairness-unaware baseline algorithms and
7 fairness-aware algorithms, carefully curated to cover the breadth of its
typology, across 5 datasets including public and proprietary data, and
evaluated them using 3 predictive performance and 10 fairness metrics. In doing
so, we show that (1) data distributional drift is not a trivial occurrence, and
in several cases can lead to serious deterioration of fairness in so-called
fair models; (2) contrary to some existing literature, the size and direction
of data distributional drift is not correlated to the resulting size and
direction of unfairness; and (3) choice of, and training of fairness algorithms
is impacted by the effect of data distributional drift which is largely ignored
in the literature. Emanating from our findings, we synthesize several policy
implications of data distributional drift on fairness algorithms that can be
very relevant to stakeholders and practitioners.",2024-09-19,"Oscar Blessed Deho, Michael Bewong, Selasi Kwashie, Jiuyong Li, Jixue Liu, Lin Liu, Srecko Joksimovic",http://arxiv.org/pdf/2409.12428v1,cs.LG
Sustainable Visions: Unsupervised Machine Learning Insights on Global Development Goals,"The 2030 Agenda for Sustainable Development of the United Nations outlines 17
goals for countries of the world to address global challenges in their
development. However, the progress of countries towards these goal has been
slower than expected and, consequently, there is a need to investigate the
reasons behind this fact. In this study, we have used a novel data-driven
methodology to analyze time-series data for over 20 years (2000-2022) from 107
countries using unsupervised machine learning (ML) techniques. Our analysis
reveals strong positive and negative correlations between certain SDGs
(Sustainable Development Goals). Our findings show that progress toward the
SDGs is heavily influenced by geographical, cultural and socioeconomic factors,
with no country on track to achieve all the goals by 2030. This highlights the
need for a region-specific, systemic approach to sustainable development that
acknowledges the complex interdependencies between the goals and the variable
capacities of countries to reach them. For this our machine learning based
approach provides a robust framework for developing efficient and data-informed
strategies to promote cooperative and targeted initiatives for sustainable
progress.",2024-09-19,"Alberto García-Rodríguez, Matias Núñez, Miguel Robles Pérez, Tzipe Govezensky, Rafael A. Barrio, Carlos Gershenson, Kimmo K. Kaski, Julia Tagüeña",http://arxiv.org/pdf/2409.12427v2,cs.LG
Zero-to-Strong Generalization: Eliciting Strong Capabilities of Large Language Models Iteratively without Gold Labels,"Large Language Models (LLMs) have demonstrated remarkable performance through
supervised fine-tuning or in-context learning using gold labels. However, this
paradigm is limited by the availability of gold labels, while in certain
scenarios, LLMs may need to perform tasks that are too complex for humans to
provide such labels. To tackle this challenge, this study explores whether
solely utilizing unlabeled data can elicit strong model capabilities. We
propose a new paradigm termed zero-to-strong generalization. We iteratively
prompt LLMs to annotate unlabeled data and retain high-quality labels by
filtering. Surprisingly, we obverse that this iterative process gradually
unlocks LLMs' potential on downstream tasks. Our experiments on extensive
classification and reasoning tasks confirm the effectiveness of our proposed
framework. Our analysis indicates that this paradigm is effective for both
in-context learning and fine-tuning, and for various model sizes.",2024-09-19,"Chaoqun Liu, Qin Chao, Wenxuan Zhang, Xiaobao Wu, Boyang Li, Anh Tuan Luu, Lidong Bing",http://arxiv.org/pdf/2409.12425v1,cs.LG
From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal Reasoning with Large Language Models,"Cross-modal reasoning (CMR), the intricate process of synthesizing and
drawing inferences across divergent sensory modalities, is increasingly
recognized as a crucial capability in the progression toward more sophisticated
and anthropomorphic artificial intelligence systems. Large Language Models
(LLMs) represent a class of AI algorithms specifically engineered to parse,
produce, and engage with human language on an extensive scale. The recent trend
of deploying LLMs to tackle CMR tasks has marked a new mainstream of approaches
for enhancing their effectiveness. This survey offers a nuanced exposition of
current methodologies applied in CMR using LLMs, classifying these into a
detailed three-tiered taxonomy. Moreover, the survey delves into the principal
design strategies and operational techniques of prototypical models within this
domain. Additionally, it articulates the prevailing challenges associated with
the integration of LLMs in CMR and identifies prospective research directions.
To sum up, this survey endeavors to expedite progress within this burgeoning
field by endowing scholars with a holistic and detailed vista, showcasing the
vanguard of current research whilst pinpointing potential avenues for
advancement. An associated GitHub repository that collects the relevant papers
can be found at
https://github.com/ZuyiZhou/Awesome-Cross-modal-Reasoning-with-LLMs",2024-09-19,"Shengsheng Qian, Zuyi Zhou, Dizhan Xue, Bing Wang, Changsheng Xu",http://arxiv.org/pdf/2409.18996v1,cs.LG
Smirk: An Atomically Complete Tokenizer for Molecular Foundation Models,"Text-based foundation models have become an important part of scientific
discovery, with molecular foundation models accelerating advancements in
molecular design and materials science. However, existing models are
constrained by closed-vocabulary tokenizers which capture only a fraction of
molecular space. In this work, we systematically evaluate thirty tokenizers,
including 19 chemistry-specific ones, for their coverage of the SMILES
molecular representation language, revealing significant gaps. To assess the
impact of tokenizer choice, we introduce n-gram language models as a low-cost
proxy and validate their effectiveness by training and fine-tuning 18
RoBERTa-style encoders for molecular property prediction. To overcome the
limitations of existing tokenizers, we propose two new tokenizers -- Smirk and
Smirk-GPE -- with full coverage of the OpenSMILES specification. Our results
highlight the need for open-vocabulary modeling and chemically diverse
benchmarks in cheminformatics. The proposed tokenizer framework systematically
integrates nuclear, electronic, and geometric degrees of freedom; this
facilitates applications in pharmacology, agriculture, biology, and energy
storage.",2024-09-19,"Alexius Wadell, Anoushka Bhutani, Venkatasubramanian Viswanathan",http://arxiv.org/pdf/2409.15370v2,cs.LG
How to predict on-road air pollution based on street view images and machine learning: a quantitative analysis of the optimal strategy,"On-road air pollution exhibits substantial variability over short distances
due to emission sources, dilution, and physicochemical processes. Integrating
mobile monitoring data with street view images (SVIs) holds promise for
predicting local air pollution. However, algorithms, sampling strategies, and
image quality introduce extra errors due to a lack of reliable references that
quantify their effects. To bridge this gap, we employed 314 taxis to monitor
NO, NO2, PM2.5 and PM10 dynamically and sampled corresponding SVIs, aiming to
develop a reliable strategy. We extracted SVI features from ~ 382,000
streetscape images, which were collected at various angles (0{\deg}, 90{\deg},
180{\deg}, 270{\deg}) and ranges (buffers with radii of 100m, 200m, 300m, 400m,
500m). Also, three machine learning algorithms alongside the linear land-used
regression (LUR) model were experimented with to explore the influences of
different algorithms. Four typical image quality issues were identified and
discussed. Generally, machine learning methods outperform linear LUR for
estimating the four pollutants, with the ranking: random forest > XGBoost >
neural network > LUR. Compared to single-angle sampling, the averaging strategy
is an effective method to avoid bias of insufficient feature capture.
Therefore, the optimal sampling strategy is to obtain SVIs at a 100m radius
buffer and extract features using the averaging strategy. This approach
achieved estimation results for each aggregation location with absolute errors
almost less than 2.5 {\mu}g/m^2 or ppb. Overexposure, blur, and underexposure
led to image misjudgments and incorrect identifications, causing an
overestimation of road features and underestimation of human-activity features,
contributing to inaccurate NO, NO2, PM2.5 and PM10 estimation.",2024-09-19,"Hui Zhong, Di Chen, Pengqin Wang, Wenrui Wang, Shaojie Shen, Yonghong Liu, Meixin Zhu",http://arxiv.org/pdf/2409.12412v1,cs.LG
LMT-Net: Lane Model Transformer Network for Automated HD Mapping from Sparse Vehicle Observations,"In autonomous driving, High Definition (HD) maps provide a complete lane
model that is not limited by sensor range and occlusions. However, the
generation and upkeep of HD maps involves periodic data collection and human
annotations, limiting scalability. To address this, we investigate automating
the lane model generation and the use of sparse vehicle observations instead of
dense sensor measurements. For our approach, a pre-processing step generates
polylines by aligning and aggregating observed lane boundaries. Aligned driven
traces are used as starting points for predicting lane pairs defined by the
left and right boundary points. We propose Lane Model Transformer Network
(LMT-Net), an encoder-decoder neural network architecture that performs
polyline encoding and predicts lane pairs and their connectivity. A lane graph
is formed by using predicted lane pairs as nodes and predicted lane
connectivity as edges. We evaluate the performance of LMT-Net on an internal
dataset that consists of multiple vehicle observations as well as human
annotations as Ground Truth (GT). The evaluation shows promising results and
demonstrates superior performance compared to the implemented baseline on both
highway and non-highway Operational Design Domain (ODD).",2024-09-19,"Michael Mink, Thomas Monninger, Steffen Staab",http://arxiv.org/pdf/2409.12409v1,cs.LG
Shape-informed surrogate models based on signed distance function domain encoding,"We propose a non-intrusive method to build surrogate models that approximate
the solution of parameterized partial differential equations (PDEs), capable of
taking into account the dependence of the solution on the shape of the
computational domain. Our approach is based on the combination of two neural
networks (NNs). The first NN, conditioned on a latent code, provides an
implicit representation of geometry variability through signed distance
functions. This automated shape encoding technique generates compact,
low-dimensional representations of geometries within a latent space, without
requiring the explicit construction of an encoder. The second NN reconstructs
the output physical fields independently for each spatial point, thus avoiding
the computational burden typically associated with high-dimensional
discretizations like computational meshes. Furthermore, we show that accuracy
in geometrical characterization can be further enhanced by employing Fourier
feature mapping as input feature of the NN. The meshless nature of the proposed
method, combined with the dimensionality reduction achieved through automatic
feature extraction in latent space, makes it highly flexible and
computationally efficient. This strategy eliminates the need for manual
intervention in extracting geometric parameters, and can even be applied in
cases where geometries undergo changes in their topology. Numerical tests in
the field of fluid dynamics and solid mechanics demonstrate the effectiveness
of the proposed method in accurately predict the solution of PDEs in domains of
arbitrary shape. Remarkably, the results show that it achieves accuracy
comparable to the best-case scenarios where an explicit parametrization of the
computational domain is available.",2024-09-19,"Linying Zhang, Stefano Pagani, Jun Zhang, Francesco Regazzoni",http://arxiv.org/pdf/2409.12400v1,cs.LG
Selecting a classification performance measure: matching the measure to the problem,"The problem of identifying to which of a given set of classes objects belong
is ubiquitous, occurring in many research domains and application areas,
including medical diagnosis, financial decision making, online commerce, and
national security. But such assignments are rarely completely perfect, and
classification errors occur. This means it is necessary to compare
classification methods and algorithms to decide which is ``best'' for any
particular problem. However, just as there are many different classification
methods, so there are many different ways of measuring their performance. It is
thus vital to choose a measure of performance which matches the aims of the
research or application. This paper is a contribution to the growing literature
on the relative merits of different performance measures. Its particular focus
is the critical importance of matching the properties of the measure to the
aims for which the classification is being made.",2024-09-19,"David J. Hand, Peter Christen, Sumayya Ziyad",http://arxiv.org/pdf/2409.12391v1,cs.LG
On the Regret of Coded Caching with Adversarial Requests,"We study the well-known coded caching problem in an online learning
framework, wherein requests arrive sequentially, and an online policy can
update the cache contents based on the history of requests seen thus far. We
introduce a caching policy based on the Follow-The-Perturbed-Leader principle
and show that for any time horizon T and any request sequence, it achieves a
sub-linear regret of \mathcal{O}(\sqrt(T) ) with respect to an oracle that
knows the request sequence beforehand. Our study marks the first examination of
adversarial regret in the coded caching setup. Furthermore, we also address the
issue of switching cost by establishing an upper bound on the expected number
of cache updates made by our algorithm under unrestricted switching and also
provide an upper bound on the regret under restricted switching when cache
updates can only happen in a pre-specified subset of timeslots. Finally, we
validate our theoretical insights with numerical results using a real-world
dataset",2024-09-19,"Anupam Nayak, Kota Srinivas Reddy, Nikhil Karamchandani",http://arxiv.org/pdf/2409.12387v1,cs.LG
Look Through Masks: Towards Masked Face Recognition with De-Occlusion Distillation,"Many real-world applications today like video surveillance and urban
governance need to address the recognition of masked faces, where content
replacement by diverse masks often brings in incomplete appearance and
ambiguous representation, leading to a sharp drop in accuracy. Inspired by
recent progress on amodal perception, we propose to migrate the mechanism of
amodal completion for the task of masked face recognition with an end-to-end
de-occlusion distillation framework, which consists of two modules. The
\textit{de-occlusion} module applies a generative adversarial network to
perform face completion, which recovers the content under the mask and
eliminates appearance ambiguity. The \textit{distillation} module takes a
pre-trained general face recognition model as the teacher and transfers its
knowledge to train a student for completed faces using massive online
synthesized face pairs. Especially, the teacher knowledge is represented with
structural relations among instances in multiple orders, which serves as a
posterior regularization to enable the adaptation. In this way, the knowledge
can be fully distilled and transferred to identify masked faces. Experiments on
synthetic and realistic datasets show the efficacy of the proposed approach.",2024-09-19,"Chenyu Li, Shiming Ge, Daichi Zhang, Jia Li",http://arxiv.org/pdf/2409.12385v1,cs.LG
Privacy-Preserving Student Learning with Differentially Private Data-Free Distillation,"Deep learning models can achieve high inference accuracy by extracting rich
knowledge from massive well-annotated data, but may pose the risk of data
privacy leakage in practical deployment. In this paper, we present an effective
teacher-student learning approach to train privacy-preserving deep learning
models via differentially private data-free distillation. The main idea is
generating synthetic data to learn a student that can mimic the ability of a
teacher well-trained on private data. In the approach, a generator is first
pretrained in a data-free manner by incorporating the teacher as a fixed
discriminator. With the generator, massive synthetic data can be generated for
model training without exposing data privacy. Then, the synthetic data is fed
into the teacher to generate private labels. Towards this end, we propose a
label differential privacy algorithm termed selective randomized response to
protect the label information. Finally, a student is trained on the synthetic
data with the supervision of private labels. In this way, both data privacy and
label privacy are well protected in a unified framework, leading to
privacy-preserving models. Extensive experiments and analysis clearly
demonstrate the effectiveness of our approach.",2024-09-19,"Bochao Liu, Jianghu Lu, Pengju Wang, Junjie Zhang, Dan Zeng, Zhenxing Qian, Shiming Ge",http://arxiv.org/pdf/2409.12384v1,cs.LG
Prediction of Brent crude oil price based on LSTM model under the background of low-carbon transition,"In the field of global energy and environment, crude oil is an important
strategic resource, and its price fluctuation has a far-reaching impact on the
global economy, financial market and the process of low-carbon development. In
recent years, with the gradual promotion of green energy transformation and
low-carbon development in various countries, the dynamics of crude oil market
have become more complicated and changeable. The price of crude oil is not only
influenced by traditional factors such as supply and demand, geopolitical
conflict and production technology, but also faces the challenges of energy
policy transformation, carbon emission control and new energy technology
development. This diversified driving factor makes the prediction of crude oil
price not only very important in economic decision-making and energy planning,
but also a key issue in financial markets.In this paper, the spot price data of
European Brent crude oil provided by us energy information administration are
selected, and a deep learning model with three layers of LSTM units is
constructed to predict the crude oil price in the next few days. The results
show that the LSTM model performs well in capturing the overall price trend,
although there is some deviation during the period of sharp price fluctuation.
The research in this paper not only verifies the applicability of LSTM model in
energy market forecasting, but also provides data support for policy makers and
investors when facing the uncertainty of crude oil price.",2024-09-19,"Yuwen Zhao, Baojun Hu, Sizhe Wang",http://arxiv.org/pdf/2409.12376v1,cs.LG
Communication-Efficient Federated Low-Rank Update Algorithm and its Connection to Implicit Regularization,"Federated Learning (FL) faces significant challenges related to communication
efficiency and heterogeneity. To address these issues, we explore the potential
of using low-rank updates. Our theoretical analysis reveals that client's loss
exhibits a higher rank structure (gradients span higher rank subspace of
Hessian) compared to the server's loss. Based on this insight, we hypothesize
that constraining client-side optimization to a low-rank subspace could provide
an implicit regularization effect. Consequently, we propose FedLoRU, a general
low-rank update framework for federated learning. Our framework enforces
low-rank client-side updates and accumulates these updates to form a
higher-rank model. Additionally, variants of FedLoRU can adapt to environments
with statistical and model heterogeneity by employing multiple or hierarchical
low-rank updates. Experimental results demonstrate that FedLoRU performs
comparably to full-rank algorithms and exhibits robustness to heterogeneous and
large numbers of clients.",2024-09-19,"Haemin Park, Diego Klabjan",http://arxiv.org/pdf/2409.12371v1,cs.LG
Extracting Memorized Training Data via Decomposition,"The widespread use of Large Language Models (LLMs) in society creates new
information security challenges for developers, organizations, and end-users
alike. LLMs are trained on large volumes of data, and their susceptibility to
reveal the exact contents of the source training datasets poses security and
safety risks. Although current alignment procedures restrict common risky
behaviors, they do not completely prevent LLMs from leaking data. Prior work
demonstrated that LLMs may be tricked into divulging training data by using
out-of-distribution queries or adversarial techniques. In this paper, we
demonstrate a simple, query-based decompositional method to extract news
articles from two frontier LLMs. We use instruction decomposition techniques to
incrementally extract fragments of training data. Out of 3723 New York Times
articles, we extract at least one verbatim sentence from 73 articles, and over
20% of verbatim sentences from 6 articles. Our analysis demonstrates that this
method successfully induces the LLM to generate texts that are reliable
reproductions of news articles, meaning that they likely originate from the
source training dataset. This method is simple, generalizable, and does not
fine-tune or change the production model. If replicable at scale, this training
data extraction methodology could expose new LLM security and safety
vulnerabilities, including privacy risks and unauthorized data leaks. These
implications require careful consideration from model development to its
end-use.",2024-09-18,"Ellen Su, Anu Vellore, Amy Chang, Raffaele Mura, Blaine Nelson, Paul Kassianik, Amin Karbasi",http://arxiv.org/pdf/2409.12367v2,cs.LG
Axial Attention Transformer Networks: A New Frontier in Breast Cancer Detection,"This paper delves into the challenges and advancements in the field of
medical image segmentation, particularly focusing on breast cancer diagnosis.
The authors propose a novel Transformer-based segmentation model that addresses
the limitations of traditional convolutional neural networks (CNNs), such as
U-Net, in accurately localizing and segmenting small lesions within breast
cancer images. The model introduces an axial attention mechanism to enhance the
computational efficiency and address the issue of global contextual information
that is often overlooked by CNNs. Additionally, the paper discusses
improvements tailored to the small dataset challenge, including the
incorporation of relative position information and a gated axial attention
mechanism to refine the model's focus on relevant features. The proposed model
aims to significantly improve the segmentation accuracy of breast cancer
images, offering a more efficient and effective tool for computer-aided
diagnosis.",2024-09-18,"Weijie He, Runyuan Bao, Yiru Cang, Jianjun Wei, Yang Zhang, Jiacheng Hu",http://arxiv.org/pdf/2409.12347v1,cs.LG
Bridging the Gap Between Approximation and Learning via Optimal Approximation by ReLU MLPs of Maximal Regularity,"The foundations of deep learning are supported by the seemingly opposing
perspectives of approximation or learning theory. The former advocates for
large/expressive models that need not generalize, while the latter considers
classes that generalize but may be too small/constrained to be universal
approximators. Motivated by real-world deep learning implementations that are
both expressive and statistically reliable, we ask: ""Is there a class of neural
networks that is both large enough to be universal but structured enough to
generalize?""
  This paper constructively provides a positive answer to this question by
identifying a highly structured class of ReLU multilayer perceptions (MLPs),
which are optimal function approximators and are statistically well-behaved. We
show that any $L$-Lipschitz function from $[0,1]^d$ to $[-n,n]$ can be
approximated to a uniform $Ld/(2n)$ error on $[0,1]^d$ with a sparsely
connected $L$-Lipschitz ReLU MLP of width $\mathcal{O}(dn^d)$, depth
$\mathcal{O}(\log(d))$, with $\mathcal{O}(dn^d)$ nonzero parameters, and whose
weights and biases take values in $\{0,\pm 1/2\}$ except in the first and last
layers which instead have magnitude at-most $n$. Unlike previously known
""large"" classes of universal ReLU MLPs, the empirical Rademacher complexity of
our class remains bounded even when its depth and width become arbitrarily
large. Further, our class of MLPs achieves a near-optimal sample complexity of
$\mathcal{O}(\log(N)/\sqrt{N})$ when given $N$ i.i.d. normalized sub-Gaussian
training samples.
  We achieve this by avoiding the standard approach to constructing optimal
ReLU approximators, which sacrifices regularity by relying on small spikes.
Instead, we introduce a new construction that perfectly fits together linear
pieces using Kuhn triangulations and avoids these small spikes.",2024-09-18,"Ruiyang Hong, Anastasis Kratsios",http://arxiv.org/pdf/2409.12335v1,cs.LG
Deep vessel segmentation with joint multi-prior encoding,"The precise delineation of blood vessels in medical images is critical for
many clinical applications, including pathology detection and surgical
planning. However, fully-automated vascular segmentation is challenging because
of the variability in shape, size, and topology. Manual segmentation remains
the gold standard but is time-consuming, subjective, and impractical for
large-scale studies. Hence, there is a need for automatic and reliable
segmentation methods that can accurately detect blood vessels from medical
images. The integration of shape and topological priors into vessel
segmentation models has been shown to improve segmentation accuracy by offering
contextual information about the shape of the blood vessels and their spatial
relationships within the vascular tree. To further improve anatomical
consistency, we propose a new joint prior encoding mechanism which incorporates
both shape and topology in a single latent space. The effectiveness of our
method is demonstrated on the publicly available 3D-IRCADb dataset. More
globally, the proposed approach holds promise in overcoming the challenges
associated with automatic vessel delineation and has the potential to advance
the field of deep priors encoding.",2024-09-18,"Amine Sadikine, Bogdan Badic, Enzo Ferrante, Vincent Noblet, Pascal Ballet, Dimitris Visvikis, Pierre-Henri Conze",http://arxiv.org/pdf/2409.12334v1,cs.LG
Scale-specific auxiliary multi-task contrastive learning for deep liver vessel segmentation,"Extracting hepatic vessels from abdominal images is of high interest for
clinicians since it allows to divide the liver into functionally-independent
Couinaud segments. In this respect, an automated liver blood vessel extraction
is widely summoned. Despite the significant growth in performance of semantic
segmentation methodologies, preserving the complex multi-scale geometry of main
vessels and ramifications remains a major challenge. This paper provides a new
deep supervised approach for vessel segmentation, with a strong focus on
representations arising from the different scales inherent to the vascular tree
geometry. In particular, we propose a new clustering technique to decompose the
tree into various scale levels, from tiny to large vessels. Then, we extend
standard 3D UNet to multi-task learning by incorporating scale-specific
auxiliary tasks and contrastive learning to encourage the discrimination
between scales in the shared representation. Promising results, depicted in
several evaluation metrics, are revealed on the public 3D-IRCADb dataset.",2024-09-18,"Amine Sadikine, Bogdan Badic, Jean-Pierre Tasu, Vincent Noblet, Pascal Ballet, Dimitris Visvikis, Pierre-Henri Conze",http://arxiv.org/pdf/2409.12333v1,cs.LG
Geometric Relational Embeddings,"Relational representation learning transforms relational data into continuous
and low-dimensional vector representations. However, vector-based
representations fall short in capturing crucial properties of relational data
that are complex and symbolic. We propose geometric relational embeddings, a
paradigm of relational embeddings that respect the underlying symbolic
structures. Specifically, this dissertation introduces various geometric
relational embedding models capable of capturing: 1) complex structured
patterns like hierarchies and cycles in networks and knowledge graphs; 2)
logical structures in ontologies and logical constraints applicable for
constraining machine learning model outputs; and 3) high-order structures
between entities and relations. Our results obtained from benchmark and
real-world datasets demonstrate the efficacy of geometric relational embeddings
in adeptly capturing these discrete, symbolic, and structured properties
inherent in relational data.",2024-09-18,Bo Xiong,http://arxiv.org/pdf/2409.15369v1,cs.LG
SplitVAEs: Decentralized scenario generation from siloed data for stochastic optimization problems,"Stochastic optimization problems in large-scale multi-stakeholder networked
systems (e.g., power grids and supply chains) rely on data-driven scenarios to
encapsulate complex spatiotemporal interdependencies. However, centralized
aggregation of stakeholder data is challenging due to the existence of data
silos resulting from computational and logistical bottlenecks. In this paper,
we present SplitVAEs, a decentralized scenario generation framework that
leverages variational autoencoders to generate high-quality scenarios without
moving stakeholder data. With the help of experiments on distributed memory
systems, we demonstrate the broad applicability of SplitVAEs in a variety of
domain areas that are dominated by a large number of stakeholders. Our
experiments indicate that SplitVAEs can learn spatial and temporal
interdependencies in large-scale networks to generate scenarios that match the
joint historical distribution of stakeholder data in a decentralized manner.
Our experiments show that SplitVAEs deliver robust performance compared to
centralized, state-of-the-art benchmark methods while significantly reducing
data transmission costs, leading to a scalable, privacy-enhancing alternative
to scenario generation.",2024-09-18,"H M Mohaimanul Islam, Huynh Q. N. Vo, Paritosh Ramanan",http://arxiv.org/pdf/2409.12328v2,cs.LG
Understanding Implosion in Text-to-Image Generative Models,"Recent works show that text-to-image generative models are surprisingly
vulnerable to a variety of poisoning attacks. Empirical results find that these
models can be corrupted by altering associations between individual text
prompts and associated visual features. Furthermore, a number of concurrent
poisoning attacks can induce ""model implosion,"" where the model becomes unable
to produce meaningful images for unpoisoned prompts. These intriguing findings
highlight the absence of an intuitive framework to understand poisoning attacks
on these models. In this work, we establish the first analytical framework on
robustness of image generative models to poisoning attacks, by modeling and
analyzing the behavior of the cross-attention mechanism in latent diffusion
models. We model cross-attention training as an abstract problem of ""supervised
graph alignment"" and formally quantify the impact of training data by the
hardness of alignment, measured by an Alignment Difficulty (AD) metric. The
higher the AD, the harder the alignment. We prove that AD increases with the
number of individual prompts (or concepts) poisoned. As AD grows, the alignment
task becomes increasingly difficult, yielding highly distorted outcomes that
frequently map meaningful text prompts to undefined or meaningless visual
representations. As a result, the generative model implodes and outputs random,
incoherent images at large. We validate our analytical framework through
extensive experiments, and we confirm and explain the unexpected (and
unexplained) effect of model implosion while producing new, unforeseen
insights. Our work provides a useful tool for studying poisoning attacks
against diffusion models and their defenses.",2024-09-18,"Wenxin Ding, Cathy Y. Li, Shawn Shan, Ben Y. Zhao, Haitao Zheng",http://arxiv.org/pdf/2409.12314v1,cs.LG
Amortized Variational Inference for Deep Gaussian Processes,"Gaussian processes (GPs) are Bayesian nonparametric models for function
approximation with principled predictive uncertainty estimates. Deep Gaussian
processes (DGPs) are multilayer generalizations of GPs that can represent
complex marginal densities as well as complex mappings. As exact inference is
either computationally prohibitive or analytically intractable in GPs and
extensions thereof, some existing methods resort to variational inference (VI)
techniques for tractable approximations. However, the expressivity of
conventional approximate GP models critically relies on independent inducing
variables that might not be informative enough for some problems. In this work
we introduce amortized variational inference for DGPs, which learns an
inference function that maps each observation to variational parameters. The
resulting method enjoys a more expressive prior conditioned on fewer input
dependent inducing variables and a flexible amortized marginal posterior that
is able to model more complicated functions. We show with theoretical reasoning
and experimental results that our method performs similarly or better than
previous approaches at less computational cost.",2024-09-18,"Qiuxian Meng, Yongyou Zhang",http://arxiv.org/pdf/2409.12301v1,cs.LG
JKO for Landau: a variational particle method for homogeneous Landau equation,"Inspired by the gradient flow viewpoint of the Landau equation and
corresponding dynamic formulation of the Landau metric in [arXiv:2007.08591],
we develop a novel implicit particle method for the Landau equation in the
framework of the JKO scheme. We first reformulate the Landau metric in a
computationally friendly form, and then translate it into the Lagrangian
viewpoint using the flow map. A key observation is that, while the flow map
evolves according to a rather complicated integral equation, the unknown
component is merely a score function of the corresponding density plus an
additional term in the null space of the collision kernel. This insight guides
us in designing and training the neural network for the flow map. Additionally,
the objective function is in a double summation form, making it highly suitable
for stochastic methods. Consequently, we design a tailored version of
stochastic gradient descent that maintains particle interactions and
significantly reduces the computational complexity. Compared to other
deterministic particle methods, the proposed method enjoys exact entropy
dissipation and unconditional stability, therefore making it suitable for
large-scale plasma simulations over extended time periods.",2024-09-18,"Yan Huang, Li Wang",http://arxiv.org/pdf/2409.12296v2,cs.LG
SANE: Strategic Autonomous Non-Smooth Exploration for Multiple Optima Discovery in Multi-modal and Non-differentiable Black-box Functions,"Both computational and experimental material discovery bring forth the
challenge of exploring multidimensional and multimodal parameter spaces, such
as phase diagrams of Hamiltonians with multiple interactions, composition
spaces of combinatorial libraries, material structure image spaces, and
molecular embedding spaces. Often these systems are black-box and
time-consuming to evaluate, which resulted in strong interest towards active
learning methods such as Bayesian optimization (BO). However, these systems are
often noisy which make the black box function severely multi-modal and
non-differentiable, where a vanilla BO can get overly focused near a single or
faux optimum, deviating from the broader goal of scientific discovery. To
address these limitations, here we developed Strategic Autonomous Non-Smooth
Exploration (SANE) to facilitate an intelligent Bayesian optimized navigation
with a proposed cost-driven probabilistic acquisition function to find multiple
global and local optimal regions, avoiding the tendency to becoming trapped in
a single optimum. To distinguish between a true and false optimal region due to
noisy experimental measurements, a human (domain) knowledge driven dynamic
surrogate gate is integrated with SANE. We implemented the gate-SANE into a
pre-acquired Piezoresponse spectroscopy data of a ferroelectric combinatorial
library with high noise levels in specific regions, and a piezoresponse force
microscopy (PFM) hyperspectral data. SANE demonstrated better performance than
classical BO to facilitate the exploration of multiple optimal regions and
thereby prioritized learning with higher coverage of scientific values in
autonomous experiments. Our work showcases the potential application of this
method to real-world experiment, where such combined strategic and human
intervening approaches can be critical to unlocking new discoveries in
autonomous research.",2024-09-18,"Arpan Biswas, Rama Vasudevan, Rohit Pant, Ichiro Takeuchi, Hiroshi Funakubo, Yongtao Liu",http://arxiv.org/pdf/2409.12295v1,cs.LG
"RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models","Large language models (LLMs) have recently emerged as promising tools for
solving challenging robotic tasks, even in the presence of action and
observation uncertainties. Recent LLM-based decision-making methods (also
referred to as LLM-based agents), when paired with appropriate critics, have
demonstrated potential in solving complex, long-horizon tasks with relatively
few interactions. However, most existing LLM-based agents lack the ability to
retain and learn from past interactions - an essential trait of learning-based
robotic systems. We propose RAG-Modulo, a framework that enhances LLM-based
agents with a memory of past interactions and incorporates critics to evaluate
the agents' decisions. The memory component allows the agent to automatically
retrieve and incorporate relevant past experiences as in-context examples,
providing context-aware feedback for more informed decision-making. Further by
updating its memory, the agent improves its performance over time, thereby
exhibiting learning. Through experiments in the challenging BabyAI and AlfWorld
domains, we demonstrate significant improvements in task success rates and
efficiency, showing that the proposed RAG-Modulo framework outperforms
state-of-the-art baselines.",2024-09-18,"Abhinav Jain, Chris Jermaine, Vaibhav Unhelkar",http://arxiv.org/pdf/2409.12294v1,cs.LG
In-Context Learning of Linear Systems: Generalization Theory and Applications to Operator Learning,"We study theoretical guarantees for solving linear systems in-context using a
linear transformer architecture. For in-domain generalization, we provide
neural scaling laws that bound the generalization error in terms of the number
of tasks and sizes of samples used in training and inference. For out-of-domain
generalization, we find that the behavior of trained transformers under task
distribution shifts depends crucially on the distribution of the tasks seen
during training. We introduce a novel notion of task diversity and show that it
defines a necessary and sufficient condition for pre-trained transformers
generalize under task distribution shifts. We also explore applications of
learning linear systems in-context, such as to in-context operator learning for
PDEs. Finally, we provide some numerical experiments to validate the
established theory.",2024-09-18,"Frank Cole, Yulong Lu, Wuzhe Xu, Tianhao Zhang",http://arxiv.org/pdf/2409.12293v3,cs.LG
MetaPix: A Data-Centric AI Development Platform for Efficient Management and Utilization of Unstructured Computer Vision Data,"In today's world of advanced AI technologies, data management is a critical
component of any AI/ML solution. Effective data management is vital for the
creation and maintenance of high-quality, diverse datasets, which significantly
enhance predictive capabilities and lead to smarter business solutions. In this
work, we introduce MetaPix, a Data-centric AI platform offering comprehensive
data management solutions specifically designed for unstructured data. MetaPix
offers robust tools for data ingestion, processing, storage, versioning,
governance, and discovery. The platform operates on four key concepts:
DataSources, Datasets, Extensions and Extractors. A DataSource serves as
MetaPix top level asset, representing a narrow-scoped source of data for a
specific use. Datasets are MetaPix second level object, structured collections
of data. Extractors are internal tools integrated into MetaPix's backend
processing, facilitate data processing and enhancement. Additionally, MetaPix
supports extensions, enabling integration with external third-party tools to
enhance platform functionality. This paper delves into each MetaPix concept in
detail, illustrating how they collectively contribute to the platform's
objectives. By providing a comprehensive solution for managing and utilizing
unstructured computer vision data, MetaPix equips organizations with a powerful
toolset to develop AI applications effectively.",2024-09-18,"Sai Vishwanath Venkatesh, Atra Akandeh, Madhu Lokanath",http://arxiv.org/pdf/2409.12289v1,cs.LG
MedCodER: A Generative AI Assistant for Medical Coding,"Medical coding is essential for standardizing clinical data and communication
but is often time-consuming and prone to errors. Traditional Natural Language
Processing (NLP) methods struggle with automating coding due to the large label
space, lengthy text inputs, and the absence of supporting evidence annotations
that justify code selection. Recent advancements in Generative Artificial
Intelligence (AI) offer promising solutions to these challenges. In this work,
we introduce MedCodER, a Generative AI framework for automatic medical coding
that leverages extraction, retrieval, and re-ranking techniques as core
components. MedCodER achieves a micro-F1 score of 0.60 on International
Classification of Diseases (ICD) code prediction, significantly outperforming
state-of-the-art methods. Additionally, we present a new dataset containing
medical records annotated with disease diagnoses, ICD codes, and supporting
evidence texts (https://doi.org/10.5281/zenodo.13308316). Ablation tests
confirm that MedCodER's performance depends on the integration of each of its
aforementioned components, as performance declines when these components are
evaluated in isolation.",2024-09-18,"Krishanu Das Baksi, Elijah Soba, John J. Higgins, Ravi Saini, Jaden Wood, Jane Cook, Jack Scott, Nirmala Pudota, Tim Weninger, Edward Bowen, Sanmitra Bhattacharya",http://arxiv.org/pdf/2409.15368v1,cs.LG
Unsupervised Feature Orthogonalization for Learning Distortion-Invariant Representations,"This study introduces unORANIC+, a novel method that integrates unsupervised
feature orthogonalization with the ability of a Vision Transformer to capture
both local and global relationships for improved robustness and
generalizability. The streamlined architecture of unORANIC+ effectively
separates anatomical and image-specific attributes, resulting in robust and
unbiased latent representations that allow the model to demonstrate excellent
performance across various medical image analysis tasks and diverse datasets.
Extensive experimentation demonstrates unORANIC+'s reconstruction proficiency,
corruption resilience, as well as capability to revise existing image
distortions. Additionally, the model exhibits notable aptitude in downstream
tasks such as disease classification and corruption detection. We confirm its
adaptability to diverse datasets of varying image sources and sample sizes
which positions the method as a promising algorithm for advanced medical image
analysis, particularly in resource-constrained environments lacking large,
tailored datasets. The source code is available at
https://github.com/sdoerrich97/unoranic-plus .",2024-09-18,"Sebastian Doerrich, Francesco Di Salvo, Christian Ledig",http://arxiv.org/pdf/2409.12276v1,cs.LG
Mastering Chess with a Transformer Model,"Transformer models have demonstrated impressive capabilities when trained at
scale, excelling at difficult cognitive tasks requiring complex reasoning and
rational decision-making. In this paper, we explore the application of
transformers to chess, focusing on the critical role of the position
representation within the attention mechanism. We show that transformers
endowed with a sufficiently expressive position representation can match
existing chess-playing models at a fraction of the computational cost. Our
architecture, which we call the Chessformer, significantly outperforms
AlphaZero in both playing strength and puzzle solving ability with 8x less
computation and matches prior grandmaster-level transformer-based agents in
those metrics with 30x less computation. Our models also display an
understanding of chess dissimilar and orthogonal to that of top traditional
engines, detecting high-level positional features like trapped pieces and
fortresses that those engines struggle with. This work demonstrates that
domain-specific enhancements can in large part replace the need for model
scale, while also highlighting that deep learning can make strides even in
areas dominated by search-based methods.",2024-09-18,"Daniel Monroe, Philip A. Chalmers",http://arxiv.org/pdf/2409.12272v2,cs.LG
User-friendly Foundation Model Adapters for Multivariate Time Series Classification,"Foundation models, while highly effective, are often resource-intensive,
requiring substantial inference time and memory. This paper addresses the
challenge of making these models more accessible with limited computational
resources by exploring dimensionality reduction techniques. Our goal is to
enable users to run large pre-trained foundation models on standard GPUs
without sacrificing performance. We investigate classical methods such as
Principal Component Analysis alongside neural network-based adapters, aiming to
reduce the dimensionality of multivariate time series data while preserving key
features. Our experiments show up to a 10x speedup compared to the baseline
model, without performance degradation, and enable up to 4.5x more datasets to
fit on a single GPU, paving the way for more user-friendly and scalable
foundation models.",2024-09-18,"Vasilii Feofanov, Romain Ilbert, Malik Tiomoko, Themis Palpanas, Ievgen Redko",http://arxiv.org/pdf/2409.12264v1,cs.LG
Detecting LGBTQ+ Instances of Cyberbullying,"Social media continues to have an impact on the trajectory of humanity.
However, its introduction has also weaponized keyboards, allowing the abusive
language normally reserved for in-person bullying to jump onto the screen,
i.e., cyberbullying. Cyberbullying poses a significant threat to adolescents
globally, affecting the mental health and well-being of many. A group that is
particularly at risk is the LGBTQ+ community, as researchers have uncovered a
strong correlation between identifying as LGBTQ+ and suffering from greater
online harassment. Therefore, it is critical to develop machine learning models
that can accurately discern cyberbullying incidents as they happen to LGBTQ+
members. The aim of this study is to compare the efficacy of several
transformer models in identifying cyberbullying targeting LGBTQ+ individuals.
We seek to determine the relative merits and demerits of these existing methods
in addressing complex and subtle kinds of cyberbullying by assessing their
effectiveness with real social media data.",2024-09-18,"Muhammad Arslan, Manuel Sandoval Madrigal, Mohammed Abuhamad, Deborah L. Hall, Yasin N. Silva",http://arxiv.org/pdf/2409.12263v1,cs.LG
A constrained optimization approach to improve robustness of neural networks,"In this paper, we present a novel nonlinear programming-based approach to
fine-tune pre-trained neural networks to improve robustness against adversarial
attacks while maintaining high accuracy on clean data. Our method introduces
adversary-correction constraints to ensure correct classification of
adversarial data and minimizes changes to the model parameters. We propose an
efficient cutting-plane-based algorithm to iteratively solve the large-scale
nonconvex optimization problem by approximating the feasible region through
polyhedral cuts and balancing between robustness and accuracy. Computational
experiments on standard datasets such as MNIST and CIFAR10 demonstrate that the
proposed approach significantly improves robustness, even with a very small set
of adversarial data, while maintaining minimal impact on accuracy.",2024-09-18,"Shudian Zhao, Jan Kronqvist",http://arxiv.org/pdf/2409.13770v2,cs.LG
Fine-Tuning a Time Series Foundation Model with Wasserstein Loss,"Inspired by recent advancements in large language models (LLMs) for Natural
Language Processing (NLP), there has been a surge in research focused on
developing foundational models for time series forecasting. One approach
involves training LLM architectures on tokenized time series data using
cross-entropy loss. Although this method has demonstrated promising results,
cross-entropy loss is primarily designed for classification tasks and does not
account for the distance between classes. To address this limitation, we
propose using the Wasserstein loss for such architectures. To validate our
approach, we fine-tuned a foundational time series model on $22$ zero-shot
datasets, comparing the performance of cross-entropy loss with that of
Wasserstein loss. Our results demonstrate that replacing cross-entropy loss
with Wasserstein loss significantly improves point estimation.",2024-09-18,Andrei Chernov,http://arxiv.org/pdf/2409.15367v2,cs.LG
Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks,"Existing subset selection methods for efficient learning predominantly employ
discrete combinatorial and model-specific approaches which lack
generalizability. For an unseen architecture, one cannot use the subset chosen
for a different model. To tackle this problem, we propose $\texttt{SubSelNet}$,
a trainable subset selection framework, that generalizes across architectures.
Here, we first introduce an attention-based neural gadget that leverages the
graph structure of architectures and acts as a surrogate to trained deep neural
networks for quick model prediction. Then, we use these predictions to build
subset samplers. This naturally provides us two variants of
$\texttt{SubSelNet}$. The first variant is transductive (called as
Transductive-$\texttt{SubSelNet}$) which computes the subset separately for
each model by solving a small optimization problem. Such an optimization is
still super fast, thanks to the replacement of explicit model training by the
model approximator. The second variant is inductive (called as
Inductive-$\texttt{SubSelNet}$) which computes the subset using a trained
subset selector, without any optimization. Our experiments show that our model
outperforms several methods across several real datasets",2024-09-18,"Eeshaan Jain, Tushar Nandy, Gaurav Aggarwal, Ashish Tendulkar, Rishabh Iyer, Abir De",http://arxiv.org/pdf/2409.12255v1,cs.LG
Conformal Fields from Neural Networks,"We use the embedding formalism to construct conformal fields in $D$
dimensions, by restricting Lorentz-invariant ensembles of homogeneous neural
networks in $(D+2)$ dimensions to the projective null cone. Conformal
correlators may be computed using the parameter space description of the neural
network. Exact four-point correlators are computed in a number of examples, and
we perform a 4D conformal block decomposition that elucidates the spectrum. In
some examples the analysis is facilitated by recent approaches to Feynman
integrals. Generalized free CFTs are constructed using the infinite-width
Gaussian process limit of the neural network, enabling a realization of the
free boson. The extension to deep networks constructs conformal fields at each
subsequent layer, with recursion relations relating their conformal dimensions
and four-point functions. Numerical approaches are discussed.",2024-09-18,"James Halverson, Joydeep Naskar, Jiahua Tian",http://arxiv.org/pdf/2409.12222v1,cs.LG
DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control,"Imitation learning has proven to be a powerful tool for training complex
visuomotor policies. However, current methods often require hundreds to
thousands of expert demonstrations to handle high-dimensional visual
observations. A key reason for this poor data efficiency is that visual
representations are predominantly either pretrained on out-of-domain data or
trained directly through a behavior cloning objective. In this work, we present
DynaMo, a new in-domain, self-supervised method for learning visual
representations. Given a set of expert demonstrations, we jointly learn a
latent inverse dynamics model and a forward dynamics model over a sequence of
image embeddings, predicting the next frame in latent space, without
augmentations, contrastive sampling, or access to ground truth actions.
Importantly, DynaMo does not require any out-of-domain data such as Internet
datasets or cross-embodied datasets. On a suite of six simulated and real
environments, we show that representations learned with DynaMo significantly
improve downstream imitation learning performance over prior self-supervised
learning objectives, and pretrained representations. Gains from using DynaMo
hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,
and nearest neighbors. Finally, we ablate over key components of DynaMo and
measure its impact on downstream policy performance. Robot videos are best
viewed at https://dynamo-ssl.github.io",2024-09-18,"Zichen Jeff Cui, Hengkai Pan, Aadhithya Iyer, Siddhant Haldar, Lerrel Pinto",http://arxiv.org/pdf/2409.12192v2,cs.LG
ARTICLE: Annotator Reliability Through In-Context Learning,"Ensuring annotator quality in training and evaluation data is a key piece of
machine learning in NLP. Tasks such as sentiment analysis and offensive speech
detection are intrinsically subjective, creating a challenging scenario for
traditional quality assessment approaches because it is hard to distinguish
disagreement due to poor work from that due to differences of opinions between
sincere annotators. With the goal of increasing diverse perspectives in
annotation while ensuring consistency, we propose \texttt{ARTICLE}, an
in-context learning (ICL) framework to estimate annotation quality through
self-consistency. We evaluate this framework on two offensive speech datasets
using multiple LLMs and compare its performance with traditional methods. Our
findings indicate that \texttt{ARTICLE} can be used as a robust method for
identifying reliable annotators, hence improving data quality.",2024-09-18,"Sujan Dutta, Deepak Pandita, Tharindu Cyril Weerasooriya, Marcos Zampieri, Christopher M. Homan, Ashiqur R. KhudaBukhsh",http://arxiv.org/pdf/2409.12218v2,cs.LG
Massively Multi-Person 3D Human Motion Forecasting with Scene Context,"Forecasting long-term 3D human motion is challenging: the stochasticity of
human behavior makes it hard to generate realistic human motion from the input
sequence alone. Information on the scene environment and the motion of nearby
people can greatly aid the generation process. We propose a scene-aware social
transformer model (SAST) to forecast long-term (10s) human motion motion.
Unlike previous models, our approach can model interactions between both widely
varying numbers of people and objects in a scene. We combine a temporal
convolutional encoder-decoder architecture with a Transformer-based bottleneck
that allows us to efficiently combine motion and scene information. We model
the conditional motion distribution using denoising diffusion models. We
benchmark our approach on the Humans in Kitchens dataset, which contains 1 to
16 persons and 29 to 50 objects that are visible simultaneously. Our model
outperforms other approaches in terms of realism and diversity on different
metrics and in a user study. Code is available at
https://github.com/felixbmuller/SAST.",2024-09-18,"Felix B Mueller, Julian Tanke, Juergen Gall",http://arxiv.org/pdf/2409.12189v1,cs.LG
To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning,"Chain-of-thought (CoT) via prompting is the de facto method for eliciting
reasoning capabilities from large language models (LLMs). But for what kinds of
tasks is this extra ``thinking'' really helpful? To analyze this, we conducted
a quantitative meta-analysis covering over 100 papers using CoT and ran our own
evaluations of 20 datasets across 14 models. Our results show that CoT gives
strong performance benefits primarily on tasks involving math or logic, with
much smaller gains on other types of tasks. On MMLU, directly generating the
answer without CoT leads to almost identical accuracy as CoT unless the
question or model's response contains an equals sign, indicating symbolic
operations and reasoning. Following this finding, we analyze the behavior of
CoT on these problems by separating planning and execution and comparing
against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic
execution, but it underperforms relative to using a symbolic solver. Our
results indicate that CoT can be applied selectively, maintaining performance
while saving inference costs. Furthermore, they suggest a need to move beyond
prompt-based CoT to new paradigms that better leverage intermediate computation
across the whole range of LLM applications.",2024-09-18,"Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett",http://arxiv.org/pdf/2409.12183v3,cs.LG
A Controlled Study on Long Context Extension and Generalization in LLMs,"Broad textual understanding and in-context learning require language models
that utilize full document contexts. Due to the implementation challenges
associated with directly training long-context models, many methods have been
proposed for extending models to handle long contexts. However, owing to
differences in data and model classes, it has been challenging to compare these
approaches, leading to uncertainty as to how to evaluate long-context
performance and whether it differs from standard evaluation. We implement a
controlled protocol for extension methods with a standardized evaluation,
utilizing consistent base models and extension data. Our study yields several
insights into long-context behavior. First, we reaffirm the critical role of
perplexity as a general-purpose performance indicator even in longer-context
tasks. Second, we find that current approximate attention methods
systematically underperform across long-context tasks. Finally, we confirm that
exact fine-tuning based methods are generally effective within the range of
their extension, whereas extrapolation remains challenging. All codebases,
models, and checkpoints will be made available open-source, promoting
transparency and facilitating further research in this critical area of AI
development.",2024-09-18,"Yi Lu, Jing Nathan Yan, Songlin Yang, Justin T. Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, Alexander M. Rush",http://arxiv.org/pdf/2409.12181v2,cs.LG
Finetuning Language Models to Emit Linguistic Expressions of Uncertainty,"Large language models (LLMs) are increasingly employed in information-seeking
and decision-making tasks. Despite their broad utility, LLMs tend to generate
information that conflicts with real-world facts, and their persuasive style
can make these inaccuracies appear confident and convincing. As a result,
end-users struggle to consistently align the confidence expressed by LLMs with
the accuracy of their predictions, often leading to either blind trust in all
outputs or a complete disregard for their reliability. In this work, we explore
supervised finetuning on uncertainty-augmented predictions as a method to
develop models that produce linguistic expressions of uncertainty.
Specifically, we measure the calibration of pre-trained models and then
fine-tune language models to generate calibrated linguistic expressions of
uncertainty. Through experiments on various question-answering datasets, we
demonstrate that LLMs are well-calibrated in assessing their predictions, and
supervised finetuning based on the model's own confidence leads to
well-calibrated expressions of uncertainty, particularly for single-claim
answers.",2024-09-18,"Arslan Chaudhry, Sridhar Thiagarajan, Dilan Gorur",http://arxiv.org/pdf/2409.12180v1,cs.LG
Trajectory Anomaly Detection with Language Models,"This paper presents a novel approach for trajectory anomaly detection using
an autoregressive causal-attention model, termed LM-TAD. This method leverages
the similarities between language statements and trajectories, both of which
consist of ordered elements requiring coherence through external rules and
contextual variations. By treating trajectories as sequences of tokens, our
model learns the probability distributions over trajectories, enabling the
identification of anomalous locations with high precision. We incorporate
user-specific tokens to account for individual behavior patterns, enhancing
anomaly detection tailored to user context. Our experiments demonstrate the
effectiveness of LM-TAD on both synthetic and real-world datasets. In
particular, the model outperforms existing methods on the Pattern of Life (PoL)
dataset by detecting user-contextual anomalies and achieves competitive results
on the Porto taxi dataset, highlighting its adaptability and robustness.
Additionally, we introduce the use of perplexity and surprisal rate metrics for
detecting outliers and pinpointing specific anomalous locations within
trajectories. The LM-TAD framework supports various trajectory representations,
including GPS coordinates, staypoints, and activity types, proving its
versatility in handling diverse trajectory data. Moreover, our approach is
well-suited for online trajectory anomaly detection, significantly reducing
computational latency by caching key-value states of the attention mechanism,
thereby avoiding repeated computations.",2024-09-18,"Jonathan Mbuya, Dieter Pfoser, Antonios Anastasopoulos",http://arxiv.org/pdf/2409.15366v1,cs.LG
Novel Saliency Analysis for the Forward Forward Algorithm,"Incorporating the Forward Forward algorithm into neural network training
represents a transformative shift from traditional methods, introducing a dual
forward mechanism that streamlines the learning process by bypassing the
complexities of derivative propagation. This method is noted for its simplicity
and efficiency and involves executing two forward passes the first with actual
data to promote positive reinforcement, and the second with synthetically
generated negative data to enable discriminative learning. Our experiments
confirm that the Forward Forward algorithm is not merely an experimental
novelty but a viable training strategy that competes robustly with conventional
multi layer perceptron (MLP) architectures. To overcome the limitations
inherent in traditional saliency techniques, which predominantly rely on
gradient based methods, we developed a bespoke saliency algorithm specifically
tailored for the Forward Forward framework. This innovative algorithm enhances
the intuitive understanding of feature importance and network decision-making,
providing clear visualizations of the data features most influential in model
predictions. By leveraging this specialized saliency method, we gain deeper
insights into the internal workings of the model, significantly enhancing our
interpretative capabilities beyond those offered by standard approaches. Our
evaluations, utilizing the MNIST and Fashion MNIST datasets, demonstrate that
our method performs comparably to traditional MLP-based models.",2024-09-18,Mitra Bakhshi,http://arxiv.org/pdf/2409.15365v1,cs.LG
Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit Recommendation with Preference,"Personalized outfit recommendation remains a complex challenge, demanding
both fashion compatibility understanding and trend awareness. This paper
presents a novel framework that harnesses the expressive power of large
language models (LLMs) for this task, mitigating their ""black box"" and static
nature through fine-tuning and direct feedback integration. We bridge the item
visual-textual gap in items descriptions by employing image captioning with a
Multimodal Large Language Model (MLLM). This enables the LLM to extract style
and color characteristics from human-curated fashion images, forming the basis
for personalized recommendations. The LLM is efficiently fine-tuned on the
open-source Polyvore dataset of curated fashion images, optimizing its ability
to recommend stylish outfits. A direct preference mechanism using negative
examples is employed to enhance the LLM's decision-making process. This creates
a self-enhancing AI feedback loop that continuously refines recommendations in
line with seasonal fashion trends. Our framework is evaluated on the Polyvore
dataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,
and complementary item retrieval. These evaluations underline the framework's
ability to generate stylish, trend-aligned outfit suggestions, continuously
improving through direct feedback. The evaluation results demonstrated that our
proposed framework significantly outperforms the base LLM, creating more
cohesive outfits. The improved performance in these tasks underscores the
proposed framework's potential to enhance the shopping experience with accurate
suggestions, proving its effectiveness over the vanilla LLM based outfit
generation.",2024-09-18,"Najmeh Forouzandehmehr, Nima Farrokhsiar, Ramin Giahi, Evren Korpeoglu, Kannan Achan",http://arxiv.org/pdf/2409.12150v1,cs.LG
GRIN: GRadient-INformed MoE,"Mixture-of-Experts (MoE) models scale more effectively than dense models due
to sparse computation through expert routing, selectively activating only a
small subset of expert modules. However, sparse computation challenges
traditional training practices, as discrete expert routing hinders standard
backpropagation and thus gradient-based optimization, which are the cornerstone
of deep learning. To better pursue the scaling power of MoE, we introduce GRIN
(GRadient-INformed MoE training), which incorporates sparse gradient estimation
for expert routing and configures model parallelism to avoid token dropping.
Applying GRIN to autoregressive language modeling, we develop a top-2
16$\times$3.8B MoE model. Our model, with only 6.6B activated parameters,
outperforms a 7B dense model and matches the performance of a 14B dense model
trained on the same data. Extensive evaluations across diverse tasks
demonstrate the potential of GRIN to significantly enhance MoE efficacy,
achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.",2024-09-18,"Liyuan Liu, Young Jin Kim, Shuohang Wang, Chen Liang, Yelong Shen, Hao Cheng, Xiaodong Liu, Masahiro Tanaka, Xiaoxia Wu, Wenxiang Hu, Vishrav Chaudhary, Zeqi Lin, Chenruidong Zhang, Jilong Xue, Hany Awadalla, Jianfeng Gao, Weizhu Chen",http://arxiv.org/pdf/2409.12136v1,cs.LG
Almost Sure Convergence of Linear Temporal Difference Learning with Arbitrary Features,"Temporal difference (TD) learning with linear function approximation,
abbreviated as linear TD, is a classic and powerful prediction algorithm in
reinforcement learning. While it is well understood that linear TD converges
almost surely to a unique point, this convergence traditionally requires the
assumption that the features used by the approximator are linearly independent.
However, this linear independence assumption does not hold in many practical
scenarios. This work is the first to establish the almost sure convergence of
linear TD without requiring linearly independent features. In fact, we do not
make any assumptions on the features. We prove that the approximated value
function converges to a unique point and the weight iterates converge to a set.
We also establish a notion of local stability of the weight iterates.
Importantly, we do not need to introduce any other additional assumptions and
do not need to make any modification to the linear TD algorithm. Key to our
analysis is a novel characterization of bounded invariant sets of the mean ODE
of linear TD.",2024-09-18,"Jiuqi Wang, Shangtong Zhang",http://arxiv.org/pdf/2409.12135v2,cs.LG
BodyShapeGPT: SMPL Body Shape Manipulation with LLMs,"Generative AI models provide a wide range of tools capable of performing
complex tasks in a fraction of the time it would take a human. Among these,
Large Language Models (LLMs) stand out for their ability to generate diverse
texts, from literary narratives to specialized responses in different fields of
knowledge. This paper explores the use of fine-tuned LLMs to identify physical
descriptions of people, and subsequently create accurate representations of
avatars using the SMPL-X model by inferring shape parameters. We demonstrate
that LLMs can be trained to understand and manipulate the shape space of SMPL,
allowing the control of 3D human shapes through natural language. This approach
promises to improve human-machine interaction and opens new avenues for
customization and simulation in virtual environments.",2024-09-18,"Baldomero R. Árbol, Dan Casas",http://arxiv.org/pdf/2410.03556v1,cs.LG
Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement,"In this report, we present a series of math-specific large language models:
Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the
Qwen2.5 series lies in integrating the philosophy of self-improvement
throughout the entire pipeline, from pre-training and post-training to
inference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized
to generate large-scale, high-quality mathematical data. (2) In the
post-training phase, we develop a reward model (RM) by conducting massive
sampling from Qwen2-Math-Instruct. This RM is then applied to the iterative
evolution of data in supervised fine-tuning (SFT). With a stronger SFT model,
it's possible to iteratively train and update the RM, which in turn guides the
next round of SFT data iteration. On the final SFT model, we employ the
ultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.
(3) Furthermore, during the inference stage, the RM is used to guide sampling,
optimizing the model's performance.
  Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced
mathematical reasoning capabilities, including Chain-of-Thought (CoT) and
Tool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics
datasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and
AIME24, covering a range of difficulties from grade school level to math
competition problems.",2024-09-18,"An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, Zhenru Zhang",http://arxiv.org/pdf/2409.12122v1,cs.LG
Stronger Baseline Models -- A Key Requirement for Aligning Machine Learning Research with Clinical Utility,"Machine Learning (ML) research has increased substantially in recent years,
due to the success of predictive modeling across diverse application domains.
However, well-known barriers exist when attempting to deploy ML models in
high-stakes, clinical settings, including lack of model transparency (or the
inability to audit the inference process), large training data requirements
with siloed data sources, and complicated metrics for measuring model utility.
In this work, we show empirically that including stronger baseline models in
healthcare ML evaluations has important downstream effects that aid
practitioners in addressing these challenges. Through a series of case studies,
we find that the common practice of omitting baselines or comparing against a
weak baseline model (e.g. a linear model with no optimization) obscures the
value of ML methods proposed in the research literature. Using these insights,
we propose some best practices that will enable practitioners to more
effectively study and deploy ML models in clinical settings.",2024-09-18,"Nathan Wolfrath, Joel Wolfrath, Hengrui Hu, Anjishnu Banerjee, Anai N. Kothari",http://arxiv.org/pdf/2409.12116v1,cs.LG
Pareto Data Framework: Steps Towards Resource-Efficient Decision Making Using Minimum Viable Data (MVD),"This paper introduces the Pareto Data Framework, an approach for identifying
and selecting the Minimum Viable Data (MVD) required for enabling machine
learning applications on constrained platforms such as embedded systems, mobile
devices, and Internet of Things (IoT) devices. We demonstrate that strategic
data reduction can maintain high performance while significantly reducing
bandwidth, energy, computation, and storage costs. The framework identifies
Minimum Viable Data (MVD) to optimize efficiency across resource-constrained
environments without sacrificing performance. It addresses common inefficient
practices in an IoT application such as overprovisioning of sensors and
overprecision, and oversampling of signals, proposing scalable solutions for
optimal sensor selection, signal extraction and transmission, and data
representation. An experimental methodology demonstrates effective acoustic
data characterization after downsampling, quantization, and truncation to
simulate reduced-fidelity sensors and network and storage constraints; results
shows that performance can be maintained up to 95\% with sample rates reduced
by 75\% and bit depths and clip length reduced by 50\% which translates into
substantial cost and resource reduction. These findings have implications on
the design and development of constrained systems. The paper also discusses
broader implications of the framework, including the potential to democratize
advanced AI technologies across IoT applications and sectors such as
agriculture, transportation, and manufacturing to improve access and multiply
the benefits of data-driven insights.",2024-09-18,"Tashfain Ahmed, Josh Siegel",http://arxiv.org/pdf/2409.12112v1,cs.LG
FedLF: Adaptive Logit Adjustment and Feature Optimization in Federated Long-Tailed Learning,"Federated learning offers a paradigm to the challenge of preserving privacy
in distributed machine learning. However, datasets distributed across each
client in the real world are inevitably heterogeneous, and if the datasets can
be globally aggregated, they tend to be long-tailed distributed, which greatly
affects the performance of the model. The traditional approach to federated
learning primarily addresses the heterogeneity of data among clients, yet it
fails to address the phenomenon of class-wise bias in global long-tailed data.
This results in the trained model focusing on the head classes while neglecting
the equally important tail classes. Consequently, it is essential to develop a
methodology that considers classes holistically. To address the above problems,
we propose a new method FedLF, which introduces three modifications in the
local training phase: adaptive logit adjustment, continuous class centred
optimization, and feature decorrelation. We compare seven state-of-the-art
methods with varying degrees of data heterogeneity and long-tailed
distribution. Extensive experiments on benchmark datasets CIFAR-10-LT and
CIFAR-100-LT demonstrate that our approach effectively mitigates the problem of
model performance degradation due to data heterogeneity and long-tailed
distribution. our code is available at https://github.com/18sym/FedLF.",2024-09-18,"Xiuhua Lu, Peng Li, Xuefeng Jiang",http://arxiv.org/pdf/2409.12105v1,cs.LG
Symmetry-Enriched Learning: A Category-Theoretic Framework for Robust Machine Learning Models,"This manuscript presents a novel framework that integrates higher-order
symmetries and category theory into machine learning. We introduce new
mathematical constructs, including hyper-symmetry categories and functorial
representations, to model complex transformations within learning algorithms.
Our contributions include the design of symmetry-enriched learning models, the
development of advanced optimization techniques leveraging categorical
symmetries, and the theoretical analysis of their implications for model
robustness, generalization, and convergence. Through rigorous proofs and
practical applications, we demonstrate that incorporating higher-dimensional
categorical structures enhances both the theoretical foundations and practical
capabilities of modern machine learning algorithms, opening new directions for
research and innovation.",2024-09-18,Ronald Katende,http://arxiv.org/pdf/2409.12100v1,cs.LG
Skill matching at scale: freelancer-project alignment for efficient multilingual candidate retrieval,"Finding the perfect match between a job proposal and a set of freelancers is
not an easy task to perform at scale, especially in multiple languages. In this
paper, we propose a novel neural retriever architecture that tackles this
problem in a multilingual setting. Our method encodes project descriptions and
freelancer profiles by leveraging pre-trained multilingual language models. The
latter are used as backbone for a custom transformer architecture that aims to
keep the structure of the profiles and project. This model is trained with a
contrastive loss on historical data. Thanks to several experiments, we show
that this approach effectively captures skill matching similarity and
facilitates efficient matching, outperforming traditional methods.",2024-09-18,"Warren Jouanneau, Marc Palyart, Emma Jouffroy",http://arxiv.org/pdf/2409.12097v2,cs.LG
Assessing Reusability of Deep Learning-Based Monotherapy Drug Response Prediction Models Trained with Omics Data,"Cancer drug response prediction (DRP) models present a promising approach
towards precision oncology, tailoring treatments to individual patient
profiles. While deep learning (DL) methods have shown great potential in this
area, models that can be successfully translated into clinical practice and
shed light on the molecular mechanisms underlying treatment response will
likely emerge from collaborative research efforts. This highlights the need for
reusable and adaptable models that can be improved and tested by the wider
scientific community. In this study, we present a scoring system for assessing
the reusability of prediction DRP models, and apply it to 17 peer-reviewed
DL-based DRP models. As part of the IMPROVE (Innovative Methodologies and New
Data for Predictive Oncology Model Evaluation) project, which aims to develop
methods for systematic evaluation and comparison DL models across scientific
domains, we analyzed these 17 DRP models focusing on three key categories:
software environment, code modularity, and data availability and preprocessing.
While not the primary focus, we also attempted to reproduce key performance
metrics to verify model behavior and adaptability. Our assessment of 17 DRP
models reveals both strengths and shortcomings in model reusability. To promote
rigorous practices and open-source sharing, we offer recommendations for
developing and sharing prediction models. Following these recommendations can
address many of the issues identified in this study, improving model
reusability without adding significant burdens on researchers. This work offers
the first comprehensive assessment of reusability and reproducibility across
diverse DRP models, providing insights into current model sharing practices and
promoting standards within the DRP and broader AI-enabled scientific research
community.",2024-09-18,"Jamie C. Overbeek, Alexander Partin, Thomas S. Brettin, Nicholas Chia, Oleksandr Narykov, Priyanka Vasanthakumari, Andreas Wilke, Yitan Zhu, Austin Clyde, Sara Jones, Rohan Gnanaolivu, Yuanhang Liu, Jun Jiang, Chen Wang, Carter Knutson, Andrew McNaughton, Neeraj Kumar, Gayara Demini Fernando, Souparno Ghosh, Cesar Sanchez-Villalobos, Ruibo Zhang, Ranadip Pal, M. Ryan Weil, Rick L. Stevens",http://arxiv.org/pdf/2409.12215v1,cs.LG
The Impact of Element Ordering on LM Agent Performance,"There has been a surge of interest in language model agents that can navigate
virtual environments such as the web or desktop. To navigate such environments,
agents benefit from information on the various elements (e.g., buttons, text,
or images) present. It remains unclear which element attributes have the
greatest impact on agent performance, especially in environments that only
provide a graphical representation (i.e., pixels). Here we find that the
ordering in which elements are presented to the language model is surprisingly
impactful--randomizing element ordering in a webpage degrades agent performance
comparably to removing all visible text from an agent's state representation.
While a webpage provides a hierarchical ordering of elements, there is no such
ordering when parsing elements directly from pixels. Moreover, as tasks become
more challenging and models more sophisticated, our experiments suggest that
the impact of ordering increases. Finding an effective ordering is non-trivial.
We investigate the impact of various element ordering methods in web and
desktop environments. We find that dimensionality reduction provides a viable
ordering for pixel-only environments. We train a UI element detection model to
derive elements from pixels and apply our findings to an agent
benchmark--OmniACT--where we only have access to pixels. Our method completes
more than two times as many tasks on average relative to the previous
state-of-the-art.",2024-09-18,"Wayne Chi, Ameet Talwalkar, Chris Donahue",http://arxiv.org/pdf/2409.12089v3,cs.LG
Towards Interpretable End-Stage Renal Disease (ESRD) Prediction: Utilizing Administrative Claims Data with Explainable AI Techniques,"This study explores the potential of utilizing administrative claims data,
combined with advanced machine learning and deep learning techniques, to
predict the progression of Chronic Kidney Disease (CKD) to End-Stage Renal
Disease (ESRD). We analyze a comprehensive, 10-year dataset provided by a major
health insurance organization to develop prediction models for multiple
observation windows using traditional machine learning methods such as Random
Forest and XGBoost as well as deep learning approaches such as Long Short-Term
Memory (LSTM) networks. Our findings demonstrate that the LSTM model,
particularly with a 24-month observation window, exhibits superior performance
in predicting ESRD progression, outperforming existing models in the
literature. We further apply SHapley Additive exPlanations (SHAP) analysis to
enhance interpretability, providing insights into the impact of individual
features on predictions at the individual patient level. This study underscores
the value of leveraging administrative claims data for CKD management and
predicting ESRD progression.",2024-09-18,"Yubo Li, Saba Al-Sayouri, Rema Padman",http://arxiv.org/pdf/2409.12087v3,cs.LG
Combining LLM Code Generation with Formal Specifications and Reactive Program Synthesis,"In the past few years, Large Language Models (LLMs) have exploded in
usefulness and popularity for code generation tasks. However, LLMs still
struggle with accuracy and are unsuitable for high-risk applications without
additional oversight and verification. In particular, they perform poorly at
generating code for highly complex systems, especially with unusual or
out-of-sample logic. For such systems, verifying the code generated by the LLM
may take longer than writing it by hand. We introduce a solution that divides
the code generation into two parts; one to be handled by an LLM and one to be
handled by formal methods-based program synthesis. We develop a benchmark to
test our solution and show that our method allows the pipeline to solve
problems previously intractable for LLM code generation.",2024-09-18,"William Murphy, Nikolaus Holzer, Feitong Qiao, Leyi Cui, Raven Rothkopf, Nathan Koenig, Mark Santolucito",http://arxiv.org/pdf/2410.19736v1,cs.LG
Denoising diffusion models for high-resolution microscopy image restoration,"Advances in microscopy imaging enable researchers to visualize structures at
the nanoscale level thereby unraveling intricate details of biological
organization. However, challenges such as image noise, photobleaching of
fluorophores, and low tolerability of biological samples to high light doses
remain, restricting temporal resolutions and experiment durations. Reduced
laser doses enable longer measurements at the cost of lower resolution and
increased noise, which hinders accurate downstream analyses. Here we train a
denoising diffusion probabilistic model (DDPM) to predict high-resolution
images by conditioning the model on low-resolution information. Additionally,
the probabilistic aspect of the DDPM allows for repeated generation of images
that tend to further increase the signal-to-noise ratio. We show that our model
achieves a performance that is better or similar to the previously
best-performing methods, across four highly diverse datasets. Importantly,
while any of the previous methods show competitive performance for some, but
not all datasets, our method consistently achieves high performance across all
four data sets, suggesting high generalizability.",2024-09-18,"Pamela Osuna-Vargas, Maren H. Wehrheim, Lucas Zinz, Johanna Rahm, Ashwin Balakrishnan, Alexandra Kaminer, Mike Heilemann, Matthias Kaschube",http://arxiv.org/pdf/2409.12078v1,cs.LG
Unsupervised Domain Adaptation Via Data Pruning,"The removal of carefully-selected examples from training data has recently
emerged as an effective way of improving the robustness of machine learning
models. However, the best way to select these examples remains an open
question. In this paper, we consider the problem from the perspective of
unsupervised domain adaptation (UDA). We propose AdaPrune, a method for UDA
whereby training examples are removed to attempt to align the training
distribution to that of the target data. By adopting the maximum mean
discrepancy (MMD) as the criterion for alignment, the problem can be neatly
formulated and solved as an integer quadratic program. We evaluate our approach
on a real-world domain shift task of bioacoustic event detection. As a method
for UDA, we show that AdaPrune outperforms related techniques, and is
complementary to other UDA algorithms such as CORAL. Our analysis of the
relationship between the MMD and model accuracy, along with t-SNE plots,
validate the proposed method as a principled and well-founded way of performing
data pruning.",2024-09-18,"Andrea Napoli, Paul White",http://arxiv.org/pdf/2409.12076v1,cs.LG
Fitting Multilevel Factor Models,"We examine a special case of the multilevel factor model, with covariance
given by multilevel low rank (MLR) matrix~\cite{parshakova2023factor}. We
develop a novel, fast implementation of the expectation-maximization (EM)
algorithm, tailored for multilevel factor models, to maximize the likelihood of
the observed data. This method accommodates any hierarchical structure and
maintains linear time and storage complexities per iteration. This is achieved
through a new efficient technique for computing the inverse of the positive
definite MLR matrix. We show that the inverse of an invertible PSD MLR matrix
is also an MLR matrix with the same sparsity in factors, and we use the
recursive Sherman-Morrison-Woodbury matrix identity to obtain the factors of
the inverse. Additionally, we present an algorithm that computes the Cholesky
factorization of an expanded matrix with linear time and space complexities,
yielding the covariance matrix as its Schur complement. This paper is
accompanied by an open-source package that implements the proposed methods.",2024-09-18,"Tetiana Parshakova, Trevor Hastie, Stephen Boyd",http://arxiv.org/pdf/2409.12067v2,cs.LG
"MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning","Large Language Model can reasonably understand and generate human expressions
but may lack of thorough thinking and reasoning mechanisms. Recently there have
been several studies which enhance the thinking ability of language models but
most of them are not data-driven or training-based. In this paper, we are
motivated by the cognitive mechanism in the natural world, and design a novel
model architecture called TaS which allows it to first consider the thoughts
and then express the response based upon the query. We design several pipelines
to annotate or generate the thought contents from prompt-response samples, then
add language heads in a middle layer which behaves as the thinking layer. We
train the language model by the thoughts-augmented data and successfully let
the thinking layer automatically generate reasonable thoughts and finally
output more reasonable responses. Both qualitative examples and quantitative
results validate the effectiveness and performance of TaS. Our code is
available at https://anonymous.4open.science/r/TadE.",2024-09-18,"Ningyuan Xi, Xiaoyu Wang, Yetao Wu, Teng Chen, Qingqing Gu, Yue Zhao, Jinxian Qu, Zhonglin Jiang, Yong Chen, Luo Ji",http://arxiv.org/pdf/2409.12059v4,cs.LG
Cartan moving frames and the data manifolds,"The purpose of this paper is to employ the language of Cartan moving frames
to study the geometry of the data manifolds and its Riemannian structure, via
the data information metric and its curvature at data points. Using this
framework and through experiments, explanations on the response of a neural
network are given by pointing out the output classes that are easily reachable
from a given input. This emphasizes how the proposed mathematical relationship
between the output of the network and the geometry of its inputs can be
exploited as an explainable artificial intelligence tool.",2024-09-18,"Eliot Tron, Rita Fioresi, Nicolas Couellan, Stéphane Puechmorel",http://arxiv.org/pdf/2409.12057v2,cs.LG
Extended Deep Submodular Functions,"We introduce a novel category of set functions called Extended Deep
Submodular functions (EDSFs), which are neural network-representable. EDSFs
serve as an extension of Deep Submodular Functions (DSFs), inheriting crucial
properties from DSFs while addressing innate limitations. It is known that DSFs
can represent a limiting subset of submodular functions. In contrast, through
an analysis of polymatroid properties, we establish that EDSFs possess the
capability to represent all monotone submodular functions, a notable
enhancement compared to DSFs. Furthermore, our findings demonstrate that EDSFs
can represent any monotone set function, indicating the family of EDSFs is
equivalent to the family of all monotone set functions. Additionally, we prove
that EDSFs maintain the concavity inherent in DSFs when the components of the
input vector are non-negative real numbers-an essential feature in certain
combinatorial optimization problems. Through extensive experiments, we
illustrate that EDSFs exhibit significantly lower empirical generalization
error than DSFs in the learning of coverage functions. This suggests that EDSFs
present a promising advancement in the representation and learning of set
functions with improved generalization capabilities.",2024-09-18,"Seyed Mohammad Hosseini, Arash Jamshid, Seyed Mahdi Noormousavi, Mahdi Jafari Siavoshani, Naeimeh Omidvar",http://arxiv.org/pdf/2409.12053v1,cs.LG
A Data Envelopment Analysis Approach for Assessing Fairness in Resource Allocation: Application to Kidney Exchange Programs,"Kidney exchange programs have significantly increased transplantation rates
but raise pressing questions about fairness in organ allocation. We present a
novel framework leveraging Data Envelopment Analysis (DEA) to evaluate multiple
fairness criteria--Priority, Access, and Outcome--within a single model,
capturing complexities that may be overlooked in single-metric analyses. Using
data from the United Network for Organ Sharing, we analyze these criteria
individually, measuring Priority fairness through waitlist durations, Access
fairness through Kidney Donor Profile Index scores, and Outcome fairness
through graft lifespan. We then apply our DEA model to demonstrate significant
disparities in kidney allocation efficiency across ethnic groups. To quantify
uncertainty, we employ conformal prediction within the DEA framework, yielding
group-conditional prediction intervals with finite sample coverage guarantees.
Our findings show notable differences in efficiency distributions between
ethnic groups. Our study provides a rigorous framework for evaluating fairness
in complex resource allocation systems, where resource scarcity and mutual
compatibility constraints exist. All code for using the proposed method and
reproducing results is available on GitHub.",2024-09-18,"Ali Kaazempur-Mofrad, Xiaowu Dai",http://arxiv.org/pdf/2410.02799v1,cs.LG
Handling Long-Term Safety and Uncertainty in Safe Reinforcement Learning,"Safety is one of the key issues preventing the deployment of reinforcement
learning techniques in real-world robots. While most approaches in the Safe
Reinforcement Learning area do not require prior knowledge of constraints and
robot kinematics and rely solely on data, it is often difficult to deploy them
in complex real-world settings. Instead, model-based approaches that
incorporate prior knowledge of the constraints and dynamics into the learning
framework have proven capable of deploying the learning algorithm directly on
the real robot. Unfortunately, while an approximated model of the robot
dynamics is often available, the safety constraints are task-specific and hard
to obtain: they may be too complicated to encode analytically, too expensive to
compute, or it may be difficult to envision a priori the long-term safety
requirements. In this paper, we bridge this gap by extending the safe
exploration method, ATACOM, with learnable constraints, with a particular focus
on ensuring long-term safety and handling of uncertainty. Our approach is
competitive or superior to state-of-the-art methods in final performance while
maintaining safer behavior during training.",2024-09-18,"Jonas Günster, Puze Liu, Jan Peters, Davide Tateo",http://arxiv.org/pdf/2409.12045v2,cs.LG
Understanding the Effects of the Baidu-ULTR Logging Policy on Two-Tower Models,"Despite the popularity of the two-tower model for unbiased learning to rank
(ULTR) tasks, recent work suggests that it suffers from a major limitation that
could lead to its collapse in industry applications: the problem of logging
policy confounding. Several potential solutions have even been proposed;
however, the evaluation of these methods was mostly conducted using
semi-synthetic simulation experiments. This paper bridges the gap between
theory and practice by investigating the confounding problem on the largest
real-world dataset, Baidu-ULTR. Our main contributions are threefold: 1) we
show that the conditions for the confounding problem are given on Baidu-ULTR,
2) the confounding problem bears no significant effect on the two-tower model,
and 3) we point to a potential mismatch between expert annotations, the golden
standard in ULTR, and user click behavior.",2024-09-18,"Morris de Haan, Philipp Hager",http://arxiv.org/pdf/2409.12043v1,cs.LG
A Unified Framework for Neural Computation and Learning Over Time,"This paper proposes Hamiltonian Learning, a novel unified framework for
learning with neural networks ""over time"", i.e., from a possibly infinite
stream of data, in an online manner, without having access to future
information. Existing works focus on the simplified setting in which the stream
has a known finite length or is segmented into smaller sequences, leveraging
well-established learning strategies from statistical machine learning. In this
paper, the problem of learning over time is rethought from scratch, leveraging
tools from optimal control theory, which yield a unifying view of the temporal
dynamics of neural computations and learning. Hamiltonian Learning is based on
differential equations that: (i) can be integrated without the need of external
software solvers; (ii) generalize the well-established notion of gradient-based
learning in feed-forward and recurrent networks; (iii) open to novel
perspectives. The proposed framework is showcased by experimentally proving how
it can recover gradient-based learning, comparing it to out-of-the box
optimizers, and describing how it is flexible enough to switch from fully-local
to partially/non-local computational schemes, possibly distributed over
multiple devices, and BackPropagation without storing activations. Hamiltonian
Learning is easy to implement and can help researches approach in a principled
and innovative manner the problem of learning over time.",2024-09-18,"Stefano Melacci, Alessandro Betti, Michele Casoni, Tommaso Guidi, Matteo Tiezzi, Marco Gori",http://arxiv.org/pdf/2409.12038v1,cs.LG
Topological Deep Learning with State-Space Models: A Mamba Approach for Simplicial Complexes,"Graph Neural Networks based on the message-passing (MP) mechanism are a
dominant approach for handling graph-structured data. However, they are
inherently limited to modeling only pairwise interactions, making it difficult
to explicitly capture the complexity of systems with $n$-body relations. To
address this, topological deep learning has emerged as a promising field for
studying and modeling higher-order interactions using various topological
domains, such as simplicial and cellular complexes. While these new domains
provide powerful representations, they introduce new challenges, such as
effectively modeling the interactions among higher-order structures through
higher-order MP. Meanwhile, structured state-space sequence models have proven
to be effective for sequence modeling and have recently been adapted for graph
data by encoding the neighborhood of a node as a sequence, thereby avoiding the
MP mechanism. In this work, we propose a novel architecture designed to operate
with simplicial complexes, utilizing the Mamba state-space model as its
backbone. Our approach generates sequences for the nodes based on the
neighboring cells, enabling direct communication between all higher-order
structures, regardless of their rank. We extensively validate our model,
demonstrating that it achieves competitive performance compared to
state-of-the-art models developed for simplicial complexes.",2024-09-18,"Marco Montagna, Simone Scardapane, Lev Telyatnikov",http://arxiv.org/pdf/2409.12033v1,cs.LG
On Vision Transformers for Classification Tasks in Side-Scan Sonar Imagery,"Side-scan sonar (SSS) imagery presents unique challenges in the
classification of man-made objects on the seafloor due to the complex and
varied underwater environments. Historically, experts have manually interpreted
SSS images, relying on conventional machine learning techniques with
hand-crafted features. While Convolutional Neural Networks (CNNs) significantly
advanced automated classification in this domain, they often fall short when
dealing with diverse seafloor textures, such as rocky or ripple sand bottoms,
where false positive rates may increase. Recently, Vision Transformers (ViTs)
have shown potential in addressing these limitations by utilizing a
self-attention mechanism to capture global information in image patches,
offering more flexibility in processing spatial hierarchies. This paper
rigorously compares the performance of ViT models alongside commonly used CNN
architectures, such as ResNet and ConvNext, for binary classification tasks in
SSS imagery. The dataset encompasses diverse geographical seafloor types and is
balanced between the presence and absence of man-made objects. ViT-based models
exhibit superior classification performance across f1-score, precision, recall,
and accuracy metrics, although at the cost of greater computational resources.
CNNs, with their inductive biases, demonstrate better computational efficiency,
making them suitable for deployment in resource-constrained environments like
underwater vehicles. Future research directions include exploring
self-supervised learning for ViTs and multi-modal fusion to further enhance
performance in challenging underwater environments.",2024-09-18,"BW Sheffield, Jeffrey Ellen, Ben Whitmore",http://arxiv.org/pdf/2409.12026v1,cs.LG
Promise and Peril of Collaborative Code Generation Models: Balancing Effectiveness and Memorization,"In the rapidly evolving field of machine learning, training models with
datasets from various locations and organizations presents significant
challenges due to privacy and legal concerns. The exploration of effective
collaborative training settings capable of leveraging valuable knowledge from
distributed and isolated datasets is increasingly crucial. This study
investigates key factors that impact the effectiveness of collaborative
training methods in code next-token prediction, as well as the correctness and
utility of the generated code, demonstrating the promise of such methods.
Additionally, we evaluate the memorization of different participant training
data across various collaborative training settings, including centralized,
federated, and incremental training, highlighting their potential risks in
leaking data. Our findings indicate that the size and diversity of code
datasets are pivotal factors influencing the success of collaboratively trained
code models. We show that federated learning achieves competitive performance
compared to centralized training while offering better data protection, as
evidenced by lower memorization ratios in the generated code. However,
federated learning can still produce verbatim code snippets from hidden
training data, potentially violating privacy or copyright. Our study further
explores effectiveness and memorization patterns in incremental learning,
emphasizing the sequence in which individual participant datasets are
introduced. We also identify cross-organizational clones as a prevalent
challenge in both centralized and federated learning scenarios. Our findings
highlight the persistent risk of data leakage during inference, even when
training data remains unseen. We conclude with recommendations for
practitioners and researchers to optimize multisource datasets, propelling
cross-organizational collaboration forward.",2024-09-18,"Zhi Chen, Lingxiao Jiang",http://arxiv.org/pdf/2409.12020v1,cs.LG
All-in-one foundational models learning across quantum chemical levels,"Machine learning (ML) potentials typically target a single quantum chemical
(QC) level while the ML models developed for multi-fidelity learning have not
been shown to provide scalable solutions for foundational models. Here we
introduce the all-in-one (AIO) ANI model architecture based on multimodal
learning which can learn an arbitrary number of QC levels. Our all-in-one
learning approach offers a more general and easier-to-use alternative to
transfer learning. We use it to train the AIO-ANI-UIP foundational model with
the generalization capability comparable to semi-empirical GFN2-xTB and DFT
with a double-zeta basis set for organic molecules. We show that the AIO-ANI
model can learn across different QC levels ranging from semi-empirical to
density functional theory to coupled cluster. We also use AIO models to design
the foundational model {\Delta}-AIO-ANI based on {\Delta}-learning with
increased accuracy and robustness compared to AIO-ANI-UIP. The code and the
foundational models are available at https://github.com/dralgroup/aio-ani; they
will be integrated into the universal and updatable AI-enhanced QM (UAIQM)
library and made available in the MLatom package so that they can be used
online at the XACS cloud computing platform (see
https://github.com/dralgroup/mlatom for updates).",2024-09-18,"Yuxinxin Chen, Pavlo O. Dral",http://arxiv.org/pdf/2409.12015v1,cs.LG
Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning,"Offline multi-agent reinforcement learning (MARL) is an exciting direction of
research that uses static datasets to find optimal control policies for
multi-agent systems. Though the field is by definition data-driven, efforts
have thus far neglected data in their drive to achieve state-of-the-art
results. We first substantiate this claim by surveying the literature, showing
how the majority of works generate their own datasets without consistent
methodology and provide sparse information about the characteristics of these
datasets. We then show why neglecting the nature of the data is problematic,
through salient examples of how tightly algorithmic performance is coupled to
the dataset used, necessitating a common foundation for experiments in the
field. In response, we take a big step towards improving data usage and data
awareness in offline MARL, with three key contributions: (1) a clear guideline
for generating novel datasets; (2) a standardisation of over 80 existing
datasets, hosted in a publicly available repository, using a consistent storage
format and easy-to-use API; and (3) a suite of analysis tools that allow us to
understand these datasets better, aiding further development.",2024-09-18,"Claude Formanek, Louise Beyers, Callum Rhys Tilbury, Jonathan P. Shock, Arnu Pretorius",http://arxiv.org/pdf/2409.12001v1,cs.LG
"""It Might be Technically Impressive, But It's Practically Useless to us"": Motivations, Practices, Challenges, and Opportunities for Cross-Functional Collaboration around AI within the News Industry","Recently, an increasing number of news organizations have integrated
artificial intelligence (AI) into their workflows, leading to a further influx
of AI technologists and data workers into the news industry. This has initiated
cross-functional collaborations between these professionals and journalists.
Although prior research has explored the impact of AI-related roles entering
the news industry, there is a lack of studies on how internal cross-functional
collaboration around AI unfolds between AI professionals and journalists within
the news industry. Through interviews with 17 journalists, six AI
technologists, and three AI workers with cross-functional experience from
leading Chinese news organizations, we investigate the practices, challenges,
and opportunities for internal cross-functional collaboration around AI in news
industry. We first study how these journalists and AI professionals perceive
existing internal cross-collaboration strategies. We explore the challenges of
cross-functional collaboration and provide recommendations for enhancing future
cross-functional collaboration around AI in the news industry.",2024-09-18,"Qing Xiao, Xianzhe Fan, Felix M. Simon, Bingbing Zhang, Motahhare Eslami",http://arxiv.org/pdf/2409.12000v2,cs.LG
Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes,"The loss landscape of neural networks is a critical aspect of their training,
and understanding its properties is essential for improving their performance.
In this paper, we investigate how the loss surface changes when the sample size
increases, a previously unexplored issue. We theoretically analyze the
convergence of the loss landscape in a fully connected neural network and
derive upper bounds for the difference in loss function values when adding a
new object to the sample. Our empirical study confirms these results on various
datasets, demonstrating the convergence of the loss function surface for image
classification tasks. Our findings provide insights into the local geometry of
neural loss landscapes and have implications for the development of sample size
determination techniques.",2024-09-18,"Nikita Kiselev, Andrey Grabovoy",http://arxiv.org/pdf/2409.11995v1,cs.LG
An Efficient Model-Agnostic Approach for Uncertainty Estimation in Data-Restricted Pedometric Applications,"This paper introduces a model-agnostic approach designed to enhance
uncertainty estimation in the predictive modeling of soil properties, a crucial
factor for advancing pedometrics and the practice of digital soil mapping. For
addressing the typical challenge of data scarcity in soil studies, we present
an improved technique for uncertainty estimation. This method is based on the
transformation of regression tasks into classification problems, which not only
allows for the production of reliable uncertainty estimates but also enables
the application of established machine learning algorithms with competitive
performance that have not yet been utilized in pedometrics. Empirical results
from datasets collected from two German agricultural fields showcase the
practical application of the proposed methodology. Our results and findings
suggest that the proposed approach has the potential to provide better
uncertainty estimation than the models commonly used in pedometrics.",2024-09-18,"Viacheslav Barkov, Jonas Schmidinger, Robin Gebbers, Martin Atzmueller",http://arxiv.org/pdf/2409.11985v1,cs.LG
Metric-Semantic Factor Graph Generation based on Graph Neural Networks,"Understanding the relationships between geometric structures and semantic
concepts is crucial for building accurate models of complex environments. In
indoors, certain spatial constraints, such as the relative positioning of
planes, remain consistent despite variations in layout. This paper explores how
these invariant relationships can be captured in a graph SLAM framework by
representing high-level concepts like rooms and walls, linking them to
geometric elements like planes through an optimizable factor graph. Several
efforts have tackled this issue with add-hoc solutions for each concept
generation and with manually-defined factors.
  This paper proposes a novel method for metric-semantic factor graph
generation which includes defining a semantic scene graph, integrating
geometric information, and learning the interconnecting factors, all based on
Graph Neural Networks (GNNs). An edge classification network (G-GNN) sorts the
edges between planes into same room, same wall or none types. The resulting
relations are clustered, generating a room or wall for each cluster. A second
family of networks (F-GNN) infers the geometrical origin of the new nodes. The
definition of the factors employs the same F-GNN used for the metric attribute
of the generated nodes. Furthermore, share the new factor graph with the
S-Graphs+ algorithm, extending its graph expressiveness and scene
representation with the ultimate goal of improving the SLAM performance. The
complexity of the environments is increased to N-plane rooms by training the
networks on L-shaped rooms. The framework is evaluated in synthetic and
simulated scenarios as no real datasets of the required complex layouts are
available.",2024-09-18,"Jose Andres Millan-Romera, Hriday Bavle, Muhammad Shaheer, Holger Voos, Jose Luis Sanchez-Lopez",http://arxiv.org/pdf/2409.11972v1,cs.LG
Efficacy of Synthetic Data as a Benchmark,"Large language models (LLMs) have enabled a range of applications in
zero-shot and few-shot learning settings, including the generation of synthetic
datasets for training and testing. However, to reliably use these synthetic
datasets, it is essential to understand how representative they are of
real-world data. We investigate this by assessing the effectiveness of
generating synthetic data through LLM and using it as a benchmark for various
NLP tasks. Our experiments across six datasets, and three different tasks, show
that while synthetic data can effectively capture performance of various
methods for simpler tasks, such as intent classification, it falls short for
more complex tasks like named entity recognition. Additionally, we propose a
new metric called the bias factor, which evaluates the biases introduced when
the same LLM is used to both generate benchmarking data and to perform the
tasks. We find that smaller LLMs exhibit biases towards their own generated
data, whereas larger models do not. Overall, our findings suggest that the
effectiveness of synthetic data as a benchmark varies depending on the task,
and that practitioners should rely on data generated from multiple larger
models whenever possible.",2024-09-18,"Gaurav Maheshwari, Dmitry Ivanov, Kevin El Haddad",http://arxiv.org/pdf/2409.11968v1,cs.LG
Data Efficient Acoustic Scene Classification using Teacher-Informed Confusing Class Instruction,"In this technical report, we describe the SNTL-NTU team's submission for Task
1 Data-Efficient Low-Complexity Acoustic Scene Classification of the detection
and classification of acoustic scenes and events (DCASE) 2024 challenge. Three
systems are introduced to tackle training splits of different sizes. For small
training splits, we explored reducing the complexity of the provided baseline
model by reducing the number of base channels. We introduce data augmentation
in the form of mixup to increase the diversity of training samples. For the
larger training splits, we use FocusNet to provide confusing class information
to an ensemble of multiple Patchout faSt Spectrogram Transformer (PaSST) models
and baseline models trained on the original sampling rate of 44.1 kHz. We use
Knowledge Distillation to distill the ensemble model to the baseline student
model. Training the systems on the TAU Urban Acoustic Scene 2022 Mobile
development dataset yielded the highest average testing accuracy of (62.21,
59.82, 56.81, 53.03, 47.97)% on split (100, 50, 25, 10, 5)% respectively over
the three systems.",2024-09-18,"Jin Jie Sean Yeo, Ee-Leng Tan, Jisheng Bai, Santi Peksi, Woon-Seng Gan",http://arxiv.org/pdf/2409.11964v1,cs.LG
Combustion Condition Identification using a Decision Tree based Machine Learning Algorithm Applied to a Model Can Combustor with High Shear Swirl Injector,"Combustion is the primary process in gas turbine engines, where there is a
need for efficient air-fuel mixing to enhance performance. High-shear swirl
injectors are commonly used to improve fuel atomization and mixing, which are
key factors in determining combustion efficiency and emissions. However, under
certain conditions, combustors can experience thermoacoustic instability. In
this study, a decision tree-based machine learning algorithm is used to
classify combustion conditions by analyzing acoustic pressure and high-speed
flame imaging from a counter-rotating high-shear swirl injector of a single can
combustor fueled by methane. With a constant Reynolds number and varying
equivalence ratios, the combustor exhibits both stable and unstable states.
Characteristic features are extracted from the data using time series analysis,
providing insight into combustion dynamics. The trained supervised machine
learning model accurately classifies stable and unstable operations,
demonstrating effective prediction of combustion conditions within the studied
parameter range.",2024-09-18,"PK Archhith, SK Thirumalaikumaran, Balasundaram Mohan, Saptharshi Basu",http://arxiv.org/pdf/2409.15363v1,cs.LG
Reinforcement Learning with Lie Group Orientations for Robotics,"Handling orientations of robots and objects is a crucial aspect of many
applications. Yet, ever so often, there is a lack of mathematical correctness
when dealing with orientations, especially in learning pipelines involving, for
example, artificial neural networks. In this paper, we investigate
reinforcement learning with orientations and propose a simple modification of
the network's input and output that adheres to the Lie group structure of
orientations. As a result, we obtain an easy and efficient implementation that
is directly usable with existing learning libraries and achieves significantly
better performance than other common orientation representations. We briefly
introduce Lie theory specifically for orientations in robotics to motivate and
outline our approach. Subsequently, a thorough empirical evaluation of
different combinations of orientation representations for states and actions
demonstrates the superior performance of our proposed approach in different
scenarios, including: direct orientation control, end effector orientation
control, and pick-and-place tasks.",2024-09-18,"Martin Schuck, Jan Brüdigam, Sandra Hirche, Angela Schoellig",http://arxiv.org/pdf/2409.11935v2,cs.LG
Reinforcement Learning as an Improvement Heuristic for Real-World Production Scheduling,"The integration of Reinforcement Learning (RL) with heuristic methods is an
emerging trend for solving optimization problems, which leverages RL's ability
to learn from the data generated during the search process. One promising
approach is to train an RL agent as an improvement heuristic, starting with a
suboptimal solution that is iteratively improved by applying small changes. We
apply this approach to a real-world multiobjective production scheduling
problem. Our approach utilizes a network architecture that includes Transformer
encoding to learn the relationships between jobs. Afterwards, a probability
matrix is generated from which pairs of jobs are sampled and then swapped to
improve the solution. We benchmarked our approach against other heuristics
using real data from our industry partner, demonstrating its superior
performance.",2024-09-18,"Arthur Müller, Lukas Vollenkemper",http://arxiv.org/pdf/2409.11933v1,cs.LG
An Explainable Machine Learning Approach to Traffic Accident Fatality Prediction,"Road traffic accidents (RTA) pose a significant public health threat
worldwide, leading to considerable loss of life and economic burdens. This is
particularly acute in developing countries like Bangladesh. Building reliable
models to forecast crash outcomes is crucial for implementing effective
preventive measures. To aid in developing targeted safety interventions, this
study presents a machine learning-based approach for classifying fatal and
non-fatal road accident outcomes using data from the Dhaka metropolitan traffic
crash database from 2017 to 2022. Our framework utilizes a range of machine
learning classification algorithms, comprising Logistic Regression, Support
Vector Machines, Naive Bayes, Random Forest, Decision Tree, Gradient Boosting,
LightGBM, and Artificial Neural Network. We prioritize model interpretability
by employing the SHAP (SHapley Additive exPlanations) method, which elucidates
the key factors influencing accident fatality. Our results demonstrate that
LightGBM outperforms other models, achieving a ROC-AUC score of 0.72. The
global, local, and feature dependency analyses are conducted to acquire deeper
insights into the behavior of the model. SHAP analysis reveals that casualty
class, time of accident, location, vehicle type, and road type play pivotal
roles in determining fatality risk. These findings offer valuable insights for
policymakers and road safety practitioners in developing countries, enabling
the implementation of evidence-based strategies to reduce traffic crash
fatalities.",2024-09-18,"Md. Asif Khan Rifat, Ahmedul Kabir, Armana Sabiha Huq",http://arxiv.org/pdf/2409.11929v1,cs.LG
Generation of Complex 3D Human Motion by Temporal and Spatial Composition of Diffusion Models,"In this paper, we address the challenge of generating realistic 3D human
motions for action classes that were never seen during the training phase. Our
approach involves decomposing complex actions into simpler movements,
specifically those observed during training, by leveraging the knowledge of
human motion contained in GPTs models. These simpler movements are then
combined into a single, realistic animation using the properties of diffusion
models. Our claim is that this decomposition and subsequent recombination of
simple movements can synthesize an animation that accurately represents the
complex input action. This method operates during the inference phase and can
be integrated with any pre-trained diffusion model, enabling the synthesis of
motion classes not present in the training data. We evaluate our method by
dividing two benchmark human motion datasets into basic and complex actions,
and then compare its performance against the state-of-the-art.",2024-09-18,"Lorenzo Mandelli, Stefano Berretti",http://arxiv.org/pdf/2409.11920v1,cs.LG
SemAI: Semantic Artificial Intelligence-enhanced DNA storage for Internet-of-Things,"In the wake of the swift evolution of technologies such as the Internet of
Things (IoT), the global data landscape undergoes an exponential surge,
propelling DNA storage into the spotlight as a prospective medium for
contemporary cloud storage applications. This paper introduces a Semantic
Artificial Intelligence-enhanced DNA storage (SemAI-DNA) paradigm,
distinguishing itself from prevalent deep learning-based methodologies through
two key modifications: 1) embedding a semantic extraction module at the
encoding terminus, facilitating the meticulous encoding and storage of nuanced
semantic information; 2) conceiving a forethoughtful multi-reads filtering
model at the decoding terminus, leveraging the inherent multi-copy propensity
of DNA molecules to bolster system fault tolerance, coupled with a
strategically optimized decoder's architectural framework. Numerical results
demonstrate the SemAI-DNA's efficacy, attaining 2.61 dB Peak Signal-to-Noise
Ratio (PSNR) gain and 0.13 improvement in Structural Similarity Index (SSIM)
over conventional deep learning-based approaches.",2024-09-18,"Wenfeng Wu, Luping Xiang, Qiang Liu, Kun Yang",http://arxiv.org/pdf/2409.12213v1,cs.LG
Less Memory Means smaller GPUs: Backpropagation with Compressed Activations,"The ever-growing scale of deep neural networks (DNNs) has lead to an equally
rapid growth in computational resource requirements. Many recent architectures,
most prominently Large Language Models, have to be trained using supercomputers
with thousands of accelerators, such as GPUs or TPUs. Next to the vast number
of floating point operations the memory footprint of DNNs is also exploding. In
contrast, GPU architectures are notoriously short on memory. Even comparatively
small architectures like some EfficientNet variants cannot be trained on a
single consumer-grade GPU at reasonable mini-batch sizes. During training,
intermediate input activations have to be stored until backpropagation for
gradient calculation. These make up the vast majority of the memory footprint.
In this work we therefore consider compressing activation maps for the backward
pass using pooling, which can reduce both the memory footprint and amount of
data movement. The forward computation remains uncompressed. We empirically
show convergence and study effects on feature detection at the example of the
common vision architecture ResNet. With this approach we are able to reduce the
peak memory consumption by 29% at the cost of a longer training schedule, while
maintaining prediction accuracy compared to an uncompressed baseline.",2024-09-18,"Daniel Barley, Holger Fröning",http://arxiv.org/pdf/2409.11902v1,cs.LG
Multi-Grid Graph Neural Networks with Self-Attention for Computational Mechanics,"Advancement in finite element methods have become essential in various
disciplines, and in particular for Computational Fluid Dynamics (CFD), driving
research efforts for improved precision and efficiency. While Convolutional
Neural Networks (CNNs) have found success in CFD by mapping meshes into images,
recent attention has turned to leveraging Graph Neural Networks (GNNs) for
direct mesh processing. This paper introduces a novel model merging
Self-Attention with Message Passing in GNNs, achieving a 15\% reduction in RMSE
on the well known flow past a cylinder benchmark. Furthermore, a dynamic mesh
pruning technique based on Self-Attention is proposed, that leads to a robust
GNN-based multigrid approach, also reducing RMSE by 15\%. Additionally, a new
self-supervised training method based on BERT is presented, resulting in a 25\%
RMSE reduction. The paper includes an ablation study and outperforms
state-of-the-art models on several challenging datasets, promising advancements
similar to those recently achieved in natural language and image processing.
Finally, the paper introduces a dataset with meshes larger than existing ones
by at least an order of magnitude. Code and Datasets will be released at
https://github.com/DonsetPG/multigrid-gnn.",2024-09-18,"Paul Garnier, Jonathan Viquerat, Elie Hachem",http://arxiv.org/pdf/2409.11899v1,cs.LG
Secure Control Systems for Autonomous Quadrotors against Cyber-Attacks,"The problem of safety for robotic systems has been extensively studied.
However, little attention has been given to security issues for
three-dimensional systems, such as quadrotors. Malicious adversaries can
compromise robot sensors and communication networks, causing incidents,
achieving illegal objectives, or even injuring people. This study first designs
an intelligent control system for autonomous quadrotors. Then, it investigates
the problems of optimal false data injection attack scheduling and
countermeasure design for unmanned aerial vehicles. Using a state-of-the-art
deep learning-based approach, an optimal false data injection attack scheme is
proposed to deteriorate a quadrotor's tracking performance with limited attack
energy. Subsequently, an optimal tracking control strategy is learned to
mitigate attacks and recover the quadrotor's tracking performance. We base our
work on Agilicious, a state-of-the-art quadrotor recently deployed for
autonomous settings. This paper is the first in the United Kingdom to deploy
this quadrotor and implement reinforcement learning on its platform. Therefore,
to promote easy reproducibility with minimal engineering overhead, we further
provide (1) a comprehensive breakdown of this quadrotor, including software
stacks and hardware alternatives; (2) a detailed reinforcement-learning
framework to train autonomous controllers on Agilicious agents; and (3) a new
open-source environment that builds upon PyFlyt for future reinforcement
learning research on Agilicious platforms. Both simulated and real-world
experiments are conducted to show the effectiveness of the proposed frameworks
in section 5.2.",2024-09-18,Samuel Belkadi,http://arxiv.org/pdf/2409.11897v1,cs.LG
Recent Advances in OOD Detection: Problems and Approaches,"Out-of-distribution (OOD) detection aims to detect test samples outside the
training category space, which is an essential component in building reliable
machine learning systems. Existing reviews on OOD detection primarily focus on
method taxonomy, surveying the field by categorizing various approaches.
However, many recent works concentrate on non-traditional OOD detection
scenarios, such as test-time adaptation, multi-modal data sources and other
novel contexts. In this survey, we uniquely review recent advances in OOD
detection from the problem scenario perspective for the first time. According
to whether the training process is completely controlled, we divide OOD
detection methods into training-driven and training-agnostic. Besides,
considering the rapid development of pre-trained models, large pre-trained
model-based OOD detection is also regarded as an important category and
discussed separately. Furthermore, we provide a discussion of the evaluation
scenarios, a variety of applications, and several future research directions.
We believe this survey with new taxonomy will benefit the proposal of new
methods and the expansion of more practical scenarios. A curated list of
related papers is provided in the Github repository:
https://github.com/shuolucs/Awesome-Out-Of-Distribution-Detection",2024-09-18,"Shuo Lu, Yingsheng Wang, Lijun Sheng, Aihua Zheng, Lingxiao He, Jian Liang",http://arxiv.org/pdf/2409.11884v2,cs.LG
Location based Probabilistic Load Forecasting of EV Charging Sites: Deep Transfer Learning with Multi-Quantile Temporal Convolutional Network,"Electrification of vehicles is a potential way of reducing fossil fuel usage
and thus lessening environmental pollution. Electric Vehicles (EVs) of various
types for different transport modes (including air, water, and land) are
evolving. Moreover, different EV user groups (commuters, commercial or domestic
users, drivers) may use different charging infrastructures (public, private,
home, and workplace) at various times. Therefore, usage patterns and energy
demand are very stochastic. Characterizing and forecasting the charging demand
of these diverse EV usage profiles is essential in preventing power outages.
Previously developed data-driven load models are limited to specific use cases
and locations. None of these models are simultaneously adaptive enough to
transfer knowledge of day-ahead forecasting among EV charging sites of diverse
locations, trained with limited data, and cost-effective. This article presents
a location-based load forecasting of EV charging sites using a deep
Multi-Quantile Temporal Convolutional Network (MQ-TCN) to overcome the
limitations of earlier models. We conducted our experiments on data from four
charging sites, namely Caltech, JPL, Office-1, and NREL, which have diverse EV
user types like students, full-time and part-time employees, random visitors,
etc. With a Prediction Interval Coverage Probability (PICP) score of 93.62\%,
our proposed deep MQ-TCN model exhibited a remarkable 28.93\% improvement over
the XGBoost model for a day-ahead load forecasting at the JPL charging site. By
transferring knowledge with the inductive Transfer Learning (TL) approach, the
MQ-TCN model achieved a 96.88\% PICP score for the load forecasting task at the
NREL site using only two weeks of data.",2024-09-18,"Mohammad Wazed Ali, Asif bin Mustafa, Md. Aukerul Moin Shuvo, Bernhard Sick",http://arxiv.org/pdf/2409.11862v1,cs.LG
Tight and Efficient Upper Bound on Spectral Norm of Convolutional Layers,"Controlling the spectral norm of the Jacobian matrix, which is related to the
convolution operation, has been shown to improve generalization, training
stability and robustness in CNNs. Existing methods for computing the norm
either tend to overestimate it or their performance may deteriorate quickly
with increasing the input and kernel sizes. In this paper, we demonstrate that
the tensor version of the spectral norm of a four-dimensional convolution
kernel, up to a constant factor, serves as an upper bound for the spectral norm
of the Jacobian matrix associated with the convolution operation. This new
upper bound is independent of the input image resolution, differentiable and
can be efficiently calculated during training. Through experiments, we
demonstrate how this new bound can be used to improve the performance of
convolutional architectures.",2024-09-18,"Ekaterina Grishina, Mikhail Gorbunov, Maxim Rakhuba",http://arxiv.org/pdf/2409.11859v1,cs.LG
Edge-Based Graph Component Pooling,"Graph-structured data naturally occurs in many research fields, such as
chemistry and sociology. The relational information contained therein can be
leveraged to statistically model graph properties through geometrical deep
learning. Graph neural networks employ techniques, such as message-passing
layers, to propagate local features through a graph. However, message-passing
layers can be computationally expensive when dealing with large and sparse
graphs. Graph pooling operators offer the possibility of removing or merging
nodes in such graphs, thus lowering computational costs. However, pooling
operators that remove nodes cause data loss, and pooling operators that merge
nodes are often computationally expensive. We propose a pooling operator that
merges nodes so as not to cause data loss but is also conceptually simple and
computationally inexpensive. We empirically demonstrate that the proposed
pooling operator performs statistically significantly better than edge pool on
four popular benchmark datasets while reducing time complexity and the number
of trainable parameters by 70.6% on average. Compared to another maximally
powerful method named Graph Isomporhic Network, we show that we outperform them
on two popular benchmark datasets while reducing the number of learnable
parameters on average by 60.9%.",2024-09-18,"T. Snelleman, B. M. Renting, H. H. Hoos, J. N. van Rijn",http://arxiv.org/pdf/2409.11856v1,cs.LG
An efficient wavelet-based physics-informed neural networks for singularly perturbed problems,"Physics-informed neural networks (PINNs) are a class of deep learning models
that utilize physics in the form of differential equations to address complex
problems, including ones that may involve limited data availability. However,
tackling solutions of differential equations with rapid oscillations, steep
gradients, or singular behavior becomes challenging for PINNs. Considering
these challenges, we designed an efficient wavelet-based PINNs (W-PINNs) model
to address this class of differential equations. Here, we represent the
solution in wavelet space using a family of smooth-compactly supported
wavelets. This framework represents the solution of a differential equation
with significantly fewer degrees of freedom while still retaining the dynamics
of complex physical phenomena. The architecture allows the training process to
search for a solution within the wavelet space, making the process faster and
more accurate. Further, the proposed model does not rely on automatic
differentiations for derivatives involved in differential equations and does
not require any prior information regarding the behavior of the solution, such
as the location of abrupt features. Thus, through a strategic fusion of
wavelets with PINNs, W-PINNs excel at capturing localized nonlinear
information, making them well-suited for problems showing abrupt behavior in
certain regions, such as singularly perturbed and multiscale problems. The
efficiency and accuracy of the proposed neural network model are demonstrated
in various 1D and 2D test problems, i.e., the FitzHugh-Nagumo (FHN) model, the
Helmholtz equation, the Maxwell's equation, lid-driven cavity flow, and the
Allen-Cahn equation, along with other highly singularly perturbed nonlinear
differential equations. The proposed model significantly improves with
traditional PINNs, recently developed wavelet-based PINNs, and other
state-of-the-art methods.",2024-09-18,"Himanshu Pandey, Anshima Singh, Ratikanta Behera",http://arxiv.org/pdf/2409.11847v2,cs.LG
Graph Neural Network-State Predictive Information Bottleneck (GNN-SPIB) approach for learning molecular thermodynamics and kinetics,"Molecular dynamics simulations offer detailed insights into atomic motions
but face timescale limitations. Enhanced sampling methods have addressed these
challenges but even with machine learning, they often rely on pre-selected
expert-based features. In this work, we present the Graph Neural Network-State
Predictive Information Bottleneck (GNN-SPIB) framework, which combines graph
neural networks and the State Predictive Information Bottleneck to
automatically learn low-dimensional representations directly from atomic
coordinates. Tested on three benchmark systems, our approach predicts essential
structural, thermodynamic and kinetic information for slow processes,
demonstrating robustness across diverse systems. The method shows promise for
complex systems, enabling effective enhanced sampling without requiring
pre-defined reaction coordinates or input features.",2024-09-18,"Ziyue Zou, Dedi Wang, Pratyush Tiwary",http://arxiv.org/pdf/2409.11843v1,cs.LG
"RaggeDi: Diffusion-based State Estimation of Disordered Rags, Sheets, Towels and Blankets","Cloth state estimation is an important problem in robotics. It is essential
for the robot to know the accurate state to manipulate cloth and execute tasks
such as robotic dressing, stitching, and covering/uncovering human beings.
However, estimating cloth state accurately remains challenging due to its high
flexibility and self-occlusion. This paper proposes a diffusion model-based
pipeline that formulates the cloth state estimation as an image generation
problem by representing the cloth state as an RGB image that describes the
point-wise translation (translation map) between a pre-defined flattened mesh
and the deformed mesh in a canonical space. Then we train a conditional
diffusion-based image generation model to predict the translation map based on
an observation. Experiments are conducted in both simulation and the real world
to validate the performance of our method. Results indicate that our method
outperforms two recent methods in both accuracy and speed.",2024-09-18,"Jikai Ye, Wanze Li, Shiraz Khan, Gregory S. Chirikjian",http://arxiv.org/pdf/2409.11831v1,cs.LG
"Optimizing Job Shop Scheduling in the Furniture Industry: A Reinforcement Learning Approach Considering Machine Setup, Batch Variability, and Intralogistics","This paper explores the potential application of Deep Reinforcement Learning
in the furniture industry. To offer a broad product portfolio, most furniture
manufacturers are organized as a job shop, which ultimately results in the Job
Shop Scheduling Problem (JSSP). The JSSP is addressed with a focus on extending
traditional models to better represent the complexities of real-world
production environments. Existing approaches frequently fail to consider
critical factors such as machine setup times or varying batch sizes. A concept
for a model is proposed that provides a higher level of information detail to
enhance scheduling accuracy and efficiency. The concept introduces the
integration of DRL for production planning, particularly suited to batch
production industries such as the furniture industry. The model extends
traditional approaches to JSSPs by including job volumes, buffer management,
transportation times, and machine setup times. This enables more precise
forecasting and analysis of production flows and processes, accommodating the
variability and complexity inherent in real-world manufacturing processes. The
RL agent learns to optimize scheduling decisions. It operates within a discrete
action space, making decisions based on detailed observations. A reward
function guides the agent's decision-making process, thereby promoting
efficient scheduling and meeting production deadlines. Two integration
strategies for implementing the RL agent are discussed: episodic planning,
which is suitable for low-automation environments, and continuous planning,
which is ideal for highly automated plants. While episodic planning can be
employed as a standalone solution, the continuous planning approach
necessitates the integration of the agent with ERP and Manufacturing Execution
Systems. This integration enables real-time adjustments to production schedules
based on dynamic changes.",2024-09-18,"Malte Schneevogt, Karsten Binninger, Noah Klarmann",http://arxiv.org/pdf/2409.11820v1,cs.LG
Accelerating the Training and Improving the Reliability of Machine-Learned Interatomic Potentials for Strongly Anharmonic Materials through Active Learning,"Molecular dynamics (MD) employing machine-learned interatomic potentials
(MLIPs) serve as an efficient, urgently needed complement to ab initio
molecular dynamics (aiMD). By training these potentials on data generated from
ab initio methods, their averaged predictions can exhibit comparable
performance to ab initio methods at a fraction of the cost. However,
insufficient training sets might lead to an improper description of the
dynamics in strongly anharmonic materials, because critical effects might be
overlooked in relevant cases, or only incorrectly captured, or hallucinated by
the MLIP when they are not actually present. In this work, we show that an
active learning scheme that combines MD with MLIPs (MLIP-MD) and uncertainty
estimates can avoid such problematic predictions. In short, efficient MLIP-MD
is used to explore configuration space quickly, whereby an acquisition function
based on uncertainty estimates and on energetic viability is employed to
maximize the value of the newly generated data and to focus on the most
unfamiliar but reasonably accessible regions of phase space. To verify our
methodology, we screen over 112 materials and identify 10 examples experiencing
the aforementioned problems. Using CuI and AgGaSe$_2$ as archetypes for these
problematic materials, we discuss the physical implications for strongly
anharmonic effects and demonstrate how the developed active learning scheme can
address these issues.",2024-09-18,"Kisung Kang, Thomas A. R. Purcell, Christian Carbogno, Matthias Scheffler",http://arxiv.org/pdf/2409.11808v1,cs.LG
Constraint Guided AutoEncoders for Joint Optimization of Condition Indicator Estimation and Anomaly Detection in Machine Condition Monitoring,"The main goal of machine condition monitoring is, as the name implies, to
monitor the condition of industrial applications. The objective of this
monitoring can be mainly split into two problems. A diagnostic problem, where
normal data should be distinguished from anomalous data, otherwise called
Anomaly Detection (AD), or a prognostic problem, where the aim is to predict
the evolution of a Condition Indicator (CI) that reflects the condition of an
asset throughout its life time. When considering machine condition monitoring,
it is expected that this CI shows a monotonic behavior, as the condition of a
machine gradually degrades over time. This work proposes an extension to
Constraint Guided AutoEncoders (CGAE), which is a robust AD method, that
enables building a single model that can be used for both AD and CI estimation.
For the purpose of improved CI estimation the extension incorporates a
constraint that enforces the model to have monotonically increasing CI
predictions over time. Experimental results indicate that the proposed
algorithm performs similar, or slightly better, than CGAE, with regards to AD,
while improving the monotonic behavior of the CI.",2024-09-18,"Maarten Meire, Quinten Van Baelen, Ted Ooijevaar, Peter Karsmakers",http://arxiv.org/pdf/2409.11807v1,cs.LG
The Factuality of Large Language Models in the Legal Domain,"This paper investigates the factuality of large language models (LLMs) as
knowledge bases in the legal domain, in a realistic usage scenario: we allow
for acceptable variations in the answer, and let the model abstain from
answering when uncertain. First, we design a dataset of diverse factual
questions about case law and legislation. We then use the dataset to evaluate
several LLMs under different evaluation methods, including exact, alias, and
fuzzy matching. Our results show that the performance improves significantly
under the alias and fuzzy matching methods. Further, we explore the impact of
abstaining and in-context examples, finding that both strategies enhance
precision. Finally, we demonstrate that additional pre-training on legal
documents, as seen with SaulLM, further improves factual precision from 63% to
81%.",2024-09-18,"Rajaa El Hamdani, Thomas Bonald, Fragkiskos Malliaros, Nils Holzenberger, Fabian Suchanek",http://arxiv.org/pdf/2409.11798v1,cs.LG
Mixture of Diverse Size Experts,"The Sparsely-Activated Mixture-of-Experts (MoE) has gained increasing
popularity for scaling up large language models (LLMs) without exploding
computational costs. Despite its success, the current design faces a challenge
where all experts have the same size, limiting the ability of tokens to choose
the experts with the most appropriate size for generating the next token. In
this paper, we propose the Mixture of Diverse Size Experts (MoDSE), a new MoE
architecture with layers designed to have experts of different sizes. Our
analysis of difficult token generation tasks shows that experts of various
sizes achieve better predictions, and the routing path of the experts tends to
be stable after a training period. However, having experts of diverse sizes can
lead to uneven workload distribution. To tackle this limitation, we introduce
an expert-pair allocation strategy to evenly distribute the workload across
multiple GPUs. Comprehensive evaluations across multiple benchmarks demonstrate
the effectiveness of MoDSE, as it outperforms existing MoEs by allocating the
parameter budget to experts adaptively while maintaining the same total
parameter size and the number of experts.",2024-09-18,"Manxi Sun, Wei Liu, Jian Luan, Pengzhi Gao, Bin Wang",http://arxiv.org/pdf/2409.12210v1,cs.LG
Multitask Mayhem: Unveiling and Mitigating Safety Gaps in LLMs Fine-tuning,"Recent breakthroughs in Large Language Models (LLMs) have led to their
adoption across a wide range of tasks, ranging from code generation to machine
translation and sentiment analysis, etc. Red teaming/Safety alignment efforts
show that fine-tuning models on benign (non-harmful) data could compromise
safety. However, it remains unclear to what extent this phenomenon is
influenced by different variables, including fine-tuning task, model
calibrations, etc. This paper explores the task-wise safety degradation due to
fine-tuning on downstream tasks such as summarization, code generation,
translation, and classification across various calibration. Our results reveal
that: 1) Fine-tuning LLMs for code generation and translation leads to the
highest degradation in safety guardrails. 2) LLMs generally have weaker
guardrails for translation and classification, with 73-92% of harmful prompts
answered, across baseline and other calibrations, falling into one of two
concern categories. 3) Current solutions, including guards and safety tuning
datasets, lack cross-task robustness. To address these issues, we developed a
new multitask safety dataset effectively reducing attack success rates across a
range of tasks without compromising the model's overall helpfulness. Our work
underscores the need for generalized alignment measures to ensure safer and
more robust models.",2024-09-18,"Essa Jan, Nouar AlDahoul, Moiz Ali, Faizan Ahmad, Fareed Zaffar, Yasir Zaki",http://arxiv.org/pdf/2409.15361v1,cs.LG
Symmetry-Based Structured Matrices for Efficient Approximately Equivariant Networks,"There has been much recent interest in designing neural networks (NNs) with
relaxed equivariance, which interpolate between exact equivariance and full
flexibility for consistent performance gains. In a separate line of work,
structured parameter matrices with low displacement rank (LDR) -- which permit
fast function and gradient evaluation -- have been used to create compact NNs,
though primarily benefiting classical convolutional neural networks (CNNs). In
this work, we propose a framework based on symmetry-based structured matrices
to build approximately equivariant NNs with fewer parameters. Our approach
unifies the aforementioned areas using Group Matrices (GMs), a forgotten
precursor to the modern notion of regular representations of finite groups. GMs
allow the design of structured matrices similar to LDR matrices, which can
generalize all the elementary operations of a CNN from cyclic groups to
arbitrary finite groups. We show GMs can also generalize classical LDR theory
to general discrete groups, enabling a natural formalism for approximate
equivariance. We test GM-based architectures on various tasks with relaxed
symmetry and find that our framework performs competitively with approximately
equivariant NNs and other structured matrix-based methods, often with one to
two orders of magnitude fewer parameters.",2024-09-18,"Ashwin Samudre, Mircea Petrache, Brian D. Nord, Shubhendu Trivedi",http://arxiv.org/pdf/2409.11772v2,cs.LG
DifFaiRec: Generative Fair Recommender with Conditional Diffusion Model,"Although recommenders can ship items to users automatically based on the
users' preferences, they often cause unfairness to groups or individuals. For
instance, when users can be divided into two groups according to a sensitive
social attribute and there is a significant difference in terms of activity
between the two groups, the learned recommendation algorithm will result in a
recommendation gap between the two groups, which causes group unfairness. In
this work, we propose a novel recommendation algorithm named Diffusion-based
Fair Recommender (DifFaiRec) to provide fair recommendations. DifFaiRec is
built upon the conditional diffusion model and hence has a strong ability to
learn the distribution of user preferences from their ratings on items and is
able to generate diverse recommendations effectively. To guarantee fairness, we
design a counterfactual module to reduce the model sensitivity to protected
attributes and provide mathematical explanations. The experiments on benchmark
datasets demonstrate the superiority of DifFaiRec over competitive baselines.",2024-09-18,"Zhenhao Jiang, Jicong Fan",http://arxiv.org/pdf/2410.02791v1,cs.LG
Consistent Estimation of a Class of Distances Between Covariance Matrices,"This work considers the problem of estimating the distance between two
covariance matrices directly from the data. Particularly, we are interested in
the family of distances that can be expressed as sums of traces of functions
that are separately applied to each covariance matrix. This family of distances
is particularly useful as it takes into consideration the fact that covariance
matrices lie in the Riemannian manifold of positive definite matrices, thereby
including a variety of commonly used metrics, such as the Euclidean distance,
Jeffreys' divergence, and the log-Euclidean distance. Moreover, a statistical
analysis of the asymptotic behavior of this class of distance estimators has
also been conducted. Specifically, we present a central limit theorem that
establishes the asymptotic Gaussianity of these estimators and provides closed
form expressions for the corresponding means and variances. Empirical
evaluations demonstrate the superiority of our proposed consistent estimator
over conventional plug-in estimators in multivariate analytical contexts.
Additionally, the central limit theorem derived in this study provides a robust
statistical framework to assess of accuracy of these estimators.",2024-09-18,"Roberto Pereira, Xavier Mestre, Davig Gregoratti",http://arxiv.org/pdf/2409.11761v1,cs.LG
NPAT Null-Space Projected Adversarial Training Towards Zero Deterioration,"To mitigate the susceptibility of neural networks to adversarial attacks,
adversarial training has emerged as a prevalent and effective defense strategy.
Intrinsically, this countermeasure incurs a trade-off, as it sacrifices the
model's accuracy in processing normal samples. To reconcile the trade-off, we
pioneer the incorporation of null-space projection into adversarial training
and propose two innovative Null-space Projection based Adversarial
Training(NPAT) algorithms tackling sample generation and gradient optimization,
named Null-space Projected Data Augmentation (NPDA) and Null-space Projected
Gradient Descent (NPGD), to search for an overarching optimal solutions, which
enhance robustness with almost zero deterioration in generalization
performance. Adversarial samples and perturbations are constrained within the
null-space of the decision boundary utilizing a closed-form null-space
projector, effectively mitigating threat of attack stemming from unreliable
features. Subsequently, we conducted experiments on the CIFAR10 and SVHN
datasets and reveal that our methodology can seamlessly combine with
adversarial training methods and obtain comparable robustness while keeping
generalization close to a high-accuracy model.",2024-09-18,"Hanyi Hu, Qiao Han, Kui Chen, Yao Yang",http://arxiv.org/pdf/2409.11754v1,cs.LG
HARP: Human-Assisted Regrouping with Permutation Invariant Critic for Multi-Agent Reinforcement Learning,"Human-in-the-loop reinforcement learning integrates human expertise to
accelerate agent learning and provide critical guidance and feedback in complex
fields. However, many existing approaches focus on single-agent tasks and
require continuous human involvement during the training process, significantly
increasing the human workload and limiting scalability. In this paper, we
propose HARP (Human-Assisted Regrouping with Permutation Invariant Critic), a
multi-agent reinforcement learning framework designed for group-oriented tasks.
HARP integrates automatic agent regrouping with strategic human assistance
during deployment, enabling and allowing non-experts to offer effective
guidance with minimal intervention. During training, agents dynamically adjust
their groupings to optimize collaborative task completion. When deployed, they
actively seek human assistance and utilize the Permutation Invariant Group
Critic to evaluate and refine human-proposed groupings, allowing non-expert
users to contribute valuable suggestions. In multiple collaboration scenarios,
our approach is able to leverage limited guidance from non-experts and enhance
performance. The project can be found at https://github.com/huawen-hu/HARP.",2024-09-18,"Huawen Hu, Enze Shi, Chenxi Yue, Shuocun Yang, Zihao Wu, Yiwei Li, Tianyang Zhong, Tuo Zhang, Tianming Liu, Shu Zhang",http://arxiv.org/pdf/2409.11741v1,cs.LG
Raising the Bar(ometer): Identifying a User's Stair and Lift Usage Through Wearable Sensor Data Analysis,"Many users are confronted multiple times daily with the choice of whether to
take the stairs or the elevator. Whereas taking the stairs could be beneficial
for cardiovascular health and wellness, taking the elevator might be more
convenient but it also consumes energy. By precisely tracking and boosting
users' stairs and elevator usage through their wearable, users might gain
health insights and motivation, encouraging a healthy lifestyle and lowering
the risk of sedentary-related health problems. This research describes a new
exploratory dataset, to examine the patterns and behaviors related to using
stairs and lifts. We collected data from 20 participants while climbing and
descending stairs and taking a lift in a variety of scenarios. The aim is to
provide insights and demonstrate the practicality of using wearable sensor data
for such a scenario. Our collected dataset was used to train and test a Random
Forest machine learning model, and the results show that our method is highly
accurate at classifying stair and lift operations with an accuracy of 87.61%
and a multi-class weighted F1-score of 87.56% over 8-second time windows.
Furthermore, we investigate the effect of various types of sensors and data
attributes on the model's performance. Our findings show that combining
inertial and pressure sensors yields a viable solution for real-time activity
detection.",2024-09-18,"Hrishikesh Balkrishna Karande, Ravikiran Arasur Thippeswamy Shivalingappa, Abdelhafid Nassim Yaici, Iman Haghbin, Niravkumar Bavadiya, Robin Burchard, Kristof Van Laerhoven",http://arxiv.org/pdf/2410.02790v1,cs.LG
Memory Networks: Towards Fully Biologically Plausible Learning,"The field of artificial intelligence faces significant challenges in
achieving both biological plausibility and computational efficiency,
particularly in visual learning tasks. Current artificial neural networks, such
as convolutional neural networks, rely on techniques like backpropagation and
weight sharing, which do not align with the brain's natural information
processing methods. To address these issues, we propose the Memory Network, a
model inspired by biological principles that avoids backpropagation and
convolutions, and operates in a single pass. This approach enables rapid and
efficient learning, mimicking the brain's ability to adapt quickly with minimal
exposure to data. Our experiments demonstrate that the Memory Network achieves
efficient and biologically plausible learning, showing strong performance on
simpler datasets like MNIST. However, further refinement is needed for the
model to handle more complex datasets such as CIFAR10, highlighting the need to
develop new algorithms and techniques that closely align with biological
processes while maintaining computational efficiency.",2024-09-18,"Jacobo Ruiz, Manas Gupta",http://arxiv.org/pdf/2409.17282v1,cs.LG
From exponential to finite/fixed-time stability: Applications to optimization,"The development of finite/fixed-time stable optimization algorithms typically
involves study of specific problem instances. The lack of a unified framework
hinders understanding of more sophisticated algorithms, e.g., primal-dual
gradient flow dynamics. The purpose of this paper is to address the following
question: Given an exponentially stable optimization algorithm, can it be
modified to obtain a finite/fixed-time stable algorithm? We provide an
affirmative answer, demonstrate how the solution can be computed on a
finite-time interval via a simple scaling of the right-hand-side of the
original dynamics, and certify the desired properties of the modified algorithm
using the Lyapunov function that proves exponential stability of the original
system. Finally, we examine nonsmooth composite optimization problems and
smooth problems with linear constraints to demonstrate the merits of our
approach.",2024-09-18,"Ibrahim K. Ozaslan, Mihailo R. Jovanović",http://arxiv.org/pdf/2409.11713v1,cs.LG
From Lists to Emojis: How Format Bias Affects Model Alignment,"In this paper, we study format biases in reinforcement learning from human
feedback (RLHF). We observe that many widely-used preference models, including
human evaluators, GPT-4, and top-ranking models on the RewardBench benchmark,
exhibit strong biases towards specific format patterns, such as lists, links,
bold text, and emojis. Furthermore, large language models (LLMs) can exploit
these biases to achieve higher rankings on popular benchmarks like AlpacaEval
and LMSYS Chatbot Arena. One notable example of this is verbosity bias, where
current preference models favor longer responses that appear more
comprehensive, even when their quality is equal to or lower than shorter,
competing responses. However, format biases beyond verbosity remain largely
underexplored in the literature. In this work, we extend the study of biases in
preference learning beyond the commonly recognized length bias, offering a
comprehensive analysis of a wider range of format biases. Additionally, we show
that with a small amount of biased data (less than 1%), we can inject
significant bias into the reward model. Moreover, these format biases can also
be easily exploited by downstream alignment algorithms, such as best-of-n
sampling and online iterative DPO, as it is usually easier to manipulate the
format than to improve the quality of responses. Our findings emphasize the
need to disentangle format and content both for designing alignment algorithms
and evaluating models.",2024-09-18,"Xuanchang Zhang, Wei Xiong, Lichang Chen, Tianyi Zhou, Heng Huang, Tong Zhang",http://arxiv.org/pdf/2409.11704v2,cs.LG
Bushfire Severity Modelling and Future Trend Prediction Across Australia: Integrating Remote Sensing and Machine Learning,"Bushfire is one of the major natural disasters that cause huge losses to
livelihoods and the environment. Understanding and analyzing the severity of
bushfires is crucial for effective management and mitigation strategies,
helping to prevent the extensive damage and loss caused by these natural
disasters. This study presents an in-depth analysis of bushfire severity in
Australia over the last twelve years, combining remote sensing data and machine
learning techniques to predict future fire trends. By utilizing Landsat imagery
and integrating spectral indices like NDVI, NBR, and Burn Index, along with
topographical and climatic factors, we developed a robust predictive model
using XGBoost. The model achieved high accuracy, 86.13%, demonstrating its
effectiveness in predicting fire severity across diverse Australian ecosystems.
By analyzing historical trends and integrating factors such as population
density and vegetation cover, we identify areas at high risk of future severe
bushfires. Additionally, this research identifies key regions at risk,
providing data-driven recommendations for targeted firefighting efforts. The
findings contribute valuable insights into fire management strategies,
enhancing resilience to future fire events in Australia. Also, we propose
future work on developing a UAV-based swarm coordination model to enhance fire
prediction in real-time and firefighting capabilities in the most vulnerable
regions.",2024-09-18,"Shouthiri Partheepan, Farzad Sanati, Jahan Hassan",http://arxiv.org/pdf/2410.02963v1,cs.LG
Monomial Matrix Group Equivariant Neural Functional Networks,"Neural functional networks (NFNs) have recently gained significant attention
due to their diverse applications, ranging from predicting network
generalization and network editing to classifying implicit neural
representation. Previous NFN designs often depend on permutation symmetries in
neural networks' weights, which traditionally arise from the unordered
arrangement of neurons in hidden layers. However, these designs do not take
into account the weight scaling symmetries of $\ReLU$ networks, and the weight
sign flipping symmetries of $\sin$ or $\Tanh$ networks. In this paper, we
extend the study of the group action on the network weights from the group of
permutation matrices to the group of monomial matrices by incorporating
scaling/sign-flipping symmetries. Particularly, we encode these
scaling/sign-flipping symmetries by designing our corresponding equivariant and
invariant layers. We name our new family of NFNs the Monomial Matrix Group
Equivariant Neural Functional Networks (Monomial-NFN). Because of the expansion
of the symmetries, Monomial-NFN has much fewer independent trainable parameters
compared to the baseline NFNs in the literature, thus enhancing the model's
efficiency. Moreover, for fully connected and convolutional neural networks, we
theoretically prove that all groups that leave these networks invariant while
acting on their weight spaces are some subgroups of the monomial matrix group.
We provide empirical evidence to demonstrate the advantages of our model over
existing baselines, achieving competitive performance and efficiency.",2024-09-18,"Viet-Hoang Tran, Thieu N. Vo, Tho H. Tran, An T. Nguyen, Tan M. Nguyen",http://arxiv.org/pdf/2409.11697v3,cs.LG
Automated detection of underdiagnosed medical conditions via opportunistic imaging,"Abdominal computed tomography (CT) scans are frequently performed in clinical
settings. Opportunistic CT involves repurposing routine CT images to extract
diagnostic information and is an emerging tool for detecting underdiagnosed
conditions such as sarcopenia, hepatic steatosis, and ascites. This study
utilizes deep learning methods to promote accurate diagnosis and clinical
documentation. We analyze 2,674 inpatient CT scans to identify discrepancies
between imaging phenotypes (characteristics derived from opportunistic CT
scans) and their corresponding documentation in radiology reports and ICD
coding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans
diagnosed with sarcopenia, hepatic steatosis, and ascites (respectively)
through either opportunistic imaging or radiology reports were ICD-coded. Our
findings demonstrate opportunistic CT's potential to enhance diagnostic
precision and accuracy of risk adjustment models, offering advancements in
precision medicine.",2024-09-18,"Asad Aali, Andrew Johnston, Louis Blankemeier, Dave Van Veen, Laura T Derry, David Svec, Jason Hom, Robert D. Boutin, Akshay S. Chaudhari",http://arxiv.org/pdf/2409.11686v3,cs.LG
Recurrent Interpolants for Probabilistic Time Series Prediction,"Sequential models like recurrent neural networks and transformers have become
standard for probabilistic multivariate time series forecasting across various
domains. Despite their strengths, they struggle with capturing high-dimensional
distributions and cross-feature dependencies. Recent work explores generative
approaches using diffusion or flow-based models, extending to time series
imputation and forecasting. However, scalability remains a challenge. This work
proposes a novel method combining recurrent neural networks' efficiency with
diffusion models' probabilistic modeling, based on stochastic interpolants and
conditional generation with control features, offering insights for future
developments in this dynamic field.",2024-09-18,"Yu Chen, Marin Biloš, Sarthak Mittal, Wei Deng, Kashif Rasul, Anderson Schneider",http://arxiv.org/pdf/2409.11684v2,cs.LG
An Enhanced-State Reinforcement Learning Algorithm for Multi-Task Fusion in Large-Scale Recommender Systems,"As the last key stage of Recommender Systems (RSs), Multi-Task Fusion (MTF)
is in charge of combining multiple scores predicted by Multi-Task Learning
(MTL) into a final score to maximize user satisfaction, which decides the
ultimate recommendation results. In recent years, to maximize long-term user
satisfaction within a recommendation session, Reinforcement Learning (RL) is
widely used for MTF in large-scale RSs. However, limited by their modeling
pattern, all the current RL-MTF methods can only utilize user features as the
state to generate actions for each user, but unable to make use of item
features and other valuable features, which leads to suboptimal results.
Addressing this problem is a challenge that requires breaking through the
current modeling pattern of RL-MTF. To solve this problem, we propose a novel
method called Enhanced-State RL for MTF in RSs. Unlike the existing methods
mentioned above, our method first defines user features, item features, and
other valuable features collectively as the enhanced state; then proposes a
novel actor and critic learning process to utilize the enhanced state to make
much better action for each user-item pair. To the best of our knowledge, this
novel modeling pattern is being proposed for the first time in the field of
RL-MTF. We conduct extensive offline and online experiments in a large-scale
RS. The results demonstrate that our model outperforms other models
significantly. Enhanced-State RL has been fully deployed in our RS more than
half a year, improving +3.84% user valid consumption and +0.58% user duration
time compared to baseline.",2024-09-18,"Peng Liu, Jiawei Zhu, Cong Xu, Ming Zhao, Bin Wang",http://arxiv.org/pdf/2409.11678v3,cs.LG
Hypergraph-based Motion Generation with Multi-modal Interaction Relational Reasoning,"The intricate nature of real-world driving environments, characterized by
dynamic and diverse interactions among multiple vehicles and their possible
future states, presents considerable challenges in accurately predicting the
motion states of vehicles and handling the uncertainty inherent in the
predictions. Addressing these challenges requires comprehensive modeling and
reasoning to capture the implicit relations among vehicles and the
corresponding diverse behaviors. This research introduces an integrated
framework for autonomous vehicles (AVs) motion prediction to address these
complexities, utilizing a novel Relational Hypergraph Interaction-informed
Neural mOtion generator (RHINO). RHINO leverages hypergraph-based relational
reasoning by integrating a multi-scale hypergraph neural network to model
group-wise interactions among multiple vehicles and their multi-modal driving
behaviors, thereby enhancing motion prediction accuracy and reliability.
Experimental validation using real-world datasets demonstrates the superior
performance of this framework in improving predictive accuracy and fostering
socially aware automated driving in dynamic traffic scenarios.",2024-09-18,"Keshu Wu, Yang Zhou, Haotian Shi, Dominique Lord, Bin Ran, Xinyue Ye",http://arxiv.org/pdf/2409.11676v1,cs.LG
Few-Shot Class-Incremental Learning with Non-IID Decentralized Data,"Few-shot class-incremental learning is crucial for developing scalable and
adaptive intelligent systems, as it enables models to acquire new classes with
minimal annotated data while safeguarding the previously accumulated knowledge.
Nonetheless, existing methods deal with continuous data streams in a
centralized manner, limiting their applicability in scenarios that prioritize
data privacy and security. To this end, this paper introduces federated
few-shot class-incremental learning, a decentralized machine learning paradigm
tailored to progressively learn new classes from scarce data distributed across
multiple clients. In this learning paradigm, clients locally update their
models with new classes while preserving data privacy, and then transmit the
model updates to a central server where they are aggregated globally. However,
this paradigm faces several issues, such as difficulties in few-shot learning,
catastrophic forgetting, and data heterogeneity. To address these challenges,
we present a synthetic data-driven framework that leverages replay buffer data
to maintain existing knowledge and facilitate the acquisition of new knowledge.
Within this framework, a noise-aware generative replay module is developed to
fine-tune local models with a balance of new and replay data, while generating
synthetic data of new classes to further expand the replay buffer for future
tasks. Furthermore, a class-specific weighted aggregation strategy is designed
to tackle data heterogeneity by adaptively aggregating class-specific
parameters based on local models performance on synthetic data. This enables
effective global model optimization without direct access to client data.
Comprehensive experiments across three widely-used datasets underscore the
effectiveness and preeminence of the introduced framework.",2024-09-18,"Cuiwei Liu, Siang Xu, Huaijun Qiu, Jing Zhang, Zhi Liu, Liang Zhao",http://arxiv.org/pdf/2409.11657v1,cs.LG
How to Build the Virtual Cell with Artificial Intelligence: Priorities and Opportunities,"The cell is arguably the most fundamental unit of life and is central to
understanding biology. Accurate modeling of cells is important for this
understanding as well as for determining the root causes of disease. Recent
advances in artificial intelligence (AI), combined with the ability to generate
large-scale experimental data, present novel opportunities to model cells. Here
we propose a vision of leveraging advances in AI to construct virtual cells,
high-fidelity simulations of cells and cellular systems under different
conditions that are directly learned from biological data across measurements
and scales. We discuss desired capabilities of such AI Virtual Cells, including
generating universal representations of biological entities across scales, and
facilitating interpretable in silico experiments to predict and understand
their behavior using virtual instruments. We further address the challenges,
opportunities and requirements to realize this vision including data needs,
evaluation strategies, and community standards and engagement to ensure
biological accuracy and broad utility. We envision a future where AI Virtual
Cells help identify new drug targets, predict cellular responses to
perturbations, as well as scale hypothesis exploration. With open science
collaborations across the biomedical ecosystem that includes academia,
philanthropy, and the biopharma and AI industries, a comprehensive predictive
understanding of cell mechanisms and interactions has come into reach.",2024-09-18,"Charlotte Bunne, Yusuf Roohani, Yanay Rosen, Ankit Gupta, Xikun Zhang, Marcel Roed, Theo Alexandrov, Mohammed AlQuraishi, Patricia Brennan, Daniel B. Burkhardt, Andrea Califano, Jonah Cool, Abby F. Dernburg, Kirsty Ewing, Emily B. Fox, Matthias Haury, Amy E. Herr, Eric Horvitz, Patrick D. Hsu, Viren Jain, Gregory R. Johnson, Thomas Kalil, David R. Kelley, Shana O. Kelley, Anna Kreshuk, Tim Mitchison, Stephani Otte, Jay Shendure, Nicholas J. Sofroniew, Fabian Theis, Christina V. Theodoris, Srigokul Upadhyayula, Marc Valer, Bo Wang, Eric Xing, Serena Yeung-Levy, Marinka Zitnik, Theofanis Karaletsos, Aviv Regev, Emma Lundberg, Jure Leskovec, Stephen R. Quake",http://arxiv.org/pdf/2409.11654v2,cs.LG
Enhancing Semi-Supervised Learning via Representative and Diverse Sample Selection,"Semi-Supervised Learning (SSL) has become a preferred paradigm in many deep
learning tasks, which reduces the need for human labor. Previous studies
primarily focus on effectively utilising the labelled and unlabeled data to
improve performance. However, we observe that how to select samples for
labelling also significantly impacts performance, particularly under extremely
low-budget settings. The sample selection task in SSL has been under-explored
for a long time. To fill in this gap, we propose a Representative and Diverse
Sample Selection approach (RDSS). By adopting a modified Frank-Wolfe algorithm
to minimise a novel criterion $\alpha$-Maximum Mean Discrepancy ($\alpha$-MMD),
RDSS samples a representative and diverse subset for annotation from the
unlabeled data. We demonstrate that minimizing $\alpha$-MMD enhances the
generalization ability of low-budget learning. Experimental results show that
RDSS consistently improves the performance of several popular SSL frameworks
and outperforms the state-of-the-art sample selection approaches used in Active
Learning (AL) and Semi-Supervised Active Learning (SSAL), even with constrained
annotation budgets.",2024-09-18,"Qian Shao, Jiangrui Kang, Qiyuan Chen, Zepeng Li, Hongxia Xu, Yiwen Cao, Jiajuan Liang, Jian Wu",http://arxiv.org/pdf/2409.11653v2,cs.LG
Reward-Robust RLHF in LLMs,"As Large Language Models (LLMs) continue to progress toward more advanced
forms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is
increasingly seen as a key pathway toward achieving Artificial General
Intelligence (AGI). However, the reliance on reward-model-based (RM-based)
alignment methods introduces significant challenges due to the inherent
instability and imperfections of Reward Models (RMs), which can lead to
critical issues such as reward hacking and misalignment with human intentions.
In this paper, we introduce a reward-robust RLHF framework aimed at addressing
these fundamental challenges, paving the way for more reliable and resilient
learning in LLMs. Our approach introduces a novel optimization objective that
carefully balances performance and robustness by incorporating Bayesian Reward
Model Ensembles (BRME) to model the uncertainty set of reward functions. This
allows the framework to integrate both nominal performance and minimum reward
signals, ensuring more stable learning even with imperfect RMs. Empirical
results demonstrate that our framework consistently outperforms baselines
across diverse benchmarks, showing improved accuracy and long-term stability.
We also provide a theoretical analysis, demonstrating that reward-robust RLHF
approaches the stability of constant reward settings, which proves to be
acceptable even in a stochastic-case analysis. Together, these contributions
highlight the framework potential to enhance both the performance and stability
of LLM alignment.",2024-09-18,"Yuzi Yan, Xingzhou Lou, Jialian Li, Yiping Zhang, Jian Xie, Chao Yu, Yu Wang, Dong Yan, Yuan Shen",http://arxiv.org/pdf/2409.15360v3,cs.LG
Art and Science of Quantizing Large-Scale Models: A Comprehensive Overview,"This paper provides a comprehensive overview of the principles, challenges,
and methodologies associated with quantizing large-scale neural network models.
As neural networks have evolved towards larger and more complex architectures
to address increasingly sophisticated tasks, the computational and energy costs
have escalated significantly. We explore the necessity and impact of model size
growth, highlighting the performance benefits as well as the computational
challenges and environmental considerations. The core focus is on model
quantization as a fundamental approach to mitigate these challenges by reducing
model size and improving efficiency without substantially compromising
accuracy. We delve into various quantization techniques, including both
post-training quantization (PTQ) and quantization-aware training (QAT), and
analyze several state-of-the-art algorithms such as LLM-QAT, PEQA(L4Q),
ZeroQuant, SmoothQuant, and others. Through comparative analysis, we examine
how these methods address issues like outliers, importance weighting, and
activation quantization, ultimately contributing to more sustainable and
accessible deployment of large-scale models.",2024-09-18,"Yanshu Wang, Tong Yang, Xiyan Liang, Guoan Wang, Hanning Lu, Xu Zhe, Yaoming Li, Li Weitao",http://arxiv.org/pdf/2409.11650v1,cs.LG
Hard-Label Cryptanalytic Extraction of Neural Network Models,"The machine learning problem of extracting neural network parameters has been
proposed for nearly three decades. Functionally equivalent extraction is a
crucial goal for research on this problem. When the adversary has access to the
raw output of neural networks, various attacks, including those presented at
CRYPTO 2020 and EUROCRYPT 2024, have successfully achieved this goal. However,
this goal is not achieved when neural networks operate under a hard-label
setting where the raw output is inaccessible.
  In this paper, we propose the first attack that theoretically achieves
functionally equivalent extraction under the hard-label setting, which applies
to ReLU neural networks. The effectiveness of our attack is validated through
practical experiments on a wide range of ReLU neural networks, including neural
networks trained on two real benchmarking datasets (MNIST, CIFAR10) widely used
in computer vision. For a neural network consisting of $10^5$ parameters, our
attack only requires several hours on a single core.",2024-09-18,"Yi Chen, Xiaoyang Dong, Jian Guo, Yantian Shen, Anyu Wang, Xiaoyun Wang",http://arxiv.org/pdf/2409.11646v1,cs.LG
DAF-Net: A Dual-Branch Feature Decomposition Fusion Network with Domain Adaptive for Infrared and Visible Image Fusion,"Infrared and visible image fusion aims to combine complementary information
from both modalities to provide a more comprehensive scene understanding.
However, due to the significant differences between the two modalities,
preserving key features during the fusion process remains a challenge. To
address this issue, we propose a dual-branch feature decomposition fusion
network (DAF-Net) with domain adaptive, which introduces Multi-Kernel Maximum
Mean Discrepancy (MK-MMD) into the base encoder and designs a hybrid kernel
function suitable for infrared and visible image fusion. The base encoder built
on the Restormer network captures global structural information while the
detail encoder based on Invertible Neural Networks (INN) focuses on extracting
detail texture information. By incorporating MK-MMD, the DAF-Net effectively
aligns the latent feature spaces of visible and infrared images, thereby
improving the quality of the fused images. Experimental results demonstrate
that the proposed method outperforms existing techniques across multiple
datasets, significantly enhancing both visual quality and fusion performance.
The related Python code is available at https://github.com/xujian000/DAF-Net.",2024-09-18,"Jian Xu, Xin He",http://arxiv.org/pdf/2409.11642v1,cs.LG
Enhancing PM2.5 Data Imputation and Prediction in Air Quality Monitoring Networks Using a KNN-SINDy Hybrid Model,"Air pollution, particularly particulate matter (PM2.5), poses significant
risks to public health and the environment, necessitating accurate prediction
and continuous monitoring for effective air quality management. However, air
quality monitoring (AQM) data often suffer from missing records due to various
technical difficulties. This study explores the application of Sparse
Identification of Nonlinear Dynamics (SINDy) for imputing missing PM2.5 data by
predicting, using training data from 2016, and comparing its performance with
the established Soft Impute (SI) and K-Nearest Neighbors (KNN) methods.",2024-09-18,"Yohan Choi, Boaz Choi, Jachin Choi",http://arxiv.org/pdf/2409.11640v1,cs.LG
Multimodal Generalized Category Discovery,"Generalized Category Discovery (GCD) aims to classify inputs into both known
and novel categories, a task crucial for open-world scientific discoveries.
However, current GCD methods are limited to unimodal data, overlooking the
inherently multimodal nature of most real-world data. In this work, we extend
GCD to a multimodal setting, where inputs from different modalities provide
richer and complementary information. Through theoretical analysis and
empirical validation, we identify that the key challenge in multimodal GCD lies
in effectively aligning heterogeneous information across modalities. To address
this, we propose MM-GCD, a novel framework that aligns both the feature and
output spaces of different modalities using contrastive learning and
distillation techniques. MM-GCD achieves new state-of-the-art performance on
the UPMC-Food101 and N24News datasets, surpassing previous methods by 11.5\%
and 4.7\%, respectively.",2024-09-18,"Yuchang Su, Renping Zhou, Siyu Huang, Xingjian Li, Tianyang Wang, Ziyue Wang, Min Xu",http://arxiv.org/pdf/2409.11624v1,cs.LG
Artificial intelligence inspired freeform optics design: a review,"Integrating artificial intelligence (AI) techniques such as machine learning
and deep learning into freeform optics design has significantly enhanced design
efficiency, expanded the design space, and led to innovative solutions. This
article reviews the latest developments in AI applications within this field,
highlighting their roles in initial design generation, optimization, and
performance prediction. It also addresses the benefits of AI, such as improved
accuracy and performance, alongside challenges like data requirements, model
interpretability, and computational complexity. Despite these challenges, the
future of AI in freeform optics design looks promising, with potential
advancements in hybrid design methods, interpretable AI, AI-driven
manufacturing, and targeted research for specific applications. Collaboration
among researchers, engineers, and designers is essential to fully harness AI's
potential and drive innovation in optics.",2024-09-18,"Lei Feng, Jingxing Liao, Jingna Yang",http://arxiv.org/pdf/2410.03554v2,cs.LG
PieClam: A Universal Graph Autoencoder Based on Overlapping Inclusive and Exclusive Communities,"We propose PieClam (Prior Inclusive Exclusive Cluster Affiliation Model): a
probabilistic graph model for representing any graph as overlapping generalized
communities. Our method can be interpreted as a graph autoencoder: nodes are
embedded into a code space by an algorithm that maximizes the log-likelihood of
the decoded graph, given the input graph. PieClam is a community affiliation
model that extends well-known methods like BigClam in two main manners. First,
instead of the decoder being defined via pairwise interactions between the
nodes in the code space, we also incorporate a learned prior on the
distribution of nodes in the code space, turning our method into a graph
generative model. Secondly, we generalize the notion of communities by allowing
not only sets of nodes with strong connectivity, which we call inclusive
communities, but also sets of nodes with strong disconnection, which we call
exclusive communities. To model both types of communities, we propose a new
type of decoder based the Lorentz inner product, which we prove to be much more
expressive than standard decoders based on standard inner products or norm
distances. By introducing a new graph similarity measure, that we call the log
cut distance, we show that PieClam is a universal autoencoder, able to
uniformly approximately reconstruct any graph. Our method is shown to obtain
competitive performance in graph anomaly detection benchmarks.",2024-09-18,"Daniel Zilberg, Ron Levie",http://arxiv.org/pdf/2409.11618v1,cs.LG
"Time-Series Forecasting, Knowledge Distillation, and Refinement within a Multimodal PDE Foundation Model","Symbolic encoding has been used in multi-operator learning as a way to embed
additional information for distinct time-series data. For spatiotemporal
systems described by time-dependent partial differential equations, the
equation itself provides an additional modality to identify the system. The
utilization of symbolic expressions along side time-series samples allows for
the development of multimodal predictive neural networks. A key challenge with
current approaches is that the symbolic information, i.e. the equations, must
be manually preprocessed (simplified, rearranged, etc.) to match and relate to
the existing token library, which increases costs and reduces flexibility,
especially when dealing with new differential equations. We propose a new token
library based on SymPy to encode differential equations as an additional
modality for time-series models. The proposed approach incurs minimal cost, is
automated, and maintains high prediction accuracy for forecasting tasks.
Additionally, we include a Bayesian filtering module that connects the
different modalities to refine the learned equation. This improves the accuracy
of the learned symbolic representation and the predicted time-series.",2024-09-17,"Derek Jollie, Jingmin Sun, Zecheng Zhang, Hayden Schaeffer",http://arxiv.org/pdf/2409.11609v1,cs.LG
Watch Your Steps: Observable and Modular Chains of Thought,"We propose a variant of chain of thought (CoT) prompting called Program Trace
Prompting that makes explanations more observable while preserving the power,
generality and flexibility of CoT. In our approach, few-shot CoT demonstrations
are wrapped in a formal syntax based on Python, and each prompt: identifies and
names steps; defines the input/output behavior of steps; and replaces CoT
explanations of in-context examples with chains of these formalized steps on
the same examples. Program Trace Prompting is applicable to many tasks,
achieving strong results on the 23 diverse tasks in the BIG-Bench Hard
benchmark. More importantly, by instrumenting explanations in this way, we
enable new types of analysis. In particular, we identify ""non-local errors""
(which correspond to incorrectly learning the reasoning method illustrated in
the demonstrations) as an unaddressed issue in CoT learning, and we present
methods for verifying the modularity of steps in a CoT explanation.",2024-09-17,"Cassandra A. Cohen, William W. Cohen",http://arxiv.org/pdf/2409.15359v2,cs.LG
DiffESM: Conditional Emulation of Temperature and Precipitation in Earth System Models with 3D Diffusion Models,"Earth System Models (ESMs) are essential for understanding the interaction
between human activities and the Earth's climate. However, the computational
demands of ESMs often limit the number of simulations that can be run,
hindering the robust analysis of risks associated with extreme weather events.
While low-cost climate emulators have emerged as an alternative to emulate ESMs
and enable rapid analysis of future climate, many of these emulators only
provide output on at most a monthly frequency. This temporal resolution is
insufficient for analyzing events that require daily characterization, such as
heat waves or heavy precipitation. We propose using diffusion models, a class
of generative deep learning models, to effectively downscale ESM output from a
monthly to a daily frequency. Trained on a handful of ESM realizations,
reflecting a wide range of radiative forcings, our DiffESM model takes monthly
mean precipitation or temperature as input, and is capable of producing daily
values with statistical characteristics close to ESM output. Combined with a
low-cost emulator providing monthly means, this approach requires only a small
fraction of the computational resources needed to run a large ensemble. We
evaluate model behavior using a number of extreme metrics, showing that DiffESM
closely matches the spatio-temporal behavior of the ESM output it emulates in
terms of the frequency and spatial characteristics of phenomena such as heat
waves, dry spells, or rainfall intensity.",2024-09-17,"Seth Bassetti, Brian Hutchinson, Claudia Tebaldi, Ben Kravitz",http://arxiv.org/pdf/2409.11601v1,cs.LG
No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with Pythonic Syntax,"We developed a jitted compiler for training Artificial Neural Networks using
C++, LLVM and Cuda. It features object-oriented characteristics, strong typing,
parallel workers for data pre-processing, pythonic syntax for expressions,
PyTorch like model declaration and Automatic Differentiation. We implement the
mechanisms of cache and pooling in order to manage VRAM, cuBLAS for high
performance matrix multiplication and cuDNN for convolutional layers. Our
experiments with Residual Convolutional Neural Networks on ImageNet, we reach
similar speed but degraded performance. Also, the GRU network experiments show
similar accuracy, but our compiler have degraded speed in that task. However,
our compiler demonstrates promising results at the CIFAR-10 benchmark, in which
we reach the same performance and about the same speed as PyTorch. We make the
code publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",2024-09-17,"Augusto Seben da Rosa, Marlon Daniel Angeli, Jorge Aikes Junior, Alef Iury Ferreira, Lucas Rafael Gris, Anderson da Silva Soares, Arnaldo Candido Junior, Frederico Santos de Oliveira, Gabriel Trevisan Damke, Rafael Teixeira Sousa",http://arxiv.org/pdf/2409.11600v1,cs.LG
The Sample Complexity of Smooth Boosting and the Tightness of the Hardcore Theorem,"Smooth boosters generate distributions that do not place too much weight on
any given example. Originally introduced for their noise-tolerant properties,
such boosters have also found applications in differential privacy,
reproducibility, and quantum learning theory. We study and settle the sample
complexity of smooth boosting: we exhibit a class that can be weak learned to
$\gamma$-advantage over smooth distributions with $m$ samples, for which strong
learning over the uniform distribution requires
$\tilde{\Omega}(1/\gamma^2)\cdot m$ samples. This matches the overhead of
existing smooth boosters and provides the first separation from the setting of
distribution-independent boosting, for which the corresponding overhead is
$O(1/\gamma)$.
  Our work also sheds new light on Impagliazzo's hardcore theorem from
complexity theory, all known proofs of which can be cast in the framework of
smooth boosting. For a function $f$ that is mildly hard against size-$s$
circuits, the hardcore theorem provides a set of inputs on which $f$ is
extremely hard against size-$s'$ circuits. A downside of this important result
is the loss in circuit size, i.e. that $s' \ll s$. Answering a question of
Trevisan, we show that this size loss is necessary and in fact, the parameters
achieved by known proofs are the best possible.",2024-09-17,"Guy Blanc, Alexandre Hayderi, Caleb Koch, Li-Yang Tan",http://arxiv.org/pdf/2409.11597v1,cs.LG
Outlier Detection with Cluster Catch Digraphs,"This paper introduces a novel family of outlier detection algorithms based on
Cluster Catch Digraphs (CCDs), specifically tailored to address the challenges
of high dimensionality and varying cluster shapes, which deteriorate the
performance of most traditional outlier detection methods. We propose the
Uniformity-Based CCD with Mutual Catch Graph (U-MCCD), the Uniformity- and
Neighbor-Based CCD with Mutual Catch Graph (UN-MCCD), and their shape-adaptive
variants (SU-MCCD and SUN-MCCD), which are designed to detect outliers in data
sets with arbitrary cluster shapes and high dimensions. We present the
advantages and shortcomings of these algorithms and provide the motivation or
need to define each particular algorithm. Through comprehensive Monte Carlo
simulations, we assess their performance and demonstrate the robustness and
effectiveness of our algorithms across various settings and contamination
levels. We also illustrate the use of our algorithms on various real-life data
sets. The U-MCCD algorithm efficiently identifies outliers while maintaining
high true negative rates, and the SU-MCCD algorithm shows substantial
improvement in handling non-uniform clusters. Additionally, the UN-MCCD and
SUN-MCCD algorithms address the limitations of existing methods in
high-dimensional spaces by utilizing Nearest Neighbor Distances (NND) for
clustering and outlier detection. Our results indicate that these novel
algorithms offer substantial advancements in the accuracy and adaptability of
outlier detection, providing a valuable tool for various real-world
applications.
  Keyword: Outlier detection, Graph-based clustering, Cluster catch digraphs,
$k$-nearest-neighborhood, Mutual catch graphs, Nearest neighbor distance.",2024-09-17,"Rui Shi, Nedret Billor, Elvan Ceyhan",http://arxiv.org/pdf/2409.11596v2,cs.LG
Self-Contrastive Forward-Forward Algorithm,"Agents that operate autonomously benefit from lifelong learning capabilities.
However, compatible training algorithms must comply with the decentralized
nature of these systems, which imposes constraints on both the parameter counts
and the computational resources. The Forward-Forward (FF) algorithm is one of
these. FF relies only on feedforward operations, the same used for inference,
for optimizing layer-wise objectives. This purely forward approach eliminates
the need for transpose operations required in traditional backpropagation.
Despite its potential, FF has failed to reach state-of-the-art performance on
most standard benchmark tasks, in part due to unreliable negative data
generation methods for unsupervised learning.
  In this work, we propose the Self-Contrastive Forward-Forward (SCFF)
algorithm, a competitive training method aimed at closing this performance gap.
Inspired by standard self-supervised contrastive learning for vision tasks,
SCFF generates positive and negative inputs applicable across various datasets.
The method demonstrates superior performance compared to existing unsupervised
local learning algorithms on several benchmark datasets, including MNIST,
CIFAR-10, STL-10, and Tiny ImageNet. We extend FF's application to training
recurrent neural networks, expanding its utility to sequential data tasks.
These findings pave the way for high-accuracy, real-time learning on
resource-constrained edge devices.",2024-09-17,"Xing Chen, Dongshu Liu, Jeremie Laydevant, Julie Grollier",http://arxiv.org/pdf/2409.11593v2,cs.LG
Advances in APPFL: A Comprehensive and Extensible Federated Learning Framework,"Federated learning (FL) is a distributed machine learning paradigm enabling
collaborative model training while preserving data privacy. In today's
landscape, where most data is proprietary, confidential, and distributed, FL
has become a promising approach to leverage such data effectively, particularly
in sensitive domains such as medicine and the electric grid. Heterogeneity and
security are the key challenges in FL, however, most existing FL frameworks
either fail to address these challenges adequately or lack the flexibility to
incorporate new solutions. To this end, we present the recent advances in
developing APPFL, an extensible framework and benchmarking suite for federated
learning, which offers comprehensive solutions for heterogeneity and security
concerns, as well as user-friendly interfaces for integrating new algorithms or
adapting to new applications. We demonstrate the capabilities of APPFL through
extensive experiments evaluating various aspects of FL, including communication
efficiency, privacy preservation, computational performance, and resource
utilization. We further highlight the extensibility of APPFL through case
studies in vertical, hierarchical, and decentralized FL. APPFL is fully
open-sourced on GitHub at https://github.com/APPFL/APPFL.",2024-09-17,"Zilinghan Li, Shilan He, Ze Yang, Minseok Ryu, Kibaek Kim, Ravi Madduri",http://arxiv.org/pdf/2409.11585v2,cs.LG
Automating proton PBS treatment planning for head and neck cancers using policy gradient-based deep reinforcement learning,"Proton pencil beam scanning (PBS) treatment planning for head and neck (H&N)
cancers is a time-consuming and experience-demanding task where a large number
of planning objectives are involved. Deep reinforcement learning (DRL) has
recently been introduced to the planning processes of intensity-modulated
radiation therapy and brachytherapy for prostate, lung, and cervical cancers.
However, existing approaches are built upon the Q-learning framework and
weighted linear combinations of clinical metrics, suffering from poor
scalability and flexibility and only capable of adjusting a limited number of
planning objectives in discrete action spaces. We propose an automatic
treatment planning model using the proximal policy optimization (PPO) algorithm
and a dose distribution-based reward function for proton PBS treatment planning
of H&N cancers. Specifically, a set of empirical rules is used to create
auxiliary planning structures from target volumes and organs-at-risk (OARs),
along with their associated planning objectives. These planning objectives are
fed into an in-house optimization engine to generate the spot monitor unit (MU)
values. A decision-making policy network trained using PPO is developed to
iteratively adjust the involved planning objective parameters in a continuous
action space and refine the PBS treatment plans using a novel dose
distribution-based reward function. Proton H&N treatment plans generated by the
model show improved OAR sparing with equal or superior target coverage when
compared with human-generated plans. Moreover, additional experiments on liver
cancer demonstrate that the proposed method can be successfully generalized to
other treatment sites. To the best of our knowledge, this is the first
DRL-based automatic treatment planning model capable of achieving human-level
performance for H&N cancers.",2024-09-17,"Qingqing Wang, Chang Chang",http://arxiv.org/pdf/2409.11576v1,cs.LG
"Preference Tuning with Human Feedback on Language, Speech, and Vision Tasks: A Survey","Preference tuning is a crucial process for aligning deep generative models
with human preferences. This survey offers a thorough overview of recent
advancements in preference tuning and the integration of human feedback. The
paper is organized into three main sections: 1) introduction and preliminaries:
an introduction to reinforcement learning frameworks, preference tuning tasks,
models, and datasets across various modalities: language, speech, and vision,
as well as different policy approaches, 2) in-depth exploration of each
preference tuning approach: a detailed analysis of the methods used in
preference tuning, and 3) applications, discussion, and future directions: an
exploration of the applications of preference tuning in downstream tasks,
including evaluation methods for different modalities, and an outlook on future
research directions. Our objective is to present the latest methodologies in
preference tuning and model alignment, enhancing the understanding of this
field for researchers and practitioners. We hope to encourage further
engagement and innovation in this area.",2024-09-17,"Genta Indra Winata, Hanyang Zhao, Anirban Das, Wenpin Tang, David D. Yao, Shi-Xiong Zhang, Sambit Sahu",http://arxiv.org/pdf/2409.11564v2,cs.LG
Discrete Unit based Masking for Improving Disentanglement in Voice Conversion,"Voice conversion (VC) aims to modify the speaker's identity while preserving
the linguistic content. Commonly, VC methods use an encoder-decoder
architecture, where disentangling the speaker's identity from linguistic
information is crucial. However, the disentanglement approaches used in these
methods are limited as the speaker features depend on the phonetic content of
the utterance, compromising disentanglement. This dependency is amplified with
attention-based methods. To address this, we introduce a novel masking
mechanism in the input before speaker encoding, masking certain discrete speech
units that correspond highly with phoneme classes. Our work aims to reduce the
phonetic dependency of speaker features by restricting access to some phonetic
information. Furthermore, since our approach is at the input level, it is
applicable to any encoder-decoder based VC framework. Our approach improves
disentanglement and conversion performance across multiple VC methods, showing
significant effectiveness, particularly in attention-based method, with 44%
relative improvement in objective intelligibility.",2024-09-17,"Philip H. Lee, Ismail Rasim Ulgen, Berrak Sisman",http://arxiv.org/pdf/2409.11560v1,cs.LG
A Property Encoder for Graph Neural Networks,"Graph machine learning, particularly using graph neural networks,
fundamentally relies on node features. Nevertheless, numerous real-world
systems, such as social and biological networks, often lack node features due
to various reasons, including privacy concerns, incomplete or missing data, and
limitations in data collection. In such scenarios, researchers typically resort
to methods like structural and positional encoding to construct node features.
However, the length of such features is contingent on the maximum value within
the property being encoded, for example, the highest node degree, which can be
exceedingly large in applications like scale-free networks. Furthermore, these
encoding schemes are limited to categorical data and might not be able to
encode metrics returning other type of values. In this paper, we introduce a
novel, universally applicable encoder, termed \emph{PropEnc}, which constructs
expressive node embedding from any given graph metric. \emph{PropEnc} leverages
histogram construction combined with reversed index encoding, offering a
flexible method for node features initialization. It supports flexible encoding
in terms of both dimensionality and type of input, demonstrating its
effectiveness across diverse applications. \emph{PropEnc} allows encoding
metrics in low-dimensional space which effectively address the sparsity
challenge and enhances the efficiency of the models. We show that
\emph{PropEnc} can construct node features that either exactly replicate
one-hot encoding or closely approximate indices under various settings. Our
extensive evaluations in graph classification setting across multiple social
networks that lack node features support our hypothesis. The empirical results
conclusively demonstrate that \emph{PropEnc} is both an efficient and effective
mechanism for constructing node features from diverse set of graph metrics.",2024-09-17,"Anwar Said, Waseem Abbas, Xenofon Koutsoukos",http://arxiv.org/pdf/2409.11554v2,cs.LG
VALO: A Versatile Anytime Framework for LiDAR-based Object Detection Deep Neural Networks,"This work addresses the challenge of adapting dynamic deadline requirements
for LiDAR object detection deep neural networks (DNNs). The computing latency
of object detection is critically important to ensure safe and efficient
navigation. However, state-of-the-art LiDAR object detection DNNs often exhibit
significant latency, hindering their real-time performance on
resource-constrained edge platforms. Therefore, a tradeoff between detection
accuracy and latency should be dynamically managed at runtime to achieve
optimum results.
  In this paper, we introduce VALO (Versatile Anytime algorithm for LiDAR
Object detection), a novel data-centric approach that enables anytime computing
of 3D LiDAR object detection DNNs. VALO employs a deadline-aware scheduler to
selectively process input regions, making execution time and accuracy tradeoffs
without architectural modifications. Additionally, it leverages efficient
forecasting of past detection results to mitigate possible loss of accuracy due
to partial processing of input. Finally, it utilizes a novel input reduction
technique within its detection heads to significantly accelerate execution
without sacrificing accuracy.
  We implement VALO on state-of-the-art 3D LiDAR object detection networks,
namely CenterPoint and VoxelNext, and demonstrate its dynamic adaptability to a
wide range of time constraints while achieving higher accuracy than the prior
state-of-the-art. Code is available
athttps://github.com/CSL-KU/VALO}{github.com/CSL-KU/VALO.",2024-09-17,"Ahmet Soyyigit, Shuochao Yao, Heechul Yun",http://arxiv.org/pdf/2409.11542v1,cs.LG
Balancing Optimality and Diversity: Human-Centered Decision Making through Generative Curation,"The surge in data availability has inundated decision-makers with an
overwhelming array of choices. While existing approaches focus on optimizing
decisions based on quantifiable metrics, practical decision-making often
requires balancing measurable quantitative criteria with unmeasurable
qualitative factors embedded in the broader context. In such cases, algorithms
can generate high-quality recommendations, but the final decision rests with
the human, who must weigh both dimensions. We define the process of selecting
the optimal set of algorithmic recommendations in this context as
human-centered decision making. To address this challenge, we introduce a novel
framework called generative curation, which optimizes the true desirability of
decision options by integrating both quantitative and qualitative aspects. Our
framework uses a Gaussian process to model unknown qualitative factors and
derives a diversity metric that balances quantitative optimality with
qualitative diversity. This trade-off enables the generation of a manageable
subset of diverse, near-optimal actions that are robust to unknown qualitative
preferences. To operationalize this framework, we propose two implementation
approaches: a generative neural network architecture that produces a
distribution $\pi$ to efficiently sample a diverse set of near-optimal actions,
and a sequential optimization method to iteratively generates solutions that
can be easily incorporated into complex optimization formulations. We validate
our approach with extensive datasets, demonstrating its effectiveness in
enhancing decision-making processes across a range of complex environments,
with significant implications for policy and management.",2024-09-17,"Michael Lingzhi Li, Shixiang Zhu",http://arxiv.org/pdf/2409.11535v1,cs.LG
Adaptive Anomaly Detection in Network Flows with Low-Rank Tensor Decompositions and Deep Unrolling,"Anomaly detection (AD) is increasingly recognized as a key component for
ensuring the resilience of future communication systems. While deep learning
has shown state-of-the-art AD performance, its application in critical systems
is hindered by concerns regarding training data efficiency, domain adaptation
and interpretability. This work considers AD in network flows using incomplete
measurements, leveraging a robust tensor decomposition approach and deep
unrolling techniques to address these challenges. We first propose a novel
block-successive convex approximation algorithm based on a regularized
model-fitting objective where the normal flows are modeled as low-rank tensors
and anomalies as sparse. An augmentation of the objective is introduced to
decrease the computational cost. We apply deep unrolling to derive a novel deep
network architecture based on our proposed algorithm, treating the
regularization parameters as learnable weights. Inspired by Bayesian
approaches, we extend the model architecture to perform online adaptation to
per-flow and per-time-step statistics, improving AD performance while
maintaining a low parameter count and preserving the problem's permutation
equivariances. To optimize the deep network weights for detection performance,
we employ a homotopy optimization approach based on an efficient approximation
of the area under the receiver operating characteristic curve. Extensive
experiments on synthetic and real-world data demonstrate that our proposed deep
network architecture exhibits a high training data efficiency, outperforms
reference methods, and adapts seamlessly to varying network topologies.",2024-09-17,"Lukas Schynol, Marius Pesavento",http://arxiv.org/pdf/2409.11529v2,cs.LG
Unlocking NACE Classification Embeddings with OpenAI for Enhanced Analysis and Processing,"The Statistical Classification of Economic Activities in the European
Community (NACE) is the standard classification system for the categorization
of economic and industrial activities within the European Union. This paper
proposes a novel approach to transform the NACE classification into
low-dimensional embeddings, using state-of-the-art models and dimensionality
reduction techniques. The primary challenge is the preservation of the
hierarchical structure inherent within the original NACE classification while
reducing the number of dimensions. To address this issue, we introduce custom
metrics designed to quantify the retention of hierarchical relationships
throughout the embedding and reduction processes. The evaluation of these
metrics demonstrates the effectiveness of the proposed methodology in retaining
the structural information essential for insightful analysis. This approach not
only facilitates the visual exploration of economic activity relationships, but
also increases the efficacy of downstream tasks, including clustering,
classification, integration with other classifications, and others. Through
experimental validation, the utility of our proposed framework in preserving
hierarchical structures within the NACE classification is showcased, thereby
providing a valuable tool for researchers and policymakers to understand and
leverage any hierarchical data.",2024-09-17,"Andrea Vidali, Nicola Jean, Giacomo Le Pera",http://arxiv.org/pdf/2409.11524v1,cs.LG
Partially Observable Contextual Bandits with Linear Payoffs,"The standard contextual bandit framework assumes fully observable and
actionable contexts. In this work, we consider a new bandit setting with
partially observable, correlated contexts and linear payoffs, motivated by the
applications in finance where decision making is based on market information
that typically displays temporal correlation and is not fully observed. We make
the following contributions marrying ideas from statistical signal processing
with bandits: (i) We propose an algorithmic pipeline named EMKF-Bandit, which
integrates system identification, filtering, and classic contextual bandit
algorithms into an iterative method alternating between latent parameter
estimation and decision making. (ii) We analyze EMKF-Bandit when we select
Thompson sampling as the bandit algorithm and show that it incurs a sub-linear
regret under conditions on filtering. (iii) We conduct numerical simulations
that demonstrate the benefits and practical applicability of the proposed
pipeline.",2024-09-17,"Sihan Zeng, Sujay Bhatt, Alec Koppel, Sumitra Ganesh",http://arxiv.org/pdf/2409.11521v1,cs.LG
Learning-Augmented Frequency Estimation in Sliding Windows,"We show how to utilize machine learning approaches to improve sliding window
algorithms for approximate frequency estimation problems, under the
``algorithms with predictions'' framework. In this dynamic environment,
previous learning-augmented algorithms are less effective, since properties in
sliding window resolution can differ significantly from the properties of the
entire stream. Our focus is on the benefits of predicting and filtering out
items with large next arrival times -- that is, there is a large gap until
their next appearance -- from the stream, which we show improves the
memory-accuracy tradeoffs significantly. We provide theorems that provide
insight into how and by how much our technique can improve the sliding window
algorithm, as well as experimental results using real-world data sets. Our work
demonstrates that predictors can be useful in the challenging sliding window
setting.",2024-09-17,"Rana Shahout, Ibrahim Sabek, Michael Mitzenmacher",http://arxiv.org/pdf/2409.11516v1,cs.LG
FedNE: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction,"Federated learning (FL) has rapidly evolved as a promising paradigm that
enables collaborative model training across distributed participants without
exchanging their local data. Despite its broad applications in fields such as
computer vision, graph learning, and natural language processing, the
development of a data projection model that can be effectively used to
visualize data in the context of FL is crucial yet remains heavily
under-explored. Neighbor embedding (NE) is an essential technique for
visualizing complex high-dimensional data, but collaboratively learning a joint
NE model is difficult. The key challenge lies in the objective function, as
effective visualization algorithms like NE require computing loss functions
among pairs of data. In this paper, we introduce \textsc{FedNE}, a novel
approach that integrates the \textsc{FedAvg} framework with the contrastive NE
technique, without any requirements of shareable data. To address the lack of
inter-client repulsion which is crucial for the alignment in the global
embedding space, we develop a surrogate loss function that each client learns
and shares with each other. Additionally, we propose a data-mixing strategy to
augment the local data, aiming to relax the problems of invisible neighbors and
false neighbors constructed by the local $k$NN graphs. We conduct comprehensive
experiments on both synthetic and real-world datasets. The results demonstrate
that our \textsc{FedNE} can effectively preserve the neighborhood data
structures and enhance the alignment in the global embedding space compared to
several baseline methods.",2024-09-17,"Ziwei Li, Xiaoqi Wang, Hong-You Chen, Han-Wei Shen, Wei-Lun Chao",http://arxiv.org/pdf/2409.11509v2,cs.LG
Chess Rating Estimation from Moves and Clock Times Using a CNN-LSTM,"Current chess rating systems update ratings incrementally and may not always
accurately reflect a player's true strength at all times, especially for
rapidly improving players or very rusty players. To overcome this, we explore a
method to estimate player ratings directly from game moves and clock times. We
compiled a benchmark dataset from Lichess with over one million games,
encompassing various time controls and including move sequences and clock
times. Our model architecture comprises a CNN to learn positional features,
which are then integrated with clock-time data into a Bidirectional LSTM,
predicting player ratings after each move. The model achieved an MAE of 182
rating points on the test data. Additionally, we applied our model to the 2024
IEEE Big Data Cup Chess Puzzle Difficulty Competition dataset, predicted puzzle
ratings and achieved competitive results. This model is the first to use no
hand-crafted features to estimate chess ratings and also the first to output a
rating prediction after each move. Our method highlights the potential of using
move-based rating estimation for enhancing rating systems and potentially other
applications such as cheating detection.",2024-09-17,"Michael Omori, Prasad Tadepalli",http://arxiv.org/pdf/2409.11506v2,cs.LG
Preventing Representational Rank Collapse in MPNNs by Splitting the Computational Graph,"The ability of message-passing neural networks (MPNNs) to fit complex
functions over graphs is limited as most graph convolutions amplify the same
signal across all feature channels, a phenomenon known as rank collapse, and
over-smoothing as a special case. Most approaches to mitigate over-smoothing
extend common message-passing schemes, e.g., the graph convolutional network,
by utilizing residual connections, gating mechanisms, normalization, or
regularization techniques. Our work contrarily proposes to directly tackle the
cause of this issue by modifying the message-passing scheme and exchanging
different types of messages using multi-relational graphs. We identify a
sufficient condition to ensure linearly independent node representations. As
one instantion, we show that operating on multiple directed acyclic graphs
always satisfies our condition and propose to obtain these by defining a strict
partial ordering of the nodes. We conduct comprehensive experiments that
confirm the benefits of operating on multi-relational graphs to achieve more
informative node representations.",2024-09-17,"Andreas Roth, Franka Bause, Nils M. Kriege, Thomas Liebig",http://arxiv.org/pdf/2409.11504v2,cs.LG
Super Resolution On Global Weather Forecasts,"Weather forecasting is a vitally important tool for tasks ranging from
planning day to day activities to disaster response planning. However, modeling
weather has proven to be challenging task due to its chaotic and unpredictable
nature. Each variable, from temperature to precipitation to wind, all influence
the path the environment will take. As a result, all models tend to rapidly
lose accuracy as the temporal range of their forecasts increase. Classical
forecasting methods use a myriad of physics-based, numerical, and stochastic
techniques to predict the change in weather variables over time. However, such
forecasts often require a very large amount of data and are extremely
computationally expensive. Furthermore, as climate and global weather patterns
change, classical models are substantially more difficult and time-consuming to
update for changing environments. Fortunately, with recent advances in deep
learning and publicly available high quality weather datasets, deploying
learning methods for estimating these complex systems has become feasible. The
current state-of-the-art deep learning models have comparable accuracy to the
industry standard numerical models and are becoming more ubiquitous in practice
due to their adaptability. Our group seeks to improve upon existing deep
learning based forecasting methods by increasing spatial resolutions of global
weather predictions. Specifically, we are interested in performing super
resolution (SR) on GraphCast temperature predictions by increasing the global
precision from 1 degree of accuracy to 0.5 degrees, which is approximately
111km and 55km respectively.",2024-09-17,"Lawrence Zhang, Adam Yang, Rodz Andrie Amor, Bryan Zhang, Dhruv Rao",http://arxiv.org/pdf/2409.11502v2,cs.LG
Guess What I Think: Streamlined EEG-to-Image Generation with Latent Diffusion Models,"Generating images from brain waves is gaining increasing attention due to its
potential to advance brain-computer interface (BCI) systems by understanding
how brain signals encode visual cues. Most of the literature has focused on
fMRI-to-Image tasks as fMRI is characterized by high spatial resolution.
However, fMRI is an expensive neuroimaging modality and does not allow for
real-time BCI. On the other hand, electroencephalography (EEG) is a low-cost,
non-invasive, and portable neuroimaging technique, making it an attractive
option for future real-time applications. Nevertheless, EEG presents inherent
challenges due to its low spatial resolution and susceptibility to noise and
artifacts, which makes generating images from EEG more difficult. In this
paper, we address these problems with a streamlined framework based on the
ControlNet adapter for conditioning a latent diffusion model (LDM) through EEG
signals. We conduct experiments and ablation studies on popular benchmarks to
demonstrate that the proposed method beats other state-of-the-art models.
Unlike these methods, which often require extensive preprocessing, pretraining,
different losses, and captioning models, our approach is efficient and
straightforward, requiring only minimal preprocessing and a few components. The
code is available at https://github.com/LuigiSigillo/GWIT.",2024-09-17,"Eleonora Lopez, Luigi Sigillo, Federica Colonnese, Massimo Panella, Danilo Comminiello",http://arxiv.org/pdf/2410.02780v2,cs.LG
Beyond Algorithmic Fairness: A Guide to Develop and Deploy Ethical AI-Enabled Decision-Support Tools,"The integration of artificial intelligence (AI) and optimization hold
substantial promise for improving the efficiency, reliability, and resilience
of engineered systems. Due to the networked nature of many engineered systems,
ethically deploying methodologies at this intersection poses challenges that
are distinct from other AI settings, thus motivating the development of ethical
guidelines tailored to AI-enabled optimization. This paper highlights the need
to go beyond fairness-driven algorithms to systematically address ethical
decisions spanning the stages of modeling, data curation, results analysis, and
implementation of optimization-based decision support tools. Accordingly, this
paper identifies ethical considerations required when deploying algorithms at
the intersection of AI and optimization via case studies in power systems as
well as supply chain and logistics. Rather than providing a prescriptive set of
rules, this paper aims to foster reflection and awareness among researchers and
encourage consideration of ethical implications at every step of the
decision-making process.",2024-09-17,"Rosemarie Santa Gonzalez, Ryan Piansky, Sue M Bae, Justin Biddle, Daniel Molzahn",http://arxiv.org/pdf/2409.11489v1,cs.LG
Learning variant product relationship and variation attributes from e-commerce website structures,"We introduce VARM, variant relationship matcher strategy, to identify pairs
of variant products in e-commerce catalogs. Traditional definitions of entity
resolution are concerned with whether product mentions refer to the same
underlying product. However, this fails to capture product relationships that
are critical for e-commerce applications, such as having similar, but not
identical, products listed on the same webpage or share reviews. Here, we
formulate a new type of entity resolution in variant product relationships to
capture these similar e-commerce product links. In contrast with the
traditional definition, the new definition requires both identifying if two
products are variant matches of each other and what are the attributes that
vary between them. To satisfy these two requirements, we developed a strategy
that leverages the strengths of both encoding and generative AI models. First,
we construct a dataset that captures webpage product links, and therefore
variant product relationships, to train an encoding LLM to predict variant
matches for any given pair of products. Second, we use RAG prompted generative
LLMs to extract variation and common attributes amongst groups of variant
products. To validate our strategy, we evaluated model performance using real
data from one of the world's leading e-commerce retailers. The results showed
that our strategy outperforms alternative solutions and paves the way to
exploiting these new type of product relationships.",2024-09-17,"Pedro Herrero-Vidal, You-Lin Chen, Cris Liu, Prithviraj Sen, Lichao Wang",http://arxiv.org/pdf/2410.02779v1,cs.LG
NVLM: Open Frontier-Class Multimodal LLMs,"We introduce NVLM 1.0, a family of frontier-class multimodal large language
models (LLMs) that achieve state-of-the-art results on vision-language tasks,
rivaling the leading proprietary models (e.g., GPT-4o) and open-access models
(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved
text-only performance over its LLM backbone after multimodal training. In terms
of model design, we perform a comprehensive comparison between decoder-only
multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,
Flamingo). Based on the strengths and weaknesses of both approaches, we propose
a novel architecture that enhances both training efficiency and multimodal
reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for
tile-based dynamic high-resolution images, which significantly boosts
performance on multimodal reasoning and OCR-related tasks. Regarding training
data, we meticulously curate and provide detailed information on our multimodal
pretraining and supervised fine-tuning datasets. Our findings indicate that
dataset quality and task diversity are more important than scale, even during
the pretraining phase, across all architectures. Notably, we develop
production-grade multimodality for the NVLM-1.0 models, enabling them to excel
in vision-language tasks while maintaining and even improving text-only
performance compared to their LLM backbones. To achieve this, we craft and
integrate a high-quality text-only dataset into multimodal training, alongside
a substantial amount of multimodal math and reasoning data, leading to enhanced
math and coding capabilities across modalities. To advance research in the
field, we release the model weights at https://huggingface.co/nvidia/NVLM-D-72B
and will open-source the training code for the community soon.",2024-09-17,"Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",http://arxiv.org/pdf/2409.11402v2,cs.LG
Moshi: a speech-text foundation model for real-time dialogue,"We introduce Moshi, a speech-text foundation model and full-duplex spoken
dialogue framework. Current systems for spoken dialogue rely on pipelines of
independent components, namely voice activity detection, speech recognition,
textual dialogue and text-to-speech. Such frameworks cannot emulate the
experience of real conversations. First, their complexity induces a latency of
several seconds between interactions. Second, text being the intermediate
modality for dialogue, non-linguistic information that modifies meaning -- such
as emotion or non-speech sounds -- is lost in the interaction. Finally, they
rely on a segmentation into speaker turns, which does not take into account
overlapping speech, interruptions and interjections. Moshi solves these
independent issues altogether by casting spoken dialogue as speech-to-speech
generation. Starting from a text language model backbone, Moshi generates
speech as tokens from the residual quantizer of a neural audio codec, while
modeling separately its own speech and that of the user into parallel streams.
This allows for the removal of explicit speaker turns, and the modeling of
arbitrary conversational dynamics. We moreover extend the hierarchical
semantic-to-acoustic token generation of previous work to first predict
time-aligned text tokens as a prefix to audio tokens. Not only this ""Inner
Monologue"" method significantly improves the linguistic quality of generated
speech, but we also illustrate how it can provide streaming speech recognition
and text-to-speech. Our resulting model is the first real-time full-duplex
spoken large language model, with a theoretical latency of 160ms, 200ms in
practice, and is available at https://github.com/kyutai-labs/moshi.",2024-09-17,"Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, Neil Zeghidour",http://arxiv.org/pdf/2410.00037v2,cs.LG
Says Who? Effective Zero-Shot Annotation of Focalization,"Focalization, the perspective through which narrative is presented, is
encoded via a wide range of lexico-grammatical features and is subject to
reader interpretation. Even trained annotators frequently disagree on correct
labels, suggesting this task is both qualitatively and computationally
challenging. In this work, we test how well five contemporary large language
model (LLM) families and two baselines perform when annotating short literary
excerpts for focalization. Despite the challenging nature of the task, we find
that LLMs show comparable performance to trained human annotators, with GPT-4o
achieving an average F1 of 84.79%. Further, we demonstrate that the log
probabilities output by GPT-family models frequently reflect the difficulty of
annotating particular excerpts. Finally, we provide a case study analyzing
sixteen Stephen King novels, demonstrating the usefulness of this approach for
computational literary studies and the insights gleaned from examining
focalization at scale.",2024-09-17,"Rebecca M. M. Hicke, Yuri Bizzoni, Pascale Feldkamp, Ross Deans Kristensen-McLachlan",http://arxiv.org/pdf/2409.11390v2,cs.LG
Two Stage Segmentation of Cervical Tumors using PocketNet,"Cervical cancer remains the fourth most common malignancy amongst women
worldwide.1 Concurrent chemoradiotherapy (CRT) serves as the mainstay
definitive treatment regimen for locally advanced cervical cancers and includes
external beam radiation followed by brachytherapy.2 Integral to radiotherapy
treatment planning is the routine contouring of both the target tumor at the
level of the cervix, associated gynecologic anatomy and the adjacent organs at
risk (OARs). However, manual contouring of these structures is both time and
labor intensive and associated with known interobserver variability that can
impact treatment outcomes. While multiple tools have been developed to
automatically segment OARs and the high-risk clinical tumor volume (HR-CTV)
using computed tomography (CT) images,3,4,5,6 the development of deep
learning-based tumor segmentation tools using routine T2-weighted (T2w)
magnetic resonance imaging (MRI) addresses an unmet clinical need to improve
the routine contouring of both anatomical structures and cervical cancers,
thereby increasing quality and consistency of radiotherapy planning. This work
applied a novel deep-learning model (PocketNet) to segment the cervix, vagina,
uterus, and tumor(s) on T2w MRI. The performance of the PocketNet architecture
was evaluated, when trained on data via five-fold cross validation. PocketNet
achieved a mean Dice-Sorensen similarity coefficient (DSC) exceeding 70% for
tumor segmentation and 80% for organ segmentation. Validation on a publicly
available dataset from The Cancer Imaging Archive (TCIA) demonstrated the
models robustness, achieving DSC scores of 67.3% for tumor segmentation and
80.8% for organ segmentation. These results suggest that PocketNet is robust to
variations in contrast protocols, providing reliable segmentation of the
regions of interest.",2024-09-17,"Awj Twam, Adrian E. Celaya, Megan C. Jacobsen, Rachel Glenn, Peng Wei, Jia Sun, Ann Klopp, Aradhana M. Venkatesan, David Fuentes",http://arxiv.org/pdf/2409.11456v3,cs.LG
Normalization in Proportional Feature Spaces,"The subject of features normalization plays an important central role in data
representation, characterization, visualization, analysis, comparison,
classification, and modeling, as it can substantially influence and be
influenced by all of these activities and respective aspects. The selection of
an appropriate normalization method needs to take into account the type and
characteristics of the involved features, the methods to be used subsequently
for the just mentioned data processing, as well as the specific questions being
considered. After briefly considering how normalization constitutes one of the
many interrelated parts typically involved in data analysis and modeling, the
present work addressed the important issue of feature normalization from the
perspective of uniform and proportional (right skewed) features and comparison
operations. More general right skewed features are also considered in an
approximated manner. Several concepts, properties, and results are described
and discussed, including the description of a duality relationship between
uniform and proportional feature spaces and respective comparisons, specifying
conditions for consistency between comparisons in each of the two domains. Two
normalization possibilities based on non-centralized dispersion of features are
also presented, and also described is a modified version of the Jaccard
similarity index which incorporates intrinsically normalization. Preliminary
experiments are presented in order to illustrate the developed concepts and
methods.",2024-09-17,"Alexandre Benatti, Luciano da F. Costa",http://arxiv.org/pdf/2409.11389v1,cs.LG
Training Datasets Generation for Machine Learning: Application to Vision Based Navigation,"Vision Based Navigation consists in utilizing cameras as precision sensors
for GNC after extracting information from images. To enable the adoption of
machine learning for space applications, one of obstacles is the demonstration
that available training datasets are adequate to validate the algorithms. The
objective of the study is to generate datasets of images and metadata suitable
for training machine learning algorithms. Two use cases were selected and a
robust methodology was developed to validate the datasets including the ground
truth. The first use case is in-orbit rendezvous with a man-made object: a
mockup of satellite ENVISAT. The second use case is a Lunar landing scenario.
Datasets were produced from archival datasets (Chang'e 3), from the laboratory
at DLR TRON facility and at Airbus Robotic laboratory, from SurRender software
high fidelity image simulator using Model Capture and from Generative
Adversarial Networks. The use case definition included the selection of
algorithms as benchmark: an AI-based pose estimation algorithm and a dense
optical flow algorithm were selected. Eventually it is demonstrated that
datasets produced with SurRender and selected laboratory facilities are
adequate to train machine learning algorithms.",2024-09-17,"Jérémy Lebreton, Ingo Ahrns, Roland Brochard, Christoph Haskamp, Hans Krüger, Matthieu Le Goff, Nicolas Menga, Nicolas Ollagnier, Ralf Regele, Francesco Capolupo, Massimo Casasco",http://arxiv.org/pdf/2409.11383v2,cs.LG
"Machine Learning on Dynamic Functional Connectivity: Promise, Pitfalls, and Interpretations","An unprecedented amount of existing functional Magnetic Resonance Imaging
(fMRI) data provides a new opportunity to understand the relationship between
functional fluctuation and human cognition/behavior using a data-driven
approach. To that end, tremendous efforts have been made in machine learning to
predict cognitive states from evolving volumetric images of
blood-oxygen-level-dependent (BOLD) signals. Due to the complex nature of brain
function, however, the evaluation on learning performance and discoveries are
not often consistent across current state-of-the-arts (SOTA). By capitalizing
on large-scale existing neuroimaging data (34,887 data samples from six public
databases), we seek to establish a well-founded empirical guideline for
designing deep models for functional neuroimages by linking the methodology
underpinning with knowledge from the neuroscience domain. Specifically, we put
the spotlight on (1) What is the current SOTA performance in cognitive task
recognition and disease diagnosis using fMRI? (2) What are the limitations of
current deep models? and (3) What is the general guideline for selecting the
suitable machine learning backbone for new neuroimaging applications? We have
conducted a comprehensive evaluation and statistical analysis, in various
settings, to answer the above outstanding questions.",2024-09-17,"Jiaqi Ding, Tingting Dan, Ziquan Wei, Hyuna Cho, Paul J. Laurienti, Won Hwa Kim, Guorong Wu",http://arxiv.org/pdf/2409.11377v1,cs.LG
Towards Time Series Reasoning with LLMs,"Multi-modal large language models (MLLMs) have enabled numerous advances in
understanding and reasoning in domains like vision, but we have not yet seen
this broad success for time-series. Although prior works on time-series MLLMs
have shown promising performance in time-series forecasting, very few works
show how an LLM could be used for time-series reasoning in natural language. We
propose a novel multi-modal time-series LLM approach that learns generalizable
information across various domains with powerful zero-shot performance. First,
we train a lightweight time-series encoder on top of an LLM to directly extract
time-series information. Then, we fine-tune our model with chain-of-thought
augmented time-series tasks to encourage the model to generate reasoning paths.
We show that our model learns a latent representation that reflects specific
time-series features (e.g. slope, frequency), as well as outperforming GPT-4o
on a set of zero-shot reasoning tasks on a variety of domains.",2024-09-17,"Winnie Chow, Lauren Gardiner, Haraldur T. Hallgrímsson, Maxwell A. Xu, Shirley You Ren",http://arxiv.org/pdf/2409.11376v2,cs.LG
Golden Ratio Search: A Low-Power Adversarial Attack for Deep Learning based Modulation Classification,"We propose a minimal power white box adversarial attack for Deep Learning
based Automatic Modulation Classification (AMC). The proposed attack uses the
Golden Ratio Search (GRS) method to find powerful attacks with minimal power.
We evaluate the efficacy of the proposed method by comparing it with existing
adversarial attack approaches. Additionally, we test the robustness of the
proposed attack against various state-of-the-art architectures, including
defense mechanisms such as adversarial training, binarization, and ensemble
methods. Experimental results demonstrate that the proposed attack is powerful,
requires minimal power, and can be generated in less time, significantly
challenging the resilience of current AMC methods.",2024-09-17,"Deepsayan Sadhukhan, Nitin Priyadarshini Shankar, Sheetal Kalyani",http://arxiv.org/pdf/2409.11454v1,cs.LG
Learning Spatially-Aware Language and Audio Embeddings,"Humans can picture a sound scene given an imprecise natural language
description. For example, it is easy to imagine an acoustic environment given a
phrase like ""the lion roar came from right behind me!"". For a machine to have
the same degree of comprehension, the machine must know what a lion is
(semantic attribute), what the concept of ""behind"" is (spatial attribute) and
how these pieces of linguistic information align with the semantic and spatial
attributes of the sound (what a roar sounds like when its coming from behind).
State-of-the-art audio foundation models which learn to map between audio
scenes and natural textual descriptions, are trained on non-spatial audio and
text pairs, and hence lack spatial awareness. In contrast, sound event
localization and detection models are limited to recognizing sounds from a
fixed number of classes, and they localize the source to absolute position
(e.g., 0.2m) rather than a position described using natural language (e.g.,
""next to me""). To address these gaps, we present ELSA a spatially aware-audio
and text embedding model trained using multimodal contrastive learning. ELSA
supports non-spatial audio, spatial audio, and open vocabulary text captions
describing both the spatial and semantic components of sound. To train ELSA:
(a) we spatially augment the audio and captions of three open-source audio
datasets totaling 4,738 hours of audio, and (b) we design an encoder to capture
the semantics of non-spatial audio, and the semantics and spatial attributes of
spatial audio using contrastive learning. ELSA is competitive with
state-of-the-art for both semantic retrieval and 3D source localization. In
particular, ELSA achieves +2.8% mean audio-to-text and text-to-audio R@1 above
the baseline, and outperforms by -11.6{\deg} mean-absolute-error in 3D source
localization over the baseline.",2024-09-17,"Bhavika Devnani, Skyler Seto, Zakaria Aldeneh, Alessandro Toso, Elena Menyaylenko, Barry-John Theobald, Jonathan Sheaffer, Miguel Sarabia",http://arxiv.org/pdf/2409.11369v2,cs.LG
Clinical Validation of a Real-Time Machine Learning-based System for the Detection of Acute Myeloid Leukemia by Flow Cytometry,"Machine-learning (ML) models in flow cytometry have the potential to reduce
error rates, increase reproducibility, and boost the efficiency of clinical
labs. While numerous ML models for flow cytometry data have been proposed, few
studies have described the clinical deployment of such models. Realizing the
potential gains of ML models in clinical labs requires not only an accurate
model, but infrastructure for automated inference, error detection, analytics
and monitoring, and structured data extraction. Here, we describe an ML model
for detection of Acute Myeloid Leukemia (AML), along with the infrastructure
supporting clinical implementation. Our infrastructure leverages the resilience
and scalability of the cloud for model inference, a Kubernetes-based workflow
system that provides model reproducibility and resource management, and a
system for extracting structured diagnoses from full-text reports. We also
describe our model monitoring and visualization platform, an essential element
for ensuring continued model accuracy. Finally, we present a post-deployment
analysis of impacts on turn-around time and compare production accuracy to the
original validation statistics.",2024-09-17,"Lauren M. Zuromski, Jacob Durtschi, Aimal Aziz, Jeffrey Chumley, Mark Dewey, Paul English, Muir Morrison, Keith Simmon, Blaine Whipple, Brendan O'Fallon, David P. Ng",http://arxiv.org/pdf/2409.11350v1,cs.LG
Learning a Terrain- and Robot-Aware Dynamics Model for Autonomous Mobile Robot Navigation,"Mobile robots should be capable of planning cost-efficient paths for
autonomous navigation. Typically, the terrain and robot properties are subject
to variations. For instance, properties of the terrain such as friction may
vary across different locations. Also, properties of the robot may change such
as payloads or wear and tear, e.g., causing changing actuator gains or joint
friction. Autonomous navigation approaches should thus be able to adapt to such
variations. In this article, we propose a novel approach for learning a
probabilistic, terrain- and robot-aware forward dynamics model (TRADYN) which
can adapt to such variations and demonstrate its use for navigation. Our
learning approach extends recent advances in meta-learning forward dynamics
models based on Neural Processes for mobile robot navigation. We evaluate our
method in simulation for 2D navigation of a robot with uni-cycle dynamics with
varying properties on terrain with spatially varying friction coefficients. In
our experiments, we demonstrate that TRADYN has lower prediction error over
long time horizons than model ablations which do not adapt to robot or terrain
variations. We also evaluate our model for navigation planning in a
model-predictive control framework and under various sources of noise. We
demonstrate that our approach yields improved performance in planning
control-efficient paths by taking robot and terrain properties into account.",2024-09-17,"Jan Achterhold, Suresh Guttikonda, Jens U. Kreber, Haolong Li, Joerg Stueckler",http://arxiv.org/pdf/2409.11452v1,cs.LG
Learning Unstable Continuous-Time Stochastic Linear Control Systems,"We study the problem of system identification for stochastic continuous-time
dynamics, based on a single finite-length state trajectory. We present a method
for estimating the possibly unstable open-loop matrix by employing properly
randomized control inputs. Then, we establish theoretical performance
guarantees showing that the estimation error decays with trajectory length, a
measure of excitability, and the signal-to-noise ratio, while it grows with
dimension. Numerical illustrations that showcase the rates of learning the
dynamics, will be provided as well. To perform the theoretical analysis, we
develop new technical tools that are of independent interest. That includes
non-asymptotic stochastic bounds for highly non-stationary martingales and
generalized laws of iterated logarithms, among others.",2024-09-17,"Reza Sadeghi Hafshejani, Mohamad Kazem Shirani Fradonbeh",http://arxiv.org/pdf/2409.11327v1,cs.LG
LPT++: Efficient Training on Mixture of Long-tailed Experts,"We introduce LPT++, a comprehensive framework for long-tailed classification
that combines parameter-efficient fine-tuning (PEFT) with a learnable model
ensemble. LPT++ enhances frozen Vision Transformers (ViTs) through the
integration of three core components. The first is a universal long-tailed
adaptation module, which aggregates long-tailed prompts and visual adapters to
adapt the pretrained model to the target domain, meanwhile improving its
discriminative ability. The second is the mixture of long-tailed experts
framework with a mixture-of-experts (MoE) scorer, which adaptively calculates
reweighting coefficients for confidence scores from both visual-only and
visual-language (VL) model experts to generate more accurate predictions.
Finally, LPT++ employs a three-phase training framework, wherein each critical
module is learned separately, resulting in a stable and effective long-tailed
classification training paradigm. Besides, we also propose the simple version
of LPT++ namely LPT, which only integrates visual-only pretrained ViT and
long-tailed prompts to formulate a single model method. LPT can clearly
illustrate how long-tailed prompts works meanwhile achieving comparable
performance without VL pretrained models. Experiments show that, with only ~1%
extra trainable parameters, LPT++ achieves comparable accuracy against all the
counterparts.",2024-09-17,"Bowen Dong, Pan Zhou, Wangmeng Zuo",http://arxiv.org/pdf/2409.11323v1,cs.LG
SOAP: Improving and Stabilizing Shampoo using Adam,"There is growing evidence of the effectiveness of Shampoo, a higher-order
preconditioning method, over Adam in deep learning optimization tasks. However,
Shampoo's drawbacks include additional hyperparameters and computational
overhead when compared to Adam, which only updates running averages of first-
and second-moment quantities. This work establishes a formal connection between
Shampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficient
approximation of Adam -- showing that Shampoo is equivalent to running
Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to
the design of a simpler and computationally efficient algorithm:
$\textbf{S}$hampo$\textbf{O}$ with $\textbf{A}$dam in the
$\textbf{P}$reconditioner's eigenbasis (SOAP).
  With regards to improving Shampoo's computational efficiency, the most
straightforward approach would be to simply compute Shampoo's
eigendecomposition less frequently. Unfortunately, as our empirical results
show, this leads to performance degradation that worsens with this frequency.
SOAP mitigates this degradation by continually updating the running average of
the second moment, just as Adam does, but in the current (slowly changing)
coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a
rotated space, it introduces only one additional hyperparameter (the
preconditioning frequency) compared to Adam. We empirically evaluate SOAP on
language model pre-training with 360m and 660m sized models. In the large batch
regime, SOAP reduces the number of iterations by over 40% and wall clock time
by over 35% compared to AdamW, with approximately 20% improvements in both
metrics compared to Shampoo. An implementation of SOAP is available at
https://github.com/nikhilvyas/SOAP.",2024-09-17,"Nikhil Vyas, Depen Morwani, Rosie Zhao, Mujin Kwun, Itai Shapira, David Brandfonbrener, Lucas Janson, Sham Kakade",http://arxiv.org/pdf/2409.11321v2,cs.LG
OATH: Efficient and Flexible Zero-Knowledge Proofs of End-to-End ML Fairness,"Though there is much interest in fair AI systems, the problem of fairness
noncompliance -- which concerns whether fair models are used in practice -- has
received lesser attention. Zero-Knowledge Proofs of Fairness (ZKPoF) address
fairness noncompliance by allowing a service provider to verify to external
parties that their model serves diverse demographics equitably, with guaranteed
confidentiality over proprietary model parameters and data. They have great
potential for building public trust and effective AI regulation, but no
previous techniques for ZKPoF are fit for real-world deployment. We present
OATH, the first ZKPoF framework that is (i) deployably efficient with
client-facing communication comparable to in-the-clear ML as a Service query
answering, and an offline audit phase that verifies an asymptotically constant
quantity of answered queries, (ii) deployably flexible with modularity for any
score-based classifier given a zero-knowledge proof of correct inference, (iii)
deployably secure with an end-to-end security model that guarantees
confidentiality and fairness across training, inference, and audits. We show
that OATH obtains strong robustness against malicious adversaries at concretely
efficient parameter settings. Notably, OATH provides a 1343x improvement to
runtime over previous work for neural network ZKPoF, and scales up to much
larger models -- even DNNs with tens of millions of parameters.",2024-09-17,"Olive Franzese, Ali Shahin Shamsabadi, Hamed Haddadi",http://arxiv.org/pdf/2410.02777v1,cs.LG
Beyond LoRA: Exploring Efficient Fine-Tuning Techniques for Time Series Foundational Models,"Time Series Foundation Models (TSFMs) have recently garnered attention for
their ability to model complex, large-scale time series data across domains
such as retail, finance, and transportation. However, their application to
sensitive, domain-specific fields like healthcare remains challenging,
primarily due to the difficulty of fine-tuning these models for specialized,
out-of-domain tasks with scarce publicly available datasets. In this work, we
explore the use of Parameter-Efficient Fine-Tuning (PEFT) techniques to address
these limitations, focusing on healthcare applications, particularly ICU vitals
forecasting for sepsis patients. We introduce and evaluate two selective
(BitFit and LayerNorm Tuning) and two additive (VeRA and FourierFT) PEFT
techniques on multiple configurations of the Chronos TSFM for forecasting vital
signs of sepsis patients. Our comparative analysis demonstrates that some of
these PEFT methods outperform LoRA in terms of parameter efficiency and domain
adaptation, establishing state-of-the-art (SOTA) results in ICU vital
forecasting tasks. Interestingly, FourierFT applied to the Chronos (Tiny)
variant surpasses the SOTA model while fine-tuning only 2,400 parameters
compared to the 700K parameters of the benchmark.",2024-09-17,"Divij Gupta, Anubhav Bhatti, Surajsinh Parmar",http://arxiv.org/pdf/2409.11302v1,cs.LG
EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage,"Generalist web agents have demonstrated remarkable potential in autonomously
completing a wide range of tasks on real websites, significantly boosting human
productivity. However, web tasks, such as booking flights, usually involve
users' PII, which may be exposed to potential privacy risks if web agents
accidentally interact with compromised websites, a scenario that remains
largely unexplored in the literature. In this work, we narrow this gap by
conducting the first study on the privacy risks of generalist web agents in
adversarial environments. First, we present a realistic threat model for
attacks on the website, where we consider two adversarial targets: stealing
users' specific PII or the entire user request. Then, we propose a novel attack
method, termed Environmental Injection Attack (EIA). EIA injects malicious
content designed to adapt well to environments where the agents operate and our
work instantiates EIA specifically for privacy scenarios in web environments.
We collect 177 action steps that involve diverse PII categories on realistic
websites from the Mind2Web, and conduct experiments using one of the most
capable generalist web agent frameworks to date. The results demonstrate that
EIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user
request. Additionally, by accessing the stealthiness and experimenting with a
defensive system prompt, we indicate that EIA is hard to detect and mitigate.
Notably, attacks that are not well adapted for a webpage can be detected via
human inspection, leading to our discussion about the trade-off between
security and autonomy. However, extra attackers' efforts can make EIA
seamlessly adapted, rendering such supervision ineffective. Thus, we further
discuss the defenses at the pre- and post-deployment stages of the websites
without relying on human supervision and call for more advanced defense
strategies.",2024-09-17,"Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun",http://arxiv.org/pdf/2409.11295v5,cs.LG
Bypassing the Popularity Bias: Repurposing Models for Better Long-Tail Recommendation,"Recommender systems play a crucial role in shaping information we encounter
online, whether on social media or when using content platforms, thereby
influencing our beliefs, choices, and behaviours. Many recent works address the
issue of fairness in recommender systems, typically focusing on topics like
ensuring equal access to information and opportunities for all individual users
or user groups, promoting diverse content to avoid filter bubbles and echo
chambers, enhancing transparency and explainability, and adhering to ethical
and sustainable practices. In this work, we aim to achieve a more equitable
distribution of exposure among publishers on an online content platform, with a
particular focus on those who produce high quality, long-tail content that may
be unfairly disadvantaged. We propose a novel approach of repurposing existing
components of an industrial recommender system to deliver valuable exposure to
underrepresented publishers while maintaining high recommendation quality. To
demonstrate the efficiency of our proposal, we conduct large-scale online AB
experiments, report results indicating desired outcomes and share several
insights from long-term application of the approach in the production setting.",2024-09-17,"Václav Blahut, Karel Koupil",http://arxiv.org/pdf/2410.02776v1,cs.LG
Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5,"The surge of digital documents in various formats, including less
standardized documents such as business reports and environmental assessments,
underscores the growing importance of Document Understanding. While Large
Language Models (LLMs) have showcased prowess across diverse natural language
processing tasks, their direct application to Document Understanding remains a
challenge. Previous research has demonstrated the utility of LLMs in this
domain, yet their significant computational demands make them challenging to
deploy effectively. Additionally, proprietary Blackbox LLMs often outperform
their open-source counterparts, posing a barrier to widespread accessibility.
In this paper, we delve into the realm of document understanding, leveraging
distillation methods to harness the power of large LLMs while accommodating
computational limitations. Specifically, we present a novel approach wherein we
distill document understanding knowledge from the proprietary LLM ChatGPT into
FLAN-T5. Our methodology integrates labeling and curriculum-learning mechanisms
to facilitate efficient knowledge transfer. This work contributes to the
advancement of document understanding methodologies by offering a scalable
solution that bridges the gap between resource-intensive LLMs and practical
applications. Our findings underscore the potential of distillation techniques
in facilitating the deployment of sophisticated language models in real-world
scenarios, thereby fostering advancements in natural language processing and
document comprehension domains.",2024-09-17,"Marcel Lamott, Muhammad Armaghan Shakir",http://arxiv.org/pdf/2409.11282v1,cs.LG
LOLA -- An Open-Source Massively Multilingual Large Language Model,"This paper presents LOLA, a massively multilingual large language model
trained on more than 160 languages using a sparse Mixture-of-Experts
Transformer architecture. Our architectural and implementation choices address
the challenge of harnessing linguistic diversity while maintaining efficiency
and avoiding the common pitfalls of multilinguality. Our analysis of the
evaluation results shows competitive performance in natural language generation
and understanding tasks. Additionally, we demonstrate how the learned
expert-routing mechanism exploits implicit phylogenetic linguistic patterns to
potentially alleviate the curse of multilinguality. We provide an in-depth look
at the training process, an analysis of the datasets, and a balanced
exploration of the model's strengths and limitations. As an open-source model,
LOLA promotes reproducibility and serves as a robust foundation for future
research. Our findings enable the development of compute-efficient multilingual
models with strong, scalable performance across languages.",2024-09-17,"Nikit Srivastava, Denis Kuchelev, Tatiana Moteu Ngoli, Kshitij Shetty, Michael Röder, Hamada Zahera, Diego Moussallem, Axel-Cyrille Ngonga Ngomo",http://arxiv.org/pdf/2409.11272v7,cs.LG
Geometry Aware Meta-Learning Neural Network for Joint Phase and Precoder Optimization in RIS,"In reconfigurable intelligent surface (RIS) aided systems, the joint
optimization of the precoder matrix at the base station and the phase shifts of
the RIS elements involves significant complexity. In this paper, we propose a
complex-valued, geometry aware meta-learning neural network that maximizes the
weighted sum rate in a multi-user multiple input single output system. By
leveraging the complex circle geometry for phase shifts and spherical geometry
for the precoder, the optimization occurs on Riemannian manifolds, leading to
faster convergence. We use a complex-valued neural network for phase shifts and
an Euler inspired update for the precoder network. Our approach outperforms
existing neural network-based algorithms, offering higher weighted sum rates,
lower power consumption, and significantly faster convergence. Specifically, it
converges faster by nearly 100 epochs, with a 0.7 bps improvement in weighted
sum rate and a 1.8 dBm power gain when compared with existing work.",2024-09-17,"Dahlia Devapriya, Sheetal Kalyani",http://arxiv.org/pdf/2409.11270v1,cs.LG
Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids,"This work proposes an approach that integrates reinforcement learning and
model predictive control (MPC) to solve finite-horizon optimal control problems
in mixed-logical dynamical systems efficiently. Optimization-based control of
such systems with discrete and continuous decision variables entails the online
solution of mixed-integer linear programs, which suffer from the curse of
dimensionality. Our approach aims to mitigate this issue by decoupling the
decision on the discrete variables from the decision on the continuous
variables. In the proposed approach, reinforcement learning determines the
discrete decision variables and simplifies the online optimization problem of
the MPC controller from a mixed-integer linear program to a linear program,
significantly reducing the computational time. A fundamental contribution of
this work is the definition of the decoupled Q-function, which plays a crucial
role in making the learning problem tractable in a combinatorial action space.
We motivate the use of recurrent neural networks to approximate the decoupled
Q-function and show how they can be employed in a reinforcement learning
setting. Simulation experiments on a microgrid system using real-world data
demonstrate that the proposed method substantially reduces the online
computation time of MPC while maintaining high feasibility and low
suboptimality.",2024-09-17,"Caio Fabio Oliveira da Silva, Azita Dabiri, Bart De Schutter",http://arxiv.org/pdf/2409.11267v2,cs.LG
LC-Protonets: Multi-Label Few-Shot Learning for World Music Audio Tagging,"We introduce Label-Combination Prototypical Networks (LC-Protonets) to
address the problem of multi-label few-shot classification, where a model must
generalize to new classes based on only a few available examples. Extending
Prototypical Networks, LC-Protonets generate one prototype per label
combination, derived from the power set of labels present in the limited
training items, rather than one prototype per label. Our method is applied to
automatic audio tagging across diverse music datasets, covering various
cultures and including both modern and traditional music, and is evaluated
against existing approaches in the literature. The results demonstrate a
significant performance improvement in almost all domains and training setups
when using LC-Protonets for multi-label classification. In addition to training
a few-shot learning model from scratch, we explore the use of a pre-trained
model, obtained via supervised learning, to embed items in the feature space.
Fine-tuning improves the generalization ability of all methods, yet
LC-Protonets achieve high-level performance even without fine-tuning, in
contrast to the comparative approaches. We finally analyze the scalability of
the proposed method, providing detailed quantitative metrics from our
experiments. The implementation and experimental setup are made publicly
available, offering a benchmark for future research.",2024-09-17,"Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos",http://arxiv.org/pdf/2409.11264v2,cs.LG
A Deep Learning Approach for User-Centric Clustering in Cell-Free Massive MIMO Systems,"Contrary to conventional massive MIMO cellular configurations plagued by
inter-cell interference, cell-free massive MIMO systems distribute network
resources across the coverage area, enabling users to connect with multiple
access points (APs) and boosting both system capacity and fairness across user.
In such systems, one critical functionality is the association between APs and
users: determining the optimal association is indeed a combinatorial problem of
prohibitive complexity. In this paper, a solution based on deep learning is
thus proposed to solve the user clustering problem aimed at maximizing the sum
spectral efficiency while controlling the number of active connections. The
proposed solution can scale effectively with the number of users, leveraging
long short-term memory cells to operate without the need for retraining.
Numerical results show the effectiveness of the proposed solution, even in the
presence of imperfect channel state information due to pilot contamination.",2024-09-17,"Giovanni Di Gennaro, Amedeo Buonanno, Gianmarco Romano, Stefano Buzzi, Francesco A. N Palmieri",http://arxiv.org/pdf/2410.02775v1,cs.LG
Towards Novel Malicious Packet Recognition: A Few-Shot Learning Approach,"As the complexity and connectivity of networks increase, the need for novel
malware detection approaches becomes imperative. Traditional security defenses
are becoming less effective against the advanced tactics of today's
cyberattacks. Deep Packet Inspection (DPI) has emerged as a key technology in
strengthening network security, offering detailed analysis of network traffic
that goes beyond simple metadata analysis. DPI examines not only the packet
headers but also the payload content within, offering a thorough insight into
the data traversing the network. This study proposes a novel approach that
leverages a large language model (LLM) and few-shot learning to accurately
recognizes novel, unseen malware types with few labels samples. Our proposed
approach uses a pretrained LLM on known malware types to extract the embeddings
from packets. The embeddings are then used alongside few labeled samples of an
unseen malware type. This technique is designed to acclimate the model to
different malware representations, further enabling it to generate robust
embeddings for each trained and unseen classes. Following the extraction of
embeddings from the LLM, few-shot learning is utilized to enhance performance
with minimal labeled data. Our evaluation, which utilized two renowned
datasets, focused on identifying malware types within network traffic and
Internet of Things (IoT) environments. Our approach shows promising results
with an average accuracy of 86.35% and F1-Score of 86.40% on different malware
types across the two datasets.",2024-09-17,"Kyle Stein, Andrew A. Mahyari, Guillermo Francia III, Eman El-Sheikh",http://arxiv.org/pdf/2409.11254v1,cs.LG
Evaluation of pretrained language models on music understanding,"Music-text multimodal systems have enabled new approaches to Music
Information Research (MIR) applications such as audio-to-text and text-to-audio
retrieval, text-based song generation, and music captioning. Despite the
reported success, little effort has been put into evaluating the musical
knowledge of Large Language Models (LLM). In this paper, we demonstrate that
LLMs suffer from 1) prompt sensitivity, 2) inability to model negation (e.g.
'rock song without guitar'), and 3) sensitivity towards the presence of
specific words. We quantified these properties as a triplet-based accuracy,
evaluating the ability to model the relative similarity of labels in a
hierarchical ontology. We leveraged the Audioset ontology to generate triplets
consisting of an anchor, a positive (relevant) label, and a negative (less
relevant) label for the genre and instruments sub-tree. We evaluated the
triplet-based musical knowledge for six general-purpose Transformer-based
models. The triplets obtained through this methodology required filtering, as
some were difficult to judge and therefore relatively uninformative for
evaluation purposes. Despite the relatively high accuracy reported,
inconsistencies are evident in all six models, suggesting that off-the-shelf
LLMs need adaptation to music before use.",2024-09-17,"Yannis Vasilakis, Rachel Bittner, Johan Pauwels",http://arxiv.org/pdf/2409.11449v1,cs.LG
Spontaneous Informal Speech Dataset for Punctuation Restoration,"Presently, punctuation restoration models are evaluated almost solely on
well-structured, scripted corpora. On the other hand, real-world ASR systems
and post-processing pipelines typically apply towards spontaneous speech with
significant irregularities, stutters, and deviations from perfect grammar. To
address this discrepancy, we introduce SponSpeech, a punctuation restoration
dataset derived from informal speech sources, which includes punctuation and
casing information. In addition to publicly releasing the dataset, we
contribute a filtering pipeline that can be used to generate more data. Our
filtering pipeline examines the quality of both speech audio and transcription
text. We also carefully construct a ``challenging"" test set, aimed at
evaluating models' ability to leverage audio information to predict otherwise
grammatically ambiguous punctuation. SponSpeech is available at
https://github.com/GitHubAccountAnonymous/PR, along with all code for dataset
building and model runs.",2024-09-17,"Xing Yi Liu, Homayoon Beigi",http://arxiv.org/pdf/2409.11241v1,cs.LG
"Federated Learning with Integrated Sensing, Communication, and Computation: Frameworks and Performance Analysis","With the emergence of integrated sensing, communication, and computation
(ISCC) in the upcoming 6G era, federated learning with ISCC (FL-ISCC),
integrating sample collection, local training, and parameter exchange and
aggregation, has garnered increasing interest for enhancing training
efficiency. Currently, FL-ISCC primarily includes two algorithms: FedAVG-ISCC
and FedSGD-ISCC. However, the theoretical understanding of the performance and
advantages of these algorithms remains limited. To address this gap, we
investigate a general FL-ISCC framework, implementing both FedAVG-ISCC and
FedSGD-ISCC. We experimentally demonstrate the substantial potential of the
ISCC framework in reducing latency and energy consumption in FL. Furthermore,
we provide a theoretical analysis and comparison. The results reveal that:1)
Both sample collection and communication errors negatively impact algorithm
performance, highlighting the need for careful design to optimize FL-ISCC
applications. 2) FedAVG-ISCC performs better than FedSGD-ISCC under IID data
due to its advantage with multiple local updates. 3) FedSGD-ISCC is more robust
than FedAVG-ISCC under non-IID data, where the multiple local updates in
FedAVG-ISCC worsen performance as non-IID data increases. FedSGD-ISCC maintains
performance levels similar to IID conditions. 4) FedSGD-ISCC is more resilient
to communication errors than FedAVG-ISCC, which suffers from significant
performance degradation as communication errors increase.Extensive simulations
confirm the effectiveness of the FL-ISCC framework and validate our theoretical
analysis.",2024-09-17,"Yipeng Liang, Qimei Chen, Hao Jiang",http://arxiv.org/pdf/2409.11240v1,cs.LG
Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems,"Tracking controllers enable robotic systems to accurately follow planned
reference trajectories. In particular, reinforcement learning (RL) has shown
promise in the synthesis of controllers for systems with complex dynamics and
modest online compute budgets. However, the poor sample efficiency of RL and
the challenges of reward design make training slow and sometimes unstable,
especially for high-dimensional systems. In this work, we leverage the inherent
Lie group symmetries of robotic systems with a floating base to mitigate these
challenges when learning tracking controllers. We model a general tracking
problem as a Markov decision process (MDP) that captures the evolution of both
the physical and reference states. Next, we prove that symmetry in the
underlying dynamics and running costs leads to an MDP homomorphism, a mapping
that allows a policy trained on a lower-dimensional ""quotient"" MDP to be lifted
to an optimal tracking controller for the original system. We compare this
symmetry-informed approach to an unstructured baseline, using Proximal Policy
Optimization (PPO) to learn tracking controllers for three systems: the
Particle (a forced point mass), the Astrobee (a fullyactuated space robot), and
the Quadrotor (an underactuated system). Results show that a symmetry-aware
approach both accelerates training and reduces tracking error at convergence.",2024-09-17,"Jake Welde, Nishanth Rao, Pratik Kunapuli, Dinesh Jayaraman, Vijay Kumar",http://arxiv.org/pdf/2409.11238v3,cs.LG
Cost-informed dimensionality reduction for structural digital twin technologies,"Classification models are a key component of structural digital twin
technologies used for supporting asset management decision-making. An important
consideration when developing classification models is the dimensionality of
the input, or feature space, used. If the dimensionality is too high, then the
`curse of dimensionality' may rear its ugly head; manifesting as reduced
predictive performance. To mitigate such effects, practitioners can employ
dimensionality reduction techniques. The current paper formulates a
decision-theoretic approach to dimensionality reduction for structural asset
management. In this approach, the aim is to keep incurred misclassification
costs to a minimum, as the dimensionality is reduced and discriminatory
information may be lost. This formulation is constructed as an eigenvalue
problem, with separabilities between classes weighted according to the cost of
misclassifying them when considered in the context of a decision process. The
approach is demonstrated using a synthetic case study.",2024-09-17,"Aidan J. Hughes, Keith Worden, Nikolaos Dervilis, Timothy J. Rogers",http://arxiv.org/pdf/2409.11236v1,cs.LG
Learning Source Disentanglement in Neural Audio Codec,"Neural audio codecs have significantly advanced audio compression by
efficiently converting continuous audio signals into discrete tokens. These
codecs preserve high-quality sound and enable sophisticated sound generation
through generative models trained on these tokens. However, existing neural
codec models are typically trained on large, undifferentiated audio datasets,
neglecting the essential discrepancies between sound domains like speech,
music, and environmental sound effects. This oversight complicates data
modeling and poses additional challenges to the controllability of sound
generation. To tackle these issues, we introduce the Source-Disentangled Neural
Audio Codec (SD-Codec), a novel approach that combines audio coding and source
separation. By jointly learning audio resynthesis and separation, SD-Codec
explicitly assigns audio signals from different domains to distinct codebooks,
sets of discrete representations. Experimental results indicate that SD-Codec
not only maintains competitive resynthesis quality but also, supported by the
separation results, demonstrates successful disentanglement of different
sources in the latent space, thereby enhancing interpretability in audio codec
and providing potential finer control over the audio generation process.",2024-09-17,"Xiaoyu Bie, Xubo Liu, Gaël Richard",http://arxiv.org/pdf/2409.11228v2,cs.LG
Estimating the Unobservable Components of Electricity Demand Response with Inverse Optimization,"Understanding and predicting the electricity demand responses to prices are
critical activities for system operators, retailers, and regulators. While
conventional machine learning and time series analyses have been adequate for
the routine demand patterns that have adapted only slowly over many years, the
emergence of active consumers with flexible assets such as solar-plus-storage
systems, and electric vehicles, introduces new challenges. These active
consumers exhibit more complex consumption patterns, the drivers of which are
often unobservable to the retailers and system operators. In practice, system
operators and retailers can only monitor the net demand (metered at grid
connection points), which reflects the overall energy consumption or production
exchanged with the grid. As a result, all ""behind-the-meter"" activities-such as
the use of flexibility-remain hidden from these entities. Such behind-the-meter
behavior may be controlled by third party agents or incentivized by tariffs; in
either case, the retailer's revenue and the system loads would be impacted by
these activities behind the meter, but their details can only be inferred. We
define the main components of net demand, as baseload, flexible, and
self-generation, each having nonlinear responses to market price signals. As
flexible demand response and self generation are increasing, this raises a
pressing question of whether existing methods still perform well and, if not,
whether there is an alternative way to understand and project the unobserved
components of behavior. In response to this practical challenge, we evaluate
the potential of a data-driven inverse optimization (IO) methodology. This
approach characterizes decomposed consumption patterns without requiring direct
observation of behind-the-meter behavior or device-level metering [...]",2024-09-17,"Adrian Esteban-Perez, Derek Bunn, Yashar Ghiassi-Farrokhfal",http://arxiv.org/pdf/2410.02774v1,cs.LG
"Score Forgetting Distillation: A Swift, Data-Free Method for Machine Unlearning in Diffusion Models","The machine learning community is increasingly recognizing the importance of
fostering trust and safety in modern generative AI (GenAI) models. We posit
machine unlearning (MU) as a crucial foundation for developing safe, secure,
and trustworthy GenAI models. Traditional MU methods often rely on stringent
assumptions and require access to real data. This paper introduces Score
Forgetting Distillation (SFD), an innovative MU approach that promotes the
forgetting of undesirable information in diffusion models by aligning the
conditional scores of ""unsafe"" classes or concepts with those of ""safe"" ones.
To eliminate the need for real data, our SFD framework incorporates a
score-based MU loss into the score distillation objective of a pretrained
diffusion model. This serves as a regularization term that preserves desired
generation capabilities while enabling the production of synthetic data through
a one-step generator. Our experiments on pretrained label-conditional and
text-to-image diffusion models demonstrate that our method effectively
accelerates the forgetting of target classes or concepts during generation,
while preserving the quality of other classes or concepts. This unlearned and
distilled diffusion not only pioneers a novel concept in MU but also
accelerates the generation speed of diffusion models. Our experiments and
studies on a range of diffusion models and datasets confirm that our approach
is generalizable, effective, and advantageous for MU in diffusion models. Code
is available at https://github.com/tqch/score-forgetting-distillation.
($\textbf{Warning:}$ This paper contains sexually explicit imagery, discussions
of pornography, racially-charged terminology, and other content that some
readers may find disturbing, distressing, and/or offensive.)",2024-09-17,"Tianqi Chen, Shujian Zhang, Mingyuan Zhou",http://arxiv.org/pdf/2409.11219v3,cs.LG
Multi-modal Atmospheric Sensing to Augment Wearable IMU-Based Hand Washing Detection,"Hand washing is a crucial part of personal hygiene. Hand washing detection is
a relevant topic for wearable sensing with applications in the medical and
professional fields. Hand washing detection can be used to aid workers in
complying with hygiene rules. Hand washing detection using body-worn IMU-based
sensor systems has been shown to be a feasible approach, although, for some
reported results, the specificity of the detection was low, leading to a high
rate of false positives. In this work, we present a novel, open-source
prototype device that additionally includes a humidity, temperature, and
barometric sensor. We contribute a benchmark dataset of 10 participants and 43
hand-washing events and perform an evaluation of the sensors' benefits. Added
to that, we outline the usefulness of the additional sensor in both the
annotation pipeline and the machine learning models. By visual inspection, we
show that especially the humidity sensor registers a strong increase in the
relative humidity during a hand-washing activity. A machine learning analysis
of our data shows that distinct features benefiting from such relative humidity
patterns remain to be identified.",2024-09-17,"Robin Burchard, Kristof Van Laerhoven",http://arxiv.org/pdf/2410.03549v1,cs.LG
"LoRa Communication for Agriculture 4.0: Opportunities, Challenges, and Future Directions","The emerging field of smart agriculture leverages the Internet of Things
(IoT) to revolutionize farming practices. This paper investigates the
transformative potential of Long Range (LoRa) technology as a key enabler of
long-range wireless communication for agricultural IoT systems. By reviewing
existing literature, we identify a gap in research specifically focused on
LoRa's prospects and challenges from a communication perspective in smart
agriculture. We delve into the details of LoRa-based agricultural networks,
covering network architecture design, Physical Layer (PHY) considerations
tailored to the agricultural environment, and channel modeling techniques that
account for soil characteristics. The paper further explores relaying and
routing mechanisms that address the challenges of extending network coverage
and optimizing data transmission in vast agricultural landscapes. Transitioning
to practical aspects, we discuss sensor deployment strategies and energy
management techniques, offering insights for real-world deployments. A
comparative analysis of LoRa with other wireless communication technologies
employed in agricultural IoT applications highlights its strengths and
weaknesses in this context. Furthermore, the paper outlines several future
research directions to leverage the potential of LoRa-based agriculture 4.0.
These include advancements in channel modeling for diverse farming
environments, novel relay routing algorithms, integrating emerging sensor
technologies like hyper-spectral imaging and drone-based sensing, on-device
Artificial Intelligence (AI) models, and sustainable solutions. This survey can
guide researchers, technologists, and practitioners to understand, implement,
and propel smart agriculture initiatives using LoRa technology.",2024-09-17,"Lameya Aldhaheri, Noor Alshehhi, Irfana Ilyas Jameela Manzil, Ruhul Amin Khalil, Shumaila Javaid, Nasir Saeed, Mohamed-Slim Alouini",http://arxiv.org/pdf/2409.11200v1,cs.LG
High-Resolution Speech Restoration with Latent Diffusion Model,"Traditional speech enhancement methods often oversimplify the task of
restoration by focusing on a single type of distortion. Generative models that
handle multiple distortions frequently struggle with phone reconstruction and
high-frequency harmonics, leading to breathing and gasping artifacts that
reduce the intelligibility of reconstructed speech. These models are also
computationally demanding, and many solutions are restricted to producing
outputs in the wide-band frequency range, which limits their suitability for
professional applications. To address these challenges, we propose Hi-ResLDM, a
novel generative model based on latent diffusion designed to remove multiple
distortions and restore speech recordings to studio quality, sampled at 48kHz.
We benchmark Hi-ResLDM against state-of-the-art methods that leverage GAN and
Conditional Flow Matching (CFM) components, demonstrating superior performance
in regenerating high-frequency-band details. Hi-ResLDM not only excels in
non-instrusive metrics but is also consistently preferred in human evaluation
and performs competitively on intrusive evaluations, making it ideal for
high-resolution speech restoration.",2024-09-17,"Tushar Dhyani, Florian Lux, Michele Mancusi, Giorgio Fabbro, Fritz Hohl, Ngoc Thang Vu",http://arxiv.org/pdf/2409.11145v2,cs.LG
"Use the Force, Bot! -- Force-Aware ProDMP with Event-Based Replanning","Movement Primitives (MPs) are a well-established method for representing and
generating modular robot trajectories. This work presents FA-ProDMP, a new
approach which introduces force awareness to Probabilistic Dynamic Movement
Primitives (ProDMP). FA-ProDMP adapts the trajectory during runtime to account
for measured and desired forces. It offers smooth trajectories and captures
position and force correlations over multiple trajectories, e.g. a set of human
demonstrations. FA-ProDMP supports multiple axes of force and is thus agnostic
to cartesian or joint space control. This makes FA-ProDMP a valuable tool for
learning contact rich manipulation tasks such as polishing, cutting or
industrial assembly from demonstration. In order to reliably evaluate
FA-ProDMP, this work additionally introduces a modular, 3D printed task suite
called POEMPEL, inspired by the popular Lego Technic pins. POEMPEL mimics
industrial peg-in-hole assembly tasks with force requirements. It offers
multiple parameters of adjustment, such as position, orientation and plug
stiffness level, thus varying the direction and amount of required forces. Our
experiments show that FA-ProDMP outperforms other MP formulations on the
POEMPEL setup and a electrical power plug insertion task, due to its replanning
capabilities based on the measured forces. These findings highlight how
FA-ProDMP enhances the performance of robotic systems in contact-rich
manipulation tasks.",2024-09-17,"Paul Werner Lödige, Maximilian Xiling Li, Rudolf Lioutikov",http://arxiv.org/pdf/2409.11144v1,cs.LG
Sample Complexity Bounds for Linear System Identification from a Finite Set,"This paper considers a finite sample perspective on the problem of
identifying an LTI system from a finite set of possible systems using
trajectory data. To this end, we use the maximum likelihood estimator to
identify the true system and provide an upper bound for its sample complexity.
Crucially, the derived bound does not rely on a potentially restrictive
stability assumption. Additionally, we leverage tools from information theory
to provide a lower bound to the sample complexity that holds independently of
the used estimator. The derived sample complexity bounds are analyzed
analytically and numerically.",2024-09-17,"Nicolas Chatzikiriakos, Andrea Iannelli",http://arxiv.org/pdf/2409.11141v2,cs.LG
Scale generalisation properties of extended scale-covariant and scale-invariant Gaussian derivative networks on image datasets with spatial scaling variations,"This paper presents an in-depth analysis of the scale generalisation
properties of the scale-covariant and scale-invariant Gaussian derivative
networks, complemented with both conceptual and algorithmic extensions. For
this purpose, Gaussian derivative networks (GaussDerNets) are evaluated on new
rescaled versions of the Fashion-MNIST and the CIFAR-10 datasets, with spatial
scaling variations over a factor of 4 in the testing data, that are not present
in the training data. Additionally, evaluations on the previously existing STIR
datasets show that the GaussDerNets achieve better scale generalisation than
previously reported for these datasets for other types of deep networks.
  We first experimentally demonstrate that the GaussDerNets have quite good
scale generalisation properties on the new datasets, and that average pooling
of feature responses over scales may sometimes also lead to better results than
the previously used approach of max pooling over scales. Then, we demonstrate
that using a spatial max pooling mechanism after the final layer enables
localisation of non-centred objects in image domain, with maintained scale
generalisation properties. We also show that regularisation during training, by
applying dropout across the scale channels, referred to as scale-channel
dropout, improves both the performance and the scale generalisation.
  In additional ablation studies, we demonstrate that discretisations of
GaussDerNets, based on the discrete analogue of the Gaussian kernel in
combination with central difference operators, perform best or among the best,
compared to a set of other discrete approximations of the Gaussian derivative
kernels.
  Finally, by visualising the activation maps and the learned receptive fields,
we demonstrate that the GaussDerNets have very good explainability properties.",2024-09-17,"Andrzej Perzanowski, Tony Lindeberg",http://arxiv.org/pdf/2409.11140v2,cs.LG
Learning Generalized Hamiltonians using fully Symplectic Mappings,"Many important physical systems can be described as the evolution of a
Hamiltonian system, which has the important property of being conservative,
that is, energy is conserved throughout the evolution. Physics Informed Neural
Networks and in particular Hamiltonian Neural Networks have emerged as a
mechanism to incorporate structural inductive bias into the NN model. By
ensuring physical invariances are conserved, the models exhibit significantly
better sample complexity and out-of-distribution accuracy than standard NNs.
Learning the Hamiltonian as a function of its canonical variables, typically
position and velocity, from sample observations of the system thus becomes a
critical task in system identification and long-term prediction of system
behavior. However, to truly preserve the long-run physical conservation
properties of Hamiltonian systems, one must use symplectic integrators for a
forward pass of the system's simulation. While symplectic schemes have been
used in the literature, they are thus far limited to situations when they
reduce to explicit algorithms, which include the case of separable Hamiltonians
or augmented non-separable Hamiltonians. We extend it to generalized
non-separable Hamiltonians, and noting the self-adjoint property of symplectic
integrators, we bypass computationally intensive backpropagation through an ODE
solver. We show that the method is robust to noise and provides a good
approximation of the system Hamiltonian when the state variables are sampled
from a noisy observation. In the numerical results, we show the performance of
the method concerning Hamiltonian reconstruction and conservation, indicating
its particular advantage for non-separable systems.",2024-09-17,"Harsh Choudhary, Chandan Gupta, Vyacheslav kungrutsev, Melvin Leok, Georgios Korpas",http://arxiv.org/pdf/2409.11138v2,cs.LG
Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models,"Instruction-tuned language models (LM) are able to respond to imperative
commands, providing a more natural user interface compared to their base
counterparts. In this work, we present Promptriever, the first retrieval model
able to be prompted like an LM. To train Promptriever, we curate and release a
new instance-level instruction training set from MS MARCO, spanning nearly 500k
instances. Promptriever not only achieves strong performance on standard
retrieval tasks, but also follows instructions. We observe: (1) large gains
(reaching SoTA) on following detailed relevance instructions (+14.3 p-MRR /
+3.1 nDCG on FollowIR), (2) significantly increased robustness to lexical
choices/phrasing in the query+instruction (+12.9 Robustness@10 on InstructIR),
and (3) the ability to perform hyperparameter search via prompting to reliably
improve retrieval performance (+1.4 average increase on BEIR). Promptriever
demonstrates that retrieval models can be controlled with prompts on a
per-query basis, setting the stage for future work aligning LM prompting
techniques with information retrieval.",2024-09-17,"Orion Weller, Benjamin Van Durme, Dawn Lawrie, Ashwin Paranjape, Yuhao Zhang, Jack Hessel",http://arxiv.org/pdf/2409.11136v1,cs.LG
Can Graph Reordering Speed Up Graph Neural Network Training? An Experimental Study,"Graph neural networks (GNNs) are a type of neural network capable of learning
on graph-structured data. However, training GNNs on large-scale graphs is
challenging due to iterative aggregations of high-dimensional features from
neighboring vertices within sparse graph structures combined with neural
network operations. The sparsity of graphs frequently results in suboptimal
memory access patterns and longer training time. Graph reordering is an
optimization strategy aiming to improve the graph data layout. It has shown to
be effective to speed up graph analytics workloads, but its effect on the
performance of GNN training has not been investigated yet. The generalization
of reordering to GNN performance is nontrivial, as multiple aspects must be
considered: GNN hyper-parameters such as the number of layers, the number of
hidden dimensions, and the feature size used in the GNN model, neural network
operations, large intermediate vertex states, and GPU acceleration.
  In our work, we close this gap by performing an empirical evaluation of 12
reordering strategies in two state-of-the-art GNN systems, PyTorch Geometric
and Deep Graph Library. Our results show that graph reordering is effective in
reducing training time for CPU- and GPU-based training, respectively. Further,
we find that GNN hyper-parameters influence the effectiveness of reordering,
that reordering metrics play an important role in selecting a reordering
strategy, that lightweight reordering performs better for GPU-based than for
CPU-based training, and that invested reordering time can in many cases be
amortized.",2024-09-17,"Nikolai Merkel, Pierre Toussing, Ruben Mayer, Hans-Arno Jacobsen",http://arxiv.org/pdf/2409.11129v1,cs.LG
Gradient-free Post-hoc Explainability Using Distillation Aided Learnable Approach,"The recent advancements in artificial intelligence (AI), with the release of
several large models having only query access, make a strong case for
explainability of deep models in a post-hoc gradient free manner. In this
paper, we propose a framework, named distillation aided explainability (DAX),
that attempts to generate a saliency-based explanation in a model agnostic
gradient free application. The DAX approach poses the problem of explanation in
a learnable setting with a mask generation network and a distillation network.
The mask generation network learns to generate the multiplier mask that finds
the salient regions of the input, while the student distillation network aims
to approximate the local behavior of the black-box model. We propose a joint
optimization of the two networks in the DAX framework using the locally
perturbed input samples, with the targets derived from input-output access to
the black-box model. We extensively evaluate DAX across different modalities
(image and audio), in a classification setting, using a diverse set of
evaluations (intersection over union with ground truth, deletion based and
subjective human evaluation based measures) and benchmark it with respect to
$9$ different methods. In these evaluations, the DAX significantly outperforms
the existing approaches on all modalities and evaluation metrics.",2024-09-17,"Debarpan Bhattacharya, Amir H. Poorjam, Deepak Mittal, Sriram Ganapathy",http://arxiv.org/pdf/2409.11123v1,cs.LG
ULOC: Learning to Localize in Complex Large-Scale Environments with Ultra-Wideband Ranges,"While UWB-based methods can achieve high localization accuracy in small-scale
areas, their accuracy and reliability are significantly challenged in
large-scale environments. In this paper, we propose a learning-based framework
named ULOC for Ultra-Wideband (UWB) based localization in such complex
large-scale environments. First, anchors are deployed in the environment
without knowledge of their actual position. Then, UWB observations are
collected when the vehicle travels in the environment. At the same time,
map-consistent pose estimates are developed from registering (onboard
self-localization) data with the prior map to provide the training labels. We
then propose a network based on MAMBA that learns the ranging patterns of UWBs
over a complex large-scale environment. The experiment demonstrates that our
solution can ensure high localization accuracy on a large scale compared to the
state-of-the-art. We release our source code to benefit the community at
https://github.com/brytsknguyen/uloc.",2024-09-17,"Thien-Minh Nguyen, Yizhuo Yang, Tien-Dat Nguyen, Shenghai Yuan, Lihua Xie",http://arxiv.org/pdf/2409.11122v1,cs.LG
Fractional Naive Bayes (FNB): non-convex optimization for a parsimonious weighted selective naive Bayes classifier,"We study supervised classification for datasets with a very large number of
input variables. The na\""ive Bayes classifier is attractive for its simplicity,
scalability and effectiveness in many real data applications. When the strong
na\""ive Bayes assumption of conditional independence of the input variables
given the target variable is not valid, variable selection and model averaging
are two common ways to improve the performance. In the case of the na\""ive
Bayes classifier, the resulting weighting scheme on the models reduces to a
weighting scheme on the variables. Here we focus on direct estimation of
variable weights in such a weighted na\""ive Bayes classifier. We propose a
sparse regularization of the model log-likelihood, which takes into account
prior penalization costs related to each input variable. Compared to averaging
based classifiers used up until now, our main goal is to obtain parsimonious
robust models with less variables and equivalent performance. The direct
estimation of the variable weights amounts to a non-convex optimization problem
for which we propose and compare several two-stage algorithms. First, the
criterion obtained by convex relaxation is minimized using several variants of
standard gradient methods. Then, the initial non-convex optimization problem is
solved using local optimization methods initialized with the result of the
first stage. The various proposed algorithms result in optimization-based
weighted na\""ive Bayes classifiers, that are evaluated on benchmark datasets
and positioned w.r.t. to a reference averaging-based classifier.",2024-09-17,"Carine Hue, Marc Boullé",http://arxiv.org/pdf/2409.11100v1,cs.LG
Efficient Numerical Calibration of Water Delivery Network Using Short-Burst Hydrant Trials,"Calibration is a critical process for reducing uncertainty in Water
Distribution Network Hydraulic Models (WDN HM). However, features of certain
WDNs, such as oversized pipelines, lead to shallow pressure gradients under
normal daily conditions, posing a challenge for effective calibration. This
study proposes a calibration methodology using short hydrant trials conducted
at night, which increase the pressure gradient in the WDN. The data is
resampled to align with hourly consumption patterns. In a unique real-world
case study of a WDN zone, we demonstrate the statistically significant
superiority of our method compared to calibration based on daily usage. The
experimental methodology, inspired by a machine learning cross-validation
framework, utilises two state-of-the-art calibration algorithms, achieving a
reduction in absolute error of up to 45% in the best scenario.",2024-09-17,"Katarzyna Kołodziej, Michał Cholewa, Przemysław Głomb, Wojciech Koral, Michał Romaszewski",http://arxiv.org/pdf/2410.02772v1,cs.LG
Online Combinatorial Allocations and Auctions with Few Samples,"In online combinatorial allocations/auctions, n bidders sequentially arrive,
each with a combinatorial valuation (such as submodular/XOS) over subsets of m
indivisible items. The aim is to immediately allocate a subset of the remaining
items to maximize the total welfare, defined as the sum of bidder valuations. A
long line of work has studied this problem when the bidder valuations come from
known independent distributions. In particular, for submodular/XOS valuations,
we know 2-competitive algorithms/mechanisms that set a fixed price for each
item and the arriving bidders take their favorite subset of the remaining items
given these prices. However, these algorithms traditionally presume the
availability of the underlying distributions as part of the input to the
algorithm. Contrary to this assumption, practical scenarios often require the
learning of distributions, a task complicated by limited sample availability.
This paper investigates the feasibility of achieving O(1)-competitive
algorithms under the realistic constraint of having access to only a limited
number of samples from the underlying bidder distributions.
  Our first main contribution shows that a mere single sample from each bidder
distribution is sufficient to yield an O(1)-competitive algorithm for
submodular/XOS valuations. This result leverages a novel extension of the
secretary-style analysis, employing the sample to have the algorithm compete
against itself. Although online, this first approach does not provide an online
truthful mechanism. Our second main contribution shows that a polynomial number
of samples suffices to yield a $(2+\epsilon)$-competitive online truthful
mechanism for submodular/XOS valuations and any constant $\epsilon>0$. This
result is based on a generalization of the median-based algorithm for the
single-item prophet inequality problem to combinatorial settings with multiple
items.",2024-09-17,"Paul Dütting, Thomas Kesselheim, Brendan Lucier, Rebecca Reiffenhäuser, Sahil Singla",http://arxiv.org/pdf/2409.11091v1,cs.LG
Three Approaches to the Automation of Laser System Alignment and Their Resource Implications: A Case Study,"The alignment of optical systems is a critical step in their manufacture.
Alignment normally requires considerable knowledge and expertise of skilled
operators. The automation of such processes has several potential advantages,
but requires additional resource and upfront costs. Through a case study of a
simple two mirror system we identify and examine three different automation
approaches. They are: artificial neural networks; practice-led, which mimics
manual alignment practices; and design-led, modelling from first principles. We
find that these approaches make use of three different types of knowledge 1)
basic system knowledge (of controls, measurements and goals); 2) behavioural
skills and expertise, and 3) fundamental system design knowledge. We
demonstrate that the different automation approaches vary significantly in
human resources, and measurement sampling budgets. This will have implications
for practitioners and management considering the automation of such tasks.",2024-09-17,"David A. Robb, Donald Risbridger, Ben Mills, Ildar Rakhmatulin, Xianwen Kong, Mustafa Erden, M. J. Daniel Esser, Richard M. Carter, Mike J. Chantler",http://arxiv.org/pdf/2409.11090v1,cs.LG
Insightful Railway Track Evaluation: Leveraging NARX Feature Interpretation,"The classification of time series is essential for extracting meaningful
insights and aiding decision-making in engineering domains. Parametric modeling
techniques like NARX are invaluable for comprehending intricate processes, such
as environmental time series, owing to their easily interpretable and
transparent structures. This article introduces a classification algorithm,
Logistic-NARX Multinomial, which merges the NARX methodology with logistic
regression. This approach not only produces interpretable models but also
effectively tackles challenges associated with multiclass classification.
Furthermore, this study introduces an innovative methodology tailored for the
railway sector, offering a tool by employing NARX models to interpret the
multitude of features derived from onboard sensors. This solution provides
profound insights through feature importance analysis, enabling informed
decision-making regarding safety and maintenance.",2024-09-17,"P. H. O. Silva, A. S. Cerqueira, E. G. Nepomuceno",http://arxiv.org/pdf/2410.02770v1,cs.LG
AutoFlow: An Autoencoder-based Approach for IP Flow Record Compression with Minimal Impact on Traffic Classification,"Network monitoring generates massive volumes of IP flow records, posing
significant challenges for storage and analysis. This paper presents a novel
deep learning-based approach to compressing these records using autoencoders,
enabling direct analysis of compressed data without requiring decompression.
Unlike traditional compression methods, our approach reduces data volume while
retaining the utility of compressed data for downstream analysis tasks,
including distinguishing modern application protocols and encrypted traffic
from popular services. Through extensive experiments on a real-world network
traffic dataset, we demonstrate that our autoencoder-based compression achieves
a 1.313x reduction in data size while maintaining 99.27% accuracy in a
multi-class traffic classification task, compared to 99.77% accuracy with
uncompressed data. This marginal decrease in performance is offset by
substantial gains in storage and processing efficiency. The implications of
this work extend to more efficient network monitoring and scalable, real-time
network management solutions.",2024-09-17,Adrian Pekar,http://arxiv.org/pdf/2410.00030v2,cs.LG
MonoKAN: Certified Monotonic Kolmogorov-Arnold Network,"Artificial Neural Networks (ANNs) have significantly advanced various fields
by effectively recognizing patterns and solving complex problems. Despite these
advancements, their interpretability remains a critical challenge, especially
in applications where transparency and accountability are essential. To address
this, explainable AI (XAI) has made progress in demystifying ANNs, yet
interpretability alone is often insufficient. In certain applications, model
predictions must align with expert-imposed requirements, sometimes exemplified
by partial monotonicity constraints. While monotonic approaches are found in
the literature for traditional Multi-layer Perceptrons (MLPs), they still face
difficulties in achieving both interpretability and certified partial
monotonicity. Recently, the Kolmogorov-Arnold Network (KAN) architecture, based
on learnable activation functions parametrized as splines, has been proposed as
a more interpretable alternative to MLPs. Building on this, we introduce a
novel ANN architecture called MonoKAN, which is based on the KAN architecture
and achieves certified partial monotonicity while enhancing interpretability.
To achieve this, we employ cubic Hermite splines, which guarantee monotonicity
through a set of straightforward conditions. Additionally, by using positive
weights in the linear combinations of these splines, we ensure that the network
preserves the monotonic relationships between input and output. Our experiments
demonstrate that MonoKAN not only enhances interpretability but also improves
predictive performance across the majority of benchmarks, outperforming
state-of-the-art monotonic MLP approaches.",2024-09-17,"Alejandro Polo-Molina, David Alfaya, Jose Portela",http://arxiv.org/pdf/2409.11078v1,cs.LG
Improve Machine Learning carbon footprint using Parquet dataset format and Mixed Precision training for regression models -- Part II,"This is the 2nd part of the dissertation for my master degree and compared
the power consumption using the Comma-Separated-Values (CSV) and parquet
dataset format with the default floating point (32bit) and Nvidia mixed
precision (16bit and 32bit) while training a regression ML model. The same
custom PC as per the 1st part, which was dedicated to the classification
testing and analysis, was built to perform the experiments, and different ML
hyper-parameters, such as batch size, neurons, and epochs, were chosen to build
Deep Neural Networks (DNN). A benchmarking test with default hyper-parameter
values for the DNN was used as a reference, while the experiments used a
combination of different settings. The results were recorded in Excel, and
descriptive statistics were chosen to calculate the mean between the groups and
compare them using graphs and tables. The outcome was positive when using mixed
precision combined with specific hyper-parameters. Compared to the
benchmarking, optimising the regression models reduced the power consumption
between 7 and 11 Watts. The regression results show that while mixed precision
can help improve power consumption, we must carefully consider the
hyper-parameters. A high number of batch sizes and neurons will negatively
affect power consumption. However, this research required inferential
statistics, specifically ANOVA and T-test, to compare the relationship between
the means. The results reported no statistical significance between the means
in the regression tests and accepted H0. Therefore, choosing different ML
techniques and the Parquet dataset format will not improve the computational
power consumption and the overall ML carbon footprint. However, a more
extensive implementation with a cluster of GPUs can increase the sample size
significantly, as it is an essential factor and can change the outcome of the
statistical analysis.",2024-09-17,Andrew Antonopoulos,http://arxiv.org/pdf/2409.11071v2,cs.LG
A Reinforcement Learning Environment for Automatic Code Optimization in the MLIR Compiler,"Code optimization is a crucial task aimed at enhancing code performance.
However, this process is often tedious and complex, highlighting the necessity
for automatic code optimization techniques. Reinforcement Learning (RL), a
machine learning technique, has emerged as a promising approach for tackling
such complex optimization problems. In this project, we introduce the first RL
environment for the MLIR compiler, dedicated to facilitating MLIR compiler
research, and enabling automatic code optimization using Multi-Action
Reinforcement Learning. We also propose a novel formulation of the action space
as a Cartesian product of simpler action subspaces, enabling more efficient and
effective optimizations. Experimental results demonstrate that our proposed
environment allows for an effective optimization of MLIR operations, and yields
comparable performance to TensorFlow, surpassing it in multiple cases,
highlighting the potential of RL-based optimization in compiler frameworks.",2024-09-17,"Nazim Bendib, Iheb Nassim Aouadj, Riyadh Baghdadi",http://arxiv.org/pdf/2409.11068v1,cs.LG
A Hybrid Multi-Factor Network with Dynamic Sequence Modeling for Early Warning of Intraoperative Hypotension,"Intraoperative hypotension (IOH) prediction using past physiological signals
is crucial, as IOH can lead to inadequate organ perfusion, increasing the risk
of severe complications and mortality. However, existing IOH prediction methods
often rely on static modeling, overlooking the complex temporal dependencies
and non-stationary nature of physiological signals. In this paper, we propose a
Hybrid Multi-Factor (HMF) network that models IOH prediction as a dynamic
sequence forecasting problem, explicitly capturing temporal dependencies and
physiological non-stationarity.. Specifically, we formalize physiological
signal dynamics as a sequence of multivariate time series, and decompose them
into trend and seasonal components, enabling distinct modeling of long-term and
periodic variations. For each component, we employ a patch-based Transformer
encoder to extract representative features with the concern of computational
efficiency and representation quality. Furthermore, to mitigate distributional
drift arising from the evolving signals, we introduce a symmetric normalization
mechanism. Extensive experiments on both a publicly available dataset and a
private dataset collected from real-world hospital settings demonstrate that
our approach significantly outperforms competitive baselines. We hope HMF
offers a new perspective on IOH prediction and further enhances surgical
safety. Our code is open-sourced and available at
\footnote{https://github.com/Mingyue-Cheng/HMF}.",2024-09-17,"Mingyue Cheng, Jintao Zhang, Zhiding Liu, Chunli Liu, Yanhu Xie",http://arxiv.org/pdf/2409.11064v2,cs.LG
OneEncoder: A Lightweight Framework for Progressive Alignment of Modalities,"Cross-modal alignment Learning integrates information from different
modalities like text, image, audio and video to create unified models. This
approach develops shared representations and learns correlations between
modalities, enabling applications such as visual question answering and
audiovisual content analysis. Current techniques rely on large
modality-specific encoders, necessitating fine-tuning or training from scratch
on vast aligned datasets (e.g., text-image, text-audio, image-audio). This
approach has limitations: (i) it is very expensive due to the need for training
large encoders on extensive datasets, (ii) acquiring aligned large paired
datasets is challenging, and (iii) adding new modalities requires retraining
the entire framework to incorporate these modalities. To address these issues,
we propose OneEncoder, a lightweight framework that progressively represents
and aligns four modalities (image, text, audio, video). Initially, we train a
lightweight Universal Projection module (UP) to align image and text
modalities. Then, we freeze the pretrained UP and progressively align future
modalities to those already aligned. OneEncoder operates efficiently and
cost-effectively, even in scenarios where vast aligned datasets are
unavailable, due to its lightweight design. Trained on small paired datasets,
it shows strong performance in tasks like classification, querying, and visual
question answering, surpassing methods that rely on large datasets and
specialized encoders.",2024-09-17,"Bilal Faye, Hanane Azzag, Mustapha Lebbah",http://arxiv.org/pdf/2409.11059v2,cs.LG
On-policy Actor-Critic Reinforcement Learning for Multi-UAV Exploration,"Unmanned aerial vehicles (UAVs) have become increasingly popular in various
fields, including precision agriculture, search and rescue, and remote sensing.
However, exploring unknown environments remains a significant challenge. This
study aims to address this challenge by utilizing on-policy Reinforcement
Learning (RL) with Proximal Policy Optimization (PPO) to explore the {two
dimensional} area of interest with multiple UAVs. The UAVs will avoid collision
with obstacles and each other and do the exploration in a distributed manner.
The proposed solution includes actor-critic networks using deep convolutional
neural networks {(CNN)} and long short-term memory (LSTM) for identifying the
UAVs and areas that have already been covered. Compared to other RL techniques,
such as policy gradient (PG) and asynchronous advantage actor-critic (A3C), the
simulation results demonstrate the superiority of the proposed PPO approach.
Also, the results show that combining LSTM with CNN in critic can improve
exploration. Since the proposed exploration has to work in unknown
environments, the results showed that the proposed setup can complete the
coverage when we have new maps that differ from the trained maps. Finally, we
showed how tuning hyper parameters may affect the overall performance.",2024-09-17,"Ali Moltajaei Farid, Jafar Roshanian, Malek Mouhoub",http://arxiv.org/pdf/2409.11058v1,cs.LG
A logical alarm for misaligned binary classifiers,"If two agents disagree in their decisions, we may suspect they are not both
correct. This intuition is formalized for evaluating agents that have carried
out a binary classification task. Their agreements and disagreements on a joint
test allow us to establish the only group evaluations logically consistent with
their responses. This is done by establishing a set of axioms (algebraic
relations) that must be universally obeyed by all evaluations of binary
responders. A complete set of such axioms are possible for each ensemble of
size N. The axioms for $N = 1, 2$ are used to construct a fully logical alarm -
one that can prove that at least one ensemble member is malfunctioning using
only unlabeled data. The similarities of this approach to formal software
verification and its utility for recent agendas of safe guaranteed AI are
discussed.",2024-09-17,"Andrés Corrada-Emmanuel, Ilya Parker, Ramesh Bharadwaj",http://arxiv.org/pdf/2409.11052v1,cs.LG
Volvo Discovery Challenge at ECML-PKDD 2024,"This paper presents an overview of the Volvo Discovery Challenge, held during
the ECML-PKDD 2024 conference. The challenge's goal was to predict the failure
risk of an anonymized component in Volvo trucks using a newly published
dataset. The test data included observations from two generations (gen1 and
gen2) of the component, while the training data was provided only for gen1. The
challenge attracted 52 data scientists from around the world who submitted a
total of 791 entries. We provide a brief description of the problem definition,
challenge setup, and statistics about the submissions. In the section on
winning methodologies, the first, second, and third-place winners of the
competition briefly describe their proposed methods and provide GitHub links to
their implemented code. The shared code can be interesting as an advanced
methodology for researchers in the predictive maintenance domain. The
competition was hosted on the Codabench platform.",2024-09-17,"Mahmoud Rahat, Peyman Sheikholharam Mashhadi, Sławomir Nowaczyk, Shamik Choudhury, Leo Petrin, Thorsteinn Rognvaldsson, Andreas Voskou, Carlo Metta, Claudio Savelli",http://arxiv.org/pdf/2409.11446v1,cs.LG
Prompt Obfuscation for Large Language Models,"System prompts that include detailed instructions to describe the task
performed by the underlying LLM can easily transform foundation models into
tools and services with minimal overhead. Because of their crucial impact on
the utility, they are often considered intellectual property, similar to the
code of a software product. However, extracting system prompts is easily
possible. As of today, there is no effective countermeasure to prevent the
stealing of system prompts and all safeguarding efforts could be evaded. In
this work, we propose an alternative to conventional system prompts. We
introduce prompt obfuscation to prevent the extraction of the system prompt
with only little overhead. The core idea is to find a representation of the
original system prompt that leads to the same functionality, while the
obfuscated system prompt does not contain any information that allows
conclusions to be drawn about the original system prompt. We evaluate our
approach by comparing our obfuscated prompt output with the output of the
original prompt, using eight distinct metrics, to measure the lexical,
character-level, and semantic similarity. We show that the obfuscated version
is constantly on par with the original one. We further perform three different
deobfuscation attacks with varying attacker knowledge--covering both black-box
and white-box conditions--and show that in realistic attack scenarios an
attacker is not able to extract meaningful information. Overall, we demonstrate
that prompt obfuscation is an effective mechanism to safeguard the intellectual
property of a system prompt while maintaining the same utility as the original
prompt.",2024-09-17,"David Pape, Sina Mavali, Thorsten Eisenhofer, Lea Schönherr",http://arxiv.org/pdf/2409.11026v3,cs.LG
D2Vformer: A Flexible Time Series Prediction Model Based on Time Position Embedding,"Time position embeddings capture the positional information of time steps,
often serving as auxiliary inputs to enhance the predictive capabilities of
time series models. However, existing models exhibit limitations in capturing
intricate time positional information and effectively utilizing these
embeddings. To address these limitations, this paper proposes a novel model
called D2Vformer. Unlike typical prediction methods that rely on RNNs or
Transformers, this approach can directly handle scenarios where the predicted
sequence is not adjacent to the input sequence or where its length dynamically
changes. In comparison to conventional methods, D2Vformer undoubtedly saves a
significant amount of training resources. In D2Vformer, the Date2Vec module
uses the timestamp information and feature sequences to generate time position
embeddings. Afterward, D2Vformer introduces a new fusion block that utilizes an
attention mechanism to explore the similarity in time positions between the
embeddings of the input sequence and the predicted sequence, thereby generating
predictions based on this similarity. Through extensive experiments on six
datasets, we demonstrate that Date2Vec outperforms other time position
embedding methods, and D2Vformer surpasses state-of-the-art methods in both
fixed-length and variable-length prediction tasks.",2024-09-17,"Xiaobao Song, Hao Wang, Liwei Deng, Yuxin He, Wenming Cao, Chi-Sing Leungc",http://arxiv.org/pdf/2409.11024v1,cs.LG
Latent mixed-effect models for high-dimensional longitudinal data,"Modelling longitudinal data is an important yet challenging task. These
datasets can be high-dimensional, contain non-linear effects and time-varying
covariates. Gaussian process (GP) prior-based variational autoencoders (VAEs)
have emerged as a promising approach due to their ability to model time-series
data. However, they are costly to train and struggle to fully exploit the rich
covariates characteristic of longitudinal data, making them difficult for
practitioners to use effectively. In this work, we leverage linear mixed models
(LMMs) and amortized variational inference to provide conditional priors for
VAEs, and propose LMM-VAE, a scalable, interpretable and identifiable model. We
highlight theoretical connections between it and GP-based techniques, providing
a unified framework for this class of methods. Our proposal performs
competitively compared to existing approaches across simulated and real-world
datasets.",2024-09-17,"Priscilla Ong, Manuel Haußmann, Otto Lönnroth, Harri Lähdesmäki",http://arxiv.org/pdf/2409.11008v1,cs.LG
GINTRIP: Interpretable Temporal Graph Regression using Information bottleneck and Prototype-based method,"Deep neural networks (DNNs) have demonstrated remarkable performance across
various domains, yet their application to temporal graph regression tasks faces
significant challenges regarding interpretability. This critical issue, rooted
in the inherent complexity of both DNNs and underlying spatio-temporal patterns
in the graph, calls for innovative solutions. While interpretability concerns
in Graph Neural Networks (GNNs) mirror those of DNNs, to the best of our
knowledge, no notable work has addressed the interpretability of temporal GNNs
using a combination of Information Bottleneck (IB) principles and
prototype-based methods. Our research introduces a novel approach that uniquely
integrates these techniques to enhance the interpretability of temporal graph
regression models. The key contributions of our work are threefold: We
introduce the \underline{G}raph \underline{IN}terpretability in
\underline{T}emporal \underline{R}egression task using \underline{I}nformation
bottleneck and \underline{P}rototype (GINTRIP) framework, the first combined
application of IB and prototype-based methods for interpretable temporal graph
tasks. We derive a novel theoretical bound on mutual information (MI),
extending the applicability of IB principles to graph regression tasks. We
incorporate an unsupervised auxiliary classification head, fostering multi-task
learning and diverse concept representation, which enhances the model
bottleneck's interpretability. Our model is evaluated on real-world traffic
datasets, outperforming existing methods in both forecasting accuracy and
interpretability-related metrics.",2024-09-17,"Ali Royat, Seyed Mohamad Moghadas, Lesley De Cruz, Adrian Munteanu",http://arxiv.org/pdf/2409.10996v1,cs.LG
SynthSOD: Developing an Heterogeneous Dataset for Orchestra Music Source Separation,"Recent advancements in music source separation have significantly progressed,
particularly in isolating vocals, drums, and bass elements from mixed tracks.
These developments owe much to the creation and use of large-scale, multitrack
datasets dedicated to these specific components. However, the challenge of
extracting similarly sounding sources from orchestra recordings has not been
extensively explored, largely due to a scarcity of comprehensive and clean (i.e
bleed-free) multitrack datasets. In this paper, we introduce a novel multitrack
dataset called SynthSOD, developed using a set of simulation techniques to
create a realistic (i.e. using high-quality soundfonts), musically motivated,
and heterogeneous training set comprising different dynamics, natural tempo
changes, styles, and conditions. Moreover, we demonstrate the application of a
widely used baseline music separation model trained on our synthesized dataset
w.r.t to the well-known EnsembleSet, and evaluate its performance under both
synthetic and real-world conditions.",2024-09-17,"Jaime Garcia-Martinez, David Diaz-Guerra, Archontis Politis, Tuomas Virtanen, Julio J. Carabias-Orti, Pedro Vera-Candeas",http://arxiv.org/pdf/2409.10995v2,cs.LG
Towards Gaussian Process for operator learning: an uncertainty aware resolution independent operator learning algorithm for computational mechanics,"The growing demand for accurate, efficient, and scalable solutions in
computational mechanics highlights the need for advanced operator learning
algorithms that can efficiently handle large datasets while providing reliable
uncertainty quantification. This paper introduces a novel Gaussian Process (GP)
based neural operator for solving parametric differential equations. The
approach proposed leverages the expressive capability of deterministic neural
operators and the uncertainty awareness of conventional GP. In particular, we
propose a ``neural operator-embedded kernel'' wherein the GP kernel is
formulated in the latent space learned using a neural operator. Further, we
exploit a stochastic dual descent (SDD) algorithm for simultaneously training
the neural operator parameters and the GP hyperparameters. Our approach
addresses the (a) resolution dependence and (b) cubic complexity of traditional
GP models, allowing for input-resolution independence and scalability in
high-dimensional and non-linear parametric systems, such as those encountered
in computational mechanics. We apply our method to a range of non-linear
parametric partial differential equations (PDEs) and demonstrate its
superiority in both computational efficiency and accuracy compared to standard
GP models and wavelet neural operators. Our experimental results highlight the
efficacy of this framework in solving complex PDEs while maintaining robustness
in uncertainty estimation, positioning it as a scalable and reliable
operator-learning algorithm for computational mechanics.",2024-09-17,"Sawan Kumar, Rajdip Nayek, Souvik Chakraborty",http://arxiv.org/pdf/2409.10972v1,cs.LG
Relative Representations: Topological and Geometric Perspectives,"Relative representations are an established approach to zero-shot model
stitching, consisting of a non-trainable transformation of the latent space of
a deep neural network. Based on insights of topological and geometric nature,
we propose two improvements to relative representations. First, we introduce a
normalization procedure in the relative transformation, resulting in invariance
to non-isotropic rescalings and permutations. The latter coincides with the
symmetries in parameter space induced by common activation functions. Second,
we propose to deploy topological densification when fine-tuning relative
representations, a topological regularization loss encouraging clustering
within classes. We provide an empirical investigation on a natural language
task, where both the proposed variations yield improved performance on
zero-shot model stitching.",2024-09-17,"Alejandro García-Castellanos, Giovanni Luca Marchetti, Danica Kragic, Martina Scolamiero",http://arxiv.org/pdf/2409.10967v2,cs.LG
A Machine Learning-Driven Wireless System for Structural Health Monitoring,"The paper presents a wireless system integrated with a machine learning (ML)
model for structural health monitoring (SHM) of carbon fiber reinforced polymer
(CFRP) structures, primarily targeting aerospace applications. The system
collects data via carbon nanotube (CNT) piezoresistive sensors embedded within
CFRP coupons, wirelessly transmitting these data to a central server for
processing. A deep neural network (DNN) model predicts mechanical properties
and can be extended to forecast structural failures, facilitating proactive
maintenance and enhancing safety. The modular design supports scalability and
can be embedded within digital twin frameworks, offering significant benefits
to aircraft operators and manufacturers. The system utilizes an ML model with a
mean absolute error (MAE) of 0.14 on test data for forecasting mechanical
properties. Data transmission latency throughout the entire system is less than
one second in a LAN setup, highlighting its potential for real-time monitoring
applications in aerospace and other industries. However, while the system shows
promise, challenges such as sensor reliability under extreme environmental
conditions and the need for advanced ML models to handle diverse data streams
have been identified as areas for future research.",2024-09-17,"Marius Pop, Mihai Tudose, Daniel Visan, Mircea Bocioaga, Mihai Botan, Cesar Banu, Tiberiu Salaoru",http://arxiv.org/pdf/2410.20678v1,cs.LG
Cross-lingual transfer of multilingual models on low resource African Languages,"Large multilingual models have significantly advanced natural language
processing (NLP) research. However, their high resource demands and potential
biases from diverse data sources have raised concerns about their effectiveness
across low-resource languages. In contrast, monolingual models, trained on a
single language, may better capture the nuances of the target language,
potentially providing more accurate results. This study benchmarks the
cross-lingual transfer capabilities from a high-resource language to a
low-resource language for both, monolingual and multilingual models, focusing
on Kinyarwanda and Kirundi, two Bantu languages. We evaluate the performance of
transformer based architectures like Multilingual BERT (mBERT), AfriBERT, and
BantuBERTa against neural-based architectures such as BiGRU, CNN, and char-CNN.
The models were trained on Kinyarwanda and tested on Kirundi, with fine-tuning
applied to assess the extent of performance improvement and catastrophic
forgetting. AfriBERT achieved the highest cross-lingual accuracy of 88.3% after
fine-tuning, while BiGRU emerged as the best-performing neural model with 83.3%
accuracy. We also analyze the degree of forgetting in the original language
post-fine-tuning. While monolingual models remain competitive, this study
highlights that multilingual models offer strong cross-lingual transfer
capabilities in resource limited settings.",2024-09-17,"Harish Thangaraj, Ananya Chenat, Jaskaran Singh Walia, Vukosi Marivate",http://arxiv.org/pdf/2409.10965v1,cs.LG
Active learning for energy-based antibody optimization and enhanced screening,"Accurate prediction and optimization of protein-protein binding affinity is
crucial for therapeutic antibody development. Although machine learning-based
prediction methods $\Delta\Delta G$ are suitable for large-scale mutant
screening, they struggle to predict the effects of multiple mutations for
targets without existing binders. Energy function-based methods, though more
accurate, are time consuming and not ideal for large-scale screening. To
address this, we propose an active learning workflow that efficiently trains a
deep learning model to learn energy functions for specific targets, combining
the advantages of both approaches. Our method integrates the RDE-Network deep
learning model with Rosetta's energy function-based Flex ddG to efficiently
explore mutants. In a case study targeting HER2-binding Trastuzumab mutants,
our approach significantly improved the screening performance over random
selection and demonstrated the ability to identify mutants with better binding
properties without experimental $\Delta\Delta G$ data. This workflow advances
computational antibody design by combining machine learning, physics-based
computations, and active learning to achieve more efficient antibody
development.",2024-09-17,"Kairi Furui, Masahito Ohue",http://arxiv.org/pdf/2409.10964v2,cs.LG
"Trends, Advancements and Challenges in Intelligent Optimization in Satellite Communication","Efficient satellite communications play an enormously important role in all
of our daily lives. This includes the transmission of data for communication
purposes, the operation of IoT applications or the provision of data for ground
stations. More and more, AI-based methods are finding their way into these
areas. This paper gives an overview of current research in the field of
intelligent optimization of satellite communication. For this purpose, a
text-mining based literature review was conducted and the identified papers
were thematically clustered and analyzed. The identified clusters cover the
main topics of routing, resource allocation and, load balancing. Through such a
clustering of the literature in overarching topics, a structured analysis of
the research papers was enabled, allowing the identification of latest
technologies and approaches as well as research needs for intelligent
optimization of satellite communication.",2024-09-17,"Philippe Krajsic, Viola Suess, Zehong Cao, Ryszard Kowalczyk, Bogdan Franczyk",http://arxiv.org/pdf/2410.03674v1,cs.LG
Leveraging Reviewer Experience in Code Review Comment Generation,"Modern code review is a ubiquitous software quality assurance process aimed
at identifying potential issues within newly written code. Despite its
effectiveness, the process demands large amounts of effort from the human
reviewers involved. To help alleviate this workload, researchers have trained
deep learning models to imitate human reviewers in providing natural language
code reviews. Formally, this task is known as code review comment generation.
Prior work has demonstrated improvements in this task by leveraging machine
learning techniques and neural models, such as transfer learning and the
transformer architecture. However, the quality of the model generated reviews
remain sub-optimal due to the quality of the open-source code review data used
in model training. This is in part due to the data obtained from open-source
projects where code reviews are conducted in a public forum, and reviewers
possess varying levels of software development experience, potentially
affecting the quality of their feedback. To accommodate for this variation, we
propose a suite of experience-aware training methods that utilise the
reviewers' past authoring and reviewing experiences as signals for review
quality. Specifically, we propose experience-aware loss functions (ELF), which
use the reviewers' authoring and reviewing ownership of a project as weights in
the model's loss function. Through this method, experienced reviewers' code
reviews yield larger influence over the model's behaviour. Compared to the SOTA
model, ELF was able to generate higher quality reviews in terms of accuracy,
informativeness, and comment types generated. The key contribution of this work
is the demonstration of how traditional software engineering concepts such as
reviewer experience can be integrated into the design of AI-based automated
code review models.",2024-09-17,"Hong Yi Lin, Patanamon Thongtanunam, Christoph Treude, Michael W. Godfrey, Chunhua Liu, Wachiraphan Charoenwet",http://arxiv.org/pdf/2409.10959v1,cs.LG
Fair Anomaly Detection For Imbalanced Groups,"Anomaly detection (AD) has been widely studied for decades in many real-world
applications, including fraud detection in finance, and intrusion detection for
cybersecurity, etc. Due to the imbalanced nature between protected and
unprotected groups and the imbalanced distributions of normal examples and
anomalies, the learning objectives of most existing anomaly detection methods
tend to solely concentrate on the dominating unprotected group. Thus, it has
been recognized by many researchers about the significance of ensuring model
fairness in anomaly detection. However, the existing fair anomaly detection
methods tend to erroneously label most normal examples from the protected group
as anomalies in the imbalanced scenario where the unprotected group is more
abundant than the protected group. This phenomenon is caused by the improper
design of learning objectives, which statistically focus on learning the
frequent patterns (i.e., the unprotected group) while overlooking the
under-represented patterns (i.e., the protected group). To address these
issues, we propose FairAD, a fairness-aware anomaly detection method targeting
the imbalanced scenario. It consists of a fairness-aware contrastive learning
module and a rebalancing autoencoder module to ensure fairness and handle the
imbalanced data issue, respectively. Moreover, we provide the theoretical
analysis that shows our proposed contrastive learning regularization guarantees
group fairness. Empirical studies demonstrate the effectiveness and efficiency
of FairAD across multiple real-world datasets.",2024-09-17,"Ziwei Wu, Lecheng Zheng, Yuancheng Yu, Ruizhong Qiu, John Birge, Jingrui He",http://arxiv.org/pdf/2409.10951v1,cs.LG
Contrasformer: A Brain Network Contrastive Transformer for Neurodegenerative Condition Identification,"Understanding neurological disorder is a fundamental problem in neuroscience,
which often requires the analysis of brain networks derived from functional
magnetic resonance imaging (fMRI) data. Despite the prevalence of Graph Neural
Networks (GNNs) and Graph Transformers in various domains, applying them to
brain networks faces challenges. Specifically, the datasets are severely
impacted by the noises caused by distribution shifts across sub-populations and
the neglect of node identities, both obstruct the identification of
disease-specific patterns. To tackle these challenges, we propose
Contrasformer, a novel contrastive brain network Transformer. It generates a
prior-knowledge-enhanced contrast graph to address the distribution shifts
across sub-populations by a two-stream attention mechanism. A cross attention
with identity embedding highlights the identity of nodes, and three auxiliary
losses ensure group consistency. Evaluated on 4 functional brain network
datasets over 4 different diseases, Contrasformer outperforms the
state-of-the-art methods for brain networks by achieving up to 10.8\%
improvement in accuracy, which demonstrates its efficacy in neurological
disorder identification. Case studies illustrate its interpretability,
especially in the context of neuroscience. This paper provides a solution for
analyzing brain networks, offering valuable insights into neurological
disorders. Our code is available at
\url{https://github.com/AngusMonroe/Contrasformer}.",2024-09-17,"Jiaxing Xu, Kai He, Mengcheng Lan, Qingtian Bian, Wei Li, Tieying Li, Yiping Ke, Miao Qiao",http://arxiv.org/pdf/2409.10944v1,cs.LG
Optimizing TinyML: The Impact of Reduced Data Acquisition Rates for Time Series Classification on Microcontrollers,"Tiny Machine Learning (TinyML) enables efficient, lowcost, and privacy
preserving machine learning inference directly on microcontroller units (MCUs)
connected to sensors. Optimizing models for these constrained environments is
crucial. This paper investigates how reducing data acquisition rates affects
TinyML models for time series classification, focusing on resource-constrained,
battery operated IoT devices. By lowering data sampling frequency, we aim to
reduce computational demands RAM usage, energy consumption, latency, and MAC
operations by approximately fourfold while maintaining similar classification
accuracies. Our experiments with six benchmark datasets (UCIHAR, WISDM, PAMAP2,
MHEALTH, MITBIH, and PTB) showed that reducing data acquisition rates
significantly cut energy consumption and computational load, with minimal
accuracy loss. For example, a 75\% reduction in acquisition rate for MITBIH and
PTB datasets led to a 60\% decrease in RAM usage, 75\% reduction in MAC
operations, 74\% decrease in latency, and 70\% reduction in energy consumption,
without accuracy loss. These results offer valuable insights for deploying
efficient TinyML models in constrained environments.",2024-09-17,"Riya Samanta, Bidyut Saha, Soumya K. Ghosh, Ram Babu Roy",http://arxiv.org/pdf/2409.10942v1,cs.LG
Early Detection of Coronary Heart Disease Using Hybrid Quantum Machine Learning Approach,"Coronary heart disease (CHD) is a severe cardiac disease, and hence, its
early diagnosis is essential as it improves treatment results and saves money
on medical care. The prevailing development of quantum computing and machine
learning (ML) technologies may bring practical improvement to the performance
of CHD diagnosis. Quantum machine learning (QML) is receiving tremendous
interest in various disciplines due to its higher performance and capabilities.
A quantum leap in the healthcare industry will increase processing power and
optimise multiple models. Techniques for QML have the potential to forecast
cardiac disease and help in early detection. To predict the risk of coronary
heart disease, a hybrid approach utilizing an ensemble machine learning model
based on QML classifiers is presented in this paper. Our approach, with its
unique ability to address multidimensional healthcare data, reassures the
method's robustness by fusing quantum and classical ML algorithms in a
multi-step inferential framework. The marked rise in heart disease and death
rates impacts worldwide human health and the global economy. Reducing cardiac
morbidity and mortality requires early detection of heart disease. In this
research, a hybrid approach utilizes techniques with quantum computing
capabilities to tackle complex problems that are not amenable to conventional
machine learning algorithms and to minimize computational expenses. The
proposed method has been developed in the Raspberry Pi 5 Graphics Processing
Unit (GPU) platform and tested on a broad dataset that integrates clinical and
imaging data from patients suffering from CHD and healthy controls. Compared to
classical machine learning models, the accuracy, sensitivity, F1 score, and
specificity of the proposed hybrid QML model used with CHD are manifold higher.",2024-09-17,"Mehroush Banday, Sherin Zafar, Parul Agarwal, M Afshar Alam, Abubeker K M",http://arxiv.org/pdf/2409.10932v2,cs.LG
FSL-HDnn: A 5.7 TOPS/W End-to-end Few-shot Learning Classifier Accelerator with Feature Extraction and Hyperdimensional Computing,"This paper introduces FSL-HDnn, an energy-efficient accelerator that
implements the end-to-end pipeline of feature extraction, classification, and
on-chip few-shot learning (FSL) through gradient-free learning techniques in a
40 nm CMOS process. At its core, FSL-HDnn integrates two low-power modules:
Weight clustering feature extractor and Hyperdimensional Computing (HDC).
Feature extractor utilizes advanced weight clustering and pattern reuse
strategies for optimized CNN-based feature extraction. Meanwhile, HDC emerges
as a novel approach for lightweight FSL classifier, employing hyperdimensional
vectors to improve training accuracy significantly compared to traditional
distance-based approaches. This dual-module synergy not only simplifies the
learning process by eliminating the need for complex gradients but also
dramatically enhances energy efficiency and performance. Specifically, FSL-HDnn
achieves an Intensity unprecedented energy efficiency of 5.7 TOPS/W for feature
1 extraction and 0.78 TOPS/W for classification and learning Training Intensity
phases, achieving improvements of 2.6X and 6.6X, respectively, Storage over
current state-of-the-art CNN and FSL processors.",2024-09-17,"Haichao Yang, Chang Eun Song, Weihong Xu, Behnam Khaleghi, Uday Mallappa, Monil Shah, Keming Fan, Mingu Kang, Tajana Rosing",http://arxiv.org/pdf/2409.10918v1,cs.LG
A Physics Informed Neural Network (PINN) Methodology for Coupled Moving Boundary PDEs,"Physics-Informed Neural Network (PINN) is a novel multi-task learning
framework useful for solving physical problems modeled using differential
equations (DEs) by integrating the knowledge of physics and known constraints
into the components of deep learning. A large class of physical problems in
materials science and mechanics involve moving boundaries, where interface flux
balance conditions are to be satisfied while solving DEs. Examples of such
systems include free surface flows, shock propagation, solidification of pure
and alloy systems etc. While recent research works have explored applicability
of PINNs for an uncoupled system (such as solidification of pure system), the
present work reports a PINN-based approach to solve coupled systems involving
multiple governing parameters (energy and species, along with multiple
interface balance equations). This methodology employs an architecture
consisting of a separate network for each variable with a separate treatment of
each phase, a training strategy which alternates between temporal learning and
adaptive loss weighting, and a scheme which progressively reduces the
optimisation space. While solving the benchmark problem of binary alloy
solidification, it is distinctly successful at capturing the complex
composition profile, which has a characteristic discontinuity at the interface
and the resulting predictions align well with the analytical solutions. The
procedure can be generalised for solving other transient multiphysics problems
especially in the low-data regime and in cases where measurements can reveal
new physics.",2024-09-17,"Shivprasad Kathane, Shyamprasad Karagadde",http://arxiv.org/pdf/2409.10910v1,cs.LG
Clustering with Non-adaptive Subset Queries,"Recovering the underlying clustering of a set $U$ of $n$ points by asking
pair-wise same-cluster queries has garnered significant interest in the last
decade. Given a query $S \subset U$, $|S|=2$, the oracle returns yes if the
points are in the same cluster and no otherwise. For adaptive algorithms with
pair-wise queries, the number of required queries is known to be $\Theta(nk)$,
where $k$ is the number of clusters. However, non-adaptive schemes require
$\Omega(n^2)$ queries, which matches the trivial $O(n^2)$ upper bound attained
by querying every pair of points.
  To break the quadratic barrier for non-adaptive queries, we study a
generalization of this problem to subset queries for $|S|>2$, where the oracle
returns the number of clusters intersecting $S$. Allowing for subset queries of
unbounded size, $O(n)$ queries is possible with an adaptive scheme
(Chakrabarty-Liao, 2024). However, the realm of non-adaptive algorithms is
completely unknown.
  In this paper, we give the first non-adaptive algorithms for clustering with
subset queries. Our main result is a non-adaptive algorithm making $O(n \log k
\cdot (\log k + \log\log n)^2)$ queries, which improves to $O(n \log \log n)$
when $k$ is a constant. We also consider algorithms with a restricted query
size of at most $s$. In this setting we prove that $\Omega(\max(n^2/s^2,n))$
queries are necessary and obtain algorithms making $\tilde{O}(n^2k/s^2)$
queries for any $s \leq \sqrt{n}$ and $\tilde{O}(n^2/s)$ queries for any $s
\leq n$. We also consider the natural special case when the clusters are
balanced, obtaining non-adaptive algorithms which make $O(n \log k) +
\tilde{O}(k)$ and $O(n\log^2 k)$ queries. Finally, allowing two rounds of
adaptivity, we give an algorithm making $O(n \log k)$ queries in the general
case and $O(n \log \log k)$ queries when the clusters are balanced.",2024-09-17,"Hadley Black, Euiwoong Lee, Arya Mazumdar, Barna Saha",http://arxiv.org/pdf/2409.10908v2,cs.LG
A Joint Spectro-Temporal Relational Thinking Based Acoustic Modeling Framework,"Relational thinking refers to the inherent ability of humans to form mental
impressions about relations between sensory signals and prior knowledge, and
subsequently incorporate them into their model of their world. Despite the
crucial role relational thinking plays in human understanding of speech, it has
yet to be leveraged in any artificial speech recognition systems. Recently,
there have been some attempts to correct this oversight, but these have been
limited to coarse utterance-level models that operate exclusively in the time
domain. In an attempt to narrow the gap between artificial systems and human
abilities, this paper presents a novel spectro-temporal relational thinking
based acoustic modeling framework. Specifically, it first generates numerous
probabilistic graphs to model the relationships among speech segments across
both time and frequency domains. The relational information rooted in every
pair of nodes within these graphs is then aggregated and embedded into latent
representations that can be utilized by downstream tasks. Models built upon
this framework outperform state-of-the-art systems with a 7.82\% improvement in
phoneme recognition tasks over the TIMIT dataset. In-depth analyses further
reveal that our proposed relational thinking modeling mainly improves the
model's ability to recognize vowels, which are the most likely to be confused
by phoneme recognizers.",2024-09-17,"Zheng Nan, Ting Dang, Vidhyasaharan Sethu, Beena Ahmed",http://arxiv.org/pdf/2409.15357v1,cs.LG
Analysis of Convolutional Neural Network-based Image Classifications: A Multi-Featured Application for Rice Leaf Disease Prediction and Recommendations for Farmers,"This study presents a novel method for improving rice disease classification
using 8 different convolutional neural network (CNN) algorithms, which will
further the field of precision agriculture. Tkinter-based application that
offers farmers a feature-rich interface. With the help of this cutting-edge
application, farmers will be able to make timely and well-informed decisions by
enabling real-time disease prediction and providing personalized
recommendations. Together with the user-friendly Tkinter interface, the smooth
integration of cutting-edge CNN transfer learning algorithms-based technology
that include ResNet-50, InceptionV3, VGG16, and MobileNetv2 with the UCI
dataset represents a major advancement toward modernizing agricultural
practices and guaranteeing sustainable crop management. Remarkable outcomes
include 75% accuracy for ResNet-50, 90% accuracy for DenseNet121, 84% accuracy
for VGG16, 95.83% accuracy for MobileNetV2, 91.61% accuracy for DenseNet169,
and 86% accuracy for InceptionV3. These results give a concise summary of the
models' capabilities, assisting researchers in choosing appropriate strategies
for precise and successful rice crop disease identification. A severe
overfitting has been seen on VGG19 with 70% accuracy and Nasnet with 80.02%
accuracy. On Renset101, only an accuracy of 54% could be achieved, along with
only 33% on efficientNetB0. A MobileNetV2-trained model was successfully
deployed on a TKinter GUI application to make predictions using image or
real-time video capture.",2024-09-17,"Biplov Paneru, Bishwash Paneru, Krishna Bikram Shah",http://arxiv.org/pdf/2410.01827v1,cs.LG
LLMs & XAI for Water Sustainability: Seasonal Water Quality Prediction with LIME Explainable AI and a RAG-based Chatbot for Insights,"Ensuring safe water supplies requires effective water quality monitoring,
especially in developing countries like Nepal, where contamination risks are
high. This paper introduces a hybrid deep learning model to predict Nepal's
seasonal water quality using a small dataset with multiple water quality
parameters. Models such as CatBoost, XGBoost, Extra Trees, and LightGBM, along
with a neural network combining CNN and RNN layers, are used to capture
temporal and spatial patterns in the data. The model demonstrated notable
accuracy improvements, aiding proactive water quality control. CatBoost,
XGBoost, and Extra Trees Regressor predicted Water Quality Index (WQI) values
with an average RMSE of 1.2 and an R2 score of 0.99. Additionally, classifiers
achieved 99 percent accuracy, cross-validated across models. LIME analysis
highlighted the importance of indicators like EC and DO levels in XGBoost
classification decisions. The neural network model achieved 92 percent
classification accuracy and an R2 score of 0.97, with an RMSE of 2.87 in
regression analysis. Furthermore, a multifunctional application was developed
to predict WQI values using both regression and classification methods.",2024-09-17,"Biplov Paneru, Bishwash Paneru",http://arxiv.org/pdf/2409.10898v2,cs.LG
AutoSpec: Automated Generation of Neural Network Specifications,"The increasing adoption of neural networks in learning-augmented systems
highlights the importance of model safety and robustness, particularly in
safety-critical domains. Despite progress in the formal verification of neural
networks, current practices require users to manually define model
specifications -- properties that dictate expected model behavior in various
scenarios. This manual process, however, is prone to human error, limited in
scope, and time-consuming. In this paper, we introduce AutoSpec, the first
framework to automatically generate comprehensive and accurate specifications
for neural networks in learning-augmented systems. We also propose the first
set of metrics for assessing the accuracy and coverage of model specifications,
establishing a benchmark for future comparisons. Our evaluation across four
distinct applications shows that AutoSpec outperforms human-defined
specifications as well as two baseline approaches introduced in this study.",2024-09-17,"Shuowei Jin, Francis Y. Yan, Cheng Tan, Anuj Kalia, Xenofon Foukas, Z. Morley Mao",http://arxiv.org/pdf/2409.10897v2,cs.LG
Sparks of Artificial General Intelligence(AGI) in Semiconductor Material Science: Early Explorations into the Next Frontier of Generative AI-Assisted Electron Micrograph Analysis,"Characterizing materials with electron micrographs poses significant
challenges for automated labeling due to the complex nature of nanomaterial
structures. To address this, we introduce a fully automated, end-to-end
pipeline that leverages recent advances in Generative AI. It is designed for
analyzing and understanding the microstructures of semiconductor materials with
effectiveness comparable to that of human experts, contributing to the pursuit
of Artificial General Intelligence (AGI) in nanomaterial identification. Our
approach utilizes Large MultiModal Models (LMMs) such as GPT-4V, alongside
text-to-image models like DALLE-3. We integrate a GPT-4 guided Visual Question
Answering (VQA) method to analyze nanomaterial images, generate synthetic
nanomaterial images via DALLE-3, and employ in-context learning with few-shot
prompting in GPT-4V for accurate nanomaterial identification. Our method
surpasses traditional techniques by enhancing the precision of nanomaterial
identification and optimizing the process for high-throughput screening.",2024-09-17,"Sakhinana Sagar Srinivas, Geethan Sannidhi, Sreeja Gangasani, Chidaksh Ravuru, Venkataramana Runkana",http://arxiv.org/pdf/2409.12244v1,cs.LG
Adaptive Large Language Models By Layerwise Attention Shortcuts,"Transformer architectures are the backbone of the modern AI revolution.
However, they are based on simply stacking the same blocks in dozens of layers
and processing information sequentially from one block to another. In this
paper, we propose to challenge this and introduce adaptive computations for
LLM-like setups, which allow the final layer to attend to all of the
intermediate layers as it deems fit through the attention mechanism, thereby
introducing computational \textbf{attention shortcuts}. These shortcuts can
thus make the architecture depth and context adaptive. We showcase four
different datasets, namely acoustic tokens, natural language, and symbolic
music, and we achieve superior performance for GPT-like architecture. We give
evidence via attention maps that the models learn complex dependencies across
layers that are adaptive in context and depth depending on the input tokens.",2024-09-17,"Prateek Verma, Mert Pilanci",http://arxiv.org/pdf/2409.10870v1,cs.LG
Jailbreaking Large Language Models with Symbolic Mathematics,"Recent advancements in AI safety have led to increased efforts in training
and red-teaming large language models (LLMs) to mitigate unsafe content
generation. However, these safety mechanisms may not be comprehensive, leaving
potential vulnerabilities unexplored. This paper introduces MathPrompt, a novel
jailbreaking technique that exploits LLMs' advanced capabilities in symbolic
mathematics to bypass their safety mechanisms. By encoding harmful natural
language prompts into mathematical problems, we demonstrate a critical
vulnerability in current AI safety measures. Our experiments across 13
state-of-the-art LLMs reveal an average attack success rate of 73.6\%,
highlighting the inability of existing safety training mechanisms to generalize
to mathematically encoded inputs. Analysis of embedding vectors shows a
substantial semantic shift between original and encoded prompts, helping
explain the attack's success. This work emphasizes the importance of a holistic
approach to AI safety, calling for expanded red-teaming efforts to develop
robust safeguards across all potential input types and their associated risks.",2024-09-17,"Emet Bethany, Mazal Bethany, Juan Arturo Nolazco Flores, Sumit Kumar Jha, Peyman Najafirad",http://arxiv.org/pdf/2409.11445v2,cs.LG
Dynamic Range Reduction via Branch-and-Bound,"The demand for high-performance computing in machine learning and artificial
intelligence has led to the development of specialized hardware accelerators
like Tensor Processing Units (TPUs), Graphics Processing Units (GPUs), and
Field-Programmable Gate Arrays (FPGAs). A key strategy to enhance these
accelerators is the reduction of precision in arithmetic operations, which
increases processing speed and lowers latency - crucial for real-time AI
applications. Precision reduction minimizes memory bandwidth requirements and
energy consumption, essential for large-scale and mobile deployments, and
increases throughput by enabling more parallel operations per cycle, maximizing
hardware resource utilization. This strategy is equally vital for solving
NP-hard quadratic unconstrained binary optimization (QUBO) problems common in
machine learning, which often require high precision for accurate
representation. Special hardware solvers, such as quantum annealers, benefit
significantly from precision reduction. This paper introduces a fully
principled Branch-and-Bound algorithm for reducing precision needs in QUBO
problems by utilizing dynamic range as a measure of complexity. Experiments
validate our algorithm's effectiveness on an actual quantum annealer.",2024-09-17,"Thore Gerlach, Nico Piatkowski",http://arxiv.org/pdf/2409.10863v1,cs.LG
3DFacePolicy: Speech-Driven 3D Facial Animation with Diffusion Policy,"Audio-driven 3D facial animation has made immersive progress both in research
and application developments. The newest approaches focus on Transformer-based
methods and diffusion-based methods, however, there is still gap in the
vividness and emotional expression between the generated animation and real
human face. To tackle this limitation, we propose 3DFacePolicy, a diffusion
policy model for 3D facial animation prediction. This method generates variable
and realistic human facial movements by predicting the 3D vertex trajectory on
the 3D facial template with diffusion policy instead of facial generation for
every frame. It takes audio and vertex states as observations to predict the
vertex trajectory and imitate real human facial expressions, which keeps the
continuous and natural flow of human emotions. The experiments show that our
approach is effective in variable and dynamic facial motion synthesizing.",2024-09-17,"Xuanmeng Sha, Liyun Zhang, Tomohiro Mashita, Yuki Uranishi",http://arxiv.org/pdf/2409.10848v1,cs.LG
BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation,"Autoregressive models excel in modeling sequential dependencies by enforcing
causal constraints, yet they struggle to capture complex bidirectional patterns
due to their unidirectional nature. In contrast, mask-based models leverage
bidirectional context, enabling richer dependency modeling. However, they often
assume token independence during prediction, which undermines the modeling of
sequential dependencies. Additionally, the corruption of sequences through
masking or absorption can introduce unnatural distortions, complicating the
learning process. To address these issues, we propose Bidirectional
Autoregressive Diffusion (BAD), a novel approach that unifies the strengths of
autoregressive and mask-based generative models. BAD utilizes a
permutation-based corruption technique that preserves the natural sequence
structure while enforcing causal dependencies through randomized ordering,
enabling the effective capture of both sequential and bidirectional
relationships. Comprehensive experiments show that BAD outperforms
autoregressive and mask-based models in text-to-motion generation, suggesting a
novel pre-training strategy for sequence modeling. The codebase for BAD is
available on https://github.com/RohollahHS/BAD.",2024-09-17,"S. Rohollah Hosseyni, Ali Ahmad Rahmani, S. Jamal Seyedmohammadi, Sanaz Seyedin, Arash Mohammadi",http://arxiv.org/pdf/2409.10847v1,cs.LG
Implicit Reasoning in Deep Time Series Forecasting,"Recently, time series foundation models have shown promising zero-shot
forecasting performance on time series from a wide range of domains. However,
it remains unclear whether their success stems from a true understanding of
temporal dynamics or simply from memorizing the training data. While implicit
reasoning in language models has been studied, similar evaluations for time
series models have been largely unexplored. This work takes an initial step
toward assessing the reasoning abilities of deep time series forecasting
models. We find that certain linear, MLP-based, and patch-based Transformer
models generalize effectively in systematically orchestrated
out-of-distribution scenarios, suggesting underexplored reasoning capabilities
beyond simple pattern memorization.",2024-09-17,"Willa Potosnak, Cristian Challu, Mononito Goswami, Michał Wiliński, Nina Żukowska, Artur Dubrawski",http://arxiv.org/pdf/2409.10840v4,cs.LG
Machine Learning for Public Good: Predicting Urban Crime Patterns to Enhance Community Safety,"In recent years, urban safety has become a paramount concern for city
planners and law enforcement agencies. Accurate prediction of likely crime
occurrences can significantly enhance preventive measures and resource
allocation. However, many law enforcement departments lack the tools to analyze
and apply advanced AI and ML techniques that can support city planners, watch
programs, and safety leaders to take proactive steps towards overall community
safety.
  This paper explores the effectiveness of ML techniques to predict spatial and
temporal patterns of crimes in urban areas. Leveraging police dispatch call
data from San Jose, CA, the research goal is to achieve a high degree of
accuracy in categorizing calls into priority levels particularly for more
dangerous situations that require an immediate law enforcement response. This
categorization is informed by the time, place, and nature of the call. The
research steps include data extraction, preprocessing, feature engineering,
exploratory data analysis, implementation, optimization and tuning of different
supervised machine learning models and neural networks. The accuracy and
precision are examined for different models and features at varying granularity
of crime categories and location precision.
  The results demonstrate that when compared to a variety of other models,
Random Forest classification models are most effective in identifying dangerous
situations and their corresponding priority levels with high accuracy (Accuracy
= 85%, AUC = 0.92) at a local level while ensuring a minimum amount of false
negatives. While further research and data gathering is needed to include other
social and economic factors, these results provide valuable insights for law
enforcement agencies to optimize resources, develop proactive deployment
approaches, and adjust response patterns to enhance overall public safety
outcomes in an unbiased way.",2024-09-17,"Sia Gupta, Simeon Sayer",http://arxiv.org/pdf/2409.10838v1,cs.LG
Efficient and Personalized Mobile Health Event Prediction via Small Language Models,"Healthcare monitoring is crucial for early detection, timely intervention,
and the ongoing management of health conditions, ultimately improving
individuals' quality of life. Recent research shows that Large Language Models
(LLMs) have demonstrated impressive performance in supporting healthcare tasks.
However, existing LLM-based healthcare solutions typically rely on cloud-based
systems, which raise privacy concerns and increase the risk of personal
information leakage. As a result, there is growing interest in running these
models locally on devices like mobile phones and wearables to protect users'
privacy. Small Language Models (SLMs) are potential candidates to solve privacy
and computational issues, as they are more efficient and better suited for
local deployment. However, the performance of SLMs in healthcare domains has
not yet been investigated. This paper examines the capability of SLMs to
accurately analyze health data, such as steps, calories, sleep minutes, and
other vital statistics, to assess an individual's health status. Our results
show that, TinyLlama, which has 1.1 billion parameters, utilizes 4.31 GB
memory, and has 0.48s latency, showing the best performance compared other four
state-of-the-art (SOTA) SLMs on various healthcare applications. Our results
indicate that SLMs could potentially be deployed on wearable or mobile devices
for real-time health monitoring, providing a practical solution for efficient
and privacy-preserving healthcare.",2024-09-17,"Xin Wang, Ting Dang, Vassilis Kostakos, Hong Jia",http://arxiv.org/pdf/2409.18987v1,cs.LG
PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing,"The recent explosion of generative AI-Music systems has raised numerous
concerns over data copyright, licensing music from musicians, and the conflict
between open-source AI and large prestige companies. Such issues highlight the
need for publicly available, copyright-free musical data, in which there is a
large shortage, particularly for symbolic music data. To alleviate this issue,
we present PDMX: a large-scale open-source dataset of over 250K public domain
MusicXML scores collected from the score-sharing forum MuseScore, making it the
largest available copyright-free symbolic music dataset to our knowledge. PDMX
additionally includes a wealth of both tag and user interaction metadata,
allowing us to efficiently analyze the dataset and filter for high quality
user-generated scores. Given the additional metadata afforded by our data
collection process, we conduct multitrack music generation experiments
evaluating how different representative subsets of PDMX lead to different
behaviors in downstream models, and how user-rating statistics can be used as
an effective measure of data quality. Examples can be found at
https://pnlong.github.io/PDMX.demo/.",2024-09-17,"Phillip Long, Zachary Novack, Taylor Berg-Kirkpatrick, Julian McAuley",http://arxiv.org/pdf/2409.10831v2,cs.LG
Unveiling and Mitigating Bias in Large Language Model Recommendations: A Path to Fairness,"excel in delivering comprehensive suggestions by deeply analyzing content and
user behavior. However, they often inherit biases from skewed training data,
favoring mainstream content while underrepresenting diverse or non-traditional
options. This study explores the interplay between bias and LLM-based
recommendation systems, focusing on music, song, and book recommendations
across diverse demographic and cultural groups. This paper analyzes bias in
LLM-based recommendation systems across multiple models (GPT, LLaMA, and
Gemini), revealing its deep and pervasive impact on outcomes. Intersecting
identities and contextual factors, like socioeconomic status, further amplify
biases, complicating fair recommendations across diverse groups. Our findings
reveal that bias in these systems is deeply ingrained, yet even simple
interventions like prompt engineering can significantly reduce it. We further
propose a retrieval-augmented generation strategy to mitigate bias more
effectively. Numerical experiments validate these strategies, demonstrating
both the pervasive nature of bias and the impact of the proposed solutions.",2024-09-17,"Anindya Bijoy Das, Shahnewaz Karim Sakib",http://arxiv.org/pdf/2409.10825v3,cs.LG
PReLU: Yet Another Single-Layer Solution to the XOR Problem,"This paper demonstrates that a single-layer neural network using Parametric
Rectified Linear Unit (PReLU) activation can solve the XOR problem, a simple
fact that has been overlooked so far. We compare this solution to the
multi-layer perceptron (MLP) and the Growing Cosine Unit (GCU) activation
function and explain why PReLU enables this capability. Our results show that
the single-layer PReLU network can achieve 100\% success rate in a wider range
of learning rates while using only three learnable parameters.",2024-09-17,"Rafael C. Pinto, Anderson R. Tavares",http://arxiv.org/pdf/2409.10821v1,cs.LG
Fault Detection and Identification Using a Novel Process Decomposition Algorithm for Distributed Process Monitoring,"Recent progress in fault detection and identification increasingly relies on
sophisticated techniques for fault detection, applied through either
centralized or distributed approaches. Instead of increasing the sophistication
of the fault detection method, this work introduces a novel algorithm for
determining process blocks of interacting measurements and applies principal
component analysis (PCA) at the block level to identify fault occurrences.
Additionally, we define a novel contributions map that scales the magnitudes of
disparate faults to facilitate the visual identification of abnormal values of
measured variables and analysis of fault propagation. Bayesian aggregate fault
index and block fault indices vs. time pinpoint origins of the fault. The
proposed method yields fault detection rates on par with most sophisticated
centralized or distributed methods on the Tennessee Eastman Plant benchmark.
Since the decomposition algorithm relies on the process flowsheet and control
loop structures, practicing control engineers can implement the proposed method
in a straightforward manner.",2024-09-17,"Enrique Luna Villagomez, Vladimir Mahalec",http://arxiv.org/pdf/2409.11444v3,cs.LG
Quantum Kernel Learning for Small Dataset Modeling in Semiconductor Fabrication: Application to Ohmic Contact,"Complex semiconductor fabrication processes, such as Ohmic contact formation
in unconventional semiconductor devices, pose significant modeling challenges
due to a large number of operational variables and the difficulty of collecting
large, high-quality datasets. Classical machine learning (CML) models often
struggle in such scenarios, where the data is both high-dimensional and limited
in quantity, leading to overfitting and reduced predictive accuracy. To address
this challenge, we develop the first application of quantum machine learning
(QML) to model this semiconductor process, leveraging quantum systems' capacity
to efficiently capture complex correlations in high-dimensional spaces and
generalize well with small datasets. Using only 159 experimental samples
augmented via a variational autoencoder, we report a quantum kernel-based
regressor (SQKR) with a static 2-level ZZ feature map. The SQKR consistently
outperformed six mainstream CML models across all evaluation metrics, achieving
the lowest mean absolute error (MAE), mean squared error (MSE), and root mean
squared error (RMSE), with repeated experiments confirming its robustness.
Notably, SQKR achieved an MAE of 0.314 Ohm-mm with data from experimental
verification, demonstrating its ability to effectively model semiconductor
fabrication processes despite limited data availability. These results
highlight QML's unique capability to handle small yet high-dimensional datasets
in the semiconductor industry, making it a promising alternative to classical
approaches for semiconductor process modeling.",2024-09-17,"Zeheng Wang, Fangzhou Wang, Liang Li, Zirui Wang, Timothy van der Laan, Ross C. C. Leon, Jing-Kai Huang, Muhammad Usman",http://arxiv.org/pdf/2409.10803v2,cs.LG
Multi-frequency Electrical Impedance Tomography Reconstruction with Multi-Branch Attention Image Prior,"Multi-frequency Electrical Impedance Tomography (mfEIT) is a promising
biomedical imaging technique that estimates tissue conductivities across
different frequencies. Current state-of-the-art (SOTA) algorithms, which rely
on supervised learning and Multiple Measurement Vectors (MMV), require
extensive training data, making them time-consuming, costly, and less practical
for widespread applications. Moreover, the dependency on training data in
supervised MMV methods can introduce erroneous conductivity contrasts across
frequencies, posing significant concerns in biomedical applications. To address
these challenges, we propose a novel unsupervised learning approach based on
Multi-Branch Attention Image Prior (MAIP) for mfEIT reconstruction. Our method
employs a carefully designed Multi-Branch Attention Network (MBA-Net) to
represent multiple frequency-dependent conductivity images and simultaneously
reconstructs mfEIT images by iteratively updating its parameters. By leveraging
the implicit regularization capability of the MBA-Net, our algorithm can
capture significant inter- and intra-frequency correlations, enabling robust
mfEIT reconstruction without the need for training data. Through simulation and
real-world experiments, our approach demonstrates performance comparable to, or
better than, SOTA algorithms while exhibiting superior generalization
capability. These results suggest that the MAIP-based method can be used to
improve the reliability and applicability of mfEIT in various settings.",2024-09-17,"Hao Fang, Zhe Liu, Yi Feng, Zhen Qiu, Pierre Bagnaninchi, Yunjie Yang",http://arxiv.org/pdf/2409.10794v1,cs.LG
Physics-Informed Neural Networks with Trust-Region Sequential Quadratic Programming,"Physics-Informed Neural Networks (PINNs) represent a significant advancement
in Scientific Machine Learning (SciML), which integrate physical domain
knowledge into an empirical loss function as soft constraints and apply
existing machine learning methods to train the model. However, recent research
has noted that PINNs may fail to learn relatively complex Partial Differential
Equations (PDEs). This paper addresses the failure modes of PINNs by
introducing a novel, hard-constrained deep learning method -- trust-region
Sequential Quadratic Programming (trSQP-PINN). In contrast to directly training
the penalized soft-constrained loss as in PINNs, our method performs a
linear-quadratic approximation of the hard-constrained loss, while leveraging
the soft-constrained loss to adaptively adjust the trust-region radius. We only
trust our model approximations and make updates within the trust region, and
such an updating manner can overcome the ill-conditioning issue of PINNs. We
also address the computational bottleneck of second-order SQP methods by
employing quasi-Newton updates for second-order information, and importantly,
we introduce a simple pretraining step to further enhance training efficiency
of our method. We demonstrate the effectiveness of trSQP-PINN through extensive
experiments. Compared to existing hard-constrained methods for PINNs, such as
penalty methods and augmented Lagrangian methods, trSQP-PINN significantly
improves the accuracy of the learned PDE solutions, achieving up to 1-3 orders
of magnitude lower errors. Additionally, our pretraining step is generally
effective for other hard-constrained methods, and experiments have shown the
robustness of our method against both problem-specific parameters and algorithm
tuning parameters.",2024-09-16,"Xiaoran Cheng, Sen Na",http://arxiv.org/pdf/2409.10777v1,cs.LG
Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?,"Image classification models, including convolutional neural networks (CNNs),
perform well on a variety of classification tasks but struggle under conditions
of partial occlusion, i.e., conditions in which objects are partially covered
from the view of a camera. Methods to improve performance under occlusion,
including data augmentation, part-based clustering, and more inherently robust
architectures, including Vision Transformer (ViT) models, have, to some extent,
been evaluated on their ability to classify objects under partial occlusion.
However, evaluations of these methods have largely relied on images containing
artificial occlusion, which are typically computer-generated and therefore
inexpensive to label. Additionally, methods are rarely compared against each
other, and many methods are compared against early, now outdated, deep learning
models. We contribute the Image Recognition Under Occlusion (IRUO) dataset,
based on the recently developed Occluded Video Instance Segmentation (OVIS)
dataset (arXiv:2102.01558). IRUO utilizes real-world and artificially occluded
images to test and benchmark leading methods' robustness to partial occlusion
in visual recognition tasks. In addition, we contribute the design and results
of a human study using images from IRUO that evaluates human classification
performance at multiple levels and types of occlusion. We find that modern
CNN-based models show improved recognition accuracy on occluded images compared
to earlier CNN-based models, and ViT-based models are more accurate than
CNN-based models on occluded images, performing only modestly worse than human
accuracy. We also find that certain types of occlusion, including diffuse
occlusion, where relevant objects are seen through ""holes"" in occluders such as
fences and leaves, can greatly reduce the accuracy of deep recognition models
as compared to humans, especially those with CNN backbones.",2024-09-16,"Kaleb Kassaw, Francesco Luzi, Leslie M. Collins, Jordan M. Malof",http://arxiv.org/pdf/2409.10775v1,cs.LG
Provably Efficient Infinite-Horizon Average-Reward Reinforcement Learning with Linear Function Approximation,"This paper proposes a computationally tractable algorithm for learning
infinite-horizon average-reward linear Markov decision processes (MDPs) and
linear mixture MDPs under the Bellman optimality condition. While guaranteeing
computational efficiency, our algorithm for linear MDPs achieves the best-known
regret upper bound of
$\widetilde{\mathcal{O}}(d^{3/2}\mathrm{sp}(v^*)\sqrt{T})$ over $T$ time steps
where $\mathrm{sp}(v^*)$ is the span of the optimal bias function $v^*$ and $d$
is the dimension of the feature mapping. For linear mixture MDPs, our algorithm
attains a regret bound of
$\widetilde{\mathcal{O}}(d\cdot\mathrm{sp}(v^*)\sqrt{T})$. The algorithm
applies novel techniques to control the covering number of the value function
class and the span of optimistic estimators of the value function, which is of
independent interest.",2024-09-16,"Woojin Chae, Dabeen Lee",http://arxiv.org/pdf/2409.10772v2,cs.LG
Federated Learning for Smart Grid: A Survey on Applications and Potential Vulnerabilities,"The Smart Grid (SG) is a critical energy infrastructure that collects
real-time electricity usage data to forecast future energy demands using
information and communication technologies (ICT). Due to growing concerns about
data security and privacy in SGs, federated learning (FL) has emerged as a
promising training framework. FL offers a balance between privacy, efficiency,
and accuracy in SGs by enabling collaborative model training without sharing
private data from IoT devices. In this survey, we thoroughly review recent
advancements in designing FL-based SG systems across three stages: generation,
transmission and distribution, and consumption. Additionally, we explore
potential vulnerabilities that may arise when implementing FL in these stages.
Finally, we discuss the gap between state-of-the-art FL research and its
practical applications in SGs and propose future research directions. These
focus on potential attack and defense strategies for FL-based SG systems and
the need to build a robust FL-based SG infrastructure. Unlike traditional
surveys that address security issues in centralized machine learning methods
for SG systems, this survey specifically examines the applications and security
concerns in FL-based SG systems for the first time. Our aim is to inspire
further research into applications and improvements in the robustness of
FL-based SG systems.",2024-09-16,"Zikai Zhang, Suman Rath, Jiaohao Xu, Tingsong Xiao",http://arxiv.org/pdf/2409.10764v1,cs.LG
Multidimensional Human Activity Recognition With Large Language Model: A Conceptual Framework,"In high-stake environments like emergency response or elder care, the
integration of large language model (LLM), revolutionize risk assessment,
resource allocation, and emergency responses in Human Activity Recognition
(HAR) systems by leveraging data from various wearable sensors. We propose a
conceptual framework that utilizes various wearable devices, each considered as
a single dimension, to support a multidimensional learning approach within HAR
systems. By integrating and processing data from these diverse sources, LLMs
can process and translate complex sensor inputs into actionable insights. This
integration mitigates the inherent uncertainties and complexities associated
with them, and thus enhancing the responsiveness and effectiveness of emergency
services. This paper sets the stage for exploring the transformative potential
of LLMs within HAR systems in empowering emergency workers to navigate the
unpredictable and risky environments they encounter in their critical roles.",2024-09-16,Syed Mhamudul Hasan,http://arxiv.org/pdf/2410.03546v1,cs.LG
Trustworthy Conceptual Explanations for Neural Networks in Robot Decision-Making,"Black box neural networks are an indispensable part of modern robots.
Nevertheless, deploying such high-stakes systems in real-world scenarios poses
significant challenges when the stakeholders, such as engineers and legislative
bodies, lack insights into the neural networks' decision-making process.
Presently, explainable AI is primarily tailored to natural language processing
and computer vision, falling short in two critical aspects when applied in
robots: grounding in decision-making tasks and the ability to assess
trustworthiness of their explanations. In this paper, we introduce a
trustworthy explainable robotics technique based on human-interpretable,
high-level concepts that attribute to the decisions made by the neural network.
Our proposed technique provides explanations with associated uncertainty scores
by matching neural network's activations with human-interpretable
visualizations. To validate our approach, we conducted a series of experiments
with various simulated and real-world robot decision-making models,
demonstrating the effectiveness of the proposed approach as a post-hoc,
human-friendly robot learning diagnostic tool.",2024-09-16,"Som Sagar, Aditya Taparia, Harsh Mankodiya, Pranav Bidare, Yifan Zhou, Ransalu Senanayake",http://arxiv.org/pdf/2409.10733v1,cs.LG
On the effects of similarity metrics in decentralized deep learning under distributional shift,"Decentralized Learning (DL) enables privacy-preserving collaboration among
organizations or users to enhance the performance of local deep learning
models. However, model aggregation becomes challenging when client data is
heterogeneous, and identifying compatible collaborators without direct data
exchange remains a pressing issue. In this paper, we investigate the
effectiveness of various similarity metrics in DL for identifying peers for
model merging, conducting an empirical analysis across multiple datasets with
distribution shifts. Our research provides insights into the performance of
these metrics, examining their role in facilitating effective collaboration. By
exploring the strengths and limitations of these metrics, we contribute to the
development of robust DL methods.",2024-09-16,"Edvin Listo Zec, Tom Hagander, Eric Ihre-Thomason, Sarunas Girdzijauskas",http://arxiv.org/pdf/2409.10720v2,cs.LG
Online Learning via Memory: Retrieval-Augmented Detector Adaptation,"This paper presents a novel way of online adapting any off-the-shelf object
detection model to a novel domain without retraining the detector model.
Inspired by how humans quickly learn knowledge of a new subject (e.g.,
memorization), we allow the detector to look up similar object concepts from
memory during test time. This is achieved through a retrieval augmented
classification (RAC) module together with a memory bank that can be flexibly
updated with new domain knowledge. We experimented with various off-the-shelf
open-set detector and close-set detectors. With only a tiny memory bank (e.g.,
10 images per category) and being training-free, our online learning method
could significantly outperform baselines in adapting a detector to novel
domains.",2024-09-16,"Yanan Jian, Fuxun Yu, Qi Zhang, William Levine, Brandon Dubbs, Nikolaos Karianakis",http://arxiv.org/pdf/2409.10716v1,cs.LG
Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation with LLMs,"The growing demand for AI training data has transformed data annotation into
a global industry, but traditional approaches relying on human annotators are
often time-consuming, labor-intensive, and prone to inconsistent quality. We
propose the Model-in-the-Loop (MILO) framework, which integrates AI/ML models
into the annotation process. Our research introduces a collaborative paradigm
that leverages the strengths of both professional human annotators and large
language models (LLMs). By employing LLMs as pre-annotation and real-time
assistants, and judges on annotator responses, MILO enables effective
interaction patterns between human annotators and LLMs. Three empirical studies
on multimodal data annotation demonstrate MILO's efficacy in reducing handling
time, improving data quality, and enhancing annotator experiences. We also
introduce quality rubrics for flexible evaluation and fine-grained feedback on
open-ended annotations. The MILO framework has implications for accelerating
AI/ML development, reducing reliance on human annotation alone, and promoting
better alignment between human and machine values.",2024-09-16,"Yifan Wang, David Stevens, Pranay Shah, Wenwen Jiang, Miao Liu, Xu Chen, Robert Kuo, Na Li, Boying Gong, Daniel Lee, Jiabo Hu, Ning Zhang, Bob Kamma",http://arxiv.org/pdf/2409.10702v2,cs.LG
A Green Multi-Attribute Client Selection for Over-The-Air Federated Learning: A Grey-Wolf-Optimizer Approach,"Federated Learning (FL) has gained attention across various industries for
its capability to train machine learning models without centralizing sensitive
data. While this approach offers significant benefits such as privacy
preservation and decreased communication overhead, it presents several
challenges, including deployment complexity and interoperability issues,
particularly in heterogeneous scenarios or resource-constrained environments.
Over-the-air (OTA) FL was introduced to tackle these challenges by
disseminating model updates without necessitating direct device-to-device
connections or centralized servers. However, OTA-FL brought forth limitations
associated with heightened energy consumption and network latency. In this
paper, we propose a multi-attribute client selection framework employing the
grey wolf optimizer (GWO) to strategically control the number of participants
in each round and optimize the OTA-FL process while considering accuracy,
energy, delay, reliability, and fairness constraints of participating devices.
We evaluate the performance of our multi-attribute client selection approach in
terms of model loss minimization, convergence time reduction, and energy
efficiency. In our experimental evaluation, we assessed and compared the
performance of our approach against the existing state-of-the-art methods. Our
results demonstrate that the proposed GWO-based client selection outperforms
these baselines across various metrics. Specifically, our approach achieves a
notable reduction in model loss, accelerates convergence time, and enhances
energy efficiency while maintaining high fairness and reliability indicators.",2024-09-16,"Maryam Ben Driss, Essaid Sabir, Halima Elbiaze, Abdoulaye Baniré Diallo, Mohamed Sadik",http://arxiv.org/pdf/2409.11442v1,cs.LG
Using Generative Models to Produce Realistic Populations of the United Kingdom Windstorms,"Windstorms significantly impact the UK, causing extensive damage to property,
disrupting society, and potentially resulting in loss of life. Accurate
modelling and understanding of such events are essential for effective risk
assessment and mitigation. However, the rarity of extreme windstorms results in
limited observational data, which poses significant challenges for
comprehensive analysis and insurance modelling. This dissertation explores the
application of generative models to produce realistic synthetic wind field
data, aiming to enhance the robustness of current CAT models used in the
insurance industry. The study utilises hourly reanalysis data from the ERA5
dataset, which covers the period from 1940 to 2022. Three models, including
standard GANs, WGAN-GP, and U-net diffusion models, were employed to generate
high-quality wind maps of the UK. These models are then evaluated using
multiple metrics, including SSIM, KL divergence, and EMD, with some assessments
performed in a reduced dimensionality space using PCA. The results reveal that
while all models are effective in capturing the general spatial
characteristics, each model exhibits distinct strengths and weaknesses. The
standard GAN introduced more noise compared to the other models. The WGAN-GP
model demonstrated superior performance, particularly in replicating
statistical distributions. The U-net diffusion model produced the most visually
coherent outputs but struggled slightly in replicating peak intensities and
their statistical variability. This research underscores the potential of
generative models in supplementing limited reanalysis datasets with synthetic
data, providing valuable tools for risk assessment and catastrophe modelling.
However, it is important to select appropriate evaluation metrics that assess
different aspects of the generated outputs. Future work could refine these
models and incorporate more ...",2024-09-16,Etron Yee Chun Tsoi,http://arxiv.org/pdf/2409.10696v1,cs.LG
Mitigating Partial Observability in Adaptive Traffic Signal Control with Transformers,"Efficient traffic signal control is essential for managing urban
transportation, minimizing congestion, and improving safety and sustainability.
Reinforcement Learning (RL) has emerged as a promising approach to enhancing
adaptive traffic signal control (ATSC) systems, allowing controllers to learn
optimal policies through interaction with the environment. However, challenges
arise due to partial observability (PO) in traffic networks, where agents have
limited visibility, hindering effectiveness. This paper presents the
integration of Transformer-based controllers into ATSC systems to address PO
effectively. We propose strategies to enhance training efficiency and
effectiveness, demonstrating improved coordination capabilities in real-world
scenarios. The results showcase the Transformer-based model's ability to
capture significant information from historical observations, leading to better
control policies and improved traffic flow. This study highlights the potential
of leveraging the advanced Transformer architecture to enhance urban
transportation management.",2024-09-16,"Xiaoyu Wang, Ayal Taitler, Scott Sanner, Baher Abdulhai",http://arxiv.org/pdf/2409.10693v1,cs.LG
Mitigating Sex Bias in Audio Data-driven COPD and COVID-19 Breathing Pattern Detection Models,"In the healthcare industry, researchers have been developing machine learning
models to automate diagnosing patients with respiratory illnesses based on
their breathing patterns. However, these models do not consider the demographic
biases, particularly sex bias, that often occur when models are trained with a
skewed patient dataset. Hence, it is essential in such an important industry to
reduce this bias so that models can make fair diagnoses. In this work, we
examine the bias in models used to detect breathing patterns of two major
respiratory diseases, i.e., chronic obstructive pulmonary disease (COPD) and
COVID-19. Using decision tree models trained with audio recordings of breathing
patterns obtained from two open-source datasets consisting of 29 COPD and 680
COVID-19-positive patients, we analyze the effect of sex bias on the models.
With a threshold optimizer and two constraints (demographic parity and
equalized odds) to mitigate the bias, we witness 81.43% (demographic parity
difference) and 71.81% (equalized odds difference) improvements. These findings
are statistically significant.",2024-09-16,"Rachel Pfeifer, Sudip Vhaduri, James Eric Dietz",http://arxiv.org/pdf/2409.10677v1,cs.LG
Toward Mitigating Sex Bias in Pilot Trainees' Stress and Fatigue Modeling,"While researchers have been trying to understand the stress and fatigue among
pilots, especially pilot trainees, and to develop stress/fatigue models to
automate the process of detecting stress/fatigue, they often do not consider
biases such as sex in those models. However, in a critical profession like
aviation, where the demographic distribution is disproportionately skewed to
one sex, it is urgent to mitigate biases for fair and safe model predictions.
In this work, we investigate the perceived stress/fatigue of 69 college
students, including 40 pilot trainees with around 63% male. We construct models
with decision trees first without bias mitigation and then with bias mitigation
using a threshold optimizer with demographic parity and equalized odds
constraints 30 times with random instances. Using bias mitigation, we achieve
improvements of 88.31% (demographic parity difference) and 54.26% (equalized
odds difference), which are also found to be statistically significant.",2024-09-16,"Rachel Pfeifer, Sudip Vhaduri, Mark Wilson, Julius Keller",http://arxiv.org/pdf/2409.10676v1,cs.LG
A Bayesian Interpretation of Adaptive Low-Rank Adaptation,"Motivated by the sensitivity-based importance score of the adaptive low-rank
adaptation (AdaLoRA), we utilize more theoretically supported metrics,
including the signal-to-noise ratio (SNR), along with the Improved Variational
Online Newton (IVON) optimizer, for adaptive parameter budget allocation. The
resulting Bayesian counterpart not only has matched or surpassed the
performance of using the sensitivity-based importance metric but is also a
faster alternative to AdaLoRA with Adam. Our theoretical analysis reveals a
significant connection between the two metrics, providing a Bayesian
perspective on the efficacy of sensitivity as an importance score. Furthermore,
our findings suggest that the magnitude, rather than the variance, is the
primary indicator of the importance of parameters.",2024-09-16,"Haolin Chen, Philip N. Garner",http://arxiv.org/pdf/2409.10673v2,cs.LG
Continual Learning of Conjugated Visual Representations through Higher-order Motion Flows,"Learning with neural networks from a continuous stream of visual information
presents several challenges due to the non-i.i.d. nature of the data. However,
it also offers novel opportunities to develop representations that are
consistent with the information flow. In this paper we investigate the case of
unsupervised continual learning of pixel-wise features subject to multiple
motion-induced constraints, therefore named motion-conjugated feature
representations. Differently from existing approaches, motion is not a given
signal (either ground-truth or estimated by external modules), but is the
outcome of a progressive and autonomous learning process, occurring at various
levels of the feature hierarchy. Multiple motion flows are estimated with
neural networks and characterized by different levels of abstractions, spanning
from traditional optical flow to other latent signals originating from
higher-level features, hence called higher-order motions. Continuously learning
to develop consistent multi-order flows and representations is prone to trivial
solutions, which we counteract by introducing a self-supervised contrastive
loss, spatially-aware and based on flow-induced similarity. We assess our model
on photorealistic synthetic streams and real-world videos, comparing to
pre-trained state-of-the art feature extractors (also based on Transformers)
and to recent unsupervised learning models, significantly outperforming these
alternatives.",2024-09-16,"Simone Marullo, Matteo Tiezzi, Marco Gori, Stefano Melacci",http://arxiv.org/pdf/2409.11441v1,cs.LG
Logic Synthesis Optimization with Predictive Self-Supervision via Causal Transformers,"Contemporary hardware design benefits from the abstraction provided by
high-level logic gates, streamlining the implementation of logic circuits.
Logic Synthesis Optimization (LSO) operates at one level of abstraction within
the Electronic Design Automation (EDA) workflow, targeting improvements in
logic circuits with respect to performance metrics such as size and speed in
the final layout. Recent trends in the field show a growing interest in
leveraging Machine Learning (ML) for EDA, notably through ML-guided logic
synthesis utilizing policy-based Reinforcement Learning (RL) methods.Despite
these advancements, existing models face challenges such as overfitting and
limited generalization, attributed to constrained public circuits and the
expressiveness limitations of graph encoders. To address these hurdles, and
tackle data scarcity issues, we introduce LSOformer, a novel approach
harnessing Autoregressive transformer models and predictive SSL to predict the
trajectory of Quality of Results (QoR). LSOformer integrates cross-attention
modules to merge insights from circuit graphs and optimization sequences,
thereby enhancing prediction accuracy for QoR metrics. Experimental studies
validate the effectiveness of LSOformer, showcasing its superior performance
over baseline architectures in QoR prediction tasks, where it achieves
improvements of 5.74%, 4.35%, and 17.06% on the EPFL, OABCD, and proprietary
circuits datasets, respectively, in inductive setup.",2024-09-16,"Raika Karimi, Faezeh Faez, Yingxue Zhang, Xing Li, Lei Chen, Mingxuan Yuan, Mahdi Biparva",http://arxiv.org/pdf/2409.10653v2,cs.LG
CaBaGe: Data-Free Model Extraction using ClAss BAlanced Generator Ensemble,"Machine Learning as a Service (MLaaS) is often provided as a pay-per-query,
black-box system to clients. Such a black-box approach not only hinders open
replication, validation, and interpretation of model results, but also makes it
harder for white-hat researchers to identify vulnerabilities in the MLaaS
systems. Model extraction is a promising technique to address these challenges
by reverse-engineering black-box models. Since training data is typically
unavailable for MLaaS models, this paper focuses on the realistic version of
it: data-free model extraction. We propose a data-free model extraction
approach, CaBaGe, to achieve higher model extraction accuracy with a small
number of queries. Our innovations include (1) a novel experience replay for
focusing on difficult training samples; (2) an ensemble of generators for
steadily producing diverse synthetic data; and (3) a selective filtering
process for querying the victim model with harder, more balanced samples. In
addition, we create a more realistic setting, for the first time, where the
attacker has no knowledge of the number of classes in the victim training data,
and create a solution to learn the number of classes on the fly. Our evaluation
shows that CaBaGe outperforms existing techniques on seven datasets -- MNIST,
FMNIST, SVHN, CIFAR-10, CIFAR-100, ImageNet-subset, and Tiny ImageNet -- with
an accuracy improvement of the extracted models by up to 43.13%. Furthermore,
the number of queries required to extract a clone model matching the final
accuracy of prior work is reduced by up to 75.7%.",2024-09-16,"Jonathan Rosenthal, Shanchao Liang, Kevin Zhang, Lin Tan",http://arxiv.org/pdf/2409.10643v1,cs.LG
Exploring Fine-tuned Generative Models for Keyphrase Selection: A Case Study for Russian,"Keyphrase selection plays a pivotal role within the domain of scholarly
texts, facilitating efficient information retrieval, summarization, and
indexing. In this work, we explored how to apply fine-tuned generative
transformer-based models to the specific task of keyphrase selection within
Russian scientific texts. We experimented with four distinct generative models,
such as ruT5, ruGPT, mT5, and mBART, and evaluated their performance in both
in-domain and cross-domain settings. The experiments were conducted on the
texts of Russian scientific abstracts from four domains: mathematics & computer
science, history, medicine, and linguistics. The use of generative models,
namely mBART, led to gains in in-domain performance (up to 4.9% in BERTScore,
9.0% in ROUGE-1, and 12.2% in F1-score) over three keyphrase extraction
baselines for the Russian language. Although the results for cross-domain usage
were significantly lower, they still demonstrated the capability to surpass
baseline performances in several cases, underscoring the promising potential
for further exploration and refinement in this research field.",2024-09-16,"Anna Glazkova, Dmitry Morozov",http://arxiv.org/pdf/2409.10640v2,cs.LG
RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval,"Transformer-based Large Language Models (LLMs) have become increasingly
important. However, due to the quadratic time complexity of attention
computation, scaling LLMs to longer contexts incurs extremely slow inference
speed and high GPU memory consumption for caching key-value (KV) vectors. This
paper proposes RetrievalAttention, a training-free approach to both accelerate
attention computation and reduce GPU memory consumption. By leveraging the
dynamic sparsity of attention mechanism, RetrievalAttention proposes to build
approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory
and retrieve the most relevant ones through vector search during generation.
Unfortunately, we observe that the off-the-shelf ANNS indexes are often
ineffective for such retrieval tasks due to the out-of-distribution (OOD)
between query vectors and key vectors in the attention mechanism.
RetrievalAttention addresses the OOD challenge by designing an attention-aware
vector search algorithm that can adapt to the distribution of query vectors.
Our evaluation demonstrates that RetrievalAttention achieves near full
attention accuracy while only requiring access to 1--3% of the data. This leads
to a significant reduction in the inference cost of long-context LLMs, with a
much lower GPU memory footprint. In particular, RetrievalAttention only needs a
single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,
which is capable of generating one token in 0.188 seconds.",2024-09-16,"Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu",http://arxiv.org/pdf/2409.10516v3,cs.LG
Context-Aware Predictive Coding: A Representation Learning Framework for WiFi Sensing,"WiFi sensing is an emerging technology that utilizes wireless signals for
various sensing applications. However, the reliance on supervised learning, the
scarcity of labelled data, and the incomprehensible channel state information
(CSI) pose significant challenges. These issues affect deep learning models'
performance and generalization across different environments. Consequently,
self-supervised learning (SSL) is emerging as a promising strategy to extract
meaningful data representations with minimal reliance on labelled samples. In
this paper, we introduce a novel SSL framework called Context-Aware Predictive
Coding (CAPC), which effectively learns from unlabelled data and adapts to
diverse environments. CAPC integrates elements of Contrastive Predictive Coding
(CPC) and the augmentation-based SSL method, Barlow Twins, promoting temporal
and contextual consistency in data representations. This hybrid approach
captures essential temporal information in CSI, crucial for tasks like human
activity recognition (HAR), and ensures robustness against data distortions.
Additionally, we propose a unique augmentation, employing both uplink and
downlink CSI to isolate free space propagation effects and minimize the impact
of electronic distortions of the transceiver. Our evaluations demonstrate that
CAPC not only outperforms other SSL methods and supervised approaches, but also
achieves superior generalization capabilities. Specifically, CAPC requires
fewer labelled samples while significantly outperforming supervised learning
and surpassing SSL baselines. Furthermore, our transfer learning studies on an
unseen dataset with a different HAR task and environment showcase an accuracy
improvement of 1.8 percent over other SSL baselines and 24.7 percent over
supervised learning, emphasizing its exceptional cross-domain adaptability.",2024-09-16,"B. Barahimi, H. Tabassum, M. Omer, O. Waqar",http://arxiv.org/pdf/2410.01825v1,cs.LG
Kolmogorov-Arnold Transformer,"Transformers stand as the cornerstone of mordern deep learning.
Traditionally, these models rely on multi-layer perceptron (MLP) layers to mix
the information between channels. In this paper, we introduce the
Kolmogorov-Arnold Transformer (KAT), a novel architecture that replaces MLP
layers with Kolmogorov-Arnold Network (KAN) layers to enhance the
expressiveness and performance of the model. Integrating KANs into
transformers, however, is no easy feat, especially when scaled up.
Specifically, we identify three key challenges: (C1) Base function. The
standard B-spline function used in KANs is not optimized for parallel computing
on modern hardware, resulting in slower inference speeds. (C2) Parameter and
Computation Inefficiency. KAN requires a unique function for each input-output
pair, making the computation extremely large. (C3) Weight initialization. The
initialization of weights in KANs is particularly challenging due to their
learnable activation functions, which are critical for achieving convergence in
deep neural networks. To overcome the aforementioned challenges, we propose
three key solutions: (S1) Rational basis. We replace B-spline functions with
rational functions to improve compatibility with modern GPUs. By implementing
this in CUDA, we achieve faster computations. (S2) Group KAN. We share the
activation weights through a group of neurons, to reduce the computational load
without sacrificing performance. (S3) Variance-preserving initialization. We
carefully initialize the activation weights to make sure that the activation
variance is maintained across layers. With these designs, KAT scales
effectively and readily outperforms traditional MLP-based transformers.",2024-09-16,"Xingyi Yang, Xinchao Wang",http://arxiv.org/pdf/2409.10594v1,cs.LG
Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles,"Causal language modeling using the Transformer architecture has yielded
remarkable capabilities in Large Language Models (LLMs) over the last few
years. However, the extent to which fundamental search and reasoning
capabilities emerged within LLMs remains a topic of ongoing debate. In this
work, we study if causal language modeling can learn a complex task such as
solving Sudoku puzzles. To solve a Sudoku, the model is first required to
search over all empty cells of the puzzle to decide on a cell to fill and then
apply an appropriate strategy to fill the decided cell. Sometimes, the
application of a strategy only results in thinning down the possible values in
a cell rather than concluding the exact value of the cell. In such cases,
multiple strategies are applied one after the other to fill a single cell. We
observe that Transformer models trained on this synthetic task can indeed learn
to solve Sudokus (our model solves $94.21\%$ of the puzzles fully correctly)
when trained on a logical sequence of steps taken by a solver. We find that
training Transformers with the logical sequence of steps is necessary and
without such training, they fail to learn Sudoku. We also extend our analysis
to Zebra puzzles (known as Einstein puzzles) and show that the model solves
$92.04 \%$ of the puzzles fully correctly. In addition, we study the internal
representations of the trained Transformer and find that through linear
probing, we can decode information about the set of possible values in any
given cell from them, pointing to the presence of a strong reasoning engine
implicit in the Transformer weights.",2024-09-16,"Kulin Shah, Nishanth Dikkala, Xin Wang, Rina Panigrahy",http://arxiv.org/pdf/2409.10502v1,cs.LG
Partial Distribution Matching via Partial Wasserstein Adversarial Networks,"This paper studies the problem of distribution matching (DM), which is a
fundamental machine learning problem seeking to robustly align two probability
distributions. Our approach is established on a relaxed formulation, called
partial distribution matching (PDM), which seeks to match a fraction of the
distributions instead of matching them completely. We theoretically derive the
Kantorovich-Rubinstein duality for the partial Wasserstain-1 (PW) discrepancy,
and develop a partial Wasserstein adversarial network (PWAN) that efficiently
approximates the PW discrepancy based on this dual form. Partial matching can
then be achieved by optimizing the network using gradient descent. Two
practical tasks, point set registration and partial domain adaptation are
investigated, where the goals are to partially match distributions in 3D space
and high-dimensional feature space respectively. The experiment results confirm
that the proposed PWAN effectively produces highly robust matching results,
performing better or on par with the state-of-the-art methods.",2024-09-16,"Zi-Ming Wang, Nan Xue, Ling Lei, Rebecka Jörnsten, Gui-Song Xia",http://arxiv.org/pdf/2409.10499v2,cs.LG
CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios,"Large Language Models (LLMs) have been widely adopted to process long-context
tasks. However, the large memory overhead of the key-value (KV) cache poses
significant challenges in long-context scenarios. Existing training-free KV
cache compression methods typically focus on quantization and token pruning,
which have compression limits, and excessive sparsity can lead to severe
performance degradation. Other methods design new architectures with less KV
overhead but require significant training overhead. To address the above two
drawbacks, we further explore the redundancy in the channel dimension and apply
an architecture-level design with minor training costs. Therefore, we introduce
CSKV, a training-efficient Channel Shrinking technique for KV cache
compression: (1) We first analyze the singular value distribution of the KV
cache, revealing significant redundancy and compression potential along the
channel dimension. Based on this observation, we propose using low-rank
decomposition for key and value layers and storing the low-dimension features.
(2) To preserve model performance, we introduce a bi-branch KV cache, including
a window-based full-precision KV cache and a low-precision compressed KV cache.
(3) To reduce the training costs, we minimize the layer-wise reconstruction
loss for the compressed KV cache instead of retraining the entire LLMs.
Extensive experiments show that CSKV can reduce the memory overhead of the KV
cache by 80% while maintaining the model's long-context capability. Moreover,
we show that our method can be seamlessly combined with quantization to further
reduce the memory overhead, achieving a compression ratio of up to 95%. Code is
available at https://github.com/wln20/CSKV.",2024-09-16,"Luning Wang, Shiyao Li, Xuefei Ning, Zhihang Yuan, Shengen Yan, Guohao Dai, Yu Wang",http://arxiv.org/pdf/2409.10593v3,cs.LG
MusicLIME: Explainable Multimodal Music Understanding,"Multimodal models are critical for music understanding tasks, as they capture
the complex interplay between audio and lyrics. However, as these models become
more prevalent, the need for explainability grows-understanding how these
systems make decisions is vital for ensuring fairness, reducing bias, and
fostering trust. In this paper, we introduce MusicLIME, a model-agnostic
feature importance explanation method designed for multimodal music models.
Unlike traditional unimodal methods, which analyze each modality separately
without considering the interaction between them, often leading to incomplete
or misleading explanations, MusicLIME reveals how audio and lyrical features
interact and contribute to predictions, providing a holistic view of the
model's decision-making. Additionally, we enhance local explanations by
aggregating them into global explanations, giving users a broader perspective
of model behavior. Through this work, we contribute to improving the
interpretability of multimodal music models, empowering users to make informed
choices, and fostering more equitable, fair, and transparent music
understanding systems.",2024-09-16,"Theodoros Sotirou, Vassilis Lyberatos, Orfeas Menis Mastromichalakis, Giorgos Stamou",http://arxiv.org/pdf/2409.10496v5,cs.LG
Flash STU: Fast Spectral Transform Units,"Recent advances in state-space model architectures have shown great promise
for efficient sequence modeling, but challenges remain in balancing
computational efficiency with model expressiveness. We propose the Flash STU
architecture, a hybrid model that interleaves spectral state space model layers
with sliding window attention, enabling scalability to billions of parameters
for language modeling while maintaining a near-linear time complexity. We
evaluate the Flash STU and its variants on diverse sequence prediction tasks,
including linear dynamical systems, robotics control, and language modeling. We
find that, given a fixed parameter budget, the Flash STU architecture
consistently outperforms the Transformer and other leading state-space models
such as S4 and Mamba-2.",2024-09-16,"Y. Isabel Liu, Windsor Nguyen, Yagiz Devre, Evan Dogariu, Anirudha Majumdar, Elad Hazan",http://arxiv.org/pdf/2409.10489v4,cs.LG
Online Nonconvex Bilevel Optimization with Bregman Divergences,"Bilevel optimization methods are increasingly relevant within machine
learning, especially for tasks such as hyperparameter optimization and
meta-learning. Compared to the offline setting, online bilevel optimization
(OBO) offers a more dynamic framework by accommodating time-varying functions
and sequentially arriving data. This study addresses the online
nonconvex-strongly convex bilevel optimization problem. In deterministic
settings, we introduce a novel online Bregman bilevel optimizer (OBBO) that
utilizes adaptive Bregman divergences. We demonstrate that OBBO enhances the
known sublinear rates for bilevel local regret through a novel hypergradient
error decomposition that adapts to the underlying geometry of the problem. In
stochastic contexts, we introduce the first stochastic online bilevel optimizer
(SOBBO), which employs a window averaging method for updating outer-level
variables using a weighted average of recent stochastic approximations of
hypergradients. This approach not only achieves sublinear rates of bilevel
local regret but also serves as an effective variance reduction strategy,
obviating the need for additional stochastic gradient samples at each timestep.
Experiments on online hyperparameter optimization and online meta-learning
highlight the superior performance, efficiency, and adaptability of our
Bregman-based algorithms compared to established online and offline bilevel
benchmarks.",2024-09-16,"Jason Bohne, David Rosenberg, Gary Kazantsev, Pawel Polak",http://arxiv.org/pdf/2409.10470v1,cs.LG
Kolmogorov-Arnold Networks in Low-Data Regimes: A Comparative Study with Multilayer Perceptrons,"Multilayer Perceptrons (MLPs) have long been a cornerstone in deep learning,
known for their capacity to model complex relationships. Recently,
Kolmogorov-Arnold Networks (KANs) have emerged as a compelling alternative,
utilizing highly flexible learnable activation functions directly on network
edges, a departure from the neuron-centric approach of MLPs. However, KANs
significantly increase the number of learnable parameters, raising concerns
about their effectiveness in data-scarce environments. This paper presents a
comprehensive comparative study of MLPs and KANs from both algorithmic and
experimental perspectives, with a focus on low-data regimes. We introduce an
effective technique for designing MLPs with unique, parameterized activation
functions for each neuron, enabling a more balanced comparison with KANs. Using
empirical evaluations on simulated data and two real-world data sets from
medicine and engineering, we explore the trade-offs between model complexity
and accuracy, with particular attention to the role of network depth. Our
findings show that MLPs with individualized activation functions achieve
significantly higher predictive accuracy with only a modest increase in
parameters, especially when the sample size is limited to around one hundred.
For example, in a three-class classification problem within additive
manufacturing, MLPs achieve a median accuracy of 0.91, significantly
outperforming KANs, which only reach a median accuracy of 0.53 with default
hyperparameters. These results offer valuable insights into the impact of
activation function selection in neural networks.",2024-09-16,Farhad Pourkamali-Anaraki,http://arxiv.org/pdf/2409.10463v1,cs.LG
Signed Graph Autoencoder for Explainable and Polarization-Aware Network Embeddings,"Autoencoders based on Graph Neural Networks (GNNs) have garnered significant
attention in recent years for their ability to extract informative latent
representations, characterizing the structure of complex topologies, such as
graphs. Despite the prevalence of Graph Autoencoders, there has been limited
focus on developing and evaluating explainable neural-based graph generative
models specifically designed for signed networks. To address this gap, we
propose the Signed Graph Archetypal Autoencoder (SGAAE) framework. SGAAE
extracts node-level representations that express node memberships over distinct
extreme profiles, referred to as archetypes, within the network. This is
achieved by projecting the graph onto a learned polytope, which governs its
polarization. The framework employs a recently proposed likelihood for
analyzing signed networks based on the Skellam distribution, combined with
relational archetypal analysis and GNNs. Our experimental evaluation
demonstrates the SGAAEs' capability to successfully infer node memberships over
the different underlying latent structures while extracting competing
communities formed through the participation of the opposing views in the
network. Additionally, we introduce the 2-level network polarization problem
and show how SGAAE is able to characterize such a setting. The proposed model
achieves high performance in different tasks of signed link prediction across
four real-world datasets, outperforming several baseline models.",2024-09-16,"Nikolaos Nakis, Chrysoula Kosma, Giannis Nikolentzos, Michalis Chatzianastasis, Iakovos Evdaimon, Michalis Vazirgiannis",http://arxiv.org/pdf/2409.10452v3,cs.LG
Structure-preserving learning for multi-symplectic PDEs,"This paper presents an energy-preserving machine learning method for
inferring reduced-order models (ROMs) by exploiting the multi-symplectic form
of partial differential equations (PDEs). The vast majority of
energy-preserving reduced-order methods use symplectic Galerkin projection to
construct reduced-order Hamiltonian models by projecting the full models onto a
symplectic subspace. However, symplectic projection requires the existence of
fully discrete operators, and in many cases, such as black-box PDE solvers,
these operators are inaccessible. In this work, we propose an energy-preserving
machine learning method that can infer the dynamics of the given PDE using data
only, so that the proposed framework does not depend on the fully discrete
operators. In this context, the proposed method is non-intrusive. The proposed
method is grey box in the sense that it requires only some basic knowledge of
the multi-symplectic model at the partial differential equation level. We prove
that the proposed method satisfies spatially discrete local energy conservation
and preserves the multi-symplectic conservation laws. We test our method on the
linear wave equation, the Korteweg-de Vries equation, and the
Zakharov-Kuznetsov equation. We test the generalization of our learned models
by testing them far outside the training time interval.",2024-09-16,"Süleyman Yıldız, Pawan Goyal, Peter Benner",http://arxiv.org/pdf/2409.10432v1,cs.LG
TPFL: Tsetlin-Personalized Federated Learning with Confidence-Based Clustering,"The world of Machine Learning (ML) has witnessed rapid changes in terms of
new models and ways to process users data. The majority of work that has been
done is focused on Deep Learning (DL) based approaches. However, with the
emergence of new algorithms such as the Tsetlin Machine (TM) algorithm, there
is growing interest in exploring alternative approaches that may offer unique
advantages in certain domains or applications. One of these domains is
Federated Learning (FL), in which users privacy is of utmost importance. Due to
its novelty, FL has seen a surge in the incorporation of personalization
techniques to enhance model accuracy while maintaining user privacy under
personalized conditions. In this work, we propose a novel approach called TPFL:
Tsetlin-Personalized Federated Learning, in which models are grouped into
clusters based on their confidence towards a specific class. In this way,
clustering can benefit from two key advantages. Firstly, clients share only
what they are confident about, resulting in the elimination of wrongful weight
aggregation among clients whose data for a specific class may have not been
enough during the training. This phenomenon is prevalent when the data are
non-Independent and Identically Distributed (non-IID). Secondly, by sharing
only weights towards a specific class, communication cost is substantially
reduced, making TPLF efficient in terms of both accuracy and communication
cost. The TPFL results were compared with 6 other baseline methods; namely
FedAvg, FedProx, FLIS DC, FLIS HC, IFCA and FedTM. The results demonstrated
that TPFL performance better than baseline methods with 98.94% accuracy on
MNIST, 98.52% accuracy on FashionMNIST and 91.16% accuracy on FEMNIST dataset.",2024-09-16,"Rasoul Jafari Gohari, Laya Aliahmadipour, Ezat Valipour",http://arxiv.org/pdf/2409.10392v4,cs.LG
Revising the Structure of Recurrent Neural Networks to Eliminate Numerical Derivatives in Forming Physics Informed Loss Terms with Respect to Time,"Solving unsteady partial differential equations (PDEs) using recurrent neural
networks (RNNs) typically requires numerical derivatives between each block of
the RNN to form the physics informed loss function. However, this introduces
the complexities of numerical derivatives into the training process of these
models. In this study, we propose modifying the structure of the traditional
RNN to enable the prediction of each block over a time interval, making it
possible to calculate the derivative of the output with respect to time using
the backpropagation algorithm. To achieve this, the time intervals of these
blocks are overlapped, defining a mutual loss function between them.
Additionally, the employment of conditional hidden states enables us to achieve
a unique solution for each block. The forget factor is utilized to control the
influence of the conditional hidden state on the prediction of the subsequent
block. This new model, termed the Mutual Interval RNN (MI-RNN), is applied to
solve three different benchmarks: the Burgers equation, unsteady heat
conduction in an irregular domain, and the Green vortex problem. Our results
demonstrate that MI-RNN can find the exact solution more accurately compared to
existing RNN models. For instance, in the second problem, MI-RNN achieved one
order of magnitude less relative error compared to the RNN model with numerical
derivatives.",2024-09-16,"Mahyar Jahani-nasab, Mohamad Ali Bijarchi",http://arxiv.org/pdf/2409.10388v1,cs.LG
Offline Reinforcement Learning for Learning to Dispatch for Job Shop Scheduling,"The Job Shop Scheduling Problem (JSSP) is a complex combinatorial
optimization problem. While online Reinforcement Learning (RL) has shown
promise by quickly finding acceptable solutions for JSSP, it faces key
limitations: it requires extensive training interactions from scratch leading
to sample inefficiency, cannot leverage existing high-quality solutions, and
often yields suboptimal results compared to traditional methods like Constraint
Programming (CP). We introduce Offline Reinforcement Learning for Learning to
Dispatch (Offline-LD), which addresses these limitations by learning from
previously generated solutions. Our approach is motivated by scenarios where
historical scheduling data and expert solutions are available, although our
current evaluation focuses on benchmark problems. Offline-LD adapts two
CQL-based Q-learning methods (mQRDQN and discrete mSAC) for maskable action
spaces, introduces a novel entropy bonus modification for discrete SAC, and
exploits reward normalization through preprocessing. Our experiments
demonstrate that Offline-LD outperforms online RL on both generated and
benchmark instances. Notably, by introducing noise into the expert dataset, we
achieve similar or better results than those obtained from the expert dataset,
suggesting that a more diverse training set is preferable because it contains
counterfactual information.",2024-09-16,"Jesse van Remmerden, Zaharah Bukhsh, Yingqian Zhang",http://arxiv.org/pdf/2409.10589v3,cs.LG
Learning Gentle Grasping from Human-Free Force Control Demonstration,"Humans can steadily and gently grasp unfamiliar objects based on tactile
perception. Robots still face challenges in achieving similar performance due
to the difficulty of learning accurate grasp-force predictions and force
control strategies that can be generalized from limited data. In this article,
we propose an approach for learning grasping from ideal force control
demonstrations, to achieve similar performance of human hands with limited data
size. Our approach utilizes objects with known contact characteristics to
automatically generate reference force curves without human demonstrations. In
addition, we design the dual convolutional neural networks (Dual-CNN)
architecture which incorporats a physics-based mechanics module for learning
target grasping force predictions from demonstrations. The described method can
be effectively applied in vision-based tactile sensors and enables gentle and
stable grasping of objects from the ground. The described prediction model and
grasping strategy were validated in offline evaluations and online experiments,
and the accuracy and generalizability were demonstrated.",2024-09-16,"Mingxuan Li, Lunwei Zhang, Tiemin Li, Yao Jiang",http://arxiv.org/pdf/2409.10371v2,cs.LG
Uncovering the Mechanism of Hepatotoxiciy of PFAS Targeting L-FABP Using GCN and Computational Modeling,"Per- and polyfluoroalkyl substances (PFAS) are persistent environmental
pollutants with known toxicity and bioaccumulation issues. Their widespread
industrial use and resistance to degradation have led to global environmental
contamination and significant health concerns. While a minority of PFAS have
been extensively studied, the toxicity of many PFAS remains poorly understood
due to limited direct toxicological data. This study advances the predictive
modeling of PFAS toxicity by combining semi-supervised graph convolutional
networks (GCNs) with molecular descriptors and fingerprints. We propose a novel
approach to enhance the prediction of PFAS binding affinities by isolating
molecular fingerprints to construct graphs where then descriptors are set as
the node features. This approach specifically captures the structural,
physicochemical, and topological features of PFAS without overfitting due to an
abundance of features. Unsupervised clustering then identifies representative
compounds for detailed binding studies. Our results provide a more accurate
ability to estimate PFAS hepatotoxicity to provide guidance in chemical
discovery of new PFAS and the development of new safety regulations.",2024-09-16,"Lucas Jividen, Tibo Duran, Xi-Zhi Niu, Jun Bai",http://arxiv.org/pdf/2409.10370v1,cs.LG
2D or not 2D: How Does the Dimensionality of Gesture Representation Affect 3D Co-Speech Gesture Generation?,"Co-speech gestures are fundamental for communication. The advent of recent
deep learning techniques has facilitated the creation of lifelike, synchronous
co-speech gestures for Embodied Conversational Agents. ""In-the-wild"" datasets,
aggregating video content from platforms like YouTube via human pose detection
technologies, provide a feasible solution by offering 2D skeletal sequences
aligned with speech. Concurrent developments in lifting models enable the
conversion of these 2D sequences into 3D gesture databases. However, it is
important to note that the 3D poses estimated from the 2D extracted poses are,
in essence, approximations of the ground-truth, which remains in the 2D domain.
This distinction raises questions about the impact of gesture representation
dimensionality on the quality of generated motions - a topic that, to our
knowledge, remains largely unexplored. Our study examines the effect of using
either 2D or 3D joint coordinates as training data on the performance of
speech-to-gesture deep generative models. We employ a lifting model for
converting generated 2D pose sequences into 3D and assess how gestures created
directly in 3D stack up against those initially generated in 2D and then
converted to 3D. We perform an objective evaluation using widely used metrics
in the gesture generation field as well as a user study to qualitatively
evaluate the different approaches.",2024-09-16,"Téo Guichoux, Laure Soulier, Nicolas Obin, Catherine Pelachaud",http://arxiv.org/pdf/2409.10357v2,cs.LG
Hyperedge Modeling in Hypergraph Neural Networks by using Densest Overlapping Subgraphs,"Hypergraphs tackle the limitations of traditional graphs by introducing {\em
hyperedges}. While graph edges connect only two nodes, hyperedges connect an
arbitrary number of nodes along their edges. Also, the underlying
message-passing mechanisms in Hypergraph Neural Networks (HGNNs) are in the
form of vertex-hyperedge-vertex, which let HGNNs capture and utilize richer and
more complex structural information than traditional Graph Neural Networks
(GNNs). More recently, the idea of overlapping subgraphs has emerged. These
subgraphs can capture more information about subgroups of vertices without
limiting one vertex belonging to just one group, allowing vertices to belong to
multiple groups or subgraphs. In addition, one of the most important problems
in graph clustering is to find densest overlapping subgraphs (DOS). In this
paper, we propose a solution to the DOS problem via Agglomerative Greedy
Enumeration (DOSAGE) algorithm as a novel approach to enhance the process of
generating the densest overlapping subgraphs and, hence, a robust construction
of the hypergraphs. Experiments on standard benchmarks show that the DOSAGE
algorithm significantly outperforms the HGNNs and six other methods on the node
classification task.",2024-09-16,"Mehrad Soltani, Luis Rueda",http://arxiv.org/pdf/2409.10340v1,cs.LG
Machine Learning to Detect Anxiety Disorders from Error-Related Negativity and EEG Signals,"Anxiety is a common mental health condition characterised by excessive worry,
fear and apprehension about everyday situations. Even with significant progress
over the past few years, predicting anxiety from electroencephalographic (EEG)
signals, specifically using error-related negativity (ERN), still remains
challenging. Following the PRISMA protocol, this paper systematically reviews
54 research papers on using EEG and ERN markers for anxiety detection published
in the last 10 years (2013 -- 2023). Our analysis highlights the wide usage of
traditional machine learning, such as support vector machines and random
forests, as well as deep learning models, such as convolutional neural networks
and recurrent neural networks across different data types. Our analysis reveals
that the development of a robust and generic anxiety prediction method still
needs to address real-world challenges, such as task-specific setup, feature
selection and computational modelling. We conclude this review by offering
potential future direction for non-invasive, objective anxiety diagnostics,
deployed across diverse populations and anxiety sub-types.",2024-09-16,"Ramya Chandrasekar, Md Rakibul Hasan, Shreya Ghosh, Tom Gedeon, Md Zakir Hossain",http://arxiv.org/pdf/2410.00028v1,cs.LG
VAE-QWGAN: Addressing Mode Collapse in Quantum GANs via Autoencoding Priors,"Recent proposals for quantum generative adversarial networks (GANs) suffer
from the issue of mode collapse, analogous to classical GANs, wherein the
distribution learnt by the GAN fails to capture the high mode complexities of
the target distribution. Mode collapse can arise due to the use of uninformed
prior distributions in the generative learning task. To alleviate the issue of
mode collapse for quantum GANs, this work presents a novel \textbf{hybrid
quantum-classical generative model}, the VAE-QWGAN, which combines the
strengths of a classical Variational AutoEncoder (VAE) with a hybrid Quantum
Wasserstein GAN (QWGAN). The VAE-QWGAN fuses the VAE decoder and QWGAN
generator into a single quantum model, and utilizes the VAE encoder for
data-dependant latent vector sampling during training. This in turn, enhances
the diversity and quality of generated images. To generate new data from the
trained model at inference, we sample from a Gaussian mixture model (GMM) prior
that is learnt on the latent vectors generated during training. We conduct
extensive experiments for image generation QGANs on MNIST/Fashion-MNIST
datasets and compute a range of metrics that measure the diversity and quality
of generated samples. We show that VAE-QWGAN demonstrates significant
improvement over existing QGAN approaches.",2024-09-16,"Aaron Mark Thomas, Harry Youel, Sharu Theresa Jose",http://arxiv.org/pdf/2409.10339v2,cs.LG
Research and Design of a Financial Intelligent Risk Control Platform Based on Big Data Analysis and Deep Machine Learning,"In the financial field of the United States, the application of big data
technology has become one of the important means for financial institutions to
enhance competitiveness and reduce risks. The core objective of this article is
to explore how to fully utilize big data technology to achieve complete
integration of internal and external data of financial institutions, and create
an efficient and reliable platform for big data collection, storage, and
analysis. With the continuous expansion and innovation of financial business,
traditional risk management models are no longer able to meet the increasingly
complex market demands. This article adopts big data mining and real-time
streaming data processing technology to monitor, analyze, and alert various
business data. Through statistical analysis of historical data and precise
mining of customer transaction behavior and relationships, potential risks can
be more accurately identified and timely responses can be made. This article
designs and implements a financial big data intelligent risk control platform.
This platform not only achieves effective integration, storage, and analysis of
internal and external data of financial institutions, but also intelligently
displays customer characteristics and their related relationships, as well as
intelligent supervision of various risk information",2024-09-16,"Shuochen Bi, Yufan Lian, Ziyue Wang",http://arxiv.org/pdf/2409.10331v1,cs.LG
On the Hardness of Meaningful Local Guarantees in Nonsmooth Nonconvex Optimization,"We study the oracle complexity of nonsmooth nonconvex optimization, with the
algorithm assumed to have access only to local function information. It has
been shown by Davis, Drusvyatskiy, and Jiang (2023) that for nonsmooth
Lipschitz functions satisfying certain regularity and strictness conditions,
perturbed gradient descent converges to local minimizers asymptotically.
Motivated by this result and by other recent algorithmic advances in nonconvex
nonsmooth optimization concerning Goldstein stationarity, we consider the
question of obtaining a non-asymptotic rate of convergence to local minima for
this problem class.
  We provide the following negative answer to this question: Local algorithms
acting on regular Lipschitz functions cannot, in the worst case, provide
meaningful local guarantees in terms of function value in sub-exponential time,
even when all near-stationary points are global minima. This sharply contrasts
with the smooth setting, for which it is well-known that standard gradient
methods can do so in a dimension-independent rate. Our result complements the
rich body of work in the theoretical computer science literature that provide
hardness results conditional on conjectures such as $\mathsf{P}\neq\mathsf{NP}$
or cryptographic assumptions, in that ours holds unconditional of any such
assumptions.",2024-09-16,"Guy Kornowski, Swati Padmanabhan, Ohad Shamir",http://arxiv.org/pdf/2409.10323v1,cs.LG
SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary Learning for Closed-Loop Scenario Generation,"Verification and validation of autonomous driving (AD) systems and components
is of increasing importance, as such technology increases in real-world
prevalence. Safety-critical scenario generation is a key approach to robustify
AD policies through closed-loop training. However, existing approaches for
scenario generation rely on simplistic objectives, resulting in
overly-aggressive or non-reactive adversarial behaviors. To generate diverse
adversarial yet realistic scenarios, we propose SEAL, a scenario perturbation
approach which leverages learned objective functions and adversarial,
human-like skills. SEAL-perturbed scenarios are more realistic than SOTA
baselines, leading to improved ego task success across real-world,
in-distribution, and out-of-distribution scenarios, of more than 20%. To
facilitate future research, we release our code and tools:
https://github.com/cmubig/SEAL",2024-09-16,"Benjamin Stoler, Ingrid Navarro, Jonathan Francis, Jean Oh",http://arxiv.org/pdf/2409.10320v2,cs.LG
Deep Learning tools to support deforestation monitoring in the Ivory Coast using SAR and Optical satellite imagery,"Deforestation is gaining an increasingly importance due to its strong
influence on the sorrounding environment, especially in developing countries
where population has a disadvantaged economic condition and agriculture is the
main source of income. In Ivory Coast, for instance, where the cocoa production
is the most remunerative activity, it is not rare to assist to the replacement
of portion of ancient forests with new cocoa plantations. In order to monitor
this type of deleterious activities, satellites can be employed to recognize
the disappearance of the forest to prevent it from expand its area of interest.
In this study, Forest-Non-Forest map (FNF) has been used as ground truth for
models based on Sentinel images input. State-of-the-art models U-Net, Attention
U-Net, Segnet and FCN32 are compared over different years combining Sentinel-1,
Sentinel-2 and cloud probability to create forest/non-forest segmentation.
Although Ivory Coast lacks of forest coverage datasets and is partially covered
by Sentinel images, it is demonstrated the feasibility to create models
classifying forest and non-forests pixels over the area using open datasets to
predict where deforestation could have occurred. Although a significant portion
of the deforestation research is carried out on visible bands, SAR acquisitions
are employed to overcome the limits of RGB images over areas often covered by
clouds. Finally, the most promising model is employed to estimate the hectares
of forest has been cut between 2019 and 2020.",2024-09-16,"Gabriele Sartor, Matteo Salis, Stefano Pinardi, Ozgur Saracik, Rosa Meo",http://arxiv.org/pdf/2409.11186v1,cs.LG
Spiers Memorial Lecture: How to do impactful research in artificial intelligence for chemistry and materials science,"Machine learning has been pervasively touching many fields of science.
Chemistry and materials science are no exception. While machine learning has
been making a great impact, it is still not reaching its full potential or
maturity. In this perspective, we first outline current applications across a
diversity of problems in chemistry. Then, we discuss how machine learning
researchers view and approach problems in the field. Finally, we provide our
considerations for maximizing impact when researching machine learning for
chemistry.",2024-09-16,"Austin Cheng, Cher Tian Ser, Marta Skreta, Andrés Guzmán-Cordero, Luca Thiede, Andreas Burger, Abdulrahman Aldossary, Shi Xuan Leong, Sergio Pablo-García, Felix Strieth-Kalthoff, Alán Aspuru-Guzik",http://arxiv.org/pdf/2409.10304v2,cs.LG
ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework,"Empathetic response generation necessitates the integration of emotional and
intentional dynamics to foster meaningful interactions. Existing research
either neglects the intricate interplay between emotion and intent, leading to
suboptimal controllability of empathy, or resorts to large language models
(LLMs), which incur significant computational overhead. In this paper, we
introduce ReflectDiffu, a lightweight and comprehensive framework for
empathetic response generation. This framework incorporates emotion contagion
to augment emotional expressiveness and employs an emotion-reasoning mask to
pinpoint critical emotional elements. Additionally, it integrates intent
mimicry within reinforcement learning for refinement during diffusion. By
harnessing an intent twice reflect mechanism of Exploring-Sampling-Correcting,
ReflectDiffu adeptly translates emotional decision-making into precise intent
actions, thereby addressing empathetic response misalignments stemming from
emotional misrecognition. Through reflection, the framework maps emotional
states to intents, markedly enhancing both response empathy and flexibility.
Comprehensive experiments reveal that ReflectDiffu outperforms existing models
regarding relevance, controllability, and informativeness, achieving
state-of-the-art results in both automatic and human evaluations.",2024-09-16,"Jiahao Yuan, Zixiang Di, Zhiqing Cui, Guisong Yang, Usman Naseem",http://arxiv.org/pdf/2409.10289v3,cs.LG
Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation,"Accurate and robust medical image classification is a challenging task,
especially in application domains where available annotated datasets are small
and present high imbalance between target classes. Considering that data
acquisition is not always feasible, especially for underrepresented classes,
our approach introduces a novel synthetic augmentation strategy using
class-specific Variational Autoencoders (VAEs) and latent space interpolation
to improve discrimination capabilities.
  By generating realistic, varied synthetic data that fills feature space gaps,
we address issues of data scarcity and class imbalance. The method presented in
this paper relies on the interpolation of latent representations within each
class, thus enriching the training set and improving the model's
generalizability and diagnostic accuracy. The proposed strategy was tested in a
small dataset of 321 images created to train and validate an automatic method
for assessing the quality of cleanliness of esophagogastroduodenoscopy images.
By combining real and synthetic data, an increase of over 18\% in the accuracy
of the most challenging underrepresented class was observed. The proposed
strategy not only benefited the underrepresented class but also led to a
general improvement in other metrics, including a 6\% increase in global
accuracy and precision.",2024-09-16,"Neil De La Fuente, Mireia Majó, Irina Luzko, Henry Córdova, Gloria Fernández-Esparrach, Jorge Bernal",http://arxiv.org/pdf/2409.10286v2,cs.LG
A Simple Model to Estimate Sharing Effects in Social Networks,"Randomised Controlled Trials (RCTs) are the gold standard for estimating
treatment effects across many fields of science. Technology companies have
adopted A/B-testing methods as a modern RCT counterpart, where end-users are
randomly assigned various system variants and user behaviour is tracked
continuously. The objective is then to estimate the causal effect that the
treatment variant would have on certain metrics of interest to the business.
  When the outcomes for randomisation units -- end-users in this case -- are
not statistically independent, this obfuscates identifiability of treatment
effects, and harms decision-makers' observability of the system. Social
networks exemplify this, as they are designed to promote inter-user
interactions. This interference by design notoriously complicates measurement
of, e.g., the effects of sharing. In this work, we propose a simple Markov
Decision Process (MDP)-based model describing user sharing behaviour in social
networks. We derive an unbiased estimator for treatment effects under this
model, and demonstrate through reproducible synthetic experiments that it
outperforms existing methods by a significant margin.",2024-09-16,Olivier Jeunen,http://arxiv.org/pdf/2409.12203v1,cs.LG
BAFNet: Bilateral Attention Fusion Network for Lightweight Semantic Segmentation of Urban Remote Sensing Images,"Large-scale semantic segmentation networks often achieve high performance,
while their application can be challenging when faced with limited sample sizes
and computational resources. In scenarios with restricted network size and
computational complexity, models encounter significant challenges in capturing
long-range dependencies and recovering detailed information in images. We
propose a lightweight bilateral semantic segmentation network called bilateral
attention fusion network (BAFNet) to efficiently segment high-resolution urban
remote sensing images. The model consists of two paths, namely dependency path
and remote-local path. The dependency path utilizes large kernel attention to
acquire long-range dependencies in the image. Besides, multi-scale local
attention and efficient remote attention are designed to construct remote-local
path. Finally, a feature aggregation module is designed to effectively utilize
the different features of the two paths. Our proposed method was tested on
public high-resolution urban remote sensing datasets Vaihingen and Potsdam,
with mIoU reaching 83.20% and 86.53%, respectively. As a lightweight semantic
segmentation model, BAFNet not only outperforms advanced lightweight models in
accuracy but also demonstrates comparable performance to non-lightweight
state-of-the-art methods on two datasets, despite a tenfold variance in
floating-point operations and a fifteenfold difference in network parameters.",2024-09-16,"Wentao Wang, Xili Wang",http://arxiv.org/pdf/2409.10269v1,cs.LG
Enhancing Personalized Recipe Recommendation Through Multi-Class Classification,"This paper intends to address the challenge of personalized recipe
recommendation in the realm of diverse culinary preferences. The problem domain
involves recipe recommendations, utilizing techniques such as association
analysis and classification. Association analysis explores the relationships
and connections between different ingredients to enhance the user experience.
Meanwhile, the classification aspect involves categorizing recipes based on
user-defined ingredients and preferences. A unique aspect of the paper is the
consideration of recipes and ingredients belonging to multiple classes,
recognizing the complexity of culinary combinations. This necessitates a
sophisticated approach to classification and recommendation, ensuring the
system accommodates the nature of recipe categorization. The paper seeks not
only to recommend recipes but also to explore the process involved in achieving
accurate and personalized recommendations.",2024-09-16,"Harish Neelam, Koushik Sai Veerella",http://arxiv.org/pdf/2409.10267v1,cs.LG
MDL-Pool: Adaptive Multilevel Graph Pooling Based on Minimum Description Length,"Graph pooling compresses graphs and summarises their topological properties
and features in a vectorial representation. It is an essential part of deep
graph representation learning and is indispensable in graph-level tasks like
classification or regression. Current approaches pool hierarchical structures
in graphs by iteratively applying shallow pooling operators up to a fixed
depth. However, they disregard the interdependencies between structures at
different hierarchical levels and do not adapt to datasets that contain graphs
with different sizes that may require pooling with various depths. To address
these issues, we propose MDL-Pool, a pooling operator based on the minimum
description length (MDL) principle, whose loss formulation explicitly models
the interdependencies between different hierarchical levels and facilitates a
direct comparison between multiple pooling alternatives with different depths.
MDP-Pool builds on the map equation, an information-theoretic objective
function for community detection, which naturally implements Occam's razor and
balances between model complexity and goodness-of-fit via the MDL. We
demonstrate MDL-Pool's competitive performance in an empirical evaluation
against various baselines across standard graph classification datasets.",2024-09-16,"Jan von Pichowski, Christopher Blöcker, Ingo Scholtes",http://arxiv.org/pdf/2409.10263v2,cs.LG
Self-Updating Vehicle Monitoring Framework Employing Distributed Acoustic Sensing towards Real-World Settings,"The recent emergence of Distributed Acoustic Sensing (DAS) technology has
facilitated the effective capture of traffic-induced seismic data. The
traffic-induced seismic wave is a prominent contributor to urban vibrations and
contain crucial information to advance urban exploration and governance.
However, identifying vehicular movements within massive noisy data poses a
significant challenge. In this study, we introduce a real-time semi-supervised
vehicle monitoring framework tailored to urban settings. It requires only a
small fraction of manual labels for initial training and exploits unlabeled
data for model improvement. Additionally, the framework can autonomously adapt
to newly collected unlabeled data. Before DAS data undergo object detection as
two-dimensional images to preserve spatial information, we leveraged
comprehensive one-dimensional signal preprocessing to mitigate noise.
Furthermore, we propose a novel prior loss that incorporates the shapes of
vehicular traces to track a single vehicle with varying speeds. To evaluate our
model, we conducted experiments with seismic data from the Stanford 2 DAS
Array. The results showed that our model outperformed the baseline model
Efficient Teacher and its supervised counterpart, YOLO (You Only Look Once), in
both accuracy and robustness. With only 35 labeled images, our model surpassed
YOLO's mAP 0.5:0.95 criterion by 18% and showed a 7% increase over Efficient
Teacher. We conducted comparative experiments with multiple update strategies
for self-updating and identified an optimal approach. This approach surpasses
the performance of non-overfitting training conducted with all data in a single
pass.",2024-09-16,"Xi Wang, Xin Liu, Songming Zhu, Zhanwen Li, Lina Gao",http://arxiv.org/pdf/2409.10259v1,cs.LG
Hedging Is Not All You Need: A Simple Baseline for Online Learning Under Haphazard Inputs,"Handling haphazard streaming data, such as data from edge devices, presents a
challenging problem. Over time, the incoming data becomes inconsistent, with
missing, faulty, or new inputs reappearing. Therefore, it requires models that
are reliable. Recent methods to solve this problem depend on a hedging-based
solution and require specialized elements like auxiliary dropouts, forked
architectures, and intricate network design. We observed that hedging can be
reduced to a special case of weighted residual connection; this motivated us to
approximate it with plain self-attention. In this work, we propose HapNet, a
simple baseline that is scalable, does not require online backpropagation, and
is adaptable to varying input types. All present methods are restricted to
scaling with a fixed window; however, we introduce a more complex problem of
scaling with a variable window where the data becomes positionally
uncorrelated, and cannot be addressed by present methods. We demonstrate that a
variant of the proposed approach can work even for this complex scenario. We
extensively evaluated the proposed approach on five benchmarks and found
competitive performance.",2024-09-16,"Himanshu Buckchash, Momojit Biswas, Rohit Agarwal, Dilip K. Prasad",http://arxiv.org/pdf/2409.10242v2,cs.LG
Safety-Oriented Pruning and Interpretation of Reinforcement Learning Policies,"Pruning neural networks (NNs) can streamline them but risks removing vital
parameters from safe reinforcement learning (RL) policies. We introduce an
interpretable RL method called VERINTER, which combines NN pruning with model
checking to ensure interpretable RL safety. VERINTER exactly quantifies the
effects of pruning and the impact of neural connections on complex safety
properties by analyzing changes in safety measurements. This method maintains
safety in pruned RL policies and enhances understanding of their safety
dynamics, which has proven effective in multiple RL settings.",2024-09-16,"Dennis Gross, Helge Spieker",http://arxiv.org/pdf/2409.10218v1,cs.LG
Embedded Image-to-Image Translation for Efficient Sim-to-Real Transfer in Learning-based Robot-Assisted Soft Manipulation,"Recent advances in robotic learning in simulation have shown impressive
results in accelerating learning complex manipulation skills. However, the
sim-to-real gap, caused by discrepancies between simulation and reality, poses
significant challenges for the effective deployment of autonomous surgical
systems. We propose a novel approach utilizing image translation models to
mitigate domain mismatches and facilitate efficient robot skill learning in a
simulated environment. Our method involves the use of contrastive unpaired
Image-to-image translation, allowing for the acquisition of embedded
representations from these transformed images. Subsequently, these embeddings
are used to improve the efficiency of training surgical manipulation models. We
conducted experiments to evaluate the performance of our approach,
demonstrating that it significantly enhances task success rates and reduces the
steps required for task completion compared to traditional methods. The results
indicate that our proposed system effectively bridges the sim-to-real gap,
providing a robust framework for advancing the autonomy of surgical robots in
minimally invasive procedures.",2024-09-16,"Jacinto Colan, Keisuke Sugita, Ana Davila, Yutaro Yamada, Yasuhisa Hasegawa",http://arxiv.org/pdf/2409.10204v1,cs.LG
Efficient Milling Quality Prediction with Explainable Machine Learning,"This paper presents an explainable machine learning (ML) approach for
predicting surface roughness in milling. Utilizing a dataset from milling
aluminum alloy 2017A, the study employs random forest regression models and
feature importance techniques. The key contributions include developing ML
models that accurately predict various roughness values and identifying
redundant sensors, particularly those for measuring normal cutting force. Our
experiments show that removing certain sensors can reduce costs without
sacrificing predictive accuracy, highlighting the potential of explainable
machine learning to improve cost-effectiveness in machining.",2024-09-16,"Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch, Mohamed Elmansori",http://arxiv.org/pdf/2409.10203v1,cs.LG
A Literature Review of Keyword Spotting Technologies for Urdu,"This literature review surveys the advancements of keyword spotting (KWS)
technologies, specifically focusing on Urdu, Pakistan's low-resource language
(LRL), which has complex phonetics. Despite the global strides in speech
technology, Urdu presents unique challenges requiring more tailored solutions.
The review traces the evolution from foundational Gaussian Mixture Models to
sophisticated neural architectures like deep neural networks and transformers,
highlighting significant milestones such as integrating multi-task learning and
self-supervised approaches that leverage unlabeled data. It examines emerging
technologies' role in enhancing KWS systems' performance within multilingual
and resource-constrained settings, emphasizing the need for innovations that
cater to languages like Urdu. Thus, this review underscores the need for
context-specific research addressing the inherent complexities of Urdu and
similar URLs and the means of regions communicating through such languages for
a more inclusive approach to speech technology.",2024-09-16,Syed Muhammad Aqdas Rizvi,http://arxiv.org/pdf/2409.16317v1,cs.LG
Enhancing RL Safety with Counterfactual LLM Reasoning,"Reinforcement learning (RL) policies may exhibit unsafe behavior and are hard
to explain. We use counterfactual large language model reasoning to enhance RL
policy safety post-training. We show that our approach improves and helps to
explain the RL policy safety.",2024-09-16,"Dennis Gross, Helge Spieker",http://arxiv.org/pdf/2409.10188v1,cs.LG
TCDformer-based Momentum Transfer Model for Long-term Sports Prediction,"Accurate sports prediction is a crucial skill for professional coaches, which
can assist in developing effective training strategies and scientific
competition tactics. Traditional methods often use complex mathematical
statistical techniques to boost predictability, but this often is limited by
dataset scale and has difficulty handling long-term predictions with variable
distributions, notably underperforming when predicting point-set-game
multi-level matches. To deal with this challenge, this paper proposes TM2, a
TCDformer-based Momentum Transfer Model for long-term sports prediction, which
encompasses a momentum encoding module and a prediction module based on
momentum transfer. TM2 initially encodes momentum in large-scale unstructured
time series using the local linear scaling approximation (LLSA) module. Then it
decomposes the reconstructed time series with momentum transfer into trend and
seasonal components. The final prediction results are derived from the additive
combination of a multilayer perceptron (MLP) for predicting trend components
and wavelet attention mechanisms for seasonal components. Comprehensive
experimental results show that on the 2023 Wimbledon men's tournament datasets,
TM2 significantly surpasses existing sports prediction models in terms of
performance, reducing MSE by 61.64% and MAE by 63.64%.",2024-09-16,"Hui Liu, Jiacheng Gu, Xiyuan Huang, Junjie Shi, Tongtong Feng, Ning He",http://arxiv.org/pdf/2409.10176v1,cs.LG
Safe and Stable Closed-Loop Learning for Neural-Network-Supported Model Predictive Control,"Safe learning of control policies remains challenging, both in optimal
control and reinforcement learning. In this article, we consider safe learning
of parametrized predictive controllers that operate with incomplete information
about the underlying process. To this end, we employ Bayesian optimization for
learning the best parameters from closed-loop data. Our method focuses on the
system's overall long-term performance in closed-loop while keeping it safe and
stable. Specifically, we parametrize the stage cost function of an MPC using a
feedforward neural network. This allows for a high degree of flexibility,
enabling the system to achieve a better closed-loop performance with respect to
a superordinate measure. However, this flexibility also necessitates safety
measures, especially with respect to closed-loop stability. To this end, we
explicitly incorporated stability information in the
Bayesian-optimization-based learning procedure, thereby achieving rigorous
probabilistic safety guarantees. The proposed approach is illustrated using a
numeric example.",2024-09-16,"Sebastian Hirt, Maik Pfefferkorn, Rolf Findeisen",http://arxiv.org/pdf/2409.10171v1,cs.LG
